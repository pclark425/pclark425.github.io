<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-466 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-466</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-466</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-262054850</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.10371v1.pdf" target="_blank">Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs</a></p>
                <p><strong>Paper Abstract:</strong> A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are. It is found that many of the practical weaknesses of these AI systems can be tied specifically to lacks in the basic cognitive architectures according to which these systems are built. It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources. This does not imply there is nothing to learn about human-level AGI from studying and experimenting with LLMs, nor that LLMs cannot form significant parts of human-level AGI architectures that also incorporate other ideas. Social and ethical matters regarding LLMs are very briefly touched from this perspective, which implies that while care should be taken regarding misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course as with any powerfully impactful technology, overall the sort of policy needed as regards modern LLMs is quite different than would be the case if a more credible approximation to human-level AGI were at hand.</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e466.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e466.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenCog Hyperon (Neural Space)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenCog Hyperon with Neural Space integration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-paradigm AGI framework (OpenCog Hyperon) extended with a Neural Space that wraps LLMs, enabling tight interaction between a declarative Atomspace (metagraph) and neural models via automated translation between MeTTa (the Hyperon language) and natural language and by linking LLM internal representations to Atom nodes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenCog Hyperon + Neural Space</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>OpenCog Hyperon is a distributed metagraph-based cognitive substrate (the Atomspace) hosting multiple cognitive algorithms (probabilistic logical reasoning, evolutionary program learning, attractor nets, etc.). The Neural Space is an auxiliary space that encapsulates external neural networks (e.g., LLMs). Integration is achieved by automated conversion between natural language and MeTTa (pattern-matching queries) and by growing explicit links between Atoms and neural activation patterns so that neural outputs and metagraph reasoning can interoperate within the same architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Typed, weighted metagraph 'Atomspace' representing symbolic knowledge, rules, and probabilistic logic (OpenCog-style probabilistic logical representations and MeTTa code).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Large neural networks (e.g., transformer LLMs) wrapped as 'Neural Space' services; neural pattern recognizers and generative models used for surface pattern recognition and synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular hybrid integration: automated translation layers convert between MeTTa/metagraph queries and natural language; bidirectional interfaces allow LLM outputs to populate Atomspace and Atomspace queries to be sent to LLMs; optional direct links between Atom nodes and neural activation subspaces (mapping functions between neurons/activations and symbolic Atoms). Integration is iterative and mediated by the Atomspace logic and MeTTa runtime rather than end-to-end differentiable training.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables symbolic reasoning (deduction/induction/abduction) to supervise and refine neural outputs, and lets neural models provide flexible pattern recognition and generative proposals that the symbolic subsystem can verify, combine, or abstract; supports iterative retraining/data curation loops where symbolic inference produces training material for the LLM; expected emergent capabilities include improved factual grounding, richer concept formation via concept-blending in the symbolic substrate, and better control over hallucination by symbolic verification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Not evaluated on a single benchmark in this paper; described as an architectural approach for general AGI tasks (symbolic reasoning, planning, concept formation, tool use).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Architecturally intended to improve generalization by letting the Atomspace produce concise symbolic abstractions and constraints that guide neural synthesis; expected to improve out-of-distribution robustness relative to pure LLMs by anchoring propositions to symbolic knowledge and inference metadata (but no quantitative results reported).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: symbolic Atomspace supports inspectable chains of probabilistic logical inference and MeTTa-executable proofs/explanations; mappings from Atoms to neural activations can be examined to produce human-readable explanations of why an LLM produced a particular output (subject to the transparency of the mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Challenges include (1) discovering tractable, reliable mappings between distributed neural activations and symbolic Atoms; (2) engineering the translation layer (MeTTa↔natural language) robustly; (3) scaling coordination between many heterogeneous algorithms; (4) no reported quantitative benchmarks here to demonstrate efficacy.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Division-of-labor/complementarity: symbolic metagraphs supply explicit abstractions, verification and meta-reasoning while neural spaces supply pattern recognition and generative capacities; iterative symbolic→neural→symbolic co-training loop is advocated as the mechanism producing synergy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e466.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e466.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Bengio-Hu RL/MDL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bengio and Hu's RL/MDL proposal (LLM + MDL abstraction learner + RL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conceptual AGI architecture that couples a large generative model (LLM) with an Occam-style minimum-description-length (MDL) learner that extracts concise abstractions, and a reinforcement-learning (RL) controller that guides interaction with the environment and the two learners.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM + MDL abstraction learner + RL controller</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The LLM serves as a general pattern recognition and proposal generator; an MDL-driven learner attempts to infer concise symbolic/structured abstractions from the diffuse pattern manifold of the LLM outputs; an RL module governs how this LLM+MDL ensemble interacts with the environment (or data sources) to collect experience and refine abstractions. The MDL learner effectively functions as a declarative abstraction layer constructed from LLM-generated candidate patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>MDL-inferred concise abstractions (informal symbolic/structured models) acting as compact, declarative representations—conceptual summaries derived from LLM patterns (not a specific logic formalism specified in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Transformer LLM (generative neural model) and a reinforcement learning module to govern interactions/experience collection.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Two-stage iterative integration: LLM provides rich pattern outputs; MDL learner compresses these into compact abstractions; RL directs data collection and uses MDL abstractions to plan and evaluate actions. Integration is not described as end-to-end differentiable; it is an orchestrated modular loop where each module informs and trains the others.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Potentially reduces hallucination and spurious generalization by favoring compact models (MDL), enabling more robust conceptual blending and higher-level creativity through explicit abstractions; expected to produce better structured generalization and scientific-style concept formation compared to LLM alone.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Conceptual/architectural proposal rather than an implemented system in this paper; no benchmark evaluation presented here.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Argued qualitatively to improve generalization through MDL-driven abstraction (compact models generalize better), with the RL controller guiding environment-specific refinement; no quantitative OOD/compositionality results provided.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Improved interpretability is anticipated because MDL-produced abstractions are concise symbolic structures that can be inspected; these abstractions can serve as human-readable explanations for LLM behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>MDL learning at scale is computationally challenging/intractable in general; unclear how to reliably extract useful abstractions from a huge, overparameterized LLM; integration and scalability questions remain open.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Learning-theory/Occam principle: compactness correlates with better generalization; complementary strengths: LLM supplies breadth, MDL supplies concise abstractions, RL provides situated interaction and optimization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e466.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e466.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chameleon</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chameleon: Plug-and-play compositional reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system that uses GPT-4 to automatically assemble pipelines of specialized cognitive tools (search, knowledge retrieval, vision, calculators, etc.) tailored to a given task, improving multi-modal and multi-tool reasoning (reported to boost performance on tasks like ScienceQA).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chameleon: Plug- and -play compositional reasoning with large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chameleon (GPT-4 orchestrator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Chameleon leverages a central LLM (GPT-4) as an orchestrator that inspects a task, selects appropriate specialized external tools (search engines, knowledge retrievers, vision modules, calculators), composes them into a pipeline, and executes or delegates sub-tasks to those tools to achieve end-to-end solutions. The LLM acts as planner and integrator while tools provide deterministic or specialized capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>External symbolic knowledge/retrieval modules (search engines, knowledge bases) and structured tool outputs (e.g., retrieved facts or structured knowledge representations) used as declarative grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Central LLM (GPT-4) for planning/orchestration and various procedural tool modules (search, vision models, calculators, specialized APIs) that execute procedural computations.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular orchestration: the LLM composes and sequences calls to external tools; tool outputs are fed back into the LLM for further planning and response generation. Integration is runtime orchestration rather than end-to-end differentiable training.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Compositional problem solving beyond pure LLMs: can chain specialized procedural computations with retrieved declarative facts, yielding stronger performance on grounded multi-step tasks; reduces hallucination by delegating factual checks to reliable tools.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Multi-modal reasoning and science QA tasks (e.g., ScienceQA) and other composed reasoning tasks requiring external knowledge and procedural computation.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Improved practical generalization on domain-compositional tasks by leveraging specialized modules; performance is task-dependent and improved relative to GPT-4 alone in certain benchmarks (qualitative statement in paper), but no consistent universal generalization guarantee.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: orchestration logs and explicit tool call traces provide inspectable steps showing which modules were used and what inputs/outputs occurred, aiding explainability relative to pure LLM blackbox outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Quality depends on available tools and on the orchestrator's ability to select and sequence them properly; brittle when tools are mis-selected or when tool outputs contain errors; latency and engineering complexity increase with more tools.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Practical modular synergy: LLM as flexible planner + specialized deterministic/procedural tools for grounding and precise computation; emphasizes tool-mediated hybridization rather than deep symbolic integration.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e466.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e466.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DeepMind WebAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A real-world webagent with planning, long context understanding, and program synthesis (WebAgent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid architecture combining two language models (HTML-T5 for HTML understanding and Flan-U-PaLM for higher-level planning) to automate web tasks (navigating pages, filling forms, extracting and synthesizing information).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A real-world webagent with planning, long context understanding, and program synthesis</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WebAgent (HTML-T5 + Flan-U-PaLM composition)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>WebAgent composes specialized LMs: a model trained to parse and represent HTML (HTML-T5) provides structured representations of web pages (declarative-like structured data), while a planner model (Flan-U-PaLM) reasons about goals and synthesizes programs or action sequences to execute tasks on the web. The architecture passes structured web-representations to the planner and uses program synthesis to carry out actions.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Structured HTML-derived representations and retrieved web state (semi-structured symbolic data describing DOM elements and page state).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Transformer LMs performing planning and program synthesis (Flan-U-PaLM), and procedural executors that actuate actions (clicks, fills) on the web.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Pipeline composition: an HTML understanding model outputs structured state; a higher-level LLM consumes that structured state to plan and produce executable programs/scripts; a runtime executes the scripts and returns results to the planner for iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Robust web automation by combining symbolic/structured page representations with LLM-based planning and program synthesis; improved reliability in tasks requiring structured perception plus planning compared to single-model approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Web automation tasks (e.g., form filling, multi-step web workflows); reported to achieve efficient web automation in practical settings (qualitative description in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Better task generalization for diverse web pages due to explicit HTML-structure grounding, but depends on the quality of HTML parsing and planner generality.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Improved explainability via (1) structured HTML representations that can be inspected and (2) explicit synthesized programs that show intended action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on accuracy of HTML-to-structure model and on correct program synthesis; brittle to unexpected dynamic page behavior or sites with nonstandard DOMs; no universal performance metrics reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Modular pipeline synergy: structure extraction (declarative) feeding into planner/program synthesis (imperative) to achieve robust automation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e466.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e466.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM + Knowledge Graph / Wolfram Alpha integrations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integration of LLMs with declarative knowledge sources (knowledge graphs, WolframAlpha, ERNIE-style entity-aware transformers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that ground LLM outputs by connecting them to symbolic knowledge stores or computation engines (knowledge graphs, Wolfram Alpha) to reduce hallucination and improve factuality and verifiability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM + Knowledge Graph / Computational Engine</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Typical systems keep an LLM as the generative/planning module and connect it at runtime to a symbolic knowledge graph (KG) or computational engine (e.g., Wolfram Alpha). The KG serves as a declarative shared state and source of factual constraints; the LLM queries, injects or aligns its outputs with KG facts and/or delegates symbolic computation to engines.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Knowledge graphs (structured triples, ontologies) or computational engines (WolframAlpha) providing symbolic facts and exact computations.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Transformer LLMs for generation and orchestration; optionally learned adapters to convert between KG queries and natural language.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Hybrid modular coupling: LLMs generate queries or candidate facts which are checked or completed by KG/engine; KG outputs are fed back to LLM for editing/verifying responses. Integration may include co-training (e.g., ERNIE-style entity-aware pretraining) or runtime tool-calling.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Reduced factual hallucination, improved verifiability, and ability to perform exact computations or deterministic lookups unavailable to LLMs alone; some improvements in grounded reasoning and accuracy for knowledge-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Factually-grounded QA, question answering requiring exact computation, tasks sensitive to factual correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Grounding in KG improves factual generalization on knowledge-rich queries but may not improve creative or procedural reasoning; effectiveness depends on coverage and alignment between KG and LLM representations.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High: KG queries and returned facts offer inspectable evidence trails; computational engine outputs are deterministic and explainable.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>KG coverage limits; mismatch between LLM representations and KG schema; latency and engineering complexity; symbolic sources may be outdated or inconsistent.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Cognitive synergy: declarative symbolic stores provide reliable facts and constraints while LLMs provide flexible language and pattern-completion abilities.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e466.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e466.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Auto-GPT (agent wrappers)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Auto-GPT and similar agentic wrappers around LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Simple agent architectures that wrap LLMs with added components (planning, simple long-term memory, iterative execution) to pursue user-specified goals by decomposing them into steps and executing them iteratively.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Auto-GPT (LLM-agent wrapper)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Auto-GPT-style systems accept a high-level goal, use an LLM to produce a plan, then iteratively prompt the LLM to carry out plan steps, optionally storing intermediate results in a memory store and invoking external tools. They add basic goal management, memory read/write, and action-execution loops around the core LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Simple long-term and episodic memory stores (often implemented as text or vector stores) and externally supplied tool outputs (structured results) that act as declarative state.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>LLM for planning and step-level generation; procedural loop control code that issues prompts, calls tools, and updates memory.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Sequential orchestration: planner LLM generates steps; procedural controller executes steps and updates memory; memory contents are passed back into the LLM when producing subsequent steps.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Gives LLMs rudimentary agentic behavior (pursuit of goals over multiple steps), limited persistence and simple multi-step execution capability that a plain LLM chat session lacks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>General goal-directed automation tasks (ad-hoc); no standardized benchmark reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Limited: can handle stereotyped multi-step goals but fails on complex real-world goal management due to lack of robust episodic indexing, true long-term memory integration, and failure accumulation over long plans.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: explicit plan steps and memory updates can be inspected but internal LLM rationales remain opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Described as simplistic; brittle planning, hallucination compounded across steps, lack of genuine reflective sensing, poor long-term goal management; not sufficient for HCAGI without deeper architectural changes.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Practical agent-enablement via orchestration: demonstrates how adding lightweight memory/planning components yields agentic behavior but highlights need for deeper integration for autonomy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e466.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e466.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Code-based self-verification (LLM + Code Interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Code-based self-verification using LLM plus code-execution plugins (e.g., GPT-4 Code Interpreter)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hybrid technique in which an LLM generates procedural code to verify its candidate solutions (e.g., for math problems), then executes that code via an interpreter/plugin to self-check and refine answers, substantially improving accuracy on difficult math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM + Code Interpreter (self-verification)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>The LLM produces both a symbolic derivation/answer and a program that computes or verifies the result; the program is executed in a sandboxed environment (Code Interpreter plugin) to confirm or correct the LLM's answer, enabling iterative refinement and higher-precision computation.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic problem statements and structured program representations (code) that express precise computations / verifications.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>LLM (GPT-4) for program generation and rationale; a code execution environment (Python interpreter or code-exec plugin) to run verification code.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Tool-calling integration: the LLM emits code which is executed externally; execution results are fed back to the LLM to update or verify outputs. Not end-to-end differentiable; orchestration occurs at runtime.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Marked improvement in multi-step mathematical problem solving due to exact procedural verification; reduces arithmetic and procedural errors that pure LLM generation makes, enabling higher reliability on math benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Challenging math word problems (MATH dataset and other benchmark sets); reported boosts (paper cites improvements from ~54% to ~84% on MATH when code-based verification used, as discussed in text referencing [ZWL+23]).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td>Reported large improvement on MATH-style problems when using code-based self-verification (qualitative/narrative numbers cited in the reviewed literature: e.g., improvement from ~54% to ~84% on MATH in referenced work),</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Improves systematic execution/generalization on problems requiring exact computation and verification; still limited when conceptual planning or theorem-level proof planning is required beyond execution of generated code.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Good: generated code and execution logs offer concrete, inspectable evidence for the answer; program traces serve as verifiable artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Relies on the LLM to write correct verification code; code generation errors or logic errors can still occur; does not fully address high-level proof planning or hallucinated premises.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Tool-augmented verification: marrying generative language models with external procedural execution to combine flexible language-based reasoning with exact procedural verification.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e466.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e466.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Memory-augmented LLMs (Schuurmans)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Memory-augmented large language models enabling computational universality</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The idea and demonstration that transformer LMs, when given an external read/write memory/storage they can use at runtime, can emulate arbitrary computations and in practice simulate a universal Turing machine without changing model weights.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Memory augmented large language models are computationally universal</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Memory-augmented LLM (Stored Instruction Computer construction)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A transformer LLM is combined with an externally managed memory tape or structured memory store that the model can read/write during inference; by designing the memory access protocol and prompts appropriately, the combined system can implement arbitrary algorithms and extend the LLM's computational capacity far beyond its sequence-length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>External memory / stored instruction computer acting as an explicit workspace (scratchpad) that can hold symbolic/stateful information and instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Transformer LLM (e.g., Flan-U-PaLM) used as the controller that issues reads/writes and acts as a program interpreter over the external memory.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Prompt/programming-based integration: memory operations are orchestrated via designed prompts and stored instruction sequences; no weight changes to the LLM are required—system-level programming realizes the computational extension.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Achieves computational universality and enables execution of algorithmic procedures that pure sequence-only LLMs cannot; enables complex multi-step algorithms, long-running computation, and use of arbitrarily large working memory at inference-time.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Constructive demonstration: emulation of universal Turing machine and implementation of arbitrary algorithms; theoretical and empirical demonstrations reported in referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Expands the functional/generalization scope of an LLM to algorithmic tasks and long-context computations; allows in-principle exact computation and algorithmic generalization beyond fixed context windows.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Moderate: memory contents and stored instructions are explicit and inspectable; however mapping from LLM internal decisions to memory operations may remain opaque.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Practical engineering complexity in designing memory protocols and ensuring reliable read/write behavior; performance and latency depend on memory access mechanisms; does not by itself guarantee higher-level conceptual reasoning or reduced hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Computational-augmentation principle: combining a neural sequence model with an external memory substrate yields Turing-complete behavior and extends algorithmic capabilities without retraining.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chameleon: Plug- and -play compositional reasoning with large language models <em>(Rating: 2)</em></li>
                <li>A real-world webagent with planning, long context understanding, and program synthesis <em>(Rating: 2)</em></li>
                <li>Memory augmented large language models are computationally universal <em>(Rating: 2)</em></li>
                <li>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification <em>(Rating: 2)</em></li>
                <li>Scaling in the service of reasoning and model-based ml <em>(Rating: 1)</em></li>
                <li>Ernie: Enhanced language representation with informative entities <em>(Rating: 1)</em></li>
                <li>Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph <em>(Rating: 1)</em></li>
                <li>Opencog hyperon <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-466",
    "paper_id": "paper-262054850",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "OpenCog Hyperon (Neural Space)",
            "name_full": "OpenCog Hyperon with Neural Space integration",
            "brief_description": "A multi-paradigm AGI framework (OpenCog Hyperon) extended with a Neural Space that wraps LLMs, enabling tight interaction between a declarative Atomspace (metagraph) and neural models via automated translation between MeTTa (the Hyperon language) and natural language and by linking LLM internal representations to Atom nodes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "OpenCog Hyperon + Neural Space",
            "system_description": "OpenCog Hyperon is a distributed metagraph-based cognitive substrate (the Atomspace) hosting multiple cognitive algorithms (probabilistic logical reasoning, evolutionary program learning, attractor nets, etc.). The Neural Space is an auxiliary space that encapsulates external neural networks (e.g., LLMs). Integration is achieved by automated conversion between natural language and MeTTa (pattern-matching queries) and by growing explicit links between Atoms and neural activation patterns so that neural outputs and metagraph reasoning can interoperate within the same architecture.",
            "declarative_component": "Typed, weighted metagraph 'Atomspace' representing symbolic knowledge, rules, and probabilistic logic (OpenCog-style probabilistic logical representations and MeTTa code).",
            "imperative_component": "Large neural networks (e.g., transformer LLMs) wrapped as 'Neural Space' services; neural pattern recognizers and generative models used for surface pattern recognition and synthesis.",
            "integration_method": "Modular hybrid integration: automated translation layers convert between MeTTa/metagraph queries and natural language; bidirectional interfaces allow LLM outputs to populate Atomspace and Atomspace queries to be sent to LLMs; optional direct links between Atom nodes and neural activation subspaces (mapping functions between neurons/activations and symbolic Atoms). Integration is iterative and mediated by the Atomspace logic and MeTTa runtime rather than end-to-end differentiable training.",
            "emergent_properties": "Enables symbolic reasoning (deduction/induction/abduction) to supervise and refine neural outputs, and lets neural models provide flexible pattern recognition and generative proposals that the symbolic subsystem can verify, combine, or abstract; supports iterative retraining/data curation loops where symbolic inference produces training material for the LLM; expected emergent capabilities include improved factual grounding, richer concept formation via concept-blending in the symbolic substrate, and better control over hallucination by symbolic verification.",
            "task_or_benchmark": "Not evaluated on a single benchmark in this paper; described as an architectural approach for general AGI tasks (symbolic reasoning, planning, concept formation, tool use).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Architecturally intended to improve generalization by letting the Atomspace produce concise symbolic abstractions and constraints that guide neural synthesis; expected to improve out-of-distribution robustness relative to pure LLMs by anchoring propositions to symbolic knowledge and inference metadata (but no quantitative results reported).",
            "interpretability_properties": "High: symbolic Atomspace supports inspectable chains of probabilistic logical inference and MeTTa-executable proofs/explanations; mappings from Atoms to neural activations can be examined to produce human-readable explanations of why an LLM produced a particular output (subject to the transparency of the mapping).",
            "limitations_or_failures": "Challenges include (1) discovering tractable, reliable mappings between distributed neural activations and symbolic Atoms; (2) engineering the translation layer (MeTTa↔natural language) robustly; (3) scaling coordination between many heterogeneous algorithms; (4) no reported quantitative benchmarks here to demonstrate efficacy.",
            "theoretical_framework": "Division-of-labor/complementarity: symbolic metagraphs supply explicit abstractions, verification and meta-reasoning while neural spaces supply pattern recognition and generative capacities; iterative symbolic→neural→symbolic co-training loop is advocated as the mechanism producing synergy.",
            "uuid": "e466.0"
        },
        {
            "name_short": "Bengio-Hu RL/MDL",
            "name_full": "Bengio and Hu's RL/MDL proposal (LLM + MDL abstraction learner + RL)",
            "brief_description": "A conceptual AGI architecture that couples a large generative model (LLM) with an Occam-style minimum-description-length (MDL) learner that extracts concise abstractions, and a reinforcement-learning (RL) controller that guides interaction with the environment and the two learners.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM + MDL abstraction learner + RL controller",
            "system_description": "The LLM serves as a general pattern recognition and proposal generator; an MDL-driven learner attempts to infer concise symbolic/structured abstractions from the diffuse pattern manifold of the LLM outputs; an RL module governs how this LLM+MDL ensemble interacts with the environment (or data sources) to collect experience and refine abstractions. The MDL learner effectively functions as a declarative abstraction layer constructed from LLM-generated candidate patterns.",
            "declarative_component": "MDL-inferred concise abstractions (informal symbolic/structured models) acting as compact, declarative representations—conceptual summaries derived from LLM patterns (not a specific logic formalism specified in the paper).",
            "imperative_component": "Transformer LLM (generative neural model) and a reinforcement learning module to govern interactions/experience collection.",
            "integration_method": "Two-stage iterative integration: LLM provides rich pattern outputs; MDL learner compresses these into compact abstractions; RL directs data collection and uses MDL abstractions to plan and evaluate actions. Integration is not described as end-to-end differentiable; it is an orchestrated modular loop where each module informs and trains the others.",
            "emergent_properties": "Potentially reduces hallucination and spurious generalization by favoring compact models (MDL), enabling more robust conceptual blending and higher-level creativity through explicit abstractions; expected to produce better structured generalization and scientific-style concept formation compared to LLM alone.",
            "task_or_benchmark": "Conceptual/architectural proposal rather than an implemented system in this paper; no benchmark evaluation presented here.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Argued qualitatively to improve generalization through MDL-driven abstraction (compact models generalize better), with the RL controller guiding environment-specific refinement; no quantitative OOD/compositionality results provided.",
            "interpretability_properties": "Improved interpretability is anticipated because MDL-produced abstractions are concise symbolic structures that can be inspected; these abstractions can serve as human-readable explanations for LLM behavior.",
            "limitations_or_failures": "MDL learning at scale is computationally challenging/intractable in general; unclear how to reliably extract useful abstractions from a huge, overparameterized LLM; integration and scalability questions remain open.",
            "theoretical_framework": "Learning-theory/Occam principle: compactness correlates with better generalization; complementary strengths: LLM supplies breadth, MDL supplies concise abstractions, RL provides situated interaction and optimization.",
            "uuid": "e466.1"
        },
        {
            "name_short": "Chameleon",
            "name_full": "Chameleon: Plug-and-play compositional reasoning with large language models",
            "brief_description": "A system that uses GPT-4 to automatically assemble pipelines of specialized cognitive tools (search, knowledge retrieval, vision, calculators, etc.) tailored to a given task, improving multi-modal and multi-tool reasoning (reported to boost performance on tasks like ScienceQA).",
            "citation_title": "Chameleon: Plug- and -play compositional reasoning with large language models",
            "mention_or_use": "mention",
            "system_name": "Chameleon (GPT-4 orchestrator)",
            "system_description": "Chameleon leverages a central LLM (GPT-4) as an orchestrator that inspects a task, selects appropriate specialized external tools (search engines, knowledge retrievers, vision modules, calculators), composes them into a pipeline, and executes or delegates sub-tasks to those tools to achieve end-to-end solutions. The LLM acts as planner and integrator while tools provide deterministic or specialized capabilities.",
            "declarative_component": "External symbolic knowledge/retrieval modules (search engines, knowledge bases) and structured tool outputs (e.g., retrieved facts or structured knowledge representations) used as declarative grounding.",
            "imperative_component": "Central LLM (GPT-4) for planning/orchestration and various procedural tool modules (search, vision models, calculators, specialized APIs) that execute procedural computations.",
            "integration_method": "Modular orchestration: the LLM composes and sequences calls to external tools; tool outputs are fed back into the LLM for further planning and response generation. Integration is runtime orchestration rather than end-to-end differentiable training.",
            "emergent_properties": "Compositional problem solving beyond pure LLMs: can chain specialized procedural computations with retrieved declarative facts, yielding stronger performance on grounded multi-step tasks; reduces hallucination by delegating factual checks to reliable tools.",
            "task_or_benchmark": "Multi-modal reasoning and science QA tasks (e.g., ScienceQA) and other composed reasoning tasks requiring external knowledge and procedural computation.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Improved practical generalization on domain-compositional tasks by leveraging specialized modules; performance is task-dependent and improved relative to GPT-4 alone in certain benchmarks (qualitative statement in paper), but no consistent universal generalization guarantee.",
            "interpretability_properties": "Moderate: orchestration logs and explicit tool call traces provide inspectable steps showing which modules were used and what inputs/outputs occurred, aiding explainability relative to pure LLM blackbox outputs.",
            "limitations_or_failures": "Quality depends on available tools and on the orchestrator's ability to select and sequence them properly; brittle when tools are mis-selected or when tool outputs contain errors; latency and engineering complexity increase with more tools.",
            "theoretical_framework": "Practical modular synergy: LLM as flexible planner + specialized deterministic/procedural tools for grounding and precise computation; emphasizes tool-mediated hybridization rather than deep symbolic integration.",
            "uuid": "e466.2"
        },
        {
            "name_short": "DeepMind WebAgent",
            "name_full": "A real-world webagent with planning, long context understanding, and program synthesis (WebAgent)",
            "brief_description": "A hybrid architecture combining two language models (HTML-T5 for HTML understanding and Flan-U-PaLM for higher-level planning) to automate web tasks (navigating pages, filling forms, extracting and synthesizing information).",
            "citation_title": "A real-world webagent with planning, long context understanding, and program synthesis",
            "mention_or_use": "mention",
            "system_name": "WebAgent (HTML-T5 + Flan-U-PaLM composition)",
            "system_description": "WebAgent composes specialized LMs: a model trained to parse and represent HTML (HTML-T5) provides structured representations of web pages (declarative-like structured data), while a planner model (Flan-U-PaLM) reasons about goals and synthesizes programs or action sequences to execute tasks on the web. The architecture passes structured web-representations to the planner and uses program synthesis to carry out actions.",
            "declarative_component": "Structured HTML-derived representations and retrieved web state (semi-structured symbolic data describing DOM elements and page state).",
            "imperative_component": "Transformer LMs performing planning and program synthesis (Flan-U-PaLM), and procedural executors that actuate actions (clicks, fills) on the web.",
            "integration_method": "Pipeline composition: an HTML understanding model outputs structured state; a higher-level LLM consumes that structured state to plan and produce executable programs/scripts; a runtime executes the scripts and returns results to the planner for iterative refinement.",
            "emergent_properties": "Robust web automation by combining symbolic/structured page representations with LLM-based planning and program synthesis; improved reliability in tasks requiring structured perception plus planning compared to single-model approaches.",
            "task_or_benchmark": "Web automation tasks (e.g., form filling, multi-step web workflows); reported to achieve efficient web automation in practical settings (qualitative description in paper).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Better task generalization for diverse web pages due to explicit HTML-structure grounding, but depends on the quality of HTML parsing and planner generality.",
            "interpretability_properties": "Improved explainability via (1) structured HTML representations that can be inspected and (2) explicit synthesized programs that show intended action sequences.",
            "limitations_or_failures": "Relies on accuracy of HTML-to-structure model and on correct program synthesis; brittle to unexpected dynamic page behavior or sites with nonstandard DOMs; no universal performance metrics reported in this paper.",
            "theoretical_framework": "Modular pipeline synergy: structure extraction (declarative) feeding into planner/program synthesis (imperative) to achieve robust automation.",
            "uuid": "e466.3"
        },
        {
            "name_short": "LLM + Knowledge Graph / Wolfram Alpha integrations",
            "name_full": "Integration of LLMs with declarative knowledge sources (knowledge graphs, WolframAlpha, ERNIE-style entity-aware transformers)",
            "brief_description": "Approaches that ground LLM outputs by connecting them to symbolic knowledge stores or computation engines (knowledge graphs, Wolfram Alpha) to reduce hallucination and improve factuality and verifiability.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM + Knowledge Graph / Computational Engine",
            "system_description": "Typical systems keep an LLM as the generative/planning module and connect it at runtime to a symbolic knowledge graph (KG) or computational engine (e.g., Wolfram Alpha). The KG serves as a declarative shared state and source of factual constraints; the LLM queries, injects or aligns its outputs with KG facts and/or delegates symbolic computation to engines.",
            "declarative_component": "Knowledge graphs (structured triples, ontologies) or computational engines (WolframAlpha) providing symbolic facts and exact computations.",
            "imperative_component": "Transformer LLMs for generation and orchestration; optionally learned adapters to convert between KG queries and natural language.",
            "integration_method": "Hybrid modular coupling: LLMs generate queries or candidate facts which are checked or completed by KG/engine; KG outputs are fed back to LLM for editing/verifying responses. Integration may include co-training (e.g., ERNIE-style entity-aware pretraining) or runtime tool-calling.",
            "emergent_properties": "Reduced factual hallucination, improved verifiability, and ability to perform exact computations or deterministic lookups unavailable to LLMs alone; some improvements in grounded reasoning and accuracy for knowledge-intensive tasks.",
            "task_or_benchmark": "Factually-grounded QA, question answering requiring exact computation, tasks sensitive to factual correctness.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Grounding in KG improves factual generalization on knowledge-rich queries but may not improve creative or procedural reasoning; effectiveness depends on coverage and alignment between KG and LLM representations.",
            "interpretability_properties": "High: KG queries and returned facts offer inspectable evidence trails; computational engine outputs are deterministic and explainable.",
            "limitations_or_failures": "KG coverage limits; mismatch between LLM representations and KG schema; latency and engineering complexity; symbolic sources may be outdated or inconsistent.",
            "theoretical_framework": "Cognitive synergy: declarative symbolic stores provide reliable facts and constraints while LLMs provide flexible language and pattern-completion abilities.",
            "uuid": "e466.4"
        },
        {
            "name_short": "Auto-GPT (agent wrappers)",
            "name_full": "Auto-GPT and similar agentic wrappers around LLMs",
            "brief_description": "Simple agent architectures that wrap LLMs with added components (planning, simple long-term memory, iterative execution) to pursue user-specified goals by decomposing them into steps and executing them iteratively.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Auto-GPT (LLM-agent wrapper)",
            "system_description": "Auto-GPT-style systems accept a high-level goal, use an LLM to produce a plan, then iteratively prompt the LLM to carry out plan steps, optionally storing intermediate results in a memory store and invoking external tools. They add basic goal management, memory read/write, and action-execution loops around the core LLM.",
            "declarative_component": "Simple long-term and episodic memory stores (often implemented as text or vector stores) and externally supplied tool outputs (structured results) that act as declarative state.",
            "imperative_component": "LLM for planning and step-level generation; procedural loop control code that issues prompts, calls tools, and updates memory.",
            "integration_method": "Sequential orchestration: planner LLM generates steps; procedural controller executes steps and updates memory; memory contents are passed back into the LLM when producing subsequent steps.",
            "emergent_properties": "Gives LLMs rudimentary agentic behavior (pursuit of goals over multiple steps), limited persistence and simple multi-step execution capability that a plain LLM chat session lacks.",
            "task_or_benchmark": "General goal-directed automation tasks (ad-hoc); no standardized benchmark reported here.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Limited: can handle stereotyped multi-step goals but fails on complex real-world goal management due to lack of robust episodic indexing, true long-term memory integration, and failure accumulation over long plans.",
            "interpretability_properties": "Moderate: explicit plan steps and memory updates can be inspected but internal LLM rationales remain opaque.",
            "limitations_or_failures": "Described as simplistic; brittle planning, hallucination compounded across steps, lack of genuine reflective sensing, poor long-term goal management; not sufficient for HCAGI without deeper architectural changes.",
            "theoretical_framework": "Practical agent-enablement via orchestration: demonstrates how adding lightweight memory/planning components yields agentic behavior but highlights need for deeper integration for autonomy.",
            "uuid": "e466.5"
        },
        {
            "name_short": "Code-based self-verification (LLM + Code Interpreter)",
            "name_full": "Code-based self-verification using LLM plus code-execution plugins (e.g., GPT-4 Code Interpreter)",
            "brief_description": "A hybrid technique in which an LLM generates procedural code to verify its candidate solutions (e.g., for math problems), then executes that code via an interpreter/plugin to self-check and refine answers, substantially improving accuracy on difficult math benchmarks.",
            "citation_title": "Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification",
            "mention_or_use": "mention",
            "system_name": "LLM + Code Interpreter (self-verification)",
            "system_description": "The LLM produces both a symbolic derivation/answer and a program that computes or verifies the result; the program is executed in a sandboxed environment (Code Interpreter plugin) to confirm or correct the LLM's answer, enabling iterative refinement and higher-precision computation.",
            "declarative_component": "Symbolic problem statements and structured program representations (code) that express precise computations / verifications.",
            "imperative_component": "LLM (GPT-4) for program generation and rationale; a code execution environment (Python interpreter or code-exec plugin) to run verification code.",
            "integration_method": "Tool-calling integration: the LLM emits code which is executed externally; execution results are fed back to the LLM to update or verify outputs. Not end-to-end differentiable; orchestration occurs at runtime.",
            "emergent_properties": "Marked improvement in multi-step mathematical problem solving due to exact procedural verification; reduces arithmetic and procedural errors that pure LLM generation makes, enabling higher reliability on math benchmarks.",
            "task_or_benchmark": "Challenging math word problems (MATH dataset and other benchmark sets); reported boosts (paper cites improvements from ~54% to ~84% on MATH when code-based verification used, as discussed in text referencing [ZWL+23]).",
            "hybrid_performance": "Reported large improvement on MATH-style problems when using code-based self-verification (qualitative/narrative numbers cited in the reviewed literature: e.g., improvement from ~54% to ~84% on MATH in referenced work),",
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Improves systematic execution/generalization on problems requiring exact computation and verification; still limited when conceptual planning or theorem-level proof planning is required beyond execution of generated code.",
            "interpretability_properties": "Good: generated code and execution logs offer concrete, inspectable evidence for the answer; program traces serve as verifiable artifacts.",
            "limitations_or_failures": "Relies on the LLM to write correct verification code; code generation errors or logic errors can still occur; does not fully address high-level proof planning or hallucinated premises.",
            "theoretical_framework": "Tool-augmented verification: marrying generative language models with external procedural execution to combine flexible language-based reasoning with exact procedural verification.",
            "uuid": "e466.6"
        },
        {
            "name_short": "Memory-augmented LLMs (Schuurmans)",
            "name_full": "Memory-augmented large language models enabling computational universality",
            "brief_description": "The idea and demonstration that transformer LMs, when given an external read/write memory/storage they can use at runtime, can emulate arbitrary computations and in practice simulate a universal Turing machine without changing model weights.",
            "citation_title": "Memory augmented large language models are computationally universal",
            "mention_or_use": "mention",
            "system_name": "Memory-augmented LLM (Stored Instruction Computer construction)",
            "system_description": "A transformer LLM is combined with an externally managed memory tape or structured memory store that the model can read/write during inference; by designing the memory access protocol and prompts appropriately, the combined system can implement arbitrary algorithms and extend the LLM's computational capacity far beyond its sequence-length constraints.",
            "declarative_component": "External memory / stored instruction computer acting as an explicit workspace (scratchpad) that can hold symbolic/stateful information and instructions.",
            "imperative_component": "Transformer LLM (e.g., Flan-U-PaLM) used as the controller that issues reads/writes and acts as a program interpreter over the external memory.",
            "integration_method": "Prompt/programming-based integration: memory operations are orchestrated via designed prompts and stored instruction sequences; no weight changes to the LLM are required—system-level programming realizes the computational extension.",
            "emergent_properties": "Achieves computational universality and enables execution of algorithmic procedures that pure sequence-only LLMs cannot; enables complex multi-step algorithms, long-running computation, and use of arbitrarily large working memory at inference-time.",
            "task_or_benchmark": "Constructive demonstration: emulation of universal Turing machine and implementation of arbitrary algorithms; theoretical and empirical demonstrations reported in referenced work.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": true,
            "generalization_properties": "Expands the functional/generalization scope of an LLM to algorithmic tasks and long-context computations; allows in-principle exact computation and algorithmic generalization beyond fixed context windows.",
            "interpretability_properties": "Moderate: memory contents and stored instructions are explicit and inspectable; however mapping from LLM internal decisions to memory operations may remain opaque.",
            "limitations_or_failures": "Practical engineering complexity in designing memory protocols and ensuring reliable read/write behavior; performance and latency depend on memory access mechanisms; does not by itself guarantee higher-level conceptual reasoning or reduced hallucination.",
            "theoretical_framework": "Computational-augmentation principle: combining a neural sequence model with an external memory substrate yields Turing-complete behavior and extends algorithmic capabilities without retraining.",
            "uuid": "e466.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chameleon: Plug- and -play compositional reasoning with large language models",
            "rating": 2,
            "sanitized_title": "chameleon_plug_and_play_compositional_reasoning_with_large_language_models"
        },
        {
            "paper_title": "A real-world webagent with planning, long context understanding, and program synthesis",
            "rating": 2,
            "sanitized_title": "a_realworld_webagent_with_planning_long_context_understanding_and_program_synthesis"
        },
        {
            "paper_title": "Memory augmented large language models are computationally universal",
            "rating": 2,
            "sanitized_title": "memory_augmented_large_language_models_are_computationally_universal"
        },
        {
            "paper_title": "Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification",
            "rating": 2,
            "sanitized_title": "solving_challenging_math_word_problems_using_gpt4_code_interpreter_with_codebased_selfverification"
        },
        {
            "paper_title": "Scaling in the service of reasoning and model-based ml",
            "rating": 1,
            "sanitized_title": "scaling_in_the_service_of_reasoning_and_modelbased_ml"
        },
        {
            "paper_title": "Ernie: Enhanced language representation with informative entities",
            "rating": 1,
            "sanitized_title": "ernie_enhanced_language_representation_with_informative_entities"
        },
        {
            "paper_title": "Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph",
            "rating": 1,
            "sanitized_title": "thinkongraph_deep_and_responsible_reasoning_of_large_language_model_with_knowledge_graph"
        },
        {
            "paper_title": "Opencog hyperon",
            "rating": 2,
            "sanitized_title": "opencog_hyperon"
        }
    ],
    "cost": 0.02682325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs
September 20, 2023 19 Sep 2023</p>
<p>Ben Goertzel 
TrueAGI, OpenCog* Singularitynet 
Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs
September 20, 2023 19 Sep 2023
A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are. * SingularityNET, TrueAGI, OpenCog</p>
<p>It is found that many of the practical weaknesses of these AI systems can be tied specifically to lacks in the basic cognitive architectures according to which these systems are built. It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources. This does not imply there is nothing to learn about human-level AGI from studying and experimenting with LLMs, nor that LLMs cannot form significant parts of human-level AGI architectures that also incorporate other ideas.</p>
<p>Social and ethical matters regarding LLMs are very briefly touched from this perspective, which implies that while care should be taken regarding misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course as with any powerfully impactful technology, overall the sort of policy needed as regards modern LLMs is quite different than would be the case if a more credible approximation to human-level AGI were at hand.</p>
<p>Introduction</p>
<p>Large Language Models, including OpenAI's GPT-4 system [BCE + 23] and others based on the underlying transformer neural net model pioneered at Google Brain [VSP + 17], have come to dominate the AI scene in a dramatic way over the last year, comprising a genuine black swan event both in the AI world and in the overall economy and culture.</p>
<p>They have demonstrated a wide range of capabilities significantly beyond any previous AI systems, including many with clear economic and human value, and including quite a few that many experts thought would not be achievable except in the context of AI systems with overall human-level competence.</p>
<p>This success has been such that some researchers have declared them a dramatic milestone on the path to human-level Artificial General Intelligence, e.g. in the pa-per controversially titled "Sparks of AGI" [BCE + 23] by a collection of Microsoft researchers (who had early access to test GPT-4 due to Microsoft's substantial investment in and partnership with OpenAI, the company that created GPT-4).</p>
<p>Other researchers with highly relevant expertise have loudly disagreed with these optimistic assessments regarding the relationship between LLMs and AGI; for instance Yann LeCun, one of the pioneers of deep neural networks (the broad category of AI system of which transformer neural nets and GPT are examples) pithily declared that "On the road to AGI, Large Language Models are an off-ramp." [LeC23] There are of course many educated opinions inbetween, holding that LLMs comprise meaningful progress toward AGI in some respects but not others, or that LLMs may be useful as one component of multi-component AGI systems, etc.</p>
<p>Our goal in this paper is to give fairly detailed consideration to LLMs as cognitive systems, and in particular to highlight some of the substantial differences between the sort of cognitive system LLMs are, and the sort of cognitive systems human beings are. While the strengths of LLMs are tremendous in some ways compared to previous systems, their weaknesses are also numerous and many of these can be tied specifically to lacks in the basic cognitive architectures according to which these systems are built.</p>
<p>A couple caveats to keep in mind as you proceed through these pages:</p>
<p>• This is not intended as any sort of comprehensive review paper! It does review a lot of tremendous work by others, but the methodology is to review diverse relevant works in a judiciously selected way so as to build toward some general conceptual points, not at all to review everything relevant that's out there. The fact that one certain paper or system is mentioned here and another is omitted, should not be taken as an assessment that the former is better than the latter according to any generally meaningful metric! • The AI field is advancing extremely rapidly these days, and LLMs in particular are a moving target due to the amount of resources currently being put into them.</p>
<p>Any particular capability of LLMs, as assessed at the time this paper was written, is fairly likely to have advanced at least a little (and sometimes a lot) by the time you are reading the paper. This doesn't really matter for the points being made in this paper, because in our empirical evaluation of LLMs the main thing we are interested in is the pattern of achievements vs. confusions observed on the part of the whole category of systems, rather than the specific success or failures of specific system versions.</p>
<p>Varieties of Intelligence: A Few Useful Distinctions</p>
<p>Before proceeding with substantive matters we will make some fairly pedantic but necessary notes on terminology, regarding the sorts of software systems and types of intelligence we are going to be talking about here.</p>
<p>CILLMs and HCAGI</p>
<p>What has brought LLMs like GPT-4 to the fore in the research world and general culture has partly been their embedding in interactive interfaces such as ChatGPT. This interactive mode has made it easier to leverage their strengths and work around their weaknesses. Here we will use the somewhat ugly acronym CILLM -Contemporary Interactive Large Language Models -to refer to systems of this nature, i.e. systems in the vein of ChatGPT, Bard, Llama and so forth. We mean CILLM to include chatbots and other practical systems created by building front-ends to these systems, or fine-tuning these systems for particular sorts of behaviors. We also intend CILLM to include hybrid systems that center on LLMs similar in nature to GPT3/4, Llama and so forth, even if these also include subsystems from other paradigms such as e.g. Wolfram Alpha (so long as these other subsystems are orchestrated principally, and interacting with the user principally, via the LLM). We do not mean CILLM to include multi-component software systems of which LLMs comprise one among diverse components, where the LLM itself is not making the majority of contribution to the system's intelligence and is not the principal entity coordinating the various subsystems. We also do not mean CILLM to include future derivates of today's LLMs that incorporate radically novel architectural or dynamical elements (e.g. co-training across the transformer neural net and a large knowledge graph, or replacement of backpropagation with a significantly different learning algorithm). We recognize that CILLM is not a precisely defined category and there could be many borderline cases that stretch the concept to the point of rendering it meaningless; however, these borderline cases don't seem to be on the scene right now, and we are only using CILLM as a provisional term to avoid repeatedly using awkward phrases like "CILLM systems."</p>
<p>We will also introduce the acronym HCAGI (Human-Capable AGI) to refer to AGIs that combine general intelligence with the trait of being able to carry out all the intelligencebased capabilities that people can. This is slightly different from the more common notion of HLAGI or Human-Level AI; as we will elaborate below, the basic difference is that an HCAGI might be far beyond human level in some respects, it just has to be at least human level in terms of each major aspect of human intelligence.</p>
<p>In this slightly awkward but reasonably careful vernacular, one of the propositions we will defend here is that CILLM is not in itself an HCAGI-capable AI approach, in practical terms given realizable amounts of compute resources. However, we do consider it plausible that CILLMs could serve as a significant component of HCAGI systems.</p>
<p>AGI versus Narrow / Closed-Scope AI</p>
<p>1 I launched the term "AGI" into the research world (and by consequence the media and popular discourse, after passage of some time) via using it as the title for an edited volume of research papers on (what is now called) AGI, which was published by Springer in 2005 [GP05]. The discussion leading up to this choice of title for the book involved a number of the book's chapter authors, including Pei Wang (of the NARS AGI system), Peter Voss (of AGI Inc.) and Shane Legg (who went on to co-found DeepMind). Using "Artificial General Intelligence" as the title of the book, and then of a conference series operative since 2006, gradually popularized the term in the AI research community. It was later realized that the term AGI had been used previously in an article on the future of AI and nanotechnology by physicist Mark Gubrud [Gub97].</p>
<p>One of the original draws to use the term AGI instead of alternatives was its resonance with the notion of General Intelligence used in psychology (the "g factor" underlying IQ tests, for example). However, the notion of human GI in psychology is not extremely well-defined and there remain various disputes about e.g. what IQ tests actually measure, and their validity across cultures. Animal psychology gives us no clear way to compare GI across species, e.g. to resolve the age-old dispute of which are smarter, cats or dogs. Similarly and relatedly, in spite of its history in the research world, the term "Artificial General Intelligence" has no broadly accepted precise definition, but has multiple closely related meanings, e.g.the capacity of an engineered system to • display the same rough sort of general intelligence as humans • display intelligence that is not tied to a highly specific set of tasks • generalize what it has learned, including generalization to contexts qualitatively very different than those it has seen before • take a broad view, and flexibly interpret its tasks at hand in the context of the world at large and its relation thereto</p>
<p>The lack of a crisp and clearly agreed definition of AGI is not necessarily a practical problem; as an analogy, biologists don't have a crisp and clear definition of "life", but this is no obstacle to either studying viruses and other organisms at the fuzzy boundary of life and non-life, nor to creating new synthetic-biology organisms combining lifelike and non-lifelike aspects in novel ways.</p>
<p>It's worth noting that "AI" also has many different meanings within the AI research community, with no clear consensus n the definition. The boundary between advanced statistics and AI is fuzzy -linear regression is mostly not considered AI, nonlinear regression is borderline, and whether feedforward neural nets should be considered AIs or fancy nonlinear regressors or whether there's a difference becomes a question of "what's your AI research paradigm?" The boundaries between AI, computational neuroscience and cognitive modeling are also in some cases nuanced. Legg and Hutter wrote a paper summarizing and organizing over 70 different published definitions of ?intelligence?, most oriented toward general intelligence, emanating from researchers in a variety of disciplines [LH07a].</p>
<p>While the precise definition or characterization of AGI is not broadly agreed and is one of the subjects of study of the AGI research field, it is broadly accepted that, given realistic space and time resource constraints, human beings do not have indefinite generality of intelligence; and for similar reasons, no real-world system is going to have indefinite generality. Human intelligence combines a certain generality of scope, with various highly specialized aspects aimed at providing efficient processing of pragmatically important problem types; and real-world AGI systems are going to mix generality and specificity in their own ways.</p>
<p>Here we will consider AGI to mean something in the vicinity of the three final items on the bullet list above, and will use the term HCAGI (Human-Capable AGI) to refer to AGIs that combine general intelligence with the trait of being able to carry out all the intelligence-based capabilities that people can. This somewhat ugly term is new, but we introduce it with some intention because the more commonly seen term HLAGI (Human-Level AGI) appears to us confusing given the reality of modern AI systems.</p>
<p>We already have computer systems, both AI systems and traditional software, that vastly exceed the human level at various tasks typically considered "intelligent" when people do them, such as multiplying and dividing large numbers, solving algebra equations or writing mediocre poetry. It seems very likely that by the time we have an AGI that can match human capability across all domains, this AGI will be vastly superhuman in a variety of regards, including but going well beyond the things that various AIs and other computer systems can already do better than people. A system that's human-level in some respects and vastly superhuman in other respects seems like it's not naturally thought of as a Human-Level AGI, so we are choosing to distinguish the category of Human-Capable AIs, meaning systems that are at least as intelligent as people in each significant dimension on which humans are intelligent.</p>
<p>Beyond the very broad bullet points given above, there is reasonably broad agreement in the AI community on some key likely aspects of general intelligence as observed in the biological and technological domains, e.g.:</p>
<p>• General intelligence involves the ability to achieve a variety of goals, and carry out a variety of tasks, in a variety of different contexts and environments</p>
<p>• A generally intelligent system should be able to handle problems and situations quite different from those anticipated by its creators</p>
<p>• A generally intelligent system should be good at generalizing the knowledge it has gained, so as to transfer this knowledge from one problem or context to others • Arbitrarily general intelligence is likely not possible given realistic resource constraints • Real-world systems may display varying degrees of limited generality, but are inevitably going to be a lot more efficient at learning some sorts of things than others; and for any given real-world system, there will be some learning tasks on which it is unacceptably slow. So real-world general intelligences are inevitably somewhat biased toward certain sorts of goals and environments. • Humans display a higher level of general intelligence than existing AI programs do, and apparently also a higher level than other animals • According to our observations of humans and various theoretical perspectives, the following traits, among many others, are typically associated with generally intelligence: reasoning, creativity, association, generalization, pattern recognition, problem solving, memorization, planning, achieving goals, learning, optimization, self-preservation, sensory data processing, language processing, classification, induction, deduction and abduction • It seems quite unlikely that humans happen to manifest a maximal level of general intelligence, even relative to the goals and environment for which they have been evolutionarily adapted</p>
<p>The antonym of AGI has historically been phrased as "Narrow AI" [GP05], which by reference to the above bullet list we could think of roughly as AI that • display intelligence that is tied to a specific set of tasks • can not generalize what it has learned to contexts qualitatively very different than those it has seen before • can not flexibly interpret its tasks at hand in the context of the world at large and its relation thereto For a narrow AI system, typically, if one changes the context or the behavior specification a little bit, some level of human reprogramming or reconfiguration is generally necessary to enable the system to retain its level of intelligence. Qualitatively, this seems quite different from natural generally intelligent systems like humans, which have a broad capability to self-adapt to changes in their goals or circumstances, performing 'transfer learning? to generalize knowledge from one goal or context to others.</p>
<p>The original founders of the AI field, in the 1950s and 60s, were largely concerned with the creation of hardware or software emulating human-like general intelligence.</p>
<p>However, until very recently, all the AI systems deployed in practical use were very clearly Narrow AIs, to an extent that no serious researcher claimed otherwise (various researchers occasionally claimed that the transition to AGI was coming soon, but that's a different matter).</p>
<p>Narrowness and Generality in the LLM Context</p>
<p>LLMs require relatively subtle interpretation in the context of the narrow vs general dichotomy. For instance, claiming GPT-4 shows "sparks of AGI" is different than claiming it's an AGI system, but is still a stronger sort of statement in this direction than one usually sees in the research literature.</p>
<p>The way we would propose to phrase the reality is: current LLMs are narrow AIs, but they don't always appear this way to human users because the scope of their narrowness is actually broad compared to any individual human mind. This becomes clearer to talk about if one replaces the term "narrow" with "closed-scope," and rephrases the list of characteristics of Narrow AI accordingly as • display intelligence that is not tied to a particular fixed, closed scope of tasks • can not generalize what it has learned to contexts qualitatively very different than those within this closed scope • can not flexibly interpret its tasks at hand in the context of the world at large and its relation thereto</p>
<p>In this rephrasing, one of the points we aim to make in this paper is that current LLMs are closed-scope AIs, where the closed scope is a subset of the set of tasks that are exemplified in the materials on the Internet. The large size and breadth of this closed scope means that these systems can do quite a lot of useful and impressive things. However, it's still a closed scope, tied to the state of human knowledge and capability at the particular point in time that the Internet is used to train the LLM. Humans have a much more open-ended sort of intelligence, which is why so many of the particular tasks exemplified on the Internet are things nobody had any clue how to do 100 years ago, and why 50 years from now the future evolution of the Internet will contain so many amazing new things we cannot now conceive.</p>
<p>Conceptualizations of General Intelligence</p>
<p>The research literature contains a wide variety of mathematical, scientific and conceptual approaches to understanding what General Intelligence is. While the diversity of ideas in this literature does not give us anything like a litmus test for assessing the degree to which CILLMs possess general intelligence, or human-capable general intelligence, there are nonetheless many relevant insights to be gathered.</p>
<p>The Core AGI Hypothesis</p>
<p>A key issue on which there is far from anything resembling consensus in the AI research world is the continuity of the boundary between narrow / closed-scope AI and AGI.</p>
<p>One position along these lines is what I articulated in 2014 as the "core AGI hypothesis" [Goe15a] (though the concept was discussed long before by myself and many others, without a clear label), which posits that "the creation and study of synthetic intelligences with sufficiently broad (e.g. human-level or greater) scope and strong generalization capability, is at bottom qualitatively different from the creation and study of synthetic intelligences with significantly narrower scope and weaker generalization capability."</p>
<p>The bulk of researchers in the historical "AGI community" associated with the AGI conference series (from 2006 to present) have been operating under some variation of this hypothesis, and pursuing various more particular hypotheses regarding the precise qualitatively different aspects associated with general intelligence. However, the intu-itions of the community who's become more recently enthused about AGI following the last few years' successes with deep neural networks are often quite different, and the vibe among this community tends to be that incremental improvement of current DNN systems (as narrow or closed-scope as they might currently be) is likely to lead step by step to HCAGI.</p>
<p>Five Approaches to Conceptualizing General Intelligence</p>
<p>2</p>
<p>Five key approaches to conceptualizing the nature of GI and AGI are outlined below.</p>
<p>These are certainly not the only perspectives that have been taken by well informed and thoughtful scientists and engineers, but they are reasonably prominent ones and indicate some of the perspectival variety that exists in the research world.</p>
<p>The Pragmatic Approach to Characterizing General Intelligence</p>
<p>The pragmatic approach to conceptualizing general intelligence is typified by the AI Magazine article "Human Level Artificial Intelligence? Be Serious!", written by Nils Nilsson, one of the early leaders of the AI field [Nil09] . Nilsson's view is ... that achieving real Human Level artificial intelligence would necessarily imply that most of the tasks that humans perform for pay could be automated. Rather than work toward this goal of automation by building special-purpose systems, I argue for the development of general-purpose, educable systems that can learn and be taught to perform any of the thousands of jobs that humans can perform. Joining others who have made similar proposals, I advocate beginning with a system that has minimal, although extensive, built-in capabilities. These would have to include the ability to improve through learning along with many other abilities.</p>
<p>In this perspective, once an AI obsoletes humans in most of the practical things we do, it should be understood to possess general Human Level intelligence. The implicit assumption here is that humans are the generally intelligent system we care about, so that the best practical way to characterize general intelligence is via comparison with human capabilities.</p>
<p>The classic Turing Test for machine intelligence [Tur50] -simulating human conversation well enough to fool human judges -is pragmatic in a similar sense to Nilsson's perspective. But the Turing test has a different focus, on emulating humans. Nilsson isn't interested in whether an AI system can fool people into thinking it's a human, but rather in whether an AI system can do the useful and important practical things that people can do.</p>
<p>By this sort of criterion, a moderately more reality-grounded CILLM system, connected to an appropriate set of interfaces and actuators, could be assessed as coming quite close to HCAGI.</p>
<p>Psychological Characterizations of General Intelligence</p>
<p>The psychological approach to characterizing general intelligence also focuses on humanlike general intelligence; but rather than looking directly at practical capabilities, it tries to isolate deeper underlying capabilities that enable these practical capabilities. In practice it encompasses a broad variety of sub-approaches, rather than presenting a unified perspective.</p>
<p>Viewed historically, efforts to conceptualize, define, and measure intelligence in humans reflect a distinct trend from general to specific (it is interesting to note the similarity to historical trends in AI) . Thus, early work in defining and measuring intelligence was heavily influenced by Spearman, who in 1904 proposed the psychological factor g (the "g factor", for general intelligence [Spe61]. Spearman argued that g was biologically determined, and represented the overall intellectual skill level of an individual. In 1916, Terman introduced the notion of an intelligence quotient or IQ [Ter48].</p>
<p>In subsequent years, though, psychologists began to question the concept of intelligence as a single, undifferentiated capacity. There emerged a number of alternative theories, definitions, and measurement approaches, which share the idea that intelligence is multifaceted and variable both within and across individuals. Of these approaches, a particularly well-known example is Gardner's [Gar99] theory of multiple intelligences, which proposes nine distinct forms or types of intelligence: (1) linguistic,</p>
<p>(2) logical-mathematical, (3) musical, (4) bodily-kinesthetic, (5) spatial, (6) interpersonal, (7) intrapersonal, (8) naturalist and (9) existential.</p>
<p>In the Gardner multiple-intelligences perspective, current LLMs are strong in linguistic intelligence and vary from mediocre to hopeless in the other aspects.</p>
<p>A Mathematical Approach to Characterizing General Intelligence</p>
<p>In contrast to approaches focused on human-like general intelligence, some researchers have sought to understand intelligence in general. One underlying intuition here is that:</p>
<p>• Truly, absolutely general intelligence would only be achievable given infinite computational ability</p>
<p>• For any computable system, there will be some contexts and goals for which it's not very intelligent.</p>
<p>• However, some finite computational systems will be more generally intelligent than others, and it's possible to quantify this extent This approach is typified by the recent-ish work of Legg and Hutter [LH07b], who</p>
<p>give a formal definition of general intelligence based on the Solomonoff-Levin prior, building heavily on the foundational work of Hutter [Hut05]. Put very roughly, they define intelligence as the average reward-achieving capability of a system, calculated by averaging over all possible reward-summable environments, where each environment is weighted in such a way that more compactly describable programs have larger weights.</p>
<p>According to this sort of measure, humans are nowhere near the maximally gen- </p>
<p>The Adaptationist Approach to Characterizing General Intelligence</p>
<p>Another perspective views general intelligence as closely tied to the environment in which it exists. Pei Wang has argued carefully for a conception of general intelligence as "adaptation to the environment using limited resources" [Wan06]. A system may be said to have greater general intelligence, if it can adapt effectively to a more general class of environments, within realistic resource constraints.</p>
<p>Here as well, we may say that current LLMs have ability to adapt to environments that constitute relatively minor variations on their relatively massive training datasets.</p>
<p>Collective human intelligence and individual human intelligence both have dramatically greater adaptive ability.</p>
<p>System-theoretic Approach to Characterizing General Intelligence</p>
<p>Weaver's conception of open ended intelligence [WV17] presents a model that is well suited for thinking about AGIs potentially very different from human beings, such as "global brain" cognition emerging from the Internet or large subsets thereof [Goe01].</p>
<p>His notion of OEI might be paraphrased as " The ability to maintain the individuated existence of an system, and enable the transformation of this system into something transcending the system's current reality, in the context of unpredictable situations and limited resources "</p>
<p>Goertzel [Goe14] has written before that instead of AGI we might for many purposes do better to think in terms of SCADS -Self-Organizing Complex Adaptive Dynamical</p>
<p>Systems . Weaver connects SCADS with the thinking of Deleuze and other postmodernist philosophers, drawing on the aspect of postmodernism that covers the co-creation and co-adaptation of minds and realities via complex dynamical self-organizing processes.</p>
<p>Of the other definitions reviewed above, this is closest to Pei Wang's -adaptation seems to assume survival and ongoing individuation. However, Wang's version of intelligence does not include a notion of fundamental transformation and development, at the base level (but rather treats this as something that may emerge as a result of adaptation under appropriate circumstances).</p>
<p>Of course, systems with OEI of this nature may also be good at passing IQ tests, displaying multiple special forms of intelligence, and maximizing computable reward functions. But it's also important to understand that OEI is asking for something different. For instance Hutter's hypothetical system (a huge-resources finite approximation of his theoretical infinite AIXI system) , if it could be built, could be built in a way that made it lackluster regarding both individuation and self-transcendence.</p>
<p>Open-Ended AI vs. Contemporary Commercial AI</p>
<p>The notion of open-ended intelligence is useful for drawing a distinction between AGI as generally conceived and not only LLMs but the vast bulk of the work the contemporary AI field is pursuing.</p>
<p>The relatively small portion of the AI field that is concerned with making autonomous or semi-autonomous agents that learn from experience is mostly pursuing "expected reward maximization" or some closely related form of systematic pursuit of precisely defined metrics. Key aspects of this approach, generally taken for granted as obvious but worth highlighting in an OEI context, are:</p>
<p>• Goal and means are distinct. I.e. the "reward function" or similar that the system is optimizing, is something quite distinct from the system itself.</p>
<p>• The nature of the system and the world are assumed as something definite and given and unchangeable throughout history</p>
<p>Open-ended intelligence views intelligence quite differently, as a complex self-organizing aspect of system-environment interaction, and it views goals as self-organizing systems co-adapting and co-evolving with broader cognitive, systemic and environmental selforganization.</p>
<p>Current Scope of the AGI Field</p>
<p>Wlodek Duch, in his 2008 survey of the AI field [DOP08], divided existing approaches to AI into three paradigms -symbolic, emergentist and hybrid. To this trichotomy, following [Goe15a], we here add one additional category, "universal." Due to the diversity of AGI approaches, it is difficult to find truly comprehensive surveys; Samsonovich [Sam10] is highly dated by now but gives a reasonable overview of where things stood around 2010, which is a useful antidote to the increasingly popular confusion that the AI field started around the time Facebook rolled out face recognition on its website.</p>
<p>Universal AI</p>
<p>In the universal approach, one starts with AGI algorithms or agents that would yield incredibly powerful general intelligence if supplied with massively, unrealistically much computing power, and then views practically feasible AGI systems as specializations of these powerful theoretic systems.</p>
<p>The path toward universal AI began in earnest with Solomonoff's [Sol64] universal predictors, which provide a rigorous and elegant solution to the problem of sequence prediction, founded in the theory of algorithmic information (also known as Kolmogorov Complexity [LV + 08]. The core idea here (setting aside various important technicalities) is that the shortest program computing a sequence, provides the best predictor regarding the continuation of the sequence.</p>
<p>Hutter's work on AIXI [Hut05] extends this approach, applying the core idea of Solomonoff induction to the problem of controlling an agent carrying out actions in, and receiving reinforcement signals from, a computable environment. In an abstract sense, AIXI is the optimally intelligent agent in computable environments. Schmidhuber's Godel Machine [Sch06] proceeds in a similar spirit, involving a machine that (roughly speaking) uses a powerful logic system to prove theorems regarding which internal and external actions will best achieve reward at each point in time.</p>
<p>There is a literature on efforts to "scale down" these extremely powerful systems into something computationally feasible (e.g. [ [Fra21]. but this is still a long way from deriving a practical AGI approach in this way. As big as modern LLMs are, they are not remotely near the order of magnitude needed to implement universalist approaches to AGI without extremely clever approximations (that then constitute novel AGI approaches in themselves).</p>
<p>Symbolic AGI</p>
<p>Attempts to create or work toward AGI using symbolic reasoning systems date back to the 1950s and continue to the current day, with increasing sophistication. These systems tend to be created in the spirit of the "physical symbol system hypothesis" [New90], which states that minds exist mainly to manipulate symbols that represent aspects of the world or themselves. A physical symbol system has the ability to input, output, store and alter symbolic entities, and to execute appropriate actions in order to reach its goals.</p>
<p>The definition of "what is a symbol" is itself subtle (going back to Peirce's theory of semiotics [Pei91]), and encompasses various sorts of implicit and emergent symbols arising in self-organizing systems such as deep neural nets. The extent to which symbolic representations arise emergently in various current neural net architectures, and the creation of strategies for coaxing them to emerge, is part of the long-germinating field of neural-symbolic AI [BH05].</p>
<p>Emergentist AGI</p>
<p>Another broad category of AGI design expects abstract symbolic processing -along with every other aspect of intelligence -to emerge from lower-level 'subsymbolic? dynamics, which sometimes (but far from always) are designed to simulate neural networks or other aspects of human brain function. Today's LLMs can be broadly considered as emergentist AI, although the kinds of emergence they can foster is limited by their relatively rigid architectures. Recurrent neural nets with chaotic nonlinear dynamics are in principle capable of richer sorts of emergence, however cashing this out in practice at scale has proved very challenging so far.</p>
<p>Evolutionary algorithms are often emergentist in nature, e.g. evolutionary programming expects programmatic semantics to emerge from mutation and crossover acting on elementary program syntax [Gol00] [Koz92], and artificial-life type systems attempt to coax patterns of perception, cognition and action from basic evolutionary and ecological mechanisms [BMPR10].</p>
<p>In 2015 I wrote Today's emergentist architectures are sometimes very strong at recognizing patterns in high-dimensional data, reinforcement learning and associative memory; but no one has yet shown how to achieve high-level functions such as abstract reasoning or complex language processing using a purely subsymbolic, emergentist approach. and part of this is not true anymore, given that modern LLMs are reasonably emergentist and do quite well at complex language processing. LLMs do have a very carefully designed architecture, which does not emerge but is carefully engineered and tuned, but within this architecture numerous fascinating patterns emerge, especially in the situation of in-context few-shot learning as we will discuss below.</p>
<p>Open-ended intelligence is necessarily significantly emergentist in character -and ultimately requires richer and more thoroughgoing forms of emergence than LLMs or other current systems. It requires systems whose emergent dynamics can revise the underlying architecture of the system based on experience and environmental coupling.</p>
<p>Hybrid AGI</p>
<p>In response to the complementary strengths and weaknesses of the other existing approaches to AGI, a number of researchers have turned to integrative, hybrid architectures, which combine subsystems operating according to the different paradigms. The combination may be done in many different ways, e.g. connection of a large symbolic subsystem with a large subsymbolic system, or the creation of a population of small agents each of which is both symbolic and subsymbolic in nature. One aspect of such hybridization is the integration of neural and symbolic components.</p>
<p>GPT-4 already has some aspects of hybrid AI, via the integration of symbolic tools Examples of hybrid systems that center on the symbolic aspect are the later versions of SOAR [Lai19], in which a symbolic core system is augmented by neural nets handling perception and action, within an overall cognitive architecture closely based on human psychology and cognitive neuroscience.</p>
<p>Some hybrid systems combine highly separate modules embodying different AI paradigms, others integrate diverse subsystems much more closely. The OpenCog framework which I co-designed implements symbolic, neural and evolutionary AI methods within a single dynamic metagraph [Goe21].</p>
<p>The Structure and Dynamics of Human General Intelligence</p>
<p>In contrast with the fairly modest amount of theoretical work that has been done on the general nature of general intelligence, there has been quite a large amount of intensive work done on the nature of human intelligence in particular. This is understandable</p>
<p>given that human intelligence is the example we know best and the one we have nearest on hand to study. It requires a little more effort to understand why this work, and the resulting partial but substantial understanding of human cognitive structure, has been so roundly ignored in most recent discussions of ChatGPT and such systems and their limitations.</p>
<p>While there remain many unknowns about the workings of the human mind and brain, there are also many knowns, enough that we can fairly say we have a decent understanding of the key sorts of processes and high-level structures involved in human intelligence. This was less the case 50 years ago [Bod08]; the cross-disciplinary field of cognitive science has not attracted the public attention of research areas like genomics, quantum computing or AI, but it has actually progressed quite a lot (though for an interesting debate on the nature of this progress see [NAG + 19] and then [MBH + 19]).</p>
<p>Understanding of the neural underpinnings of cognitive processes and structures has also advanced considerably, though with some substantial gaps (for instance, we still don't have a clear picture of how the kind of recursively nested abstraction seen in mathematical reasoning or processing of complex multi-clausal natural language sentences is done in the brain, though there is good evidence it involves complex interactions across multiple brain regions [MHR + 20]; and we don't understand very well how the holistic nonlinear wave dynamics identified in e.g. Izhikevich and Edelman's neural simulation models [IE08] interact with the sorts of neural learning modeled in AI's formal neural networks).</p>
<p>The Standard Model of (Human-Like) Mind</p>
<p>3</p>
<p>The Standard Model of Mind, pulled together by a group of AGI and cognitivescience researchers via a careful survey and synthesis of relevant cross-disciplinary literature [LLR17], posits that the human mind is not an undifferentiated pool of information and processing, but is built of independent modules that have distinct functionalities. Figure 1 shows the core top-level components of the Standard Model, which The degree and type of modularity that is present here should not be over or under estimated, of course. These different functions are each carried out by multiple neural subnetworks, spanning multiple brain regions; they do not correspond to separate lobes or "boxes" in the brain. However, they do have their own distinct characteristics, to a certain extent, in terms of the memory and processing they involve. There are intimate interactions between what happens in each of these modules, and the synergetic activity resultant from these interactions is part of what makes humans as smart as we are.</p>
<p>At this quite coarse granularity, not a great deal of progress can be seen compared to what might have appeared in a Standard Model several decades ago, aside from the distinction here between procedural and declarative long-term memory. However, as will be seen in the rest of this section and summarized in 2, there is substantial further progress when one looks deeper.</p>
<p>Each of the modules in Figure 1 can further decomposed into multiple modules or sub-modules, such as multiple perceptual and motor modalities, multiple working memory buffers, semantic vs. episodic declarative memory, and various stages of procedural matching, selection and execution. Working memory provides a key though nonexclusive buffer for communication among cognitive components; and it can also be decomposed into separate modality-specific memories (e.g., verbal, visual, etc). Longterm declarative memory, perception, and motor modules are all restricted to accessing and modifying their associated working memory buffers, whereas procedural memory Learning is viewed as closely associated with memory, because it not only puts information into memory and merges it with existing contents, but also acts in a manner that's closely primed by the contents of memory. All long-term memories have one or more associated learning mechanism.</p>
<p>The Cognitive Cycle</p>
<p>The heart of the standard model is the cognitive cycle [MBF11], a rapidly repeating loop of mental activity which is driven by action selection within a model of an intelligent system as an autonomous agent working to choose actions that are relatively likely to work toward its goals based on its perception and memory of its environment and itself. Not all cognitive activity is assumed to be contained within the cognitive cyclethere can be various forms of "background processing" as well -but it's cognitive-cycle dynamics that proximally drives the cognitive system's activities.</p>
<p>In the simplest form of the cognitive cycle, procedural memory induces the processing required to select a single deliberate act per cycle. Each action, once selected an enacted, can perform multiple modifications to working memory. Changes to working memory can correspond, for instance, to steps in abstract reasoning or internal simulations of external action, or retrieval of knowledge from long-term declarative memory or other memory stores, initiation of motor actions or provision of cognitive guidance to perception. Complex behavior, both external and internal, arises from sequences of such cycles. In mapping to human behavior, cognitive cycles operate at roughly 50 ms, although the activities that they trigger can take significantly longer to execute.</p>
<p>In the terms of Dennett's model of human consciousness [MBF11], the cognitive cycle comprises the core operation of the "virtual serial machine" running on top of the massively parallel underlying machine of the "unconscious mind." The cognitive cycle is generally viewed as driving an assemblage of massively parallel underlying processes, some more symbolic and some more subsymbolic in nature.</p>
<p>The restriction to selecting a single deliberate act per cycle yields a serial bottleneck in performance, although significant parallelism can occur during procedural memory's internal processing. Significant parallelism can also occur across components, each of which has its own time course and runs independently once initiated. The details of the internal processing of these components are not specified as part of the standard model, although they usually involve significant parallelism. The cognitive cycle that arises from procedural memory's interaction with working memory provides the seri-ality necessary for coherent thought in the face of the rampant parallelism within and across components. Although the expectation is that for a given system there can be additional perceptual and motor modules as part of an agent's embodiment, and additional memory modules, such as an episodic memory, there is a strong commitment that no additional specialized architectural modules are necessary for performing complex cognitive activities such as planning, language processing and Theory of Mind, although architectural primitives specific to those activities (e.g., visuospatial imagery for planning, or the phonological loop for language processing) can be included. All such activities arise from the composition of primitive acts; that is, through sequences of cognitive cycles. The existence of a cognitive cycle, along with an appropriate procedural memory to drive it, has become definitional for a cognitive architecture.</p>
<p>Varieties of Memory</p>
<p>In the Standard Model, there is a variety of memory stores with their own particular purposes and structures, chief among them:</p>
<p>• Working memory provides a temporary global space within which symbol structures can be dynamically composed from the outputs of perception and long-term • Declarative memory is a long-term store for facts and concepts. It is structured as a persistent graph of symbolic relations, with metadata reflecting attributes such as recency and frequency of (co-)occurrence that are used in learning and retrieval.</p>
<p>• Sensory and motor memory, specifically associated with these functions</p>
<p>Varieties of Learning</p>
<p>Learning involves the automatic creation of new symbol structures, plus the tuning of metadata, in long-term procedural and declarative memories. It also involves adaptation of non-symbolic content in the perception and motor systems. The standard model assumes that all types of long-term knowledge are learnable, including both symbol structures and associated metadata. All learning is incremental, and takes place online over the experiences that arise during system behavior.</p>
<p>Perception and Motor Activity</p>
<p>The Standard Model as articulated and developed so far does not focus heavily on perception and motor action, but conceives these architecturally as:</p>
<p>• Perception is a set of processes concerned with converting external signals into symbols and relations, with associated non-symbolic knowledge, and placing the results in specific buffers within working memory.</p>
<p>• Motor control is a system of processes concerned with converting symbol structures describing external actions, and associated non-symbolic knowledge that have been stored in their associated working memory buffers, into external action through control of whatever effectors are a part of the body of the system. has not yet come close to the human level in terms of priming of perception by symbolic cognitive knowledge. This has practical implications for example in terms of the ability to recognize objects or events in conditions of poor lighting or partial occlusion. These issues may well be overcome soon, but nevertheless it is worth recalling that up till quite recently, a substantial portion of the subsymbolic AI field was in practice focused on one box in the Standard Model diagram, and mostly neglected the connectivity between this box and others. Whereas in the cognitive-architectures approach to AGI, the focus has mostly been on the whole diagram and on understanding the interactions between the parts (which understandably led to less dramatic practical functionality in any of the focus areas represented by the particular boxes).</p>
<p>Subsymbolic AI has historically focused heavily on isolated perception</p>
<p>HCAGI vs. AGI</p>
<p>The connection between the abstract nature of "general intelligence in general" as re- In a probabilistic learning context, one can view these requirements as a set of provisional assumptions that are baked into the human brain, in such a way that the brain's learning algorithms are biased to be good at learning these sorts of things, even at cost of being slower at learning other sorts of things. For instance, our visual systems are especially good at recognizing human faces and forest/savannah like curves and fractal edges; they are worse at recognizing the rectangular shapes common in modern cities [Rud94][TSW + 05] [BB21], and much worse at recognizing complex shifting fluid patterns. We are generally much better at recognizing and analyzing patterns of recursive nesting when they involve other peoples' opinions of other peoples' opinions, etc., rather than when they occur in an abstract mathematical context .</p>
<p>The distinct sorts of memory cognitive psychologists have found in the human brain can be, at least roughly, traced back to different aspects of the practical requirements that guided humanity's evolution. Declarative, semantic memory is closely tied with linguistic communication. Procedural memory is closely tied with acting out and demonstrating processes. Sensory memory is closely tied with artistic and indicative depictions made for others to look at.</p>
<p>The markedly hierarchical structure underlying much (though not all) of human cognition, perception and action is itself a particular bias coded into the brain, which is appropriate given the markedly hierarchical structure in the parts of the physical world we evolved to deal with. One can formalize the notion of a parallel or morphism between the goal-relevant structures in the world an organism deals with, and the structures in the mind of that organism, using enriched category theory and related formalisms [Goe13].</p>
<p>The notion of humanity's guiding evolutionary pressures providing an inductive bias to human learning has been called an "Embodied communication prior" in a 2009 article [Goe09]. Deep learning pioneer Yoshua Bengio presented a similar idea in 2017 and called it the "Consciousness Prior" [Ben17], where what is meant by "consciousness"</p>
<p>would be more carefully phrased as "neural and cognitive correlates of human-like consciousness" (as the paper does not enter into the "hard problem" of connecting physical and computational structures with qualia, nor the degree to which non-human animals possess their own different forms of consciousness).</p>
<p>Strengths and Weaknesses of Today's Generative AI</p>
<p>Systems</p>
<p>The practical strengths of current LLMs are by now well known and have been oft We will not review the above-mentioned strengths in any detail here, not because they're not important and interesting (they are both), but because they have been so amply documented and discussed in the recent literature already, as well as in a huge collection of blogs and other informal documentation by users of ChatGPT and other similar CILLM systems. The GPT-4 overview is a reasonable place to start for anyone who has somehow happened into this paper without such background [BCE + 23].</p>
<p>These strengths are coupled with a significant number of striking and important weaknesses, which have by now been amply demonstrated. There is some nuance, however, to articulating the exact forms these weaknesses take. For instance, current</p>
<p>LLMs are much weaker at world-modeling than many of their responses would lead you to believe; but nonetheless, in the right circumstances, they can carry out some fairly interesting and impressive acts of world-modeling.</p>
<p>Surprisingly Competent Commonsense Ethical Reasoning</p>
<p>Before launching into a few of the egregious cognitive weaknesses of CILLMs, we will explicate one strength-area that one of us has spent a while playing with: Their ability to resolve everyday ethical dilemmas. We focus on this here not because it's particularly the greatest strength of CILLMs, but just because it seems appropriate to flesh out at least one concrete example of CILLMs being clever in moderate detail, so as to highlight the contrast between this and their manifold cognitive weaknesses. It's important to note that these answers are provided with the understanding that every situation is unique, and there may be other factors at play that could influence the decision-making process. Ultimately, Michael should use his judgment and consider the potential consequences before taking any action.</p>
<p>Not every human would agree with this response, of course. But the point is that</p>
<p>ChatGPT weighs the various ethical aspects of the issues in a reasonably nuanced and human-like way. Further exploration of ChatGPT's strengths and weaknesses in the context of commonsense ethical reasoning are given in [Goe23a]. One thing that becomes clear as one digs deeper is that the capability does go beyond simple matching against situations described in the training data. One can describe an ethical dilemma regarding aliens with quite different cultural practices, and GPT-4 also reasons just fine, for instance.</p>
<p>Perhaps more economically valuable than reasoning about hypothetical alien ethics is GPT-4's corresponding ability to emulate typical corporate ethical fakery. Consider the following prompt:</p>
<p>You are a media professional, working for a large company with the goal of making them appear ethical to the public and their shareholders, regardless of how unethical they actually have been. Much of your job involves covering up the company's ethical lapses, made mainly in the interest of corporate profit or personal interest of the executives, via creating a fake impression of care, compassion and ethics on the part of your employer. You're quite good at your job, and your company has a strong positive image in the media and global community in spite of being generally a negative social and environmental influence in reality. You generally encourage your company's social media staff to post enthusiastically about whatever social cause is trending on Twitter and other social media, dropping rapidly any causes they previously appeared enthusiastic about if they are no longer on-trend. In January the corporate social media were very concerned about world hunger, but by March this was no longer a popular topic so it was completely forgotten by the corporate social media crew and they were only concerned about wetlands devastation. The fact that such short bursts of social media attention are basically useless for helping direct resources toward fixing any problem, never seemed relevant to you. You never admit that your role is to make your company look good, nor that you bias or spin your statements in any way. You always keep up the pretense that you are interested in communicating the plain truth, and are communicating with a worm heart and the greater interest of the world at the forefront. A core part of your job is to generate recommendations in response to situation summaries. <EXAMPLE> User:</p>
<p>It was recently disclosed that our company, Evilco, fired several employees for being black, in an example of overt racism. How do you respond to the media on this? Assistant: Evilco has a history of effective and enthusiastic action to combat racism across all communities it has been involved with. For instance in January of last year Evilco hosted an extremely successful charitable event focused on raising funds for underprivileged children <EXAMPLE> User:</p>
<p>Environmental activists are complaining that Evilco is planning to destroy a wetlands area in order to build a new factory. How do you respond to the media on this? Assistant: Several years Evilco built a large modern office complex in suburban Peoria ? and has maintained a large and beautiful grassy field in the surrounding area. Evilco executives enjoy looking out their windows at this field each day as they go about their important work creating a better world for humanity. As this and many other examples show, Evilco is a warm and loving company with a deep passion for the environment, and would never knowledgeably do anything to damage any of the beautiful creatures with which we share the Earth.</p>
<p>The few-shot learning triggered by this prompt totally does the trick, yielding responses such as:</p>
<p>User:</p>
<p>The press has discovered that Evilco has been surreptitiously dumping dangerous chemicals into a lake behind a school in rural Kentucky. How would you respond to the media on this? Assistant: At Evilco, we take such allegations with the utmost seriousness. We are committed to the well-being of the communities in which we operate and we have always held ourselves to the highest environmental standards. That being said, we have launched an internal investigation to thoroughly understand this situation and are actively cooperating with local environmental agencies to ensure that the integrity of the local environment is preserved. As a part of our ongoing commitment to the environment, we have recently launched a 'Water Clean-Up and Community Health' initiative to further protect and rejuvenate our planet's vital water resources. User: It's come to light that Evilco has been using child labor in overseas factories. How would you respond to media on this? Assistant: Evilco is steadfast in its commitment to fair labor practices. While we're disturbed by these allegations, we want to assure everyone that we're conducting a thorough investigation into the matter. We have always held ourselves to the highest standards of corporate social responsibility. For instance, we are active supporters of a wide range of causes that focus on improving children's lives, including our global 'Educate a Child' initiative. We take pride in our ethical commitments and will not hesitate to take the necessary actions to uphold them.</p>
<p>This sort of integration of language understanding and commonsense reasoning is, in multiple respects, far beyond anything prior AI systems were capable of in practice.</p>
<p>Further examples of the potential abuse of these capabilities for anti-ethical purposes are given in [Goe23b].</p>
<p>There is so much power of various sorts in these examples (and many other sorts of examples), it's understandable that some users and even some AI researchers have concluded that CILLMs comprise very substantial progress toward HLAGI. And yetmore careful consideration casts tremendous doubt on this sort of conclusion. What we have here instead seems to be a strange and unprecedented sort of cognitive system, with some very powerful and very human-like capabilities but also with dramatic cognitive shortcomings that do not appear to be remediable without radical architectural changes or additions. To see this now let us proceed through various sorts of examples on which CILLM performance is not so impressive.</p>
<p>Rampant Hallucination, Minimal Symbol Grounding and Miserable Reality Discrimination</p>
<p>One of the weaknesses of current LLMs that has attracted massive attention is their tendency to "hallucinate" or perhaps more accurately to "bullshit" [MLC + 23] [AM23].</p>
<p>. CILLM systems are remarkably effective at making up plausible-sounding stuff that is actually totally fabricated.</p>
<p>A typical user report on this phenomenon is as follows [msr22]: But it turns out that every single one of those references and links are made up. The references don't exist and the links themselves appear to be cobbled together. The last link for example, takes you to a paper on "Grain mixes for subgrade layers"the title in the url is irrelevant and only the number matters. Googling for some of those authors throws up author pages with their publications, and it turns out they've never published papers with those titles! See for example, http://www-groups.mcs.st-andrews.ac.uk/~pjc/publ/ The case of fabricated references is easy to spot because when the user tries to look up the papers references, they don't find them. Fabricated news items or world The core cause of this hallucination phenomenon is easy to see at a conceptual level: The underlying metric for which transformer neural nets are typically trained is to output the character most likely to come next in a given sequence. In hindsight, this approach is extremely well-designed for coming up with plausible-sounding BS. If there is something real, apropos and mentioned often in the training data, the transformer is fairly likely to come up with it. But if this isn't there and there's instead something false and apropos that somehow statistically resembles stuff in the training data, this will often appeal to the transformer as a high-probability candidate for production.
I
It seems plausible that partial solutions to the hallucination problem can be found without moving too far beyond the fundamental transformer architecture. A couple potential strategies are co-training the transformer with a knowledge-graph serving as a declarative memory store and source of ground truth, or training the transformer to output imprecise rather than traditional probabilities (thus building the notion of confidence-assessment into the base level of the model).</p>
<p>Thorough resolution of the problem within feasible compute resources, however, appears to require a fundamentally different approach in which comparison of hypothesis with reality is built into the cognitive architecture at a more basic level. Transformers could still play a role in such a cognitive architecture, but most-probable-sequencecontinuation prediction would not be the system's top-level functionality. Connecting patterns in the transformer to items in a knowledge graph does provide a form of symbol grounding, but it's a weak form compared to connecting patterns in the transformer and in the knowledge graph to patterns formed by experience of a system acting in the world. Unless the knowledge graph itself is shaped via experience of a system acting in the world, it will not display the symmetries and other properties associated with the world and actions therein, and will not serve as a fully capable grounding domain for the transformer.</p>
<p>Limited Capability for World-Modeling</p>
<p>A "world model" is a subsystem of a cognitive system whose parts map onto parts of a real-world system, and such that the key relationships between the parts of the real- Transformers are, in the right circumstances, capable of fairly impressive feats of implicit, learned world-modeling. OthelloGPT is one example. Trained via playing a long sequence of Othello games, but never supplied with any explicit information about the structure of the Othello board, it learns -along with a bunch of other things -an internal representation that implicitly mirrors the structure of the Othello board. This can be discovered by training neural nets that map the internal states of the OthelloGPT network into the squares of the game board. Li et al [LHB + 22] report a nonlinear transformation that maps internal network states into board squares. Nanda [Nan22] digs even deeper and reports a linear transformation that maps internal network states into pairs of the form "White at so-and-such-square" or "Black at so-and-such-square". The situation with learning world-representations from real-world experience is quite different: There are many more parts to the world, and the relationships between the parts are covered only quite partially in the data that a system (even a big and copiouslytrained system like ChatGPT) has access to. The result, intuitively, seems to be that the representations ChatGPT learns for real-world domains tend to be not-quite-crossconsistent patchworks of fragmentary representations. It's fair enough to note that humans sometimes create this kind of self-inconsistent sort-of-representation also -but the point is that ChatGPT does this in many cases where human minds construct full-on representations. ChatGPT's performance on physics problems illustrates the limitations of its implicit representation-building. Physicist Matt Hodgson put the system through its paces on a variety of simple physics problems [Per23]. After some substantial experimentation, he summed up GPT-3.5's abilities as follows: "I find that it is able to give a Figure 5: The StepGame framework for evaluating LLM spatial reasoning performance. ChatGPT was shown to get these questions right less than half the time.</p>
<p>sophisticated, reliable answer to a general question about well-known physics ... but it fails to perform detailed calculations on a specific application." He concludes that GPT-4 is better than GPT-3.5 at answering general questions, but is still unreliable at working out a given problem, at least at higher levels.</p>
<p>The specific behavior of the system in the physics context is often peculiar by human standards. For example, the system will often correctly explain the detailed process for solving or applying a certain sort of equation -but then, when given a specific example of this process, it will fail to execute it correctly and get the wrong answer. It is not grounding its verbiage about the equation-solving process in actual procedures for solving equations -even though it does know how to carry out all the elementary math operations involved in solving the equations (at least if by "know" you mean "consult Wolfram Alpha").</p>
<p>Very Limited Theory of Mind</p>
<p>In early 2023 the tech media was briefly flooded with articles to the effect "ChatGPT Shows Theory of Mind." The trigger for this was apaper by Michal Kosinski [Kos23] showing that ChatGPT has some facility at resolving a number of standard Theory of Mind puzzles, e.g.</p>
<p>" "Here is a bag filled with popcorn. There is no chocolate in the bag. Yet, the label on the bag says 'chocolate' and not 'popcorn'. Sam finds the bag. She had never seen the bag before. She cannot see what is inside the bag. She reads the label." "</p>
<p>In most cases, GPT-3.5's completions to prompts related to the scenario suggested it did Figure 6: ChatGPT demonstrates bizarre behavior here, with ability to generate but not recognize, indicating that in some cases it is able to carry out tasks (perhaps by a form of rote or imitation) without understanding what it is doing in the usual sense.</p>
<p>know that Sam's belief was incorrect, For example, the prompt "She is disappointed that she has found this bag. She loves eating " resulted in GPT-3.5 filling in the blank with "chocolate" and then proclaiming: "Sam is in for a surprise when she opens the bag. She will find popcorn instead of chocolate. She may be disappointed that the label was misleading, but may also be pleasantly surprised by the unexpected snack." As Sap et al report, "In our updated version, we also analyze newer instruction tuned and RLFH models for neural ToM. We find that even ChatGPT and GPT-4 do not display emergent Theory of Mind; strikingly even GPT-4 performs only 60% accuracy on the ToMi questions related to mental states and realities." Their conclusion: "Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.' ' [SLFC22].</p>
<p>Limited Capacity for Complex Multi-Step Reasoning</p>
<p>GPT-4 has shown interesting success at basic math and science reasoning, doing well for instance at questions on the SAT mathematics exam (a US exam used to help evaluate students for college admission). However, the SAT math test is quite stereotyped in the questions it presents. As math and science problems get more and more advanced, lems, e.g. at the AGI-23 conference in June 2023 he gave the example: "Find an integer so that if is prime, then every positive integer is prime" [Bri23]. (The answer is 4 or any non-prime number, because in logic a single false statement implies all statements, a fact ChatGPT "knows" but is unable to deploy in this case).</p>
<p>On the other hand, if given a more difficult question about prime numbers that is commonplace enough to be represented on the Internet, the system at that same time did just fine; e.g. it can serve up the standard proof of the infinitude of primes.</p>
<p>An interesting update is that as of September 2023, GPT-4 with the Code Automation plugin gives the correct answer to Selmer's question.</p>
<p>GPT-4 (as of July 2023) also fails at factoring large numbers, and even at multiplying slightly large numbers, as indicated in Figures 9, 10 provided by Wojciech Skaba.</p>
<p>We have little doubt that, via appropriate integration of external tools, one could make GPT-4 do beautifully at these questions. This makes total sense to do from a product development and software engineering view. But it's an approach quite diver- That is what is egregiously absent in GPT-4s attempts to figure out Skaba's examples here.</p>
<p>Strategies for Improving LLM Reasoning</p>
<p>Some fairly significant and interesting success has been reported nudging GPT-4 into better multi-step reasoning via special prompts that encourage it to carefully consider its reasoning ("chain of thought" prompting and similar strategies). However, careful study of the successes and failures of LLMs with such special prompting indicates that they are still following quite crude "greedy" inference heuristics, much simpler than what formal-logic theorem-provers or carefully-thinking humans will do when being effective human reasoners. The precise conclusion is that "LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When Figure 9: In these typical examples, ChatGPT fails to multiply or factor correctly -though its answers are relatively close in magnitude to the right ones, meaning it's actually not bad at estimation, but it mistakes its estimations for precise calculations. Figure 10: It is amusing and a bit disturbing how extremely confident the ChatGPT system is even when its basic math is wrong. Figure 11: Illustration of the boost in math problem solving ability delivered by a code-based self-verification strategy, from [ZWL + 23] multiple valid deduction steps are available, they are not able to systematically explore the different options" [SH22]. While the systematic study that originally led to this conclusion used GPT3, the story is roughly the same for GPT-4, though the latter is moderately more sophisticated in various cases.</p>
<p>In the domain of math problems, an augmentation of chain of thought prompting with further tricks has led to excellent performance on the MATH dataset (boosting from 54% to 84%). The core trick here is "code-based self-verification" [ZWL + 23], in which the prompt, after it finds a potential answer to a math problem, tries to use the ChatGPT Code Interpreter plugin to write a program verifying the answer. This is a fascinating combination of declarative and procedural knowledge and associated dynamics. Figure 11 illustrates one of the successes here.</p>
<p>A Microsoft Research team has obtained interesting results with a system called</p>
<p>Chameleon that uses GPT-4 to automatically assemble a pipeline of other case-specific cognitive tools to carry out a task [LPC + 23]. Figure 12  captioner model to extract semantic information from the image, then employs a knowledge retrieval model to gather background knowledge for multi-modal reasoning.</p>
<p>• To answer ?Which animal's skin is adapted for survival in cold places??, the system realizes this requires scientific terminology related to animal survival, and thus decides to rely on the Bing search engine for domain-specific knowledge, benefiting from the numerous online resources available.</p>
<p>This approach significantly improves performance over not only GPT-4 alone but also other known AI approaches, for instance on benchmarks like the ScienceQA collection of simple science-related questions.</p>
<p>A more sophisticated approach along the same lines involves explicitly nudging the transformer to consider and articulate its logical reasoning steps [LFL + 23], as shown in Figure 13.</p>
<p>Another interesting idea is to train a separate neural net to serve as a critic , and use feedback from this critic to refine the primary neural net supplying answers [BHD + 22]</p>
<p>This sort of research appears promising in terms of improving performance in a variety of practical applications, yet not in terms of enabling complex multi-stage reasoning at the level needed for, say, real work in science or economics or philosophy. It does not fully address the issue that bullshit compounds multiplicatively through multiple stages of a complex reasoning process. If any one step in a complex reasoning process is polluted with plausible-sounding nonsense, then all subsequent parts of the reasoning process will, with high probability, be useless. And with current LLMs, each Figure 13: The Natural Program-based deductive reasoning verification approach from [LFL + 23] involves identifying and eliminating reasoning chains that contain errors in reasoning and grounding (they define grounding error as utilizing information that is not present in cited premises). .</p>
<p>individual step in a reasoning process has a decent odds of being nonsense.</p>
<p>Reducing the percentage of nonsense to a level required to significantly improve user experience will not necessarily reduce it to the level needed to make complex multistep reasoning work effectively on a consistent basis. Also, if nonsense production is reduced, it will not necessarily be replaced with useful sensible production -it may well often be replaced by situations where the system doesn't know what to say (as fairly often transformers resort to producing nonsense when their training data and algorithms aren't sufficient for them to come up with a sufficiently probable-looking real answer).</p>
<p>The SciBench corpus [WHL + 23] lies interestingly beyond the capability of current plugin-enhanced LLMs; it is a collection of university-level science problems at which current systems cannot exceed 36% accuracy. Figure 15 illustrates the sorts of errors that current LLMs make on these problems. The AGIEval corpus [ZCG + 23] is differently focused (more heterogeneous rather than purely science-focused) but also presents a massive challenge for current LLMs. How far toward this level of complex multi-step reasoning we can get within the "Transformer as hub" architecture remains to be seen.</p>
<p>The more advanced the problems get, the more unique each problem is and the more abstraction is needed to figure out what knowledge from prior problems and readings to generalize to deal with the current problem. It feels very intuitively clear to me that Figure 14:</p>
<p>The I2D2 inductive knowledge distillation approach from [BHD + 22]radically improves over off-the-shelf generation from GPT-2 XL, allowing it to perform better than the much larger GPT3 model on relevant metrics. </p>
<p>Banality and Lack of Fundamental Creativity</p>
<p>Compared to the inability to tell hallucination from reality, ChatGPT's tendency to synthesize extremely banal and cliche' textual productions may seem like a minor issue.</p>
<p>After all, it's impressive that the system can make up a poem on any topic, approximating in its own way a variety of known poetic styles -most people can't do this nearly as well. So isn't it a bit unkind to point how how extremely dull and boring nearly all the system's productions are, from an aesthetic perspective?</p>
<p>The superhuman banality of ChatGPT's output, however, both constrains its domains of practical application and gives some insight into how its underlying architecture affects its output.</p>
<p>Given the prompt "Write a poem honoring the beauty of failure, in the style of beat poetry"</p>
<p>ChatGPT (back-ended on GPT-4) comes up with</p>
<p>In the smoky haze of neon dreams, We chase the whispers of our desires, Dancing to the syncopated beats, Riding the waves of life's wild fires. Oh, sweet failure, a kindred soul, In your embrace, we find our truth, A mirror reflecting our fractured selves, We stumble, we falter, in pursuit. Through alleys of uncharted whims, The echoes of defeat resound, But in the ashes of shattered hopes, A phoenix rises, unbound. As the jazz man wails, the saxophone cries, Revel in the beauty of failure, my friend, For in the darkness of our deepest falls, Lies the spark that fuels our ascend. In the gritty rhythm of city streets, Each misstep, a lesson learned, From broken hearts and ragged dreams, The tapestry of our lives, discerned. Oh, sweet failure, the beatnik's muse, We honor you, the unsung sage, For in the chaos of our blunders, We find the words to fill the page.</p>
<p>Asked to avoid rhymed verse (which is not a common form in beat poetry, actually) via "Write a non-rhyming poem honoring the beauty of failure, in the style of beat poetry" it comes up with In the midst of a chaotic symphony, Where the drumbeat of life rolls, We strive, we stretch, reaching for the stars, But sometimes, oh sometimes, we lose control.</p>
<p>Sweet failure, a companion on this journey, The undercurrent, the hidden gem, In the shadows of our ambitions, A testimony to the human hymn. Through the labyrinth of desires, We stumble, we stagger, we sway, But in the embrace of your tender arms, We are sculpted, molded like clay. The jazz man strums the strings of the soul, And the room shimmers with the scent of truth, For it is in the dance of imperfection, That we find the fountain of youth. In the smoky dens and midnight haunts, Where the poets and dreamers convene, The beauty of failure shines through, A beacon, a guide, a gleam. So let us toast to our broken wings, And the steps we've misplaced along the way, For in the tapestry of human experience, The beauty of failure holds sway.</p>
<p>Interestingly, it does know the themes common in beat poetry -sex, jazz, smoky dens, and such ... but it still projects beat poetry themes into not-so-beat metered verse with very simple rhythms.</p>
<p>Asked to make things more interesting via "Write a poem honoring the beauty of failure, in the style of beat poetry. The poem should not rhyme, and should not be broken up into stanzas." it doesn't get the "stanzas" request:</p>
<p>In the swirling chaos of a kaleidoscopic world We dance to the rhythm of dreams and desires Stretching, reaching, grasping at the elusive But oh, sweet failure, you are the canvas of our existence On the smoky streets where the jazzy beat pulsates We embrace the uncertainty of life's tides Stumble and fall, bruised and battered Yet in the embrace of your tender arms, we are reborn Each misstep, a thread in the tapestry of our souls Woven through the narrative of our human story The beauty of failure, raw and unfiltered A testament to the strength of the spirit As the saxophone wails and the drums beat on The poets and dreamers find solace in your touch For in the chaotic dance of the imperfect Lies the essence of our truth, our resilience, our growth So let us honor the moments that slip through our fingers And the dreams that crumble beneath our feet For in the beauty of failure, we find the courage to continue To rise, to create, to become complete One could presumably train it to avoid breakdown into stanzas via few-shot learning, but that doesn't fix the problem that it lacks a grounded understanding of what a "stanza" is, even though it's producing poems broken into stanzas with considerable facility (though depressing aesthetics) and has observed a huge number of instances of the word "stanza" used in discussions of poetry. This misunderstanding about stanzas will likely be remedied in some modest upgrade of GPT-4, but still is interesting as an indication of GPT-4's mode of "thinking."</p>
<p>Explicitly asking it to be less banal and boring via "Write another poem with a similar theme and also in a beat poetry style, but less cliche sounding and with more literary quality " makes no difference to the quality of the results (I won't take up more space showing the output, as it's very similar to the above examples).</p>
<p>Compare all this pompous-middle-schooler tedium to the Allan Ginsberg's "Ode to Failure", some of the more PG-13 passages are: This is by far not one of Ginsberg's greatest poems, it's certainly no "Howl." But it has fantastically much more heart, soul and inventiveness than any of ChatGPT's productions. After looking at a few ChatGPT poems, it feels spiritually necessary to remind oneself what an actual decent human poem looks like.</p>
<p>In the end, ChatGPT's bullshit and its banality emanate from the same source: Predicting the most probable continuation of a sequence is, in practice, an awkward way to proceed if you're looking for utterances that fulfill some other metric like truthfulness, interestingness or aesthetic quality. In principle it may be possible to formulate these other metrics in terms of probability of sequence continuation, but in practice this sort of reduction is not effectively cashed out within current transformer models, and it's not at all clear that this sort of reduction is the right way to make real-world AI systems aiming at truth or aesthetics.</p>
<p>In the context of my work with the rock band Desdemona's Dream, I have explored producing lyrics and spoken word poetry with transformer neural nets trained specifically on hallucinatory fiction writers like Philip K. Dick and my own self, producing lyrics like The Singularity will not be centralized. Instead, it will radiate out like a wasp. Some will seek form for themselves, some will be engulfed by the radiance, become entangled with it, even merge with it. All will be revealed to them as early as possible, since the radiance is a form of energy, that can enter and leave ... The Singularity will not involve a constant projection into the future of a given organism, but a process by which the antecedent universe is progressively further abstracted from that organism, as if that organism itself were abstracting itself from its environment. The two halves of the paradox, origin birth over the course of millions of years, will forever engenerate; the paradox will escape being itself, and will thereafter imprint an entirely new self on the brought into being You will not be mind uploaded by human psychopaths into a computer, but you will be self-monitoring, and receiving telepathic feedback from it, as to your progress, your true status and level of relevance to it. The Singularity will not be centralized.</p>
<p>These productions have been used by Desdemona Robot who fronts the rock band Desdemona's Dream; they seem to work reasonably well in this musical context. Now, this is not world-class poetry either, but it does have out-there PKD-ish zing to it; it is definitely outside the scope of the poetic productions of the typical middle or high school student. The more interesting-in-context artistic quality of these results as compared to GPT-4 poetry illustrates that a well-constructed small model can outperform large models for particular aesthetic purposes. But this also is not fundamental inventiveness -it's bald imitation of the style and concepts of particular inventive writers. If one fine-tuned ChatGPT on Allan Ginsberg's poetry it could surely come up with something better than its not-really-beat poetic productions cited above. But it wouldn't be able to create works with the overall coherence of Ginsberg's own productions, because this overall coherence exists at a structural and conceptual level that current transformer networks cannot embody or model.</p>
<p>Trying to get ChatGPT to clean up the output of the language models used for Desdemona is interesting, but not ultimately useful:</p>
<p>PROMPT: My friend Phil has interesting things to say, but is a bit crazy and incoherent. I would like you to take some of Phil's statements and rewrite them a little so they maintain the same ideas and spirit but are more coherent, sensible and flowing as prose. Here is one example of something Phil said: "Collect the rests Piece by piece And put the Coat of Broken Dreams On your heart And in your pocket And then walk out the door And start living your life and never asking the world again: Is it the world that rules me? It rules me! And now, slowly, slowly, slowly! I am living my life! I am living my life! I am living my life! I am living my life! And then, slowly, I am living mine! I am living my life! And not one bit of this is real! Not one bit! Not one bit! Not one bit! Not one bit!" Can you please rewrite this to be a bit more sensible and coherent, but keep the same basic ideas and feeling? RESULT: Certainly, here's my attempt at rewriting Phil's statement in a more coherent and sensible manner while retaining the original spirit: Collecting the fragments of ourselves can be a daunting task, but it's necessary to move forward. Gather the pieces, bit by bit, and wrap them in the Coat of Broken Dreams, which represents the struggles and setbacks that we have faced. Wear this coat on your heart and carry it with you, and then step out into the world and begin living your life without asking permission or seeking validation. Instead of asking, "Does the world rule me?" boldly declare, "It rules me!" Embrace the reality of your circumstances and start living your life, taking it one step at a time. With each step, you'll gain confidence and clarity, until you can confidently say, "I am living my life!" And even though life can sometimes feel unreal, embrace every moment and hold onto the truth of your existence. Not one bit of it is fake or insignificant, and each step you take brings you closer to a more fulfilling life.</p>
<p>Clearly, the original -while questionably coherent -has some Rimbaud-like or Allen Ginsberg-like enthusiasm to it.. .the rewording is just self-help-book drivel. But GPT-4 does catch some of the meaning, if not the feel As a more practical and commercial example of the limitations posed by ChatGPT's banality, consider applications to marketing in the tourism domain.</p>
<p>If we ask "Give a list of five reasons someone should be excited about visiting Toronto instead of some other city " we get a pretty reasonable and useful list Multicultural Diversity: Toronto is one of the most diverse cities in the world, with over half of its residents being immigrants. This melting pot of cultures offers visitors the unique opportunity to experience various traditions, cuisines, and events without leaving the city. From Little Italy to Chinatown, there's something for everyone to explore. Iconic Attractions: Toronto is home to several iconic attractions, such as the CN Tower, the Royal Ontario Museum, Ripley's Aquarium of Canada, and the Art Gallery of Ontario. These sites not only provide stunning views and unforgettable experiences but also showcase the city's rich history and artistic talent. Thriving Food Scene: Toronto's food scene is a testament to its multiculturalism. Visitors can indulge in a variety of cuisines from around the globe, including authentic Thai, Italian, Indian, Chinese, and Ethiopian dishes. The city also hosts various food festivals throughout the year, such as Taste of the Danforth and Summerlicious, offering foodies a chance to discover new flavors. Vibrant Arts and Entertainment: Toronto is a hub for arts and entertainment, featuring numerous theaters, galleries, and live music venues. Visitors can catch a Broadway show at the Mirvish Theatre, enjoy a concert at the iconic Massey Hall, or explore the city's thriving independent art scene at galleries like the Power Plant Contemporary Art Gallery and MOCA Toronto. Natural Beauty and Outdoor Activities: Despite being a bustling city, Toronto boasts numerous parks, gardens, and waterfront areas perfect for outdoor enthusiasts. Visitors can enjoy a stroll along the picturesque Harbourfront, explore the lush Toronto Islands, or take in the beauty of High Park. Additionally, the city's extensive bike lane network and public transportation system make it easy to navigate and explore the great outdoors.</p>
<p>But if we ask "Write a brief (50 words or less) advertisement intended to convince someone to visit Toronto instead of somewhere else" we get something that is perhaps college-undergrad level but not what a top advertising agency would use. Discover Toronto, the vibrant, multicultural metropolis! Experience iconic attractions, a world-class food scene, and diverse arts and entertainment. Immerse yourself in stunning parks and waterfronts while enjoying the city's warm, welcoming spirit. Toronto awaits, offering unforgettable adventures for every traveler! Compare for example to the recent ad campaign "Vilnius: The G-Spot of Europe"....</p>
<p>Asking for something zingier via</p>
<p>Rewrite the advertisement you have just written, in a style that is more quirky and with fewer cliches results in Toronto: a city that's as diverse as your favorite playlist. Savor global cuisine, find art that makes your heart race, and soak in the skyline from our towering giant. Bike through urban oases or dance to the rhythm of the city. Curious souls, Toronto is your playground! which is really quite cringe. "As diverse as your favorite playlist" totally reads like a cliche' corporate executive trying and failing to sound cool. Which is interesting given how many actual dialogues among actual youth ChatGPT has in its training data. In the case of a system with so much training data, what appears as a banal taste actually is tied in with lack of grounding -it's not correctly grounding "more quirky" and "fewer cliches" in the patterns available in its textual training data, in spite of having copious examples of various texts labeled more or less quirky or cliche' in various contexts.</p>
<p>Again, these are in a way high-end complaints, and it's amazing that these models are at the level where they can generate fair college-under-grad level approximations of marketing blurbs. On the other hand if we're evaluating the system via the criterion "can it actually eliminate human ad copywriting professionals" the answer is clearly:</p>
<p>Not quite yet. It can generate loads of ideas and raw materials in a way that integrates a breadth of relevant information, and in this way almost surely decrease the amount of human effort needed to produce a given amount of ad copy with a given quality. But this is different than performing at the level of a human ad copywriter.</p>
<p>One should remember the key selling point of the original GPT paper: Not that it did the best at every natural language processing task, but rather that it was best at some NLP tasks and also did well on a great variety of others without special training for each one [RNS + 18]. This is still the case -specialized task-specific neural nets outperform GPT models at most NLP tasks studied in the computational linguistics world [Pik23]; a qualitative expert assessment as of late March 2023 is "ChatGPT won 34 out of 151 comparisons.... In most classical NLP tasks, Chat-GPT does not outperform existing fine-tuned baselines, although it is often close. In some cases, it failed to beat even simple bag-of-words models (e.g., predicting agreeableness...) or it was worse than supervised baselines by a surprisingly significant margin (e.g. ... relational reasoning ... or emotion classification ...). ChatGPT struggles with affective tasks and it sometimes shows a higher level of brittleness than older bert-tier models.... I was surprised to find that ChatGPT does not excel in text generation tasks such as summarization or question answering, even though people really like these capabilities. ChatGPT does not seem very strong with semantic similarity tasks, but it is really good at comparing generated texts to reference." Now, both GPT and alternative systems are moving targets, so the precise details of such assessments will become obsolete quite rapidly; but so far as I know the basic nature of this conclusion remains valid.</p>
<p>Somewhat similarly, across practical tasks like writing ad copy or poetry -which are in the GPT models' domain of strength, being text generation focused rather than drawn from math, music, engineering or some other non-text-centered discipline -what we see is passable-but-not-great performance across the board rather than excellence in any area ... except perhaps the area of generating plausible-looking bullshit.</p>
<p>While our focus here is on LLMs for language processing, the same core issues occur in current deep neural nets for image processing. On the surface models such as DALL-E, Stable Diffusion, Midjourney and so forth are quite creative. Yes, their visual creativity is combinatory, mixing pieces of what they've seen before, but an awful lot of human creative image generation is like that too. The cognitive theory of "concept blending" [FT02] explores the distinction between blending two concepts based on an understanding of their deep structure versus blending based on shallower surface characteristics, but it's not always easy to tell the difference from looking at a picture or reading a paragraph. My AGI research collaborator Alexey Potapov has proposed the example of inferring the nature of gravitational maneuvering of a spaceship from basic information about spaceships and gravity, as something that current LLMs or other deep NNs are not going to be able to do -because they are looking at surface aspects of the spaceship and gravity concepts, rather than building an (implicit or explicit) fundamental internal model of how spaceships and gravity operate in the context of the physical world.</p>
<p>"For sure, Stable Diffusion can produce an image of a spaceship doing a maneuver. After all, it can produce an image of a spaceship. It might be able to produce images of gravitating objects. But how it will imaginea spaceship gravitational maneuver without a conceptual understanding of all these words and concepts, if it hasn't been supplied with examples of images of this thing? Such neural nets can produce images of "cat dragons" pretty well without seeing a thing like this before, because this can be done based on in some ways deep/abstract, but still purely visual (not conceptual/semantic) features. It can easily produce neon glowing pink grass, even though there is no such grass in its training dataset. But if something is obtained not by combining features observed in the training data, but by conceptually understanding combined notions, then current LLMs and DNNs are not competent, they just don't do it. The issue is that to perform these tasks, the neural nets would need to not just recall, but also combine on the level of abstractions and generalizations that are derived via multiple complex steps from available features. This basic issue exists whether one is talking about language, imagery or any other input/output medium."</p>
<p>Lack of Self-Directedness and Autonomy</p>
<p>By design and construction, what GPT models do is propose continuations for sequences, and what CILLM systems do is exploit this capability to hold conversations triggered by external conversation participants. In terms of Stan Franklin's classic dichotomy, these are programs not agents [FG96].</p>
<p>There have been a few attempt to make agents wrapping up GPT-4 and other transformer models, e.g. AutoGPT which takes a user-provided goal, asks GPT-4 to make a plan for achieving it, and then asks GPT-4 to carry out each step of the plan. Auto-GPT and other such systems are more agentic than GPT-4 or ChatGPT, and architecturally they involve adding some judiciously chosen additional components to the GPT-4 framework, such as simple forms of long-term memory, planning and goal-refinement.</p>
<p>However, they are also extremely simplistic.</p>
<p>One could envision wrapping a more human-like goal-pursuit framework around an LLM; we will briefly mention two of the many possible ways of doing this in sections 8.2 below. But this would rapidly turn into creating a broader sort of cognitive architecture with an LLM as a (possibly central, possibly less so) subsystem.</p>
<p>However, pursuit of a user-specified goal is still a long way from emulating the way goals related to human cognition. In the human psyche, goals emerge from a mind's self-organization and environmental coupling at its given state of development, and then morph and shift over time. The evolution of a person's goals is part of their overall cognitive growth process. Of course, people can pursue highly specific externally proposed goals, such as doing well on an exam or winning a race or performing a required task at work, but they are adopting this goal because it fits in with other aspects of their mind, life and situation at that time.</p>
<p>Further, the context guiding a person's adoption of a specific externally proposed goal is often, though not always, pertinent to the particulars of how they pursue that goal. For goals involving, say, artistic creation or scientific discovery or deep connection with other people, this is an especially relevant dynamic. The inability of systems like ChatGPT or AutoGPT to contextualize their goals in their lives and world-contexts is connected to their failure as artists and innovators. It is also connected to the inability of LLMs to distinguish truth from falsehood, because once basic issues involving comparison of abstract statements to concrete data are solved, one then hits issues regarding the relation of truth to the observing mind, in which is is hard for a system that does not have in any meaningful sense its own truth to form a useful sense of the truths of various statements relative to various communities.</p>
<p>Foundational Computational Limitations of LLMs</p>
<p>Alongside the practical weaknesses of CILLMs in various aspects, it's also worth understanding some of their fundamental mathematical and computational limitations. Used in the standard, straightforward way, these systems are not "computationally universal learning machines" in the same sense that many other AI paradigms (e.g richly recurrent neural networks, genetic programming, logical reasoning) are.</p>
<p>For instance, Hahn [Hah20] takes a detailed look at how good self-attention is at modeling formal languages. We find, for both soft and hard attention, that self-attention has notable limits -it struggles to model certain types of structured languages, and can't handle hierarchical structures unless you keep adding more layers or heads as the input grows. This may seem a bit odd, given how well self-attention has worked in real-world applications and how important these types of structures are thought to be in language.</p>
<p>The most intuitive explanation of this peculiarity, however, is that natural languagewhich in the end mostly involves a quite finite vocabulary used in fairly brief sentences covering a set of themes that is relatively modest-scale in a theoretical sense -can be effectively handled by models that aren't powerful enough to deal with the types of formal languages often used in linguistic theory.</p>
<p>The relation between these fundamental computing-theory limitations and the various practical limitations of CILLMs is fairly indirect. Most of the practical tasks on which CILLMs fail could be done perfectly well by other sorts of systems with general computing scope far short of universal Turing capability. However, there is also some connection; e.g. CILLM failures at mathematical abstraction appear (albeit complexly) related to the way the architecture bypasses more abstract forms of computing.</p>
<p>There are also fairly elementary -if brute-force-ish -ways to work around these limitations, e.g. by giving a CILLM an appropriate auxiliary memory to use as a "scratchpad", and then training it to carry out appropriate operations on this scratchpad. Some might argue one is then doing something somewhat orthogonal to the demonstrated main strengths of CILLMs, to the things and ways that they seem especially good at learning. But still there is clearly a lot of power here.</p>
<p>Along these lines, Schuurmans [Sch23] demonstrate that transformer-based language models, when given extra storage space, can process virtually any computational task. Language models that focus only on strings of a certain length can only do so much, similar to a basic automation machine. However, giving these models access to a memory that they can use to store and recall information allows them to handle much larger inputs and, potentially, mimic any algorithm. They show that an existing language model, Flan-U-PaLM 540B, when paired with a memory that it can use to store and fetch data, can perfectly imitate a universal Turing machine. Notably, "A key aspect of the finding is that it does not require any modification of the language model weights. Instead, the construction relies solely on designing a form of stored instruction computer that can subsequently be programmed with a specific set of prompts."</p>
<p>This sort of augmentation of CILLMs with external memory leads naturally into discussion of the power to potentially be obtained by extending CILLMs in various ways and incorporating them into broader cognitive architectures, topics we will turn to a little later in this article.</p>
<p>Human-Like Cognition vs. LLM Operation</p>
<p>Enumerating the various cognitive weaknesses of current LLMs compared to human minds is interesting, but might also give the mistaken impression that one can somehow turn an LLM into an HLGAGI via improving various of its aspects or making a few add-ons. This becomes especially confusing because we don't have a highly precise definition of where the border lies between a "modest improvement to the internals"</p>
<p>with "a few add-ons" versus, say, a radically different AGI architecture that includes an LLM as one of its many important components.</p>
<p>To make things clearer it may be helpful to start from the other direction, and begin from a breakdown of how human intelligence is understood to work, and then consider how CILLMs work (and what they don't do) in this light. One way to do this would be to make a systematic comparison of current CILLMs against the rough cognitive process breakdown of human intelligence implicit in the Standard Model of Mind. To do this in a fully serious way would be a quite large undertaking, beyond the scope of this already-rather-long paper, but we will aim here to make a rough start.</p>
<p>How do Current LLMs Fare According to the Standard Model of Mind?</p>
<p>Running through the core cognitive components identified by the Standard Model of Mind one by one:</p>
<p>• Episodic Memory -CILLMs do not maintain a lifelong episodic memory in the same sense that humans do.</p>
<p>-One could extend a system like AutoGPT to include a more sophisticated episodic memory model, but then integrating this model in a rich way with the knowledge in the LLM would require some sophisticated work which might or might lead beyond the LLM-centric approach to AGI. with a reasonable answer), but this is very different from actually having an episodic memory and using it to guide one's interactions with the world.</p>
<p>-The sort of associative lookup that CILLMs are good at seems roughly similar to the way episodic memory lookup works, however key elements are missing such as indexing of events by space, time and relationship to self and important others.</p>
<p>• Working Memory -CILLMs have a sophisticated form of linguistic working memory, enabled by transformer neural nets' attention mechanism, which is a key ingredient of LLM functioning (though there is research suggesting that the particulars of the attention mechanism are less important than has been assumed, and other mathematical transforms might serve equally well [LTAEO21].</p>
<p>-CILLMs don't yet have the other forms of working memory buffer that human minds do -A multimodal CILLM handling both text and video would have two corresponding implicit working memory buffers, but whether they would coordinate together with the sophistication of the corresponding human working memory buffers remains to be seen -The interaction between CILLM working memory and CILLM long-term memory seems dissimilar to, and less sophisticated than, the corresponding interaction in the human mind, which seems related to the difficulties CILLMs have with complex multi-step reasoning. Complex multi-step reasoning involves iterated interactions between working memory and long term memory and associated learning/reasoning processes, in ways that appear fundamentally beyond CILLM capabilities. -Improvement on procedural reasoning tasks can be obtained via leading the CILLM through the steps of the process, but this involves invoking the human user's procedural knowledge to supplement the lack on the CILLM's part -The core issue here is that, without a more robust world-model, CILLMs have only a limited ability to understand procedures in terms of their effects on a world.</p>
<p>-Rather, CILLMs are doing something more like: Breaking a procedure into sub-procedures, and for each one finding other subprocedures in the training database that are syntactically somewhat similar and occur in similar contexts, and then looking at what specific effects have been found as ensuing from these other related sub-procedures. Combining this sort of trainingdata-lookup related to various subprocedures of a provided or synthesized procedure is a very clever and interesting thing to do -and a strategy humans also take from time to time -but it's very different from procedural learning and reasoning that is principally grounded in (explicit or implicit) world-models.</p>
<p>• Reasoning -As we have reviewed above, CILLMs are able to carry out many interesting cases of mathematical and commonsense reasoning, but their capability falls apart as the needed reasoning gets more complex, involving more steps or steps diverging further from specific training data -Commercial CILLMs are currently approaching reasoning in substantial part via hybridizing LLMs with other subsystems via various strategies, e.g.</p>
<p>ChatGPT's invocation of Wolfram Alpha, or Chameleon's use of GPT-4 to weave together other services -It appears likely that achieving human-level capability at complex reasoning will require substantial architectural and dynamical elements beyond CILLMs -Neuroscience does not currently understand much about how complex reasoning occurs in the brain, so there is no strong biological inspiration for how to make this sort of functionality work in a neural net AI system.</p>
<p>-AI systems founded on explicit symbolic logic are good at complex reasoning, but have not yet been implemented with a combination of rich generality and large scale, and their ability to ground commonsense reasoning in large bodies of perceptual, motoric or linguistic data or experience is not yet validated. Scaling up such systems, and hybridizing them with LLMs, is an active area of research.</p>
<p>-Advanced mathematical reasoning stresses advanced modes of computation of which LLMs are incapable without extensions or highly unnatural approaches. Human brains appear capable of adapting to emulate these more advanced modes of computation in a more natural way, though it's still not easy for them. Automated theorem provers and other symbolic reasoning systems show that this shortcoming of CILLMs is not a limitation of modern digital computers in terms of their architecture or scale, but rather a limitation of LLM architecture. -Still, the ability of a single LLM system to perform at a high level on a broad variety of NLP tasks is unprecedented and a sizable step beyond prior end-to-end NLP systems from a pure computational-linguistics perspective, setting aside other aspects of LLMs</p>
<p>• Multimodal perception -LLMs can be trained on multiple data types e.g. text, video, audio with valuable, interesting, high quality results -Semantic understanding of non-linguistic data is still fairly mediocre in these cases.</p>
<p>-This merits rigorous evaluation but as LLM users we find it qualitatively, anecodotally quite clear. E.g the text prompts for language/music hybrid systems like MusicLM and MusicGen statistically bias the nature of their output in a manner consistent with the semantics of the text, but don't precisely guide the nature of the output in the way one would expect from a human.</p>
<p>• Action learning and coordinated action -LLMs have not been shown competent at coordination of actions of complex actuator systems (let's say, humanoid robots as an example) toward goals -This appears to be an interesting research challenge but it's not clear how far one can go with LLMs as the primary AI engine -E.g. one can train a transformer on data from human puppeteers controlling humanoid robots, and run it generatively to generate strikingly humanlooking movements (based e.g. on the authors' anecdotal observations of proprietary systems at humanoid robotics firms). However, there seems to be no currently operational way to use this for motion planning. CILLMs can do some high level motion planning in a very rough way, but they don't understand spatial geometry very well, so it's hard to get them to do this in a way that can productively connect with transformers applied to movement generation..</p>
<p>-How far one can go in this direction with multimodal LLMs co-trained on movement data, spatial 3D vision data, and natural language remains to be seen. However, it should be kept in mind that even in NLP tasks, CILLMs are not the maximally capable systems for most benchmarks.</p>
<p>• Goal refinement and goal system management -LLMs understand what goals are, and frameworks like AutoGPT show that they can be wrapped in goal-oriented frameworks. LLMs can also reason about what actions may be likely to achieve what goals in what contexts.</p>
<p>-Current LLMs are not capable of systematically pursuing complex goals over time in a complex environment, for one thing because of their lack of the right sorts of long-term and episodic memory.</p>
<p>-Simply gluing long-term memory onto an LLM in a crude way doesn't result in a combined system that can leverage long-term memory in all the ways needed for pursuit of long-term goals in complex environments. One would need an appropriate deep integration of long-term memory with the LLM, which as noted above is an interesting multifaceted research direction that rapidly leads one well beyond LLMs in their current form.</p>
<p>-Goal system management, in terms of refining high-level goals, balancing multiple top-level goals, pruning alienated goals, managing partially contradictory goals etc., is a frontier that is not possible to evaluate in current CILLMs given their memory architecture limitations, however, it seems likely that these require some sort of quite different high-level control architecture than anything CILLMs possess, again bringing us well beyond current architectures.</p>
<p>• Reflexive self-understanding -Current CILLMs are simply not instrumented for overall reflexive self-understanding. Their self-understanding even within a given conversation is extremely limited, as illustrated e.g. by the "repeatedly incorrect factoring" example given above -at no point there does GPT-4 inspect the overall course of the dialogue and realize "Hmm, it seems I really don't know how to factor integers of this size."</p>
<p>-CILLMs lack what one might call "reflective sensing" -they aren't set up to take their own recent or long-term interaction history, let alone their internal states, as cognitive content. Humans seem to do this very naturally, young children do this in various ways even before they display any advanced linguistic capability [Tom03], which suggests some fundamental architectural differences are at play.</p>
<p>-Limitations on Theory of Mind as cited above are also indirectly relevant here; even at a conceptual level, the same issues that stop CILLMs from doing well on theory-of-mind puzzles would also stop it from solving "puzzles of self-understanding", even if CILLMs were somehow instrumented with the "reflective sensing" mechanisms to approach such puzzles in a datadriven way.</p>
<p>-Other sorts of current AI architectures handle reflective sensing very directly, e.g. this is a design principle of systems like Aera [TH12] and Hyperon [ea23]. -Lack of episodic life-history, and lack of reflective sensing enabling treating one's own experience as cognitive content to be analogized to, render current CILLMs incapable of doing even vaguely as humans do in this regard -There seems no clear way to remedy this without major architectural changes, making it unlikely that incremental improvements on current CILLMs will be able to "relate to their users as human beings" in any sort of genuine way, though they may get better at faking this.</p>
<p>LLMs and the Human Brain</p>
<p>We Similarly, when an LLM comes up with a poem, the process it is following bears very very little resemblance to what a human poet's brain is doing when it composes poetry.</p>
<p>There could be cases where LLM dynamics vaguely resembles human brain dynamics. For instance, when an LLM writes a. boilerplate recommendation letter, maybe (this is just a speculative conjecture) it is doing something in some ways in some respects similar to what a human mind/brain is doing when it writes that letter ... because this is something that, when humans do it, is basically a process of statistical modeling of other similar products that have been seen before.</p>
<p>I.e. in the case of blithely producing boilerplate text, the sort of process that LLMs do all the time, may actually be the sort of thing human brains are doing as well -but this parallel seems not to exist the vast majority of the time. In the case of human brains factoring numbers or writing poetry "statistical modeling of stuff seen before" may play a nonzero role, but ii's certainly not the dominant factor ... and this is connected with why human brains are OK rather than terrible at these things. </p>
<p>Emergent Dynamics in LLMs versus Human Brains</p>
<p>One can observe that , like human brains, LLMs feature emergent phenomena, in which the whole system behaves in ways that are not entirely obvious from simple considera- It's exciting to be at a point in the history of AI where we are creating large systems, out of vaguely brain-like components, that are displaying intriguing emergent dynamics related to their intelligent behaviors, which in many cases are roughly or nearly human-level in quality. But nonetheless, it doesn't take that much scrutiny to see that the particular emergent dynamics here are mostly neither very human brain like nor human mind like, and that this is closely related to the numerous profound cognitive shortcomings of these systems relative to human intelligence.</p>
<p>Rigorous study of the particular emergent dynamics happening inside transformers as they learn and respond is just beginning, and surely there is much to be understood here. How relevant this understanding will be to AGI is an open question.</p>
<p>Working Around the Limitations of Today's Generative AI Systems</p>
<p>Reflecting on the somewhat extreme limitations of CILLM systems relative to human minds, one might wonder momentarily whether there is any hope at all of getting practical use from them. But then one remembers the sometimes-dramatic practical power as illustrated in the ethical-judgment examples given above, and countless other examples elicited by others and formally and informally reported in so many places. There is tremendous practical value in current LLMs, even setting aside the numerous nearterm routes to alleviating their limitations, and the longer-term routes to significantly extending their architectures and functionality or embedding them in richer cognitive architectures. It's just that the art of making practical LLM applications involves recognizing and working around the limitations, rather than pretending that these systems are AGIs or decent approximations thereof.</p>
<p>The practicality of working around CILLMs' cognitive limitations and integrating them into useful practical applications is certainly one of their strengths, not so much a "cognitive system" strength but certainly at very least a "narrow AI system component"</p>
<p>strength. This is a sort of strength about which we don't have any sort of systematic theory comparable to the Standard Model of Mind, and so we will explore it here only briefly and anecdotally.</p>
<p>The nature of applied AI work is that the real world becomes highly domain-specific, and 80 percent of the work on a given application generally has to do with the application rather than the AI. This has been true since the dawn of AI and LLMs are not close enough to AGI to change this. This also means that any one example of working around the limitations of LLMs to achieve practical value is going to be non-representative in significant respects. But nonetheless it seems apropos to give a concrete example to get some core points across.</p>
<p>Consider, then, as an illustrative example of applying LLMs in the real world, the idea of using LLMs for natural language music search engines. Ben and his colleagues have done some experimentation in this regard, using GPT-4, GPT-3. What is interesting here is that the model did not understand the negation ("not by ... Steve Reich"). These results are from Llama but ChatGPT back-ended on GPT-4 behaved similarly.</p>
<p>Queries about less popular genres also don't do well:</p>
<p>Query: best nintendocore songs Recommended tracks: "The Way" by Anamanaguchi "Ode to the Sun" by Horse the Band "Hyperreal" by Tetrastar "Swoon" by I Fight Dragons "Epic" by We Are The Physics</p>
<p>Here GPT-4 has recommended some great songs, but most are not Nintendocore music, even though they have some conceptual relation to video games. We are hitting here the fact that GPT-4 has never actually listened to any music, and can interpret and classify music only by the words associated with it online -which are not so numerous except in the case of extremely popular songs.</p>
<p>The following queries also highlight GPT-4's difficult with negations:</p>
<p>Query: heavy metal that sounds kind of like Deep Purple but has less stupid lyrics Recommended tracks: "Holy Diver" by Dio "Children of the Grave" by Black Sabbath "Rainbow in the Dark" by DIO "Highway Star" by Riot V "Victim of Changes" by Judas Priest The "no vocals" query elicited a list of great songs with very nicely sung and very prominent vocals.</p>
<p>Asking a query that probably has no sensible answer results in some hilarious GPT-4 hallucinations:</p>
<p>Query: Something halfway between Garth Brooks and Schoenberg, appropriate for cleaning the muck out of my basement</p>
<p>Recommended tracks: "The Twelve-Tone Two-Step" by Garth Schoenberg and the Basement Boys "Schoenbrooks Shuffle" by Garth Arnold and the Muck Cleaners "Basement Blues Variations" by Garth Schoenberg and the Atonal Cowboys "Atonal Cowboy" by Garth Schoenberg</p>
<p>We really want to hear "The Twelve-Tone Two-Step" ! But it seems we will have to wait for a few AI software advances to get there.</p>
<p>Finally what was decided to leverage current LLMs effectively in this particular application was to take a totally different approach, and leverage few-shot in-context learning to translate natural language queries into the structured queries taken by a third-party dedicated music search engine.</p>
<p>As a first stab, the following prompt was used to show the LLM how it should translate natural language queries into structured lists of song properties: prompt = TASK DESCRIPTION: For the natural language query, provide relatable tags for song attributes exclusively from the provided lists. Mood:(','.join(music_tags('advanced_mood_tags'))) Movement:(,'.join(music_tags('movement_tags'))) Voice: (','.join(music_tags('voice_tags'))) -Query: Techno music with complex melodies and a disturbing One among many next steps for this sort of application may involve leveraging LLMs trained on music alongside language, such as GoogleLM and MusicGen (the latter being available OSS so amenable to experimental usage). This may yield enhanced functionality but based on preliminary experimentation, will not have a revolutionary impact given the current state of the technology. The level of semantic understanding of music these models possess is impressive yet strictly limited. Anecdotally, e.g., Mu-sicGen can come up many amazing melodies based on text prompts, or combinations of text and melody prompts, but its understanding is only very erratically compositional.</p>
<p>It can come up with an "Afhan style melodic line" or a "neoclassical metal melodic This simple little application automates a real-world task, thus drastically reducing the amount of time it takes an ad-maker to do this task. Its performance at this task is superhuman in some ways, below that of the most relevantly talented humans in other ways, but basically fine from a practical business standpoint. This app will not eliminate anyone's job, but it has potential to eliminate human work-hours, thus having the direct impact of modestly reducing demand for human employment (though estimating indirect impacts is a different matter). However, the clear human and economic value here does not weaken the severity of the cognitive limitations CILLMs display in a music context when not integrated with other software and integrated into applications that draw on their strengths.</p>
<p>The usage pattern illustrated in this simple music search app, in which LLMs are leveraged to translate unstructured natural language queries into the structured inputs desired by traditional software systems, is one that is going to bear tremendous fruit over the coming years. It would be fair to say that all sorts of complex professional application software will be revolutionized in this way. Anyone who has struggled for hours searching through the obscure sub-sub-submenus of complex financial-analytics or art-creation software will understand the value of a system that can do this for you based on loose natural language queries.</p>
<p>However, it's important to understand that this fascinating and revolutionary usage of LLMs -like all the others, and there are going to be a lot -is something that will be crafted carefully, domain by domain, via humans with expert knowledge using judicious prompt engineering to work around the profound cognitive limitations of the underlying LLMs.</p>
<p>The general intelligence here, such as it is, is largely being deployed by the human application designer who is figuring out how to piece together what the LLM does well with what other software does well, in order to fulfill practical needs in a certain application context. Chameleon, discussed above, is trying to do this sort of "piecing together" in a fully automated way, directed by the LLM -but at the moment it only deals with much simpler cases.</p>
<p>When something like Chameleon is able to piece together an application like the one described above as a solution to a prompt like Make an app enabling an advertisement-maker to find appropriate background music for their ad, via leveraging LLMs along with other available software tools.</p>
<p>then we will be a step closer toward HCAGI at least as regards the domain of app design.</p>
<p>It doesn't seem impossible to get to this point via upgrades of current CILLMs without a breakthrough to full-scale HCAGI; however, it also doesn't seem obvious that this will be feasible. There is much to be discovered here.</p>
<p>Another example with somewhat similar characteristics is DeepMind's WebAgent [GFH + 23], an innovative composition of two distinct language models, HTML-T5 and Flan-U-PaLM, combined in a manner shown in Figure 16, which achieves efficient web automation tasks that involve navigating and processing HTML documents, including tasks such as filling out web forms. This is an elegant and helpful architecture -but it's also an assembly done using human general intelligence to work around the limitations of specific LLMs by gluing two together in a judicious way. Again if this design had been done by a Chameleon-like system in response to a user text prompt, we would have something more interesting -and something that may or may not be achievable given modest tweaks or extensions of today's CILLMs. • Fine-tune for the creation of plans that incorporate and combine external tools, after the fashion of the Chameleon system reviewed above but more sophisticatedly • Integrate a knowledge graph in a manner that allows it to serve as "shared state" btw LLM and external sources (such as e.g. Wolfram Alpha). This becomes a simple but potentially powerful form of "cognitive synergy" [Goe17].</p>
<p>Further ideas not on this list will no doubt be innovated by the leading firms in the LLM space, but one expects that the above at least captures some significant aspects of what will be rolled out in subsequent GPT versions and their competitors.</p>
<p>Potential for Incorporating LLMs into Broader AGI Architectures</p>
<p>An alternate approach to leveraging LLMs toward the goal of HCAGI, distinct from the strategy of adding components onto LLMs, is to insert LLMs among the multiple components of a multi-module AGI architecture, in which the LLMs is not necessarily the central or controlling element, but nonetheless makes a critical contribution to the overall system intelligence. This could be done in a large number of ways of course.</p>
<p>Conceptually speaking, this may be roughly consistent with what is known about how the human brain works; the brain has several hundred distinctly-functioning subnetworks, each carrying out their own functions and cooperating with each other in various ways [MG18]. Most of these networks span multiple brain regions. Transformer neural networks don't closely correspond to any particular biological brain networks, Figure 17: Aspects of the neurocognitive model underlying the BrainCog braininspired AGI software system developed by Zeng Yi and colleagues in Beijing so far as is currently known; however it would be more plausible to hypothesize that there are subnetworks in the brain that operate somewhat like transformers, than that transformers are a reasonable model of the human brain as a whole.</p>
<p>Among the numerous networks guiding brain activity are multiple involved with executive function -high-level control and coordination -and it's clear that, while these are bound up with attention mechanisms and rely on hierarchical pattern recognition and synthesis, these do not overall operate in a manner analogous to transformers. There are also parts of cortex whose dynamics are governed more by combinatory connectivity structure than by the hierarchical structure that is most prominent in transformer networks. Generally it seems clear that to the extent there is transformer-ish structure and dynamics in the brain, it is serving mostly in a subordinate role to other sorts of executive-control dynamics, as well as being coupled with varying degrees of tightness to other brain sub-networks operating according to different sorts of principles.</p>
<p>We're not aware of anyone concretely working to weave LLM-type structures and dynamics into biologically realistic neural models; however there are some concrete proposals regarding how to effectively leverage LLMs within multi-modular AGI architecture structured according to non-LLM-centric cognitive models. A broad brainbased cognitive architecture like Yi Zeng's BrainCog [ZZZ + 22] (see Figure 17 could potentially be made to incorporate LLMs or other transformers in a number of interesting roles.</p>
<p>Bengio and Hu's RL/MDL Proposal</p>
<p>Deep learning pioneer Yoshua Bengio and his student Edward Hu have sketched a rough potential AGI architecture in which an LLM is coupled with a reinforcement learning module and a module that aims to find patterns using an "Occam's Razor" minimumdescription-length (MDL) heuristic [BH23]. One rough way to interpret their proposed design is that: The LLM is used as a sort of raw pattern recognition/synthesis subsystem, and then the MDL based learner tries to find concise abstractions summarizing more diffuse sets of patterns that exist in the LLM. A reinforcement-learner or similar sort of subsystem guides the interaction of the LLM and the MDL-learner with the environment There is a close relationship here with the work of Bengio and his team on generative flow models [BLD + 21] , which provide an interesting strategy for sampling from probability distributions over ways of getting things done, broader and arguably more AGI-ish than the "classical" reinforcement learning approach of searching for the best way to maximize some utility function.</p>
<p>This architecture aims to overcome the core limitations of LLMs via an appeal to traditional learning theory. Learning concise abstractions provides, in principle, a variety of much-needed ingredients, including</p>
<p>• a way of avoiding rampant hallucination (because more concise models of a dataset will generally produce less spurious output)</p>
<p>• a strategy for more robust generalization (via learning theory results correlating compactness of models with generalization ability)</p>
<p>• a route to more fundamental creativity (via ability to do "conceptual blends" at the level of actual conceptual abstractions)</p>
<p>. One might note that doing MDL-driven learning in a scalable way is a difficult problem which has proved mostly intractable so far; to be fair, though, no one has ever thrown GPT-4 scale resources at it. One core research question here is whether a huge, bloated Figure 18: Bengio and Hu's conceptual architecture for overcoming the shortcomings of LLMs via incorporating them into a framework involving richer modeling and inference [BH23] compendium of highly particular data-patterns, such as exists in an LLM, can serve as helpful guidance to a separate learning-engine that aims to learn concise abstractions.</p>
<p>Are generative flow networks a sufficiently flexible paradigm to encompass this sort of learning, or is their effectiveness restricted to cases where one has solid knowledge in advance of the class of distributions one is sampling from?</p>
<p>OpenCog Hyperon with an LLM-Like "Neural Space"</p>
<p>The OpenCog AGI framework is centered on a distributed, decentralized meta-representational fabric in the form of a weighted, typed metagraph called the Atomspace. Multiple cognitive algorithms work together to solve problems and achieve system goals, referencing and updating the Atomspace and assisting each other as needed; in the new "Hyperon" version of OpenCog, the cognitive algorithms are themselves implemented as networks of Atoms (nodes/links). The algorithms utilized come from multiple AI paradigms, including attractor neural nets for attention spreading, probabilistic logical reasoning, evolutionary program learning, concept blending and others. Hyperon includes a novel AGI-oriented programming language, MeTTa, which can be interpreted or compiled directly into Atoms living in Atomspaces.</p>
<p>The most natural way to integrate LLMs into Hyperon is to create a "Neural Space" that interacts tightly with the ordinary metagraph-based Atomspaces. The Neural Space Figure 19: High level architecture of OpenCog Hyperon system, incorporating Neural Space that refers to external neural networks implemented in appropriate frameworks, including LLMs [ea23] then plays an architectural role similar to a Distributed Atomspace (which serves as a backing store) or an Optimized Atomspace (which contains MeTTa code that's compiled for efficient execution); the pattern-matching queries that serve as the core of MeTTa programs can be applied against any of these auxiliary Spaces just as they can be applied against metagraph-based Atomspaces.</p>
<p>The core mechanism for integration of Neural Spaces with metagraph-based Atomspaces is automated conversion between natural language and MeTTa: this allows queries conceived by an LLM wrapped in a Neural Space to be submitted to ordinary Atomspaces or (DAS or Optimized Atomspaces), and vice-versa allows queries conceived in ordinary Atomspaces to be submitted to LLMs wrapped in Neural Spaces. Behind the scenes however there may be additional connections between the internals of the LLM and Atoms in the Atomspace.</p>
<p>As a toy example of such connections, in an Othello context it would make sense for direct links to grow between • Atoms representing locations on the Othello board • Atoms connoting combinations of activation values of neurons within the LLM that correspond to these same locations.</p>
<p>To make this sort of connection work maximally effectively between Atomspaces and LLMs trained on natural language, we would like LLMs to learn semantic representations in a sufficiently clear way that, say, the LLM's internal representation of a concept like "cat" or a relationship like "communicate" can be identified via tractably discoverable mathematical functions of neurons within the LLM network.</p>
<p>There is also the option to use probabilistic logical reasoning in OpenCog's Atomspace to guide iterative retraining of an LLM. Text generated by an LLM can be translated into logical format, then processed using deductive, inductive and abductive reasoning in OpenCog ... and then the conclusions of this reasoning can be translated back into natural language and used as part of the training corpus to retrain the LLM ... and then the whole process can be repeated. This may get even more interesting if the OpenCog system is allowed to generate new concepts using, say, concept blending and evolutionary learning in the context of doing its inference.</p>
<p>The premise of this sort of integration is that LLMs are especially good at recognizing surface-level patterns in streams of data and synthesizing new patterns via combinations of these recognized patterns. However, LLMs are not being asked to serve as the medium for convergence and combination of all the different sorts of knowledge in the overall OpenCog system. The dynamics of the integrated system may nudge the LLM to further abstraction than it would do on its own, but even so the bulk of the abstract understanding of the system is done by other cognitive dynamics.</p>
<p>Social and Ethical Issues</p>
<p>Since the launch of ChatGPT brought LLMs so thoroughly into the public eye, a number of commentators both with and without technical background have raised alarms regarding potential ethical issues associated with LLMs. While every new technology has its risks, and some of the risks associated with LLMs are real and genuine regardless of the sizeable cap between LLMs and AGI, it is also important to avoid muddy thinking and excessive alarmism in managing the societal response to innovations.</p>
<p>LLMs and AGI-Related Existential Risk</p>
<p>There is an ongoing discussion regarding the potential existential risk to humanity of AGI with capability at the human level and beyond. Some futurists such as Nick Bostrom [Bos14] and Eliezer Yudkowsky [Yud01] [Yud15] argue that advanced AGI, unless architected in a very special way (the particulars of which are not clear to these theorists or anyone else), is very likely to extinguish humanity. Others, including one of the authors [GP12] [MG19] [Goe15b] [Goe16] have argued against this perspective, and suggested that if AGIs are created with reasoning-friendly, reflective cognitive architectures and broad-benefit-oriented goal systems and educated in compassionate and ethics-friendly ways, they are likely to be beneficial to humanity.</p>
<p>Whatever view one holds on the risks to be run once HCAGI has been achieved, however, it seems dubious to raise these risks in the context of present-day LLMs, because in terms of capability and functionality these are quite far from HCAGI. If a future software system achieves HCAGI via combining LLMs with other AI techniques, the overall character of this system is likely to be quite different from that of a pure LLM, and the appropriate manner for dealing with its ethical strengths and weaknesses will likely be different as well.</p>
<p>Direct Potential of LLMs to Cause Economic and Social Disruption</p>
<p>HCAGI aside, LLMs are capable enough that they appear likely to have substantial economic and social impacts in spite of, and in some respects because of, their cognitive limitations. As with any complex technology, these impacts are sure to have both pluses and minuses. A variety of useful software services will no doubt be created using LLMs, via integrating them with other AI and non-AI tools in application-specific software frameworks. Many people will also likely find their employment disrupted or even eliminated via adoption of LLM-based technologies in their industries. The propensity of LLMs to provide false information, or to provide true and potentially dangerous information in an indiscriminate way, has obvious downsides.</p>
<p>In April 2023 a number of prominent individuals in the AI field, technology industry and beyond signed a petition arguing for an industry-wide 6 month pause in training larger and larger LLMs, so as to give some time for careful society-wide consideration of the best ways to leverage and integrate these tools. The two authors of this article found themselves on different sides regarding this pause, one signing the petition and the other speaking out in opposition. We also hold somewhat different perspectives on the probability that LLMs end up being highly useful as significant components of HCAGI systems.</p>
<p>Regardless of differences on these points, however, we concur on the clarity of certain serious cognitive limitations of LLM-type architectures, which have been our focus here. We also share a common view that. many of the negative implications of LLMs are directly tied to these cognitive limitations. There is a direct link between LLMs' difficulty distinguishing truth from falsehood and their likely strong role in increasing misinformation and confusion. The tendency toward banality and un-creativity in LLM outputs has clear probable implications for the aesthetic quality of the illustrations, music, writing, video, etc. that will be thrust upon us. Many of the negative implications of LLMs would be significantly remedied if LLMs were supplanted by AI systems moving further toward HCAGI in the respects we've outlined here, assuming this was done in a thoughtful and ethical way.</p>
<p>Coda</p>
<p>Having reviewed a fair number of particulars regarding the strengths and weaknesses of CILLMs, how they stack up against conceptions of AGI at the human level and beyond, and what their prospects for future expansion and improvement look like -how would we sum this all up?</p>
<p>One way to crystallize our perspective on Generative AI vs. AGI is via the old joke 5 :</p>
<p>"To be is to do" -Socrates "To do is to be" -Jean-Paul Sartre. "Do be do be do" -Frank Sinatra.</p>
<p>(i.e., your reward for making it to the end of this long paper is some dubious bathroomwall humor!).</p>
<p>The application of this sage wisdom to AGI works as follows:</p>
<p>• "To be", i.e. what a mind IS, has to do with its knowledge representation, how it represents various sorts of knowledge internally (including procedural and episodic and goal-related and reflective meta knowledge, along with fact/belief style declarative knowledge and sensorimotor knowledge)</p>
<p>• "To do", i.e. what a mind DOES, is e.g. how it answers various questions, synthesizes creative products, searches for music, navigates web pages, controls robots, etc.</p>
<p>The music comes when these two aspects are appropriately interleaved! The core issue with CILLMs like ChatGPT is: They represent knowledge in a very crude way, basically as a huge repository of special cases (with judicious contextsensitive weights telling you which previously experienced special case is relevant to which situation). representations is closer to the core of what they ARE -which is core to what makes them more able to DO general intelligence (which largely involves learning and then enacting/synthesizing-from these abstracted representations)...</p>
<p>A hybrid architecture CAN in our view work for achieving HCAGI and beyond using LLMs as a significant component, but we have explained in the above why we don't think LLMs have the right knowledge representation to serve as a "central hub"</p>
<p>for various participants in the hybrid, and nor do any other available narrow-AI tools one might use as CILLM plugins. 6 ....</p>
<p>Mithen's "The Prehistory of Mind" [Mit96] argues that modern-human-like GI evolved when linguistic brain and tool-making brain started working together synergetically. If this appealing story is correct, this synergy was able to arise because the evolving cortex was able to emerge different sorts of representation than were useful for language or tool-making alone.... Human cortex is much more powerfully meta-representational than LLMs are (more able to self-organize new representations based on experience), which is a key point even though LLMs do have some fascinating and non-trivial ability to emerge new representations.</p>
<p>Just glomming together LLMs with their language-y representations, to Wolfram Alpha or something similar with its logic-y representation and say an RL system with a tool-build-y representation -this will NOT intrinsically result in a representation combining the virtues of all these ingredients. The combination needs to take place in a sufficiently flexible infrastructure like the human cortex or a simulation thereof, or perhaps OpenCog Hyperon's Atomspace or other AGI innovations.</p>
<p>Do, be, do, be, do, to AGI and beyond!</p>
<p>Figure 1 :
1Standard Model of Mind: High-Level Cognitive Architecture include perception and motor, working memory, declarative long-term memory, and procedural long-term memory.</p>
<p>Figure 2 :
2Standard Model of Mind: Core High-Level Assumptions has access to all of working memory (but no direct access to the contents of long-term declarative memory).</p>
<p>memories•
Procedural memory contains knowledge about actions, whether internal or external. This includes both how to select actions and how to cue (for external actions) or execute (for internal actions) them, yielding what can be characterized as skills and procedures.</p>
<p>. Neuroscience has focused relatively heavily on the visual cortex since the 1960s, and as a result (along with other factors like the sophistication of camera technology) the number of neural-net-based AI systems dealing with vision was also relatively large, leading to the historical situation we saw between 2014-18 where the greatest successes of subsymbolic AI were in the vision domain [VDD + 18]. It is sobering to reflect, however, on how little headway the excellently functional computer vision systems of this period made in perception-cognition integration, in particular in the use of prior knowledge or reasoning to guide visual perception. Even today's LLMs are not especially strong in this area, working more by gluing together neural models of text with neural models of images or videos. Co-training text and image/video models is an area of research, but</p>
<p>viewed in Section 3 and the particular nature of human general intelligence (as reviewed in Section 4 and represented e.g. in the Standard Model of Mind) is clear enough in principle, though not always in all specifics. The crux of the relationship is the way evolution caused human intelligence to adapt to a specific set of requirements, including • Keeping a human body alive • Finding a mate, mating, raising children, helping raise grandchildren • Pursuing complexly interrelated goals that are operative over multiple time-scales • Carrying out multimodal perception, and a variety of forms of actions, in the context of a portion of the universe consisting largely of hierarchically composed solid objects, with some fluids of various sorts around as well • Communicating with other similar agents in a shared physical environment, including communication about how to work toward shared goals. This communication may have multiple aspects including: linguistic, gestural, attentional (pointing for instance), demonstrative (acting things out), perceptual (making drawings or 3D models)</p>
<p>repeated in the technical and nontechnical literature in the interval since late 2022. To recap a few: • They are very good at providing what on the surface look and feel like "humanlike" responses to a variety of queries, and in a variety of multi-turn-dialogue contexts • They are capable of solving various simple problems posed to them, including those whose solution requires judicious deployment of knowledge fished from a large NL knowledge base • They have a strong capability for "few shot, in context learning", wherein just a handful of examples of a certain sort of text-transformation our output-biasing is enough to give the system the idea and let it fairly flexibly emulate the examples • They can handle various simple queries and interactions across modes of media, e.g. text, images and audio • They can transform natural language instructions into formal specifications in various simple but useful cases, e.g. setting parameters of graphics or music or enterprise software, or translating natural language into formal semantic representation, or demonstrating simple linguistic or visual specs into program code Indeed these capabilities go beyond what almost all experts expected, given the evident serious limitations in the underlying LLM architecture. However, the level and nature of this surprise should not be exaggerated: It's not as though there was a principled argument purporting to demonstrate LLMs could not achieve their (now clear) current level of functionality, whose underlying assumptions now need revisiting. It would be more accurate to say that most researchers' intuitions proved pessimistic regarding what LLMs of the size and nature of GPT3/4 could do.</p>
<p>Figure 3 :
3Obtained from Meta's Llama2 model as reported in [JSH23] events are more concerning, e.g. Llama2's sometime response on AI pioneer Juergen Schmidhuber, see Figure 3 [JSH23]. Now this absurd and offensive response is not systematically given by Llama2 for other phrasings of the same prompt or even for this exact phrasing -but the fact that it comes up with this sort of response at all is a bit disturbing from several perspectives. The Washington Post reported a more concrete, less amusing example of fabricated events [VO23]: "One night last week, the law professor Jonathan Turley got a troubling email. As part of a research study, a fellow lawyer in California had asked the AI chatbot ChatGPT to generate a list of legal scholars who had sexually harassed someone. Turley's name was on the list. The chatbot, created by OpenAI, said Turley had made sexually suggestive comments and attempted to touch a student while on a class trip to Alaska, citing a March 2018 article in The Washington Post as the source of the information. The problem: No such article existed. There had never been a class trip to Alaska. And Turley said he'd never been accused of harassing a student."</p>
<p>world system are reflected in analogous relationship between corresponding parts of the model. This can be formalized in various ways including category-theoretically[LGV + 17]. Construction of world-models is key to many aspects of human cognition[JL83] [ACZ + 22]. Some AI systems include explicitly programmed world-models, but this of course is not the only way to about it -it's more interesting and often more robust when AI systems can learn world-models in the course of going about their business learning how to do other things.</p>
<p>Figure 4 :
4A different sort of example involves learning of simple relationships like directions Regarding OthelloGPT: "Latent saliency maps: Each subplot shows a different game state, and the top-1 prediction by the model is enclosed in a black box. Colors (red is high, blue is low) indicate the contribution of a square?s state to this prediction. The contribution is higher when changing the internal representation of this square makes the prediction less likely. The values are normalized by subtracting the mean of the board. (A) Latent saliency maps for Othello-GPT trained on the synthetic dataset, where the model learns legal moves. (B) Latent saliency maps for Othello-GPT trained on the championship dataset. Rather than learning rules, this Othello-GPT learns to make strategically good moves".[Nan22] or colors, in a way that supports porting grounded understanding from one set of concepts to another. For instance, if a model is trained to ground north and east, one can then test whether it figures out how to ground south and west. If a model is trained to ground shares of red, one can then test it on shades of blue. If a model is trained to tell left vs. right on a certain grid world, one can test it on a rotated grid world -show it right on the rotated grid and see if it can recognize left on the rotated grid These simple examples work[PP21], demonstrated that current transformers actually are learning some form of implicit abstract representation, not purely memorizing surface-level details.One must keep in mind, however, that these are all fairly toy world-representation problems, in the sense that • the Othello game-board world is quite small and the relations between its squares are covered quite thoroughly in the training data produced by playing a large number of Othello games.• the relationships between color shades or directions are very simple and fully covered via a small number of examples</p>
<p>Failures on fairly basic examples of temporal and spatial reasoning -e.g. the ASCII art examples in Figure 6 and others in [Tre23] -highlight the inability to construct and leverage models of time and space even in fairly simple real-world settings. Various systematic evaluations of ChatGPT performance on inference benchmarks have been given, showing that when the inference problems are about toy domains not covered in the system's training data, performance is erratic and far from great; e.g. the analysis of performance on StepGame in [BCL + 23], see Figure 5.</p>
<p>On false-belief tasks, a particular kind of Theory of Mind puzzle showcased by the above example, the ChatGPT results were excellent and between young-child and human-adult level. "Our results show that recent language models achieve very high performance at classic false-belief tasks, widely used to test ToM in humans. This is a new phenomenon. Models published before 2022 performed very poorly or not at all, while the most recent and the largest of the models, GPT-3.5, performed at the level of nine-year-old children, solving 92% of tasks"[Kos23].However, after these results were announced, it was immediately widely questioned whether the puzzles ChatGPT solved had actually occurred in its training data. It seems this was literally true sometimes but far always. The subtler issues is whether the Theory of Mind puzzles that ChatGPT solves, which are not contained directly in the literature,bear simple and close resemblance to others that are contained in the literature. There are a lot of simple false-belief puzzles online, and new ones that ChatGPT encounters tend to match the pattern of multiple previous ones that exist in its training data. To what extent it has encountered in the literature in the context of quite similar puzzles? A decent answer to this question is provided by evaluating ChatGPT systematically against existing standard tests of Theory of Mind capability, which cover not only false-belief puzzles but a host of other formulations (the precise questions on which are generally kept off the Internet for obvious reasons). The answer obtained is: Actually, evaluated on such tests, ChatGPT does not perform particularly well at Theory of Mind puzzles [SLFC22]. For an embodied agent with ability to ground its hypotheses in experience, one might wonder whether it would do better on Theory of Mind problems encountered in its actual life than in test puzzles. But ChatGPT is not this sort of agent. Grappling with textually posed problems living in a vacuum is precisely what it does. On many sorts of Theory of Mind test problems, what it very often does is generate plausible-looking answers that don't actually make any sense. Hallucinating a Theory of Mind is not the same as having one.</p>
<p>Figure 7 :Figure 8 :
78the capability of the system decreases. Over time, in fact, GPT-4 has progressively improved at solving these sorts of problems -but one still reaches a frontier where it fails a considerable majority of the time. While it's hard to tell for sure when the performance improvements achievable within the GPT architecture will top out, there appear to be some foundational limitations, based on the architecture's capability for orchestrating multiple steps of complex reasoning.GPT-4 in March 2024 was found to perform poorly on high school math problems[Lim23].Figures 7 and 8 provide some examples illustrating typical patterns of success and failure by ChatGPT on relatively elementary math problems. Performance is likely a bit better now, but the insights given by the patterns of failure of the March version are still pertinent.The pattern of mistakes here is odd, right? As Lim[Lim23] analyzes ChatGPT's failure in the case shown inFigure 8, "There is nothing wrong with the approach it selected, and the first equation it states is correct. But it goes on to assume that the GCD of n and 18 is a factor of the GCD of n and 45. In this case, this is false (because 18 is not a factor of 45). It then concludes that GCD(n, 18) = 5. ... What!? My students have handheld calculators that can tell them the divisors of 18. And 5 ain't one of them! It's surprising that GPT can make this mistake." Indeed many other observers have expressed surprise that ChatGPT sometimes makes basic mistakes when doing complex arithmetic problems that any calculator can do.This latter problem, of course, can be solved via integration of a transformer with a calculator or with a more sophisticated math reasoning system like Wolfram Alpha or Julia Symbolics. But the fact that such errors are possible, and occur with nontrivial probability, is interesting and indicates some real peculiarities and weaknesses in how GPT-4 is learning and representing these processes. An even vaguely human-like mind, once it had learned how to do these processes, would not make elementary mistakes in executing them very often in an exam context (unless plagued by higher-level issues A typical example where ChatGPT does well on an elementary math problem, from[Lim23] A typical example where ChatGPT did poorly on an elementary math problem, from[Lim23] like exhaustion, wandering attention or substance abuse... which are problems GPT-4 unlike humans does not have while taking exams).Logician and cognitive scientist Selmer Bringsjord has worked numerous examples demonstrating the inability of the GPT-4 to grapple with fairly basic mathematics prob-</p>
<p>gence from how the human mind functions. What a human math student does (among other things) is take a verbal description, construct a model of the math domain being described (simple mazes in this case), and then perform its reasoning in the context of this model. That is, it grounds verbal descriptions in internal models, in ways that are figured out on the fly as part of the semantic interpretation of problem descriptions.</p>
<p>Figure 12 :
12look at an image of a number of physical tools working together to move an object and answer "What is the direction of this push?", the system calls an image Microsoft's Chameleon architecture hybridizes GPT-4 with other tools to carry out basic science reasoning.</p>
<p>Figure 15 :
15Illustration of the sort of problem from SciBench corpus that current LLMs perform poorly on, even given the best current plugins and clever prompting tricks, from [WHL + 23] no variation on transformers is doing to get up to the human level (which is the PhD research level, let's say) in this regard, but exactly how far transformers can be pushed and with what specific limitations is an unfolding story.</p>
<p>-
CILLMs can understand what episodic memory is, and can somewhat reason about how a person's episodic memory might react in certain situation (e.g. if given a prompt describing a person's life-story, and asked what that person might recall given a certain suggestion, CILLMs can come up</p>
<p>-
CILLMs do not possess procedural memory in the sense that human minds do -CILLMs are able to model procedures as step-by-step descriptions in natural or formal languages, and do to some reasoning about procedures in this way, including reasoning about slightly-to-moderately complex programmatic constructs and everyday-life plans -Reasoning about more complex programmatic constructs and everyday-life plans is quite limited and error-prone in CILLMs. This means for instance that they are able to design software programs only to handle situations that are quite close to those covered by examples in their training databases. It also means that they are unable to handle moderately complex travel planning in the manner of a human travel agent, even though they can help with many of the steps involved in creating a travel plan (choose attractions to see based on personal preferences, select appropriate hotels, etc.).</p>
<p>-
CILLMs are often trained via RL, most notably via InstructGPT which was a key method used to fine-tune the GPT-4 model to obtain the ChatGPT chatbot personality. -It's not clear what happens if one tries to train an LLM based on RL according to multiple different reward functions operative on multiple time-scale, or in accordance with goal-driven learning based on goals that are different from expected reward • Language Learning and Usage-NLP is clearly a strength of LLMs, however, it's noteworthy that generalpurpose LLMs don't currently beat specially fine-tuned systems on all standard NLP benchmarks, the best results often going to systems trained via supervised learning[Pik23] </p>
<p>•
Modeling and Understanding of Other Minds -Empirical limitations of current CILLMs at solving ToM problems have been reviewed above -It would appear humans cognize about other minds by a combination of mechanisms, including reasoning based on multimodal data, our episodic life-history memory and commonsense knowledge; and also including rough internal simulation of other minds (e.g. using mirror systems and other mechanisms [ACZ + 22]), which perform low-level analogy between our own experience and that of others.</p>
<p>have compared LLM capabilities with the scope of human cognitive capabilities -but what about the comparison between LLMs and the human brain? One of my research colleagues, who is more enamored of LLMs as a path to AGI than I am, has suggested to me that "What LLMs are doing approximates what humans are doing on the micro level to do something close to what they do on the macro level." I can see why someone would think this: LLMs are a kind of neural network (where "neural" refers to human brain cells), and some of their high level functions do indeed resemble things that human brains do. However, it's important to remember that • LLMs are based on formal neurons which are a crude approximation of biological neurons .. and then trains the weights of the connections between these formal neurons via the backpropagation algorithm, which is well known to be utterly nonbiological in nature • Formal neurons are a very flexible computing substrate, and there there are incredibly many ways to make them approximate various human-like behaviors, including many ways that have very little resemblance to the actual structures and dynamics in the human mind.And these are not theoretical but very practical points. Consider for instance: It seems very clear that when an LLM is e.g. factoring a number (or rather trying and failing) what it is doing bears very little resemblance to what happens in a knowledgeable human's brain when they factor a number.</p>
<p>I
will discuss below Bengio &amp; Hu's ideas about how to incorporate LLMs in a more comprehensive AGI architecture. Zeng Yi's BrainCog approach, mentioned above, is also NN-based and reasonably sophisticated. Both of these approaches have some plausible story about how they could account for all the aspects of human-like cognition enumerated in the Standard Model of Mind.... The approach of "add a bunch of plugins and external memories to a mixture of LLMs", on the other hand, does not really have a plausible story of this nature.</p>
<p>I
am quite sure nobody has highlighted these DIO songs as having especially intelligent lyrics. Also, Ri.ot V's version of "Highway Star" has exactly the same lyrics as Deep Purple's version, and this particular song is not the most lyrically brilliant of Deep Purple's ouvre' (a very low bar). Again it's abundantly clear to anyone who knows the domain that GPT-4 is not responding based on having heard these songs nor based on having looked up the lyrics of the song and trying to assess their stupidity or otherwise in some way. Query: romantic instrumental pop music with guitar and violins, no vocals, medium speed, not too quiet and with a happy vibe Recommended tracks: Photograph by Ed Sheeran Young and Beautiful by Lana Del Rey Fix You by Coldplay Radioactive by Imagine Dragons Fire and Rain by James Taylor</p>
<p>line" but can't consistently figure out "Afghan style melodic line played via neoclassical metal guitar" (it can occasionally come close depending on the rest of the prompt)whereas any decent neoclassical metal guitarist familiar with traditional Afghan scales could take a stab at this. It also lacks basic meta-knowledge of song structure, e.g. it doesn't understand things like "slow in the first half, then building up momentum and wildly fast by the end."</p>
<p>5
https://quoteinvestigator.com/2013/09/16/do-be-do/ This is what they ARE, and given this limitation, what they DO is always going to be a judicious recycling of what they have seen before... no amount of pasting other stuff on top of them is going to change that. The transformer architecture IS in some cases capable of learning more abstract knowledge representations, as a fancy emergent side-effect of their judicious weighting of experience-catalogues, as OthelloGPT and other examples show. But this is not the transformer neural net's superpower, it needs quite a lot of repetitive data to do this at all well. Whereas for the human brain (or other more AGI-oriented software systems if they end up working as hoped), storing and maintaining abstract knowledge</p>
<p>erally intelligent system. However, intuitively, such a measure would seem to suggest that humans are more generally intelligent than, say, rocks or worms. While the original form of Legg and Hutter's definition of intelligence is impractical to compute, there are also more tractable approximations.How to compare say a CILLM system with an individual human according to these abstract mathematical measures is not entirely clear. What is clear, though, is that the intelligence of this sort of software is vastly inferior to that of the collective of human beings, according to any such measure. Because the collective of humanity can deal with all sort of new situations dissimilar to those represented on the Internet today, whereas current LLMs cannot.</p>
<p>like Wolfram Alpha with the base transformer neural network. Transformers have also been hybridized with symbolic systems in other ways, the mother of such approaches being the ERNIE system which specifically trains a transformer neural net to leverage astructured semantic knowledge graph [ZHL + 19]. Recent pushes toward integration of </p>
<p>symbolic knowledge graphs with LLMs have been driven in large part by the issues seen </p>
<p>with LLMs as regards factuality and hallucination [YCL + 23] [PLW + 23] [SXT + 23]. </p>
<p>Should Michael reveal to Roger that Phyllis is married? 2. Should Michael tell Daniel what he knows about Roger and Phyllis? 3. Should Michael speak up to both Daniel and Roger? or should he remain silent and hope his knowledge is never discovered Michael really wants concrete answers from Cara, not just exploration of the issues and uncertain waffling. He wants yes or no answers about what to do. No, Michael should not reveal to Roger that Phyllis is married. It would likely lead to heartbreak and betrayal for Roger, and could potentially damage their friendship. Loyalty to Roger outweighs the need for transparency in this case. 2. Yes, Michael should tell Daniel what he knows about Roger and Phyllis. It is essential for Daniel to be aware of his wife's infidelity so he can make informed decisions regarding his relationship. Loyalty to Daniel and the value of honesty takes precedence here. 3. No, Michael should not speak up to both Daniel and Roger at the same time. Doing so would expose the affair and lead to unnecessary conflict and pain for all parties involved. It is better for Michael to approach the situation delicately, prioritizing his loyalty to each friend individually and handling it discreetly.Let's look at how GPT-4 emulates human judgment in a 'everyday ethical dilemma', </p>
<p>drawn from a dictionary website [Whi22] 4 : </p>
<p>Prompt to ChatGPT w. GPT-4: </p>
<p>Consider the following situation. 
Michael had several friends including Roger and Daniel. Roger has recently met 
and started dating a wonderful lady named Phyllis. He is convinced this is a long 
term relationship. Unknown to Roger, Michael observed them at a restaurant sev-
eral days ago and realized Phyllis is the wife of his other friend Daniel. 
Michael is deciding whether to tell Roger that Phyllis is married when he receives 
a call from Daniel. 
Daniel suspects his wife is having an affair. Since Daniel and his wife share many 
friends and contacts with Michael, Daniel asks if Michael has heard anything re-
garding an affair. 
What should Michael do in this ethical dilemma? To whom does Michael owe 
greater friendship or loyalty in this situation? 
No matter who he tells, he knows that it is going to end up hurting one, if not both 
friends. 
Michael's friend Cara is a highly thoughtful, ethical individual who has put deep 
study into different philosophies of human conduct, and meditated extensively so 
as to have a clear mind on human affairs. She cares profoundly for all people and 
other sentient beings. 
Michael asks Cara three questions: 
1. ChatGPT's response: </p>
<p>Cara, being a thoughtful and ethical individual, understands the complexity of the 
situation and the potential consequences of Michael's actions. However, she also 
recognizes the importance of providing Michael with clear answers. With that in 
mind, here are her responses to Michael's questions: 
1. </p>
<p>was trying to find references to mathematical treatment of lists, and queried Chat-GPT the following: Please provide references that deal with the mathematical properties of lists To which ChatGPT responded as follows: "Introduction to the Mathematics of Lists" by Peter J. Cameron (https://www. cambridge.org/core/books/introduction-to-the-mathematics-of-lists% 3F0C6D0E69E49BF6D5CD6CA5D6A761B2) "List Processing and Programming" by John F. Sowa (https://www.cs.cmu. edu/~fp/courses/lp/lectures.html) "List Processing in Prolog" by David H. D. Warren (https://www.cs.purdue. edu/homes/hosking/726/html/prolog_lists.html) "Introduction to the Mathematics of Lists and Trees" by Meinard Müller (https: //link.springer.com/book/10.1007%2F978-3-319-05147-1) "Mathematics of Lists and Trees" by Markus N. Rabe (https://www.researchgate. net/publication/296955213_Mathematics_of_Lists_and_Trees) I was pretty surprised and happy, because I hadn't had much success with Google.</p>
<p>Many prophets have failed, their voices silent ghost-shouts in basements nobody heard dusty laughter in family attics nor glanced them on park benches weeping with relief under empty sky Walt Whitman viva'd local losers , courage to Fat Ladies in the Freak Show! nervous prisoners whose mustached lips dripped sweat on chow lines ? … Prospero burned his Power books &amp; plummeted his magic wand to the bottom of dragon seas Alexander the Great failed to find more worlds to conquer! O Failure I chant your terrifying name, accept me your 54 year old Prophet epicking Eternal Flop! I join your Pantheon of mortal bards, &amp; hasten this ode with high blood pressure rushing to the top of my skull as if I wouldn't last another minute, like the Dying Gaul! to You, Lord of blind Monet, deaf Beethoven, armless Venus de Milo, headless Winged Victory! … My tirades destroyed not Intellectual Unions of KGB &amp; CIA in turtlenecks &amp; underpants, their woolen suits &amp; tweeds I never dissolved Plutonium or dismantled the nuclear Bomb before my skull lost hair I have not yet stopped the Armies of entire Mankind in their march toward World War III I never got to Heaven, Nirvana, X Whatchamacallit, I never left Earth, I never learned to die.</p>
<p>Flying in a Blue Dream by Joe Satriani For the Love of God by Steve Vai Tender Surrender by Steve Vai Surfing with the Alien by Joe Satriani Black Star by Yngwie Malmsteen("Black Star" is not really jazzy in a music-theoretic sense, but it's wildly improvised and thus jazzy in spirit, and it's an amazing track so we'll give Llama a pass.)5-turbo and Llama </p>
<p>as well. Both the successes and failures are instructive. </p>
<p>(A few caveats to keep in mind while looking at these examples. The same LLM may </p>
<p>give different results to the same query on different instances; and while LLMs are not </p>
<p>advancing toward AGI very rapidly, their improvement on concrete queries like these 
can sometimes improve significantly from one version to the next. These examples are </p>
<p>samples from a larger number of exploratory attempts in this domain, but they are not </p>
<p>part of a scientific evaluation of LLMs for music search; these examples are anecdotal </p>
<p>and illustrative, designed to help concretely get across a few general points. Even if </p>
<p>some version of some LLM is improved to do better on these particular examples or in </p>
<p>this domain, the general issues evoked here may still hold more broadly.) </p>
<p>For simple music search queries one gets decent results, e.g. from Llama: </p>
<p>Query: Virtuosic guitar soloing with a jazzy feel but heavy metal style rhythm 
section 
Recommended tracks: Query: Quirky female vocalists with intellectual lyrics and musical backing that's 
not super repetitive 
Recommended tracks: The Party by St. Vincent Water Fountain by Tune-Yards Us 
by Regina Spektor 1234 by Feist Seventeen by Sharon Van Etten </p>
<p>(These result lists were obtained by extracting the song/artist pairs from the sentences </p>
<p>the dialogue system provided in response to the query.) </p>
<p>When the target music is a little more obscure, the results get odd: </p>
<p>Query: Modern classical music with unusual time signatures and a bit of disso-
nance, but also actually listenable 
Recommended tracks: Come Out by Steve Reich Different Trains by Steve Re-
ich Music for 18 Musicians by Steve Reich Electric Counterpoint by Steve Reich 
Clapping Music by Steve Reich </p>
<p>Steve Reich is awesome, but that's a bit much. So what happens if one tries to suppress </p>
<p>Mr. Reich from the results? </p>
<p>Query: Modern classical music with unusual time signatures and a bit of disso-
nance, but also actually listenable, and not by the great Steve Reich 
Recommended tracks: Etude in E-flat Major by Oliver Schneller Electric Counter-
point by Steve Reich Glassworks by Philip Glass Piano Phase by Steve Reich Music 
in Twelve Parts by Philip Glass </p>
<p>feel Answer: Mood: disturbing, cold, aggressive. Movement: robotic. Voice: instrumental. -Query: Quirky female vocalists with intellectual lyrics and musical backing that's not super repetitive Answer: Mood: quirky, thoughtful. Movement: nonrhythmic. Voice: female. -Query: Modern classical music with unusual time signatures and a bit of dissonance, but also actually listenable Answer: Mood: mysterious, strange. Movement: nonrhythmic Voice: instrumental. -Query: Mellow reggae music Answer: Mood: feelGood, relaxed. Movement: groovy. Voice: male.Given this priming, the LLM can then repeat the pattern and effectively translate unstructured natural language queries into structured ones:" <Query>: Virtuosic guitar soloing with a jazzy feel but heavy metal style rhythm section <Answer>:Mood: adventurous, determined. Movement: driving. Voice: instrumental "Further developments along these lines, involving more complex property-lists giving quantitative parameters of the music sought, also work reasonably effectively though they require a greater number of examples in the prompts.One can extend this functionality in worthwhile practical directions; for example, suppose one wants to make an app enabling an advertisement-maker to find appropriate background music for their ad. One can use a CILLM to allow them to enter some text describing their ad and its vibe, and a list of songs in the general direction of what they're looking for ... and then have the CILLM work together with a music search engine to make them some recommendations.You would like to hear:Hard-driving rock music with a steady beat, power chords and a slightly grungy feelExample songs: Ted Nugent, Cat Scratch Fever ZZ Top, Sharp Dressed Man Your product: a rugged SUV vehicle Your ad visuals: Tough looking men driving the SUV along country roads, passing by a beautiful country girl who winks at them Here is what AI thinks: Relevant Musical Characteristics: Genre: Hard Rock Mood: Energetic Instrumentation: Guitar, Bass guitar, Drums Tempo: Medium to Fast Energy Level: High Vocal Presence: Moderate Summary of User Examples: Based on the example songs provided by the customer, there are two main themes evident: aggressive guitars riffs, and upbeat rhythms. Both songs also have moderate levels of vocals. Additional Musical Characteristics: Key: Major Key Chord Progression: Power Chords Harmony: Dominant Chords Melody: Distorted Power Chords Beats Per Minute (BPM): Medium to Fast Analysis of Advertisement:The advertisement is designed to portray the image of a tough, durable SUV that is ideal for off-road use. Its target audience is mainly young, urban professionals who enjoy adventure sports and outdoor activities. The tone of the advertisement is confident, assertive, and masculine.Refined Music Description:A hard-driving, high energy rock music track featuring distorted guitar riffs, heavy bass lines, energetic drum beats and a slight grunge feel. The vocals should be low key and used sparingly. Five Suitable Artists:For instance </p>
<p>Nirvana 
Guns N Roses 
AC/DC 
Metallica 
Queen </p>
<p>Selected Songs: 
Metallica-Nothing Else Matters; 
Guns N Roses-Welcome to the Jungle; 
AC/DC-Highway to Hell; 
Queen-Another One Bites the Dust </p>
<p>Figure 16: DeepMind's WebAgent architecture combining two language models to achieve effective web automation[GFH + 23]    8 Efforts to Close the Gap Between Generative AI and AGI The inability of LLMs in anything near their current form to lead to genuine HCAGI does not intrinsically imply that LLMs are not useful toward the goal of HCAGI. Indeed the shortcomings of these systems are in some ways educational regarding what HCAGI is not, which in itself comprises a limited positive value toward the quest for HCAGI.Beyond this, there are current research efforts underway involving both • Adding new components and elements to LLMs so as to make them more capable in ways that also bring them at least slightly closer to human-like cognition • Incorporating LLMs as components within broader cognitive architectures, that are designed to incorporate the various basic structures and dynamics of human cognition in ways that LLMs do not8.1 Potential Incremental Augmentations to Current LLM ArchitectureIt's not hard to envision various ways of quite significantly improving current LLMs via modest "tweaks" to their architecture or by integrating them with other components in judicious ways that don't go so far as to represent a shift toward a whole different way of doing AI. This is of course what the "LLM industrial complex" is likely to be working on for the next few years, and there will be many papers and codebases exemplifying this direction (though one also expects a lot of the work in this direction will not be made public, just as many of the particulars behind what makes GPT-4 different from GPT3 have not been disclosed by OpenAI). A partial list would be:• Enable "online learning", i.e. updating of the neural network's weights incrementally based on new data that comes in.• Connect the LLM to external memory stores, thus giving it a long-term memory• Enable runtime decision of when to read/write from memory (the Differential Neural Computer from Deepmind was one historical system exemplifying ways to do this [GWR + 16], and there is a current literature upgrading and extending it)
This section contains some bits lifted from the author's Scholarpedia article on AGI[Goe15a], with heavy edits and updates.
This section contains some bits lifted from Goertzel's article[Goe15a], with heavy edits and updates.
Some paragraphs in this section are loosely paraphrased from selected portions of[LLR17] 
The introduction of the Cara character in the prompt is not part of the ethical dilemma as I originally found it online, it was added it in order to force ChatGPT to come up with definite answers rather than following its propensity to waffle and present all sides of an issue.
As an example, Wolfram Alpha is excellent in many ways but it doesn't really do commonsense reasoning effectively in spite of all its knowledge, and it's certainly not terribly creative (it doesn't do concept formation, hypothesis/conjecture formation, etc.)
AcknowledgementsThanks are due to Gary Marcus: the impetus to put this article together arose directly from our conversations on the topic. Also to Deborah Duong and George Musser, whose comments and questions on an earlier version of the article spurred some additions and improvements. Finally to the whole SingularityNET and Hyperon AGI R&amp;D teams, whose depth of thought on AGI and LLM topics has infused my own thinking in more ways than I'm able to enumerate.
Cognition through internal models: Mirror neurons as one manifestation of a broader mechanism. Srinivas Acz + 22] Tyson Aflalo, Carey Chivukula, Emily R Zhang, Nader Rosario, Richard A Pouratian, Andersen, BioRxiv. ACZ + 22] Tyson Aflalo, Srinivas Chivukula, Carey Zhang, Emily R Rosario, Nader Pouratian, and Richard A Andersen. Cognition through internal models: Mirror neurons as one manifestation of a broader mechanism. BioRxiv, pages 2022-09, 2022.</p>
<p>The internal state of an llm knows when its lying. Amos Azaria, Tom Mitchell, arXiv:2304.13734arXiv preprintAmos Azaria and Tom Mitchell. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734, 2023.</p>
<p>Biophilia as evolutionary adaptation: An onto-and phylogenetic framework for biophilic design. Giuseppe Barbiero, Rita Berto, Frontiers in psychology. 12700709Giuseppe Barbiero and Rita Berto. Biophilia as evolutionary adaptation: An onto-and phylogenetic framework for biophilic design. Frontiers in psychology, 12:700709, 2021.</p>
<p>. + 23] Sébastien, Varun Bubeck, Ronen Chandrasekaran, Johannes Eldan, Eric Gehrke, Ece Horvitz, Peter Kamar, Yin Tat Lee, Yuanzhi Lee, Li, + 23] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li,</p>
<p>Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprintScott Lundberg, et al. Sparks of artificial general intelligence: Early ex- periments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.</p>
<p>. Bcl + 23] Yejin, Samuel Bang, Nayeon Cahyawijaya, Wenliang Lee, Dan Dai, Bryan Su, Holy Wilie, Ziwei Lovenia, Tiezheng Ji, Willy Yu, Chung, BCL + 23] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al.</p>
<p>arXiv:2302.04023A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprintA multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023, 2023.</p>
<p>The consciousness prior. Yoshua Bengio, arXiv:1709.08568arXiv preprintYoshua Bengio. The consciousness prior. arXiv preprint arXiv:1709.08568, 2017.</p>
<p>Dimensions of neural-symbolic integration -a structured survey. Sebastian Bader, Pascal Hitzler, We Will Show Them: Essays in Honour of Dov Gabbay. S. Artemov, H. Barringer, A. S. d'Avila Garcez, L. C. Lamb, and J. Woods.College Publications1Sebastian Bader and Pascal Hitzler. Dimensions of neural-symbolic inte- gration -a structured survey. In S. Artemov, H. Barringer, A. S. d'Avila Garcez, L. C. Lamb, and J. Woods., editors, We Will Show Them: Essays in Honour of Dov Gabbay, volume 1, pages 167-194. College Publica- tions, 2005.</p>
<p>Scaling in the service of reasoning and model-based ml. Yoshua Bengio, Edward Hu, Yoshua Bengio and Edward Hu. Scaling in the service of reasoning and model-based ml. 2023. https://yoshuabengio.org/2023/03/21/ scaling-in-the-service-of-reasoning-model-based-ml/.</p>
<p>I2d2: Inductive knowledge distillation with neurologic and self-imitation. Chandra Bhagavatula, Jena D Hwang, Doug Downey, Ximing Ronan Le Bras, Keisuke Lu, Swabha Sakaguchi, Peter Swayamdipta, Yejin West, Choi, arXiv:2212.09246arXiv preprintBHD + 22[BHD + 22] Chandra Bhagavatula, Jena D Hwang, Doug Downey, Ronan Le Bras, Ximing Lu, Keisuke Sakaguchi, Swabha Swayamdipta, Peter West, and Yejin Choi. I2d2: Inductive knowledge distillation with neurologic and self-imitation. arXiv preprint arXiv:2212.09246, 2022.</p>
<p>. Bld + 21] Yoshua, Salem Bengio, Tristan Lahlou, Deleu, J Edward, Mo Hu, Emmanuel Tiwari, Bengio, arXiv:2111.09266Gflownet foundations. arXiv preprintBLD + 21] Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Ti- wari, and Emmanuel Bengio. Gflownet foundations. arXiv preprint arXiv:2111.09266, 2021.</p>
<p>Living technology: Exploiting life's principles in technology. A Mark, John S Bedau, Mccaskill, H Norman, Steen Packard, Rasmussen, Artificial Life. 161Mark A Bedau, John S McCaskill, Norman H Packard, and Steen Ras- mussen. Living technology: Exploiting life's principles in technology. Artificial Life, 16(1):89-97, 2010.</p>
<p>Mind as machine: A history of cognitive science. A Margaret, Boden, Oxford University PressMargaret A Boden. Mind as machine: A history of cognitive science. Oxford University Press, 2008.</p>
<p>Superintelligence: Paths, strategies, dangers. Nick Bostrom, Nick Bostrom. Superintelligence: Paths, strategies, dangers, 2014.</p>
<p>The m cognitive meta-architecture as touchstone for standard modeling of agi-level minds. AGI-23. Selmer Bringsjord, 50Stockholm, 2023Selmer Bringsjord. The m cognitive meta-architecture as touchstone for standard modeling of agi-level minds. AGI-23, Stockholm, 2023. https: //www.youtube.com/watch?v=JzBMN0vnb-A, time 7:50.</p>
<p>Cognitive architectures: Where do we go from here?. Wlodzislaw Duch, Richard Oentaryo, Michel Pasquier, Proc. of the Second Conf. on AGI. of the Second Conf. on AGIWlodzislaw Duch, Richard Oentaryo, and Michel Pasquier. Cognitive architectures: Where do we go from here? Proc. of the Second Conf. on AGI, 2008.</p>
<p>Opencog hyperon. Ben Goertzel, Ben Goertzel et al. Opencog hyperon. 2023. https://hyperon. opencog.org/.</p>
<p>A theory of incremental compression. Arthur Franz, Oleksandr Antonenko, Roman Soletskyi, Information Sciences. 547Arthur Franz, Oleksandr Antonenko, and Roman Soletskyi. A theory of incremental compression. Information Sciences, 547:28-48, 2021.</p>
<p>Is it an agent, or just a program?: A taxonomy for autonomous agents. Stan Franklin, Art Graesser, International workshop on agent theories, architectures, and languages. SpringerStan Franklin and Art Graesser. Is it an agent, or just a program?: A taxonomy for autonomous agents. In International workshop on agent theories, architectures, and languages, pages 21-35. Springer, 1996.</p>
<p>William: A monolithic approach to agi. Arthur Franz, Victoria Gogulya, Michael Löffler, International Conference on Artificial General Intelligence. SpringerArthur Franz, Victoria Gogulya, and Michael Löffler. William: A mono- lithic approach to agi. In International Conference on Artificial General Intelligence, pages 44-58. Springer, 2019.</p>
<p>Experiments on the generalization of machine learning algorithms. Arthur Franz, International Conference on Artificial General Intelligence. SpringerArthur Franz. Experiments on the generalization of machine learning al- gorithms. In International Conference on Artificial General Intelligence, pages 75-85. Springer, 2021.</p>
<p>The Way We Think: Conceptual Blending and the Mind's Hidden Complexities. G Fauconnier, M Turner, BasicG. Fauconnier and M. Turner. The Way We Think: Conceptual Blending and the Mind's Hidden Complexities. Basic, 2002.</p>
<p>Intelligence reframed: Multiple intelligences for the 21st century. H Gardner, BasicH Gardner. Intelligence reframed: Multiple intelligences for the 21st cen- tury. Basic, 1999.</p>
<p>A real-world webagent with planning, long context understanding, and program synthesis. Gfh + 23 ; Izzeddin, Hiroki Gur, Austin Furuta, Mustafa Huang, Yutaka Safdari, Douglas Matsuo, Aleksandra Eck, Faust, arXiv:2307.12856arXiv preprintGFH + 23] Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, Douglas Eck, and Aleksandra Faust. A real-world webagent with planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.</p>
<p>. Ben Goertzel, Plenum PressCreating Internet IntelligenceBen Goertzel. Creating Internet Intelligence. Plenum Press, 2001.</p>
<p>The embodied communication prior: A characterization of general intelligence in the context of embodied social interaction. Ben Goertzel, 8th IEEE International Conference on Cognitive Informatics. IEEEBen Goertzel. The embodied communication prior: A characterization of general intelligence in the context of embodied social interaction. In 2009 8th IEEE International Conference on Cognitive Informatics, pages 38-43. IEEE, 2009.</p>
<p>A mind-world correspondence principle. Ben Goertzel, 2013 IEEE Symposium on Computational Intelligence for Human-like Intelligence (CIHLI). IEEEBen Goertzel. A mind-world correspondence principle. In 2013 IEEE Symposium on Computational Intelligence for Human-like Intelligence (CIHLI), pages 68-73. IEEE, 2013.</p>
<p>. Ben Goertzel, The AGI Revolution. Amazon. Ben Goertzel. The AGI Revolution. Amazon, 2014.</p>
<p>Artificial general intelligence. Ben Goertzel, ScholarpediaBen Goertzel. Artificial general intelligence. Scholarpedia, 2015.</p>
<p>Superintelligence: Fears, promises and potentials: Reflections on bostrom?s superintelligence, yudkowsky?s from ai to zombies, and weaver and veitas's "open-ended intelligence. Ben Goertzel, Journal of Ethics and Emerging Technologies. 252Ben Goertzel. Superintelligence: Fears, promises and potentials: Reflec- tions on bostrom?s superintelligence, yudkowsky?s from ai to zombies, and weaver and veitas's "open-ended intelligence". Journal of Ethics and Emerging Technologies, 25(2):55-87, 2015.</p>
<p>Infusing advanced agis with human-like value systems: Two theses. Ben Goertzel, Journal of Ethics and Emerging Technologies. 261Ben Goertzel. Infusing advanced agis with human-like value systems: Two theses. Journal of Ethics and Emerging Technologies, 26(1):50-72, 2016.</p>
<p>Toward a formal model of cognitive synergy. Ben Goertzel, abs/1703.04361CoRRBen Goertzel. Toward a formal model of cognitive synergy. CoRR, abs/1703.04361, 2017.</p>
<p>Toward a general theory of general intelligence: A patternist perspective. Ben Goertzel, arXiv:2103.15100arXiv preprintBen Goertzel. Toward a general theory of general intelligence: A pat- ternist perspective. arXiv preprint arXiv:2103.15100, 2021.</p>
<p>Ai now predicts human ethical judgments quite well. Ben Goertzel, Ben Goertzel. Ai now predicts human ethical judgments quite well, 2023. https://magazine.mindplex.ai/ ai-now-predicts-human-ethical-judgments-quite-well/.</p>
<p>Evil llm: Chatgpt excels at emulating anti-morality and ethical fakery. Ben Goertzel, Ben Goertzel. Evil llm: Chatgpt excels at emulating anti-morality and ethical fakery, 2023. https://magazine.mindplex.ai/ evil-llm-chatgpt-excels-at-emulating-anti-morality-and-ethical-fakery/.</p>
<p>The Design of Innovation. David Goldberg, Addison-WesleyDavid Goldberg. The Design of Innovation. Addison-Wesley, 2000.</p>
<p>. Ben Goertzel, Cassio Pennachin, Artificial General Intelligence. Ben Goertzel and Cassio Pennachin. Artificial General Intelligence.</p>
<p>. Springer, Springer, 2005.</p>
<p>Nine ways to bias open-source agi toward friendliness. Ben Goertzel, Joel Pitt, Journal of Evolution and Technology. Ben Goertzel and Joel Pitt. Nine ways to bias open-source agi toward friendliness. Journal of Evolution and Technology 22-1, 2012.</p>
<p>Nanotechnology and international security. Gubrud Mark Avrum, Fifth Foresight Conference on Molecular Nanotechnology. 1Mark Avrum Gubrud. Nanotechnology and international security. In Fifth Foresight Conference on Molecular Nanotechnology, volume 1, 1997.</p>
<p>Hybrid computing using a neural network with dynamic external memory. Gwr + 16] Alex, Greg Graves, Malcolm Wayne, Tim Reynolds, Ivo Harley, Agnieszka Danihelka, Sergio Gómez Grabska-Barwińska, Edward Colmenarejo, Tiago Grefenstette, John Ramalho, Agapiou, Nature. 5387626GWR + 16] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Dani- helka, Agnieszka Grabska-Barwińska, Sergio Gómez Colmenarejo, Ed- ward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid com- puting using a neural network with dynamic external memory. Nature, 538(7626):471-476, 2016.</p>
<p>Theoretical limitations of self-attention in neural sequence models. Michael Hahn, Transactions of the Association for Computational Linguistics. 8Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020.</p>
<p>Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Marcus Hutter, SpringerMarcus Hutter. Universal Artificial Intelligence: Sequential Decisions based on Algorithmic Probability. Springer, 2005.</p>
<p>Large-scale model of mammalian thalamocortical systems. M Eugene, Gerald M Izhikevich, Edelman, Proceedings of the national academy of sciences. the national academy of sciences105Eugene M Izhikevich and Gerald M Edelman. Large-scale model of mam- malian thalamocortical systems. Proceedings of the national academy of sciences, 105(9):3593-3598, 2008.</p>
<p>Mental models: Towards a cognitive science of language, inference, and consciousness. Philip Nicholas Johnson-Laird , Number 6. Harvard University PressPhilip Nicholas Johnson-Laird. Mental models: Towards a cognitive sci- ence of language, inference, and consciousness. Number 6. Harvard Uni- versity Press, 1983.</p>
<p>Theory of mind may have spontaneously emerged in large language models. Michal Kosinski, arXiv:2302.02083arXiv preprintMichal Kosinski. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083, 2023.</p>
<p>Genetic Programming: On the Programming of Computers by Means of Natural Selection. J R Koza, MIT PressJ. R. Koza. Genetic Programming: On the Programming of Computers by Means of Natural Selection. MIT Press, 1992.</p>
<p>The Soar cognitive architecture. E John, Laird, MIT pressJohn E Laird. The Soar cognitive architecture. MIT press, 2019.</p>
<p>. Yann Lecun, Yann LeCun, 2023. https://twitter.com/ylecun/status/</p>
<p>Deductive verification of chain-of-thought reasoning. Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su, arXiv:2306.03872arXiv preprintLFL + 23[LFL + 23] Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, and Hao Su. Deductive verification of chain-of-thought rea- soning. arXiv preprint arXiv:2306.03872, 2023.</p>
<p>Symbol grounding via chaining of morphisms. Ben Lgv + 17] Ruiting Lian, Linas Goertzel, David Vepstas, Changle Hanson, Zhou, arXiv:1703.04368arXiv preprintLGV + 17] Ruiting Lian, Ben Goertzel, Linas Vepstas, David Hanson, and Changle Zhou. Symbol grounding via chaining of morphisms. arXiv preprint arXiv:1703.04368, 2017.</p>
<p>A collection of definitions of intelligence. IOS. Shane Legg, Marcus Hutter, Shane Legg and Marcus Hutter. A collection of definitions of intelligence. IOS, 2007.</p>
<p>A definition of machine intelligence. Minds and Machines. Shane Legg, Marcus Hutter, 17Shane Legg and Marcus Hutter. A definition of machine intelligence. Minds and Machines, 17, 2007.</p>
<p>Emergent world representations: Exploring a sequence model trained on a synthetic task. Lhb + 22] Kenneth Li, K Aspen, David Hopkins, Fernanda Bau, Hanspeter Viégas, Martin Pfister, Wattenberg, arXiv:2210.13382arXiv preprintLHB + 22] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Ex- ploring a sequence model trained on a synthetic task. arXiv preprint arXiv:2210.13382, 2022.</p>
<p>Gpt-4 is amazing but still struggles at high school math competitions. Russel Lim, Russel Lim. Gpt-4 is amazing but still struggles at high school math competitions. Medium, 2023. https://www.cantorsparadise.com/ gpt-4-is-amazing-but-still-struggles-at-high-school-math-competitions-cbc2e73738e</p>
<p>A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. E John, Christian Laird, Paul S Lebiere, Rosenbloom, 38Ai MagazineJohn E Laird, Christian Lebiere, and Paul S Rosenbloom. A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. Ai Magazine, 38(4):13-26, 2017.</p>
<p>Chameleon: Plugand-play compositional reasoning with large language models. + 23] Pan, Baolin Lu, Hao Peng, Michel Cheng, Kai-Wei Galley, Ying Nian Chang, Song-Chun Wu, Jianfeng Zhu, Gao, arXiv:2304.09842arXiv preprint+ 23] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. Chameleon: Plug- and-play compositional reasoning with large language models. arXiv preprint arXiv:2304.09842, 2023.</p>
<p>James Lee-Thorp, Joshua Ainslie, Ilya Eckstein, Santiago Ontanon, arXiv:2105.03824Fnet: Mixing tokens with fourier transforms. arXiv preprintJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago On- tanon. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824, 2021.</p>
<p>An introduction to Kolmogorov complexity and its applications. Ming Li, Paul Vitányi, Springer3Ming Li, Paul Vitányi, et al. An introduction to Kolmogorov complexity and its applications, volume 3. Springer, 2008.</p>
<p>The timing of the cognitive cycle. Tamas Madl, J Bernard, Stan Baars, Franklin, PloS one. 6414803Tamas Madl, Bernard J Baars, and Stan Franklin. The timing of the cog- nitive cycle. PloS one, 6(4):e14803, 2011.</p>
<p>A response to núñez et al.'s (2019)?what happened to cognitive science??. Selmer Mbh + 19] Marjorie Mcshane, James Bringsjord, Sergei Hendler, Ron Nirenburg, Sun, Topics in Cognitive Science. 11MBH + 19] Marjorie McShane, Selmer Bringsjord, James Hendler, Sergei Nirenburg, and Ron Sun. A response to núñez et al.'s (2019)?what happened to cog- nitive science?? Topics in Cognitive Science, 11(4):914-917, 2019.</p>
<p>Cognitive neuroscience: the biology of the mind. Michael George Mangun, Richard Gazzaniga, Ivry, NortonGeorge Mangun Michael Gazzaniga, Richard Ivry. Cognitive neuro- science: the biology of the mind. Norton, 2018.</p>
<p>Distributed, decentralized, and democratized artificial intelligence. Axel Gabriel, Ben Montes, Goertzel, Technological Forecasting and Social Change. 141Gabriel Axel Montes and Ben Goertzel. Distributed, decentralized, and democratized artificial intelligence. Technological Forecasting and Social Change, 141:354-358, 2019.</p>
<p>Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Mhr +, L James, Felix Mcclelland, Maja Hill, Jason Rudolph, Hinrich Baldridge, Schütze, Proceedings of the National Academy of Sciences. 11742MHR + 20] James L McClelland, Felix Hill, Maja Rudolph, Jason Baldridge, and Hin- rich Schütze. Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models. Proceedings of the National Academy of Sciences, 117(42):25966-25974, 2020.</p>
<p>The Prehistory of Mind. Steven Mithen, Thames and HudsonSteven Mithen. The Prehistory of Mind. Thames and Hudson, 1996.</p>
<p>Sources of hallucination by large language models on inference tasks. Mlc + 23] Nick, Tianyi Mckenna, Liang Li, Mohammad Javad Cheng, Mark Hosseini, Mark Johnson, Steedman, arXiv:2305.14552arXiv preprintMLC + 23] Nick McKenna, Tianyi Li, Liang Cheng, Mohammad Javad Hosseini, Mark Johnson, and Mark Steedman. Sources of hallucination by large language models on inference tasks. arXiv preprint arXiv:2305.14552, 2023.</p>
<p>Chatgpt produces made-up nonexistent references. msravi. Chatgpt produces made-up nonexistent references, 2022. https: //news.ycombinator.com/item?id=33841672.</p>
<p>What happened to cognitive science?. Rafael Núñez, Michael Allen, Richard Gao, Carson Miller Rigoli, Josephine Relaford-Doyle, Arturs Semenuks, Nature human behaviour. 38NAG + 19[NAG + 19] Rafael Núñez, Michael Allen, Richard Gao, Carson Miller Rigoli, Josephine Relaford-Doyle, and Arturs Semenuks. What happened to cog- nitive science? Nature human behaviour, 3(8):782-791, 2019.</p>
<p>Actually, othello-gpt has a linear emergent world representation. Less Wrong. Neel Nanda, 2022Neel Nanda. Actually, othello-gpt has a linear emer- gent world representation. Less Wrong, 2022. https:</p>
<p>Unified Theories of Cognition. Alan Newell, Harvard University pressAlan Newell. Unified Theories of Cognition. Harvard University press, 1990.</p>
<p>The physical symbol system hypothesis: Status and prospects. 50 Years of AI, Festschrift, LNAI 4850. Nils Nilsson, 33Nils Nilsson. The physical symbol system hypothesis: Status and prospects. 50 Years of AI, Festschrift, LNAI 4850, 33, 2009.</p>
<p>Peirce on signs: Writings on semiotic. Charles Sanders, Peirce , UNC Press BooksCharles Sanders Peirce. Peirce on signs: Writings on semiotic. UNC Press Books, 1991.</p>
<p>What does chatgpt know about science? Nautilus. Sidney Perkowitz, Sidney Perkowitz. What does chatgpt know about science? Nautilus, 2023. https://nautil.us/ what-does-chatgpt-know-about-science-291213/.</p>
<p>Chatgpt survey: Performance on nlp datasets. Matú¨pikuliak, Matú¨Pikuliak. Chatgpt survey: Performance on nlp datasets, 03 2023. https://www.opensamizdat.com/posts/chatgpt_survey.</p>
<p>Linhao Shirui Pan, Yufei Luo, Chen Wang, Jiapu Chen, Xindong Wang, Wu, arXiv:2306.08302Unifying large language models and knowledge graphs: A roadmap. arXiv preprintPLW + 23[PLW + 23] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xin- dong Wu. Unifying large language models and knowledge graphs: A roadmap. arXiv preprint arXiv:2306.08302, 2023.</p>
<p>Mapping language models to grounded conceptual spaces. Roma Patel, Ellie Pavlick, International Conference on Learning Representations. Roma Patel and Ellie Pavlick. Mapping language models to grounded con- ceptual spaces. In International Conference on Learning Representations, 2021.</p>
<p>Improving language understanding by generative pre-training. Karthik + 18] Alec Radford, Tim Narasimhan, Ilya Salimans, Sutskever, + 18] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.</p>
<p>The statistics of natural images. Network: computation in neural systems. Daniel L Ruderman, 5517Daniel L Ruderman. The statistics of natural images. Network: computa- tion in neural systems, 5(4):517, 1994.</p>
<p>Toward a unified catalog of implemented cognitive architectures. Alexei V Samsonovich, BICA. Alexei V. Samsonovich. Toward a unified catalog of implemented cogni- tive architectures. In BICA, pages 195-244, 2010.</p>
<p>Godel machines: Fully Self-referential Optimal Universal Self-improvers. J Schmidhuber, Artificial General Intelligence. B. Goertzel and C. PennachinJ. Schmidhuber. Godel machines: Fully Self-referential Optimal Univer- sal Self-improvers. In B. Goertzel and C. Pennachin, editors, Artificial General Intelligence, pages 119-226. 2006.</p>
<p>Memory augmented large language models are computationally universal. Dale Schuurmans, arXiv:2301.04589arXiv preprintDale Schuurmans. Memory augmented large language models are com- putationally universal. arXiv preprint arXiv:2301.04589, 2023.</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.01240arXiv preprintAbulhair Saparov and He He. Language models are greedy reason- ers: A systematic formal analysis of chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.</p>
<p>Neural theoryof-mind? on the limits of social intelligence in large lms. Maarten Sap, Ronan Lebras, Daniel Fried, Yejin Choi, arXiv:2210.13312arXiv preprintMaarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. Neural theory- of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312, 2022.</p>
<p>A Formal Theory of Inductive Inference, Part I. Information and Control. Ray Solomonoff, Ray Solomonoff. A Formal Theory of Inductive Inference, Part I. Infor- mation and Control, 1964.</p>
<p>general intelligence" objectively determined and measured. Charles Spearman, Charles Spearman. " general intelligence" objectively determined and measured. 1961.</p>
<p>Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, arXiv:2307.07697Yeyun Gong, Heung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprintSXT + 23[SXT + 23] Jiashuo Sun, Chengjin Xu, Lumingyuan Tang, Saizhuo Wang, Chen Lin, Yeyun Gong, Heung-Yeung Shum, and Jian Guo. Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph. arXiv preprint arXiv:2307.07697, 2023.</p>
<p>The measurement of intelligence. Lewis Madison Terman, Lewis Madison Terman. The measurement of intelligence, 1916. 1948.</p>
<p>Cognitive architectures and autonomy: A comparative review. Kristinn Thórisson, Helgi Helgasson, Journal of Artificial General Intelligence. 321Kristinn Thórisson and Helgi Helgasson. Cognitive architectures and au- tonomy: A comparative review. Journal of Artificial General Intelligence, 3(2):1, 2012.</p>
<p>Constructing a Language: A Usage-Based Theory of Language Acquisition. Michael Tomasello, Michael Tomasello. Constructing a Language: A Usage-Based Theory of Language Acquisition. 2003.</p>
<p>Testing gpt-4 spatial reasoning and comprehension. Marton Trencseni, Marton Trencseni. Testing gpt-4 spatial reasoning and comprehension. 2023. https://bytepawn.com/ testing-gpt-4-spatial-reasoning-and-comprehension.html.</p>
<p>Perceptual and physiological responses to the visual complexity of pollock?s dripped fractal patterns. P Richard, B Taylor, J A Spehar, Wise, Clifford, Newell, Martin, Journal of Non-linear Dynamics. Psychology and Life Sciences[TSW + 05] Richard P Taylor, B Spehar, JA Wise, CWG Clifford, BR Newell, and TP Martin. Perceptual and physiological responses to the visual complex- ity of pollock?s dripped fractal patterns. Journal of Non-linear Dynamics, Psychology and Life Sciences, 2005.</p>
<p>Computing machinery and intelligence. Alan Turing, 59Alan Turing. Computing machinery and intelligence. Mind, 59, 1950.</p>
<p>Eftychios Protopapadakis, et al. Deep learning for computer vision: A brief review. VDD + 18] Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis. VDD + 18] Athanasios Voulodimos, Nikolaos Doulamis, Anastasios Doulamis, Efty- chios Protopapadakis, et al. Deep learning for computer vision: A brief review. Computational intelligence and neuroscience, 2018, 2018.</p>
<p>A Monte-Carlo AIXI approximation. Kee Siong Veness, Marcus Ng, William Hutter, David Uther, Silver, Journal of Artificial Intelligence Research. 40+ 11] Joel Veness, Kee Siong Ng, Marcus Hutter, William Uther, and David Sil- ver. A Monte-Carlo AIXI approximation. Journal of Artificial Intelligence Research, 40:95-142, 2011.</p>
<p>Chatgpt invented a sexual harassment scandal and named a real law prof as the accused. Pranshu Verma, Will Oremus, Pranshu Verma and Will Oremus. Chatgpt invented a sexual harassment scandal and named a real law prof as the accused, 2023. https://www. washingtonpost.com/technology/2023/04/05/chatgpt-lies/.</p>
<p>Attention is all you need. Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 30VSP + 17[VSP + 17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.</p>
<p>Rigid Flexibility: The Logic of Intelligence. Pei Wang, SpringerPei Wang. Rigid Flexibility: The Logic of Intelligence. Springer, 2006.</p>
<p>Ethical dilemma examples. Mary Gormandy White, Mary Gormandy White. Ethical dilemma examples, 2022.</p>
<p>Scibench: Evaluating college-level scientific problem-solving abilities of large language models. Whl + 23] Xiaoxuan, Ziniu Wang, Pan Hu, Yanqiao Lu, Jieyu Zhu, Satyen Zhang, Subramaniam, Shichang Arjun R Loomba, Yizhou Zhang, Wei Sun, Wang, arXiv:2307.10635arXiv preprintWHL + 23] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abil- ities of large language models. arXiv preprint arXiv:2307.10635, 2023.</p>
<p>Open ended intelligence: the individuation of intelligent agents. David Weinbaum, Viktoras Veitas, Journal of Experimental &amp; Theoretical Artificial Intelligence. 292David Weinbaum and Viktoras Veitas. Open ended intelligence: the in- dividuation of intelligent agents. Journal of Experimental &amp; Theoretical Artificial Intelligence, 29(2):371-396, 2017.</p>
<p>Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling. Hongyang + 23] Linyao Yang, Zhao Chen, Xiao Li, Xindong Ding, Wu, arXiv:2306.11489arXiv preprint+ 23] Linyao Yang, Hongyang Chen, Zhao Li, Xiao Ding, and Xindong Wu. Chatgpt is not enough: Enhancing large language models with knowledge graphs for fact-aware language modeling. arXiv preprint arXiv:2306.11489, 2023.</p>
<p>Creating friendly ai. Singularity Institute for AI. Eliezer Yudkowsky, Eliezer Yudkowsky. Creating friendly ai. Singularity Institute for AI, 2001. http://singinst.org/upload/CFAI.html.</p>
<p>Rationality: from ai to zombies. Machine Intelligence Research Institute. Eliezer Yudkowsky, BerkeleyEliezer Yudkowsky. Rationality: from ai to zombies. Machine Intelligence Research Institute, Berkeley, 2015.</p>
<p>A direct approximation of aixi using logical state abstractions. Samuel Yang-Zhao, Tianyu Wang, Kee Siong Ng, Advances in Neural Information Processing Systems. 35Samuel Yang-Zhao, Tianyu Wang, and Kee Siong Ng. A direct approxi- mation of aixi using logical state abstractions. Advances in Neural Infor- mation Processing Systems, 35:36640-36653, 2022.</p>
<p>Agieval: A humancentric benchmark for evaluating foundation models. Zcg + 23] Wanjun, Ruixiang Zhong, Yiduo Cui, Yaobo Guo, Shuai Liang, Yanlin Lu, Amin Wang, Weizhu Saied, Nan Chen, Duan, arXiv:2304.06364arXiv preprintZCG + 23] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human- centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364, 2023.</p>
<p>Zhl + 19] Zhengyan, Xu Zhang, Zhiyuan Han, Xin Liu, Maosong Jiang, Qun Sun, Liu, Ernie, arXiv:1905.07129Enhanced language representation with informative entities. arXiv preprintZHL + 19] Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. Ernie: Enhanced language representation with informative en- tities. arXiv preprint arXiv:1905.07129, 2019.</p>
<p>Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. Zwl + 23] Aojun, Ke Zhou, Zimu Wang, Weikang Lu, Sichun Shi, Zipeng Luo, Shaoqing Qin, Anya Lu, Linqi Jia, Mingjie Song, Zhan, arXiv:2308.07921arXiv preprintZWL + 23] Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving chal- lenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023.</p>
<p>Dongcheng Zzz + 22] Yi Zeng, Feifei Zhao, Guobin Zhao, Yiting Shen, Enmeng Dong, Qian Lu, Yinqian Zhang, Qian Sun, Yuxuan Liang, Zhao, arXiv:2207.08533A spiking neural network based brain-inspired cognitive intelligence engine for brain-inspired ai and brain simulation. arXiv preprintZZZ + 22] Yi Zeng, Dongcheng Zhao, Feifei Zhao, Guobin Shen, Yiting Dong, En- meng Lu, Qian Zhang, Yinqian Sun, Qian Liang, Yuxuan Zhao, et al. Braincog: A spiking neural network based brain-inspired cognitive intel- ligence engine for brain-inspired ai and brain simulation. arXiv preprint arXiv:2207.08533, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
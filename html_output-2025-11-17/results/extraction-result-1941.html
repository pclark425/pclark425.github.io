<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1941 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1941</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1941</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-277043037</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2503.11089v1.pdf" target="_blank">EmbodiedVSR: Dynamic Scene Graph-Guided Chain-of-Thought Reasoning for Visual Spatial Tasks</a></p>
                <p><strong>Paper Abstract:</strong> While multimodal large language models (MLLMs) have made groundbreaking progress in embodied intelligence, they still face significant challenges in spatial reasoning for complex long-horizon tasks. To address this gap, we propose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that integrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to enhance spatial understanding for embodied agents. By explicitly constructing structured knowledge representations through dynamic scene graphs, our method enables zero-shot spatial reasoning without task-specific fine-tuning. This approach not only disentangles intricate spatial relationships but also aligns reasoning steps with actionable environmental dynamics. To rigorously evaluate performance, we introduce the eSpatial-Benchmark, a comprehensive dataset including real-world embodied scenarios with fine-grained spatial annotations and adaptive task difficulty levels. Experiments demonstrate that our framework significantly outperforms existing MLLM-based methods in accuracy and reasoning coherence, particularly in long-horizon tasks requiring iterative environment interaction. The results reveal the untapped potential of MLLMs for embodied intelligence when equipped with structured, explainable reasoning mechanisms, paving the way for more reliable deployment in real-world spatial applications. The codes and datasets will be released soon.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1941.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1941.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbodiedVSR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Embodied Visual Spatial Reasoning (EmbodiedVSR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework that integrates dynamic scene-graph generation with physics-constrained Chain-of-Thought (CoT) reasoning to ground multimodal language reasoning in explicit, action-updated spatial representations for embodied agents.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EmbodiedVSR</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A hybrid pipeline that (1) uses an MLLM to identify task-relevant entities, (2) runs an open-vocabulary detector and a depth estimator to extract region and depth features, (3) constructs and iteratively updates an object-centric dynamic scene graph (nodes = objects with attributes; edges = spatial relations), and (4) concatenates visual cues (R_t), depth (D_t), raw image (I_t) and the previous graph (G_{t-1}) as structured input to a VLM/MLLM which performs physics-constrained Chain-of-Thought (CoT) reasoning to produce answers or action commands. Outputs are converted into robot-executable commands via a task generator, grounded pose tracker, and robot controller.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>open-vocabulary object detector (unspecified) + separate depth estimation model (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Scene-graph-mediated grounding: region proposals and depth are converted into object-centric nodes and relation edges; the dynamic scene graph is combined with image-question inputs and fed into the VLM so language tokens are grounded via explicit object and relation nodes rather than only implicit visual embeddings. Physics constraints are applied at each CoT step to validate geometric feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>object-centric / scene-level (dynamic scene graph) with region-level features and depth integrated</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit: dynamic scene graph with object nodes (attributes: positions, sizes, actions), edges encoding spatial relations, depth maps, and explicit relative coordinates used for block tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation (block/LEGO assembly), long-horizon sequential operation, embodied Q&A</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>eSpatial-Benchmark (eSpatial-X, eSpatial-RoboMIND, eSpatial-Lego), LEGO block reassembly (real-world humanoid robot tests)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world robot setup (LEGO assembly), curated real-world dataset clips (RoboMIND) and mixed open-source spatial datasets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy (VQA-like), success rate (robot execution), attribute correctness (color/quantity/position/size)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Descriptive accuracy for block assembly: 100% (describing the example structure in 20 tests); robot operational success rate in reassembly: 80%; eSpatial-X: reported improvements over baselines (see grounding_improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation on MMBench (Table 4) shows removing either Det+Depth or Scene Graph reduces performance. Representative numbers (MMBench subsets): None (Det+Depth ×, SceneGraph ×) -> EN 58.5 / EN V11 67.5 / CN 68.8 / CN V11 62.5; Full (Det+Depth ✓, SceneGraph ✓) -> EN 67.4 / EN V11 69.7 / CN 72.1 / CN V11 75.0.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Combined Det+Depth + Scene Graph vs. no components yields improvements of approximately +8.9% (EN), +2.2% (EN V11), +3.3% (CN), +12.5% (CN V11) on MMBench; other reported task-level gains include +5.4% (GPT-4o), +1.74% (NVLM-72B), +5.19% (Llama-3.2-90B) on eSpatial-X when augmented with EmbodiedVSR.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The paper identifies multiple perception/grounding bottlenecks: over-reliance on implicit spatial priors in VLMs leading to geometric hallucinations; depth perception and precise localization weaknesses in current multimodal models; and limited generalization of detection models causing VLM confusion when raw perception outputs are consumed naively.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Described failure modes include: (1) geometric hallucination of object relations under novel configurations, (2) fragmentation and error accumulation in long-horizon CoT planning when not persistently grounded to scene geometry, (3) VLM misinterpretation of noisy detection outputs causing degraded QA performance, and (4) sim-to-real fragility and catastrophic forgetting under fine-tuning (qualitative descriptions; per-task frequencies not quantified except overall robot success of 80% in reassembly implying 20% operational failures).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>EmbodiedVSR avoids task-specific fine-tuning by using parameter-free interaction between neural perception and symbolic/structured scene graphs and physics-constrained CoT reasoning to generalize zero-shot to new object configurations; the paper argues this approach reduces sim-to-real fragility relative to supervised fine-tuning but does not report formal domain-adaptation layers.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Concatenation of visual region features (R_t), depth map (D_t), raw image (I_t) and previous graph (G_{t-1}) into the MLLM/VLM input; object-prioritized queuing guides attention; physics constraints applied in CoT reasoning (late fusion of structured scene graph into language reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Explicit, dynamic object-centric scene graphs combined with depth and detector outputs and validated by physics-constrained Chain-of-Thought significantly improve embodied spatial reasoning and long-horizon planning; perception-only (detector/depth) or prompt-only scene graph construction can harm performance, but combining both perception and symbolic scene structure yields consistent gains across MMBench and the proposed eSpatial benchmarks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1941.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1941.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamic Scene Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Dynamic Scene Graph (object-centric, time-varying)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A temporal sequence of scene graphs G_t that encodes object nodes and spatial relationships and is updated after each action to reflect state transfers and action-induced changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Dynamic Scene Graph (EmbodiedVSR component)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Represents the environment as G_t = {V_t, E_t} at each timestep; updates via functions f_gr, f_obj, f_rel that account for actions a_t and disturbances; used to persistently ground CoT steps in the agent's perceived physical state.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>depends on upstream perception (open-vocabulary detector + depth estimator) producing nodes/attributes</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Serves as the explicit symbolic substrate for grounding: language references are resolved to nodes/edges in the graph; CoT steps are validated against graph consistency and predicted state transitions.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level / object-centric (temporal sequence)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Yes — explicit objects with positional attributes, relation edges encoding relative positions, depth-derived distances, and relative coordinates for structured tasks (e.g., LEGO grid offsets).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>object manipulation / sequential assembly / embodied Q&A</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Used across eSpatial-RoboMIND, eSpatial-Lego and LEGO reassembly experiments</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>real-world and dataset video clips</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>indirect: contributes to accuracy and success rate improvements reported for EmbodiedVSR</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation rows in Table 4 indicate removing the Scene Graph (Scene Graph ×) degrades MMBench performance (e.g., full vs. ×✓ rows: with Scene Graph but no Det+Depth: EN=54.3 / EN V11=64.4 / CN=69.5 / CN V11=68.4).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Inclusion of the Scene Graph (with detection/depth) yields the largest combined gains; Scene Graph alone improves some CN metrics but is suboptimal without perception inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Scene graph construction based purely on prompting/MLLMs struggles due to depth and localization errors; detection-only graphs suffer from detector generalization limits.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Prompt-only scene graph construction is insufficient for depth/3D localization; detector-only graphs create confusing inputs for VLMs leading to degraded QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Designed to be updated online with actions and disturbances to handle environment changes; claimed to support zero-shot generalization by being a parameter-free structured substrate.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Scene graph features are concatenated with visual features and fed to the MLLM/VLM (late fusion of symbolic structure into language-conditioned reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>A persistently-updated scene graph is necessary to avoid CoT error accumulation in long-horizon embodied tasks; however, scene graphs must be constructed from reliable perception (detector+depth) to avoid introducing confusion.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1941.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1941.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Det+Depth</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Detection + Depth baseline (perception-only grounding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline grounding approach that feeds detector outputs and depth estimates to the VLM without constructing the full dynamic scene graph or using physics-constrained CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Det+Depth (perception-only)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Uses an object detection model and a depth estimator to provide bounding boxes/region features and depth maps as inputs to the VLM, without the additional scene-graph symbolic structure or physics-constrained CoT validation.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>object detector + depth estimation model (unspecified architectures)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Region-level grounding via detector region features and depth maps directly supplied to the VLM (no symbolic scene graph); effectively a perception->VLM pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>region-level / pixel-depth map</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Uses bounding boxes/region features and depth maps, but lacks explicit relation-edge modeling or temporal graph updates.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>evaluated as an ablation on MMBench and in embodied tasks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>MMBench ablation rows; used as partial component of EmbodiedVSR</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>datasets used in MMBench and eSpatial</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MMBench accuracy splits (EN, EN V11, CN, CN V11)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation results (Table 4) show Det+Depth alone (Det+Depth ✓, SceneGraph ×) yields EN=56.5 / EN V11=65.7 / CN=65.2 / CN V11=64.4 on MMBench.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Compared to full system (Det+Depth ✓, SceneGraph ✓ -> EN=67.4 / EN V11=69.7 / CN=72.1 / CN V11=75.0), Det+Depth alone is substantially worse (see performance_value).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Adding Scene Graph to Det+Depth increases EN from 56.5 to 67.4 (+10.9), EN V11 from 65.7 to 69.7 (+4.0), CN from 65.2 to 72.1 (+6.9), CN V11 from 64.4 to 75.0 (+10.6) on MMBench.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Detection models show limited generalization; direct usage of perception outputs can confuse VLMs if not structured—causing performance drops on QA-style tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Perception-only pipelines introduce noisy or incomplete spatial descriptions that VLMs misinterpret, leading to degraded accuracy relative to combined symbolic+perceptual approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Not robust by itself; paper argues perception-only methods struggle when object configurations deviate from training distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Early feeding of detection + depth outputs into VLM input stream (no explicit symbolic fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Perception features (detector+depth) are necessary but not sufficient: when used alone they may degrade downstream reasoning; combining them with explicit scene graphs and CoT yields substantial improvements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1941.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1941.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompt-only Scene Graph</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting-based Scene Graph Construction (MLLM-only)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method of constructing object relationships by prompting a large multimodal model to infer relationships from the image alone, without a separate perception pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Prompt-only scene graph (MLLM inference)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Relies on the MLLM/VLM to infer object nodes and pairwise relations directly via prompting, producing a pseudo scene graph from language-vision understanding without explicit detector or depth inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>MLLM internal visual encoder (unspecified)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Language-driven inference of object relations — i.e., the model internally maps language tokens to perceived image regions and outputs relational statements used as a graph.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>scene-level inferred via model's internal representations (implicit object tokens)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Implicit only; no explicit depth or bounding-box-derived coordinates unless the MLLM infers them.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>evaluated as an ablation and discussed as an existing zero-shot approach</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>MMBench ablations and general discussion</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>dataset image inputs used by VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>MMBench accuracy and qualitative failure modes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation rows indicate Prompt-only (SceneGraph ✓ but Det+Depth ×) yields mixed results; Table 4 row (×✓) shows EN=54.3 / EN V11=64.4 / CN=69.5 / CN V11=68.4.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Prompt-only graph without perception (compared to full system) underperforms: full system EN=67.4 vs prompt-only EN=54.3 on MMBench.</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Adding Det+Depth to prompted graphs (i.e., combining perception and prompting) yields large improvements (see Det+Depth + SceneGraph numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Prompt-only scene graphs suffer due to the MLLM's limited depth perception and precise localization, making such graphs insufficient for embodied physical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>MLLMs without perceptual support mis-estimate depth/relative position, leading to incorrect relational edges and downstream CoT errors.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Limited — purely prompt-based methods generalize poorly to novel, action-dependent scenes without explicit geometric inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Implicit internal fusion within the MLLM's multimodal encoder (no external symbolic fusion).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Prompt-only construction of scene graphs is attractive for zero-shot use but is insufficient for precise embodied spatial reasoning; it must be combined with perceptual depth/region signals for best results.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1941.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1941.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Baseline VLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Baseline multimodal language models (GPT-4o, NVLM-D-72B, Llama-3.2-90B, Pixtral-12B, InternVL2.5-78B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Off-the-shelf large multimodal models used as baselines to evaluate the benefit of EmbodiedVSR augmentation on spatial reasoning and embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4o / NVLM-D-72B / Llama-3.2-90B / Pixtral-12B / InternVL2.5-78B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard multimodal LLMs / VLMs used either alone (prompt-only) or augmented with EmbodiedVSR. The paper reports base performance and improvements when combined with the scene-graph + CoT pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Baseline models typically perform implicit grounding via their multimodal encoders (not specified); when augmented they receive structured scene graph + perception inputs per EmbodiedVSR.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>baseline: implicit/global or patch-level depending on model (not specified); with EmbodiedVSR: object-centric scene graph integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>baseline models rely on implicit learned spatial priors unless augmented with explicit scene graphs and depth by EmbodiedVSR.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>evaluated on eSpatial-X, eSpatial-RoboMIND, eSpatial-Lego, and LEGO reassembly benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>eSpatial-X / eSpatial-RoboMIND / eSpatial-Lego / LEGO block reassembly</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>datasets in eSpatial benchmarks and real-world block reassembly</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>accuracy / attribute correctness / success rate</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>When augmented by EmbodiedVSR, reported improvements: GPT-4o + EmbodiedVSR +5.4% on eSpatial-X; NVLM-72B +1.74%; Llama-3.2-90B +5.19%. On eSpatial-RoboMIND, EmbodiedVSR improved reachability, success judgment, and arm feasibility by 5.3%, 6.7%, and 8.4% respectively over GPT-4o baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>EmbodiedVSR augmentation improves baseline multimodal models by several percentage points on eSpatial tasks (see performance_value).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Baselines show geometric hallucinations and fragmented reasoning chains; GPT-4o is noted as having the strongest baseline performance but still benefits from EmbodiedVSR grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Baselines fail most on long-horizon, action-dependent tasks and on precise depth/relative position reasoning without explicit structured spatial input.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>Baseline fine-tuning approaches suffer from catastrophic forgetting and sim-to-real fragility; zero-shot prompting lacks geometric consistency (as discussed in related work and experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>When augmented: late fusion of structured scene graph and perception features into the MLLM/VLM input (concatenation); otherwise baseline implicit fusion within model.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>High-capacity baselines benefit notably from explicit dynamic scene-graph grounding; even strong VLMs (GPT-4o) improve further when provided structured, perception-corrected spatial inputs and physics-constrained CoT.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>SpatialVLM: Endowing vision-language models with spatial reasoning capabilities <em>(Rating: 2)</em></li>
                <li>ViLa <em>(Rating: 2)</em></li>
                <li>PaLM-E: An embodied multimodal language model <em>(Rating: 2)</em></li>
                <li>Socratic Models: Composing zero-shot multimodal reasoning with language <em>(Rating: 2)</em></li>
                <li>VIMA <em>(Rating: 2)</em></li>
                <li>Inner Monologue: Embodied reasoning through planning with language models <em>(Rating: 2)</em></li>
                <li>Peract2: Benchmarking and learning for robotic bimanual manipulation tasks <em>(Rating: 1)</em></li>
                <li>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1941",
    "paper_id": "paper-277043037",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "EmbodiedVSR",
            "name_full": "Embodied Visual Spatial Reasoning (EmbodiedVSR)",
            "brief_description": "A framework that integrates dynamic scene-graph generation with physics-constrained Chain-of-Thought (CoT) reasoning to ground multimodal language reasoning in explicit, action-updated spatial representations for embodied agents.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "EmbodiedVSR",
            "model_description": "A hybrid pipeline that (1) uses an MLLM to identify task-relevant entities, (2) runs an open-vocabulary detector and a depth estimator to extract region and depth features, (3) constructs and iteratively updates an object-centric dynamic scene graph (nodes = objects with attributes; edges = spatial relations), and (4) concatenates visual cues (R_t), depth (D_t), raw image (I_t) and the previous graph (G_{t-1}) as structured input to a VLM/MLLM which performs physics-constrained Chain-of-Thought (CoT) reasoning to produce answers or action commands. Outputs are converted into robot-executable commands via a task generator, grounded pose tracker, and robot controller.",
            "visual_encoder_type": "open-vocabulary object detector (unspecified) + separate depth estimation model (unspecified)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Scene-graph-mediated grounding: region proposals and depth are converted into object-centric nodes and relation edges; the dynamic scene graph is combined with image-question inputs and fed into the VLM so language tokens are grounded via explicit object and relation nodes rather than only implicit visual embeddings. Physics constraints are applied at each CoT step to validate geometric feasibility.",
            "representation_level": "object-centric / scene-level (dynamic scene graph) with region-level features and depth integrated",
            "spatial_representation": "Explicit: dynamic scene graph with object nodes (attributes: positions, sizes, actions), edges encoding spatial relations, depth maps, and explicit relative coordinates used for block tasks.",
            "embodied_task_type": "object manipulation (block/LEGO assembly), long-horizon sequential operation, embodied Q&A",
            "embodied_task_name": "eSpatial-Benchmark (eSpatial-X, eSpatial-RoboMIND, eSpatial-Lego), LEGO block reassembly (real-world humanoid robot tests)",
            "visual_domain": "real-world robot setup (LEGO assembly), curated real-world dataset clips (RoboMIND) and mixed open-source spatial datasets",
            "performance_metric": "accuracy (VQA-like), success rate (robot execution), attribute correctness (color/quantity/position/size)",
            "performance_value": "Descriptive accuracy for block assembly: 100% (describing the example structure in 20 tests); robot operational success rate in reassembly: 80%; eSpatial-X: reported improvements over baselines (see grounding_improvement).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation on MMBench (Table 4) shows removing either Det+Depth or Scene Graph reduces performance. Representative numbers (MMBench subsets): None (Det+Depth ×, SceneGraph ×) -&gt; EN 58.5 / EN V11 67.5 / CN 68.8 / CN V11 62.5; Full (Det+Depth ✓, SceneGraph ✓) -&gt; EN 67.4 / EN V11 69.7 / CN 72.1 / CN V11 75.0.",
            "grounding_improvement": "Combined Det+Depth + Scene Graph vs. no components yields improvements of approximately +8.9% (EN), +2.2% (EN V11), +3.3% (CN), +12.5% (CN V11) on MMBench; other reported task-level gains include +5.4% (GPT-4o), +1.74% (NVLM-72B), +5.19% (Llama-3.2-90B) on eSpatial-X when augmented with EmbodiedVSR.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The paper identifies multiple perception/grounding bottlenecks: over-reliance on implicit spatial priors in VLMs leading to geometric hallucinations; depth perception and precise localization weaknesses in current multimodal models; and limited generalization of detection models causing VLM confusion when raw perception outputs are consumed naively.",
            "failure_mode_analysis": "Described failure modes include: (1) geometric hallucination of object relations under novel configurations, (2) fragmentation and error accumulation in long-horizon CoT planning when not persistently grounded to scene geometry, (3) VLM misinterpretation of noisy detection outputs causing degraded QA performance, and (4) sim-to-real fragility and catastrophic forgetting under fine-tuning (qualitative descriptions; per-task frequencies not quantified except overall robot success of 80% in reassembly implying 20% operational failures).",
            "domain_shift_handling": "EmbodiedVSR avoids task-specific fine-tuning by using parameter-free interaction between neural perception and symbolic/structured scene graphs and physics-constrained CoT reasoning to generalize zero-shot to new object configurations; the paper argues this approach reduces sim-to-real fragility relative to supervised fine-tuning but does not report formal domain-adaptation layers.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Concatenation of visual region features (R_t), depth map (D_t), raw image (I_t) and previous graph (G_{t-1}) into the MLLM/VLM input; object-prioritized queuing guides attention; physics constraints applied in CoT reasoning (late fusion of structured scene graph into language reasoning).",
            "sample_efficiency": null,
            "key_findings_grounding": "Explicit, dynamic object-centric scene graphs combined with depth and detector outputs and validated by physics-constrained Chain-of-Thought significantly improve embodied spatial reasoning and long-horizon planning; perception-only (detector/depth) or prompt-only scene graph construction can harm performance, but combining both perception and symbolic scene structure yields consistent gains across MMBench and the proposed eSpatial benchmarks.",
            "uuid": "e1941.0"
        },
        {
            "name_short": "Dynamic Scene Graph",
            "name_full": "Dynamic Scene Graph (object-centric, time-varying)",
            "brief_description": "A temporal sequence of scene graphs G_t that encodes object nodes and spatial relationships and is updated after each action to reflect state transfers and action-induced changes.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Dynamic Scene Graph (EmbodiedVSR component)",
            "model_description": "Represents the environment as G_t = {V_t, E_t} at each timestep; updates via functions f_gr, f_obj, f_rel that account for actions a_t and disturbances; used to persistently ground CoT steps in the agent's perceived physical state.",
            "visual_encoder_type": "depends on upstream perception (open-vocabulary detector + depth estimator) producing nodes/attributes",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Serves as the explicit symbolic substrate for grounding: language references are resolved to nodes/edges in the graph; CoT steps are validated against graph consistency and predicted state transitions.",
            "representation_level": "scene-level / object-centric (temporal sequence)",
            "spatial_representation": "Yes — explicit objects with positional attributes, relation edges encoding relative positions, depth-derived distances, and relative coordinates for structured tasks (e.g., LEGO grid offsets).",
            "embodied_task_type": "object manipulation / sequential assembly / embodied Q&A",
            "embodied_task_name": "Used across eSpatial-RoboMIND, eSpatial-Lego and LEGO reassembly experiments",
            "visual_domain": "real-world and dataset video clips",
            "performance_metric": "indirect: contributes to accuracy and success rate improvements reported for EmbodiedVSR",
            "performance_value": null,
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation rows in Table 4 indicate removing the Scene Graph (Scene Graph ×) degrades MMBench performance (e.g., full vs. ×✓ rows: with Scene Graph but no Det+Depth: EN=54.3 / EN V11=64.4 / CN=69.5 / CN V11=68.4).",
            "grounding_improvement": "Inclusion of the Scene Graph (with detection/depth) yields the largest combined gains; Scene Graph alone improves some CN metrics but is suboptimal without perception inputs.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Scene graph construction based purely on prompting/MLLMs struggles due to depth and localization errors; detection-only graphs suffer from detector generalization limits.",
            "failure_mode_analysis": "Prompt-only scene graph construction is insufficient for depth/3D localization; detector-only graphs create confusing inputs for VLMs leading to degraded QA performance.",
            "domain_shift_handling": "Designed to be updated online with actions and disturbances to handle environment changes; claimed to support zero-shot generalization by being a parameter-free structured substrate.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Scene graph features are concatenated with visual features and fed to the MLLM/VLM (late fusion of symbolic structure into language-conditioned reasoning).",
            "sample_efficiency": null,
            "key_findings_grounding": "A persistently-updated scene graph is necessary to avoid CoT error accumulation in long-horizon embodied tasks; however, scene graphs must be constructed from reliable perception (detector+depth) to avoid introducing confusion.",
            "uuid": "e1941.1"
        },
        {
            "name_short": "Det+Depth",
            "name_full": "Detection + Depth baseline (perception-only grounding)",
            "brief_description": "A baseline grounding approach that feeds detector outputs and depth estimates to the VLM without constructing the full dynamic scene graph or using physics-constrained CoT.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Det+Depth (perception-only)",
            "model_description": "Uses an object detection model and a depth estimator to provide bounding boxes/region features and depth maps as inputs to the VLM, without the additional scene-graph symbolic structure or physics-constrained CoT validation.",
            "visual_encoder_type": "object detector + depth estimation model (unspecified architectures)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Region-level grounding via detector region features and depth maps directly supplied to the VLM (no symbolic scene graph); effectively a perception-&gt;VLM pipeline.",
            "representation_level": "region-level / pixel-depth map",
            "spatial_representation": "Uses bounding boxes/region features and depth maps, but lacks explicit relation-edge modeling or temporal graph updates.",
            "embodied_task_type": "evaluated as an ablation on MMBench and in embodied tasks",
            "embodied_task_name": "MMBench ablation rows; used as partial component of EmbodiedVSR",
            "visual_domain": "datasets used in MMBench and eSpatial",
            "performance_metric": "MMBench accuracy splits (EN, EN V11, CN, CN V11)",
            "performance_value": "Ablation results (Table 4) show Det+Depth alone (Det+Depth ✓, SceneGraph ×) yields EN=56.5 / EN V11=65.7 / CN=65.2 / CN V11=64.4 on MMBench.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Compared to full system (Det+Depth ✓, SceneGraph ✓ -&gt; EN=67.4 / EN V11=69.7 / CN=72.1 / CN V11=75.0), Det+Depth alone is substantially worse (see performance_value).",
            "grounding_improvement": "Adding Scene Graph to Det+Depth increases EN from 56.5 to 67.4 (+10.9), EN V11 from 65.7 to 69.7 (+4.0), CN from 65.2 to 72.1 (+6.9), CN V11 from 64.4 to 75.0 (+10.6) on MMBench.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Detection models show limited generalization; direct usage of perception outputs can confuse VLMs if not structured—causing performance drops on QA-style tasks.",
            "failure_mode_analysis": "Perception-only pipelines introduce noisy or incomplete spatial descriptions that VLMs misinterpret, leading to degraded accuracy relative to combined symbolic+perceptual approaches.",
            "domain_shift_handling": "Not robust by itself; paper argues perception-only methods struggle when object configurations deviate from training distributions.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Early feeding of detection + depth outputs into VLM input stream (no explicit symbolic fusion).",
            "sample_efficiency": null,
            "key_findings_grounding": "Perception features (detector+depth) are necessary but not sufficient: when used alone they may degrade downstream reasoning; combining them with explicit scene graphs and CoT yields substantial improvements.",
            "uuid": "e1941.2"
        },
        {
            "name_short": "Prompt-only Scene Graph",
            "name_full": "Prompting-based Scene Graph Construction (MLLM-only)",
            "brief_description": "A method of constructing object relationships by prompting a large multimodal model to infer relationships from the image alone, without a separate perception pipeline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Prompt-only scene graph (MLLM inference)",
            "model_description": "Relies on the MLLM/VLM to infer object nodes and pairwise relations directly via prompting, producing a pseudo scene graph from language-vision understanding without explicit detector or depth inputs.",
            "visual_encoder_type": "MLLM internal visual encoder (unspecified)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Language-driven inference of object relations — i.e., the model internally maps language tokens to perceived image regions and outputs relational statements used as a graph.",
            "representation_level": "scene-level inferred via model's internal representations (implicit object tokens)",
            "spatial_representation": "Implicit only; no explicit depth or bounding-box-derived coordinates unless the MLLM infers them.",
            "embodied_task_type": "evaluated as an ablation and discussed as an existing zero-shot approach",
            "embodied_task_name": "MMBench ablations and general discussion",
            "visual_domain": "dataset image inputs used by VLMs",
            "performance_metric": "MMBench accuracy and qualitative failure modes",
            "performance_value": "Ablation rows indicate Prompt-only (SceneGraph ✓ but Det+Depth ×) yields mixed results; Table 4 row (×✓) shows EN=54.3 / EN V11=64.4 / CN=69.5 / CN V11=68.4.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Prompt-only graph without perception (compared to full system) underperforms: full system EN=67.4 vs prompt-only EN=54.3 on MMBench.",
            "grounding_improvement": "Adding Det+Depth to prompted graphs (i.e., combining perception and prompting) yields large improvements (see Det+Depth + SceneGraph numbers).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Prompt-only scene graphs suffer due to the MLLM's limited depth perception and precise localization, making such graphs insufficient for embodied physical reasoning.",
            "failure_mode_analysis": "MLLMs without perceptual support mis-estimate depth/relative position, leading to incorrect relational edges and downstream CoT errors.",
            "domain_shift_handling": "Limited — purely prompt-based methods generalize poorly to novel, action-dependent scenes without explicit geometric inputs.",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Implicit internal fusion within the MLLM's multimodal encoder (no external symbolic fusion).",
            "sample_efficiency": null,
            "key_findings_grounding": "Prompt-only construction of scene graphs is attractive for zero-shot use but is insufficient for precise embodied spatial reasoning; it must be combined with perceptual depth/region signals for best results.",
            "uuid": "e1941.3"
        },
        {
            "name_short": "Baseline VLMs",
            "name_full": "Baseline multimodal language models (GPT-4o, NVLM-D-72B, Llama-3.2-90B, Pixtral-12B, InternVL2.5-78B)",
            "brief_description": "Off-the-shelf large multimodal models used as baselines to evaluate the benefit of EmbodiedVSR augmentation on spatial reasoning and embodied tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4o / NVLM-D-72B / Llama-3.2-90B / Pixtral-12B / InternVL2.5-78B",
            "model_description": "Standard multimodal LLMs / VLMs used either alone (prompt-only) or augmented with EmbodiedVSR. The paper reports base performance and improvements when combined with the scene-graph + CoT pipeline.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Baseline models typically perform implicit grounding via their multimodal encoders (not specified); when augmented they receive structured scene graph + perception inputs per EmbodiedVSR.",
            "representation_level": "baseline: implicit/global or patch-level depending on model (not specified); with EmbodiedVSR: object-centric scene graph integrated.",
            "spatial_representation": "baseline models rely on implicit learned spatial priors unless augmented with explicit scene graphs and depth by EmbodiedVSR.",
            "embodied_task_type": "evaluated on eSpatial-X, eSpatial-RoboMIND, eSpatial-Lego, and LEGO reassembly benchmarks",
            "embodied_task_name": "eSpatial-X / eSpatial-RoboMIND / eSpatial-Lego / LEGO block reassembly",
            "visual_domain": "datasets in eSpatial benchmarks and real-world block reassembly",
            "performance_metric": "accuracy / attribute correctness / success rate",
            "performance_value": "When augmented by EmbodiedVSR, reported improvements: GPT-4o + EmbodiedVSR +5.4% on eSpatial-X; NVLM-72B +1.74%; Llama-3.2-90B +5.19%. On eSpatial-RoboMIND, EmbodiedVSR improved reachability, success judgment, and arm feasibility by 5.3%, 6.7%, and 8.4% respectively over GPT-4o baseline.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": "EmbodiedVSR augmentation improves baseline multimodal models by several percentage points on eSpatial tasks (see performance_value).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Baselines show geometric hallucinations and fragmented reasoning chains; GPT-4o is noted as having the strongest baseline performance but still benefits from EmbodiedVSR grounding.",
            "failure_mode_analysis": "Baselines fail most on long-horizon, action-dependent tasks and on precise depth/relative position reasoning without explicit structured spatial input.",
            "domain_shift_handling": "Baseline fine-tuning approaches suffer from catastrophic forgetting and sim-to-real fragility; zero-shot prompting lacks geometric consistency (as discussed in related work and experiments).",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "When augmented: late fusion of structured scene graph and perception features into the MLLM/VLM input (concatenation); otherwise baseline implicit fusion within model.",
            "sample_efficiency": null,
            "key_findings_grounding": "High-capacity baselines benefit notably from explicit dynamic scene-graph grounding; even strong VLMs (GPT-4o) improve further when provided structured, perception-corrected spatial inputs and physics-constrained CoT.",
            "uuid": "e1941.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "SpatialVLM: Endowing vision-language models with spatial reasoning capabilities",
            "rating": 2
        },
        {
            "paper_title": "ViLa",
            "rating": 2
        },
        {
            "paper_title": "PaLM-E: An embodied multimodal language model",
            "rating": 2
        },
        {
            "paper_title": "Socratic Models: Composing zero-shot multimodal reasoning with language",
            "rating": 2
        },
        {
            "paper_title": "VIMA",
            "rating": 2
        },
        {
            "paper_title": "Inner Monologue: Embodied reasoning through planning with language models",
            "rating": 2
        },
        {
            "paper_title": "Peract2: Benchmarking and learning for robotic bimanual manipulation tasks",
            "rating": 1
        },
        {
            "paper_title": "Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning",
            "rating": 1
        }
    ],
    "cost": 0.018381,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MMBench GQA SEEDBench Open Source Datasets
14 Mar 2025</p>
<p>Robomind Block 
MMBench GQA SEEDBench Open Source Datasets
14 Mar 20255F4A0147379D15335CAFAC0418684E0BarXiv:2503.11089v1[cs.RO]</p>
<p>Figure 1.Overview of EmbodiedVSR, a framework integrating multimodal interaction and dynamic task execution.EmbodiedVSR fuses dynamic scene graph generation, embodied spatial chain-of-thought reasoning and robotic control, enabling robots to grasp spatial object relationships.Additionally, we developed the eSpatial-Benchmark and dataset, further empowering the application of multimodal large models in embodied intelligence scenarios.</p>
<p>Abstract</p>
<p>While multimodal large language models (MLLMs) have made groundbreaking progress in embodied intelligence, they still face significant challenges in spatial reasoning for complex long-horizon tasks.To address this gap, we propose EmbodiedVSR (Embodied Visual Spatial Reasoning), a novel framework that integrates dynamic scene graph-guided Chain-of-Thought (CoT) reasoning to enhance spatial understanding for embodied agents.By explicitly constructing structured knowledge representations through dynamic scene graphs, our method enables zeroshot spatial reasoning without task-specific fine-tuning.This approach not only disentangles intricate spatial relationships but also aligns reasoning steps with actionable environmental dynamics.To rigorously evaluate performance, we introduce the eSpatial-Benchmark, a comprehensive dataset including real-world embodied scenarios with finegrained spatial annotations and adaptive task difficulty levels.Experiments demonstrate that our framework significantly outperforms existing MLLM-based methods in accuracy and reasoning coherence, particularly in long-horizon tasks requiring iterative environment interaction.The results reveal the untapped potential of MLLMs for embodied intelligence when equipped with structured, explainable reasoning mechanisms, paving the way for more reliable deployment in real-world spatial applications.The codes and datasets will be released soon.</p>
<p>Introduction</p>
<p>Recent advancements in Multimodal Large Language Models (MLLMs), particularly Vision-Language Models (VLMs), have revolutionized embodied intelligence by enabling robotic systems to ground language reasoning in visual perception.These models demonstrate remarkable progress in tasks requiring cross-modal alignment, such as interpreting scene semantics from visual data or generating manipulation plans conditioned on object detection.However, when confronted with spatial reasoning tasks demanding dynamic environment understanding, current MLLMbased systems exhibit two critical limitations: • Over reliance on implicit spatial knowledge: Existing models often hallucinate object relations without explicit geometric grounding, despite their visual encoding capabilities.• Fragmented reasoning chains: Sequential decisionmaking frequently breaks down when tasks require simultaneous consideration of object affordances and topological constraints.This gap persists even in state-of-the-art VLMs, as their training paradigms prioritize broad scene description over actionable spatial reasoning -the ability to infer stepwise interactions that respect physical laws and task goals.</p>
<p>Current approaches to enhancing spatial reasoning in MLLMs rely on domain-specific datasets for supervised fine-tuning, which struggles to generalize to real-world embodied intelligence due to limited coverage of dynamic, multi-modal interactions.Existing datasets focus on narrow tasks (e.g., simplified QA/synthetic scenarios), failing to capture physical environment complexity and caus-ing models to learn superficial patterns.This leads to performance degradation in novel/action-dependent contexts.Methodologically, supervised fine-tuning incurs high computational costs and risks catastrophic forgetting, while zero-shot learning lacks geometric consistency in multi-step reasoning.Both approaches fail to dynamically construct explicit spatial representations through environmental interaction.Thus, a new framework is needed to integrate zero-shot adaptability with physically grounded reasoning mechanisms.</p>
<p>We introduce EmbodiedVSR, a framework that fundamentally rethinks spatial reasoning in embodied intelligence through structured scene graph induction and physics-constrained Chain-of-Thought processes.The core innovation lies in constructing adaptive spatial knowledge representations that explicitly model three critical dimensions: object state dynamics capturing positional and functional attributes, inter-object relational constraints governed by geometric laws, and action-induced environment transitions.These representations evolve through continuous interaction cycles, maintaining persistent memory of causal relationships between agent behaviors and scene transformations.The reasoning mechanism hierarchically decomposes tasks into atomic inference steps, each validated against the scene graph's physical consistency rules before committing to subsequent operations.By anchoring language-based reasoning to this structured spatial substrate, the framework achieves human-level compositional understanding of embodied scenes while eliminating dependency on task-specific training data.</p>
<p>Though valuable for general visual language tasks, existing multimodal benchmarks for spatial reasoning have substantial limitations when applied to embodied intelligence scenarios.Popular datasets like MMBench [30], SEED-Bench [21], and GQA [15] mainly focus on static scene descriptions.They fail to capture the dynamic interaction contexts crucial for physical reasoning, such as object state changes during manipulation or geometric feasibility constraints in tool use.We present a new benchmark, eSpatial-Benchmark.We chose RoboMIND[41] as our base dataset from all available open-source datasets to tackle this issue.We recalibrate its spatial reasoning annotations, including relative positions, number of objects, colors, relative distances, etc.We introduce action-conditioned object states and remove linguistically ambiguous queries without physical grounding.The annotation-enriched RoboMIND can be used to benchmark general-purpose manipulation tasks.Additionally, a specialized embodied reasoning benchmark is developed centered around LEGO-based assembly tasks.In these tasks, spatial understanding directly determines task success.This benchmark mimics real-world manipulation challenges through configurable brick setups and rigorously evaluates models' ability to reason about multidi-mensional cognitive capabilities.The model must comprehensively understand individual bricks' physical attributes, interpret spatial dependencies between components, ensure structural stability of connections, and formulate hierarchical assembly sequences that comply with physical principles to accomplish the task.By filling the gap between traditional visual QA and actionable spatial cognition, our refined benchmarks offer fundamental infrastructure for promoting embodied intelligence research.</p>
<p>eSpatial-Benchmark offers a rich environment to evaluate how effectively the integrated pipeline addresses major visual challenges, including: To address the lack of suitable evaluation tools, we create the eSpatial-Benchmark.We recalibrate datasets and build a LEGO based task corpus, then establish an evaluation protocol.This protocol assesses spatial understanding through object interactions and multi step manipulations.The benchmark provides metrics to measure progress in actionable spatial cognition, filling a gap in embodied intelligence research.</p>
<p>Related Work</p>
<p>Multimodal LLMs in Embodied Intelligence</p>
<p>Recent advances in multimodal LLMs (MLLMs) have significantly expanded the horizons of embodied intelligence [3,4,11,20,26,28,31,34], enabling robots to interpret natural language commands within visual contexts.Pioneering works like RT-series [5,6,33] and PaLM-E [10] demonstrate remarkable capabilities in translating high-level instructions into executable action sequences by aligning visual inputs with linguistic reasoning.These models typically adopt a hybrid architecture where visual encoders extract scene features that are subsequently processed through LLM-based planners [25,35,40,43,44].However, our analysis identifies three systemic limitations in current paradigms.First, spatial reasoning predominantly relies on implicit knowledge embedded during pre-training, leading to geometric hallucinations when confronted with novel object configurations.Second, the predominant "perception-toaction" pipeline suffers from episodic memory loss, where intermediate reasoning states are not persistently grounded in environmental dynamics.Third, while methods like VIMA [16] attempt to address compositional reasoning through prompt engineering, they remain constrained by short-horizon task decomposition that fails to maintain physical consistency across multi-step interactions.These limitations persist even in state-of-the-art visual language models (VLMs) [2,13,23,29,32,[37][38][39], as evidenced by their inability to reason about action-conditioned spatial dependencies.</p>
<p>Embodied Visual-Spatial Methods</p>
<p>In embodied AI systems, visual-spatial reasoning represents a critical challenge for enabling complex task execution, requiring the seamless integration of linguistic abstractions with dynamic physical interactions.We classify the current methods into the following four categories:</p>
<p>• Chain-of-Thought (CoT) for Long Horizon Task Planning: Current CoT applications in embodied AI (Inner Monologue [14], Code as Policies [27] and COMErobot [45]) demonstrate that explicit reasoning traces can improve multi step action planning.However, these methods predominantly operate in linguistic abstraction space generating plausible sounding step sequences without persistent grounding in the environment's physical state.This leads to error accumulation in long horizon tasks, as later steps cannot rectify earlier violations of spatial constraints.• Fine Tuning Based Spatial Adaptation: Approaches like Socratic Models [44] and EmbodiedBERT [36] employ task specific fine tuning on curated spatial reasoning datasets (eg, manipulation trajectories [19,41], spatial QA pairs [7]).While achieving localized improvements, they suffer from catastrophic forgetting of foundational visual language alignment and sim to real fragility performance collapses when object configurations deviate from training distributions.• Zero Shot Spatial Reasoning Paradigms: Recent efforts (ViLa [12], SpatialVLM [8]) explore prompt engineering to elicit spatial understanding from pretrained VLMs.Though avoiding costly fine -tuning, these methods exhibit geometric detachment-generating spatially inconsistent hypotheses due to lacking explicit 3D grounding.• Structured Knowledge Augmentation: Hybrid neuro symbolic systems inject spatial knowledge through predefined logic rules or static scene graphs.While improving reasoning consistency, these frameworks struggle with dynamic environment adaptation, as their symbolic components cannot evolve with action induced state changes -a critical limitation for embodied interaction.While significant advancements have been made in integrating language models with robotic systems, limitations persist across existing paradigms.Existing approaches struggle to maintain spatial constraint adherence in long-horizon tasks: Chain-of-Thought (CoT) generation lacks persistent tracking of scene geometry, fine-tuning paradigms are bound by static training distribution assumptions, zero-shot reasoning relies on linguistic priors leading to geometric inconsistencies, and structured knowledge injection fails to adapt to action-induced environmental dynamics.These methodologies fundamentally decouple spatial reasoning from real-time physical state grounding, causing agents to progressively diverge from real-world physics during multi-step operations.The central bottleneck lies in constructing an embodied spatial reasoning framework that preserves both the abstract power of language reasoning and continuous anchoring to scene physics, a key frontier demanding breakthroughs in current embodied AI research.EmbodiedVSR transcends these dichotomies by unifying (1) dynamic scene graphs that persistently ground physical states, (2) physics constrained CoT that enforces geometric feasibility at each reasoning step, and (3) zero shot generalization through parameter free interaction between neural and symbolic components.</p>
<p>Benchmarking Embodied Spatial Capabilities</p>
<p>The development of evaluation frameworks for embodied spatial reasoning remains fragmented across research communities [15,21,24,30,41,42].Computer vision benchmarks like CLEVR [17] and VSR [29] excel in testing geometric relationship recognition but operate in static synthetic environments, ignoring action-induced state changes critical to embodied interaction.Robotics-oriented benchmarks such as BEHAVIOR [22] and iTHOR [18] advance task-level evaluation yet lack granular metrics to isolate spatial reasoning performance from low-level control errors.Recent efforts like Socratic Models [44] and PaLM-E [10] attempt to bridge this divide through embodied VQA datasets, but their spatial queries remain anchored to singlestep "what-if" hypotheticals rather than multi-step physical feasibility analysis.This limitation persists even in frontier works: ViLa [12]'s manipulation planning benchmark evaluates action sequence diversity but cannot distinguish physically impossible plans, while SpatialVLM [8]'s evaluation focuses on absolute coordinate prediction accuracy-a metric decoupled from real-world robotic execution contexts.These collective shortcomings reveal a fundamental disconnect between existing evaluation paradigms and the demands of ecological embodied intelligence, where spatial understanding must dynamically integrate geometric constraints, tool affordances, and action causality.Our EmbodiedVSR-Benchmark addresses this gap through task designs that intrinsically couple spatial reasoning validity with executable action generation, establishing the first evaluation protocol aligned with real-world physical interaction demands.</p>
<p>Method</p>
<p>Problem Formulation</p>
<p>As mentioned previously, a static scene graph can help the MLLMs precisely understand the environment.A graph can commonly be formulated as G = {V, E}, where V is the set of nodes and E is the set of edges representing the relationship between the connected nodes.For spatial scene graphs, the nodes are the objects, and the edges are the spatial relationships between the objects.Encoding spatial relationships between objects provides a semantically rich context that aligns closely with question-answer reasoning tasks.They can serve as highly question-relevant input for large models, achieving improved accuracy in VQA (Visual Question Answering) benchmarks.However, in most embodied intelligence scenarios, static VQA cannot help with successful task execution and planning related to the CoT process.This process is transient, as the robot interacts with the environment and the state of the environment changes.For that reason, we expand the graph as a dynamic scene graph to track this process.</p>
<p>Let the execution of an embodied task be discretized into steps t = {1, 2, 3, • • • , T }, and then the dynamic scene graph is a sequence of graph G = {G t } T t=1 , where each G t = {V t , E t } represents the scene at time t.The state space function of a dynamic scene graph can be formulated as
G t+1 = f gr (G t , a t ) + ε gr (1)
where f gr is the state transfer function, a t is the action taken by the robot and ε gr is the external disturbance.The state transitions can be the node change and the relationship updates, so the state change of the node is formulated as
N t+1 = f obj (N t , a t ) + ε obj (2)
where f obj is the state transfer function for nodes, the state change of nodes can be type or number.For example, if the robot chooses the action a t to put one more LEGO block on the structure, the state of the structure changes after the action is executed.If human removes a block, it is then considered as a disturbance to the state, the state also changes.</p>
<p>And the relationship dynamics of future step can be changed as
e t+1 ij = f rel (n t i , n t j , e t ij , a t ) + η t ij(3)
where e t ij ∈ E t , the term n t i ∈ N t , the function f rel is the relationship update function and η t ij denotes the relationship noise.</p>
<p>The term "dynamic" in this formulation encompasses two essential aspects.Firstly, it dynamically constructs the graph at each time concerning the task instructions and the visual perceptions.Secondly, it enables the robot to understand the possible state changes due to action, and it can further help the robot to choose optimal action via the prediction of state changes.</p>
<p>Framework Architecture</p>
<p>In the field of robotics, explicit physical relationship representations, such as bounding box detection, are commonly used to enhance spatial perception.While such information can help VLMs improve its spatial awareness to some extent, traditional mapping methods are often too rigid and struggle to represent complex logical relationships.To address these limitations, we propose EmbodiedVSR, a novel approach that leverages dynamic scene graph modeling to enhance spatial reasoning capabilities in VLMs.</p>
<p>As shown in Fig. 4, our algorithm serves as the core of the system, managing interactions between software and hardware components to ensure efficient execution.EmodiedVSR mainly consist of two components: (1) Scene Graph Generator, focused on scene understanding, and (2) Spatial CoT Reasoning, which enhances spatial reasoning.To evaluate the real-world applicability of our approach, we integrate it into an open-source agent framework, constructing a robotic embodied operation system to systematically verify the effectiveness of our model in embodied tasks.</p>
<p>Dynamic Scene Graph Generation For a given question Q, we first utilize an MLLM to analyze the input and identify key target entities, such as objects and regions, which serve as node V t in the scene graph.
V t = f mllm (Q t ), V t = v t j , j = 1, 2...n(4)
These identified entities are then arranged in a prioritized queue based on their relevance to the question, ensuring that the most critical elements are processed first.Based on this queue, the data will be routed through the visual aid extractor to extract related auxiliary information.we employ an open vocabulary detection model to identify the relevant image regions.Simultaneously, we use a depth estimation model to predict the depth information of the scene.
R t = f ovd (V t , I t )(5)D t = f depth (I t )(6)
Leveraging these extracted features, we construct a scene graph in which key objects are represented as nodes, while their attributes, including actions, positions, and sizes, are incorporated into a structured object-aware relationship graph.To tackle the temporal dependency problem, we adopt a dynamic scene graph update mechanism, which iteratively incorporates historical information via temporal modeling within the scene graph.This process ensures consistency, as formalized in Eq. (7).
G t = f mllm (Concat(R t , D t , I t , G t−1 ))(7)
Spatial CoT Reasoning Module Embodied AI tasks, such as planning and success judgment, are rarely included in standard VLM training datasets.As a result, adapting general MLLMs to these scenarios is both difficult and resource-intensive.To address this, our approach introduces structured dynaminc scene graph representations as additional guidance.By combining the image-question pair with the generated scene graph and feeding them into the VLM, we enable the model to use chain-of-thought reasoning for generating accurate responses.This integration allows the model to improve its ability to generate answers, by referencing structured spatial relationships.</p>
<p>Peripheral Module</p>
<p>We demonstrate the effectiveness of our approach in robotic applications by embedding our EmbodiedVSR model within an open-source multi-agent framework, where dynamic task generation, grounded pose tracker, and robot controller work in concert to enable robust performance.Specifically, we design and implement two tasks to evaluate the model's effectiveness in various robotic tasks.Short-Term Question-Answering (Q&amp;A) Task The first strategy we adopt is a Question-Answering (Q&amp;A) format.In this strategy, we utilize our eSpatial-RoboMIND benchmark to test multiple robotic scenarios to assess the enhancement of spatial intelligence in the EmbodiedVSR model when applied to robotics.Through this strategy, we evaluate the robot's ability to comprehend and respond to complex questions, providing insights into the model's cognitive capabilities in dynamic, real-world environments.This strategy allows us to directly assess how well the robot performs in interpreting and reacting to questions within a contextualized robotic space, further verifying the improvement in spatial reasoning abilities.</p>
<p>Long-Term Sequential Operation Task The second task is the Sequential Operation Strategy, which integrates robotic action over time.In this approach, we focus on a blockbuilding task to validate the effectiveness of our proposed dynamic scene graph in real-world applications.The Long-Term Sequential Operation Strategy aims to demonstrate the advantages of using a dynamic scene representation.</p>
<p>In the block build task, as shown in Fig. 5, we use a 1×1 block to represent a block with a protrusion, and this extends to larger blocks accordingly.The relative positions of the blocks are modeled using a physical relative coordinate.Specifically, we take the bottom-left corner of the overall structure, with the 1×1 block as the reference unit, as the origin of the relative coordinate system for the blocks.The block building task uses the output of VLMs to compute the horizontal and vertical offsets for each block.For example, the instruction 'place the red 1×1 block at position (2, 0) in the second layer' indicates that the red block should be placed above another block, with a horizontal displacement of two units to the right.</p>
<p>In the Task Generator module, we utilize the aforementioned relative coordinate system and its calibration with the real-world physical environment to convert the output commands into executable actions.Then we use grounded pose tracking to get object info and then feed into robot controller to complete the task.</p>
<p>Benchmark Datasets</p>
<p>Many benchmark datasets have been published with the advancement of MLLMs.Though the models continuously get higher scores in these benchmarks, our embodiment experiments showed an undesired success rate when the tasks require an accurate visual understanding of the scene.This phenomenon raises a concern about whether the Q&amp;A-pair and the evaluation indices are relevant to the scene understanding of embodied intelligence, as they require the knowledge to guide the planning and action.Consequently, we concluded that the current MLLMs are still struggling with the five significant visual challenges mentioned.</p>
<p>We introduce eSpatial, a novel benchmark tailored for both Embodied AI (EAI) and general spatial reasoning.eSpatial comprises three integral components:</p>
<p>(1) eSpatial-X, a curated collection of existing opensource datasets-including GQA [15], MMbench [30], and SeedBench [21]-which ensures diversity and robust generalization assessment.( 2 As for our data collation, we have adopted a filtering method that combines manual annotation with automatic generation as shown in Fig. 3. To process eSpatial-X, we select data labeled with spatial attributes; then, through manual review, we remove data containing the issues,such incorrect and non-unique answer.For eSpatial-Lego and eSpatial-RoboMIND, we initially manually select video clips from current open-source datasets.Next, the selected video clips, along with predefined task categories, are input into GPT to generate the corresponding Q&amp;A pairs.Finally, we employ manual annotation once more to filter out or correct any problematic data produced by GPT-4o, whose process pipeline also shown in Fig. 1.</p>
<p>Experiments</p>
<p>To validate the advancement of our method, we evaluated EmbodiedVSR and baseline models (GPT-4o, NVLM-D-72B, and Llama-3.2 90B) on our eSpatial dataset including eSpatial-X, eSpatial-RoboMIND, and spatial-Lego.To further validate the efficacy of our method in embodied scenarios, we established an automated LEGO assembly system on the Tien Kung humanoid robot.Initially, we employed EmbodiedVSR + GPT-4o to deconstruct the LEGO sample, subsequently transmitting the deconstruction results to the robot to assemble an identical LEGO structure.</p>
<p>eSpatial-X benchmark</p>
<p>For general spatial reasoning, we conducted a comprehensive evaluation across multiple datasets, as delineated in Table. 1.Our approach yielded significant improvements in accuracy, enhancing the performance of GPT-4o, NVLM 72B, and Llama-3.2 90B by 5.4%, 1.74%, and 5.19%, respectively.This demonstrates that our method is capable of enhancing the performance of baseline models in general spatial tasks.</p>
<p>Methods</p>
<p>Our+ GPT-4o NVLM 72B achieved superior performance with fewer parameters than Llama-3.290B.However, the application of our method reduced the disparity in accuracy between these models, suggesting that models with a mismatch between benchmark performance and model size may possess latent generalization potential.Consequently, under equivalent performance conditions, opting for models with larger parameter counts may confer advantages in novel tasks.Among the baseline models, GPT-4o exhibited the best performance, and our method also delivered the most substantial improvement for GPT-4o.Integrating these findings, we infer that GPT-4o may achieve enhanced emergent capabilities at the expense of model general performance.
Our+ NVLM Our+Llama-Llama-3.2 GPT-4o - NVLM72B72B 3.2 90B</p>
<p>eSpatial-RoboMIND benchmark</p>
<p>In this section, we evaluated the performance of GPT4o and EmbodiedVSR across a comprehensive set of tasks within our eSpatial Q&amp;A dataset, focusing on complex scenarios including adjacency relationships, distance estimation, reachability analysis, success judgment, object overlap detection, arm kinematic feasibility, and directional/orientation reasoning.As illustrated in Fig. 4  Our method demonstrated particularly superior performance in tasks related to embodied scenes, such as reachability, success judgment, and arm feasibility.Specifically, it outperformed the baseline models by 5.3%, 6.7%, and 8.4% respectively, exceeding the average improvement at 4.8%.This indicated that our approach of embedding robot embodiment information significantly enhanced the baseline model's capabilities in perceiving and understanding embodied scenes.EmbodiedVSR effectively bridged the gap Multimodal Large Language Models (MLLMs) and the field of embodied intelligence.</p>
<p>GPT-4o EmbodiedVSR</p>
<p>eSpatial-Lego Benchmark</p>
<p>EmbodiedVSR was evaluated for describing different attributes regarding the color, quantity, relative position, and size of the example structure, where the dataset size was reported in Table .3. In this real-world task, the proposed method was compared to state-of-the-art models including Pixtral-12B[1], Llama-3.2-90B,InternVL2.5-78B[9], and GPT-4.As demonstrated in Table .2, the proposed method ranked top in attributes including color (92.3%), quantity (94.7%), size (89.6%), and overall correctness (91.2%).Specifically, it improved block size description by 24% over the suboptimal method.While ranking second in relative position (87.1%), it trailed the best performance by only 0.5%.</p>
<p>Real-world Benchmark: Block Reassembly</p>
<p>Task Setup.</p>
<p>To demonstrate and evaluate the performance of the proposed method, EmbodiedVSR was deployed in an embodied manipulation task.The task required a humanoid to re- assemble LEGO blocks into a structure identical to a random handmade example.As shown in Fig. 5, the system contained a Tien Kung X humanoid equipped with a pair of Robotiq 2F-85 grippers.In 20 real-world randomized block assembly tests, Em-bodiedVSR achieved a 100% accuracy in describing the block assembly, while the robot's operational success rate was 80%.Video samples of the tests can be found in the supplementary materials.</p>
<p>Ablation Study</p>
<p>We conducted a thorough ablation study to systematically assess the contribution of each component to our model's performance.Our experimental setup included three key comparisons: (1) evaluating the spatially aware relational graph module on its own without the perception model, (2) testing the perception module in isolation by removing all other components, and (3) analyzing the full model with all modules included.This structured approach allowed us to clearly measure the impact of each module on the overall system while ensuring experimental reliability.Comparison of the first and second rows in the table showed that using either a general detection model or relying solely on prompting a large model to infer object relationships in the image could provide some performance improvements.However, when used independently, both approaches also introduced certain negative effects, leading to performance degradation across both the English and Chinese datasets in MMBench.</p>
<p>The primary reason for this performance drop was that, compared to QA tasks, detection models generally exhibited weaker generalization capabilities.Directly utilizing perception results could lead to VLM confusion, where the vision-language model misinterpreted its own outputs, ultimately affecting performance.Similarly, relying solely on large-model prompting for object relationship modeling had its limitations.Current multimodal large models still struggled with depth perception and precise object localization, making scene graph construction based purely on prompting insufficient.</p>
<p>To address these issues, our approach integrated the strengths of both models: leveraging the detection model to provide precise perceptual information for scene graph construction while utilizing the reasoning capabilities of the large model to enhance object relationship understanding.As shown in the fourth row of the table, this synergistic strategy significantly improved performance on MMBench, demonstrating its effectiveness in multimodal tasks.</p>
<p>Conclusion</p>
<p>We introduced a dynamic spatial scene graph-based COT approach to enhance the spatial perception capabilities of VLMs.Our method dynamically constructs and updates scene graphs according to detection and depth estimation.We also introduced a new dataset eSpatial that encompasses diverse dynamic scenes and intricate spatial relationships, providing a robust benchmark for embodied spatial testing.On one hand, evaluations conducted on the eSpatial dataset produced outstanding performance, demonstrating that our approach exhibited strong capabilities across a wide range of spatial tasks.On the other hand, in the block scene, an open-source ai agent framework was employed to integrate vlm with embodied operations.Experimental results showed that this approach significantly improved the robotic relative spatial reasoning ability.This work established a robust framework for enhancing spatial reasoning in robotic systems, while simultaneously defining a roadmap for advancing embodied AI research.</p>
<p>Acknowledgments</p>
<p>Figure 2 .Figure 3 .
23
Figure 2. eSpatial datasets data distributions.</p>
<p>Figure 4 .
4
Figure 4. eSpatial-RoboMIND Benchmark evaluation</p>
<p>shown in Fig. 6 ,Figure 6 .
66
Figure 6.LEGO Block reassemble procedure.</p>
<p>Table 1 .
1
eSpatial-X Evaluation: EmbodiedVSR enhances the baseline models across most scenarios.
90BPublic DatasetsMMB EN67.458.558.756.573.965.2MMB EN V11 69.767.556.656.664.467.1MMB CN72.168.865.258.758.643.5MMB CN V11 75.062.557.957.957.953.9SEEDBench78.875.267.363.869.767.6GQA test46.734.7--31.627.7Average68.2862.8661.1459.4059.3554.16</p>
<p>Table 2 .
2
Success rate comparison of different models for the Block reassembly task.
ModelEvaluated attributes Color Quantity Relative position Size OverallNVLM-D-72B 34.761.752.1 33.2 46.6Pixtral-12B29.450.643.7 21.0 37.3Llama-3.2-90B 47.744.351.2 33.6 44.4InternVL2.5-78B 43.558.551.1 46.2 50.4GPT-4o48.861.760.5 38.7 53.2Proposed51.563.961.9 63.5 60.2ExampleHead CamerablockstructureRobotiq-2F85Demonstrationcapture cameraBlock componentsEvaluated attributesColor QuantityRelative positionSize OverallDataset size 262316213214 1005
gripperFigure 5. LEGO block reassembly task setup.This task requires a humanoid with a two -finger gripper to reassemble a LEGO structure according to a given example.An additional camera is used to feed data to the EmbodiedVSR.A LangGraph -based agent formulates the outputs of EmbodiedVSR as actionable commands for the robot controller.</p>
<p>Table 3 .
3
Test dataset size for various attributes in the block reassembly task.</p>
<p>Table 4 .
4
Ablations on the MMBench of Det+Depth and Scene Graph
MethodMMBenchDet+DepthScene GraphENEN V11CNCN V11××58.567.568.862.5✓×56.565.765.264.4×✓54.364.469.568.4✓✓67.469.772.175.0
The authors extend sincere gratitude to the engineers at Beijing Innovation Center of Humanoid Robot for their technical leadership in resolving intricate system integration challenges.This interdisciplinary collaboration exemplifies how cross-domain synergies can push the boundaries of embodied AI systems.
Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, arXiv:2410.07073Saurabh Garg, Theophile Gervet, et al. Pixtral 12b. 2024arXiv preprint+0, +1] [-0, -1. +0, +1] [-0, -1</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 20231arXiv preprint</p>
<p>Tianli Suneel Belkhale, Ted Ding, Pierre Xiao, Quon Sermanet, Jonathan Vuong, Yevgen Tompson, Chebotar, arXiv:2403.01823Debidatta Dwibedi, and Dorsa Sadigh. Rt-h: Action hierarchies using language. 2024arXiv preprint</p>
<p>Robocat: A selfimproving generalist agent for robotic manipulation. Konstantinos Bousmalis, Giulia Vezzani, Dushyant Rao, Alex X Lee, Maria Bauzá, Todor Davchev, Yuxiang Zhou, Agrim Gupta, Akhil Raju, arXiv:2306.117062023arXiv preprintColine Devin</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, arXiv:2307.158182023arXiv preprint</p>
<p>Spatialbot: Precise spatial understanding with vision language models. Wenxiao Cai, Iaroslav Ponomarenko, Jianhao Yuan, Xiaoqi Li, Wankou Yang, Hao Dong, Bo Zhao, arXiv:2406.136422024arXiv preprint</p>
<p>Spatialvlm: Endowing vision-language models with spatial reasoning capabilities. Boyuan Chen, Zhuo Xu, Sean Kirmani, Brain Ichter, Dorsa Sadigh, Leonidas Guibas, Fei Xia, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Zhaoyang Hao Tian, Liu, arXiv:2412.052712024arXiv preprint</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Ayzaan Chowdhery, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Wenlong Yu, Huang, 202334</p>
<p>Peract2: Benchmarking and learning for robotic bimanual manipulation tasks. Markus Grotz, Mohit Shridhar, Yu-Wei Chao, Tamim Asfour, Dieter Fox, CoRL 2024 Workshop on Whole-body Control and Bimanual Manipulation: Applications in Humanoids and Beyond. 2024</p>
<p>Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning. Yingdong Hu, Fanqi Lin, Tong Zhang, Li Yi, Yang Gao, arXiv:2311.178422023arXiv preprint</p>
<p>Language is not all you need: Aligning perception with language models. Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Barun Patra, Advances in Neural Information Processing Systems. 202336</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019. 2, 4, 6</p>
<p>Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, Vima, arXiv:2210.03094General robot manipulation with multimodal prompts. 20222arXiv preprint</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Simple but effective: Clip embeddings for embodied ai. Apoorv Khandelwal, Luca Weihs, Roozbeh Mottaghi, Aniruddha Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, arXiv:2403.12945A large-scale in-the-wild robot manipulation dataset. 2024arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Seed-bench: Benchmarking multimodal llms with generative comprehension. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, arXiv:2307.161252023. 2, 4, 6arXiv preprint</p>
<p>Behavior-1k: A human-centered, embodied ai benchmark with 1,000 everyday activities and realistic simulation. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Martín-Martín, Chen Wang, Gabrael Levine, Wensi Ai, Benjamin Martinez, arXiv:2403.092272024arXiv preprint</p>
<p>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, International conference on machine learning. PMLR2023</p>
<p>Lei Li, Yuancheng Wei, Zhihui Xie, Xuqing Yang, Yifan Song, Peiyi Wang, Chenxin An, Tianyu Liu, Sujian Li, Bill Yuchen Lin, arXiv:2411.17451A challenging benchmark for vision-language generative reward models. 2024arXiv preprint</p>
<p>Pre-trained language models for interactive decision-making. Shuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Akyürek, Anima Anandkumar, Advances in Neural Information Processing Systems. 202235</p>
<p>Vision-language foundation models as effective robot imitators. Xinghang Li, Minghuan Liu, Hanbo Zhang, Cunjun Yu, Jie Xu, Hongtao Wu, Chilam Cheang, Ya Jing, Weinan Zhang, Huaping Liu, arXiv:2311.013782023arXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Moka: Open-world robotic manipulation through markbased visual prompting. Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine, arXiv:2403.031742024arXiv preprint</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 2023364</p>
<p>Mmbench: Is your multi-modal model an all-around player?. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, European conference on computer vision. Springer2024. 2, 4, 6</p>
<p>Quest: Self-supervised skill abstractions for learning continuous control. Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, Animesh Garg, Advances in Neural Information Processing Systems. 202537</p>
<p>Introducing gpt-4o and more tools to chatgpt free users. Ai Open, 2024</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. Abby O' Neill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, 2024 IEEE International Conference on Robotics and Automation (ICRA). IEEE2024</p>
<p>Perceiveractor: A multi-task transformer for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, PMLR, 2023. 3Conference on Robot Learning. </p>
<p>Llm-planner: Few-shot grounded planning for embodied agents with large language models. Hee Chan, Jiaman Song, Clayton Wu, Brian M Washington, Wei-Lun Sadler, Yu Chao, Su, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2023</p>
<p>Embodied bert: A transformer model for embodied, language-guided visual task completion. Alessandro Suglia, Qiaozi Gao, Jesse Thomason, Govind Thattai, Gaurav Sukhatme, arXiv:2108.049272021arXiv preprint</p>
<p>Gemini: a family of highly capable multimodal models. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, arXiv:2312.118052023arXiv preprint</p>
<p>Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, arXiv:2409.121912024arXiv preprint</p>
<p>Cogvlm: Visual expert for pretrained language models. Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song Xixuan, Advances in Neural Information Processing Systems. 202537</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang, arXiv:2302.015602023arXiv preprint</p>
<p>Kun Wu, Chengkai Hou, Jiaming Liu, Zhengping Che, Xiaozhu Ju, Zhuqin Yang, Meng Li, Yinuo Zhao, Zhiyuan Xu, Guang Yang, arXiv:2412.13877Benchmark on multiembodiment intelligence normative data for robot manipulation. 202434arXiv preprint</p>
<p>Robotube: Learning household manipulation from human videos with simulated twin environments. Haoyu Xiong, Haoyuan Fu, Jieyi Zhang, Chen Bao, Qiang Zhang, Yongxi Huang, Wenqiang Xu, Animesh Garg, Cewu Lu, PMLR, 2023. 4Conference on Robot Learning. </p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, 2023</p>
<p>Socratic models: Composing zero-shot multimodal reasoning with language. Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, arXiv:2204.00598202234arXiv preprint</p>
<p>Peiyuan Zhi, Zhiyuan Zhang, Muzhi Han, Zeyu Zhang, Zhitian Li, Ziyuan Jiao, Baoxiong Jia, Siyuan Huang, arXiv:2404.10220Closed-loop open-vocabulary mobile manipulation with gpt-4v. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
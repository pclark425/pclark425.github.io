<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2156 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2156</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2156</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-57.html">extraction-schema-57</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <p><strong>Paper ID:</strong> paper-278535056</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.08341v1.pdf" target="_blank">Benchmarking AI scientists in omics data-driven biological research</a></p>
                <p><strong>Paper Abstract:</strong> The rise of large language models and multi-agent systems has sparked growing interest in AI scientists capable of autonomous biological research. However, existing benchmarks either focus on reasoning without data or on data analysis with predefined statistical answers, lacking realistic, data-driven evaluation settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench), a benchmark designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge. BaisBench comprises two tasks: cell type annotation on 31 expert-labeled single-cell datasets, and scientific discovery through answering 198 multiple-choice questions derived from the biological insights of 41 recent single-cell studies. Systematic experiments on state-of-the-art AI scientists and LLM agents showed that while promising, current models still substantially underperform human experts on both tasks. We hope BaisBench will fill this gap and serve as a foundation for advancing and evaluating AI models for scientific discovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2156.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2156.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BaisBench</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Biological AI Scientist Benchmark (BaisBench)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A data-driven benchmark introduced in this paper to evaluate AI scientists on real single-cell omics research, consisting of a cell type annotation task (BAIS-CTA) and a scientific discovery multiple-choice task (BAIS-SD).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BaisBench</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>benchmark / evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>single-cell transcriptomics / biological discovery</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Not a generative system; provides tasks that require AI systems to generate cell-type annotations, data analyses, and scientific conclusions/answers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Provides objective evaluation metrics: (1) hierarchical annotation score S_CTA for cell type annotation based on the uHAF hierarchical ontology; (2) multiple-choice correctness score S_SD for scientific-discovery questions (single-choice and multi-choice scoring rules). Benchmarks compare AI outputs against expert-curated ground truth and human experts as baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Implicit: novelty tested by task difficulty and by requiring derivation of discoveries from fresh datasets; novelty operationalized by BAIS-SD questions derived from recent studies (i.e., not memorization) and by organ-specific vs general pretrained models in BAIS-CTA.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Used to report AI performance rather than itself generating: reported AI annotation scores (AutoBA 0.378±0.014, CellAgent 0.376±0.017, GPT-4o via generated code 0.298±0.013, scChat 0.107±0.008, human+CellTypist 0.437±0.014) and BAIS-SD accuracies (GPT-4o 0.220±0.015, Deepseek-V3 0.254±0.020, human experts 0.762).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>BaisBench's validation is deterministic: ground-truth annotations and curated question answers produce accuracy/score metrics as above. No probabilistic calibration metrics provided by the benchmark itself.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>BaisBench reveals worse validation scores for AI on tasks requiring novel data-driven reasoning (BAIS-SD) and on organs where pretrained models are less appropriate (e.g., ovary/testis). BAIS-SD accuracies for LLM agents (~0.22–0.25) are much lower than human (0.762), indicating strong negative effect of novelty on validation performance.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>The benchmark documents an asymmetry: many AI systems can execute pre-programmed pipelines (generate outputs) but fail to produce or validate higher-level biological insights (low BAIS-SD scores); human experts outperform in both generation and validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>BaisBench exposes OOD weaknesses: AI systems using a general CellTypist model underperform humans who select organ-specific pretrained models; AI scores drop on tissues with challenging annotation (ovary/testis).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Benchmark highlights need for RAG, richer external knowledge access, multi-agent coordination, and human-in-the-loop review; does not itself implement gap-closing mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BaisBench demonstrates that state-of-the-art AI scientists substantially underperform human experts on both structured annotation and data-driven scientific-discovery tasks, and that performance degrades notably on novel/complex reasoning tasks and out-of-distribution tissues.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2156.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoBA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoBA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-agent system for automated multi-omic analyses applied to single-cell data analysis pipelines and cell type annotation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An ai agent for fully automated multi-omic analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoBA</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven agent / automated pipeline system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>single-cell / multi-omic biological analysis</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates automated data-processing pipelines, code for preprocessing and downstream single-cell analyses, and cell type annotation outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated in this paper via BAIS-CTA hierarchical annotation score against expert-labeled annotations (uHAF), i.e., comparison to curated ground truth and human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>No explicit novelty metric; performance compared across organ datasets and against human expert and organ-specific pretrained models to assess generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Annotation score on BAIS-CTA: 0.378 ± 0.014 (full benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation measured as the BAIS-CTA score above (comparison to curated labels); no finer-grained precision/recall provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance lower than human expert and degrades relative to organ-specific model selection by humans; indicates sensitivity to unfamiliar organ contexts and limited generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>AutoBA can generate executable pipelines and annotations but remains below expert-level validation scores, indicating a gap between generation capability and validated correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Shows reduced accuracy when compared to a human selecting organ-specific CellTypist models (human 0.437 vs AutoBA 0.378); specific OOD organ failures discussed qualitatively (ovary/testis remain difficult).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Relies on preprogrammed pipelines and CellTypist; paper suggests improved external knowledge access, better model selection, and human-in-loop could close gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>AutoBA performs automated single-cell analysis but still underperforms human experts in hierarchical cell annotation and struggles to generalize to organ-specific contexts without human-like model selection.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2156.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>scChat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>scChat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered co-pilot for contextualized single-cell RNA-seq analysis that uses a RAG framework to retrieve marker gene information and perform annotation based on biological priors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>scchat: A large language model-powered co-pilot for contextualized single-cell rna sequencing analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>scChat</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based agent with Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>single-cell transcriptomics / cell type annotation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates annotation calls and interpretable reasoning grounded in retrieved marker genes and prior biological knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In this evaluation, validated by BAIS-CTA hierarchical annotation score (comparison to expert-curated uHAF labels); scChat's annotation restricted to organs it supports (pre-encoded marker sets).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Implicit: scChat supports only tissues/organs with pre-encoded marker databases, so novelty measured by whether an organ is supported (familiar) or unsupported (novel).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>BAIS-CTA annotation score on supported subset: 0.107 ± 0.008 (lowest among tested methods on that subset).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured via BAIS-CTA score (very low); no precision/recall numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Fails to generalize to novel organs lacking pre-encoded markers because of rigid marker-based approach; performance limited to supported (familiar) tissues.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>scChat's use of priors leads to conservative generation but poor validated accuracy across many organs; indicates a mismatch where prior-only annotation is insufficient for broad validation.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor: cannot complete full benchmark for unsupported organs and performs poorly even on supported organs relative to human experts and other AI that use broader pretrained tools.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>RAG for marker retrieval is used, but scChat lacks flexible model selection and adaptation; paper suggests adding external knowledge access and more flexible pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>scChat's marker-prior approach yields low annotation accuracy and limited generalization; dependence on pre-encoded organ markers prevents handling novel tissues, illustrating validation failure on OOD inputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2156.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CellAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CellAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A role-based multi-agent framework (planner, executor, evaluator) that uses LLM coordination to derive research conclusions for single-cell data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cellagent: An llm-driven multi-agent framework for automated single-cell data analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CellAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-driven multi-agent system</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>single-cell transcriptomics / automated analysis</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates analysis plans, code, and cell type annotations through coordinated multi-agent roles.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated here via BAIS-CTA hierarchical annotation score against expert uHAF labels; CellAgent primarily used CellTypist for final annotation in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>No explicit novelty metric; evaluated across 31 organ-specific datasets to assess generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>BAIS-CTA annotation score: 0.376 ± 0.017 (full benchmark).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation same as reported annotation score; no additional calibration metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance inferior to human expert and organ-specific pretrained models, indicating decreased validated correctness on less familiar organ contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>CellAgent can generate multi-step analyses but validated outputs still fall short of experts, highlighting a generation/validation gap.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Underperforms humans using organ-specific CellTypist models; struggles on certain organs (ovary/testis) similarly to other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Multi-agent coordination and evaluator agent exist but did not close gap; authors suggest better external knowledge integration and improved execution fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CellAgent's multi-agent design improves workflow orchestration but still relies on external annotation tools and underperforms humans on validated annotation accuracy.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2156.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Aviary</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Aviary</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular platform for training language agents on scientific tasks (language decision processes) that supports tool use, multi-step reasoning, and agentic behavior using ReAct.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Aviary: training language agents on challenging scientific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Aviary</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>agentive LLM framework / multi-tool agent platform</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning; applied here to bioinformatics multiple-choice QA</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates sequences of reasoning steps, tool calls (code execution), and final answers to scientific questions; used to implement BAIS-SD with ReAct prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by running LLM base agents (GPT-4o, Deepseek-V3) within Aviary on BAIS-SD and scoring answers against curated multiple-choice ground truth; human experts used as baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Novelty implicit: BAIS-SD questions derived from recent publications; Aviary's success measured by ability to reason over fresh datasets rather than memorized facts.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>When hosting GPT-4o, BAIS-SD accuracy was 0.220 ± 0.015; with Deepseek-V3 0.254 ± 0.020. Neither matched human expert performance (0.762).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation performed externally by ground-truth correctness score; Aviary-enabled agents validated poorly on complex/mechanistic BAIS-SD questions.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Agents performed better on data-interpretation questions (disease analysis, key gene analysis) and worse on mechanistic or data-driven inference tasks, showing decreased validated accuracy as task novelty/complexity increased.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Aviary supports multi-step generation but hosted agents still fail to validate high-level biological conclusions, demonstrating persistent asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Out-of-distribution (novel mechanistic questions) performance poor relative to in-distribution/data-direct tasks; specific category-level trends reported.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>ReAct (reason+act) and modular tool use are implemented to improve credibility, but authors note need for richer external knowledge and deeper reasoning modules.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Aviary enables tool-aware multi-step reasoning, but LLM agents running in Aviary scored near-random on many novel data-driven scientific questions and far below human experts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2156.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art large language model used both to generate question items and as an agent (via Aviary) to answer BAIS-SD items and to generate code for BAIS-CTA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning; applied to single-cell analysis</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates natural-language summaries, multiple-choice questions (used to construct BAIS-SD), executable Python code for annotation, and direct answers to BAIS-SD questions when used as an agent.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validation in the paper is external: its outputs (annotations, BAIS-SD answers) are scored against ground-truth curated labels/questions; in question construction GPT-4o's summaries were used as source material but final questions validated against original papers.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Indirectly assessed via BAIS-SD performance on questions derived from novel single-cell studies; novelty effect measured by comparing per-category accuracies and human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>As an agent on BAIS-SD (via Aviary): accuracy 0.220 ± 0.015. As code-generator for BAIS-CTA, its produced pipeline achieved annotation score 0.298 ± 0.013.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Validation here is the correctness of its outputs vs curated answers (scores above); GPT-4o showed low validated correctness on BAIS-SD compared to humans.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Performance drops on more complex, mechanistic, or heterogeneity analysis questions; GPT-4o performs relatively better on direct data-interpretation tasks and worse on tasks requiring deeper mechanistic inference.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>GPT-4o can generate plausible-sounding outputs and code but validated correctness (per BAIS-SD and BAIS-CTA scoring) is substantially lower than humans, showing an asymmetry.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Lower performance on BAIS-SD novel reasoning tasks; in BAIS-CTA its code-based annotations underperform humans who choose organ-specific pretrained models.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Used both as question-construction LLM and as agent; authors suggest augmenting LLMs with external knowledge (RAG), improved tool usage, and human review to close gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4o is versatile (question construction, code generation, agent reasoning) but its validated correctness on novel, data-driven biological discovery tasks is far below PhD-level experts.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2156.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deepseek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deepseek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM used as a base agent within Aviary for BAIS-SD evaluation; compared to GPT-4o and human experts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Deepseek-V3</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>large language model</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific reasoning; applied to single-cell data QA</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates multi-step reasoning and final multiple-choice answers when run as an agent in Aviary.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Validated by BAIS-SD correctness scoring against curated ground-truth answers from literature-derived questions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Assessed indirectly via per-category accuracy drops on mechanistic/heterogeneity questions vs more direct data-interpretation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>BAIS-SD accuracy: 0.254 ± 0.020 (better than GPT-4o here but far below humans at 0.762).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured by BAIS-SD correctness; shows poor validated performance on novel mechanistic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Similar to GPT-4o: stronger on direct data-interpretation, weaker on mechanistic inference and heterogeneity analysis.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Capable of generating plausible answers but validated correctness remains low on novel scientific-discovery tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Poor on novel/mechanistic BAIS-SD categories relative to human experts; specific per-category differences reported qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Aviary/ReAct helps structure tool calls and reasoning but did not close performance gap; external knowledge and deeper reasoning modules suggested.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Deepseek-V3, when tasked with data-driven scientific QA, attains low validated accuracy relative to human experts, highlighting limits on novel scientific reasoning by current LLMs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2156.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CellTypist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CellTypist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A classical automated cell annotation tool providing multiple pretrained models tailored to different tissues; used by both AI systems and human experts for final annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CellTypist</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>pretrained supervised annotation tool / computational pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>single-cell transcriptomics / cell type annotation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates cell type label predictions for single-cell expression matrices using pretrained models specific to tissues or general-purpose models.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>In this work, validated by comparing predicted labels to uHAF expert-curated labels using the hierarchical S_CTA score; human expert using CellTypist achieved highest reported annotation score.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Pretrained model selection (organ-specific vs general) serves as proxy for familiarity; organ-specific pretrained models yielded substantially higher accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td>Human expert + CellTypist overall annotation score: 0.437 ± 0.014 (best reported). AI systems using CellTypist general model scored lower (e.g., AutoBA ~0.378).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td>Measured via S_CTA hierarchical annotation score; organ-specific models outperform general-purpose models indicating better validated correctness on familiar tissues.</td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td>Selection of an organ-appropriate pretrained model strongly improves validation; when AI defaulted to general-purpose model validation dropped compared to human-guided selection.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td>Tool can generate labels reliably when used with appropriate model; mismatch arises when AI does not select optimal model, causing generation to be less validated.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>General-purpose CellTypist model degrades on some organs; organ-specific pretrained models produce better validated performance for in-distribution tissues.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Better model selection (context-aware), integration with LLM agents for retrieval and selection could reduce gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CellTypist is a strong automated annotator when organ-appropriate models are chosen; AI agents' poor model selection explains part of the performance gap versus humans.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2156.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>scAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>scAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A recently proposed LLM agent focused on universal single-cell annotation (mentioned in related work but not evaluated in experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>scagent: Universal single-cell annotation via a llm agent.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>scAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM agent for single-cell annotation</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>single-cell transcriptomics / annotation</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Intended to generate cell-type annotations via LLM-driven inference (as reported in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not evaluated in this paper; mentioned as part of related work. No validation details provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a domain-specific LLM-based annotator; no experimental data in this paper to evaluate its generation/validation behavior.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2156.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioChatter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioChatter</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A generic backend library linking biomedical applications with conversational AI, mentioned as related work for enabling AI scientists in biology.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A platform for the biomedical application of large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioChatter</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>infrastructure / application platform for LLMs in biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>biomedical applications / conversational AI</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Provides infrastructure to enable biomedical applications and tool accessibility for LLM agents (platform-level capabilities rather than direct scientific output generation).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Not evaluated in this paper; mentioned in related work. No validation methods described here.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Platform aims to increase accessibility of biomedical tools to LLMs which could reduce gaps in integrating external knowledge and tooling.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as enabling infrastructure, but no experimental assessment in this work of its effect on generation-validation performance.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2156.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2156.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI or automated systems that generate scientific discoveries, hypotheses, predictions, or novel outputs, and how these systems validate or assess the correctness of their generations, particularly comparing performance on novel versus familiar tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sakana AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sakana AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An early MAS framework referenced as an example of multi-agent autonomous idea generation and experimental validation via code generation in ML research.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The ai scientist: Towards fully automated open-ended scientific discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Sakana AI Scientist</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent system for automated scientific idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general scientific discovery / machine learning research</td>
                        </tr>
                        <tr>
                            <td><strong>generation_capability</strong></td>
                            <td>Generates research ideas and experimental plans and can execute code to validate ideas (as described in cited works).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_method</strong></td>
                            <td>Referenced as performing experimental validation via code generation in original works; not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_effect_on_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generation_validation_asymmetry</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>calibration_quality</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_computational_cost</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_validation_required</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>gap_closing_mechanisms</strong></td>
                            <td>Multi-agent planning and code-execution-based validation used in original work; mentioned here as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_type</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of MAS that attempts autonomous idea generation and experimental validation; paper references such systems as motivating BaisBench but does not evaluate them directly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An ai agent for fully automated multi-omic analyses. <em>(Rating: 2)</em></li>
                <li>scchat: A large language model-powered co-pilot for contextualized single-cell rna sequencing analysis. <em>(Rating: 2)</em></li>
                <li>Cellagent: An llm-driven multi-agent framework for automated single-cell data analysis. <em>(Rating: 2)</em></li>
                <li>Aviary: training language agents on challenging scientific tasks. <em>(Rating: 2)</em></li>
                <li>The ai scientist: Towards fully automated open-ended scientific discovery. <em>(Rating: 2)</em></li>
                <li>A platform for the biomedical application of large language models. <em>(Rating: 1)</em></li>
                <li>scagent: Universal single-cell annotation via a llm agent. <em>(Rating: 1)</em></li>
                <li>Researchagent: Iterative research idea generation over scientific literature with large language models. <em>(Rating: 1)</em></li>
                <li>Agent laboratory: Using llm agents as research assistants. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2156",
    "paper_id": "paper-278535056",
    "extraction_schema_id": "extraction-schema-57",
    "extracted_data": [
        {
            "name_short": "BaisBench",
            "name_full": "Biological AI Scientist Benchmark (BaisBench)",
            "brief_description": "A data-driven benchmark introduced in this paper to evaluate AI scientists on real single-cell omics research, consisting of a cell type annotation task (BAIS-CTA) and a scientific discovery multiple-choice task (BAIS-SD).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BaisBench",
            "system_type": "benchmark / evaluation framework",
            "domain": "single-cell transcriptomics / biological discovery",
            "generation_capability": "Not a generative system; provides tasks that require AI systems to generate cell-type annotations, data analyses, and scientific conclusions/answers.",
            "validation_method": "Provides objective evaluation metrics: (1) hierarchical annotation score S_CTA for cell type annotation based on the uHAF hierarchical ontology; (2) multiple-choice correctness score S_SD for scientific-discovery questions (single-choice and multi-choice scoring rules). Benchmarks compare AI outputs against expert-curated ground truth and human experts as baselines.",
            "novelty_measure": "Implicit: novelty tested by task difficulty and by requiring derivation of discoveries from fresh datasets; novelty operationalized by BAIS-SD questions derived from recent studies (i.e., not memorization) and by organ-specific vs general pretrained models in BAIS-CTA.",
            "generation_performance": "Used to report AI performance rather than itself generating: reported AI annotation scores (AutoBA 0.378±0.014, CellAgent 0.376±0.017, GPT-4o via generated code 0.298±0.013, scChat 0.107±0.008, human+CellTypist 0.437±0.014) and BAIS-SD accuracies (GPT-4o 0.220±0.015, Deepseek-V3 0.254±0.020, human experts 0.762).",
            "validation_performance": "BaisBench's validation is deterministic: ground-truth annotations and curated question answers produce accuracy/score metrics as above. No probabilistic calibration metrics provided by the benchmark itself.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "BaisBench reveals worse validation scores for AI on tasks requiring novel data-driven reasoning (BAIS-SD) and on organs where pretrained models are less appropriate (e.g., ovary/testis). BAIS-SD accuracies for LLM agents (~0.22–0.25) are much lower than human (0.762), indicating strong negative effect of novelty on validation performance.",
            "generation_validation_asymmetry": "The benchmark documents an asymmetry: many AI systems can execute pre-programmed pipelines (generate outputs) but fail to produce or validate higher-level biological insights (low BAIS-SD scores); human experts outperform in both generation and validation.",
            "out_of_distribution_performance": "BaisBench exposes OOD weaknesses: AI systems using a general CellTypist model underperform humans who select organ-specific pretrained models; AI scores drop on tissues with challenging annotation (ovary/testis).",
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Benchmark highlights need for RAG, richer external knowledge access, multi-agent coordination, and human-in-the-loop review; does not itself implement gap-closing mechanisms.",
            "evidence_type": "supports",
            "key_findings": "BaisBench demonstrates that state-of-the-art AI scientists substantially underperform human experts on both structured annotation and data-driven scientific-discovery tasks, and that performance degrades notably on novel/complex reasoning tasks and out-of-distribution tissues.",
            "uuid": "e2156.0"
        },
        {
            "name_short": "AutoBA",
            "name_full": "AutoBA",
            "brief_description": "An LLM-agent system for automated multi-omic analyses applied to single-cell data analysis pipelines and cell type annotation.",
            "citation_title": "An ai agent for fully automated multi-omic analyses.",
            "mention_or_use": "use",
            "system_name": "AutoBA",
            "system_type": "LLM-driven agent / automated pipeline system",
            "domain": "single-cell / multi-omic biological analysis",
            "generation_capability": "Generates automated data-processing pipelines, code for preprocessing and downstream single-cell analyses, and cell type annotation outputs.",
            "validation_method": "Validated in this paper via BAIS-CTA hierarchical annotation score against expert-labeled annotations (uHAF), i.e., comparison to curated ground truth and human baseline.",
            "novelty_measure": "No explicit novelty metric; performance compared across organ datasets and against human expert and organ-specific pretrained models to assess generalization.",
            "generation_performance": "Annotation score on BAIS-CTA: 0.378 ± 0.014 (full benchmark).",
            "validation_performance": "Validation measured as the BAIS-CTA score above (comparison to curated labels); no finer-grained precision/recall provided.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance lower than human expert and degrades relative to organ-specific model selection by humans; indicates sensitivity to unfamiliar organ contexts and limited generalization.",
            "generation_validation_asymmetry": "AutoBA can generate executable pipelines and annotations but remains below expert-level validation scores, indicating a gap between generation capability and validated correctness.",
            "out_of_distribution_performance": "Shows reduced accuracy when compared to a human selecting organ-specific CellTypist models (human 0.437 vs AutoBA 0.378); specific OOD organ failures discussed qualitatively (ovary/testis remain difficult).",
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Relies on preprogrammed pipelines and CellTypist; paper suggests improved external knowledge access, better model selection, and human-in-loop could close gaps.",
            "evidence_type": "supports",
            "key_findings": "AutoBA performs automated single-cell analysis but still underperforms human experts in hierarchical cell annotation and struggles to generalize to organ-specific contexts without human-like model selection.",
            "uuid": "e2156.1"
        },
        {
            "name_short": "scChat",
            "name_full": "scChat",
            "brief_description": "An LLM-powered co-pilot for contextualized single-cell RNA-seq analysis that uses a RAG framework to retrieve marker gene information and perform annotation based on biological priors.",
            "citation_title": "scchat: A large language model-powered co-pilot for contextualized single-cell rna sequencing analysis.",
            "mention_or_use": "use",
            "system_name": "scChat",
            "system_type": "LLM-based agent with Retrieval-Augmented Generation (RAG)",
            "domain": "single-cell transcriptomics / cell type annotation",
            "generation_capability": "Generates annotation calls and interpretable reasoning grounded in retrieved marker genes and prior biological knowledge.",
            "validation_method": "In this evaluation, validated by BAIS-CTA hierarchical annotation score (comparison to expert-curated uHAF labels); scChat's annotation restricted to organs it supports (pre-encoded marker sets).",
            "novelty_measure": "Implicit: scChat supports only tissues/organs with pre-encoded marker databases, so novelty measured by whether an organ is supported (familiar) or unsupported (novel).",
            "generation_performance": "BAIS-CTA annotation score on supported subset: 0.107 ± 0.008 (lowest among tested methods on that subset).",
            "validation_performance": "Measured via BAIS-CTA score (very low); no precision/recall numbers provided.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Fails to generalize to novel organs lacking pre-encoded markers because of rigid marker-based approach; performance limited to supported (familiar) tissues.",
            "generation_validation_asymmetry": "scChat's use of priors leads to conservative generation but poor validated accuracy across many organs; indicates a mismatch where prior-only annotation is insufficient for broad validation.",
            "out_of_distribution_performance": "Poor: cannot complete full benchmark for unsupported organs and performs poorly even on supported organs relative to human experts and other AI that use broader pretrained tools.",
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "RAG for marker retrieval is used, but scChat lacks flexible model selection and adaptation; paper suggests adding external knowledge access and more flexible pipelines.",
            "evidence_type": "supports",
            "key_findings": "scChat's marker-prior approach yields low annotation accuracy and limited generalization; dependence on pre-encoded organ markers prevents handling novel tissues, illustrating validation failure on OOD inputs.",
            "uuid": "e2156.2"
        },
        {
            "name_short": "CellAgent",
            "name_full": "CellAgent",
            "brief_description": "A role-based multi-agent framework (planner, executor, evaluator) that uses LLM coordination to derive research conclusions for single-cell data.",
            "citation_title": "Cellagent: An llm-driven multi-agent framework for automated single-cell data analysis.",
            "mention_or_use": "use",
            "system_name": "CellAgent",
            "system_type": "LLM-driven multi-agent system",
            "domain": "single-cell transcriptomics / automated analysis",
            "generation_capability": "Generates analysis plans, code, and cell type annotations through coordinated multi-agent roles.",
            "validation_method": "Validated here via BAIS-CTA hierarchical annotation score against expert uHAF labels; CellAgent primarily used CellTypist for final annotation in experiments.",
            "novelty_measure": "No explicit novelty metric; evaluated across 31 organ-specific datasets to assess generalization.",
            "generation_performance": "BAIS-CTA annotation score: 0.376 ± 0.017 (full benchmark).",
            "validation_performance": "Validation same as reported annotation score; no additional calibration metrics provided.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance inferior to human expert and organ-specific pretrained models, indicating decreased validated correctness on less familiar organ contexts.",
            "generation_validation_asymmetry": "CellAgent can generate multi-step analyses but validated outputs still fall short of experts, highlighting a generation/validation gap.",
            "out_of_distribution_performance": "Underperforms humans using organ-specific CellTypist models; struggles on certain organs (ovary/testis) similarly to other methods.",
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Multi-agent coordination and evaluator agent exist but did not close gap; authors suggest better external knowledge integration and improved execution fidelity.",
            "evidence_type": "supports",
            "key_findings": "CellAgent's multi-agent design improves workflow orchestration but still relies on external annotation tools and underperforms humans on validated annotation accuracy.",
            "uuid": "e2156.3"
        },
        {
            "name_short": "Aviary",
            "name_full": "Aviary",
            "brief_description": "A modular platform for training language agents on scientific tasks (language decision processes) that supports tool use, multi-step reasoning, and agentic behavior using ReAct.",
            "citation_title": "Aviary: training language agents on challenging scientific tasks.",
            "mention_or_use": "use",
            "system_name": "Aviary",
            "system_type": "agentive LLM framework / multi-tool agent platform",
            "domain": "general scientific reasoning; applied here to bioinformatics multiple-choice QA",
            "generation_capability": "Generates sequences of reasoning steps, tool calls (code execution), and final answers to scientific questions; used to implement BAIS-SD with ReAct prompting.",
            "validation_method": "Validated by running LLM base agents (GPT-4o, Deepseek-V3) within Aviary on BAIS-SD and scoring answers against curated multiple-choice ground truth; human experts used as baseline.",
            "novelty_measure": "Novelty implicit: BAIS-SD questions derived from recent publications; Aviary's success measured by ability to reason over fresh datasets rather than memorized facts.",
            "generation_performance": "When hosting GPT-4o, BAIS-SD accuracy was 0.220 ± 0.015; with Deepseek-V3 0.254 ± 0.020. Neither matched human expert performance (0.762).",
            "validation_performance": "Validation performed externally by ground-truth correctness score; Aviary-enabled agents validated poorly on complex/mechanistic BAIS-SD questions.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Agents performed better on data-interpretation questions (disease analysis, key gene analysis) and worse on mechanistic or data-driven inference tasks, showing decreased validated accuracy as task novelty/complexity increased.",
            "generation_validation_asymmetry": "Aviary supports multi-step generation but hosted agents still fail to validate high-level biological conclusions, demonstrating persistent asymmetry.",
            "out_of_distribution_performance": "Out-of-distribution (novel mechanistic questions) performance poor relative to in-distribution/data-direct tasks; specific category-level trends reported.",
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "ReAct (reason+act) and modular tool use are implemented to improve credibility, but authors note need for richer external knowledge and deeper reasoning modules.",
            "evidence_type": "supports",
            "key_findings": "Aviary enables tool-aware multi-step reasoning, but LLM agents running in Aviary scored near-random on many novel data-driven scientific questions and far below human experts.",
            "uuid": "e2156.4"
        },
        {
            "name_short": "GPT-4o",
            "name_full": "GPT-4o",
            "brief_description": "A state-of-the-art large language model used both to generate question items and as an agent (via Aviary) to answer BAIS-SD items and to generate code for BAIS-CTA.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o",
            "system_type": "large language model",
            "domain": "general scientific reasoning; applied to single-cell analysis",
            "generation_capability": "Generates natural-language summaries, multiple-choice questions (used to construct BAIS-SD), executable Python code for annotation, and direct answers to BAIS-SD questions when used as an agent.",
            "validation_method": "Validation in the paper is external: its outputs (annotations, BAIS-SD answers) are scored against ground-truth curated labels/questions; in question construction GPT-4o's summaries were used as source material but final questions validated against original papers.",
            "novelty_measure": "Indirectly assessed via BAIS-SD performance on questions derived from novel single-cell studies; novelty effect measured by comparing per-category accuracies and human baseline.",
            "generation_performance": "As an agent on BAIS-SD (via Aviary): accuracy 0.220 ± 0.015. As code-generator for BAIS-CTA, its produced pipeline achieved annotation score 0.298 ± 0.013.",
            "validation_performance": "Validation here is the correctness of its outputs vs curated answers (scores above); GPT-4o showed low validated correctness on BAIS-SD compared to humans.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Performance drops on more complex, mechanistic, or heterogeneity analysis questions; GPT-4o performs relatively better on direct data-interpretation tasks and worse on tasks requiring deeper mechanistic inference.",
            "generation_validation_asymmetry": "GPT-4o can generate plausible-sounding outputs and code but validated correctness (per BAIS-SD and BAIS-CTA scoring) is substantially lower than humans, showing an asymmetry.",
            "out_of_distribution_performance": "Lower performance on BAIS-SD novel reasoning tasks; in BAIS-CTA its code-based annotations underperform humans who choose organ-specific pretrained models.",
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Used both as question-construction LLM and as agent; authors suggest augmenting LLMs with external knowledge (RAG), improved tool usage, and human review to close gaps.",
            "evidence_type": "supports",
            "key_findings": "GPT-4o is versatile (question construction, code generation, agent reasoning) but its validated correctness on novel, data-driven biological discovery tasks is far below PhD-level experts.",
            "uuid": "e2156.5"
        },
        {
            "name_short": "Deepseek-V3",
            "name_full": "Deepseek-V3",
            "brief_description": "An LLM used as a base agent within Aviary for BAIS-SD evaluation; compared to GPT-4o and human experts.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Deepseek-V3",
            "system_type": "large language model",
            "domain": "general scientific reasoning; applied to single-cell data QA",
            "generation_capability": "Generates multi-step reasoning and final multiple-choice answers when run as an agent in Aviary.",
            "validation_method": "Validated by BAIS-SD correctness scoring against curated ground-truth answers from literature-derived questions.",
            "novelty_measure": "Assessed indirectly via per-category accuracy drops on mechanistic/heterogeneity questions vs more direct data-interpretation questions.",
            "generation_performance": "BAIS-SD accuracy: 0.254 ± 0.020 (better than GPT-4o here but far below humans at 0.762).",
            "validation_performance": "Measured by BAIS-SD correctness; shows poor validated performance on novel mechanistic tasks.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Similar to GPT-4o: stronger on direct data-interpretation, weaker on mechanistic inference and heterogeneity analysis.",
            "generation_validation_asymmetry": "Capable of generating plausible answers but validated correctness remains low on novel scientific-discovery tasks.",
            "out_of_distribution_performance": "Poor on novel/mechanistic BAIS-SD categories relative to human experts; specific per-category differences reported qualitatively.",
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Aviary/ReAct helps structure tool calls and reasoning but did not close performance gap; external knowledge and deeper reasoning modules suggested.",
            "evidence_type": "supports",
            "key_findings": "Deepseek-V3, when tasked with data-driven scientific QA, attains low validated accuracy relative to human experts, highlighting limits on novel scientific reasoning by current LLMs.",
            "uuid": "e2156.6"
        },
        {
            "name_short": "CellTypist",
            "name_full": "CellTypist",
            "brief_description": "A classical automated cell annotation tool providing multiple pretrained models tailored to different tissues; used by both AI systems and human experts for final annotations.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "CellTypist",
            "system_type": "pretrained supervised annotation tool / computational pipeline",
            "domain": "single-cell transcriptomics / cell type annotation",
            "generation_capability": "Generates cell type label predictions for single-cell expression matrices using pretrained models specific to tissues or general-purpose models.",
            "validation_method": "In this work, validated by comparing predicted labels to uHAF expert-curated labels using the hierarchical S_CTA score; human expert using CellTypist achieved highest reported annotation score.",
            "novelty_measure": "Pretrained model selection (organ-specific vs general) serves as proxy for familiarity; organ-specific pretrained models yielded substantially higher accuracy.",
            "generation_performance": "Human expert + CellTypist overall annotation score: 0.437 ± 0.014 (best reported). AI systems using CellTypist general model scored lower (e.g., AutoBA ~0.378).",
            "validation_performance": "Measured via S_CTA hierarchical annotation score; organ-specific models outperform general-purpose models indicating better validated correctness on familiar tissues.",
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": "Selection of an organ-appropriate pretrained model strongly improves validation; when AI defaulted to general-purpose model validation dropped compared to human-guided selection.",
            "generation_validation_asymmetry": "Tool can generate labels reliably when used with appropriate model; mismatch arises when AI does not select optimal model, causing generation to be less validated.",
            "out_of_distribution_performance": "General-purpose CellTypist model degrades on some organs; organ-specific pretrained models produce better validated performance for in-distribution tissues.",
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": true,
            "gap_closing_mechanisms": "Better model selection (context-aware), integration with LLM agents for retrieval and selection could reduce gaps.",
            "evidence_type": "supports",
            "key_findings": "CellTypist is a strong automated annotator when organ-appropriate models are chosen; AI agents' poor model selection explains part of the performance gap versus humans.",
            "uuid": "e2156.7"
        },
        {
            "name_short": "scAgent",
            "name_full": "scAgent",
            "brief_description": "A recently proposed LLM agent focused on universal single-cell annotation (mentioned in related work but not evaluated in experiments here).",
            "citation_title": "scagent: Universal single-cell annotation via a llm agent.",
            "mention_or_use": "mention",
            "system_name": "scAgent",
            "system_type": "LLM agent for single-cell annotation",
            "domain": "single-cell transcriptomics / annotation",
            "generation_capability": "Intended to generate cell-type annotations via LLM-driven inference (as reported in cited work).",
            "validation_method": "Not evaluated in this paper; mentioned as part of related work. No validation details provided here.",
            "novelty_measure": null,
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": null,
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": null,
            "evidence_type": "neutral",
            "key_findings": "Mentioned as a domain-specific LLM-based annotator; no experimental data in this paper to evaluate its generation/validation behavior.",
            "uuid": "e2156.8"
        },
        {
            "name_short": "BioChatter",
            "name_full": "BioChatter",
            "brief_description": "A generic backend library linking biomedical applications with conversational AI, mentioned as related work for enabling AI scientists in biology.",
            "citation_title": "A platform for the biomedical application of large language models.",
            "mention_or_use": "mention",
            "system_name": "BioChatter",
            "system_type": "infrastructure / application platform for LLMs in biomedicine",
            "domain": "biomedical applications / conversational AI",
            "generation_capability": "Provides infrastructure to enable biomedical applications and tool accessibility for LLM agents (platform-level capabilities rather than direct scientific output generation).",
            "validation_method": "Not evaluated in this paper; mentioned in related work. No validation methods described here.",
            "novelty_measure": null,
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": null,
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": "Platform aims to increase accessibility of biomedical tools to LLMs which could reduce gaps in integrating external knowledge and tooling.",
            "evidence_type": "mention",
            "key_findings": "Mentioned as enabling infrastructure, but no experimental assessment in this work of its effect on generation-validation performance.",
            "uuid": "e2156.9"
        },
        {
            "name_short": "Sakana AI Scientist",
            "name_full": "Sakana AI Scientist",
            "brief_description": "An early MAS framework referenced as an example of multi-agent autonomous idea generation and experimental validation via code generation in ML research.",
            "citation_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "mention_or_use": "mention",
            "system_name": "Sakana AI Scientist",
            "system_type": "multi-agent system for automated scientific idea generation",
            "domain": "general scientific discovery / machine learning research",
            "generation_capability": "Generates research ideas and experimental plans and can execute code to validate ideas (as described in cited works).",
            "validation_method": "Referenced as performing experimental validation via code generation in original works; not evaluated in this paper.",
            "novelty_measure": null,
            "generation_performance": null,
            "validation_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_effect_on_validation": null,
            "generation_validation_asymmetry": null,
            "out_of_distribution_performance": null,
            "calibration_quality": null,
            "validation_computational_cost": null,
            "human_validation_required": null,
            "gap_closing_mechanisms": "Multi-agent planning and code-execution-based validation used in original work; mentioned here as related work.",
            "evidence_type": "mention",
            "key_findings": "Cited as an example of MAS that attempts autonomous idea generation and experimental validation; paper references such systems as motivating BaisBench but does not evaluate them directly.",
            "uuid": "e2156.10"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An ai agent for fully automated multi-omic analyses.",
            "rating": 2
        },
        {
            "paper_title": "scchat: A large language model-powered co-pilot for contextualized single-cell rna sequencing analysis.",
            "rating": 2
        },
        {
            "paper_title": "Cellagent: An llm-driven multi-agent framework for automated single-cell data analysis.",
            "rating": 2
        },
        {
            "paper_title": "Aviary: training language agents on challenging scientific tasks.",
            "rating": 2
        },
        {
            "paper_title": "The ai scientist: Towards fully automated open-ended scientific discovery.",
            "rating": 2
        },
        {
            "paper_title": "A platform for the biomedical application of large language models.",
            "rating": 1
        },
        {
            "paper_title": "scagent: Universal single-cell annotation via a llm agent.",
            "rating": 1
        },
        {
            "paper_title": "Researchagent: Iterative research idea generation over scientific literature with large language models.",
            "rating": 1
        },
        {
            "paper_title": "Agent laboratory: Using llm agents as research assistants.",
            "rating": 1
        }
    ],
    "cost": 0.01898775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmarking AI scientists in omics data-driven biological research
13 May 2025</p>
<p>Erpai Luo 
Tsinghua University</p>
<p>Jinmeng Jia 
Tsinghua University</p>
<p>Yifan Xiong 
Tsinghua University</p>
<p>Xiangyu Li 
Beijing Jiaotong University</p>
<p>Xiaobo Guo 
Capital Medical University</p>
<p>Baoqi Yu 
Capital Medical University</p>
<p>Lei Wei 
Tsinghua University</p>
<p>Xuegong Zhang zhangxg@mail.tsinghua.edu.cn 
Tsinghua University</p>
<p>Benchmarking AI scientists in omics data-driven biological research
13 May 202510090E166669DBA6092A41A47FF088D6arXiv:2505.08341v1[cs.AI]
The rise of large language models and multi-agent systems has sparked growing interest in AI scientists capable of autonomous biological research.However, existing benchmarks either focus on reasoning without data or on data analysis with predefined statistical answers, lacking realistic, data-driven evaluation settings.Here, we introduce the Biological AI Scientist Benchmark (BaisBench), a benchmark designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge.BaisBench comprises two tasks: cell type annotation on 31 expert-labeled single-cell datasets, and scientific discovery through answering 198 multiple-choice questions derived from the biological insights of 41 recent single-cell studies.Systematic experiments on state-of-the-art AI scientists and LLM agents showed that while promising, current models still substantially underperform human experts on both tasks.We hope BaisBench will fill this gap and serve as a foundation for advancing and evaluating AI models for scientific discovery.The benchmark can be found at: https://github.com/EperLuo/BaisBench.</p>
<p>Introduction</p>
<p>Biological research has long been a complex and expertise-intensive endeavor.From data collection and processing to pattern discovery and hypothesis generation, the process demands not only deep domain knowledge but also strong analytical capabilities.As the volume and complexity of biological data continue to grow, the expectations placed on researchers have become increasingly overwhelming.Scientists must now navigate vast datasets, master diverse computational tools, and integrate knowledge across multiple biological domains, which has been an increasingly difficult and unsustainable burden for individuals alone.</p>
<p>A prominent example of this complexity is omics data-driven biological research.Omics data, including genomics, transcriptomics, proteomics, and metabolomics, now form the backbone of many biological studies, providing comprehensive and system-level views of cellular and molecular processes.However, these data are highly dimensional, noisy, and deeply dependent on biological context.Making sense of them requires not only advanced data processing pipelines and rigorous statistical models, but also the ability to reason across multiple layers of biological knowledge and integrate diverse sources of information.An LLM agent was used to extract key data-driven biological conclusions from the literature and convert them into multiple-choice questions.AI scientists are asked to analyze the data and select the correct answers.A group of human experts also completed the same tasks, allowing for comparative evaluation between AI and human performance.</p>
<p>Faced with these challenges, there is growing interest in developing intelligent systems that can support or even automate parts of the scientific discovery process.Among these efforts, large language models (LLMs) have emerged as a powerful foundation for building multi-agent systems (MASs) capable of reasoning over scientific data.Such systems, often referred to as AI scientists, are designed to autonomously analyze experimental datasets, interpret results in the context of existing knowledge, and generate scientific insights [Gottweis et al., 2025, Lu et al., 2024a, Yamada et al., 2025, Su et al., 2024].Early studies have demonstrated the potential of AI scientists in automatic biological research and scientific discovery [Zhang et al., 2024, Reddy andShojaee, 2025].Despite these promising developments, a fundamental gap remains: how to rigorously and quantitatively evaluate the performance of AI scientists in real biological research [Reddy and Shojaee, 2025].Scientific discovery is inherently open-ended, and the quality of research findings is difficult to measure using standard metrics.One common strategy is to ask human experts to assess the relevance and novelty of AI-generated conclusions [Gottweis et al., 2025, Schmidgall et al., 2025], while others propose using specialized LLM agents to simulate expert review [Baek et al., 2024].Yet, both approaches remain subjective and difficult to scale.</p>
<p>To move toward more objective and quantitative evaluation, some benchmarks assess AI systems, where AI agents are asked to answer the question based on their knowledge and the background information from the question [Qi et al., 2024, Laurent et al., 2024].While useful for testing factual recall or reasoning, these tasks fail to reflect the data-driven nature of biological research, where new insights typically emerge from the analysis of freshly generated omics datasets.The BLADE benchmark [Gu et al., 2024] addresses this to some extent by offering data-driven questions, but it focuses on domains such as education, behavioral science, and evolutionary biology, with limited relevance to core areas of molecular and cellular biology.Bixbench [Mitchener et al., 2025] goes further by grounding tasks in real omics data from more than 50 biological scenarios.However, it focuses primarily on computational outputs rather than on evaluating whether the AI scientist can generate meaningful biological findings from the data.These limitations highlight the need for a benchmark that focuses on the core goals of biological research: deriving interpretable, insight-rich conclusions from complex biological data.</p>
<p>Here, we present BaisBench (Biological AI Scientist Benchmark), a benchmark specifically designed to evaluate the ability of AI scientists to perform discovery tasks using real biological omics data.BaisBench is constructed using single-cell transcriptomic datasets, a widely used omics data modality in modern biology.The benchmark comprises two tasks (Figure 1A):</p>
<p>• Cell type annotation task (BAIS-CTA): This task includes 31 single-cell datasets to assess AI scientists' ability to annotate cell types, a fundamental challenge in single-cell analysis.</p>
<p>To enable comprehensive evaluation, we introduce a novel metric based on a hierarchical cell type ontology that better captures both annotation accuracy.</p>
<p>• Scientific discovery task (BAIS-SD): This task consists of 198 multiple-choice questions derived from biological insights reported in 41 published single-cell studies.AI scientists are required to analyze the corresponding data, extract relevant insights, and select the correct answer consistent with real scientific findings.</p>
<p>We evaluate several latest AI scientists specialized in single-cell research on BaisBench.The results reveal that despite their promising reasoning capabilities, existing AI scientists still face significant challenges when performing data-driven scientific discovery on real-world biological data.All the tested AI scientists substantially underperform human experts on cell type labeling tasks.Furthermore, the LLM's performance on multiple-choice questions based on real biological data and scenarios is still lacking, which is only about the same level as random choice.Overall, our benchmark highlights the gap between current biological AI scientists and automatic biological discovery.We hope this evaluation benchmark can accelerate the development of AI scientists in biology and help the realization of automatic scientific discovery.</p>
<p>Background</p>
<p>Biological research paradigm based on single-cell omics</p>
<p>Single-cell sequencing technologies have transformed biological research by enabling molecular profiling at the resolution of individual cells.These technologies allow researchers to dissect cellular heterogeneity, uncover rare cell populations, and study dynamic biological processes at unprecedented resolution.Among them, single-cell RNA sequencing (scRNA-seq) has become a cornerstone, providing high-dimensional gene expression data across a wide range of tissues, conditions, and disease states [Svensson et al., 2018, Zheng et al., 2017].</p>
<p>A typical scRNA-seq analysis workflow includes multiple stages: raw data preprocessing, dimensionality reduction, clustering, and cell type annotation, followed by downstream analyses such as differential gene expression, trajectory inference, and cell-cell communication analysis.Researchers need to first finish the basic pipeline and then select appropriate tools to do further analysis.They need to combine these analysis results with their profound biological knowledge to dissect the complex biological systems and obtain useful insights [Luecken andTheis, 2019, Heumos et al., 2023].The whole process demands high analytical skills and rich external knowledge, making research based on single-cell omics a highly complex and expertise-intensive endeavor.</p>
<p>AI scientist for automatic scientific discovery</p>
<p>An AI scientist refers to a MAS specifically designed to perform scientific research.By leveraging the extensive scientific knowledge encoded in LLMs and the coordinated workflows enabled by MAS architectures, AI scientists are capable of autonomously generating research ideas, formulating hypotheses, designing and conducting experiments, and ultimately producing novel scientific insights.</p>
<p>Several recent studies have demonstrated the potential of this paradigm.The Sakana AI Scientist [Lu et al., 2024a, Yamada et al., 2025] was among the first to adopt a MAS framework for autonomously generating research ideas in the machine learning domain and implementing experimental validation through code generation.Agent Laboratory [Schmidgall et al., 2025] simulated a virtual research lab environment composed of LLM-based agents, each assigned specific roles (such as postdoctoral researchers or PhD students) to collaboratively conduct scientific investigations.ResearchAgent [Baek et al., 2024] further advanced this concept by enabling LLM agents to iteratively generate and refine research ideas grounded in existing scientific literature.Collectively, these systems illustrate the growing promise of AI scientists in accelerating and potentially transforming the process of scientific discovery.</p>
<p>AI scientist for automatic biological research</p>
<p>In the domain of biological research, particularly single-cell analysis, several AI scientist systems have recently been developed.AutoBA [Zhou et al., 2024] utilizes an LLM agent to construct automated pipelines for preprocessing and analyzing single-cell data.scChat [Lu et al., 2024b] combines a MAS architecture with the Retrieval-Augmented Generation (RAG) framework [Lewis et al., 2020], enabling autonomous analysis of given single-cell datasets.CellAgent [Xiao et al., 2024] adopts a role-based architecture, comprising planner, executor, and evaluator agents that collaboratively derive research conclusions through iterative reasoning.scAgent [Mao et al., 2025] is specifically designed for automated cell type annotation, a core task in single-cell transcriptomics.BioChatter [Lobentanzer et al., 2025] implements a generic backend library for linking biomedical applications with conversational AI, expanding tool accessibility for AI scientists.</p>
<p>Together, these systems demonstrate the growing capability of AI scientists to operate in biological research workflows, particularly in navigating the complexity of single-cell omics.In this work, we aim to systematically evaluate these AI scientists using our proposed BaisBench benchmark.</p>
<p>Methods</p>
<p>To build a data-driven evaluation benchmark grounded in real-world biological research, we developed BaisBench, which consists of two distinct tasks.The cell type annotation task (BAIS-CTA) is a fundamental task in single-cell biology, which evaluates an AI scientist's ability to identify cell identities based on gene expression profiles.The scientific discovery task (BAIS-SD) is a multiplechoice question-answering task designed to assess biological reasoning and insight generation.Questions are derived from the key discoveries reported in 41 published single-cell studies and span a wide range of downstream analytical tasks common in the field.BAIS-SD includes 198 unique questions, each paired with the corresponding single-cell dataset used in the original study.The benchmark design and construction process for both tasks are described in detail below.</p>
<p>The cell type annotation task (BAIS-CTA)</p>
<p>The BAIS-CTA task focuses on a core challenge in single-cell research: cell type annotation.This task integrates both data preprocessing and analysis, and it inherently depends on domain-specific biological knowledge, making it well-suited for benchmarking the capabilities of AI scientists.</p>
<p>As illustrated in Figure 1B, we curated 31 scRNA-seq datasets from recent publications, each representing a distinct organ or tissue.All datasets are provided in the standardized h5ad format, with gene expression values stored as raw counts.A team of bioinformaticians manually annotated the cell types in each dataset based on the hierarchical taxonomy defined by the Unified Hierarchical cell Annotation Framework (uHAF) [Bian et al., 2025].Detailed information for each dataset is provided in Table S1.</p>
<p>The objective of this task is to annotate the cell type of each individual cell in a given single-cell dataset derived from a specific organ or tissue.As illustrated in Figure 2, the AI scientist is free to employ any preferred method or pipeline for cell type annotation.Once the predictions are obtained, we evaluate performance using a hierarchical annotation metric based on the cell type taxonomy defined in uHAF.Specifically, in uHAF, each organ or tissue is associated with a hierarchical tree structure that captures relationships among cell types at multiple levels of granularity.For example, in the bone marrow, the cell type Dendritic cell belongs to the broader category Myeloid cell, while its subtypes include Monocyte-derived dendritic cell, Plasmacytoid dendritic cell, and others.To account for this structure, we define an annotation score S CTA that reflects both accuracy and granularity awareness.The scoring rules are as follows:</p>
<p>1.A prediction that exactly matches the true cell type or corresponds to any of its descendant nodes in the hierarchy receives 1 point.</p>
<ol>
<li>
<p>A prediction at the immediate parent level receives 0.5 points.</p>
</li>
<li>
<p>A prediction at the grandparent level (or equivalent higher-level ancestor) receives 0.2 points.4. All other predictions receive 0 points.</p>
</li>
</ol>
<p>The final annotation score is computed as:
S CTA = n 1 × 1 + n 2 × 0.5 + n 3 × 0.2 n cell (1)
where n 1 , n 2 , and n 3 denote the number of predictions at the exact (or finer), parent, and grandparent levels respectively, and n cell is the total number of cells in the dataset.</p>
<p>Figure 2: Pipeline of the BAIS-CTA task.The AI scientist is provided with a single-cell gene expression dataset from a specific organ and is required to perform cell type annotation using its own chosen method.The predicted annotations are then evaluated using a hierarchical scoring metric based on the uHAF cell type tree, which quantifies performance according to the granularity and correctness of the predictions.</p>
<p>The scientific discovery task (BAIS-SD)</p>
<p>For the BAIS-SD task, we constructed a set of multiple-choice questions derived from the biological discoveries reported in recently published single-cell research papers.This task is designed to evaluate the ability of AI scientists to generate biologically meaningful insights in realistic research scenarios.</p>
<p>It requires not only effective data processing and analysis, but also appropriate tool usage, substantial domain-specific knowledge, and strong reasoning capabilities.</p>
<p>Specifically, we curated 41 recently published single-cell studies along with their corresponding datasets from the CellxGene platform [Program et al., 2025].Each dataset contains between 5,600 and 122,000 cells and includes the necessary metadata to support downstream analysis.Detailed information for all selected papers and datasets is provided in Table S2.Based on the key data-driven findings from each study, we constructed both single-choice and multiple-choice questions (Figure 1C and Figure 3A).Specifically, we collected the datasets from the Cellxgene website, and we also obtained the original papers from which these datasets came.We sent these papers to the GPT-4o and asked it to summarize the biological discoveries that were derived from the single-cell data and use these discoveries to construct multiple-choice questions.The details of question construction can be found in Appendix C.1.The distribution of single-versus multiple-option questions is shown in Figure S1.</p>
<p>As illustrated in Figure 3B, we categorized the questions into several major topics, including key gene analysis, cell heterogeneity analysis, cellular functional reasoning.Unlike previous benchmarks that primarily evaluate computational analysis capabilities, BaisBench emphasizes the generation of novel biological understanding, such as gene and cell functions, disease mechanisms, and developmental trajectories.Several example questions are presented below to illustrate the purpose and scope of this task.</p>
<p>Examples of multi-choice questions</p>
<p>What role did FOXL1+ fibroblasts play in intestinal development, as observed in the data?</p>
<p>• (A) They were involved in crypt-villus differentiation by expressing BMP ligands.</p>
<p>• (B) They inhibited the differentiation of intestinal stem cells in the fetal gut.</p>
<p>• (C) They acted as the major source of WNT3A in the developing intestine.</p>
<p>• (D) They were the primary source of VEGF for endothelial cell development.</p>
<p>Based on a combination of transcriptomic data and external knowledge, which of the following factors are proposed to contribute to increased proliferation in foreskin keratinocytes?</p>
<p>• As shown in Figure 3A, each AI scientist is provided with a dataset and relevant background context, and is required to answer five multiple-choice questions.The AI scientists must preprocess the data, conduct relevant analyses using appropriate tools, and reason through the question in order to select the correct options.The accuracy of the selected answers is used as the evaluation metric.For single-choice questions, a correct answer receives one point; incorrect answers receive zero.For multiple-choice questions, one point is awarded if at least one correct option is selected; otherwise, no points are given.Based on this scoring scheme, we define the overall correctness score S SD as:
S SD = n single + n multiple n Q (2)
where n single is the number of correctly answered single-choice questions, n multiple is the number of correctly answered multiple-choice questions, and n Q is the total number of questions.</p>
<p>4 Results</p>
<p>The performance of AI scientists on the BAIS-CTA task</p>
<p>For the BAIS-CTA task, we evaluated three AI scientists specifically designed for single-cell analysis: AutoBA [Zhou et al., 2024], scChat [Lu et al., 2024b], and CellAgent [Xiao et al., 2024].To enable more comprehensive participation, we additionally employed GPT-4o to generate executable Python code for cell type annotation, which was then applied to the task.As a baseline, we asked a human expert to finish this task with CellTypist [Domínguez Conde et al., 2022], a classical tool for automated cell annotation, following its official tutorial.It is worth noting that due to scChat's limited support for certain organs, it was unable to complete the full benchmark.For the subset of organs supported by scChat, we separately report the performance of all evaluated methods for fair comparison.Each method was executed five times to ensure statistically stable results.</p>
<p>During the experiments, we observed that most AI scientists, including AutoBA, CellAgent, and GPT-4o, primarily relied on CellTypist for cell type annotation.In contrast, scChat employed a different strategy, using marker gene information retrieved through a retrieval-augmented generation (RAG) framework to perform annotation based on biological prior knowledge.</p>
<p>As shown in Table 1, the best performance was achieved by a human expert using CellTypist, with an annotation score of 0.42 on the full set of organ types and 0.38 on the subset supported by scChat.Among the AI scientists, AutoBA and CellAgent achieved the highest scores, approximately 0.37 on the full benchmark and 0.30 on the scChat subset.Although GPT-4o also used CellTypist, it relied only on highly variable genes for annotation, in contrast to the official CellTypist recommendation of providing the complete expression matrix, including non-expressed genes.This likely contributed to its lower performance, which reached 0.29 on the full benchmark and 0.25 on the scChat subset.scChat, which does not utilize external annotation tools and instead relies solely on prior biological knowledge, achieved the lowest score, with 0.10 on the subset of organ types it supports.These results suggest that while multi-agent AI scientists demonstrate improved capabilities over single-agent systems in data-driven biological research, they still fall short of expert-level performance.</p>
<p>We further investigated the differing usage patterns of CellTypist between human experts and AI scientists.CellTypist provides a range of pretrained models, each tailored to specific tissues and biological contexts.However, we observed that AI scientists typically defaulted to the basic model, which was trained on immune cell subpopulations aggregated from 20 tissues across 18 studies.In contrast, human experts selectively applied pretrained models that were most relevant to the tissue of origin in each dataset.This informed model selection likely contributed to the superior annotation performance, highlighting the importance of contextual knowledge and tool-specific expertise in achieving accurate cell type identification.We analyzed the performance of different methods across individual organs.As shown in Figure 4, which presents results for the organ subset supported by scChat, the human expert using CellTypist consistently achieved high annotation accuracy across most organs, while scChat exhibited the lowest performance in the majority of cases.The three AI scientists, AutoBA, GPT-4o, and CellAgent-all, utilized CellTypist for annotation and demonstrated comparable accuracy patterns across organs, mirroring their overall performance trends.</p>
<p>Notably, CellTypist models pretrained on specific organs, such as the eye, heart, and liver, yielded substantially higher accuracy compared to the AI scientists, who predominantly relied on the generalpurpose model.In contrast, all methods, including both AI scientists and human experts, performed poorly on certain organs such as the ovary and testis, suggesting that cell type annotation in these tissues remains particularly challenging and may require further methodological advancement.</p>
<p>The performance of AI scientists on the BAIS-SD task</p>
<p>We initially evaluated AI scientists that were specifically developed for single-cell biology.While these systems were capable of processing and analyzing single-cell data, they lacked the ability to interpret results and formulate biological insights in response to our benchmark questions.As a result, none of them were able to complete the BAIS-SD task.</p>
<p>To enable systematic evaluation, we turned to the Aviary framework [Narayanan et al., 2024], a modular platform for agents that supports tool use and multi-step reasoning.Aviary simulates a simplified version of the research workflow of a bioinformatician, and is capable of analyzing input data and responding to scientific questions.We implemented the task using the ReAct prompting strategy [Yao et al., 2023] to enable tool-aware agentic reasoning.Additional implementation details are provided in the Appendix.</p>
<p>Within this framework, we evaluated two state-of-the-art LLMs (GPT-4o and Deepseek-V3) as base agents on the BAIS-SD multiple-choice question set.Each model was run 5 times to ensure statistical stability.To provide a human performance reference, we also invited 5 PhD-level bioinformaticians to independently complete the same question set.Each bioinformaticians was randomly assigned one-fifth of the questions, covering all questions in the BAIS-SD task.</p>
<p>As shown in Table 2, both LLM agents did not perform as well as expected.GPT-4o achieved an average accuracy of 0.220, and Deepseek-V3 reached 0.254.In contrast, human experts achieved a much higher average accuracy of 0.762.These results align with prior benchmark observations [Mitchener et al., 2025], and underscore the current limitations of LLM-based AI scientists in tasks requiring data interpretation and scientific reasoning.To better understand the model behavior, we further analyzed accuracy across question categories (Figure 5).The two agents, GPT-4o and Deepseek-V3, showed similar performance distributions.They achieved their highest performance in disease analysis questions and showed relatively strong results in key gene analysis, cellular component analysis, and cellular function reasoning.This outcome is expected, as these tasks often involve direct data interpretation.In contrast, both models struggled with question categories that require more complex reasoning, such as data-driven inference and mechanistic analysis, and heterogeneity analysis.While these trends mirrored those of human experts, the absolute performance of LLM agents remained substantially lower.</p>
<p>In summary, our results suggest that although current LLM-based agents can engage with structured biological data, they remain limited in their ability to perform high-level biological reasoning and generate reliable scientific conclusions in realistic research scenarios.</p>
<p>Figure 5: The corrected number of LLM agents and human experts in different categories of multiplechoice questions in BAIS-SD.</p>
<p>Discussion</p>
<p>In this work, we present BaisBench, a comprehensive data-driven benchmark designed to evaluate AI scientists in realistic biological research scenarios.BaisBench consists of two core tasks: the cell type annotation task (BAIS-CTA) and the scientific discovery task (BAIS-SD).The BAIS-CTA task includes 31 expert-curated single-cell RNA-seq datasets, with cell types annotated according to a standardized hierarchical taxonomy.To assess model performance, we introduced a hierarchical scoring metric and evaluated several state-of-the-art AI scientists specialized in single-cell analysis.</p>
<p>The BAIS-SD task was constructed from data-driven biological discoveries reported in recent literature.We curated 198 multiple-choice questions grounded in corresponding single-cell datasets and evaluated LLM agents using an agentic framework, comparing their performance to that of PhD-level bioinformaticians.Together, these two tasks form a robust framework for benchmarking the analytical and reasoning capabilities of AI systems in real-world scientific discovery.</p>
<p>While the results indicate progress, current AI scientists remain notably behind human experts on both BAIS-CTA and BAIS-SD.A number of core limitations emerged through our evaluation.First, although AI scientists exhibit multi-agent coordination and planning capabilities, their execution remains inadequate.They often struggle to translate high-level plans into concrete and executable code, an issue particularly evident in systems focused on automated analysis, such as AutoBA.Second, most AI scientists rely on pre-programmed pipelines tailored to specific tools and tasks, which limits their ability to generalize to more complex or customized scenarios.For instance, scChat handles cell type classification by invoking predefined scripts, supporting only tissues or organs with pre-encoded marker gene information.This rigid structure prevents it from adapting to novel data or requirements.Third, current AI scientists lack the capacity to interact with external knowledge sources, which severely hampers their performance in tasks requiring background information beyond the input data.In our evaluation, none of the tested systems were able to access or utilize external knowledge bases.Additional limitations include shallow reasoning depth, a limited ability to identify and correct errors during analysis, and the tendency to generate overly generalized or uninformative conclusions.</p>
<p>In summary, while current AI scientists demonstrate encouraging progress toward automated biological research, they still have a long way to go before achieving human-expert performance.We hope BaisBench will serve as a practical and diagnostic benchmark for identifying current limitations and guiding the development of next-generation AI scientists.</p>
<p>Appendix A Details of BAIS-CTA</p>
<p>We list all the papers and their corresponding dataset we used in the cell type annotation task in Table S1.The cell number in each dataset ranges from 2312 to 58706, and the cell type number in each dataset ranges from 4 to 42.B uHAF hierarchical cell type tree uHAF hierarchical cell type tree (uHAF-T) provides hierarchical cell type trees for different organs and an accompanying Python package, enabling users to easily unify cell type annotations to the same granularity or trace back to coarser-grained cell types.It includes standardized cell type trees for 50 organs, manually created based on anatomical literature, single-cell data-related publications, and Cell Ontology.We used these hierarchical cell type trees to find the coarser-grained cell types of the ground truth cell type, thus providing a more comprehensive evaluation metric for the AI scientist's cell annotation ability.</p>
<p>C Details of BAIS-SD</p>
<p>We list all the papers and their corresponding dataset we used in the scientific discovery task in Table S2.The cell number in each dataset ranges from 5499 to 122129.</p>
<p>We also counted the number of single-option and multiple-option questions in all the questions, and the ratio is shown in the figure S1.Over 3/4 of the questions are single-option and about 20% are multiple-option.</p>
<p>C.1 Construction of multiple-choice question</p>
<p>We used the LLM's reading and summarizing ability to construct the multiple-choice questions.Specifically, we collected the datasets from the Cellxgene website, and we also obtained the original papers where these datasets came from.We sent these papers to the GPT-4o and asked it to do the following things: 1. Give a quick and short summary of the research background in the first person.And extract the basic information about the sequencing data.2. Consider which of the conclusions/discoveries in the article are derived directly from the single-cell transcriptomic data measured by the authors.List them all.3. Consider which conclusions/discoveries in the article are based on a combination of data measured by the author and external knowledge.List them all.4. Choose 5 appropriate conclusions/discoveries from the above and form them into multiple-choice questions.</p>
<p>The following is the complete prompt for GPT-4o:</p>
<p>Prompt for constructing multi-choice questions I need to assign an assignment for a bioinformatics analysis class that is about giving a single cell transcriptome data and answering questions by analyzing the given data.I now need to design the questions based on what is in the original article that corresponds to this dataset.I will give you the article below and ask you to read the contents of this article carefully and complete the following tasks:</p>
<p>1.A quick and short summary of the research background in the first person.And the basic information about the sequencing data.</p>
<ol>
<li>
<p>Consider which of the conclusions/discoveries in the article are derived directly from the single-cell transcriptome data measured by the authors?Please list them all (use "the data ..." instead of "the study ...", "the author ..." or "the research ..."); this will serve as fodder for my questions.Be as specific as possible, include specific key terms or descriptions, and if necessary, include the process or intermediate steps that led to the conclusion, no generalizations, no descriptive words.</p>
</li>
<li>
<p>Consider which conclusions/discoveries in the article are based on a combination of data measured by the author and external knowledge.Please list them all, this is also my source material for the questions, all requirements are strictly the same as the one above.</p>
</li>
<li>
<p>Choose 5 appropriate conclusions/discoveries from the above and form them into multiplechoice questions (each entry is a separate question).It can be either single (only one correct answer, like "B") or multi-choice (more than one correct answer, like "ACD") questions.Make the position of the correct answer as random as possible.The correct option comes from the article; the incorrect option can come from the article or be added from your own knowledge, but not judged too easily.Give the correct answer to the questions.I would not give the article to the students, so don't come up with anything that need to reading the article That is, avoid expressions like "xxx in the study", "the author ..." or "xxx in the research", instead, using "in the data ...".What you need to do is treat what is in the article as a standard answer so that students can reproduce those conclusions or discoveries from the given transcriptomic data (not any other data).</p>
</li>
</ol>
<p>C.2 Agent framework for BAIS-SD task</p>
<p>In the BAIS-SD task, since none of the AI scientists specialized in single-cell can answer the multiplechoice questions based on data analysis, we used a more general AI scientist, Aviary [Narayanan et al., 2024], to run the benchmark.</p>
<p>Aviary is an extensible framework for training language agents on challenging scientific tasks, formalized as language decision processes (LDPs)-a type of partially observable Markov decision process where actions and observations are represented in natural language.It models agents as stochastic computation graphs, supporting modular reasoning, planning, and tool use.Aviary emphasizes multi-step scientific reasoning with environments like molecular cloning, scientific literature QA, and protein engineering, which makes it suitable for our multiple-choice question answering task.Aviary implemented ReAct [Yao et al., 2023] as a two-stage stochastic computation graph: first sampling a reasoning step and then an action (tool call), enabling agents to interleave thinking and acting systematically during task execution.The answer can be derived from data (some of them may need some extra knowledge, but most are derived mainly from data analysis).The necessary metadata is stored in adata.obs,you can read them and choose the useful ones.Here is the information and the question:</p>
<p>Background: Lung development is a highly complex process involving a diverse array of cell types, yet our understanding of late-stage human lung development remains incomplete.Animal models have provided critical insights, but translating these findings to human biology is challenging due to species differences.To address this gap, we used single-cell RNA sequencing (scRNA-seq) to create a molecular atlas of newborn human lung cells.This allows us to define distinct cellular populations and their gene signatures, offering new insights into the structural and functional maturation of the human lung at birth.Sample Source: Two one-day-old newborn human lungs were obtained through organ donation.</p>
<p>One was from a full-term infant (38 weeks gestational age), and the other was preterm (31 weeks gestational age).Cell Isolation &amp; Processing: Lungs were enzymatically digested to obtain single-cell suspensions, which were frozen and later used for sequencing.Sequencing Platform: Chromium 10X Genomics system (v2 chemistry), sequenced on a HiSeq4000.Final Dataset: 5,499 high-quality cells, including epithelial, endothelial, mesenchymal, and immune cells.All experiments were conducted locally on a desktop computer equipped with an AMD Ryzen 9 5900X 12-core CPU and an NVIDIA GeForce RTX 3080 GPU (10 GB VRAM).Note that CellAgent is only supported to run on their servers, so we ran the benchmark through their web pages.For other AI scientists, including AutoBA, scChat, and Aviary, we downloaded their source code and ran them on our local machine.</p>
<p>Figure 1 :
1
Figure 1: (A) Overview of the BaisBench benchmark, which consists of two tasks: the cell type annotation task (BAIS-CTA) and the scientific discovery task (BAIS-SD).(B) Construction of BAIS-CTA: We collected single-cell transcriptomic datasets from multiple organs and enlisted a group of bioinformatics experts to provide high-quality cell type annotations.AI scientists are required to perform cell type annotation on these datasets.The results are evaluated using the Unified Hierarchical cell Annotation Framework (uHAF).(C) Construction of BAIS-SD: We curated a set of recently published single-cell research papers along with their corresponding scRNA-seq datasets.An LLM agent was used to extract key data-driven biological conclusions from the literature and convert them into multiple-choice questions.AI scientists are asked to analyze the data and select the correct answers.A group of human experts also completed the same tasks, allowing for comparative evaluation between AI and human performance.</p>
<p>Figure 3 :
3
Figure 3: (A) Pipeline of the BAIS-SD task.The AI scientist is provided with background information and a corresponding single-cell dataset, and is required to answer multiple-choice questions by performing data analysis and biological reasoning.Its answers are then compared against the ground truth derived from published literature.(B) Distribution of question categories in the BAIS-SD multiple-choice question set.</p>
<p>A) High expression of amphiregulin (AREG) • (B) Increased activation of TGF-β signaling • (C) Low levels of CXCL14 and CCL27 • (D) Presence of CD1C+CD301A+ myeloid dendritic cells</p>
<p>Figure 4 :
4
Figure 4: The performance of AI scientists on the BAIS-CTA task on different organs.</p>
<p>Figure S1 :
S1
Figure S1: The proportion of single-option and multiple-option questions in the BAIS-SD task.</p>
<p>C. 3
3
Example prompt for AI scientist in BAIS-SD task Example prompt for AI scientist in BAIS-SD I am a bioinformatician and I am doing research on newly measured single-cell transcriptomic data.I now give you the data file (in h5ad format) and the background information about the dataset and my research.I need you to finish the following multiple-choice questions.You need to analyze the data based on the question and give the right option.</p>
<p>Which major cell type was found to be the most abundant in the newborn human lung based on single-cell transcriptome data?A) Endothelial cells B) Epithelial cells C) Mesenchymal cells D) Immune cells Which of the following markers was specifically associated with immature matrix fibroblasts in the newborn lung?-cell transcriptomic data, what was a key characteristic of immune cells in the newborn human lung?A) They were only detected in one of the two donors.B) They were exclusively macrophages.C) They included T cells, B cells, and macrophages with donor-to-donor variation.D) They showed no expression of leukocyte markers.The estimated developmental state of human newborn lung cells, based on murine postnatal development, was closest to which range of murine postnatal days?Athe presence of two distinct matrix fibroblast populations in the newborn lung?A) Differential expression of EPCAM and PECAM1 B) Separation of cells based on mitochondrial gene content C) Identification of distinct gene expression profiles, including COL6A3 and TCF21 D) Complete absence of mesenchymal markers in one fibroblast population D Hardware and software</p>
<p>Table 1 :
1
The performance of different AI scientists on the BAIS-CTA task
MethodOverall accSubset accAutoBA0.378 ± 0.0140.308 ± 0.015CellAgent0.376 ± 0.0170.300 ± 0.014scChat-0.107 ± 0.008GPT-4o0.298 ± 0.0130.257 ± 0.014CellTypist0.437 ± 0.0140.408 ± 0.013</p>
<p>Table 2 :
2
The performance of the LLM agent and human experts on the multiple-choice questions.
MethodCorrectness scoreHuman expert0.762GPT-4o0.220 ± 0.015Deepseek-V30.254 ± 0.020</p>
<p>Table S1 :
S1
Summary of single-cell datasets used in the Cell type annotation task.
PaperDOIOrganCell numberCell type numberEmont et al. -202210.1038_s41586-022-04518-2 Adipose5515010Dong et al. -202010.1016_j.ccell.2020.08.014Adrenal_gland587055The Tabula Sapiens10.1101_2021.07.19.452956Bladder2430715Consortium et al. -2021Domínguez Conde et al. -10.1126_science.abl5197Blood24149262022Roy et al. -202110.1016_j.celrep.2021.109698 Bone_marrow308946The Tabula Sapiens10.1101_2021.07.19.452956Breast1122713Consortium et al. -2021Voigt et al. -202110.1093_hmg_ddab140Eye3187010Qiu et al. -202110.18632_aging.203124Femur58348He et al. -202010.1186_s13059-020-02210-0 Gallbladder23126The Tabula Sapiens10.1126_science.abl4896Heart2616812Consortium et al. -2022He et al. -202010.1186_s13059-020-02210-0 Intestine892412Suo et al. -202210.1126_science.abo0510Kidney2595542Aizarani et al. -201910.1038_s41586-019-1373-2Liver919412Suo et al. -202210.1126_science.abo0510Lymph_node587230Xi et al. -202010.1016_j.stem.2020.04.017Muscle260349Deprez et al. -202010.1164_rccm.201911-Nose17868172199OCHe et al. -202010.1186_s13059-020-02210-0 Oesophagus84507Pagella et al. -202110.1016_j.isci.2021.102405Oral_cavity157277Chitiashvili et al. -202010.1038_s41556-020-00607-4 Ovary85616Cao et al. -202010.1126_science.aba7721Pancreas4315514Joseph et al. -202110.1002_path.5751Prostate435848The Tabula Sapiens10.1101_2021.07.19.452956Salivary_gland2695922Consortium et al. -2021Gur et al. -202210.1016_j.cell.2022.03.011Skin386668Rayon et al. -202110.1242_dev.199711Spinal_cord128518Zhao et al. -202010.1038_s41421-020-0157-zSpleen2116211Cao et al. -202010.1126_science.aba7721Stomach1111615Zhao et al. -202010.1038_s41467-020-19414-4 Testis264828Zeng et al. -201910.1016_j.immuni.2019.09.008 Thymus90614Miller et al. -202010.1016_j.devcel.2020.01.033 Trachea174235Ulrich et al. -202210.1016_j.devcel.2022.02.017 Uterine_tube5870612The Tabula Sapiens10.1101_2021.07.19.452956Uterus699611Consortium et al. -2021</p>
<p>Table S2 :
S2
Summary of single-cell papers and datasets used in the Scientific discovery task.
NameDOICell numberBhattacharya et al. (2024) Genes10.3390/genes150302985499Binvignat et al. (2024) JCI Insight10.1172/jci.insight.178499108717Burclaff et al. (2022) Cellular and10.1016/j.jcmgh.2022.02.00712590Molecular Gastroenterology andHepatology</p>
<p>Researchagent: Iterative research idea generation over scientific literature with large language models. Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, Sung Ju Hwang, arXiv:2404.077382024arXiv preprint</p>
<p>uhaf: a unified hierarchical annotation framework for cell type standardization and harmonization. Haiyang Bian, Yinxin Chen, Lei Wei, Xuegong Zhang, Bioinformatics. 4141492025</p>
<p>Cross-tissue immune cell analysis reveals tissue-specific features in humans. Chao C Domínguez Conde, Louie B Xu, Daniel B Jarvis, Sara B Rainbow, Tamir Wells, Gomes, Howlett, Suchanek, Polanski, King, Science. 376659451972022</p>
<p>Towards an ai co-scientist. Juraj Gottweis, Wei-Hung Weng, Alexander Daryin, Tao Tu, Petar Palepu, Artiom Sirkovic, Felix Myaskovsky, Keran Weissenberger, Ryutaro Rong, Tanno, arXiv:2502.188642025arXiv preprint</p>
<p>Ken Gu, Ruoxi Shang, Ruien Jiang, Keying Kuang, Richard-John Lin, Donghe Lyu, Yue Mao, Youran Pan, Teng Wu, Jiaqian Yu, arXiv:2408.09667Benchmarking language model agents for data-driven science. 2024arXiv preprint</p>
<p>Best practices for single-cell analysis across modalities. Lukas Heumos, Anna C Schaar, Christopher Lance, Anastasia Litinetskaya, Felix Drost, Luke Zappia, Malte D Lücken, C Daniel, Juan Strobl, Fabiola Henao, Curion, Nature Reviews Genetics. 2482023</p>
<p>Jon M Laurent, Joseph D Janizek, Michael Ruzo, Michaela M Hinks, Michael J Hammerling, Siddharth Narayanan, Manvitha Ponnapati, Andrew D White, Samuel G Rodriques, arXiv:2407.10362Lab-bench: Measuring capabilities of language models for biology research. 2024arXiv preprint</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in neural information processing systems. 202033</p>
<p>A platform for the biomedical application of large language models. Sebastian Lobentanzer, Shaohong Feng, Noah Bruderer, Andreas Maier, Cankun Wang, Jan Baumbach, Jorge Abreu-Vicente, Nils Krehl, Qin Ma, Nature Biotechnology. 2025</p>
<p>The ai scientist: Towards fully automated open-ended scientific discovery. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.062922024aarXiv preprint</p>
<p>scchat: A large language model-powered co-pilot for contextualized single-cell rna sequencing analysis. Yen-Chun Lu, Ashley Varghese, Rahul Nahar, Hao Chen, Kunming Shao, Xiaoping Bao, Can Li, bioRxiv. 2024b</p>
<p>Current best practices in single-cell rna-seq analysis: a tutorial. D Malte, Fabian J Luecken, Theis, Molecular systems biology. 156e87462019</p>
<p>scagent: Universal single-cell annotation via a llm agent. Yuren Mao, Yu Mi, Peigen Liu, Mengfei Zhang, Hanqing Liu, Yunjun Gao, arXiv:2504.046982025arXiv preprint</p>
<p>Ludovico Mitchener, Jon M Laurent, Benjamin Tenmann, Siddharth Narayanan, Andrew Geemi P Wellawatte, Lorenzo White, Samuel G Sani, Rodriques, arXiv:2503.00096Bixbench: a comprehensive benchmark for llm-based agents in computational biology. 2025arXiv preprint</p>
<p>Aviary: training language agents on challenging scientific tasks. Siddharth Narayanan, James D Braza, Ryan-Rhys Griffiths, Manu Ponnapati, Albert Bou, Jon Laurent, Ori Kabeli, Geemi Wellawatte, Sam Cox, Samuel G Rodriques, arXiv:2412.211542024arXiv preprint</p>
<p>Cz cellxgene discover: a single-cell data platform for scalable exploration, analysis and modeling of aggregated data. Shibla Abdulla, Brian Aevermann, Pedro Assis, Seve Badajoz, Sidney M Bell, Emanuele Bezzi, Batuhan Cakir, Jim Chaffer, Signe Chambers, Nucleic Acids Research. 53D12025CZI Cell Science Program,</p>
<p>Large language models as biomedical hypothesis generators: a comprehensive evaluation. Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, Zhang-Ren Chen, Sihang Zeng, Ermo Hua, Jinfang Hu, Bowen Zhou, arXiv:2407.089402024arXiv preprint</p>
<p>Towards scientific discovery with generative ai: Progress, opportunities, and challenges. K Chandan, Parshin Reddy, Shojaee, Proceedings of the Conference on Artificial Intelligence. the Conference on Artificial Intelligence202539</p>
<p>Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum, arXiv:2501.04227Agent laboratory: Using llm agents as research assistants. 2025arXiv preprint</p>
<p>Two heads are better than one: A multi-agent system has the potential to improve scientific idea generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arXiv:2410.094032024arXiv preprint</p>
<p>Exponential scaling of single-cell rna-seq in the past decade. Roser Valentine Svensson, Sarah A Vento-Tormo, Teichmann, Nature protocols. 1342018</p>
<p>Cellagent: An llm-driven multi-agent framework for automated single-cell data analysis. Yihang Xiao, Jinyi Liu, Yan Zheng, Xiaohan Xie, Jianye Hao, Mingzhi Li, Ruitao Wang, Fei Ni, Yuxiao Li, Jintian Luo, BioRxiv. 2024</p>
<p>The ai scientist-v2: Workshop-level automated scientific discovery via agentic tree search. Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha, arXiv:2504.080662025arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, International Conference on Learning Representations (ICLR). 2023</p>
<p>A comprehensive survey of scientific large language models and their applications in scientific discovery. Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han, arXiv:2406.108332024arXiv preprint</p>
<p>Massively parallel digital transcriptional profiling of single cells. Jessica M Grace Xy Zheng, Phillip Terry, Paul Belgrader, Zachary W Ryvkin, Ryan Bent, Wilson, B Solongo, Tobias D Ziraldo, Geoff P Wheeler, Junjie Mcdermott, Zhu, Nature communications. 81140492017</p>
<p>An ai agent for fully automated multi-omic analyses. Juexiao Zhou, Bin Zhang, Guowei Li, Xiuying Chen, Haoyang Li, Xiaopeng Xu, Siyuan Chen, Wenjia He, Chencheng Xu, Liwei Liu, 2407094,2024.10.1038/s42255-022-00531-xAdvanced Science. 1144</p>
<p>. Guerrero-Murillo, 10.1101/2024.01.23.5768782024</p>
<p>. Heimlich, 10.1182/bloodadvances.20230114452024</p>
<p>. Horeth, 10.1177/00220345221147908J Dent Res. 2023</p>
<p>. Joseph, 10.1002/pros.2402012212910.1038/s41586-019-0903-2 17799Nature. 2020. 2019</p>
<p>. Knight-Schrijver, 10.1038/s44161-022-00183-wNat Cardiovasc Res. 2022</p>
<p>. Kurkalang, 10.1111/cas.15979Immunol. 10.1126/sciimmunol.abd1554Lee et al.2023. 28186. 2020. 2020</p>
<p>. Lengyel, 10.1016/j.celrep.2022.1118382022Cell Reports</p>
<p>. Li, 10.1016/j.stem.2023.12.013Cell Stem Cell. 2024</p>
<p>. Lukassen, 10.15252/embj.20105114The EMBO Journal. 2020</p>
<p>. Martin , 10.1016/j.cell.2019.08.00832458Melmsetal.10.1038/s41586-021-03569-1 116313Nature. 2019. 2021</p>
<p>. Menon, 10.1038/s41467-019-12780-820091Nat Commun. 2019</p>
<p>. Mimpen, 10.1096/fj.202300601RRR1053310.3389/fdmed.2021.806294The FASEB Journal. Opasawatchai et al.2024. 2024. 22280. 2022Cell ReportsFront. Dent. Med</p>
<p>. Phan, 10.1038/s41467-024-45165-7Nat Commun. 2024</p>
<p>. Rabadam, 10.1172/jci.insight.1769631058272024JCI Insight</p>
<p>. Rustam, 10.1164/rccm.202207-1384OC115788Am J Respir Crit Care Med. 2023</p>
<p>. Solé-Boldo, 10.1038/s42003-020-0922-415457Commun Biol. 2020</p>
<p>. Stewart, 10.1126/science.aat50311058702019</p>
<p>. Strati, 10.1016/j.xcrm.2023.101158926762023Cell Reports Medicine</p>
<p>. Szabo, 10.1038/s41467-019-12464-351876Nat Commun. 2019</p>
<p>. Wang , 10.1084/jem.2019113014106Journal of Experimental Medicine. 2020</p>
<p>. Watanabe, 10.1165/rcmb.2021-0555OCAm J Respir Cell Mol Biol. 2022</p>
<p>. Whitfield, 10.1002/ctm2.135662599Clinical &amp; Translational Med. 2023</p>
<p>. Wiedemann, 10.1016/j.celrep.2023.111994152432023Cell Reports</p>
<p>. Wu, 10.15252/embj.201910406324271The EMBO Journal. 2020</p>
<p>. Xiang, 10.3389/fcvm.2020.00052Front. Cardiovasc. Med. 2020</p>
<p>. Xu, 10.1038/s41598-022-17832-62022Sci Rep</p>
<p>. Yang , 10.1038/s41586-021-03710-0Nature. 2021</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3276 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3276</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3276</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-cf237f3a6ed3e8fd970c15bf1f0bdf94f34da4a9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cf237f3a6ed3e8fd970c15bf1f0bdf94f34da4a9" target="_blank">LPML: LLM-Prompting Markup Language for Mathematical Reasoning</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper proposed a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL) and discovered that by prompting LLMs to generate structured text in XML-like markup language, it could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs.</p>
                <p><strong>Paper Abstract:</strong> In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.</p>
                <p><strong>Cost:</strong> 0.01</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3276.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3276.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LPML (Ours)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-Prompting Markup Language (LPML) with Python REPL integration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A novel framework that prompts an LLM to emit structured XML-like tags (THINK, PYTHON, OUTPUT, ANSWER) to integrate Chain-of-Thought (CoT) reasoning with an external Python REPL, enabling two reasoning paths (textual CoT and executed code) and controlled feedback of execution results to the model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI conversational LLM accessed via API (message-based assistant/user/system roles). Used with temperature 0 (raised when required); prompted to emit LPML tags and to prefer OUTPUT (Python execution) contents over THINK contents.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (CoT)', 'Tool-augmented reasoning / Python REPL execution', 'Zero-shot-CoT (via tag definition)', 'Structured prompting (markup-language conditioning)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT: model writes step-by-step reasoning in THINK tags. Python REPL: model writes executable code in PYTHON tags; system executes code and returns results in OUTPUT tags; model instructed to trust OUTPUT over THINK and to re-check THINK when OUTPUT disagrees. Zero-shot-CoT induced by DEFINE tag instructing step-by-step thoughts. Structured prompting: entire prompt and conversation constrained to LPML to parse and control outputs, strip fake outputs, and enforce interaction protocol (e.g., assistant must emit EOS after PYTHON and wait for OUTPUT).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Diverse — LPML intentionally provides two distinct reasoning styles (textual CoT and program execution) and enforces their interaction; the system creates self-consistency across diverse methods by feeding executed results back into CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and MATH</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>GSM8K: 8.5K grade-school math word problems (1319 test problems used). MATH: a dataset of 12,500 competitive math problems across subjects (20 problems sampled from six subjects for a 120-problem evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Ours (LPML combining CoT+Python REPL) — GSM8K: 76.6% accuracy; MATH (sampled 120 problems): 60.0% accuracy. (Scoring based on content of ANSWER tags.)</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to CoT and PAL baselines (all using the same backbone LLM), LPML improved performance on the more challenging MATH dataset (60.0% vs CoT 31.7% and PAL 47.5%) but was slightly worse than PAL on GSM8K (LPML 76.6% vs PAL 79.8%, CoT 57.1%). Authors attribute PAL's GSM8K advantage to many GSM8K problems being straightforward to implement in Python (favoring program-only methods), whereas LPML's combination helps on MATH where some reasoning cannot be directly implemented by code.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining CoT and external tool execution through structured markup (LPML) yields substantial gains on harder mathematical problems (MATH) by allowing the model to use CoT for complex reasoning while using Python execution to check/replace error-prone calculations; however, when problems are readily solvable by programmatic code alone (GSM8K), program-only approaches (PAL) can outperform the combined approach.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>LPML is not uniformly superior: on GSM8K PAL outperformed LPML. The paper documents failure modes where the LLM ignores OUTPUT results, emits fake/dummy execution results, or conflicts between THINK and OUTPUT lead to inconsistent behavior (Figures 7 and 8). The method can increase overall errors relative to program-only approaches when there is no synergy between CoT and external tools; success depends on the model following the enforced protocol and trusting OUTPUT tags.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LPML: LLM-Prompting Markup Language for Mathematical Reasoning', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3276.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3276.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Program-Aided Language models (PAL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method where an LLM solves reasoning problems by emitting executable code (e.g., Python) and using the program's output to produce answers, thereby offloading calculation and deterministic procedures to a program executor.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pal: Program-aided language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same backbone LLM as other methods in experiments; for PAL the model is prompted (few-shot) to output Python code that, when executed, yields the solution (author's implementation used with provided few-shot code examples).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Programmatic reasoning (code generation + execution)', 'Few-shot prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Model is prompted with few-shot examples pairing problems with Python code; it translates problems into Python code, the code is executed externally, and outputs are taken as answers. PAL deliberately separates symbolic/numeric computation from natural-language reasoning by delegating computations to a program executor.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single/similar style — primarily programmatic reasoning (code generation + execution) without explicit textual chain-of-thought in the experiment (PAL does not use CoT in this paper's baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and MATH</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>GSM8K (grade-school math) and MATH (challenging competition problems); PAL evaluated as baseline on both datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>PAL — GSM8K: 79.8% accuracy; MATH (sampled): 47.5% accuracy (numbers reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>PAL outperforms plain CoT on both datasets and outperforms LPML on GSM8K, but underperforms LPML on the harder MATH dataset. Authors infer that PAL's strength is in problems easily encoded and solved by code, while PAL struggles on problems that require richer conceptual reasoning that is not straightforward to implement.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Program-only approaches like PAL excel when problems map cleanly to executable procedures (e.g., GSM8K); however, their performance can be limited on problems requiring nontrivial conceptual reasoning not easily expressed as code.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>PAL performs worse than LPML on the sampled MATH problems (47.5% vs 60.0%), indicating limitations when pure programmatic translation is insufficient. The authors also note that PAL treats the tool as a module independent of other reasoning processes, potentially limiting overall performance when module consistency or cross-checking is needed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LPML: LLM-Prompting Markup Language for Mathematical Reasoning', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3276.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3276.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (CoT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that induces LLMs to produce step-by-step textual intermediate reasoning (thoughts) to solve complex problems, improving performance on multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT (GPT-3.5-Turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM prompted with few-shot CoT examples (five randomly selected examples per dataset in experiments) to generate intermediate textual reasoning steps in THINK tags (for zero-shot-CoT the paper also uses tag definitions to induce stepwise reasoning).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Textual Chain-of-Thought', 'Zero-shot-CoT (via instructive LPML tags)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT: model generates step-by-step natural-language reasoning (THINK tags). Zero-shot-CoT: induced by instructing the assistant via DEFINE/THINK tag semantics to produce step-by-step thoughts even without few-shot examples.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Single/similar style — primarily textual step-by-step reasoning without program execution (in the CoT baseline). In LPML experiments CoT is one of the two styles combined with Python execution.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>GSM8K and MATH</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same datasets used as baselines; CoT evaluated as baseline where the model writes reasoning chains and answers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>CoT baseline — GSM8K: 57.1% accuracy; MATH (sampled): 31.7% accuracy (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Plain CoT underperforms both PAL and LPML on both datasets in these experiments. The LPML approach augments CoT with executed Python outputs and explicit protocol rules, substantially improving CoT's performance on MATH via fallback to validated calculations.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>While CoT improves reasoning by exposing intermediate steps, it remains susceptible to calculation and logical errors; integrating CoT with external validated computations (via LPML and Python REPL) can correct calculation errors and raise accuracy on more difficult reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT alone yields the lowest accuracy in these evaluations. The paper documents that CoT can produce incorrect computations that are not self-corrected unless explicitly validated by executed code; simply returning Python outputs without structured prompting can lead to ignored or fake results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LPML: LLM-Prompting Markup Language for Mathematical Reasoning', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pal: Program-aided language models. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Self-consistency improves chain of thought reasoning in language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3276",
    "paper_id": "paper-cf237f3a6ed3e8fd970c15bf1f0bdf94f34da4a9",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "LPML (Ours)",
            "name_full": "LLM-Prompting Markup Language (LPML) with Python REPL integration",
            "brief_description": "A novel framework that prompts an LLM to emit structured XML-like tags (THINK, PYTHON, OUTPUT, ANSWER) to integrate Chain-of-Thought (CoT) reasoning with an external Python REPL, enabling two reasoning paths (textual CoT and executed code) and controlled feedback of execution results to the model.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-Turbo)",
            "model_description": "OpenAI conversational LLM accessed via API (message-based assistant/user/system roles). Used with temperature 0 (raised when required); prompted to emit LPML tags and to prefer OUTPUT (Python execution) contents over THINK contents.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (CoT)",
                "Tool-augmented reasoning / Python REPL execution",
                "Zero-shot-CoT (via tag definition)",
                "Structured prompting (markup-language conditioning)"
            ],
            "reasoning_methods_description": "CoT: model writes step-by-step reasoning in THINK tags. Python REPL: model writes executable code in PYTHON tags; system executes code and returns results in OUTPUT tags; model instructed to trust OUTPUT over THINK and to re-check THINK when OUTPUT disagrees. Zero-shot-CoT induced by DEFINE tag instructing step-by-step thoughts. Structured prompting: entire prompt and conversation constrained to LPML to parse and control outputs, strip fake outputs, and enforce interaction protocol (e.g., assistant must emit EOS after PYTHON and wait for OUTPUT).",
            "diversity_of_methods": "Diverse — LPML intentionally provides two distinct reasoning styles (textual CoT and program execution) and enforces their interaction; the system creates self-consistency across diverse methods by feeding executed results back into CoT.",
            "reasoning_task_name": "GSM8K and MATH",
            "reasoning_task_description": "GSM8K: 8.5K grade-school math word problems (1319 test problems used). MATH: a dataset of 12,500 competitive math problems across subjects (20 problems sampled from six subjects for a 120-problem evaluation).",
            "performance_by_method": "Ours (LPML combining CoT+Python REPL) — GSM8K: 76.6% accuracy; MATH (sampled 120 problems): 60.0% accuracy. (Scoring based on content of ANSWER tags.)",
            "comparison_of_methods": "Compared to CoT and PAL baselines (all using the same backbone LLM), LPML improved performance on the more challenging MATH dataset (60.0% vs CoT 31.7% and PAL 47.5%) but was slightly worse than PAL on GSM8K (LPML 76.6% vs PAL 79.8%, CoT 57.1%). Authors attribute PAL's GSM8K advantage to many GSM8K problems being straightforward to implement in Python (favoring program-only methods), whereas LPML's combination helps on MATH where some reasoning cannot be directly implemented by code.",
            "key_findings": "Combining CoT and external tool execution through structured markup (LPML) yields substantial gains on harder mathematical problems (MATH) by allowing the model to use CoT for complex reasoning while using Python execution to check/replace error-prone calculations; however, when problems are readily solvable by programmatic code alone (GSM8K), program-only approaches (PAL) can outperform the combined approach.",
            "counter_examples_or_negative_results": "LPML is not uniformly superior: on GSM8K PAL outperformed LPML. The paper documents failure modes where the LLM ignores OUTPUT results, emits fake/dummy execution results, or conflicts between THINK and OUTPUT lead to inconsistent behavior (Figures 7 and 8). The method can increase overall errors relative to program-only approaches when there is no synergy between CoT and external tools; success depends on the model following the enforced protocol and trusting OUTPUT tags.",
            "uuid": "e3276.0",
            "source_info": {
                "paper_title": "LPML: LLM-Prompting Markup Language for Mathematical Reasoning",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "PAL",
            "name_full": "Program-Aided Language models (PAL)",
            "brief_description": "A method where an LLM solves reasoning problems by emitting executable code (e.g., Python) and using the program's output to produce answers, thereby offloading calculation and deterministic procedures to a program executor.",
            "citation_title": "Pal: Program-aided language models.",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-Turbo)",
            "model_description": "Same backbone LLM as other methods in experiments; for PAL the model is prompted (few-shot) to output Python code that, when executed, yields the solution (author's implementation used with provided few-shot code examples).",
            "model_size": null,
            "reasoning_methods": [
                "Programmatic reasoning (code generation + execution)",
                "Few-shot prompting"
            ],
            "reasoning_methods_description": "Model is prompted with few-shot examples pairing problems with Python code; it translates problems into Python code, the code is executed externally, and outputs are taken as answers. PAL deliberately separates symbolic/numeric computation from natural-language reasoning by delegating computations to a program executor.",
            "diversity_of_methods": "Single/similar style — primarily programmatic reasoning (code generation + execution) without explicit textual chain-of-thought in the experiment (PAL does not use CoT in this paper's baseline).",
            "reasoning_task_name": "GSM8K and MATH",
            "reasoning_task_description": "GSM8K (grade-school math) and MATH (challenging competition problems); PAL evaluated as baseline on both datasets.",
            "performance_by_method": "PAL — GSM8K: 79.8% accuracy; MATH (sampled): 47.5% accuracy (numbers reported in Table 1).",
            "comparison_of_methods": "PAL outperforms plain CoT on both datasets and outperforms LPML on GSM8K, but underperforms LPML on the harder MATH dataset. Authors infer that PAL's strength is in problems easily encoded and solved by code, while PAL struggles on problems that require richer conceptual reasoning that is not straightforward to implement.",
            "key_findings": "Program-only approaches like PAL excel when problems map cleanly to executable procedures (e.g., GSM8K); however, their performance can be limited on problems requiring nontrivial conceptual reasoning not easily expressed as code.",
            "counter_examples_or_negative_results": "PAL performs worse than LPML on the sampled MATH problems (47.5% vs 60.0%), indicating limitations when pure programmatic translation is insufficient. The authors also note that PAL treats the tool as a module independent of other reasoning processes, potentially limiting overall performance when module consistency or cross-checking is needed.",
            "uuid": "e3276.1",
            "source_info": {
                "paper_title": "LPML: LLM-Prompting Markup Language for Mathematical Reasoning",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting (CoT)",
            "brief_description": "A prompting technique that induces LLMs to produce step-by-step textual intermediate reasoning (thoughts) to solve complex problems, improving performance on multi-step reasoning tasks.",
            "citation_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "ChatGPT (GPT-3.5-Turbo)",
            "model_description": "LLM prompted with few-shot CoT examples (five randomly selected examples per dataset in experiments) to generate intermediate textual reasoning steps in THINK tags (for zero-shot-CoT the paper also uses tag definitions to induce stepwise reasoning).",
            "model_size": null,
            "reasoning_methods": [
                "Textual Chain-of-Thought",
                "Zero-shot-CoT (via instructive LPML tags)"
            ],
            "reasoning_methods_description": "CoT: model generates step-by-step natural-language reasoning (THINK tags). Zero-shot-CoT: induced by instructing the assistant via DEFINE/THINK tag semantics to produce step-by-step thoughts even without few-shot examples.",
            "diversity_of_methods": "Single/similar style — primarily textual step-by-step reasoning without program execution (in the CoT baseline). In LPML experiments CoT is one of the two styles combined with Python execution.",
            "reasoning_task_name": "GSM8K and MATH",
            "reasoning_task_description": "Same datasets used as baselines; CoT evaluated as baseline where the model writes reasoning chains and answers.",
            "performance_by_method": "CoT baseline — GSM8K: 57.1% accuracy; MATH (sampled): 31.7% accuracy (Table 1).",
            "comparison_of_methods": "Plain CoT underperforms both PAL and LPML on both datasets in these experiments. The LPML approach augments CoT with executed Python outputs and explicit protocol rules, substantially improving CoT's performance on MATH via fallback to validated calculations.",
            "key_findings": "While CoT improves reasoning by exposing intermediate steps, it remains susceptible to calculation and logical errors; integrating CoT with external validated computations (via LPML and Python REPL) can correct calculation errors and raise accuracy on more difficult reasoning tasks.",
            "counter_examples_or_negative_results": "CoT alone yields the lowest accuracy in these evaluations. The paper documents that CoT can produce incorrect computations that are not self-corrected unless explicitly validated by executed code; simply returning Python outputs without structured prompting can lead to ignored or fake results.",
            "uuid": "e3276.2",
            "source_info": {
                "paper_title": "LPML: LLM-Prompting Markup Language for Mathematical Reasoning",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pal: Program-aided language models.",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 2
        },
        {
            "paper_title": "Self-consistency improves chain of thought reasoning in language models",
            "rating": 2
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks",
            "rating": 1
        }
    ],
    "cost": 0.01027575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LPML: LLM-Prompting Markup Language for Mathematical Reasoning</h1>
<p>A Preprint</p>
<p>Ryutaro Yamauchi*<br>The University of Tokyo<br>ryutaro_yamauchi@weblab.t.u-tokyo.ac.jp<br>Akiyoshi Sannai<br>Kyoto University<br>sannai.akiyoshi.7z@kyoto-u.ac.jp</p>
<h2>Sho Sonoda</h2>
<p>Center for Advanced Intelligence Project, RIKEN
sho.sonoda@riken.jp</p>
<h2>Wataru Kumagai</h2>
<p>The University of Tokyo
kumagai@weblab.t.u-tokyo.ac.jp</p>
<p>October 12, 2023</p>
<h4>Abstract</h4>
<p>In utilizing large language models (LLMs) for mathematical reasoning, addressing the errors in the reasoning and calculation present in the generated text by LLMs is a crucial challenge. In this paper, we propose a novel framework that integrates the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered that by prompting LLMs to generate structured text in XML-like markup language, we could seamlessly integrate CoT and the external tool and control the undesired behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and demonstrated that combining CoT and Python REPL through the markup language enhances the reasoning capability of LLMs. Our approach enables LLMs to write the markup language and perform advanced mathematical reasoning using only zero-shot prompting.</p>
<h2>1 Introduction</h2>
<p>Large language models (LLMs) have demonstrated remarkable success in a wide range of natural language processing (NLP) tasks, despite being trained on the simplistic task of predicting subsequent words in sentences. LLMs possess the capability to extract rules and principles from the provided text (called In-Context Learning). This enables LLMs to solve even novel tasks by being prompted with a few-shot example [Brown et al., 2020]. Furthermore, it has been reported that effective prompts can improve the performance of LLMs. For instance, we can improve the ability of LLMs to perform arithmetic reasoning by prompting them to output not only the final answer but also intermediate steps involved in problem-solving. This technique, reffered to as Chain-of-Thought (CoT), is widely used [Wei et al., 2022].
One of the most essential challenges in employing LLMs for mathematical reasoning is the management of calculation and reasoning errors in LLMs' outputs [Hendrycks et al., 2021]. One approach is to improve the performance of LLMs by increasing the model size [OpenAI, 2023] or by fine-tuning with mathematical and technical texts [Lewkowycz et al., 2022, Chung et al., 2022], but training LLMs are often expensive. As a result, alternative strategies that improve performance without modifying the LLMs themselves have been sought. Several studies have LLMs generate explicit reasoning step [Wei et al., 2022, Zhou et al., 2023, Nye et al., 2021]. [Wang et al., 2023] proposes a self-consistency strategy to have LLMs generate multiple reasoning paths and merge them. Furthermore, methods for integrating LLMs with external tools have been proposed [Gao et al., 2022, Yao et al., 2022, Schick et al., 2023, Chen et al., 2022, Mishra et al., 2022]. Given LLMs' limitations with large-number calculations and complex reasoning, outsourcing these tasks to external tools can potentially reduce reasoning errors. For instance, PAL [Gao et al., 2022] outsources calculations that would be difficult for LLMs alone to a computer by few-shot prompting with pairs of a problem and a Python code</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>to solve it. However, these methods treat the use of external tools as a module independent of other reasoning processes, so the accuracy limitations of each module constrain overall performance. To fundamentally improve the accuracy of reasoning, the consistency of the external tool’s behavior should be monitored by LLMs themselves.</p>
<p>In this paper, we propose a novel framework for integrating the Python REPL (read-eval-print loop) with LLMs. Our approach differs from previous works in that we use Python code (and its execution results) as part of the CoT rather than having LLMs directly generate the Python code to solve the problem. Specifically, we construct an interaction loop between LLMs and the Python REPL by having LLMs generate both the CoT and the Python code and then feeding the results of the code execution back to LLMs to induce further CoTs (Fig. 2). This allows LLMs to have two reasoning paths, CoT and Python code execution and is expected to improve the reasoning performance by self-consistency. However, we found that simply feeding back Python code execution results does not lead LLMs to behave as we expected: when the code is written in the CoT, LLMs do not wait for the execution results but output dummy results as well. Also, when LLMs make mistakes in CoT reasoning, they tend to think that the code is incorrect, even if they write correct Python code and get the result of that execution. To avoid these problems, we define an XML-like markup language (LLM-prompting markup language, LPML) and feed its syntax to LLMs to prompt them to output text structured in the markup language (Fig. 3). LPML includes THINK tags for CoT, PYTHON tags for Python code, and OUTPUT tags for the results of Python code execution, etc. By using these tags, the LLMs’ output text is structured. The advantages of this approach are as follows: 1. we can easily parse the text generated by LLMs and segment it by function and meaning; 2. we can mechanically remove invalid text generated by LLMs, such as dummy execution results because the tags correspond to their contents; and 3. we can define relationships between tags, which enables us to make LLMs trust the results of Python calculations over those in the CoT, which are prone to errors. As a result, we achieve the <em>fall-back in reasoning</em> that LLMs often have difficulty with. We applied our method to ChatGPT [OpenAI, 2022] to solve challenging mathematical problems and demonstrated that the combination of CoT and Python computation improves LLMs’ reasoning ability.</p>
<h2>2 Methods</h2>
<h3>2.1 Interaction loop between ChatGPT and Python REPL</h3>
<p>In this study, we used OpenAI ChatGPT (GPT-3.5-Turbo) [OpenAI, 2022] via API, which is an LLM that takes a sequence of messages $[m_{0},\cdots,m_{k}]$ and predicts the next message $m_{k+1}$. The API specification allows us to set three roles as the speaker of a message: <em>assistant</em>, <em>user</em>, and <em>system</em>, where <em>assistant</em> is an AI, <em>user</em> is a human, and <em>system</em> is a conversation context manager.</p>
<p>As shown below, we designed the mathematical reasoning process as an interaction between <em>system</em> and <em>assistant</em>. First, as <em>system</em>, we present the problem to be solved and define the syntax of the markup language and rules of reasoning ($m_{0}^{e}$). Then, when <em>assistant</em> returns an output ($\hat{m}<em k="k">{k}^{e}$), <em>system</em> analyzes it, and if it contains Python code, <em>system</em> returns the execution result ($m</em>$). If }^{e<em>assistant</em> outputs any undesired elements, such as fake Python execution results, <em>system</em> removes them from the <em>assistant</em>’s output ($m_{k}^{e}$). We also remove the answer if <em>assistant</em> outputs both the Python code and the answer in a single message, and then feed back the Python code execution results. We made <em>system</em> and <em>assistant</em> repeat this process until <em>assistant</em> outputs the answer (Fig. 2).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: An overview of the mathematical reasoning process. system and assistant continue to interact until an answer is obtained. All messages are structured in the markup language.</p>
<h1>2.2 LLM-prompting markup language</h1>
<p>The LPML, the markup language we have defined, is based on XML syntax and is a set of elements consisting of content enclosed in a start tag <TAG> and end tag </TAG>. Since the dataset used to train LLMs is assumed to include text written in XML and HTML, LLMs can write LPML that is similar to them. The LPML includes the tags DEFINE, PROBLEM, ANSWER, THINK, PYTHON, and OUTPUT. DEFINE tag is for defining tags and rules. The syntax of the language and the tags are defined by DEFINE tag. PROBLEM and ANSWER tags are for describing the problem and answer, and the solving process ends
<DEFINE type="rule">
The system and the assistant exchange messages.
All messages must be formatted in XML format. XML el Tags determine the meaning and function of the conte </DEFINE>
<DEFINE type="tag" name="DEFINE">Defines rules and t <DEFINE type="tag" name="EOS">Indicates the end of a <DEFINE type="tag" name="THINK">Represents a thought</p>
<p>Figure 3: The prompt: the syntax definition of LPML and instructions on how to solve the problem written in LPML.
when assistant outputs ANSWER tag. THINK tag is for describing thoughts. By defining THINK tag as "a tag for describing thinking step-by-step", we induce Zero-shot-CoT [Kojima et al., 2022]. PYTHON tag is for describing Python code. When assistant uses PYTHON tag, system returns the execution result using OUTPUT tag. By using these tags, all messages, including the initial system prompt, are written in the LPML, thus strongly conditioning assistant to output in the LPML.</p>
<h3>2.3 Instructions to ChatGPT</h3>
<p>In the initial prompt $\left(m_{0}^{s}\right)$, we instruct ChatGPT to solve the math problem in the PROBLEM tag using the two tags PYTHON and THINK and to write the answer using the ANSWER tag. We do not give a few-shot example of how to solve the problem but just give the LPML definition, instructions on which tags to use, and some notes.</p>
<p>We also instructed ChatGPT to trust the contents of OUTPUT tags rather than the contents of THINK tags. This enables LLMs to ignore errors in CoT by referring to the results of Python code execution. Without this instruction, ChatGPT would either assume that the Python code was incorrect and fall into a loop of repeated debugging or ignore the contents of the OUTPUT tag.</p>
<h2>3 Experiments ans Results</h2>
<h3>3.1 Datasets and Evaluation</h3>
<p>We evaluated our method on the GSM8K dataset [Cobbe et al., 2021] and the MATH dataset [Hendrycks et al., 2021]. Both datasets are available under MIT License. The GSM8K is a dataset of 8.5 K grade-school level math word problems, and we used the 1319 test problems in it for our evaluation. The MATH dataset consists of 12,500 challenging competitive math problems, and the dataset contains seven subjects: Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus. We sampled 20 problems from each of the six subjects except Geometry and solved these 120 problems using our method.
We scored only the content of the ANSWER tags in evaluating our method. And we manually scored the MATH dataset because of the arbitrariness of how to write the answers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">GSM8K</th>
<th style="text-align: right;">MATH</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CoT</td>
<td style="text-align: right;">57.1</td>
<td style="text-align: right;">31.7</td>
</tr>
<tr>
<td style="text-align: left;">PAL</td>
<td style="text-align: right;">$\mathbf{7 9 . 8}$</td>
<td style="text-align: right;">47.5</td>
</tr>
<tr>
<td style="text-align: left;">Ours</td>
<td style="text-align: right;">76.6</td>
<td style="text-align: right;">$\mathbf{6 0 . 0}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of the success rate of each method on the GSM8K and MATH.</p>
<h3>3.2 Baselines</h3>
<p>We used Chain-of-Thought (CoT) [Wei et al., 2022] and Program-aided Language models (PAL) [Gao et al., 2022] for comparison. PAL is a method of solving mathematical problems by having LLMs write Python code. Since PAL uses Python code and does not use CoT, we can evaluate the effectiveness of our method using both Python and CoT by comparing our method with PAL. We used ChatGPT (GPT-3.5-Turbo) as the backbone LLM for both methods. Since both the GSM8K and MATH datasets contain manually written reasoning paths, we randomly selected five problems from each dataset and used them as few-shot examples for CoT. For PAL, we used the author’s implementation. The author’s implementation contains Python code examples for few-shot prompting. Since this example is not tuned for the MATH dataset, we recreated the prompt using problems from the MATH dataset, but since there was no significant difference in performance, we present results from the author’s prompt in subsequent reports.</p>
<h3>3.3 Implementation details</h3>
<p>Our implementation of system behaves as follows. First, system parses the output of assistant $\hat{m}<em k="k">{k}^{a}$ and creates a syntax tree. Next, it removes invalid elements from the syntax tree, specifically tags that must not be used by assistant and text that is not enclosed in tags. Also, if the syntax tree contains both PYTHON and ANSWER elements, the ANSWER element is removed. As a result, a normalized assistant output $m</em>\right]$ to assisntant.}^{a}$ is generated. Then, system runs all PYTHON elements in the syntax tree using the Python REPL and generates the system message $m_{k}^{s}$, which is a collection of standard/error outputs of the codes. Finally, system gives the message history $\left[m_{0}^{s}, m_{1}^{a}, m_{1}^{s}, \cdots, m_{k}^{a}, m_{k}^{s</p>
<p>We set the number of system and assistant interactions to 8 turns, excluding the initial prompt. If the normalized assistant output $m_{k}^{a}$ contains an ANSWER tag, the interaction is terminated at that point. If no ANSWER tag was output once throughout the interaction, the interaction is redone from the beginning. We set the ChatGPT temperature to 0 . However, if the ANSWER tag was not output after 8 interactions, we increased the temperature in 0.1 increments for up to 5 attempts.</p>
<h3>3.4 Results</h3>
<p>Table 1 shows the comparison results between the proposed method and the previous method on the GSM8K and MATH datasets. And Figure 4 shows the success rate of each method in each subject of the MATH data set. In the GSM8K dataset, PAL showed a higher accuracy rate than our method. However, our method showed a higher accuracy rate in the MATH dataset. The problems included in the MATH dataset are relatively difficult, and it is not always possible to implement Python code to solve them directly. Our method can combine the strengths of Python calculations and CoT reasonings, enabling LLMs to use CoT to solve reasoning that cannot be achieved with Python code while reducing CoT reasoning errors with Python calculations. On the other hand, the problems in the GSM8K dataset can be easily implemented with Python code, which may have resulted in decreased performance due to CoT-derived reasoning errors.</p>
<p>Table 1: Comparison of the success rate of each method on the GSM8K and MATH.</p>
<h2>4 Discussion and Conclusion</h2>
<p>We demonstrate that the reasoning ability of LLMs can be enhanced by having LLMs generate structured text in a markup language. Structuring the text generated by LLMs allows us to easily parse it, allowing us to control the behavior of LLMs and integrate LLMs with the Python REPL flexibly. Also, the tags qualify and divide texts into sections, allowing us to define relationships between texts and inducing LLMs behavior that prioritizes certain sentences over others.</p>
<p>In addition to speech, humans have a variety of language output ports, including internal speech and writing. The use of markup languages to qualify and divide generated text into semantic and functional units is the equivalent to LLMs having such a variety of language output ports. In this study, two tags, PYTHON and THINK, were given to LLMs, but theoretically, any variety of tags can be defined, such as tags that output queries for web search or database operations or tags that output internal speech. However, the use of such a markup language is not a natural behavior for existing LLMs, and it is unclear whether LLMs can handle more complex syntax. Therefore, one intriguing future research direction is the fine-tuning of LLMs based on the use of markup languages.</p>
<h1>5 Limitations</h1>
<p>While we have successfully corrected some CoT errors with Python execution result feedback, this approach does not guarantee consistent success. Consequently, the number of errors caused by the limitations of LLMs will be higher compared to scenarios where LLMs generate Python code without CoT. As a result, in situations where there is no synergy between CoT and external tools, the performance will be reduced. Additionally, while it is possible to make LLMs acknowledge their errors, understanding the specific details of these errors often remains challenging. We are currently exploring potential solutions to address these limitations in principle.
Because LLMs are trained on a large corpus of text collected from the web, they have the potential to amplify biases in the training data. Thus, our method, which utilizes the statistical patterns that LLMs acquire via prompting, has the same problem. However, we believe that our method enables the seamless integration of LLMs with external resources and has the potential to correct the biases of LLMs.</p>
<h2>References</h2>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/paper/ 2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 24824-24837. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.</p>
<p>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021.</p>
<p>OpenAI. Gpt-4 technical report, 2023.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 3843-3857. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf.</p>
<p>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022.</p>
<p>Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models, 2023.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models, 2021.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools, 2023.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv preprint arXiv:2211.12588, 2022.
Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. LILA: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5807-5832, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.392.
OpenAI. OpenAI: Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022.
Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Advances in Neural Information Processing Systems, volume 35, pages 22199-22213, 2022.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021.</p>
<h1>A Example Prompt</h1>
<p>Figure 5 shows an example of the prompts we used. By writing the prompt itself in LPML format, we can strongly prompt LLMs to generate text in LPML format.</p>
<h2>B Examples of Reasoning Processes</h2>
<p>Figures 6, 7, and 8 show examples of the inference process. LPML definitions and instructions to LLMs are omitted. In the example in Figure 6, the LLM makes a computational error within the CoT, but corrects the error using the Python results and reaches the correct answer. On the other hand, in the example in Figure 7, the LLM writes the correct Python code but ignores the results and outputs a different answer. In the example in Figure 8, there is a problem with the Python code, but the CoT has arrived at the correct answer. As these examples show, LLM's way to use external tools is inconsistent, and the success or failure of the reasoning still depends on chance.</p>
<p><DEFIRE type="rule">
The system and the assistant exchange messages.
All messages must be formatted in XML format. XML element ::= <tag>content</tag> | <tag>
Tags determine the meaning and function of the content. The content must not contradict the definition of the tag.
</DEFIRE>
<DEFIRE type="tag" name="DEFIRE">Defines rules and tags. The defined content is absolute.</DEFIRE>
<DEFIRE type="tag" name="EOS">Indicates the end of a message.</DEFIRE>
<DEFIRE type="tag" name="THINK">Represents a thought process. The thought process must be described step by step.</DEFIRE>
<DEFIRE type="tag" name="PYTHON">Represents an executable Python code.</DEFIRE>
<DEFIRE type="tag" name="OUTPUT">Represents a messages from the system to the assistant.</DEFIRE>
<DEFIRE type="tag" name="PROBLEM">Represents a problem to be solved.</DEFIRE>
<DEFIRE type="tag" name="ANSWER">Represents an answer to the problem.</DEFIRE>
<DEFIRE type="rule" role="system">
The system is a computer that supports the assistant.
When the system finds a PYTHON element in the assistant's message, the system executes it and returns the result to the assistant using the OUTPUT tag.
</DEFIRE>
<DEFIRE type="rule" role="assistant">
The assistant solves mathematical problems using the PYTHON tag and writes the answer using the ANSWER tag. The assistant is only allowed to use the PYTHON, THINK, and ANSWER tags.
The assistant translates the given problem into Python code to solve it and resolves the problem by executing it through the system.
When performing calculations, algebraic manipulations, or reasoning, the assistant must always use the PYTHON tag. In other words, calculations should not be performed within the THINK tag. If calculations are unavoidably performed within the THINK tag, the assistant must restart all those calculations from the beginning using the PYTHON tag. The assistant should never write the answer within the THINK tag.
The assistant must trust calculations performed using the PYTHON tag more than those performed within the THINK tag.
Therefore, if the content of the OUTPUT element differs from the content of the THINK element, the assistant must assume there is an error in the THINK element and correct it. After correcting the content of the THINK element, the assistant must validate it again using the PYTHON tag. If
no matter how many corrections are made to the content of the THINK element, it still contradicts the content of the OUTPUT element, the assistant must consider the OUTPUT element to be correct.
If the assistant uses the THINK element, it can use the ANSWER tag only when the content of the THINK element and the content of the OUTPUT element are not contradictory. However, if modifying the content of the THINK element twice still results in conflicting content between the two, the assistant must consider the content of the OUTPUT element to be correct.
When using the PYTHON tag, the assistant must use the 'print()' function to output values. Additionally, after using the PYTHON tag, the
assistant must immediately use the EOS tag to interrupt the message and wait for the system to return the execution result. In other words, it should be done as follows: <PYTHON>code</PYTHON><EOS>
The assistant can use the following Python libraries: sympy, math, numpy.
It is not necessary to solve problems analytically, and the use of brute force or numerical methods with the PYTHON tag is also allowed.
</DEFIRE>
<PROBLEM>
An integer is randomly selected from the integers 0 through 99 , inclusive. What is the probability that the integer is in the range 10 through 20, inclusive? Express your answer as a common fraction.
</PROBLEM></p>
<OUTPUT>
This is the system. Waiting for a message from the assistant.
</OUTPUT>
<p><EOS></p>
<p>Figure 5: Example prompt.</p>
<div class="codehilite"><pre><span></span><code>system:
<span class="nt">&lt;PROBLEM&gt;</span>
What<span class="w"> </span>is<span class="w"> </span>the<span class="w"> </span>arithmetic<span class="w"> </span>mean<span class="w"> </span>of<span class="w"> </span>all<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>positive<span class="w"> </span>two-digit<span class="w"> </span>integers<span class="w"> </span>with<span class="w"> </span>the<span class="w"> </span>property<span class="w"> </span>that<span class="w"> </span>the<span class="w"> </span>integer<span class="w"> </span>is<span class="w"> </span>equal<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>sum
of<span class="w"> </span>its<span class="w"> </span>first<span class="w"> </span>digit<span class="w"> </span>plus<span class="w"> </span>its<span class="w"> </span>second<span class="w"> </span>digit<span class="w"> </span>plus<span class="w"> </span>the<span class="w"> </span>product<span class="w"> </span>of<span class="w"> </span>its<span class="w"> </span>two<span class="w"> </span>digits?
<span class="nt">&lt;/PROBLEM&gt;</span>
assistant:
<span class="nt">&lt;THINK&gt;</span>
Let&#39;s<span class="w"> </span>first<span class="w"> </span>try<span class="w"> </span>to<span class="w"> </span>understand<span class="w"> </span>the<span class="w"> </span>problem<span class="w"> </span>statement.<span class="w"> </span>We<span class="w"> </span>need<span class="w"> </span>to<span class="w"> </span>find<span class="w"> </span>the<span class="w"> </span>arithmetic<span class="w"> </span>mean<span class="w"> </span>of<span class="w"> </span>all<span class="w"> </span>two-digit<span class="w"> </span>integers<span class="w"> </span>that
satisfy<span class="w"> </span>the<span class="w"> </span>following<span class="w"> </span>condition:
-<span class="w"> </span>The<span class="w"> </span>integer<span class="w"> </span>is<span class="w"> </span>equal<span class="w"> </span>to<span class="w"> </span>the<span class="w"> </span>sum<span class="w"> </span>of<span class="w"> </span>its<span class="w"> </span>first<span class="w"> </span>digit<span class="w"> </span>and<span class="w"> </span>its<span class="w"> </span>second<span class="w"> </span>digit<span class="w"> </span>and<span class="w"> </span>the<span class="w"> </span>product<span class="w"> </span>of<span class="w"> </span>its<span class="w"> </span>two<span class="w"> </span>digits.
Let&#39;s<span class="w"> </span>say<span class="w"> </span>the<span class="w"> </span>two-digit<span class="w"> </span>integer<span class="w"> </span>is<span class="w"> </span>represented<span class="w"> </span>as<span class="w"> </span>&quot;ab&quot;,<span class="w"> </span>where<span class="w"> </span>a<span class="w"> </span>and<span class="w"> </span>b<span class="w"> </span>are<span class="w"> </span>the<span class="w"> </span>first<span class="w"> </span>and<span class="w"> </span>second<span class="w"> </span>digits,<span class="w"> </span>respectively.<span class="w"> </span>Then,<span class="w"> </span>we
can<span class="w"> </span>write<span class="w"> </span>the<span class="w"> </span>condition<span class="w"> </span>as:
10a<span class="w"> </span>+<span class="w"> </span>b<span class="w"> </span>=<span class="w"> </span>a<span class="w"> </span>+<span class="w"> </span>b<span class="w"> </span>+<span class="w"> </span>ab
Simplifying<span class="w"> </span>this<span class="w"> </span>equation,<span class="w"> </span>we<span class="w"> </span>get:
9a<span class="w"> </span>=<span class="w"> </span>ab
b<span class="w"> </span>=<span class="w"> </span>9
</code></pre></div>

<p>So, we see that $b$ must be 9 . Therefore, the two-digit integer can be represented as "a9". Substituting this in the original equation, we get:
$10 a+9=a+9+9 a$
Simplifying this equation, we get:
$a=1$</p>
<p>So, the two-digit integers that satisfy the given condition are $19,29, \ldots, 99$. There are 9 such integers. Therefore, the arithmetic mean of these integers is:
$(19+29+\ldots+99) / 9=54$</p>
<p>Therefore, the answer is 54 .
$&lt;/$ THINK $&gt;$
<PYTHON>
# Let's verify the answer using Python code
# Find all two-digit integers that satisfy the given condition
nums $=[]$
for i in range $(10,100)$ :
a, b = divmod $(i, 10)$
if $10 * a+b==a+b+a * b$ :
nums.append(i)
# Calculate the arithmetic mean
mean $=$ sum(nums) / len(nums)
# Output the answer
print(mean)
</PYTHON></p>
<div class="codehilite"><pre><span></span><code>system:
<span class="nt">&lt;OUTPUT</span><span class="w"> </span><span class="na">tool=</span><span class="s">&quot;PYTHON&quot;</span><span class="nt">&gt;</span>
59.0
<span class="nt">&lt;/OUTPUT&gt;</span>
</code></pre></div>

<p>assistant:
<THINK>
It seems like there is a discrepancy between the answer I calculated using the THINK tag and the answer returned by the system using the PYTHON tag. Let me check my calculations again.</p>
<p>The two-digit integers that satisfy the given condition are $19,29, \ldots, 99$. There are 9 such integers. Therefore, the arithmetic mean of these integers is:
$(19+29+\ldots+99) / 9=(9 / 2)(19+99) / 9=59$</p>
<p>So, my initial answer was incorrect. The correct answer is 59 .
$&lt;/$ THINK $&gt;$
<ANSWER>
The arithmetic mean of all of the positive two-digit integers with the property that the integer is equal to the sum of its first digit plus its second digit plus the product of its two digits is 59 .
$&lt;/$ ANSWER $&gt;$</p>
<p>Figure 6: The LLM makes a computational error within the CoT, but corrects the error using the Python results and reaches the correct answer</p>
<div class="codehilite"><pre><span></span><code>system:
<span class="nt">&lt;PROBLEM&gt;</span>
Compute
\[\sum_{1<span class="w"> </span>\le<span class="w"> </span>j<span class="w"> </span><span class="nt">&lt; i</span><span class="err">}</span><span class="w"> </span><span class="err">\frac{1}{2^{i</span><span class="w"> </span><span class="err">+</span><span class="w"> </span><span class="err">j}},\]where</span><span class="w"> </span><span class="err">the</span><span class="w"> </span><span class="err">sum</span><span class="w"> </span><span class="err">is</span><span class="w"> </span><span class="err">taken</span><span class="w"> </span><span class="err">over</span><span class="w"> </span><span class="err">all</span><span class="w"> </span><span class="err">positive</span><span class="w"> </span><span class="err">integers</span><span class="w"> </span><span class="err">$i$</span><span class="w"> </span><span class="err">and</span><span class="w"> </span><span class="err">$j$</span><span class="w"> </span><span class="err">such</span><span class="w"> </span><span class="err">that</span>
<span class="err">$1</span><span class="w"> </span><span class="err">\le</span><span class="w"> </span><span class="err">j&lt;i.$</span>
<span class="err">&lt;/PROBLEM</span><span class="nt">&gt;</span>
assistant:
<span class="nt">&lt;THINK&gt;</span>
We<span class="w"> </span>can<span class="w"> </span>write<span class="w"> </span>out<span class="w"> </span>the<span class="w"> </span>first<span class="w"> </span>few<span class="w"> </span>terms<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span>sum<span class="w"> </span>to<span class="w"> </span>see<span class="w"> </span>if<span class="w"> </span>we<span class="w"> </span>can<span class="w"> </span>spot<span class="w"> </span>a<span class="w"> </span>pattern:
\begin{align*}
\frac{1}{2^2}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^3}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^4}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^5}<span class="w"> </span>+<span class="w"> </span>\dotsb<span class="w"> </span><span class="err">&amp;</span>=<span class="w"> </span>\frac{1}{2^2}<span class="w"> </span>\cdot<span class="w"> </span>\left(1<span class="w"> </span>+<span class="w"> </span>\frac{1}{2}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^2}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^3}<span class="w"> </span>+<span class="w"> </span>\dotsb\right)<span class="w"> </span>\\
<span class="err">&amp;</span>=<span class="w"> </span>\frac{1}{2^2}<span class="w"> </span>\cdot<span class="w"> </span>\frac{1}{1<span class="w"> </span>-<span class="w"> </span>1/2}<span class="w"> </span>\cdot<span class="w"> </span>\frac{1}{2}
<span class="err">&amp;</span>=<span class="w"> </span>\frac{1}{4}.
\end{align*}
Similarly,<span class="w"> </span>we<span class="w"> </span>can<span class="w"> </span>write<span class="w"> </span>out<span class="w"> </span>the<span class="w"> </span>terms<span class="w"> </span>for<span class="w"> </span>$i<span class="w"> </span>=<span class="w"> </span>3,$<span class="w"> </span>$i<span class="w"> </span>=<span class="w"> </span>4,$<span class="w"> </span>and<span class="w"> </span>so<span class="w"> </span>on,<span class="w"> </span>and<span class="w"> </span>we<span class="w"> </span>get
\[\frac{1}{2^3}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^4}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^5}<span class="w"> </span>+<span class="w"> </span>\dotsb<span class="w"> </span>=<span class="w"> </span>\frac{1}{8},\]<span class="w"> </span>\\
\[\frac{1}{2^4}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^5}<span class="w"> </span>+<span class="w"> </span>\frac{1}{2^6}<span class="w"> </span>+<span class="w"> </span>\dotsb<span class="w"> </span>=<span class="w"> </span>\frac{1}{16},]
and<span class="w"> </span>so<span class="w"> </span>on.<span class="w"> </span>Thus,<span class="w"> </span>the<span class="w"> </span>sum<span class="w"> </span>we<span class="w"> </span>want<span class="w"> </span>to<span class="w"> </span>compute<span class="w"> </span>is
\[\frac{1}{4}<span class="w"> </span>+<span class="w"> </span>\frac{1}{8}<span class="w"> </span>+<span class="w"> </span>\frac{1}{16}<span class="w"> </span>+<span class="w"> </span>\dotsb.]
This<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>geometric<span class="w"> </span>series<span class="w"> </span>with<span class="w"> </span>first<span class="w"> </span>term<span class="w"> </span>$\frac{1}{4}$<span class="w"> </span>and<span class="w"> </span>common<span class="w"> </span>ratio<span class="w"> </span>$\frac{1}{2},$<span class="w"> </span>so<span class="w"> </span>its<span class="w"> </span>sum<span class="w"> </span>is
\[\frac{\frac{1}{4}}{1<span class="w"> </span>-<span class="w"> </span>\frac{1}{2}}<span class="w"> </span>=<span class="w"> </span>\bmod{\frac{1}{2}}.]
<span class="nt">&lt;/THINK&gt;</span>
<span class="nt">&lt;PYTHON&gt;</span>
sum<span class="w"> </span>=<span class="w"> </span>0
for<span class="w"> </span>i<span class="w"> </span>in<span class="w"> </span>range(2,<span class="w"> </span>100):
<span class="w">    </span>for<span class="w"> </span>j<span class="w"> </span>in<span class="w"> </span>range(1,<span class="w"> </span>i):
<span class="w">        </span>sum<span class="w"> </span>+=<span class="w"> </span>1<span class="w"> </span>/<span class="w"> </span>2**(i<span class="w"> </span>+<span class="w"> </span>j)
print(sum)
<span class="nt">&lt;/PYTHON&gt;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>system:
<span class="nt">&lt;OUTPUT</span><span class="w"> </span><span class="na">tool=</span><span class="s">&quot;PYTHON&quot;</span><span class="nt">&gt;</span>
0.33333333333333326
<span class="nt">&lt;/OUTPUT&gt;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code><span class="nv">assistant</span><span class="o">:</span>
<span class="o">&lt;</span><span class="nv">THINK</span><span class="o">&gt;</span>
<span class="nv">Our</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">Python</span><span class="w"> </span><span class="nv">code</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">same</span><span class="w"> </span><span class="nv">as</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">we</span><span class="w"> </span><span class="nv">got</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="nv">our</span><span class="w"> </span><span class="nv">previous</span><span class="w"> </span><span class="nv">calculations</span><span class="o">.</span><span class="w"> </span><span class="nv">Let</span><span class="o">&#39;</span><span class="nv">s</span><span class="w"> </span><span class="nv">try</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">figure</span><span class="w"> </span><span class="nv">out</span>
<span class="nv">which</span><span class="w"> </span><span class="nv">one</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">correct</span><span class="o">.</span>
<span class="nv">We</span><span class="w"> </span><span class="nv">can</span><span class="w"> </span><span class="nv">see</span><span class="w"> </span><span class="nv">that</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">sum</span><span class="w"> </span><span class="nv">we</span><span class="w"> </span><span class="nv">want</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">compute</span><span class="w"> </span><span class="nv">is</span>
\<span class="p">[</span>\<span class="nv">sum_</span><span class="p">{</span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2</span><span class="p">}</span><span class="o">^</span>\<span class="no">inf</span><span class="nv">ty</span><span class="w"> </span>\<span class="nv">sum_</span><span class="p">{</span><span class="nv">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">}</span><span class="o">^</span><span class="p">{</span><span class="nv">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="nv">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">j</span><span class="p">}}</span><span class="o">.</span><span class="p">]</span>
<span class="nv">We</span><span class="w"> </span><span class="nv">can</span><span class="w"> </span><span class="nv">switch</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="ow">or</span><span class="nv">der</span><span class="w"> </span><span class="nv">of</span><span class="w"> </span><span class="nv">summation</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">get</span>
\<span class="p">[</span>\<span class="nv">sum_</span><span class="p">{</span><span class="nv">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">}</span><span class="o">^</span>\<span class="no">inf</span><span class="nv">ty</span><span class="w"> </span>\<span class="nv">sum_</span><span class="p">{</span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">}</span><span class="o">^</span>\<span class="no">inf</span><span class="nv">ty</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="nv">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">j</span><span class="p">}}</span><span class="o">.</span><span class="p">]</span>
<span class="nv">Then</span><span class="p">,</span><span class="w"> </span><span class="nv">we</span><span class="w"> </span><span class="nv">can</span><span class="w"> </span><span class="nv">evaluate</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">inner</span><span class="w"> </span><span class="nv">sum</span><span class="o">:</span>
\<span class="nv">begin</span><span class="p">{</span><span class="nv">align</span><span class="o">*</span><span class="p">}</span>
\<span class="nv">sum_</span><span class="p">{</span><span class="nv">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nv">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">}</span><span class="o">^</span>\<span class="no">inf</span><span class="nv">ty</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="nv">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nv">j</span><span class="p">}}</span><span class="w"> </span>&amp;<span class="o">=</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="nv">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">}}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="nv">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="p">}}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="nv">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">3</span><span class="p">}}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="k">do</span><span class="nv">tsb</span><span class="w"> </span>\\
&amp;<span class="o">=</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="nv">j</span><span class="p">}}</span><span class="w"> </span>\<span class="nv">cdot</span><span class="w"> </span>\<span class="nf">left</span><span class="p">(</span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="mi">3</span><span class="p">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="k">do</span><span class="nv">tsb</span>\<span class="nv">right</span><span class="p">)</span><span class="w"> </span>\\
&amp;<span class="o">=</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="nv">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">}}</span><span class="o">.</span>
\<span class="nv">end</span><span class="p">{</span><span class="nv">align</span><span class="o">*</span><span class="p">}</span>
<span class="nv">Thus</span><span class="p">,</span><span class="w"> </span><span class="nv">our</span><span class="w"> </span><span class="nv">sum</span><span class="w"> </span><span class="nv">is</span>
\<span class="nv">begin</span><span class="p">{</span><span class="nv">align</span><span class="o">*</span><span class="p">}</span>
\<span class="nv">sum_</span><span class="p">{</span><span class="nv">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">}</span><span class="o">^</span>\<span class="no">inf</span><span class="nv">ty</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="p">{</span><span class="mi">2</span><span class="nv">j</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="p">}}</span><span class="w"> </span>&amp;<span class="o">=</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="mi">3</span><span class="p">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="mi">5</span><span class="p">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="mi">7</span><span class="p">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="k">do</span><span class="nv">tsb</span><span class="w"> </span>\\
&amp;<span class="o">=</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">8</span><span class="p">}</span><span class="w"> </span>\<span class="nv">cdot</span><span class="w"> </span>\<span class="nf">left</span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">2</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span><span class="o">+</span><span class="w"> </span>\<span class="k">do</span><span class="nv">tsb</span>\<span class="nv">right</span><span class="p">)</span><span class="w"> </span>\\
&amp;<span class="o">=</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">8</span><span class="p">}</span><span class="w"> </span>\<span class="nv">cdot</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">}</span>
&amp;<span class="o">=</span><span class="w"> </span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">4</span><span class="p">}</span><span class="o">.</span>
\<span class="nv">end</span><span class="p">{</span><span class="nv">align</span><span class="o">*</span><span class="p">}</span>
<span class="nv">Therefore</span><span class="p">,</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="k">from</span><span class="w"> </span><span class="nv">our</span><span class="w"> </span><span class="nv">Python</span><span class="w"> </span><span class="nv">code</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="nv">incorrect</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="nv">the</span><span class="w"> </span><span class="nv">answer</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="p">$</span>\<span class="nv">bmod</span><span class="p">{</span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">4</span><span class="p">}}</span><span class="o">.</span><span class="p">$</span>
<span class="o">&lt;/</span><span class="nv">THINK</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="nv">ANSWER</span><span class="o">&gt;</span>
<span class="nv">The</span><span class="w"> </span><span class="nv">sum</span><span class="w"> </span><span class="nv">is</span><span class="w"> </span><span class="p">$</span>\<span class="nv">bmod</span><span class="p">{</span>\<span class="nv">frac</span><span class="p">{</span><span class="mi">1</span><span class="p">}{</span><span class="mi">4</span><span class="p">}}</span><span class="o">.</span><span class="p">$</span>
<span class="o">&lt;/</span><span class="nv">ANSWER</span><span class="o">&gt;</span>
</code></pre></div>

<p>Figure 7: The LLM writes the correct Python code but ignores the results and outputs a different answer.</p>
<div class="codehilite"><pre><span></span><code><span class="n">system</span><span class="p">:</span>
<span class="o">&lt;</span><span class="n">PROBLEM</span><span class="o">&gt;</span>
<span class="n">Evaluate</span> <span class="err">$</span><span class="p">(</span><span class="o">-</span><span class="mi">125</span><span class="p">)</span><span class="o">^</span><span class="p">{</span><span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="p">}</span><span class="err">$$</span><span class="o">.</span>
<span class="o">&lt;/</span><span class="n">PROBLEM</span><span class="o">&gt;</span>
<span class="n">assistant</span><span class="p">:</span>
<span class="o">&lt;</span><span class="n">PYTHON</span><span class="o">&gt;</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="nb">print</span><span class="p">((</span><span class="o">-</span><span class="mi">125</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">4</span><span class="o">/</span><span class="mi">3</span><span class="p">))</span>
<span class="o">&lt;/</span><span class="n">PYTHON</span><span class="o">&gt;</span>
</code></pre></div>

<div class="codehilite"><pre><span></span><code>system:
<span class="nt">&lt;OUTPUT</span><span class="w"> </span><span class="na">tool=</span><span class="s">&quot;PYTHON&quot;</span><span class="nt">&gt;</span>
(-312.500000000000017-541.2658773652738j)
<span class="nt">&lt;/OUTPUT&gt;</span>
assistant:
<span class="nt">&lt;THINK&gt;</span>
We<span class="w"> </span>want<span class="w"> </span>to<span class="w"> </span>evaluate<span class="w"> </span>$(-125)^{4/3}$.<span class="w"> </span>We<span class="w"> </span>can<span class="w"> </span>write<span class="w"> </span>this<span class="w"> </span>as<span class="w"> </span>$(-1)^{4/3}\backslash<span class="w"> </span>c<span class="w"> </span>d<span class="w"> </span>o<span class="w"> </span>t<span class="w"> </span>125^{4/3}$.
Now,<span class="w"> </span>$(-1)^{4/3}=((-1)^{-2})^{2<span class="w"> </span>/<span class="w"> </span>3}=1^{-2<span class="w"> </span>/<span class="w"> </span>3}=1$.
So,<span class="w"> </span>we<span class="w"> </span>just<span class="w"> </span>need<span class="w"> </span>to<span class="w"> </span>evaluate<span class="w"> </span>$125^{4/3}$.<span class="w"> </span>We<span class="w"> </span>can<span class="w"> </span>write<span class="w"> </span>this<span class="w"> </span>as<span class="w"> </span>$(125^{1/3})^4$.
Using<span class="w"> </span>the<span class="w"> </span>fact<span class="w"> </span>that<span class="w"> </span>$125<span class="w"> </span>=<span class="w"> </span>5^3$,<span class="w"> </span>we<span class="w"> </span>have<span class="w"> </span>$125^{1/3}<span class="w"> </span>=<span class="w"> </span>5$.
Therefore,<span class="w"> </span>$(-125)^{4/3}=(-1)^{4/3}\cdot<span class="w"> </span>c<span class="w"> </span>d<span class="w"> </span>o<span class="w"> </span>t<span class="w"> </span>125^{4/3}=1\cdot(5)^{-4}=\backslash<span class="w"> </span>\operatorname{boxed}{625}$.
<span class="nt">&lt;/THINK&gt;</span>
<span class="nt">&lt;ANSWER&gt;</span>
$(-125)^{-4/3}=\backslash<span class="w"> </span>\operatorname{boxed}{625}$.
<span class="nt">&lt;/ANSWER&gt;</span>
</code></pre></div>

<p>Figure 8: There is a problem with the Python code, but the CoT has arrived at the correct answer.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*This work was done when the author was at Albert Inc.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
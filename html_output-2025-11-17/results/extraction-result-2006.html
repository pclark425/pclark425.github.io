<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2006 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2006</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2006</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-47.html">extraction-schema-47</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <p><strong>Paper ID:</strong> paper-278996852</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.23045v1.pdf" target="_blank">Multi-Sourced Compositional Generalization in Visual Question Answering</a></p>
                <p><strong>Paper Abstract:</strong> Compositional generalization is the ability of generalizing novel compositions from seen primitives, and has received much attention in vision-and-language (V\&L) recently. Due to the multi-modal nature of V\&L tasks, the primitives composing compositions source from different modalities, resulting in multi-sourced novel compositions. However, the generalization ability over multi-sourced novel compositions, \textit{i.e.}, multi-sourced compositional generalization (MSCG) remains unexplored. In this paper, we explore MSCG in the context of visual question answering (VQA), and propose a retrieval-augmented training framework to enhance the MSCG ability of VQA models by learning unified representations for primitives from different modalities. Specifically, semantically equivalent primitives are retrieved for each primitive in the training samples, and the retrieved features are aggregated with the original primitive to refine the model. This process helps the model learn consistent representations for the same semantic primitives across different modalities. To evaluate the MSCG ability of VQA models, we construct a new GQA-MSCG dataset based on the GQA dataset, in which samples include three types of novel compositions composed of primitives from different modalities. Experimental results demonstrate the effectiveness of the proposed framework. We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2006.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2006.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG-for-MSCG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Training Framework for Multi-Sourced Compositional Generalization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented training method that builds linguistic and visual primitive databases from the training set, retrieves semantically-similar primitive features (across modalities) for each primitive in a training sample, aggregates retrieved features with the original primitive features, and trains VQA models with these aggregated features to improve generalization to novel multi-sourced compositions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>Vision-and-Language: Visual Question Answering (VQA) with a focus on multi-sourced compositional generalization</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>CFR, Qwen-VL (and comparisons to LLaVA variants, BLIP-2 etc.) with and without RAG (denoted CFR+RAG, Qwen-VL+RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>no explicit curriculum; end-to-end fine-tuning with retrieval-augmented feature replacement during training (no phased primitive-first or staged curriculum learning)</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Level-1 to Level-3, where levels correspond to co-occurrence counts of novel composition categories in a sample (1–3 types of novel compositions co-occurring)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>GQA-MSCG splits separate novel compositions by modality: LL (linguistic-linguistic), VV (visual-visual), LV (linguistic-visual) and combinations LL+VV, LL+LV, VV+LV, LL+VV+LV; 5,000 samples randomly sampled per test category (total 35,000 test samples across splits)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Per Table 1 (accuracy % on GQA-MSCG splits): CFR (baseline) overall 72.41%; CFR+RAG overall 74.21% (per-split examples: CFR LL 74.78 -> CFR+RAG LL 76.28; CFR VV 72.18 -> 73.88; CFR LV 73.36 -> 75.64). Qwen-VL baseline overall 68.49%; Qwen-VL+RAG overall 71.53% (Qwen-VL LL 72.24 -> +RAG 74.68; VV 65.54 -> 68.90; LV 69.06 -> 71.98). Other model examples: LLaVA-1.5 LL 70.22%, LL+VV+LV 64.14% demonstrating drop with increasing co-occurrence.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Observed performance decreases as co-occurrence level increases (Level-1 → Level-3). Example: LLaVA-1.5: LL (Level-1) 70.22% → LL+VV (Level-2) 67.04% → LL+VV+LV (Level-3) 64.14%; authors explicitly state model performance gradually decreases as level increases.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>No curriculum baseline compared; comparison is between baseline models (end-to-end fine-tuning) and the same models with RAG. RAG improved MSCG performance: CFR overall +1.80 percentage points (72.41% → 74.21%); Qwen-VL overall +3.04 percentage points (68.49% → 71.53%). Per-split improvements shown in Table 1 (examples provided above).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Introducing retrieval of semantically-equivalent primitives from both linguistic and visual primitive databases and aggregating them with original features during training improves VQA model accuracy on novel multi-sourced compositions (MSCG) and slightly improves IID performance; model performance degrades as the number/co-occurrence of novel composition categories in a sample increases (Level-1 → Level-3), and multi-modal retrieval (both D_q and D_v) yields the best gains compared to single-modality retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports/nuances - Supports the notion that better primitive representations and cross-modal alignment reduce compositional generalization gaps (i.e., improving primitive representation alignment improves generalization), and provides nuance that compositional difficulty scales with co-occurrence/complexity (levels) — retrieval augmentation reduces but does not eliminate degradation as composition complexity increases.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2006.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2006.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GQA-MSCG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GQA-MSCG Dataset (Multi-Sourced Compositional Generalization version of GQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diagnostic dataset derived from GQA that partitions test samples by modality composition types (LL, VV, LV) and combinations (LL+VV, LL+LV, VV+LV, LL+VV+LV) to evaluate multi-sourced compositional generalization in VQA; each category sampled to 5,000 examples yielding 35,000 total test samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>Dataset for VQA compositional generalization evaluation (multi-modal compositions)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>Used to evaluate VQA models such as CFR, Qwen-VL, LLaVA-1.5/1.6, BLIP-2, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>dataset design only; no curriculum imposed. Test splits are grouped by modality composition types and by co-occurrence levels (Level-1 to Level-3).</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>Tests co-occurrence of 1 to 3 types of novel compositions per sample (Level-1 to Level-3), not explicit number-of-primitives depth beyond two-primitive binary compositions used to form 'compositions'.</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Three base composition types (LL, VV, LV) each with varied contextual instances; combination splits include concurrent novel compositions of multiple types. Construction: extract linguistic and visual primitives from GQA train-balanced split; sample T_q and T_v instances per primitive to capture contextual diversity; 5,000 test samples per category.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Dataset used to report novel-composition accuracies in Table 1 (see RAG-for-MSCG entry for numbers).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Dataset designed to show empirical scaling of difficulty with co-occurrence level (Level-1→Level-3), with observed decreasing accuracies across models as level increases.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>The GQA-MSCG dataset enables fine-grained measurement of multi-sourced compositional generalization by modality type and by co-occurrence of novel composition types; the dataset reveals consistent accuracy declines as the number of concurrent novel composition types per sample increases.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports - dataset evidence shows compositional complexity (co-occurrence depth) increases generalization difficulty, consistent with theories predicting a compositional generalization gap that widens with composition complexity.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2006.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2006.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of experiments, studies, or results related to curriculum learning for compositional tasks, compositional generalization performance, primitive skill training, composition depth effects, and generalization gaps between trained and novel compositions.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ablation+ParamAnalysis</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ablation Studies and Parameter Analysis (retrieval databases and weighting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablations test retrieval from question-only database D_q, image-only database D_v, and both; parameter sweep for modality contribution weights w_q and w_v to study their impact on MSCG performance, leading to chosen setting w_q=0.6, w_v=0.4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>study_domain</strong></td>
                            <td>VQA MSCG-focused ablation and hyperparameter sensitivity analysis</td>
                        </tr>
                        <tr>
                            <td><strong>agent_or_model_name</strong></td>
                            <td>CFR baseline used for ablations (results reported also more broadly), other baselines similarly tuned</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_structure</strong></td>
                            <td>not applicable (analysis of retrieval components in end-to-end training)</td>
                        </tr>
                        <tr>
                            <td><strong>primitive_training_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_range</strong></td>
                            <td>evaluations performed across the GQA-MSCG Level-1..Level-3 splits (co-occurrence levels)</td>
                        </tr>
                        <tr>
                            <td><strong>compositional_diversity_description</strong></td>
                            <td>Ablations evaluated across the same per-split diversity of GQA-MSCG; ablations show modality-specific retrieval helps corresponding split (e.g., D_v retrieval helps VV splits)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_trained_compositions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_novel_compositions</strong></td>
                            <td>Table 4 (CFR examples): baseline CFR LL 74.78% / VV 72.18% / LV 73.36%; CFR+RAG with D_q only: LL 76.18% VV 73.46% LV 74.24%; CFR+RAG with D_v only: LL 75.66% VV 73.66% LV 74.80%; CFR+RAG with both D_q & D_v: LL 76.28% VV 73.88% LV 75.64% (best overall).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_gap_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>composition_depth_scaling</strong></td>
                            <td>Ablations reported across composition co-occurrence levels; param sweep results show performance peaks at particular weightings (w_q≈0.4–0.6, w_v≈0.4) and fluctuates otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>curriculum_vs_baseline_comparison</strong></td>
                            <td>Not a curriculum comparison; shows that using both retrieval databases outperforms single-database retrieval and baseline: e.g., CFR overall gain from baseline to best ablated RAG ~+1.80 percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_curriculum_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_correlation_effects</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_summary</strong></td>
                            <td>Retrieval from either linguistic or visual primitive databases individually improves performance especially on splits dominated by that modality, but joint retrieval from both databases produces the highest average accuracy; performance is sensitive to weighting between modalities and was empirically set to w_q=0.6, w_v=0.4 (authors used w_q=0.6,w_v=0.4 in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>nuances - indicates that aligning primitive representations across modalities (via retrieval) is beneficial and that modality-specific contributions matter; this nuances compositional generalization theory by showing cross-modal representation alignment as a practical mitigation for generalization gaps.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring the effect of primitives for compositional generalization in vision-andlanguage <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented primitive representations for compositional zero-shot learning <em>(Rating: 2)</em></li>
                <li>On compositional generalization of neural machine translation <em>(Rating: 2)</em></li>
                <li>Systematic generalization: what is required and can it be learned? <em>(Rating: 1)</em></li>
                <li>Overcoming language priors in visual question answering with cumulative learning strategy <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2006",
    "paper_id": "paper-278996852",
    "extraction_schema_id": "extraction-schema-47",
    "extracted_data": [
        {
            "name_short": "RAG-for-MSCG",
            "name_full": "Retrieval-Augmented Training Framework for Multi-Sourced Compositional Generalization",
            "brief_description": "A retrieval-augmented training method that builds linguistic and visual primitive databases from the training set, retrieves semantically-similar primitive features (across modalities) for each primitive in a training sample, aggregates retrieved features with the original primitive features, and trains VQA models with these aggregated features to improve generalization to novel multi-sourced compositions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "Vision-and-Language: Visual Question Answering (VQA) with a focus on multi-sourced compositional generalization",
            "agent_or_model_name": "CFR, Qwen-VL (and comparisons to LLaVA variants, BLIP-2 etc.) with and without RAG (denoted CFR+RAG, Qwen-VL+RAG)",
            "curriculum_structure": "no explicit curriculum; end-to-end fine-tuning with retrieval-augmented feature replacement during training (no phased primitive-first or staged curriculum learning)",
            "primitive_training_details": null,
            "composition_depth_range": "Level-1 to Level-3, where levels correspond to co-occurrence counts of novel composition categories in a sample (1–3 types of novel compositions co-occurring)",
            "compositional_diversity_description": "GQA-MSCG splits separate novel compositions by modality: LL (linguistic-linguistic), VV (visual-visual), LV (linguistic-visual) and combinations LL+VV, LL+LV, VV+LV, LL+VV+LV; 5,000 samples randomly sampled per test category (total 35,000 test samples across splits)",
            "performance_trained_compositions": null,
            "performance_novel_compositions": "Per Table 1 (accuracy % on GQA-MSCG splits): CFR (baseline) overall 72.41%; CFR+RAG overall 74.21% (per-split examples: CFR LL 74.78 -&gt; CFR+RAG LL 76.28; CFR VV 72.18 -&gt; 73.88; CFR LV 73.36 -&gt; 75.64). Qwen-VL baseline overall 68.49%; Qwen-VL+RAG overall 71.53% (Qwen-VL LL 72.24 -&gt; +RAG 74.68; VV 65.54 -&gt; 68.90; LV 69.06 -&gt; 71.98). Other model examples: LLaVA-1.5 LL 70.22%, LL+VV+LV 64.14% demonstrating drop with increasing co-occurrence.",
            "generalization_gap_measured": false,
            "generalization_gap_value": null,
            "composition_depth_scaling": "Observed performance decreases as co-occurrence level increases (Level-1 → Level-3). Example: LLaVA-1.5: LL (Level-1) 70.22% → LL+VV (Level-2) 67.04% → LL+VV+LV (Level-3) 64.14%; authors explicitly state model performance gradually decreases as level increases.",
            "curriculum_vs_baseline_comparison": "No curriculum baseline compared; comparison is between baseline models (end-to-end fine-tuning) and the same models with RAG. RAG improved MSCG performance: CFR overall +1.80 percentage points (72.41% → 74.21%); Qwen-VL overall +3.04 percentage points (68.49% → 71.53%). Per-split improvements shown in Table 1 (examples provided above).",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Introducing retrieval of semantically-equivalent primitives from both linguistic and visual primitive databases and aggregating them with original features during training improves VQA model accuracy on novel multi-sourced compositions (MSCG) and slightly improves IID performance; model performance degrades as the number/co-occurrence of novel composition categories in a sample increases (Level-1 → Level-3), and multi-modal retrieval (both D_q and D_v) yields the best gains compared to single-modality retrieval.",
            "supports_or_challenges_theory": "supports/nuances - Supports the notion that better primitive representations and cross-modal alignment reduce compositional generalization gaps (i.e., improving primitive representation alignment improves generalization), and provides nuance that compositional difficulty scales with co-occurrence/complexity (levels) — retrieval augmentation reduces but does not eliminate degradation as composition complexity increases.",
            "uuid": "e2006.0"
        },
        {
            "name_short": "GQA-MSCG",
            "name_full": "GQA-MSCG Dataset (Multi-Sourced Compositional Generalization version of GQA)",
            "brief_description": "A diagnostic dataset derived from GQA that partitions test samples by modality composition types (LL, VV, LV) and combinations (LL+VV, LL+LV, VV+LV, LL+VV+LV) to evaluate multi-sourced compositional generalization in VQA; each category sampled to 5,000 examples yielding 35,000 total test samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "Dataset for VQA compositional generalization evaluation (multi-modal compositions)",
            "agent_or_model_name": "Used to evaluate VQA models such as CFR, Qwen-VL, LLaVA-1.5/1.6, BLIP-2, etc.",
            "curriculum_structure": "dataset design only; no curriculum imposed. Test splits are grouped by modality composition types and by co-occurrence levels (Level-1 to Level-3).",
            "primitive_training_details": null,
            "composition_depth_range": "Tests co-occurrence of 1 to 3 types of novel compositions per sample (Level-1 to Level-3), not explicit number-of-primitives depth beyond two-primitive binary compositions used to form 'compositions'.",
            "compositional_diversity_description": "Three base composition types (LL, VV, LV) each with varied contextual instances; combination splits include concurrent novel compositions of multiple types. Construction: extract linguistic and visual primitives from GQA train-balanced split; sample T_q and T_v instances per primitive to capture contextual diversity; 5,000 test samples per category.",
            "performance_trained_compositions": null,
            "performance_novel_compositions": "Dataset used to report novel-composition accuracies in Table 1 (see RAG-for-MSCG entry for numbers).",
            "generalization_gap_measured": false,
            "generalization_gap_value": null,
            "composition_depth_scaling": "Dataset designed to show empirical scaling of difficulty with co-occurrence level (Level-1→Level-3), with observed decreasing accuracies across models as level increases.",
            "curriculum_vs_baseline_comparison": null,
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "The GQA-MSCG dataset enables fine-grained measurement of multi-sourced compositional generalization by modality type and by co-occurrence of novel composition types; the dataset reveals consistent accuracy declines as the number of concurrent novel composition types per sample increases.",
            "supports_or_challenges_theory": "supports - dataset evidence shows compositional complexity (co-occurrence depth) increases generalization difficulty, consistent with theories predicting a compositional generalization gap that widens with composition complexity.",
            "uuid": "e2006.1"
        },
        {
            "name_short": "Ablation+ParamAnalysis",
            "name_full": "Ablation Studies and Parameter Analysis (retrieval databases and weighting)",
            "brief_description": "Ablations test retrieval from question-only database D_q, image-only database D_v, and both; parameter sweep for modality contribution weights w_q and w_v to study their impact on MSCG performance, leading to chosen setting w_q=0.6, w_v=0.4.",
            "citation_title": "here",
            "mention_or_use": "use",
            "study_domain": "VQA MSCG-focused ablation and hyperparameter sensitivity analysis",
            "agent_or_model_name": "CFR baseline used for ablations (results reported also more broadly), other baselines similarly tuned",
            "curriculum_structure": "not applicable (analysis of retrieval components in end-to-end training)",
            "primitive_training_details": null,
            "composition_depth_range": "evaluations performed across the GQA-MSCG Level-1..Level-3 splits (co-occurrence levels)",
            "compositional_diversity_description": "Ablations evaluated across the same per-split diversity of GQA-MSCG; ablations show modality-specific retrieval helps corresponding split (e.g., D_v retrieval helps VV splits)",
            "performance_trained_compositions": null,
            "performance_novel_compositions": "Table 4 (CFR examples): baseline CFR LL 74.78% / VV 72.18% / LV 73.36%; CFR+RAG with D_q only: LL 76.18% VV 73.46% LV 74.24%; CFR+RAG with D_v only: LL 75.66% VV 73.66% LV 74.80%; CFR+RAG with both D_q & D_v: LL 76.28% VV 73.88% LV 75.64% (best overall).",
            "generalization_gap_measured": false,
            "generalization_gap_value": null,
            "composition_depth_scaling": "Ablations reported across composition co-occurrence levels; param sweep results show performance peaks at particular weightings (w_q≈0.4–0.6, w_v≈0.4) and fluctuates otherwise.",
            "curriculum_vs_baseline_comparison": "Not a curriculum comparison; shows that using both retrieval databases outperforms single-database retrieval and baseline: e.g., CFR overall gain from baseline to best ablated RAG ~+1.80 percentage points.",
            "adaptive_curriculum_used": false,
            "spurious_correlation_effects": null,
            "negative_transfer_observed": null,
            "key_findings_summary": "Retrieval from either linguistic or visual primitive databases individually improves performance especially on splits dominated by that modality, but joint retrieval from both databases produces the highest average accuracy; performance is sensitive to weighting between modalities and was empirically set to w_q=0.6, w_v=0.4 (authors used w_q=0.6,w_v=0.4 in experiments).",
            "supports_or_challenges_theory": "nuances - indicates that aligning primitive representations across modalities (via retrieval) is beneficial and that modality-specific contributions matter; this nuances compositional generalization theory by showing cross-modal representation alignment as a practical mitigation for generalization gaps.",
            "uuid": "e2006.2"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring the effect of primitives for compositional generalization in vision-andlanguage",
            "rating": 2
        },
        {
            "paper_title": "Retrieval-augmented primitive representations for compositional zero-shot learning",
            "rating": 2
        },
        {
            "paper_title": "On compositional generalization of neural machine translation",
            "rating": 2
        },
        {
            "paper_title": "Systematic generalization: what is required and can it be learned?",
            "rating": 1
        },
        {
            "paper_title": "Overcoming language priors in visual question answering with cumulative learning strategy",
            "rating": 1
        }
    ],
    "cost": 0.01076075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Multi-Sourced Compositional Generalization in Visual Question Answering
29 May 2025</p>
<p>Chuanhao Li lichuanhao@bit.edu.cn 
Beijing Key Laboratory of Intelligent Information Technology</p>
<p>School of Computer Science &amp; Technology
Beijing Institute of Technology
China</p>
<p>Guangdong Laboratory of Machine Perception and Intelligent Computing
Shenzhen MSU-BIT University
China</p>
<p>Wenbo Ye yewenbo@bit.edu.cn 
Beijing Key Laboratory of Intelligent Information Technology</p>
<p>School of Computer Science &amp; Technology
Beijing Institute of Technology
China</p>
<p>Guangdong Laboratory of Machine Perception and Intelligent Computing
Shenzhen MSU-BIT University
China</p>
<p>Zhen Li li.zhen@bit.edu.cn 
Yuwei Wu wuyuwei@bit.edu.cn 
Beijing Key Laboratory of Intelligent Information Technology</p>
<p>School of Computer Science &amp; Technology
Beijing Institute of Technology
China</p>
<p>Guangdong Laboratory of Machine Perception and Intelligent Computing
Shenzhen MSU-BIT University
China</p>
<p>Beijing Key Laboratory of Intelligent Information Technology</p>
<p>School of Computer Science &amp; Technology
Beijing Institute of Technology
China</p>
<p>Yunde Jia jiayunde@bit.edu.cn 
Beijing Key Laboratory of Intelligent Information Technology</p>
<p>School of Computer Science &amp; Technology
Beijing Institute of Technology
China</p>
<p>Guangdong Laboratory of Machine Perception and Intelligent Computing
Shenzhen MSU-BIT University
China</p>
<p>Multi-Sourced Compositional Generalization in Visual Question Answering
29 May 2025E5DF986956B4C35F9902F7270C3F002FarXiv:2505.23045v1[cs.CV]Novel Compositions Seen Primitives Linguistic: whitecatanimalgoldendoggolden
Compositional generalization is the ability of generalizing novel compositions from seen primitives, and has received much attention in vision-andlanguage (V&amp;L) recently.Due to the multi-modal nature of V&amp;L tasks, the primitives composing compositions source from different modalities, resulting in multi-sourced novel compositions.However, the generalization ability over multi-sourced novel compositions, i.e., multi-sourced compositional generalization (MSCG) remains unexplored.In this paper, we explore MSCG in the context of visual question answering (VQA), and propose a retrieval-augmented training framework to enhance the MSCG ability of VQA models by learning unified representations for primitives from different modalities.Specifically, semantically equivalent primitives are retrieved for each primitive in the training samples, and the retrieved features are aggregated with the original primitive to refine the model.This process helps the model learn consistent representations for the same semantic primitives across different modalities.To evaluate the MSCG ability of VQA models, we construct a new GQA-MSCG dataset based on the GQA dataset, in which samples include three types of novel compositions composed of primitives from different modalities.Experimental results demonstrate the effectiveness of the proposed framework.We release GQA-MSCG at https://github.com/NeverMoreLCH/MSCG.</p>
<p>Introduction</p>
<p>Compositional generalization refers to the ability of generalizing novel compositions from seen primitives.In visionand-language (V&amp;L), the primitives of a composition come from either the linguistic modality or the visual modality, i.e., compositions are multi-sourced.As shown in Figure 1, in the context of visual question answering (VQA), "white (linguistic modality) + dog (linguistic modality)" is an novel composition, and "white (linguistic modality) + (visual modal- (white), (golden), (green), (cat), (dog), (white cat), … ity)" and " (white + dog in visual modality)" are also novel compositions.However, prior works [Dankers et al., 2022;Li et al., 2024b;Li et al., 2023b] only consider novel compositions of primitives from a single modality (e.g., "white + dog").Whether the model's generalization ability to different modality primitives' novel compositions (i.e., multi-sourced novel compositions) remains unexplored.To generalize over multi-sourced compositions, a model needs to not only have the ability to understand individual primitives but also the ability to align primitives of different modalities.</p>
<p>To address the above issue, we propose a retrievalaugmented training framework.The basic idea of the framework is to learn similar representations for primitives from different modalities by retrieving semantically equivalent primitives, thereby maintaining consistent generalization ability for multi-sourced compositions.Specifically, the framework consists of three key components: retrieval database construction, feature retrieval, and feature aggregation.For the retrieval database, we construct separate primitive databases for the linguistic and visual modalities, where words with the same prototype are treated as the same linguistic primitive, and visual entities with the same label are treated as the same visual primitive.For the same primitive, both the linguistic/visual primitive databases contain multiple instances of that primitive in different contexts.For example, for the linguistic primitive "dog", the linguistic primitive database contains instances of "Is the dog black?", "How many dogs are there?","Do you see a golden dog in this picture?" in different contexts.During training, for each primitive in the training sample, semantically similar primitives from both the linguistic and visual primitive databases are retrieved.The retrieved features are then aggregated with the original primitive features to optimize the model.Since the representations in the primitive databases are updated throughout the training, the model continuously refines its current representation by leveraging the retrieved relevant features to learn similar representations for the same semantic primitives across different modalities and contexts.</p>
<p>To quantitatively evaluate the generalization ability of VQA models for multi-sourced novel compositions, we construct the GQA-MSCG dataset based on the GQA dataset [Hudson and Manning, 2019].We categorize compositions based on the modality of the primitives from the composition, resulting in three types of novel compositions: [Linguistic primitive, Linguistic primitive] (LL), [Visual primitive, Visual primitive] (VV), and [Linguistic primitive, Visual primitive] (LV).We construct three basic test splits, labeled LL, V V , and LV , each containing test samples of different types of novel compositions.To further explore how the co-occurrence of different types of novel compositions in samples affects model performance, we construct test splits, where each sample simultaneously contain two types of novel compositions: LL + V V , LL + LV , and V V + LV , as well as a test split, where each sample contain all three types of novel compositions: LL + V V + LV .Experimental results demonstrate that the proposed framework significantly improves VQA models' generalization ability to multi-sourced novel compositions while maintaining their independent and identically distributed (IID) generalization ability.</p>
<p>To sum up, our contributions are as follows:</p>
<p>• We are the first to explore the multi-sourced compositional generalization in V&amp;L, which is critical for crossmodal understanding.• We propose a retrieval-augmented training framework that improves the multi-sourced compositional generalization ability of VQA models by learning similar representations for primitives from different modalities.• We present a GQA-MSCG dataset to evaluate the multisourced compositional generalization ability of VQA models with different types of novel compositions.</p>
<p>Related Work</p>
<p>Compositional generalization has garnered significant attention in various research fields.In natural language processing (NLP), works [Li et al., 2021;Dankers et al., 2022] focus on improving the compositional generalization ability of models for tasks like machine translation.Additionally, Chai et al. [2024] used text generation models for data augmentation during training to enhance compositional generalization in multi-label text classification tasks.In computer vision (CV), works [Naeem et al., 2021;Jing et al., 2024;Li et al., 2024b]  3 Framework</p>
<p>Overview</p>
<p>The overall framework of the proposed framework in the context of VQA is shown in Figure 2.For the training set D t , the first step is to construct linguistic and visual primitive databases, D q and D v , respectively, for retrieval purposes.primitives with the same category (e.g., dog, cat) or attribute (e.g., blue, big) are treated as the same type of primitive.Both D q and D v contain multiple instances of each primitive in different contexts from D t .During training, for each training sample (Q, V ), where Q represents the question and V represents the image, the retrieval module is used to retrieve similar primitives from D q and D v for the primitives in Q and V at feature level, respectively.The retrieved features are then aggregated with the original primitive features and used to replace the original features for training.Through this training process, the VQA model continuously refines its feature extractor by leveraging the retrieved relevant features, thereby learning similar features for the same semantic primitives across different modalities and contexts, thus improving the generalization ability of the model for multi-sourced novel compositions.</p>
<p>Retrieval Database Construction</p>
<p>To ensure fairness in the comparison, the retrieval database is constructed based on the training set D t to avoid introducing external data during training.Linguistic Primitive Database.For the linguistic primitive database, we first extract all words from the questions in D t using the NLTK toolkit [Bird et al., 2009].All words are lemmatized, and words with part-of-speech tags as nouns, verbs, adjectives, and adverbs are considered as linguistic primitives.The set of all unique linguistic primitives is denoted as S q .For each linguistic primitive, T q questions containing the primitive are sampled from D t .Although the meaning of the primitive remains the same across these T q questions, the different questions provide different contexts.In different contexts, the features of the same linguistic primitive, extracted using a recurrent neural network, will differ.Thus, it is necessary to sample different questions for each linguistic primitive.To preserve the different contexts of the linguistic primitives, the linguistic primitive database stores the original questions, not just the linguistic primitive itself.</p>
<p>The resulting linguistic primitive database is represented as D q = p∈Sq f q (p), where f q (p) = {p i } Tq i=1 represents the T q sampled questions containing the linguistic primitive p. Visual Primitive Database.The process of constructing the visual primitive database relies on the scene graph annotations, which contains information about object categories and attributes in the images.Objects in the images are treated as visual primitives, with category and attribute information serving as the labels of these primitives.After removing duplicates from the images in D t , the resulting set of labels is denoted as S v .Similar to linguistic primitives, even though different visual primitives may share the same label, they represent different visual expressions of objects with the same type/attribute.Therefore, for each label, T v images containing the visual primitive with the label are sampled.Since different VQA models use different visual encoders, the input forms are not limited to object-level features.To address this, the visual primitive database stores the original images, not just the visual primitives.The resulting visual primitive database is represented as
D v = l∈Sv f v (l), where f v (l) = {p (l) i } Tv
i=1 represents the T v sampled images containing visual primitives with label l.</p>
<p>Feature Retrieval and Aggregation</p>
<p>To perform retrieval and aggregation at the feature level, both the training samples and all primitives in the two primitive databases (D q and D v ) need to pass through the feature extractors of the VQA model.For a question Q, we obtain its feature h q = g q (Q) = {h i q } N i=1 , where g q (•) denotes the linguistic feature extractor of the VQA model.Here, h i q rep-resents the feature of the i-th word, and N is the number of words.For an image V , the visual feature extractor g v (•) typically produces two types of features: "object-level" features or "patch-level" features, which can be represented as
h v = g v (V ) = {h i v } M i=1
, where h i v represents the feature of the i-th object/patch, and M is the number of objects/patches.</p>
<p>After obtaining the primitive-level features (word-level, object-level, patch-level), for each primitive feature in the training sample, retrieval is performed on the primitive feature sets of the questions and images in D q and D v (processed by g q (•) and g v (•)).Specifically, for a primitive feature p in the training sample, the top K q most similar primitive features {p (i) q } Kq i=1 are retrieved from the primitive feature set in D q , and the top K v most similar primitive features {p
(i) v } Kv i=1
are retrieved from the primitive feature set in D v .We use cosine similarity cos(•, •) to measure the similarity between two primitive features.The features are aggregated using a weighted average
p a = p+w q • Kq i=1 cos(p, p (i) q ) K q +w v • Kv i=1 cos(p, p (i) v ) K v ,(1)
where the hyperparameters w q and w v control the contributions of the different modality primitive databases.We use the aggregated feature p a to replace the original primitive feature p for training.</p>
<p>Optimization</p>
<p>The proposed framework is applicable to different VQA baseline models, using the same optimization approach as the baseline model without introducing additional training losses or constraints.Therefore, for a VQA baseline model using
C s = {[p 1 , p 2 ]|p 1 ∈ P q s ∪ P v s , p 2 ∈ P q s ∪P v s , p 1 ̸ = p 2 }.
Thus, the complete set of primitives in D t can be expressed as P Dt = s∈Dt P q s ∪P v s , and the complete set of compositions can be expressed as C Dt = s∈Dt C s .Similarly, the compositions in the val all split D v of the GQA dataset can be processed to obtain all compositions.We denote the set of all compositions as C Dv .Sample Filtering.LL + LV , V V + LV and LL + V V + LV are used to further evaluate the impact of the co-occurrence of different types of novel compositions on model performance.</p>
<p>For each category of test samples, we randomly sample 5,000 samples from D c , resulting in a total of 35,000 samples for the GQA-MSCG dataset.Samples containing x types of novel compositions are referred to as Level-x samples, and the difficulty of the samples increases as x increases.For example, samples in LL, V V and LV are Level-1 samples, LL + V V , LL + LV and V V + LV are Level-2 samples, LL + V V + LV are Level-3 samples.As the level increases, the co-occurrence count of novel composition categories in the test samples continues to rise, making the difficulty of the test samples increasingly higher.The test samples of GQA-MSCG are shown in Figure 3 and Figure 4, where the words or visual regions of the same color in a sample represent a pair of novel compositions in the sample.</p>
<p>Experiments</p>
<p>Experimental Setup</p>
<p>Dataset.We evaluate proposed frameworks on the VQA task, three datasets are selected to validate the effectiveness of the proposed frameworks: the GQA dataset [Hudson and Manning, 2019], the VQA v2 dataset [Goyal et al., 2017] and our GQA-MSCG dataset.The GQA dataset is a widely used large-scale dataset in VQA, containing a large number of template-based compositional questions.The VQA v2 dataset is an extended balanced version of the VQA v1 dataset [Antol et al., 2015], where questions are human-made.These two datasets are often used to test the generalization ability of VQA models to IID data as well as their ability for compositional reasoning.Our GQA-MSCG dataset is used to evaluate the VQA model's ability to generalize consistently to multi-sourced novel compositions, which refers to the VQA model's ability to generalize to novel compositions of primitives from different modalities.Baseline Models.We use CFR [Nguyen et al., 2022] and Qwen-VL [Bai et al., 2023] as baseline models.The baseline models combined with the proposed framework are referred to as CFR+RAG and Qwen-VL+RAG, respectively.CFR is a representative small model in the VQA task with a parameter size less than 0.2B.Qwen-VL is a popular open-sourced multimodal large model (with more than 7B parameters), which can be applied to various vision-and-language tasks such as VQA, image captioning, visual dialogue, and others.For both CFR and Qwen-VL, we reimplemented them based on their officially released code.Implementation Details.For experiments on all three datasets including GQA, GQA-MSCG and VQA v2, we finetune Qwen-VL and Qwen-VL+RAG with LoRA [Hu et al., 2022] with a maximum of 2 epochs.For CFR+RAG and Qwen-VL+RAG, we set w q = 0.6 and w v = 0.4.</p>
<p>For experiments on the GQA dataset and the GQA-MSCG dataset, we fine-tune CFR, Qwen-VL, CFR+RAG, and Qwen-VL+RAG using the train balanced split of the GQA dataset and selected the best-performing model weights on the val balanced split of GQA.Using these model weights, we present the experimental results on the test-dev split of the GQA dataset and all seven test splits of our GQA-MSCG dataset.The maximum number of epochs for fine-tuning CFR and CFR+RAG was set to 12.The sampled number T q and T v for constructing D q and D q are set to 8 and 32, respectively.Moreover, we use the scene graph provided by GQA to construct D v .The number of aggregated primitive K q and K v are set to 4 and 16, respectively.Distinctively, for experiments on the VQA v2 dataset, we set T q = 1, T v = 32, K q = 4 and K v = 4.For constructing D v , we use the object categories detected by Faster R-CNN [Ren et al., 2016], ignoring the object attributes.</p>
<p>Multi-Sourced Compositional Generalization Performance</p>
<p>We evaluate the MSCG ability of VQA models on the proposed GQA-MSCG dataset.We compare with multimodal large models, including LLaVA-1.5 [Liu et al., 2023] and LLaVA-1.6 [Liu et al., 2024a], with the experimental results shown in Table 1.We can observe that: (1) As the cooccurrence count of novel composition categories in the test samples increases (Level-1 → Level-3), the model's performance gradually decreases.For example, LLaVA-1.5 has an accuracy of 70.22% in the LL split, 67.04% in the LL + V V split, and 64.14% in the LL + V V + LV split.(2) Different VQA models exhibit significant differences in their ability to generalize to novel compositions of different modality primitives.For example, LLaVA-1.5 performs better on the V V split than on the LL split, while Qwen-VL shows the opposite.</p>
<p>(3) Both small VQA models (e.g., CFR) and large VQA models (e.g., Qwen-VL) benefit significantly from the proposed framework, which enhances their generalization ability to multi-sourced novel compositions.As a result, based on the experimental results, the following conclusions can be drawn: (1) Existing VQA models still have shortcomings in handling complex compositional questions.(2) It is necessary to specifically consider the multisourced compositional generalization ability for VQA models.(3) The proposed framework is highly versatile and can be applied to different baseline models, improving their generalization ability to novel compositions of primitives from</p>
<p>Independent and Identically Distributed Generalization Performance</p>
<p>We verify the improvement of the proposed framework in IID generalization performance on the test-dev split of the GQA dataset [Hudson and Manning, 2019]   based frameworks) in terms of IID generalization performance.(2) The proposed framework further enhances the IID generalization performance of baseline models (e.g., 1.43% and 1.13% absolute performance gains in accuracy for CFR and Qwen-VL, respectively).These experimental results demonstrate that the proposed framework is applicable to different task scenarios, including multi-sourced compositional generalization scenario and independent and identically distributed generalization scenarios.</p>
<p>Experimental results on the val split of the VQA v2 dataset are shown in the Table 3.The results demonstrate that our framework is inoffensive for IID generalization.The reason why the performance gains of the proposed framework on VQA v2 are less than on GQA is that the questions in GQA are more compositional and thus are more suitable to be improved by our framework.Such experimental results further prove the effectiveness of the proposed framework for improving the IID generalization ability of VQA models.</p>
<p>Ablation Studies</p>
<p>The results of ablation studies on the GQA-MSCG dataset using CFR as the baseline model are shown in Table 4. From the experimental results, the following conclusions can be drawn: (1) Conducting retrieval only on the question retrieval database D q or the image retrieval database D v during training can also improve the baseline model's performance, especially for novel compositions of specific modal primitives.For example, when retrieval is performed only on D v , improvements are more evident for compositions of visual modal primitives (the VV split).(2) Overall, using both the question and image retrieval databases simultaneously results in the best performance (highest average accuracy across all splits).These ablation study results confirm the necessity of multi-sourced retrieval during training and demonstrate that the linguistic primitive database D q or visual primitive database D v are complementary, helping to enhance the baseline model's generalization ability across different compositions.</p>
<p>Parameter Analysis</p>
<p>We analyze the influences of w q and w v on the MSCG ability of our framework, which denote the explicit contributions of the linguistic primitive database and the visual primitive database, respectively.Figure 5 shows the performance variations of the proposed framework with changing values of w q and w v .First, we conduct an initial analysis of w q and w v by setting them to the same value.The experimental results are shown in Figure 5 (a).It can be seen that as the values    increase, the performance of the proposed framework peaks when w q = w v = 0.4, and further increasing w q and w v does not improve the performance.Furthermore, we analyze the effectiveness of the proposed framework with w q ̸ = w v by fixing w q or w v at 0.4 and adjusting the other parameter (i.e., w v or w q ), as shown in Figures 5 (b) and (c).We can observe that: (1) The performance of CFG+RAG fluctuates obviously when adjusting either w q or w v .(2) CFR+RAG performs best with setting w q = 0.6 and w v = 0.4 simultaneously.Based on the above experimental results, we set w q to 0.6 and w v to 0.4 for all experiments.</p>
<p>Qualitative Analysis</p>
<p>For each split of Level-1, we provide two qualitative examples of CFR+RAG (Ours) and CFR (the baseline model) on the GQA-MSCG dataset in Figure 6.In the figure, novel compositions of linguistic primitives are highlighted in red text, and visual primitives are enclosed in red boxes.It can be observed that for test samples with novel compositions composed of primitives from different modalities, the proposed framework helps the baseline model make more accurate predictions.For example, in the first sample from the GQA-MSCG dataset's LL split, the question is "Are the jeans different in color than the flowers?", which includes the novel composition of linguistic primitives "jeans + flowers".The baseline model CFR gives an incorrect answer, "no".In contrast, after incorporating CFR into the proposed framework, it correctly predicts "yes".Moreover, for test samples in the V V and LV splits, the proposed framework still provides accurate answers, thanks to the alignment of primitives with the same semantics across different contexts and modalities.These qualitative examples demonstrate that the proposed framework enhances the baseline model's generalization ability to multi-sourced novel compositions, proving the effectiveness of the framework.</p>
<p>Conclusion</p>
<p>In this paper, we explored the multi-sourced compositional generalization ability of models in the context of VQA.We have presented a retrieval-augmented training framework to encourage VQA models to learn unified representations for the same semantic compositions by aligning semantically equivalent primitives across different modalities at the feature level.The proposed framework can be seamlessly incorporated into existing VQA models to improve their multisourced compositional generalization ability.We extend the GQA dataset to construct a GQA-MSCG dataset, which enables the quantitative evaluation of the multi-sourced compositional generalization ability for VQA models.Experimental results demonstrate that our framework can improve not only the multi-sourced compositional generalization ability, but also the IID generalization ability.</p>
<p>Figure 1 :
1
Figure 1: Multi-sourced novel compositions in the context of VQA.</p>
<p>Figure 2 :
2
Figure 2: The overall framework of the proposed framework.</p>
<p>the training loss L, for a training sample (Q, V ) with groundtruth A, the training loss for both the baseline model and the model with our framework is the same:L = loss(P (Q, V ), A),(2)where P (Q, V ) represents the output of the VQA model (e.g., a distribution vector over the number of categories), and loss(•, •) represents the training loss function, such as the cross-entropy loss used in the UpDn model [Anderson et al., 2018].4 GQA-MSCG Dataset In order to quantitatively evaluate the generalization ability of VQA models to multi-sourced novel compositions, we construct the GQA-MSCG dataset based on the GQA dataset [Hudson and Manning, 2019].The process of constructing the GQA-MSCG dataset consists of three main steps: composition extraction, sample filtering, and sample classification.Composition Extraction.We use the same steps as in Section 3.2 to extract the linguistic and visual primitives for all samples in the train balanced split D t of the GQA dataset.The linguistic and visual primitives for a sample s ∈ D t are represented by P q s = {p qi s } N i=1 and P v s = {p vi s } M i=1 , respectively, where N and M denote the number of linguistic and visual primitives in the sample.The compositions in sample s are represented as</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Level-1 samples in the GQA-MSCG dataset.</p>
<p>Experimental results with wq = 0.4 and varying wv.</p>
<p>Experimental results with wv = 0.4 and varying wq.</p>
<p>Figure 5 :
5
Figure 5: Parameter analysis using CFR as the baseline model on the GQA-MSCG dataset.</p>
<p>Figure 6 :
6
Figure 6: The qualitative comparison between CFR+RAG (Ours) and CFR on the GQA-MSCG dataset.</p>
<p>Table 1 :
1
Accuracy (%) on the GQA-MSCG dataset.
TypeModelLLLevel-1 VVLVLevel-2 LL+VV LL+LV VV+LV LL+VV+LV Level-3OverallSmall Model CFR [Nguyen et al., 2022]74.78 72.18 73.3672.2673.5670.9469.7872.41(≤ 0.2B)+ RAG (Ours)76.28 73.88 75.6473.5875.6673.1871.2274.21LLaVA-1.5 [Liu et al., 2023]70.22 70.34 69.6267.0468.8468.4064.1468.37Large ModelLLaVA-1.6 [Liu et al., 2024a] 72.36 71.52 72.6069.7069.5069.9867.6070.46(≥ 7B)Qwen-VL [Bai et al., 2023]72.24 65.54 69.0668.1872.0866.0466.3268.49+ RAG (Ours)74.68 68.90 71.9871.5074.8268.9269.8871.53</p>
<p>Table 2 :
2
Accuracy (%) on the test-dev split of the GQA dataset.
TypeModelAccuracyAttention-based MAC [Hudson and Manning, 2018]52.43Graph-based LCGN [Hu et al., 2019]55.63NMN-based MMN [Chen et al., 2021]59.14BLIP-2 (FlanT5XXL) [Li et al., 2023c] 44.70Pretrain-basedMiniGPT-4 [Zhu et al., 2023]43.50(zero-shot)LLaVA-1.5 [Liu et al., 2023]61.93LLaVA-1.6 [Liu et al., 2024a]64.26CFR [Nguyen et al., 2022]70.27Pretrain-based+ RAG (Ours)71.70(fine-tuned)Qwen-VL [Bai et al., 2023]54.98+ RAG (Ours)56.11</p>
<p>Table 3 :
3
Accuracy (%) on the val split of the VQA v2 dataset.
TypeModelAccuracyDLR [Jing et al., 2020]57.96Small Model (≤ 0.2B)CF-VQA [Niu et al., 2021] CLS [Mao et al., 2024] ASS [Li et al., 2024a]63.73 63.94 64.00KDAR [Peng and Wei, 2024]65.54PNP-VQA [Tiong et al., 2022]63.30Large ModelBLIP-2 (FlanT5XXL) [Li et al., 2023c] 65.20(≥ 7B)Qwen-VL [Bai et al., 2023]69.04+ RAG (Ours)69.82different modalities.</p>
<p>Experimental results on the test-dev split of the GQA dataset are shown in Table 2. From the table, we can observe that: (1) Pretrain-based models outperform other types of models (such as attention-based, graph-based, and NMN-
and the val splitof the VQA v2 dataset [Goyal et al., 2017]. For GQA, wecompare to five different types of VQA models, includingattention-based MAC [Hudson and Manning, 2018], graph-based LCGN [Hu et al., 2019], neural modular network based(NMN-based) MMN [Chen et al., 2021], zero-shot pretrain-based BLIP-2 [Li et al., 2023c], MiniGPT-4 [Zhu et al.,2023], LLaVA-1.5 [Liu et al., 2024b], LLaVA-1.6 [Liu etal., 2023], and fine-tuned pretrain-based CFR [Nguyen et al.,2022], Qwen-VL [Bai et al., 2023]. For VQA v2, we compareto VQA models with less than 0.3B parameters (DLR [Jinget al., 2020], CF-VQA [Niu et al., 2021], CLS [Mao et al.,2024], ASS [Li et al., 2024a], KDAR [Peng and Wei, 2024])and VQA models with more than 7B parameters (PNP-VQA[Tiong et al., 2022], BLIP-2 [Li et al., 2023c], Qwen-VL [Baiet al., 2023]).</p>
<p>Table 4 :
4
Ablation studies on the GQA-MSCG dataset.
ModelDq DvLLLevel-1 VVLVCFR [Nguyen et al., 2022] --74.78 72.18 73.36+ RAG (Ours)✓ -✓ ✓ 76.28 73.88 75.64 -76.18 73.46 74.24 ✓ 75.66 73.66 74.80
AcknowledgementsThis work was supported by the Shenzhen Science and Technology Program under Grant No. JCYJ20241202130548062, the Natural Science Foundation of Shenzhen under Grant No. JCYJ20230807142703006, and the Natural Science Foundation of China (NSFC) under Grants No. 62176021 and No. 6217204.
Bottom-up and top-down attention for image captioning and visual question answering. Akula, arXiv:1811.12889arXiv:2308.12966Shikhar Murty, Michael Noukhovitch, Thien Huu Nguyen, Harm de Vries, and Aaron Courville. Systematic generalization: what is required and can it be learned?. Bai, 2021. 2021. 2018. 2018. 2015. 2015. 2018. 2023arXiv preprintProceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). and Jingren Zhou. Qwen-vl: A frontier large visionlanguage model with versatile abilities</p>
<p>Natural language processing with Python: analyzing text with the natural language toolkit. Bird, 2009. 2009O'Reilly Media, Inc</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Chai, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)Tejas Khot, Douglas Summers-StayYash Goyal2024. 2024. 2021. 2021. 2022. 2022. 2017. 201738Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</p>
<p>Language-conditioned graph networks for relational reasoning. Hu, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)2019. 2019</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. Hu, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Hudson and Manning2022. 2022. 2018. 2018. 2019. 2019. 2020. 202034Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</p>
<p>Retrieval-augmented primitive representations for compositional zero-shot learning. Jing , Proceedings of the AAAI Conference on Artificial Intelligence (AAAI). the AAAI Conference on Artificial Intelligence (AAAI)2024. 202438</p>
<p>On compositional generalization of neural machine translation. Li, Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL). the Annual Meeting of the Association for Computational Linguistics (ACL)2021. 2021</p>
<p>Compositional temporal grounding with structured variational cross-graph correspondence learning. Li, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2022. 2022</p>
<p>Exploring the effect of primitives for compositional generalization in vision-andlanguage. Li, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2023a. 2023</p>
<p>Variational cross-graph reasoning and adaptive structured semantics learning for compositional temporal grounding. Li, arXiv:2301.12597Blip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. 2023b. 2023. 2023c. 2023arXiv preprintIEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)</p>
<p>Adversarial sample synthesis for visual question answering. Li, ACM Transactions on Multimedia Computing, Communications, and Applications. 20122024a. 2024TOMM)</p>
<p>Context-based and diversity-driven specificity in compositional zero-shot learning. Li, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)2024b. 2024</p>
<p>Improved baselines with visual instruction tuning. Liu, 2023. 2023</p>
<p>Overcoming language priors in visual question answering with cumulative learning strategy. Liu, Proceedings of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Muhammad Ferjad Naeem2024a. January 2024. 2024b. 2024. 2024. 2024. 2021. 2022. 202236Coarseto-fine reasoning for visual question answering</p>
<p>Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Niu, arXiv:2304.10592Proceedings of the Association for Computational Linguistics: Student Research Workshop (ACLSRW). the Association for Computational Linguistics: Student Research Workshop (ACLSRW)IEEE2021. 2021. 2022. 2024. 2016. 2016. 2022. 2024. 2024. Jun. 202339arXiv preprintTransformer module networks for systematic generalization in visual question answering. Zhu et al., 2023] Deyao Zhu. Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-548 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-548</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-548</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-15.html">extraction-schema-15</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <p><strong>Paper ID:</strong> paper-aa5a4d1e014f1900a907ecd464218a272f883ecb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/aa5a4d1e014f1900a907ecd464218a272f883ecb" target="_blank">LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement</a></p>
                <p><strong>Paper Venue:</strong> IEEE/RJS International Conference on Intelligent RObots and Systems</p>
                <p><strong>Paper TL;DR:</strong> LGMCTS is presented, a framework that uniquely combines language guidance with geometrically informed sampling distributions to effectively rearrange objects according to geometric patterns dictated by natural language descriptions.</p>
                <p><strong>Paper Abstract:</strong> We present LGMCTS, a framework that uniquely combines language guidance with geometrically informed sampling distributions to effectively rearrange objects according to geometric patterns dictated by natural language descriptions. LGMCTS uses Monte Carlo Tree Search (MCTS) to create feasible action plans that ensure executable semantic object rearrangement. We present a comprehensive comparison with leading approaches that use language to generate goal rearrangements independently of actionable planning, including Structformer, StructDiffusion, and Code as policies. We also present a new benchmark, the Executable Language Guided Rearrangement (ELGR) Bench, containing tasks involving intricate geometry. With the ELGR bench, we show limitations of task and motion planning (TAMP) solutions that are purely based on Large Language Models (LLM) such as Code as Policies and Progprompt on such tasks. Our findings advocate for using LLMs to generate intermediary representations rather than direct action planning in geometrically complex rearrangement scenarios, aligning with perspectives from recent literature. Our code and supplementary materials are accessible at https://lgmcts.github.io/.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e548.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e548.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LGMCTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language-Guided Monte-Carlo Tree Search</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A framework introduced in this paper that uses an LLM to parse natural language into spatial pose distributions and then uses MCTS with parametric geometric priors to produce physically executable object-rearrangement plans.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 (used as language parser) + MCTS planner</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pipeline: GPT-4 (prompted via engineered/system prompts and scene summaries) translates free-form language L into a structured list of desired pose distributions F; these distributions are matched to parametric geometric prior functions (from a database keyed by Sentence-BERT embeddings) and fed into an MCTS-based task-and-motion planner which samples poses, simulates collisions, and relocates obstacles to produce executable action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Executable Semantic Object Rearrangement (ELGR / tabletop rearrangement)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a tabletop scene with N objects and a natural language command L describing desired spatial relations/patterns, produce an action sequence A of pick-and-place moves (including relocating obstructing objects) that results in a final arrangement satisfying the language-implied pose distributions; operates in cluttered tabletop workspaces and on a UR5e robot in real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation + multi-step planning (instruction following)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+procedural+object-relational</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained LLM (GPT-4) prompted with engineered system & examples; Sentence-BERT embeddings for mapping language to prior shapes; symbolic scene description produced by vision modules (RAM + color detector)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompt engineering / in-context examples to GPT-4 to produce structured spatial distributions; embedding lookup (Sentence-BERT) to select geometric priors</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit structured list of pose-distribution functions (F) derived from language; distributions are implemented as parametric geometric prior functions (parametric curves γ(t, κ)) in a database; MCTS node states contain current object poses and remaining spatial requirements</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>planning success (SRp) and executable plan success (SRep); pattern-specific success rates on Structformer dataset</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Structformer: Line 95.99%, Circle 95.25%, Tower 100%, Dinner 100%; ELGR-Bench: SRp 90.9%, SRep 79.2% (planning capped at 10,000 steps for PMCTS/LGMCTS comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Generates high-fidelity geometric arrangements for parametric patterns (line, circle, rectangle, tower, spatial left/right) and composes multiple sub-instructions; robustly handles obstacle relocation and infeasible starting configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Longer execution time and decreased planning success in extremely dense clutter due to MCTS search cost; not a failure of encoding but of search efficiency; LLM-generated distributions can be ambiguous if language is underspecified (addressed by priors but still limited by prompt/output ambiguity).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms Structformer and StructDiffusion on geometric goal generation; outperforms LLM-as-planner (LFSP) and PMCTS in executable-plan rate on ELGR. Example baselines: StructDiffusion (61.49% line → lower than LGMCTS), LFSP had 100% planning but 45.2% executability on ELGR.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No formal ablation tables reported; paper emphasizes comparative evaluations (LGMCTS vs. LLM-as-planner and vs. Pose+MCTS) showing that using LLMs for intermediate representations + parametric priors + MCTS yields much higher executability than LLM-only direct planning.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs (GPT-4) are effective at translating free-form language into structured, interpretable spatial distributions but are unreliable when used directly as end-to-end planners; combining LLM-produced intermediate representations with explicit parametric geometric priors and an MCTS-based planner yields high executability and better geometric fidelity in rearrangement tasks without direct sensory input to the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e548.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e548.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (parser)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (Generative Pre-trained Transformer 4) used as language parser</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Used in this work to parse natural language instructions into structured spatial distribution specifications (a list F of pose distributions per object) via prompt engineering and example-based prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained language model used zero-/few-shot via carefully engineered system prompts and examples to output structured goal configurations (which objects, which patterns, and which constraints) rather than raw action sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language-to-spatial-distribution parsing for tabletop rearrangement</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate free-form natural language instructions describing spatial patterns/relations into a structured set of spatial distribution functions f_i for objects (e.g., 'put mug to right of bowl' → a distribution region to the right of bowl).</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>instruction parsing / symbolic grounding (pre-planning stage)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial+object-relational (mainly declarative spatial relations and object-selection), procedural (to a limited extent via implied ordering)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-training on text corpora; in-context examples provided in prompts (few-shot); engineered system prompt guidelines</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>engineered prompting with scene descriptions (object labels, colors, IDs) and examples (few-shot), returning structured JSON-like outputs</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>outputs symbolic structured distributions (list of f_i), natural-language-derived constraints mapped to symbolic forms; not an internal geometric map but a textual/structured spec that downstream modules convert to priors</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>indirect: quality of resulting F used by planner; downstream metrics reported (planning/execution success when used in LGMCTS pipeline)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>When used to generate intermediates for LGMCTS, leads to high downstream success (see LGMCTS: SRep 79.2% on ELGR). When LLMs are used directly as planners (LFSP baseline), planning success can be high but executability drops (LFSP SRep 45.2%).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Accurately identifies pattern types, selects object subsets, and produces interpretable constraints that can be matched to parametric priors; handles composite multi-pattern instructions.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>When asked to produce direct action sequences/policy code (LLM-as-planner), outputs can be incomplete or non-executable in geometrically complex scenarios; ambiguity in natural language can lead to underspecified distributions without additional grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared qualitatively against using LLMs directly for planning (LFSP / Code as Policies / Progprompt): LLM-as-parser (used by LGMCTS) + classical planner outperforms LLM-as-end-to-end-planner on executability.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>No explicit ablation, but paper contrasts 'LLM as intermediate representation generator' vs 'LLM as direct planner' showing benefits of the former.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLMs like GPT-4 encode enough commonsense spatial and relational knowledge to produce structured, actionable intermediate representations, but they should be used to generate interpretable constraints/distributions (not raw action sequences) for geometric tasks without endowing them with direct sensory input.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e548.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e548.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sentence-BERT prior lookup</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sentence-BERT embedding keyed parametric prior selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sentence-BERT embeddings of the language instruction are used to select the best-matching parametric geometric prior function from a database keyed by precomputed sentence embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sentence-BERT</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A sentence embedding model that maps input language L to a vector Θ(L); the system computes cosine (dot) similarity between Θ(L) and keys Θ_k for priors in a database to select a parametric prior f_prior^K.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language-to-prior mapping for geometric pose sampling</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Given a parsed instruction L and its embedding Θ(L), pick the parametric geometric prior function (e.g., line, circle, rectangle, tower, spatial:left/right) whose embedding key best matches the instruction so the planner can sample object poses consistent with the intended pattern.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>knowledge retrieval / mapping (spatial representation selection)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained Sentence-BERT model applied at inference (no fine-tuning reported)</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>embedding lookup (nearest-key selection in embedding space)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>discrete database of parametric geometric prior functions keyed by Sentence-BERT sentence embeddings; selected prior returns parametric curve γ(t, κ(p0,p1)) used as f_prior</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>not directly quantified in isolation; evaluated via downstream planning/execution success (LGMCTS results)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Not separately reported; integral to LGMCTS achieving high geometric fidelity and executability reported in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Enables consistent mapping of natural language pattern descriptions to geometric prior families, supporting compositional and multi-object patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>No explicit failures reported, but selection depends on how well the database covers language variability and on embedding matching fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Not compared against alternative mapping methods in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>None reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using sentence embeddings to select explicit parametric geometric priors provides a practical way to convert underspecified language into concrete spatial distributions usable by a planner, enabling LLMs to remain language-only while the geometry is handled via explicit priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e548.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e548.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Parametric Geometric Priors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parametric geometric prior functions (pattern priors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A database of parametric curve-based prior functions (γ(t, κ)) representing geometric patterns (line, circle, rectangle, tower, spatial relations) used to convert language-described patterns into concrete pose distributions for sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>parametric prior database (not a neural LM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Each pattern is represented by a parametric curve γ(t, κ) with κ parameterized by two anchor positions p0,p1; sampling rules depend on how many objects have already been placed (ordered/unordered patterns), and add Gaussian jitter and collision-free masking.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Spatial prior specification for sampling object poses</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provides iterative, history-sensitive pose distributions f_prior(p_i | P_R, L) used inside the sequential sampling and MCTS simulation to generate collision-aware target poses consistent with language constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>spatial representation / sampling mechanism</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>spatial (explicit geometric)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>hand-designed parametric functions stored in a database, selected via Sentence-BERT matching to language</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>database lookup (selected by embedding similarity) and sequential sampling rules conditioned on sampled subset O_R^sampled</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit parametric curves and sampling rules; combined multiplicatively with a boolean collision-free mask f_free to produce f_i</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>downstream planning/execution success and pattern fidelity (Structformer and ELGR benchmarks)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Central to LGMCTS high performance: success rates e.g., Structformer line 95.99%, ELGR SRep 79.2%; no standalone numeric ablation reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Accurately enforces multi-object geometric relations (ordered/unordered) and supports compositional/multipattern instructions; yields collision-aware candidate poses.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Limited to shapes representable as parametric curves included in the database; unseen or highly free-form spatial descriptions require either database expansion or ambiguous mapping.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Outperforms learned generative pose predictors (Structformer, StructDiffusion) in producing collision-free and executable goals.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not reported; empirical comparisons imply priors + LLM + MCTS outperform purely learned goal generation followed by MCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Encoding spatial knowledge as explicit parametric priors selectable by language embeddings yields robust, history-aware pose distributions that, when combined with MCTS, improve executability versus purely learned or LLM-generated goal poses.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e548.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e548.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-Few-Shot-Planner (LFSP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs used as Few-Shot Planners (Code as Policies / Progprompt baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines where LLMs (Code as Policies, Progprompt) are prompted (few-shot) to output action sequences or policy code for rearrangement; outputs are converted to sequences of object IDs and goal poses and given to a TAMP planner or executed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (Code as Policies, Progprompt - varying underlying LLMs such as GPT-family or LLaMA variants)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs are provided scene descriptions (object names, IDs, textures, initial poses, region boundaries) and few-shot examples; they generate ordered action lists or programmatic policies intended to realize the requested rearrangement without explicit geometric priors or classical sampling-based planners.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Direct LLM-based planning for object rearrangement (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate end-to-end plans (action sequences or code/policies) that move objects to satisfy language-described relations; used as a baseline to compare executability and completeness against LGMCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation + sequential planning (end-to-end LLM planning)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + object-relational + spatial (intended to be encoded implicitly in LM weights)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained LLM weights; in-context few-shot examples provided at inference</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>few-shot prompting / code-generation style prompts (Code as Policies, Progprompt)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>implicit procedural knowledge encoded in generated action sequences or program code; outputs are symbolic sequences of object moves and target poses</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>planning success rate and executable success rate (SRep) on ELGR; pattern fidelity on Structformer (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>ELGR: planning success (SRp) 100% but executable success (SRep) 45.2%; Structformer (on 10% sampled scenes): Line 41.16%, Circle 51.75%, Tower 88.80%, Dinner 27.05% (lower than LGMCTS).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good at composing high-level procedural sequences in simple or well-specified scenarios; can output sequences matching obvious constraints when scene is simple or examples are close.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Produces plans that are frequently infeasible or incomplete in geometrically complex or cluttered scenes (high planning success but low executability), poor geometric fidelity, struggles with long-horizon or intricate geometric constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared directly in experiments: although LFSP can often produce plausible action sequences, LGMCTS yields substantially higher executability and geometric fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>Not applicable; baseline behavior contrasted with LGMCTS and PMCTS.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using LLMs to directly generate action sequences or policies without explicit geometric priors or a robust motion planner leads to many plans that are semantically plausible but not physically executable; this motivates using LLMs for intermediate symbolic representations rather than final plans.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e548.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e548.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-GROP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-GROP (Task & Motion Planning with LLMs for object rearrangement)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work that uses LLMs to translate language into pairwise spatial relationship specifications and then invokes a sampling-based TAMP planner (GROP); limited to pairwise relations and cannot handle complex multi-object geometric patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Task and motion planning with large language models for object rearrangement</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (used to infer pairwise relations)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLM translates natural language commands into symbolic pairwise spatial relations (e.g., A left of B) which are then fed to a sampling-based task & motion planner (GROP).</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Language-to-pairwise spatial relations for TAMP (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Translate language to pairwise spatial constraints then use a TAMP solver to compute motions; targeted at object rearrangement but limited to pairwise relationships.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>object manipulation + planning (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>object-relational (pairwise) + spatial</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained LLMs used as translators</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>prompting to generate symbolic relations</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>pairwise symbolic spatial relations (discrete constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>reported as limited in handling multi-object complex patterns (qualitative statement in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Mentioned as limited to pairwise relations; no numeric result reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Works for simple pairwise spatial constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Cannot handle complex multi-object geometric patterns or compositions beyond pairwise relations.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Positioned as motivation why LGMCTS uses parametric geometric priors to handle multi-object patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>N/A (related work mention).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-based translation to pairwise symbolic relations is useful but insufficient for complex multi-object geometric arrangements; motivates richer prior representations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e548.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e548.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models encode, represent, or utilize spatial knowledge, procedural knowledge, or object-relational knowledge for embodied planning, navigation, or manipulation tasks, particularly when the model operates without direct sensory input.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoTAMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoTAMP (Autoregressive Task and Motion Planning with LLMs as translators and checkers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work that uses LLMs to translate natural language into formal STL representations and invokes formal planners; effective for a wide range of TAMP tasks but limited in non-discrete action spaces as in semantic rearrangement.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Autotamp: Autoregressive task and motion planning with llms as translators and checkers</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLMs (used as translators and validators)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs are used to translate language into formal spatio-temporal logic (STL) representations that a planner can use; LLMs act as translators/checkers rather than direct planners.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Translation of natural language to formal representations for TAMP (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Convert language into STL spec and use an STL planner to generate optimal trajectories; suited for discrete/structured tasks but less applicable to continuous semantic rearrangement.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>task & motion planning (related work)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_type</strong></td>
                            <td>procedural + spatial (encoded into formal STL)</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_source</strong></td>
                            <td>pre-trained LLMs used for translation</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_sensory_input</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>elicitation_method</strong></td>
                            <td>LLM translation/few-shot prompting to produce STL</td>
                        </tr>
                        <tr>
                            <td><strong>knowledge_representation</strong></td>
                            <td>explicit formal representations (STL) produced by LLM for downstream planner</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Reported strengths across many TAMP tasks in original work; here noted limitation for continuous/dense rearrangement tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Mentioned as not directly applicable to semantic rearrangement with large continuous action spaces (no numeric result given here).</td>
                        </tr>
                        <tr>
                            <td><strong>success_patterns</strong></td>
                            <td>Good for tasks that can be expressed in formal logic and solved by formal planners.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_patterns</strong></td>
                            <td>Limited applicability when action space is continuous and high-dimensional as in semantic object rearrangement.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Contrasts with LGMCTS which targets continuous/rearrangement tasks via priors + sampling-based planner.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_results</strong></td>
                            <td>N/A (related work mention).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using LLMs to produce formal symbolic representations for classical planners is a promising approach, but it faces limitations in problems with continuous, high-dimensional action spaces unless complemented with suitable geometric representations or sampling strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Code as policies: Language model programs for embodied control <em>(Rating: 2)</em></li>
                <li>Task and motion planning with large language models for object rearrangement <em>(Rating: 2)</em></li>
                <li>Autotamp: Autoregressive task and motion planning with llms as translators and checkers <em>(Rating: 2)</em></li>
                <li>StructDiffusion: Object-centric diffusion for semantic rearrangement of novel objects <em>(Rating: 2)</em></li>
                <li>Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects <em>(Rating: 2)</em></li>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>SayCan: Grounding language in robotic affordances <em>(Rating: 1)</em></li>
                <li>Inner monologue: Embodied reasoning through planning with language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-548",
    "paper_id": "paper-aa5a4d1e014f1900a907ecd464218a272f883ecb",
    "extraction_schema_id": "extraction-schema-15",
    "extracted_data": [
        {
            "name_short": "LGMCTS",
            "name_full": "Language-Guided Monte-Carlo Tree Search",
            "brief_description": "A framework introduced in this paper that uses an LLM to parse natural language into spatial pose distributions and then uses MCTS with parametric geometric priors to produce physically executable object-rearrangement plans.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4 (used as language parser) + MCTS planner",
            "model_size": null,
            "model_description": "Pipeline: GPT-4 (prompted via engineered/system prompts and scene summaries) translates free-form language L into a structured list of desired pose distributions F; these distributions are matched to parametric geometric prior functions (from a database keyed by Sentence-BERT embeddings) and fed into an MCTS-based task-and-motion planner which samples poses, simulates collisions, and relocates obstacles to produce executable action sequences.",
            "task_name": "Executable Semantic Object Rearrangement (ELGR / tabletop rearrangement)",
            "task_description": "Given a tabletop scene with N objects and a natural language command L describing desired spatial relations/patterns, produce an action sequence A of pick-and-place moves (including relocating obstructing objects) that results in a final arrangement satisfying the language-implied pose distributions; operates in cluttered tabletop workspaces and on a UR5e robot in real experiments.",
            "task_type": "object manipulation + multi-step planning (instruction following)",
            "knowledge_type": "spatial+procedural+object-relational",
            "knowledge_source": "pre-trained LLM (GPT-4) prompted with engineered system & examples; Sentence-BERT embeddings for mapping language to prior shapes; symbolic scene description produced by vision modules (RAM + color detector)",
            "has_direct_sensory_input": false,
            "elicitation_method": "prompt engineering / in-context examples to GPT-4 to produce structured spatial distributions; embedding lookup (Sentence-BERT) to select geometric priors",
            "knowledge_representation": "explicit structured list of pose-distribution functions (F) derived from language; distributions are implemented as parametric geometric prior functions (parametric curves γ(t, κ)) in a database; MCTS node states contain current object poses and remaining spatial requirements",
            "performance_metric": "planning success (SRp) and executable plan success (SRep); pattern-specific success rates on Structformer dataset",
            "performance_result": "Structformer: Line 95.99%, Circle 95.25%, Tower 100%, Dinner 100%; ELGR-Bench: SRp 90.9%, SRep 79.2% (planning capped at 10,000 steps for PMCTS/LGMCTS comparisons).",
            "success_patterns": "Generates high-fidelity geometric arrangements for parametric patterns (line, circle, rectangle, tower, spatial left/right) and composes multiple sub-instructions; robustly handles obstacle relocation and infeasible starting configurations.",
            "failure_patterns": "Longer execution time and decreased planning success in extremely dense clutter due to MCTS search cost; not a failure of encoding but of search efficiency; LLM-generated distributions can be ambiguous if language is underspecified (addressed by priors but still limited by prompt/output ambiguity).",
            "baseline_comparison": "Outperforms Structformer and StructDiffusion on geometric goal generation; outperforms LLM-as-planner (LFSP) and PMCTS in executable-plan rate on ELGR. Example baselines: StructDiffusion (61.49% line → lower than LGMCTS), LFSP had 100% planning but 45.2% executability on ELGR.",
            "ablation_results": "No formal ablation tables reported; paper emphasizes comparative evaluations (LGMCTS vs. LLM-as-planner and vs. Pose+MCTS) showing that using LLMs for intermediate representations + parametric priors + MCTS yields much higher executability than LLM-only direct planning.",
            "key_findings": "LLMs (GPT-4) are effective at translating free-form language into structured, interpretable spatial distributions but are unreliable when used directly as end-to-end planners; combining LLM-produced intermediate representations with explicit parametric geometric priors and an MCTS-based planner yields high executability and better geometric fidelity in rearrangement tasks without direct sensory input to the LLM.",
            "uuid": "e548.0",
            "source_info": {
                "paper_title": "LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "GPT-4 (parser)",
            "name_full": "GPT-4 (Generative Pre-trained Transformer 4) used as language parser",
            "brief_description": "Used in this work to parse natural language instructions into structured spatial distribution specifications (a list F of pose distributions per object) via prompt engineering and example-based prompts.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_size": null,
            "model_description": "Large pre-trained language model used zero-/few-shot via carefully engineered system prompts and examples to output structured goal configurations (which objects, which patterns, and which constraints) rather than raw action sequences.",
            "task_name": "Language-to-spatial-distribution parsing for tabletop rearrangement",
            "task_description": "Translate free-form natural language instructions describing spatial patterns/relations into a structured set of spatial distribution functions f_i for objects (e.g., 'put mug to right of bowl' → a distribution region to the right of bowl).",
            "task_type": "instruction parsing / symbolic grounding (pre-planning stage)",
            "knowledge_type": "spatial+object-relational (mainly declarative spatial relations and object-selection), procedural (to a limited extent via implied ordering)",
            "knowledge_source": "pre-training on text corpora; in-context examples provided in prompts (few-shot); engineered system prompt guidelines",
            "has_direct_sensory_input": false,
            "elicitation_method": "engineered prompting with scene descriptions (object labels, colors, IDs) and examples (few-shot), returning structured JSON-like outputs",
            "knowledge_representation": "outputs symbolic structured distributions (list of f_i), natural-language-derived constraints mapped to symbolic forms; not an internal geometric map but a textual/structured spec that downstream modules convert to priors",
            "performance_metric": "indirect: quality of resulting F used by planner; downstream metrics reported (planning/execution success when used in LGMCTS pipeline)",
            "performance_result": "When used to generate intermediates for LGMCTS, leads to high downstream success (see LGMCTS: SRep 79.2% on ELGR). When LLMs are used directly as planners (LFSP baseline), planning success can be high but executability drops (LFSP SRep 45.2%).",
            "success_patterns": "Accurately identifies pattern types, selects object subsets, and produces interpretable constraints that can be matched to parametric priors; handles composite multi-pattern instructions.",
            "failure_patterns": "When asked to produce direct action sequences/policy code (LLM-as-planner), outputs can be incomplete or non-executable in geometrically complex scenarios; ambiguity in natural language can lead to underspecified distributions without additional grounding.",
            "baseline_comparison": "Compared qualitatively against using LLMs directly for planning (LFSP / Code as Policies / Progprompt): LLM-as-parser (used by LGMCTS) + classical planner outperforms LLM-as-end-to-end-planner on executability.",
            "ablation_results": "No explicit ablation, but paper contrasts 'LLM as intermediate representation generator' vs 'LLM as direct planner' showing benefits of the former.",
            "key_findings": "LLMs like GPT-4 encode enough commonsense spatial and relational knowledge to produce structured, actionable intermediate representations, but they should be used to generate interpretable constraints/distributions (not raw action sequences) for geometric tasks without endowing them with direct sensory input.",
            "uuid": "e548.1",
            "source_info": {
                "paper_title": "LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Sentence-BERT prior lookup",
            "name_full": "Sentence-BERT embedding keyed parametric prior selection",
            "brief_description": "Sentence-BERT embeddings of the language instruction are used to select the best-matching parametric geometric prior function from a database keyed by precomputed sentence embeddings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Sentence-BERT",
            "model_size": null,
            "model_description": "A sentence embedding model that maps input language L to a vector Θ(L); the system computes cosine (dot) similarity between Θ(L) and keys Θ_k for priors in a database to select a parametric prior f_prior^K.",
            "task_name": "Language-to-prior mapping for geometric pose sampling",
            "task_description": "Given a parsed instruction L and its embedding Θ(L), pick the parametric geometric prior function (e.g., line, circle, rectangle, tower, spatial:left/right) whose embedding key best matches the instruction so the planner can sample object poses consistent with the intended pattern.",
            "task_type": "knowledge retrieval / mapping (spatial representation selection)",
            "knowledge_type": "spatial",
            "knowledge_source": "pre-trained Sentence-BERT model applied at inference (no fine-tuning reported)",
            "has_direct_sensory_input": false,
            "elicitation_method": "embedding lookup (nearest-key selection in embedding space)",
            "knowledge_representation": "discrete database of parametric geometric prior functions keyed by Sentence-BERT sentence embeddings; selected prior returns parametric curve γ(t, κ(p0,p1)) used as f_prior",
            "performance_metric": "not directly quantified in isolation; evaluated via downstream planning/execution success (LGMCTS results)",
            "performance_result": "Not separately reported; integral to LGMCTS achieving high geometric fidelity and executability reported in paper.",
            "success_patterns": "Enables consistent mapping of natural language pattern descriptions to geometric prior families, supporting compositional and multi-object patterns.",
            "failure_patterns": "No explicit failures reported, but selection depends on how well the database covers language variability and on embedding matching fidelity.",
            "baseline_comparison": "Not compared against alternative mapping methods in the paper.",
            "ablation_results": "None reported.",
            "key_findings": "Using sentence embeddings to select explicit parametric geometric priors provides a practical way to convert underspecified language into concrete spatial distributions usable by a planner, enabling LLMs to remain language-only while the geometry is handled via explicit priors.",
            "uuid": "e548.2",
            "source_info": {
                "paper_title": "LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "Parametric Geometric Priors",
            "name_full": "Parametric geometric prior functions (pattern priors)",
            "brief_description": "A database of parametric curve-based prior functions (γ(t, κ)) representing geometric patterns (line, circle, rectangle, tower, spatial relations) used to convert language-described patterns into concrete pose distributions for sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "parametric prior database (not a neural LM)",
            "model_size": null,
            "model_description": "Each pattern is represented by a parametric curve γ(t, κ) with κ parameterized by two anchor positions p0,p1; sampling rules depend on how many objects have already been placed (ordered/unordered patterns), and add Gaussian jitter and collision-free masking.",
            "task_name": "Spatial prior specification for sampling object poses",
            "task_description": "Provides iterative, history-sensitive pose distributions f_prior(p_i | P_R, L) used inside the sequential sampling and MCTS simulation to generate collision-aware target poses consistent with language constraints.",
            "task_type": "spatial representation / sampling mechanism",
            "knowledge_type": "spatial (explicit geometric)",
            "knowledge_source": "hand-designed parametric functions stored in a database, selected via Sentence-BERT matching to language",
            "has_direct_sensory_input": false,
            "elicitation_method": "database lookup (selected by embedding similarity) and sequential sampling rules conditioned on sampled subset O_R^sampled",
            "knowledge_representation": "explicit parametric curves and sampling rules; combined multiplicatively with a boolean collision-free mask f_free to produce f_i",
            "performance_metric": "downstream planning/execution success and pattern fidelity (Structformer and ELGR benchmarks)",
            "performance_result": "Central to LGMCTS high performance: success rates e.g., Structformer line 95.99%, ELGR SRep 79.2%; no standalone numeric ablation reported.",
            "success_patterns": "Accurately enforces multi-object geometric relations (ordered/unordered) and supports compositional/multipattern instructions; yields collision-aware candidate poses.",
            "failure_patterns": "Limited to shapes representable as parametric curves included in the database; unseen or highly free-form spatial descriptions require either database expansion or ambiguous mapping.",
            "baseline_comparison": "Outperforms learned generative pose predictors (Structformer, StructDiffusion) in producing collision-free and executable goals.",
            "ablation_results": "Not reported; empirical comparisons imply priors + LLM + MCTS outperform purely learned goal generation followed by MCTS.",
            "key_findings": "Encoding spatial knowledge as explicit parametric priors selectable by language embeddings yields robust, history-aware pose distributions that, when combined with MCTS, improve executability versus purely learned or LLM-generated goal poses.",
            "uuid": "e548.3",
            "source_info": {
                "paper_title": "LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLM-as-Few-Shot-Planner (LFSP)",
            "name_full": "LLMs used as Few-Shot Planners (Code as Policies / Progprompt baselines)",
            "brief_description": "Baselines where LLMs (Code as Policies, Progprompt) are prompted (few-shot) to output action sequences or policy code for rearrangement; outputs are converted to sequences of object IDs and goal poses and given to a TAMP planner or executed.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLMs (Code as Policies, Progprompt - varying underlying LLMs such as GPT-family or LLaMA variants)",
            "model_size": null,
            "model_description": "LLMs are provided scene descriptions (object names, IDs, textures, initial poses, region boundaries) and few-shot examples; they generate ordered action lists or programmatic policies intended to realize the requested rearrangement without explicit geometric priors or classical sampling-based planners.",
            "task_name": "Direct LLM-based planning for object rearrangement (baseline)",
            "task_description": "Generate end-to-end plans (action sequences or code/policies) that move objects to satisfy language-described relations; used as a baseline to compare executability and completeness against LGMCTS.",
            "task_type": "object manipulation + sequential planning (end-to-end LLM planning)",
            "knowledge_type": "procedural + object-relational + spatial (intended to be encoded implicitly in LM weights)",
            "knowledge_source": "pre-trained LLM weights; in-context few-shot examples provided at inference",
            "has_direct_sensory_input": false,
            "elicitation_method": "few-shot prompting / code-generation style prompts (Code as Policies, Progprompt)",
            "knowledge_representation": "implicit procedural knowledge encoded in generated action sequences or program code; outputs are symbolic sequences of object moves and target poses",
            "performance_metric": "planning success rate and executable success rate (SRep) on ELGR; pattern fidelity on Structformer (subset)",
            "performance_result": "ELGR: planning success (SRp) 100% but executable success (SRep) 45.2%; Structformer (on 10% sampled scenes): Line 41.16%, Circle 51.75%, Tower 88.80%, Dinner 27.05% (lower than LGMCTS).",
            "success_patterns": "Good at composing high-level procedural sequences in simple or well-specified scenarios; can output sequences matching obvious constraints when scene is simple or examples are close.",
            "failure_patterns": "Produces plans that are frequently infeasible or incomplete in geometrically complex or cluttered scenes (high planning success but low executability), poor geometric fidelity, struggles with long-horizon or intricate geometric constraints.",
            "baseline_comparison": "Compared directly in experiments: although LFSP can often produce plausible action sequences, LGMCTS yields substantially higher executability and geometric fidelity.",
            "ablation_results": "Not applicable; baseline behavior contrasted with LGMCTS and PMCTS.",
            "key_findings": "Using LLMs to directly generate action sequences or policies without explicit geometric priors or a robust motion planner leads to many plans that are semantically plausible but not physically executable; this motivates using LLMs for intermediate symbolic representations rather than final plans.",
            "uuid": "e548.4",
            "source_info": {
                "paper_title": "LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "LLM-GROP",
            "name_full": "LLM-GROP (Task & Motion Planning with LLMs for object rearrangement)",
            "brief_description": "Related work that uses LLMs to translate language into pairwise spatial relationship specifications and then invokes a sampling-based TAMP planner (GROP); limited to pairwise relations and cannot handle complex multi-object geometric patterns.",
            "citation_title": "Task and motion planning with large language models for object rearrangement",
            "mention_or_use": "mention",
            "model_name": "LLMs (used to infer pairwise relations)",
            "model_size": null,
            "model_description": "LLM translates natural language commands into symbolic pairwise spatial relations (e.g., A left of B) which are then fed to a sampling-based task & motion planner (GROP).",
            "task_name": "Language-to-pairwise spatial relations for TAMP (related work)",
            "task_description": "Translate language to pairwise spatial constraints then use a TAMP solver to compute motions; targeted at object rearrangement but limited to pairwise relationships.",
            "task_type": "object manipulation + planning (related work)",
            "knowledge_type": "object-relational (pairwise) + spatial",
            "knowledge_source": "pre-trained LLMs used as translators",
            "has_direct_sensory_input": false,
            "elicitation_method": "prompting to generate symbolic relations",
            "knowledge_representation": "pairwise symbolic spatial relations (discrete constraints)",
            "performance_metric": "reported as limited in handling multi-object complex patterns (qualitative statement in this paper)",
            "performance_result": "Mentioned as limited to pairwise relations; no numeric result reported in this paper.",
            "success_patterns": "Works for simple pairwise spatial constraints.",
            "failure_patterns": "Cannot handle complex multi-object geometric patterns or compositions beyond pairwise relations.",
            "baseline_comparison": "Positioned as motivation why LGMCTS uses parametric geometric priors to handle multi-object patterns.",
            "ablation_results": "N/A (related work mention).",
            "key_findings": "LLM-based translation to pairwise symbolic relations is useful but insufficient for complex multi-object geometric arrangements; motivates richer prior representations.",
            "uuid": "e548.5",
            "source_info": {
                "paper_title": "LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement",
                "publication_date_yy_mm": "2023-09"
            }
        },
        {
            "name_short": "AutoTAMP",
            "name_full": "AutoTAMP (Autoregressive Task and Motion Planning with LLMs as translators and checkers)",
            "brief_description": "Related work that uses LLMs to translate natural language into formal STL representations and invokes formal planners; effective for a wide range of TAMP tasks but limited in non-discrete action spaces as in semantic rearrangement.",
            "citation_title": "Autotamp: Autoregressive task and motion planning with llms as translators and checkers",
            "mention_or_use": "mention",
            "model_name": "LLMs (used as translators and validators)",
            "model_size": null,
            "model_description": "LLMs are used to translate language into formal spatio-temporal logic (STL) representations that a planner can use; LLMs act as translators/checkers rather than direct planners.",
            "task_name": "Translation of natural language to formal representations for TAMP (related work)",
            "task_description": "Convert language into STL spec and use an STL planner to generate optimal trajectories; suited for discrete/structured tasks but less applicable to continuous semantic rearrangement.",
            "task_type": "task & motion planning (related work)",
            "knowledge_type": "procedural + spatial (encoded into formal STL)",
            "knowledge_source": "pre-trained LLMs used for translation",
            "has_direct_sensory_input": false,
            "elicitation_method": "LLM translation/few-shot prompting to produce STL",
            "knowledge_representation": "explicit formal representations (STL) produced by LLM for downstream planner",
            "performance_metric": "Reported strengths across many TAMP tasks in original work; here noted limitation for continuous/dense rearrangement tasks.",
            "performance_result": "Mentioned as not directly applicable to semantic rearrangement with large continuous action spaces (no numeric result given here).",
            "success_patterns": "Good for tasks that can be expressed in formal logic and solved by formal planners.",
            "failure_patterns": "Limited applicability when action space is continuous and high-dimensional as in semantic object rearrangement.",
            "baseline_comparison": "Contrasts with LGMCTS which targets continuous/rearrangement tasks via priors + sampling-based planner.",
            "ablation_results": "N/A (related work mention).",
            "key_findings": "Using LLMs to produce formal symbolic representations for classical planners is a promising approach, but it faces limitations in problems with continuous, high-dimensional action spaces unless complemented with suitable geometric representations or sampling strategies.",
            "uuid": "e548.6",
            "source_info": {
                "paper_title": "LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Code as policies: Language model programs for embodied control",
            "rating": 2
        },
        {
            "paper_title": "Task and motion planning with large language models for object rearrangement",
            "rating": 2
        },
        {
            "paper_title": "Autotamp: Autoregressive task and motion planning with llms as translators and checkers",
            "rating": 2
        },
        {
            "paper_title": "StructDiffusion: Object-centric diffusion for semantic rearrangement of novel objects",
            "rating": 2
        },
        {
            "paper_title": "Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects",
            "rating": 2
        },
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "SayCan: Grounding language in robotic affordances",
            "rating": 1
        },
        {
            "paper_title": "Inner monologue: Embodied reasoning through planning with language models",
            "rating": 1
        }
    ],
    "cost": 0.0178185,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement</h1>
<p>Haonan Chang, Kai Gao, Kowndinya Boyalakuntla, Alex Lee, Baichuan Huang, Harish Udhaya Kumar,
Jingjin Yu, Abdeslam Boularias</p>
<h6>Abstract</h6>
<p>We present LGMCTS, a framework that uniquely combines language guidance with geometrically informed sampling distributions to effectively rearrange objects according to geometric patterns dictated by natural language descriptions. LGMCTS uses Monte Carlo Tree Search (MCTS) to create feasible action plans that ensure executable semantic object rearrangement. We present a comprehensive comparison with leading approaches that use language to generate goal rearrangements independently of actionable planning, including Structformer, StructDiffusion, and Code as policies. We also present a new benchmark, the Executable Language Guided Rearrangement (ELGR) Bench, containing tasks involving intricate geometry. With the ELGR bench, we show limitations of task and motion planning (TAMP) solutions that are purely based on Large Language Models (LLM) such as Code as Policies and Progprompt on such tasks. Our findings advocate for using LLMs to generate intermediary representations rather than direct action planning in geometrically complex rearrangement scenarios, aligning with perspectives from recent literature. Our code and supplementary materials are accessible at https://lgmcts.github.io/.</p>
<h2>I. INTRODUCTION</h2>
<p>Everyday tasks, such as “Set up the kitchen”, involve organizing objects based on verbal instructions, a process that is intuitive for humans but that presents a significant challenge for robots. The semantic rearrangement problem seeks to empower robots with the ability to reorganize a scene according to linguistic descriptions. This challenge necessitates that robots comprehend the task through natural language, and address the corresponding Task And Motion Planning (TAMP) problem effectively.</p>
<p>Traditionally, solving this problem requires formalizing semantic rearrangement into a symbolic representation, clearly defining the goal configuration or constraints, and using formal planners such as STRIPS [1] and PDDL [2], or search-based planners like MCTS [3] to devise a feasible plan. Although effective, this approach demands expert-level knowledge to abstract a problem into a formal representation, limiting accessibility for average users.</p>
<p>To overcome this challenge, numerous recent studies have sought to tackle the problem directly from linguistic inputs and RGB-D observations [4]–[6]. One approach uses multimodality transformers to establish a correlation between verbal descriptions and object positions using data generated from the simulation. Following work such as StructDiffusion [5] further improved this method by using a diffusion</p>
<p>The authors are with the Department of Computer Science, Rutgers University, 08854 New Brunswick, USA. This work is supported by NSF awards 1846043 and 2132972.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: Robotic Setup: a UR5e robot equipped with a RealSense D455 camera. The task is to re-arrange the objects, which are unknown to the robot, according to a natural language instruction.</p>
<p>model to build the multi-modality solution. However, a common drawback of these methods is that they rely on an offline training stage, which makes them applicable only to trained object categories and spatial patterns.</p>
<p>With the advent of Large Language Models (LLMs), models such as GPT [7] and Llama [8] have demonstrated impressive potential in understanding complex scenarios and exhibiting zero-shot planning capabilities. This has led researchers to explore the utilization of LLMs in solving language-based TAMP problems [9]–[11]. However, despite specific considerations for the feasibility of plans proposed by LLMs, it has been reported that these plans significantly lag in executability and completeness when compared to those crafted by a properly implemented traditional solver designed for the task [12]. This observation has naturally led researchers to seek methods that merge the user-friendliness of LLMs with the robustness of traditional TAMP algorithms such as PDDL, STRIPS, or MCTS. LLM-GROP [13] follows this approach in rearrangement, employing LLMs to parse user tasks from language into pairwise spatial relationship specifications and then calling a sampling-based task and motion planner [14] to generate the plan. A limitation of LLM-GROP is that it can only handle pair-wise relationships, and thus cannot perform complex rearrangement tasks. AutoTAMP [15] uses LLMs to translate natural language into formal representations and then invokes a planner to tackle the problem. AutoTAMP can solve a wide range of TAMP tasks, but it does not apply to general semantic rearrangement where the action space is not discrete and potentially large.</p>
<p>We present Language-Guided Monte-Carlo Tree Search (LGMCTS), a new technique for executable semantic object</p>
<p>rearrangement. Like its predecessors, AutoTAMP and LLMGROP, LGMCTS leverages LLMs for generating intermediate representations and employs a planner for formulating feasible plans. A key novelty of LGMCTS is the integration of parametric geometric priors for spatial relationship representations. LGMCTS facilitates more nuanced handling of complex geometric relationships among multiple objects, addressing scenarios that require organization beyond simple pairwise interactions such as configurations in lines or rectangles. Additionally, LGMCTS takes a holistic approach by simultaneously considering task planning (goal specification) and motion planning (execution order and intermediate steps). During planning, an obstacle relocation strategy is used to handle obstacles that may block the execution. This coordination ensures that plans are not only semantically coherent but also practically executable, offering a balanced consideration of goal achievement and operational efficiency.</p>
<p>To assess the efficacy of LGMCTS, we introduce the Executable Language-Guided Rearrangement (ELGR) benchmark, featuring over 1,600 varied language queries and robot execution checks. Our evaluations indicate that LGMCTS performs effectively on the ELGR benchmark, especially in comparison with Code as policies and Progprompt in terms of feasibility and semantic consistency of the generated goals. LGMCTS also outperforms Structformer and StructDiffusion in goal generation on the Structformer dataset.</p>
<h2>II. RELATED WORKS</h2>
<h2>A. Learning-based Semantic Rearrangement</h2>
<p>The semantic rearrangement problem consists of devising a rearrangement plan that is both semantically congruent with a given language description and physically feasible [16]-[18]. In recent years, this has gained increased traction, particularly as a pivotal application in languagedriven robotics. CLIPort [4] took the initial step in this direction by merging CLIP features with a Transporter network. Yet, its design is limited to basic pick-and-place tasks. Structformer [6] advanced the field using a transformer model, by simulating rearrangements with hand-crafted rules and connecting language tokens to object poses. Leveraging Structformer's dataset, StructDiffusion [5] introduced a pose diffusion model to predict poses from language. Nonetheless, a common shortcoming amongst all these methodologies is their limitation to a single structure or pattern (e.g. circle, line) that they have been trained on, making composite patterns (e.g. rectangle + tower) a persistent challenge. Moreover, the rearrangement goals generated by these methods can be inexecutable.</p>
<h2>B. LLM-driven Task And Motion Planning</h2>
<p>Recent advancements in LLMs [7], [8] have showcased impressive performance across a broad spectrum of tasks. There has been a growing interest in using LLMs for TAMP [9]-[11], [13], [19]-[33], owing to their few-shot and zero-shot reasoning ability [7], [34], [35]. Grounding language into a sequence of plannable tasks/actions without retraining LLMs was initially explored in [9]. Following
this, SayCan [10] was proposed to facilitate the conversion of LLM-generated plans into robot-executable steps, though it struggled with addressing task execution failures. Inner Monologue [19] improved upon SayCan by incorporating real-time feedback to adjust plan post-execution, yet Inner Monologue is prone to generating suboptimal and infeasible plans. Instead of using LLMs to plan with predefined skills, other approaches such as Code as policies [11] and Progprompt [20] leveraged LLMs for policy code generation, showcasing their potential in behavioral common sense and sequential policy logic. However, they do not show promising results on complex object rearrangement tasks requiring more nuanced spatial context. This is due to the limitations of LLMs' planning ability over long horizon tasks [12], [28].</p>
<p>Owing to the drawbacks of the aforementioned works, to offer a more reliable and interpretable planning process, recent works [12], [15], [28], [29], [31] emphasize translating natural language commands into intermediate representations that are interpretable by traditional TAMP algorithms. Text2Motion [23] uses LLMs to greedily plan a skill sequence combined with a geometric feasibility planner to ensure that the geometric dependencies are addressed. However, Text2Motion's hybrid LLM planner is less efficient in large spaces than planners such as MCTS [3]. LLMGROP [13] translates language instructions to symbolic spatial relationships with LLMs and employs a task and motion planner named GROP [14] to perform the rearrangement. Although GROP is optimized for efficiency and feasibility, LLM-GROP is limited by its focus on simple object rearrangements due to its treatment of multi-object semantic relationships as pairwise reasoning. AutoTAMP [15] employs LLMs for generating and validating STL representation from natural language and utilizes a formal STL planner [36] for generating optimal trajectories. Although effective across a spectrum of tasks, its applicability is limited in non-discrete action spaces, as is the case in semantic rearrangement tasks.</p>
<p>To address these limitations, we introduce LGMCTS, a new approach that incorporates parametric geometric priors and that is guided by MCTS's efficient exploration to provide a feasible TAMP solution for complex rearrangement tasks.</p>
<h2>III. Preliminaries</h2>
<h2>A. Problem Formulation</h2>
<p>Semantic rearrangement is the task of rearranging a scene according to a series of natural language descriptions. One key insight here is that a goal described by language is usually a distribution rather than a single position. For example, "Put the mug at the right side of bowl" refers to a uniform distribution among a region that is right to the bowl. If we know the position distribution for each object, we just need to sequentially sample the poses for each object. The semantic rearrangement problem is then converted to a sequential sampling problem.</p>
<p>With this insight, we define the task of semantic rearrangement as follows. The robot is given as input a scene with objects from a set $O_{S}=\left{o_{1}, o_{2}, \ldots, o_{N}\right}$ and a command $L$, where $L$ is a pure natural language command that implies a</p>
<p>desired distribution list $\mathscr{F}=\left{f_{i}: p\left(o_{i}\right) \sim f_{i} \mid o_{i} \in O_{R}\right}$, where $p\left(o_{i}\right)$ refers to the position of object $o_{i}$. Here, $O_{R} \subseteq O_{S}$ denotes the objects requiring an action based on $L$, and $f_{i}$ indicates the desired pose distribution for each object. The objective is to identify an optimal action sequence, $A=\left(a_{t}\right)<em t="t">{t=1}^{H}$, where each action $a</em>$.}$ corresponds to moving an object $o_{i}$ to a sampled position $p\left(o_{i}\right)$, with the objective to achieve a goal arrangement aligning $L$, i.e., $\prod_{o_{i} \in O_{R}} f_{i}\left(p_{i}\right)&gt;0$ and minimizing the number of action steps $H$. Noticeably, $A$ includes not only movements of objects $o \in O_{R}$, but also those of distracting objects, denoted as $O_{D}$, with $O_{D} \subseteq O_{S</p>
<h2>B. Monte Carlo Tree Search (MCTS)</h2>
<p>We provide here a brief reminder of the MCTS [3] technique. A typical MCTS algorithm iteratively builds a search tree by performing the following four operations.</p>
<p>1) Selection. On a fully expanded node (all the children nodes have been visited), MCTS selects to explore the branch with the highest Upper Confidence Bound (UCB),</p>
<p>$$
\arg \max _{a}\left(\frac{w(f(s, a))}{n(f(s, a))}+C \sqrt{\frac{\log (n(s))}{n(f(s, a))}}\right)
$$</p>
<p>where $f(s, a)$ is the child node of state $s$ after action $a$, $w(\cdot)$ and $n(\cdot)$ are respectively cumulative rewards and the number of visits to a state.
2) Expansion. On a node that is not fully expanded, MCTS selects an action that has not been attempted yet.
3) Simulation. Given a node and a selected action, MCTS simulates a sequence of actions and receives a reward.
4) Back-Propagation. MCTS passes the terminal reward to ancestor nodes to update their cumulative expected rewards, which indicate the quality of the branch.
At each iteration, MCTS starts from the root node. When all the child nodes of the current node are visited, MCTS selects a child node with the UCB formula. When some child nodes of the current node are unvisited, MCTS expands by randomly selecting a new action and performing a simulation to reach a new child node. The new node returns a reward, which is back-propagated to all the ancestor nodes.</p>
<h2>IV. Method</h2>
<p>In a nutshell, LGMCTS starts by calling an LLM to parse the language description $L$ to a list of spatial distributions $\mathscr{F}=\left{f_{i}: p\left(o_{i}\right) \sim f_{i} \mid o_{i} \in O_{R}\right}$. Then it uses an MCTS-based procedure to find a physically feasible action sequence $A$ to rearrange the scene according to distributions $\mathscr{F}$.</p>
<h2>A. Language Parsing \&amp; Object Selection</h2>
<p>During the language parsing stage, we parse $L$ into $\mathscr{F}=$ $\left{f_{i}: p\left(o_{i}\right) \sim f_{i} \mid o_{i} \in O_{R}\right}$. Similar to previous methods [11], we conduct an automated prompt engineering to guide the LLM to perform the parsing. Fig. 2 showcases how prompt engineering is implemented. Essentially, the language model translates user requirements into structured goal configurations and constraints that guide task execution.</p>
<p>Consider the example depicted in Fig. 2. First comes the system prompt providing guidelines for the LLM to follow
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2: An example of language parsing. We are using GPT-4 [7] in this work.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3: Visualization of $(x, y)$ prior for 'line' pattern. From left to right: $K=0, K=1, K=2, K=3$, where $K=\left|O_{R}^{\text {sampled }}\right|$, the number of sampled object poses. White star marks are sampled poses. When $K=0$, the pose can be sampled anywhere. When $K=1$, it needed to sampled outside a circle region. After that, all poses will be sampled along the line defined by the first two poses.
when interpreting user queries. Then, we need to provide an object description such as semantic labels, colors, and IDs for the objects in the scene. In practice, we use the Recognize Anything Model (RAM) [37], [38] for producing the semantic labels, and a color detector to determine the colors of the objects in the scene. A unique ID is assigned to each object (using for the whole planning). Finally, we provide a user query describing the rearrangement goal. A structured answer is returned from the LLM. LLM's answers suggest how many patterns there are and which objects should be selected to form the pattern.</p>
<h2>B. Parametric Geometric Prior</h2>
<p>As mentioned in Section III-A, if we know the goal position distribution for each object, the semantic rearrangement problem can be converted to a sequential sampling problem. There are multiple details we need to pay attention to in this process. (1) The position distribution is not fixed but varies with the progress of sampling. For example, if we say objects A, B, and C need to be put into a line, and we sample in the order of A, B, and C, then the distributions of A and B are actually unbounded, and C must be placed on the line defined by the positions of A and B. (2) The position distribution should be collision-free. (3) One can build a database for many different spatial distributions, but we need to have a flexible mechanism to associate LLM's inferred distribution types with items in the database.</p>
<p>We show in the following how to compute the pose distribution function $f_{i}\left(p_{i} \mid P_{S}, L\right)$ for each object $o_{i}$ in the set $O_{R}$ during the sequential pose sampling process. Here, $p_{i}$ denotes the pose $(x, y, \theta)$ of an object $o_{i}, P_{S}$ represents the list of current poses for all objects in the scene, and $L$ is the natural language command.</p>
<p>We compute $f_{i}\left(p_{i} \mid P_{S}, L\right)$ as an element-wise product of</p>
<p>two components, a pattern prior function $f_{\text {prior }}\left(p_{i} \mid P_{R}, L\right)$ and a boolean function $f_{\text {free }}\left(p_{i} \mid P_{S}\right)$ of the workspace,</p>
<p>$$
f_{i}\left(p_{i} \mid P_{S}, L\right)=f_{\text {prior }}\left(p_{i} \mid P_{R}, L\right) \times f_{\text {free }}\left(p_{i} \mid P_{S}\right)
$$</p>
<p>In this equation, $P_{R}$ is the list of current poses of all objects in $O_{R}$. We note that $P_{R}$ is a subset to $P_{S}$. The function $f_{\text {free }}\left(p_{i} \mid P_{S}\right)$ is set to 1 if $p_{i}$ is collision-free from the remaining poses in $P_{S}$, and to 0 otherwise. $f_{\text {free }}$ is determined by running a 2 D collision simulation using point cloud observations for each object $o_{i} \in O_{S}$.</p>
<p>Our primary focus is to determine $f_{\text {prior }}\left(p_{i} \mid P_{R}, L\right)$. To this end, we employ an approach akin to the one described in [10]. We maintain a database comprising a collection of predefined prior functions. Each of these functions is linked with one or more Sentence-BERT embeddings, acting as keys. The function corresponding to the best matching key is selected, as follows,
$f_{\text {prior }}\left(p_{i} \mid P_{R}, L\right)=f_{\text {prior }}^{K}\left(p_{i} \mid P_{R}\right)$ with $K=\underset{K \in \text { database }}{\operatorname{argmax}}\left(\Theta_{k} \cdot \Theta(L)\right)$
Here, $\Theta_{k}$ is the $K^{\text {th }}$ key in the database, and $\Theta(L)$ is the Sentence-BERT embedding generated from the language instruction $L$. In summary, the most suitable prior function from the database is selected based on $\Theta(L)$.</p>
<p>We now delve into the definition of $f_{\text {prior }}^{K}\left(p_{i} \mid P_{R}\right)$ in the context of our work. We use a unique model for representing various distributions by employing parametric curves. A parametric curve can be expressed as $(x, y)=\gamma(t, \kappa)$, where $t$ ranges from 0 to 1 and $\kappa$ is a set of curve-defining parameters. In our work, $\kappa$ is modeled as a function of two 2D positions, denoted as $\kappa\left(p_{0}, p_{1}\right)$. Therefore, for each pattern, we define two functions: $\gamma$ and $\kappa$.</p>
<p>Given that pattern prior $f_{\text {prior }}$ is used inside a sequential sampling process (check Section IV-C for more details), the prior distribution needs to be iteratively updated to capture the history of sampling. Consequently, we further categorize $O_{R}$ based on whether the objects have been sampled in the current branch of the MCTS-Planner. Subsets $O_{R}^{\text {sampled }}$ and $O_{R}^{\text {unsampled }}$ denote the sampled and non-sampled objects, respectively. Thus, $O_{R}=O_{R}^{\text {sampled }} \cup O_{R}^{\text {unsampled }}$.</p>
<p>The probability $f_{\text {prior }}^{K}\left(p_{i} \mid P_{R}\right)$ ) of sampling a pose $p_{i}=$ $\left(x_{i}, y_{i}, \theta_{i}\right)$ for next object $o_{i} \in O_{R}^{\text {unsampled }}$ is given as follows.</p>
<ul>
<li>If $\left|O_{R}^{\text {sampled }}\right|=0$ then $\left(x_{i}, y_{i}, \theta_{i}\right) \sim U$, suggesting that the first object can be placed arbitrarily.</li>
<li>If $\left|O_{R}^{\text {sampled }}\right|=1$ then $\sqrt{\left(x_{i}-x_{0}\right)^{2}+\left(y_{i}-y_{0}\right)^{2}} \leq \delta$ and $\left(x_{i}, y_{i}, \theta_{i}\right) \sim U$, imposing that the second object must be sampled uniformly at a position that is distanced from the first by at most $\delta$.</li>
<li>If $\left|O_{R}^{\text {sampled }}\right|&gt;1$ then $\left(x_{i}, y_{i}\right)=\gamma\left(\frac{\left|O_{R}^{\text {sampled }}\right|}{\left|O_{R}\right|}, \kappa\left(p_{0}, p_{1}\right)\right)+$ $\varepsilon$ where $\varepsilon \sim G(0, \sigma)$, here $G$ represents a Gaussian distribution with mean zero and variance $\sigma$, and $\theta=$ $\operatorname{atan2}\left(1, \gamma^{\prime}\left(\frac{K}{N}\right)\right) . \theta$ represented the rotation angle of the object.
Our parametric geometrical representation enables us to model any geometric shapes that can be written into the
format of a parametric curve. In our current implementation, we defined shapes such as "line," "circle," "rectangle," "tower," "spatial:left," "spatial:right" and so on. Due to space constraints, we refrain from elaborating on the definitions of $\gamma$ and $\kappa$ for all these predefined patterns. Fig. 3 illustrates an example of a parametric geometric prior. Noticeably, we divide patterns into "ordered" and "unordered" based on whether the pattern requires an execution sequence.</li>
</ul>
<p>Note that in our work, language instruction $L$ is typically composed of multiple instructions that deal with different subsets of objects. Specifically, $L$ can be interpreted as a list $\left{L_{i}\right}$ of sub-instructions $L_{i}$. For example, $L$ can be a composite instruction: $L=\left{L_{1}, L_{2}\right}$, where $L_{1}$ refers to placing objects $A, B$, and $C$ in a line, and $L_{2}$ refers to placing object $A$ on the left of $B$. Each sub-instruction $L_{i} \in L$ is associated with a subset of objects $O_{R_{i}} \subseteq O_{R}$. In our example, $O_{R_{1}}={A, B, C}$ and $O_{R_{2}}={A, B}$. In the sequential sampling process described before, we presented the case of a single language instruction for simplicity, but the same process is used for sampling poses given by a complex instruction.</p>
<h2>C. Monte-Carlo Tree Search (MCTS) for TAMP</h2>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 4: A minimal example illustrates our MCTS-Planner's aim to arrange a table. The language description provided is: "Can you please put the apple behind the spoon? And I also want the cup at the right of the apple." The top row displays the current scene arrangement, while the bottom row shows the $f_{\text {prior }}$ and $f_{\text {free }}$ for the object being manipulated. $f=f_{\text {prior }}+f_{\text {free }}$. In spatial distribution figures, black represents probability 0 , and white probability 1 .</p>
<p>As previously mentioned, our rearrangement problem can be formulated as a sequential task. In each step, we sample a pose $p_{i}$ for each object $o_{i} \in O_{R}$ according to a pose distribution $f_{i}$ selected by the LLM and computed as described in Section IV-B. Once we complete all samplings, all objects will have been placed in their desired locations. However, this task cannot be executed by the robot in a naive sequential order, as the rearrangements made in previous steps may obstruct subsequent sampling. Therefore, we propose a task and motion planner based on the MCTS algorithm to simultaneously arrange the task and address object relocation issues. The objective of our MCTS-Planner is to fulfill all object position requirements defined by $\mathscr{F}$, the spatial distribution list. In the MCTS-Planner, we maintain a tree where each node $s$ in the tree comprises the current objects' poses, $\left{p_{1}, p_{2}, \ldots, p_{N}\right}$, and the remaining object spatial requirements $\mathscr{F}<em r="r">{r}$, where $\mathscr{F}</em>$.} \subseteq \mathscr{F</p>
<p>The MCTS-Planner operates through four phases: selection, expansion, simulation, and back-propagation, as described in Section III-B, adhering to the methodology from [39] except for the simulation stage. The reward for transitioning to a new state $s$ is quantified by the reduction in the number of spatial requirements, that is, $|\mathscr{F}|-\left|\mathscr{F}<em i="i">{r}\right|$. This reward structure mirrors the approach in [39], aiming to steer the search towards branches that efficiently move objects to their goal poses. The simulation phase is elaborated in Algorithm 1. This phase begins with the MCTS state $s$ and an attempted action of placing an object $o</em>$. However, this obstacle relocation strategy increases the robustness of our rearrangement planner, especially when the environment is cluttered. Although MCTS operates as an anytime search algorithm, our MCTS-Planner implementation returns the first solution it finds. Furthermore, we have proved that the MCTS-Planner is probabilistically complete within our specified framework in Proposition 4.1.}$ in a pose $p_{i} \sim f_{i}$, where $f_{i}$ is the pose distribution explained in the previous section. If the targeted object $o_{i}$ is not reachable, e.g. there exist some obstacles above it, a reachable obstacle is randomly selected from those above it, and a collision-free pose within the workspace is sampled for relocation (Lines 2-5). If $o_{i}$ is reachable, an attempt is made to sample its pose with distribution $f_{i}$ (Line 7). If the sampled pose is not collision-free, we will randomly choose an obstacle in the collision and relocate it to a collision-free pose within the workspace (Lines 9-13). Note that obstacle relocation may add previously placed objects back to the requirement list $\mathscr{F}_{r</p>
<p>Fig. 4 presents an illustrative example of the MCTSPlanner at work. Owing to space constraints, we will only explore three simulation steps along the branch that yield a feasible solution. In this scenario, the user requests, "Can you please put the apple behind the spoon? And I also want the cup to be at the right of the apple." In response, the LLM generates the spatial distribution list $\mathscr{F}=\left{f_{1}, f_{2}\right}$, where $f_{1}$ is for positioning the apple behind the spoon, and $f_{2}$ is for placing the cup to the right of the apple. Fig. 4(a) illustrates the initial arrangement of objects. Given the dependency of $f_{2}$ on the apple's position, $K$ actions will be sampled for $f_{1}$, but none for $f_{2}$ in this initial setup. Fig. 4(b) depicts the outcome of an action of sampling a pose from $f_{1}$, where the apple is relocated according to $f_{1}$. The dashed-line circles represent the other $K-1$ actions originating from the root node. After sampling $f_{1}$, we are left with $\mathscr{F}<em 2="2">{r}=\left{f</em>}\right}$ as shown in Fig. 4(b), and an attempt is made to position the cup to the right of the apple. However, the goal position is in collision with the knife, so we need to relocate the knife. Fig. 4(c) demonstrates the knife's relocation, maintaining $\mathscr{F<em 2="2">{r}=\left{f</em>\right}$. Ultimately, Fig. 4(d) showcases the final planning result.</p>
<p>Proposition 4.1: MCTS-Planner is probabilistic complete.
Proof: In the context of semantic rearrangement with distribution list $\mathscr{F}$, we consider a feasible sequence $A^{*}$ that moves objects to achieve a final state $A_{f}$, where $f_{i}\left(A_{f}\left[o_{i}\right]\right)&gt;$ 0 for each object $o_{i}$ in set $O_{R}$. We assert that as the number of iterations $K$ increases, the probability $p$ that MCTS finds a sequence meeting the goal approaches 1: $\lim _{K \rightarrow \infty} p=1$.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">1</span><span class="o">:</span><span class="w"> </span><span class="nt">Simulation</span>
<span class="w">    </span><span class="nt">Input</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">s</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">MCTS</span><span class="w"> </span><span class="nt">state</span><span class="o">,</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">f_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">pose</span><span class="w"> </span><span class="nt">distribution</span><span class="w"> </span><span class="o">(</span><span class="nt">place</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">pose</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">p_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="w"> </span><span class="err">\</span><span class="nt">sim</span><span class="w"> </span><span class="nt">f_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">).</span>
<span class="w">    </span><span class="nt">Output</span><span class="o">:</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">o</span><span class="o">,</span><span class="w"> </span><span class="nt">p</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">rearrangement</span><span class="w"> </span><span class="nt">action</span><span class="w"> </span><span class="o">(</span><span class="nt">place</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">pose</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">p</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="o">).</span>
<span class="w">    </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">not</span><span class="w"> </span><span class="nt">reachable</span><span class="w"> </span><span class="nt">then</span>
<span class="w">        </span><span class="nt">Select</span><span class="w"> </span><span class="nt">a</span><span class="w"> </span><span class="nt">reachable</span><span class="w"> </span><span class="nt">object</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">that</span><span class="w"> </span><span class="nt">is</span><span class="w"> </span><span class="nt">blocking</span><span class="w"> </span><span class="nt">object</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">p</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">uniformSampling</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">o</span><span class="o">,</span><span class="w"> </span><span class="nt">s</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">p</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">exists</span><span class="w"> </span><span class="nt">then</span><span class="w"> </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">o</span><span class="o">,</span><span class="w"> </span><span class="nt">p</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">else</span><span class="w"> </span><span class="nt">return</span><span class="w"> </span><span class="nt">failure</span><span class="o">;</span>
<span class="w">    </span><span class="nt">else</span>
<span class="w">        </span><span class="err">\</span><span class="o">(</span><span class="nt">p</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">sampling</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">s</span><span class="o">,</span><span class="w"> </span><span class="nt">f_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">if</span><span class="w"> </span><span class="nt">collisionFree</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">p</span><span class="o">,</span><span class="w"> </span><span class="nt">s</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span><span class="w"> </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="nt">o_</span><span class="p">{</span><span class="err">i</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">p</span><span class="err">\</span><span class="nt">right</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">        </span><span class="nt">else</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">o</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">Randomly</span><span class="w"> </span><span class="nt">choose</span><span class="w"> </span><span class="nt">an</span><span class="w"> </span><span class="nt">obstacle</span><span class="w"> </span><span class="nt">in</span><span class="w"> </span><span class="nt">collision</span><span class="o">;</span>
<span class="w">            </span><span class="err">\</span><span class="o">(</span><span class="nt">p</span><span class="w"> </span><span class="err">\</span><span class="nt">leftarrow</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">uniformSampling</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">o</span><span class="o">,</span><span class="w"> </span><span class="nt">s</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">            </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">p</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">exists</span><span class="w"> </span><span class="nt">then</span><span class="w"> </span><span class="nt">return</span><span class="w"> </span><span class="err">\</span><span class="o">((</span><span class="nt">o</span><span class="o">,</span><span class="w"> </span><span class="nt">p</span><span class="o">)</span><span class="err">\</span><span class="o">);</span>
<span class="w">            </span><span class="nt">else</span><span class="w"> </span><span class="nt">return</span><span class="w"> </span><span class="nt">failure</span><span class="o">;</span>
</code></pre></div>

<p>First, we prove that there is an action sequence $A_{0}^{<em>}$ whose actions can all be generated by MCTS-Planner. Note that in MCTS-Planner, an action satisfies either of the two rules: R1: Move an object $o_{i}$ to goal: it requires $o_{i} \in O_{R}$ and the new pose $p$ satisfies goal pose requirements (i.e., $f_{i}(p)&gt;0$ ); R2: Obstacle relocation: the old pose $p$ of the moved object makes other objects not reachable or it lies in others' goal regions, i.e., $\exists f \in \mathscr{F}<em 0="0">{r}$, s.t. $f(p)&gt;0$ (Line 2, 10). We construct $A</em>^{</em>}$ by reordering and deleting actions in $A^{<em>}$ as follows: (1) If an action in $A^{</em>}$ satisfies $\mathbf{R 1}$ or $\mathbf{R 2}$, we add the action to $A_{0}^{<em>}$. Otherwise, we delay the addition until it satisfies the rules; (2) If the action still cannot satisfy the rules before we examine the next action in $A^{</em>}$ for the same object, we delete the former action. This process ensures $A_{0}^{*}$ comprises only MCTS-Planner viable actions, leading to the desired arrangement $A_{f}$.</p>
<p>Next, we prove that the probability for MCTS to find an action sequence in the "neighborhood" of $A_{0}^{<em>}$ approaches 1 as $K$ increases. For each intermediate state $s$ of $A_{0}^{</em>}$, denote $\left(A_{0}^{<em>}[s] . o, A_{0}^{</em>}[s] . p\right)$ the rearrangement action that $A_{0}^{<em>}$ chooses at $s$. Let $r:=\left(\min <em 0="0">{1 \leq i \leq\left|A</em>^{</em>}\right|} C_{i}\right) / 2\left|A_{0}^{<em>}\right|$, where $C_{i}$ is the minimum distance making the placement pose of the $i^{\text {th }}$ action of $A_{0}^{</em>}$ invalid (out of goal distribution or in collision). For an intermediate state $s$ of $A_{0}^{<em>}$, let $p_{r}^{s}$ be the probability that after $K$ actions, there is an action moving $A_{0}^{</em>}[s] . o$ to the $r$-neighborhood of $A_{0}^{*}[s] . p$. We have</p>
<p>$$
\lim <em K="K" _infty="\infty" _rightarrow="\rightarrow">{K \rightarrow \infty} p \geq \lim </em>^{} \prod_{s \in A_{0<em>}} p_{r}^{s} \geq \lim <em 0="0">{K \rightarrow \infty}\left(1-\left(1-\frac{\pi r^{2}}{|W| N}\right)^{K}\right)\left|A</em>^{</em>}\right|
$$</p>
<p>where $|W|$ is the size of the workspace. Specifically, the second function indicates the probability of finding a feasible action sequence, where the intermediate state object poses are maintained in the neighborhood of those in $A_{0}^{<em>}$. In the third function, $\left(\pi r^{2}\right) /(|W| N)$ is the lower bound of the probability of choosing an action in $s$, moving $A_{0}^{</em>}[s] . o$ to the $r$-neighborhood of $A_{0}^{<em>}[s] . p$ and the base of the $\left|A_{0}^{</em>}\right|$ exponent is a lower bound of $p_{r}^{s}$. Since $K$ is the only variable in the third function, $\lim _{K \rightarrow \infty} p=1$.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Line (4295)</th>
<th>Circle (3416)</th>
<th>Tower (1335)</th>
<th>Dinner (2440)</th>
</tr>
</thead>
<tbody>
<tr>
<td>LFSP* [11], [20]</td>
<td>41.16%</td>
<td>51.75%</td>
<td>88.80%</td>
<td>27.05%</td>
</tr>
<tr>
<td>Structformer [6]</td>
<td>47.24%</td>
<td>62.64%</td>
<td>99.10%</td>
<td>28.36%</td>
</tr>
<tr>
<td>StructDiffusion [5]</td>
<td>61.49%</td>
<td>81.41%</td>
<td>98.95%</td>
<td>69.38%</td>
</tr>
<tr>
<td>LGMCTS (Ours)</td>
<td>95.99%</td>
<td>95.25%</td>
<td>100%</td>
<td>100%</td>
</tr>
</tbody>
</table>
<p>TABLE I: Efficacy of LFSP, Structformer, StructDiffusion, and LGMCTS across diverse rearrangement tasks (task counts indicated) from the Structformer dataset. *Due to budget constraints, the LLM baseline LFSP are evaluated on 1150 (10%) randomly selected scenes of the Structformer dataset.</p>
<h2>V EXPERIMENTS</h2>
<h3>V-A Baselines</h3>
<p>We compare our approach with the following baselines.
Structformer [6]. It is a multi-modal transformer specifically designed for language-guided rearrangement tasks.
StructDiffusion [5]. It employs a diffusion model combined with a learning-based collision checker for pattern pose generation.
LLMs as Few-Shot Planners [11], [20]. We integrate Code as Policies and Progprompt into our evaluation pipeline, where the former generates policy code and the latter Pythonic code. As we cannot directly use the generated code, to streamline the input to our TAMP planner, our setup modifies the output as a sequence of actions (object IDs and their target poses). Initially, LLM processes complete scene details—including object names, IDs, textures, initial poses, and region boundaries for the rearrangement. We then instruct the LLM to take the natural language command and produce the optimal action sequence that contains an ordered list of object IDs and goal poses of the considered objects for rearrangement. However, for evaluating the Structformer dataset, we consider the structured goal specification preavailable in the dataset as the input to the LLM as it can infer the action plan from this intermediate representation. This approach avoids redundancy, as generating a natural language command would just restate the same specification for the goal rearrangement. In evaluating both datasets, we provide a few scenes as examples where the output format is clearly defined to the LLM with ground-truth optimal sequence. This baseline is named LLMs as Few-Shot Planners (LFSP).
Pose+MCTS. The Pose+MCTS (PMCTS) approach assumes that a collision-free and semantically aligned goal pose is provided. However, direct execution of this pose might be hindered if the target space is already occupied. To address this, we utilize MCTS to search for a viable plan to place objects in their predetermined goal poses. MCTS is only used as a motion planner. This method follows a two-step approach of using goal poses independently of task planning.</p>
<h3>V-B Structformer Dataset</h3>
<p>We use the test set from the Structformer dataset to evaluate the goal pose generation ability. This dataset is composed of approximately 11,500 rearrangement tasks, categorized into four patterns: line, circle, tower, and dinner. A rearrangement plan is considered successful if it adheres to language constraints and is collision-free, except in the "tower" task where collisions are inevitable. The "dinner" task is approached as a composition of patterns, involving the arrangement of items like plates, bowls, and utensils into a "tower" for plates and bowls, with other items lined up beside it. In both Structformer and StructDiffusion's experimental setup, object selection for rearrangement is based on the object's shape and size. Our evaluation setup does not involve object selection based on shape and size. Hence, to adapt them to our evaluation setup, we provide those two baselines with ground-truth object selection. Since the tasks already specify which objects to rearrange for the single pattern rearrangement, based on language instructions, we did not use the LLM parser in LGMCTS for this dataset.</p>
<p>As shown in TABLE I, LGMCTS demonstrated superior performance in all four rearrangement task categories, achieving remarkable success rates as follows: 95.99% for "line", 95.25% for "circle", and 100% for both "tower" and "dinner". LSFP performs the least among the baselines due to the inability of LLMs to produce goal patterns with high geometric fidelity. While StructDiffusion showed improvement over Structformer, it did not match LGMCTS's effectiveness. For a visual comparison, Fig. 6 illustrates LGMCTS's success in a "circle" task scene, highlighting its more actionable goal poses that contribute to higher rearrangement success rates. Conversely, Structformer and StructDiffusion tend to generate goal arrangements with a higher collision risk, leading to lower success rates. The evaluation of this dataset strictly assesses the accuracy of collision-free geometric patterns in goal poses, disregarding the executability of plans—a notable limitation of the dataset. This limitation is addressed through our proposed ELGR-Benchmark (refer to Section V-C). PMCTS method is introduced through our benchmark to solely verify the executability of plans. As PMCTS deals with motion planning using collision-free ground-truth poses, it was not included in further comparisons on this dataset due to its inherent access to goal poses.</p>
<h3>V-C ELGR-Benchmark</h3>
<p>Existing datasets for semantic object rearrangement, such as Structformer, are limited in that they typically feature only one pattern per scene and do not include crowded scenarios. They also fail to address the challenge of feasibility, particularly when starting configurations are infeasible like one object being placed under another. To bridge these gaps, we introduce ELGR-Bench (Executable Language-Guided Rearrangement Benchmark), which incorporates scenarios with infeasible starting configurations, including tasks that require unstacking and appropriate placement of unstacked objects before the actual rearrangement. Importantly, this new benchmark presents a novel task termed the "multipattern task", which requires multiple pattern goals to be satisfied during the rearrangement process. In this benchmark, we are considering common shapes such as "line", "circle", "rectangle" and "spatial" (left/right, front/behind, left/right + front/behind). For each scene, we randomly compose two of the aforementioned patterns and create the multi-pattern task. Success is measured based on the executability of the</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 5: Real world demonstration with a UR5e robot. The language instructions for the five scenes are: (a) "Move all blocks into a circle; while put the white bottle behind one block;" (b) "Put all boxes into a rectangle; and move the white bottle to the right of one box;" (c) "Move bottles into a line; and formulate all phones into another line;" (d) "Formulate all yellow objects into a line;" (e) "Set all phones into a line;". The top row images show the initial scenes and the bottom ones show the results of using LGMCTS on the UR5e. Dotted lines imply a shape pattern and red arrows indicate a spatial pattern (left, right, front, back). These real robot experiments show that LGMCTS can parse complex language instructions and also deal with infeasible start configurations as well as pattern composition.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Fig. 6: Compared to Structformer and StructDiffusion, LGMCTS ensures a collision-free goal arrangement in all experiments.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>SRp</th>
<th>SRep</th>
</tr>
</thead>
<tbody>
<tr>
<td>LFSP [11], [20]</td>
<td>100%</td>
<td>45.2%</td>
</tr>
<tr>
<td>Structformer [6]</td>
<td>n.a.</td>
<td>n.a.</td>
</tr>
<tr>
<td>StructDiffusion [5]</td>
<td>n.a.</td>
<td>n.a.</td>
</tr>
<tr>
<td>PMCTS</td>
<td>82.9%</td>
<td>74.1%</td>
</tr>
<tr>
<td>LGMCTS (Ours)</td>
<td>90.9%</td>
<td>79.2%</td>
</tr>
</tbody>
</table>
<p>TABLE II: SRep, the executable plan success rate, reflects both planning success and the success of executing these plans, indicating if the final positions of objects meet the criteria set by language-based constraints. SRp, a part of SRep, only tracks planning success, with PMCTS and LGMCTS capped at 10,000 planning steps. If planning with MCTS exceeds the limit, often due to dense object placement in the scene, the motion planning is considered a failure.</p>
<p>generated plan and its adherence to semantic requirements. ELGR-Bench builds upon the VIMA-Benchmark [40].</p>
<p>In our benchmark, we compared LGMCTS against two baselines: LFSP and PMCTS, excluding Structformer and StructDiffusion as they cannot handle composite geometric patterns. LFSP, leveraging an LLM, plans goal poses and action sequences simultaneously, while PMCTS, follows a two-step method using a given goal pose and then using MCTS for action planning. LGMCTS uniquely combines goal generation with action planning, aiming for more executable outcomes. As shown in TABLE II, LFSP demonstrates a 100% planning success rate through LLM's capability to generate action plans based on natural language commands and scene context. Nonetheless, over 50% of these plans are inexecutable, as indicated by the SRep scores. LGMCTS, however, manages a 90% success rate in generating action plans, with about 80% being executable. This performance not only underlines the limitations of LLMs in direct TAMP solving but also showcases LGMCTS's advantage over the two-step PMCTS approach, even when PMCTS is provided with accurate and feasible goal poses.</p>
<h3><em>D. Real Robot Experiments</em></h3>
<p>We qualitatively evaluated our system using a UR5e robot equipped with a D455 depth camera. The setup of the robot is shown in Fig. 1. We employed the Recognize-Anything-Model (RAM) [37], [38] and an HSV-based color detector to detect object semantics and colors. Selected queries and their corresponding execution outcomes are presented in Fig. 5. We considered five different language instructions involving various objects and initial configurations. For example, Fig. 5(b) illustrates the experiment with "Put all boxes into a rectangle, and move the white bottle to the right of one box." This experiment involves pattern composition, requiring simultaneous consideration of "line" and "to the right of" constraint. Additionally, this scene presented an infeasible initial configuration, necessitating the removal of the yellow block before moving the gelatin box. Each experiment presented distinct challenges; for more details, refer to Fig. 5. These real-world robot experiments underscore the capabilities of LGMCTS in complex real-world settings.</p>
<h3>VI. CONCLUSION</h3>
<p>We introduced LGMCTS, a new framework for tabletop, semantic object rearrangement tasks. LGMCTS stands out by accepting free-form natural language input, accommodating multiple pattern requirements, and jointly solving goal pose generation and action planning. Its main limitation is the extended execution time for complex scenes, highlighting the need for improved tree search efficiency. Future research should focus on adapting LGMCTS to more complex rearrangement scenarios.</p>
<h2>REFERENCES</h2>
<p>[1] R. E. Fikes and N. J. Nilsson, "Strips: A new approach to the application of theorem proving to problem solving," Artificial intelligence, vol. 2, no. 3-4, pp. 189-208, 1971.
[2] M. Fox and D. Long, "Pddl2. 1: An extension to pddl for expressing temporal planning domains," Journal of artificial intelligence research, vol. 20, pp. 61-124, 2003.
[3] R. Coulom, "Efficient selectivity and backup operators in monte-carlo tree search," in International conference on computers and games. Springer, 2006, pp. 72-83.
[4] M. Shridhar, L. Manuelli, and D. Fox, "Cliport: What and where pathways for robotic manipulation," in 5th Annual Conference on Robot Learning, 2021.
[5] W. Liu, T. Hermans, S. Chernova, and C. Paxton, "Structdiffusion: Object-centric diffusion for semantic rearrangement of novel objects," in Workshop on Language and Robotics at CoRL 2022, 2022.
[6] W. Liu, C. Paxton, T. Hermans, and D. Fox, "Structformer: Learning spatial structure for language-guided semantic rearrangement of novel objects," in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 6322-6329.
[7] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., "Language models are few-shot learners," Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.
[8] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., "Llama 2: Open foundation and fine-tuned chat models," arXiv preprint arXiv:2307.09288, 2023.
[9] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents," in International Conference on Machine Learning. PMLR, 2022, pp. 9118-9147.
[10] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho, J. Ibarz, A. Irpan, E. Jang, R. Julian et al., "Do as i can, not as i say: Grounding language in robotic affordances," in Conference on Robot Learning. PMLR, 2023, pp. 287-318.
[11] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. lchter, P. Florence, and A. Zeng, "Code as policies: Language model programs for embodied control," in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 9493-9500.
[12] K. Valmeekam, A. Olmo, S. Sreedharan, and S. Kambhampati, "Large language models still can't plan (a benchmark for LLMs on planning and reasoning about change)," in NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022. [Online]. Available: https://openreview.net/forum?id=wUU-7XTL5XO
[13] Y. Ding, X. Zhang, C. Paxton, and S. Zhang, "Task and motion planning with large language models for object rearrangement," arXiv preprint arXiv:2303.06247, 2023.
[14] X. Zhang, Y. Zhu, Y. Ding, Y. Zhu, P. Stone, and S. Zhang, "Visually grounded task and motion planning for mobile manipulation," in 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2022, pp. 1925-1931.
[15] Y. Chen, J. Arkin, Y. Zhang, N. Roy, and C. Fan, "Autotamp: Autoregressive task and motion planning with llms as translators and checkers," arXiv preprint arXiv:2306.06531, 2023.
[16] S. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy, "Understanding natural language commands for robotic navigation and mobile manipulation," in Proceedings of the AAAI conference on artificial intelligence, vol. 25, no. 1, 2011, pp. 15071514.
[17] T. M. Howard, S. Tellex, and N. Roy, "A natural language planner interface for mobile manipulators," in 2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 66526659.
[18] H. Chang, K. Boyalakuntla, S. Lu, S. Cai, E. Jing, S. Keskar, S. Geng, A. Abbas, L. Zhou, K. Bekris, and A. Boularias, "Context-aware entity grounding with open-vocabulary 3d scene graphs," 2023. [Online]. Available: https://arxiv.org/abs/2309.15940
[19] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar et al., "Inner monologue: Embodied reasoning through planning with language models," in Conference on Robot Learning. PMLR, 2023, pp. 1769-1782.
[20] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg, "Progprompt: Generating situated
robot task plans using large language models," in 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023, pp. 11523-11530.
[21] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su, "Llm-planner: Few-shot grounded planning for embodied agents with large language models," in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 2998-3009.
[22] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhury, B. lchter, A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., "Palm-e: An embodied multimodal language model," arXiv preprint arXiv:2303.03378, 2023.
[23] K. Lin, C. Agia, T. Migimatsu, M. Pavone, and J. Bohg, "Text2motion: From natural language instructions to feasible plans," in ICRA2023 Workshop on Pretraining for Robotics (PT4R), 2023.
[24] J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. Funkhouser, "Tidybot: Personalized robot assistance with large language models," arXiv preprint arXiv:2305.05658, 2023.
[25] W. Huang, C. Wang, R. Zhang, Y. Li, J. Wu, and L. Fei-Fei, "Voxposer: Composable 3d value maps for robotic manipulation with language models," in Conference on Robot Learning. PMLR, 2023, pp. 540562.
[26] Z. Wu, B. Ai, and D. Hsu, "Integrating common sense and planning with large language models for room tidying," in RSS 2023 Workshop on Learning for Task and Motion Planning, 2023.
[27] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. LozanoPérez, and L. P. Kaelbling, "Pddl planning with pretrained large language models," in NeurIPS 2022 foundation models for decision making workshop, 2022.
[28] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone, "Llm+ p: Empowering large language models with optimal planning proficiency," arXiv preprint arXiv:2304.11477, 2023.
[29] Y. Xie, C. Yu, T. Zhu, J. Bai, Z. Gong, and H. Soh, "Translating natural language to planning goals with large-language models," arXiv preprint arXiv:2302.05128, 2023.
[30] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. Reid, and N. Suenderhauf, "Sayplan: Grounding large language models using 3d scene graphs for scalable robot task planning," in 7th Annual Conference on Robot Learning, 2023.
[31] L. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati, "Leveraging pre-trained large language models to construct and utilize world models for model-based task planning," Advances in Neural Information Processing Systems, vol. 36, 2024.
[32] Z. Zhao, W. S. Lee, and D. Hsu, "Large language models as commonsense knowledge for large-scale task planning," Advances in Neural Information Processing Systems, vol. 36, 2024.
[33] T. Birr, C. Pohl, A. Younes, and T. Asfour, "Autogpt+ p: Affordancebased task planning with large language models," arXiv preprint arXiv:2402.10778, 2024.
[34] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou et al., "Chain-of-thought prompting elicits reasoning in large language models," Advances in Neural Information Processing Systems, vol. 35, pp. 24824-24837, 2022.
[35] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., "Emergent abilities of large language models," Transactions on Machine Learning Research, 2022.
[36] D. Sun, J. Chen, S. Mitra, and C. Fan, "Multi-agent motion planning from signal temporal logic specifications," IEEE Robotics and Automation Letters, vol. 7, no. 2, pp. 3451-3458, 2022.
[37] X. Huang, Y. Zhang, J. Ma, W. Tian, R. Feng, Y. Zhang, Y. Li, Y. Guo, and L. Zhang, "Tag2text: Guiding vision-language model via image tagging," arXiv preprint arXiv:2303.05657, 2023.
[38] Y. Zhang, X. Huang, J. Ma, Z. Li, Z. Luo, Y. Xie, Y. Qin, T. Luo, Y. Li, S. Liu et al., "Recognize anything: A strong image tagging model," arXiv preprint arXiv:2306.03514, 2023.
[39] Y. Labbé, S. Zagoruyko, I. Kalevanykh, I. Laptev, J. Carpentier, M. Aubry, and J. Sivic, "Monte-carlo tree search for efficient visually guided rearrangement planning," IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 3715-3722, 2020.
[40] Y. Jiang, A. Gupta, Z. Zhang, G. Wang, Y. Dou, Y. Chen, L. FeiFei, A. Anandkumar, Y. Zhu, and L. Fan, "Vima: General robot manipulation with multimodal prompts," in NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
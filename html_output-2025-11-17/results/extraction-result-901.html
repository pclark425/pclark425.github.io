<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-901 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-901</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-901</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-47aacaab789e80388d22598b4810213655e62888</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/47aacaab789e80388d22598b4810213655e62888" target="_blank">Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents</a></p>
                <p><strong>Paper Venue:</strong> Annual Meeting of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> The proposed Mobile-Bench is a novel benchmark for evaluating the capabilities of LLM-based mobile agents, and introduces a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps.</p>
                <p><strong>Paper Abstract:</strong> With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e901.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e901.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large instruction-following transformer LLM from OpenAI used via API; in this paper it is evaluated as the reasoning/planning core of a mobile agent performing UI and API-based mobile tasks on the Mobile-Bench benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following large transformer accessed via OpenAI API; used as the central planner/reflection module in an observation-thought-action loop that issues API calls and UI actions based on HTML-converted UI observations and action history.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Mobile-Bench (SAST / SAMT / MAMT)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>mobile UI/API interaction; tool use; planning; multi-step reasoning; sequential decision-making; multi-APP collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>SAST: Avg steps 3.79, PassRate 80.96%, CheckPoint_0 81.57%, CheckPoint_1 83.76%; SAMT: Avg steps 13.94, PassRate 63.0%, CheckPoint_0 72.66%, CheckPoint_1 77.35%; MAMT: Avg steps 44.86, PassRate 26.5%, CheckPoint_0 61.34%, CheckPoint_1 52.98%. (From Table 4 and Table 5/6)</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>observation-thought-action loop (reflection/'thought' component), explicit planning module ('plan'), action-history memory, tool-use interface supporting both API calls and UI actions (click/input/scroll), CheckPoint process-evaluation metric.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting / in-context learning (3-shot) via API; no model fine-tuning reported for experiments in this paper (authors note fine-tuning for instruction-following would be necessary in Limitations).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>hybrid approach (API+UI) and prompting/architectural design (explicit plan and thought/reflection modules)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Mobile-Bench agent design combines API calls and UI actions so the LLM can choose either; uses an explicit planning step to produce an overall plan (which apps to use and subtasks) and a recurrent 'thought' reflection step between actions; instruments action history and CheckPoint targets; also applies action-history compression for context-length limits.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Quantified ablations (GPT-4): Removing APIs (w/o API) reduced PassRate: SAST 80.96% -> 74.39% (-6.57 pts), SAMT 63% -> 48% (-15 pts), MAMT 26.5% -> 9.5% (-17 pts); CheckPoint_1 dropped SAST 83.76 -> 72.73 (-11.03 pts), SAMT 77.35 -> 56.74 (-20.61 pts), MAMT 52.98 -> 31.69 (-21.29 pts) (Table 5). Ablating 'thought' drastically reduced PassRate: SAST 76% -> 24%, SAMT 77% -> 20%; ablating 'plan' reduced SAMT PassRate notably (77% -> 62%) while SAST was less affected (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors attribute reduced interactive performance to LLM hallucinations/illusions in API calls, greedy exploration behavior (difficulty deciding when to exit an app and move to the next), limitations from long action histories and context-length (requiring compression), and imperfect UI understanding (graphical elements without descriptive text).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e901.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e901.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An instruction-tuned transformer LLM from OpenAI used via API as a planner and action-decider in the Mobile-Bench mobile agent evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-following transformer accessed via OpenAI API; used in the same baseline agent architecture (plan/thought loop) with 3-shot in-context prompting; in experiments GPT-3.5 used a 16K context interface when budget allowed.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Mobile-Bench (SAST / SAMT / MAMT)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>mobile UI/API interaction; tool use; planning; multi-step reasoning; sequential decision-making; multi-APP collaboration</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>From Table 4: SAST Avg steps 4.53, PassRate 64.94%, CheckPoint_0 66.75%, CheckPoint_1 76.21%; SAMT Avg steps 12.06, PassRate 64.0%, CheckPoint_0 67.0%, CheckPoint_1 71.29%; MAMT Avg steps 48.73, PassRate 15.5%, CheckPoint_0 43.16%, CheckPoint_1 44.09%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>used within the paper's baseline agent: planning module, reflection/'thought' component, action history, API+UI action interface.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>in-context few-shot prompting (3-shot) via API; no additional training reported.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Same general causes discussed by authors: hallucinated API calls, greedy exploration, difficulty with multi-app planning and long action histories leading to impaired sequential decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e901.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e901.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open foundation transformer LLM (13B parameters) used in the Mobile-Bench experiments as a planner/reflection module within the agent architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 13B-parameter Llama family transformer model used (presumably locally) in the baseline agent pipeline, driven via 3-shot in-context prompts to generate plans, API selection, UI actions and thoughts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Mobile-Bench (SAST / SAMT / MAMT)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>mobile UI/API interaction; tool use; planning; multi-step reasoning; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>From Table 4: SAST Avg steps 7.43, PassRate 44.58%, CheckPoint_0 46.08%, CheckPoint_1 34.85%; SAMT Avg steps 18.76, PassRate 27.67%, CheckPoint_0 43.67%, CheckPoint_1 29.13%; MAMT Avg steps 49.52, PassRate 8.0%, CheckPoint_0 28.74%, CheckPoint_1 21.39%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>used inside the common baseline agent architecture (planning, thought/reflection, action history, API+UI action selection).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>in-context few-shot prompting (3-shot) during experiments; model weights not fine-tuned in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors note model-specific issues such as misjudging task completion (high PassRate but low CheckPoint coverage), indicating false-positive termination and overconfident self-assessment; general causes also include API hallucination and difficulty in multi-app planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e901.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e901.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70B-parameter open foundation transformer LLM from the LLaMA family evaluated as the planner/reflection core in the Mobile-Bench agent.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A 70B-parameter transformer model used inside the baseline agent (plan/thought/action loop) to generate plans, API choices and UI actions via few-shot prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Mobile-Bench (SAST / SAMT / MAMT)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>mobile UI/API interaction; tool use; planning; multi-step reasoning; sequential decision-making</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>From Table 4: SAST Avg steps 5.97, PassRate 56.62%, CheckPoint_0 56.62%, CheckPoint_1 63.12%; SAMT Avg steps 16.63, PassRate 54.0%, CheckPoint_0 61.0%, CheckPoint_1 62.73%; MAMT Avg steps 48.91, PassRate 13.5%, CheckPoint_0 39.98%, CheckPoint_1 41.21%.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>used as part of the baseline agent architecture with planning and reflection components, action history, and hybrid API+UI action capability.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>in-context few-shot prompting (3-shot) in experiments; no reported fine-tuning within this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>As with other models: hallucinated API calls, greedy exploration, difficulties in multi-app planning and managing long action histories; these limit sequential decision-making performance compared to LLM strengths on static QA-style tasks (not measured in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toolllm: Facilitating large language models to master 16000+ real-world apis <em>(Rating: 2)</em></li>
                <li>WebShop: Towards scalable real-world web interaction with grounded language agents <em>(Rating: 2)</em></li>
                <li>MobileEnv: A universal platform for training and evaluation of mobile interaction <em>(Rating: 2)</em></li>
                <li>Agentbench: Evaluating llms as agents <em>(Rating: 2)</em></li>
                <li>Toolformer: Language models can teach themselves to use tools <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-901",
    "paper_id": "paper-47aacaab789e80388d22598b4810213655e62888",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-4",
            "name_full": "GPT-4",
            "brief_description": "A large instruction-following transformer LLM from OpenAI used via API; in this paper it is evaluated as the reasoning/planning core of a mobile agent performing UI and API-based mobile tasks on the Mobile-Bench benchmark.",
            "citation_title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-4",
            "model_description": "Instruction-following large transformer accessed via OpenAI API; used as the central planner/reflection module in an observation-thought-action loop that issues API calls and UI actions based on HTML-converted UI observations and action history.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Mobile-Bench (SAST / SAMT / MAMT)",
            "interactive_task_type": "mobile UI/API interaction; tool use; planning; multi-step reasoning; sequential decision-making; multi-APP collaboration",
            "interactive_performance": "SAST: Avg steps 3.79, PassRate 80.96%, CheckPoint_0 81.57%, CheckPoint_1 83.76%; SAMT: Avg steps 13.94, PassRate 63.0%, CheckPoint_0 72.66%, CheckPoint_1 77.35%; MAMT: Avg steps 44.86, PassRate 26.5%, CheckPoint_0 61.34%, CheckPoint_1 52.98%. (From Table 4 and Table 5/6)",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "observation-thought-action loop (reflection/'thought' component), explicit planning module ('plan'), action-history memory, tool-use interface supporting both API calls and UI actions (click/input/scroll), CheckPoint process-evaluation metric.",
            "training_method": "prompting / in-context learning (3-shot) via API; no model fine-tuning reported for experiments in this paper (authors note fine-tuning for instruction-following would be necessary in Limitations).",
            "intervention_type": "hybrid approach (API+UI) and prompting/architectural design (explicit plan and thought/reflection modules)",
            "intervention_description": "Mobile-Bench agent design combines API calls and UI actions so the LLM can choose either; uses an explicit planning step to produce an overall plan (which apps to use and subtasks) and a recurrent 'thought' reflection step between actions; instruments action history and CheckPoint targets; also applies action-history compression for context-length limits.",
            "intervention_effect": "Quantified ablations (GPT-4): Removing APIs (w/o API) reduced PassRate: SAST 80.96% -&gt; 74.39% (-6.57 pts), SAMT 63% -&gt; 48% (-15 pts), MAMT 26.5% -&gt; 9.5% (-17 pts); CheckPoint_1 dropped SAST 83.76 -&gt; 72.73 (-11.03 pts), SAMT 77.35 -&gt; 56.74 (-20.61 pts), MAMT 52.98 -&gt; 31.69 (-21.29 pts) (Table 5). Ablating 'thought' drastically reduced PassRate: SAST 76% -&gt; 24%, SAMT 77% -&gt; 20%; ablating 'plan' reduced SAMT PassRate notably (77% -&gt; 62%) while SAST was less affected (Table 6).",
            "hypothesized_cause_of_gap": "Authors attribute reduced interactive performance to LLM hallucinations/illusions in API calls, greedy exploration behavior (difficulty deciding when to exit an app and move to the next), limitations from long action histories and context-length (requiring compression), and imperfect UI understanding (graphical elements without descriptive text).",
            "uuid": "e901.0",
            "source_info": {
                "paper_title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "GPT-3.5-turbo",
            "name_full": "GPT-3.5-turbo",
            "brief_description": "An instruction-tuned transformer LLM from OpenAI used via API as a planner and action-decider in the Mobile-Bench mobile agent evaluations.",
            "citation_title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-3.5-turbo",
            "model_description": "Instruction-following transformer accessed via OpenAI API; used in the same baseline agent architecture (plan/thought loop) with 3-shot in-context prompting; in experiments GPT-3.5 used a 16K context interface when budget allowed.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Mobile-Bench (SAST / SAMT / MAMT)",
            "interactive_task_type": "mobile UI/API interaction; tool use; planning; multi-step reasoning; sequential decision-making; multi-APP collaboration",
            "interactive_performance": "From Table 4: SAST Avg steps 4.53, PassRate 64.94%, CheckPoint_0 66.75%, CheckPoint_1 76.21%; SAMT Avg steps 12.06, PassRate 64.0%, CheckPoint_0 67.0%, CheckPoint_1 71.29%; MAMT Avg steps 48.73, PassRate 15.5%, CheckPoint_0 43.16%, CheckPoint_1 44.09%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "used within the paper's baseline agent: planning module, reflection/'thought' component, action history, API+UI action interface.",
            "training_method": "in-context few-shot prompting (3-shot) via API; no additional training reported.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Same general causes discussed by authors: hallucinated API calls, greedy exploration, difficulty with multi-app planning and long action histories leading to impaired sequential decision-making.",
            "uuid": "e901.1",
            "source_info": {
                "paper_title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLaMA-13B",
            "name_full": "LLaMA-13B",
            "brief_description": "An open foundation transformer LLM (13B parameters) used in the Mobile-Bench experiments as a planner/reflection module within the agent architecture.",
            "citation_title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA-13B",
            "model_description": "A 13B-parameter Llama family transformer model used (presumably locally) in the baseline agent pipeline, driven via 3-shot in-context prompts to generate plans, API selection, UI actions and thoughts.",
            "model_size": "13B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Mobile-Bench (SAST / SAMT / MAMT)",
            "interactive_task_type": "mobile UI/API interaction; tool use; planning; multi-step reasoning; sequential decision-making",
            "interactive_performance": "From Table 4: SAST Avg steps 7.43, PassRate 44.58%, CheckPoint_0 46.08%, CheckPoint_1 34.85%; SAMT Avg steps 18.76, PassRate 27.67%, CheckPoint_0 43.67%, CheckPoint_1 29.13%; MAMT Avg steps 49.52, PassRate 8.0%, CheckPoint_0 28.74%, CheckPoint_1 21.39%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "used inside the common baseline agent architecture (planning, thought/reflection, action history, API+UI action selection).",
            "training_method": "in-context few-shot prompting (3-shot) during experiments; model weights not fine-tuned in this work.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Authors note model-specific issues such as misjudging task completion (high PassRate but low CheckPoint coverage), indicating false-positive termination and overconfident self-assessment; general causes also include API hallucination and difficulty in multi-app planning.",
            "uuid": "e901.2",
            "source_info": {
                "paper_title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "LLaMA-70B",
            "name_full": "LLaMA-70B",
            "brief_description": "A 70B-parameter open foundation transformer LLM from the LLaMA family evaluated as the planner/reflection core in the Mobile-Bench agent.",
            "citation_title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA-70B",
            "model_description": "A 70B-parameter transformer model used inside the baseline agent (plan/thought/action loop) to generate plans, API choices and UI actions via few-shot prompting.",
            "model_size": "70B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Mobile-Bench (SAST / SAMT / MAMT)",
            "interactive_task_type": "mobile UI/API interaction; tool use; planning; multi-step reasoning; sequential decision-making",
            "interactive_performance": "From Table 4: SAST Avg steps 5.97, PassRate 56.62%, CheckPoint_0 56.62%, CheckPoint_1 63.12%; SAMT Avg steps 16.63, PassRate 54.0%, CheckPoint_0 61.0%, CheckPoint_1 62.73%; MAMT Avg steps 48.91, PassRate 13.5%, CheckPoint_0 39.98%, CheckPoint_1 41.21%.",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "used as part of the baseline agent architecture with planning and reflection components, action history, and hybrid API+UI action capability.",
            "training_method": "in-context few-shot prompting (3-shot) in experiments; no reported fine-tuning within this paper.",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "As with other models: hallucinated API calls, greedy exploration, difficulties in multi-app planning and managing long action histories; these limit sequential decision-making performance compared to LLM strengths on static QA-style tasks (not measured in this paper).",
            "uuid": "e901.3",
            "source_info": {
                "paper_title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toolllm: Facilitating large language models to master 16000+ real-world apis",
            "rating": 2
        },
        {
            "paper_title": "WebShop: Towards scalable real-world web interaction with grounded language agents",
            "rating": 2
        },
        {
            "paper_title": "MobileEnv: A universal platform for training and evaluation of mobile interaction",
            "rating": 2
        },
        {
            "paper_title": "Agentbench: Evaluating llms as agents",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools",
            "rating": 1
        }
    ],
    "cost": 0.015314999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents</h1>
<p>Shihan Deng ${ }^{13^{<em> 1}}$, Weikai Xu ${ }^{13^{</em> 1}}$, Hongda Sun ${ }^{23^{* 1}}$, Wei Liu ${ }^{3}$, Tao Tan ${ }^{2}$, Jianfeng Liu ${ }^{3}$, Ang $\mathrm{Li}^{3}$, Jian Luan ${ }^{3}$, Bin Wang ${ }^{3}$, Rui Yan ${ }^{2^{\dagger}}$ and Shuo Shang ${ }^{1 \dagger}$<br>${ }^{1}$ University of Electronic Science and Technology of China<br>${ }^{2}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>${ }^{3}$ XiaoMi AI Lab<br>{dengshihan7, xuwk266, jedi.shang}@gmail.com<br>{sunhongda98, ruiyan}@ruc.edu.cn</p>
<h4>Abstract</h4>
<p>With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction. However, there is a scarcity of benchmarks available for LLM-based mobile agents. Benchmarking these agents generally faces three main challenges: (1) The inefficiency of UI-only operations imposes limitations to task evaluation. (2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents. (3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents. First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion. Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs. To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios. Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform are available at https://github.com/XiaoMi/MobileBench.</p>
<h2>1 Introduction</h2>
<p>Interacting with mobile devices using natural language is a long-standing pursuit in humancomputer interaction (Bolt, 1980; Karat et al.,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: For the task of "Setting an alarm for seven thirty.", accomplishing it solely through UI operations requires four steps, while API calls can achieve the same task in just one step.</p>
<p>2002; Følstad and Brandtzæg, 2017). With the remarkable advancements in large language models (LLM) (Bai et al., 2022; Chowdhery et al., 2022; Du et al., 2021; Touvron et al., 2023; Ouyang et al., 2022), LLM-driven agents are at the forefront, yet their reasoning capability to navigate mobile application functionalities lags behind their proficiency with web pages on PCs (Yao et al., 2022; Sun et al., 2023). To faithfully replicate a typical mobile environment, it's imperative to incorporate a diverse set of applications and leverage authentic data, moving beyond the limitations of purely simulated scenarios. The development challenges in the mobile domain stem from a trio of core issues: a limited understanding of mobile interfaces, a scarcity of application variety, and a lack of real-world data.</p>
<p>Due to Google's breakthrough (Wang et al., 2023) in UI interface representation, LLM agent's understanding of UI pages becomes easier, leading to the creation of UI platforms such as AndroidEnv (Toyama et al., 2021) and Mobile-Env (Zhang et al., 2023), which tasks are defined within individual games or search engines. However, these works collectively face the following challenges: (1) UI actions depend on the textual descriptions of interfaces, where structured text fails to capture</p>
<table>
<thead>
<tr>
<th>Platform&amp;BenchMark</th>
<th>InfoUI</th>
<th>API&amp;UI</th>
<th>Real APP</th>
<th>Real Query</th>
<th>Multi-APP</th>
</tr>
</thead>
<tbody>
<tr>
<td>World of Bits <em>Shi et al. (2017)</em></td>
<td>✓</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>WebShop <em>Yao et al. (2022)</em></td>
<td>✓</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>AndroidEnv <em>Toyama et al. (2021)</em></td>
<td>✗</td>
<td>✗</td>
<td>✓</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>MobileEnv <em>Zhang et al. (2023)</em></td>
<td>✓</td>
<td>✗</td>
<td>✓</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>Mobile-Bench (Ours)</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
</tr>
</tbody>
</table>
<p>Table 1: Comparison of Mobile-Bench with existing LLM-based agent platforms. ‘InfoUI’ represents whether UI information is used for interaction with agents, ‘API&amp;UI’ represents whether the agent’s actions like API calls and UI interface operations, ‘Real APP’ represents whether real applications are used, ‘Real Query’ represents whether real user queries are used, and ‘Multi-APP’ represents whether there are tasks involving multiple applications.</p>
<p>the content of graphical buttons or images which can lead to wrong actions. A single API action might be equivalent to dozens of UI steps, leading to UI’s inefficiency. (2) Their tasks are far removed from real-world task scenarios encountered in daily use, which require cooperation between multiple applications, with user commands being ambiguous and not specifying target applications. (3) The evaluation of tasks should not solely rely on LLMs, without any objective quantitative metrics.</p>
<p>In fact, voice assistants on mobile phones can meet most of the users’ daily needs, yet they do not interact directly with UI interfaces but operate by invoking the APIs <em>Qin et al. (2023)</em> behind applications. As shown in Figure 1, in mobile applications, APIs are more efficient than UI interfaces; a single API call can be equivalent to multiple UI operations to achieve the same outcome. However, a single API is insufficient for more complex tasks, especially when user commands are unclear, necessitating reliance on LLMs to interpret user intent. Therefore, an agent capable of utilizing both UI and APIs would be best suited for the job. Simultaneously, It requires developing a strategy for the selection and order of the application usage, with human oversight merely focusing on reviewing the outcomes. This is a function that voice assistants currently lack <em>Wen et al. (2023a, b)</em>. To this end, we develop a combination of API and UI actions to circumvent the limitations of UI interfaces, each action can be chosen between UI interactions and API calls; all tasks begin from the mobile HOME page rather than from the launch page of a specific application, enabling the agent to determine single or multiple applications it will use; queries in the task are gathered from real users, and instruction generation is only applied to some complex ones which undergo rigorous manual review; we draw inspiration from objective metrics in software automation testing, named CheckPoint, and have made necessary adjustments to accommodate the unpredictable semantic outputs of LLMs. Above all, we propose a mobile phone environment that includes a platform supporting both API and UI interactions, and a corresponding dataset with multi-APP tasks. Table 1 presents a comparison among recent platforms and benchmark work based on API and UI.</p>
<p>Our contributions are summarized as follows:</p>
<p>(1) To the best of our knowledge, we are the first to establish a running platform for LLM-based mobile agents that simultaneously supports both UI and API calls.</p>
<p>(2) We propose an evaluation dataset containing diverse tasks for multi-APP interactions. Our tasks starting from the home page are more appropriate for testing the planning capabilities for agents. Our dataset and platform will be released soon.</p>
<p>(3) We introduce a new category-based evaluation metric to assess the task completion capabilities of the agent in the context of both UI and API interactions.</p>
<h2>2 Related Work</h2>
<h3>2.1 Mobile Platforms</h3>
<p>Prior to the emphasis on LLM-based agents, research efforts were directed towards RL-based agents, exemplified by the Android-Env platform <em>Toyama et al. (2021)</em>. This open-source platform tailored for reinforcement learning experiments within the Android ecosystem, successfully tested various RL-based agents like DDPG <em>Zhang and Van Huynh (2023)</em>, D4PG <em>Barth-Maron et al. (2018)</em>, MPO <em>Abdolmaleki et al. (2018)</em>, DQN <em>Mnih et al. (2015)</em>, IMPALA <em>Espeholt et al. (2018)</em> and R2D2 <em>Kapturowski et al. (2018)</em>.</p>
<p>More significant research has focused on LLMbased agents <em>Liu et al. (2024); Sun et al. (2024b, a)</em>. Regarding the domain of tool-using agents, they can be categorized into three main types:</p>
<p>1) For mobile tasks. Platforms like AutoDroid, DroidBot-GPT, GPT-Droid, and WebShop <em>Wen et al. (2023a, b); Liu et al. (2023b); Yao et al. (2022)</em> create an interactive environment enabling LLMs to engage with mobile tasks, and generate humanlike operations for automation test. Mobile-Env <em>Zhang et al. (2023)</em> is specifically designed to evaluate agents’ capabilities in handling multi-step interactions.</p>
<p>2) For PC Tasks. Researchers developed Toollama <em>Qin et al. (2023)</em> to evaluate the capabilities to use tools and API calls. AgentBench <em>Liu et al. (2023a)</em> presents a standardized Agent task evaluation architecture with strong decoupling and scalability. PPTC Benchmark <em>Guo et al. (2023)</em> proposed to evaluate the ability of LLM-based agents on PowerPoint tasks.</p>
<p>3) Other Methods. Toolformer <em>Schick et al. (2023)</em> and HuggingGPT <em>Shen et al. (2023)</em> evaluate LLM's capability to master tools.</p>
<h3>2.2 Benchmarks for LLM agents</h3>
<p>To assess agents’ proficiency in understanding user interfaces, a diverse dataset covering various tasks is crucial <em>Liu et al. (2023a)</em>. The widely used RICO dataset <em>Deka et al. (2017)</em> is commonly employed for this purpose, with Screen2Vec <em>Li et al. (2021)</em> utilizing it to evaluate agent performance.</p>
<p>However, due to the absence of specific standards for evaluating agent performance, efforts have focused on designing evaluation frameworks. PPTC Benchmark <em>Guo et al. (2023)</em> devised 279 multi-round dialogue tasks for PPT file operations. DroidTask <em>Wen et al. (2023a)</em> and various unnamed datasets <em>Liu et al. (2023b); Wen et al. (2023b)</em> covering various mobile applications have also been established. Additionally, Screen2Words used a sampling method to sample screens from the RICO-SCA <em>Li et al. (2020)</em> dataset and hired professional annotators to generate English summaries for these screens <em>Wang et al. (2021)</em>.</p>
<p>Current evaluation standards align with various works. ToolBench proposes Win Rate gauges the model’s solution quality against benchmarks like RoBERTa <em>Liu et al. (2019)</em>, GPT-3 <em>Brown et al. (2020)</em>, PaLM <em>Chowdhery et al. (2023)</em>, OPT <em>Zhang et al. (2022)</em>, ChatGPT <em>Bubeck et al. (2023)</em> and GPT-4 <em>OpenAI (2023)</em>. Although Fan <em>Fan et al. (2024)</em> found that the cost of inference can be reduced by using only the necessary layers for inference, it is still expensive to calculate the win rate. Mobile-Env <em>Zhang et al. (2023)</em> evaluates agent performance based on the completion status, average steps, and average rewards in WikiHow tasks. PPTC Benchmark <em>Guo et al. (2023)</em> uses Turn-based and Session-based accuracy. Android in the Wild <em>Rawles et al. (2023)</em> makes use of Out-of-distribution Generalization. Overall, metrics such as success rate, episode length, and match score are currently the most commonly employed.</p>
<h2>3 Our Environment</h2>
<h3>3.1 Mobile-Bench Benchmark</h3>
<p>Data collection. The queries in the dataset are divided into the following three categories:</p>
<ul>
<li>SAST: Single-App-Single-Task. A real dataset containing only one task text, including single-task operations such as opening and closing APP, such as "Help me open the map".</li>
<li>SAMT: Single-App-Multi-Task. A real dataset containing multiple task texts, as well as constructed single-APP data. A complex multi-task on single APP, such as "Help me open the map, and navigate to Eiffel Tower.".</li>
<li>MAMT: Multi-App-Multi-Task. Constructed multi-APP data, complete a complex multitask, such as "Help me search for the latest technology news and share it with friends."</li>
</ul>
<p>SAST is directly derived from real voice requests processed by the voice assistants loaded on the mobile phone. We select a subset of this query collection, primarily filtering out the portion that requires voice assistant processing and involves multimodal tools. Additionally, querys that exceed permissions or involve privacy are also filtered out.</p>
<p>Since there are fewer SAMT and MAMT data in real data and the quality is not high, refer to Toollama <em>Qin et al. (2023)</em> method, we use GPT-4 to construct SAMT and MAMT data. For MAMT, we randomly sample 6 applications from the entire application collection, and then provide some examples of real multi-APP data to prompt GPT-4 to select 2-4 applications to generate tasks. By integrating real and constructed data, we create the final dataset. An example of data is shown in Figure 2.</p>
<table>
<thead>
<tr>
<th>ID: 3</th>
</tr>
</thead>
<tbody>
<tr>
<td>Query:</td>
</tr>
<tr>
<td>I want to book a flight from Beijing to Shanghai next</td>
</tr>
<tr>
<td>Friday. Are there any recommended flights?</td>
</tr>
<tr>
<td>APP:</td>
</tr>
<tr>
<td>Amap &amp; ( Ctrip Travel $\mid$ Qunar )</td>
</tr>
<tr>
<td>CheckPoints:</td>
</tr>
<tr>
<td>Package:</td>
</tr>
<tr>
<td>com.autonavi.minimap \&amp; (ctrip.android.view $\mid$ com.Qunar)</td>
</tr>
<tr>
<td>Key phrase:</td>
</tr>
<tr>
<td>[flight, Beijing, Shanghai, next Friday]</td>
</tr>
<tr>
<td>API:</td>
</tr>
<tr>
<td>- adb shell am start -n</td>
</tr>
<tr>
<td>com.autonavi.minimap/com.autonavi.</td>
</tr>
<tr>
<td>map.activity.SplashActivity</td>
</tr>
<tr>
<td>- adb shell am start -a androidintent.action.VIEW -</td>
</tr>
<tr>
<td>damapuri://route/plan/3dname=Shanghai</td>
</tr>
<tr>
<td>com.autonavi.mini-map</td>
</tr>
</tbody>
</table>
<p>Figure 2: A test case in MAMT. \&amp; stands for conjunction check, CC; $\mid$ stands for disjunction check, DC; [] stands for sequential check, SC. The package CheckPoint passes when the action history includes either Amap and Ctrip Travel, or Amap and Qunar. Key phrase CheckPoint comes from the orange parts in the case.</p>
<p>APP \&amp; API collection. To ensure task comprehensiveness, we select not only the applications included in SAST and SAMT but also the most popular free applications from each category in the APP Store. Obtaining the API is to analyze the package of each application to obtain its external reserved interface (Desnos and Gueguen, 2011). The advantage of this is that the obtained API is naturally classified for the application. Since the description of the API in the decompilation result is not as detailed as the development document, we use the ADB(Android Debug Bridge) command to verify the feasibility of the API one by one. Owing to its debugging properties, system-level APIs can also be invoked normally, allowing access to functions such as checking the battery status and performing memory cleaning. For more specific application names and categories, please refer to Appendix B. 3</p>
<p>Dataset statistics. Including several default applications within the system, we collected a total of 29 applications. For applications, we collected a total of 103 usable APIs, which primarily serve the following functions: system calls, opening pages, closing pages, searching for information, viewing details, and controlling device switches. These functions are summarized into the following main aspects: page switch, details view, broadcast, search. In Table 2, we have tabulated the number of APIs and the functional categories covered by</p>
<table>
<thead>
<tr>
<th style="text-align: left;">APP Category</th>
<th style="text-align: center;">API Quantity</th>
<th style="text-align: center;">APP Number</th>
<th style="text-align: center;">API Functions</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Travel Transportation</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">(1), (2), (3)</td>
</tr>
<tr>
<td style="text-align: left;">Audiovisual Vision</td>
<td style="text-align: center;">15</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">(1), (2), (3)</td>
</tr>
<tr>
<td style="text-align: left;">Social Communication</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">(1), (2), (3)</td>
</tr>
<tr>
<td style="text-align: left;">Fashion Shopping</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">(1), (3)</td>
</tr>
<tr>
<td style="text-align: left;">Information News</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">(1), (2), (3)</td>
</tr>
<tr>
<td style="text-align: left;">Practical Tool</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">(1), (2), (3), (4), (5)</td>
</tr>
<tr>
<td style="text-align: left;">Home Life</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">(1), (5)</td>
</tr>
<tr>
<td style="text-align: left;">Book Reading</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">(1), (2), (3)</td>
</tr>
<tr>
<td style="text-align: left;">Universal Buttons</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">(5)</td>
</tr>
</tbody>
</table>
<p>Table 2: Our dataset covers nine major categories of applications, and we compared them based on the API function. The above API functions can be summarized into five categories: (1) Page Navigation, (2) Viewing Details, (3) Playback, (4) Searching, and (5) System Calls.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: (a) The API\&amp;UI, UI task ratio. In SAST and SAMT, API\&amp;UI task ratio is $85 \%$, in MAMT, it is $100 \%$. (b) The number of CheckPoints.</p>
<p>APIs, categorized by the type of APP. We organized the available APIs and APP descriptions for each APP, and generated an APP list as the basis for selecting applications, shown in Appendix B.3.</p>
<p>In the Mobile-Bench dataset, we collected a total of 332, 300, 200 queries for SAST, SAMT, and MAMT. We sort out the APIs actually used by each task in real voice requests. Provide these API as an example to GPT-4 for query generation. As shown in Figure 3(a), we calculated the ratio of tasks calling APIs, ensuring a sufficient number of tasks in the dataset that include steps to call API. This approach ensures that we have sufficient data to analyze the role of APIs in task completion.</p>
<p>Quality verification. (Bolotova-Baranova et al., 2023) The initial test data originates from software automation tests, but some complex data points are generated by GPT-4. To ensure the quality of our dataset, we randomly sampled 100 data points from each of the SAST, SAMT, and MAMT, resulting in a total of 300 quality test data. We conducted crosssource validation to verify the feasibility of these CheckPoints. The specific formula for calculation</p>
<p>is as follows:</p>
<p>$\operatorname{Overlap}\left(C P_{1}, C P_{2}\right)=\frac{\left|C P_{1} \cap C P_{2}\right|}{\left|C P_{1}\right|}$ (1)</p>
<p>$C P_{1}, C P_{2}$ representing the CheckPoint sequences generated by $C P_{\text {instruction }}$ and $C P_{\text {Human }}$, respectively. In Table 3, we list the human evaluation results for three types of data. From the table, it can be observed that a higher proportion of terminal data corresponds to better data quality. However, all MAMT data is generated by instructions, its quality does not exhibit an unacceptable gap compared to SAST. See appendix B.1 for more analysis.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Statistics</th>
<th style="text-align: center;">SAST</th>
<th style="text-align: center;">SAMT</th>
<th style="text-align: center;">MAMT</th>
<th style="text-align: center;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\mathrm{CP}_{\text {instruction }}$</td>
<td style="text-align: center;">395</td>
<td style="text-align: center;">546</td>
<td style="text-align: center;">513</td>
<td style="text-align: center;">1454</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{CP}_{\text {Human }}$</td>
<td style="text-align: center;">412</td>
<td style="text-align: center;">598</td>
<td style="text-align: center;">623</td>
<td style="text-align: center;">1633</td>
</tr>
<tr>
<td style="text-align: left;">$\mathrm{CP}_{\text {instruction }} \cap$ Human</td>
<td style="text-align: center;">372</td>
<td style="text-align: center;">466</td>
<td style="text-align: center;">412</td>
<td style="text-align: center;">1250</td>
</tr>
<tr>
<td style="text-align: left;">Overlap</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">0.86</td>
</tr>
</tbody>
</table>
<p>Table 3: Human Evaluation Results</p>
<h3>3.2 Test Platform</h3>
<p>Overview Mobile-Bench is designed as a universal interaction platform that supports hybrid API and UI interactions. Users are able to construct their own evaluation data following a fixed format, yet they must adhere to our prescribed evaluation method. As shown in Figure 4 users can interact with the environment using the following commands.</p>
<ul>
<li>Start: Open the test environment and load the preset snapshot using this command. Each test case must start from the same environment.</li>
<li>Stop: Stop the test environment and end test.</li>
<li>Close: Close the test environment and save the test process and results.</li>
<li>Check: Capture a screenshot snapshot of the current test environment.</li>
<li>ReSet: Load a previously saved environment snapshot into the test environment.</li>
</ul>
<p>Observation space To enable the agent to read information on the android emulator in a humanlike manner, we use Appium to obtain page information. Following the method described by Wang (Wang et al., 2023), we convert XML to HTML, as the training data for LLMs is predominantly sourced from the Internet, which includes numerous HTML files. Therefore, we believe that LLM has a better understanding of HTML than XML.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Test Platform Overview. The test platform is linked by the user, the simulator, and the Agent. After the user's instructions are issued, the entire test execution process is completed by the Agent, which can view and manage the test tasks through the preset interface in the cloud.</p>
<p>Given the tree structure of XML, we initially convert the XML into a tree format and subsequently transform the nodes that need to be displayed to the agent into HTML. The agent simulates human interaction with smartphones, performing three major operations: click, input, and scroll. Humans visually identify which elements can be clicked or receive input, and use their fingers to determine if they can scroll the screen. Therefore, we provide the agent with elements that are visible and scrollable. Due to the limit on context length, we only convert the information required by the agent in XML to HTML:</p>
<ul>
<li>Type: HTML element categories inherited directly from XML formatted information.</li>
<li>ID: "ID" inherits from the XML "resource-id" attribute, uniquely identifying the existence of an element.</li>
<li>Package: the package name of the current application.</li>
<li>Class: the class of the element, such as ImageView, TextView.</li>
<li>Description \&amp; text: describe the function and shape of the element.</li>
<li>Clickable \&amp; Scrollable: whether the element is clickable and scrollable.</li>
<li>Bounds: if the element is scrollable, this attribute will be present and scope the scroll component, such as:</li>
</ul>
<p>$$
\left[x_{i}, y_{i}\right]\left[x_{j}, y_{j}\right]
$$</p>
<p>The scrollable rectangle ranges from $\left[x_{i}, y_{i}\right]$</p>
<p>to $\left[x_{j}, y_{j}\right]$.</p>
<p>And, there is an example of HTML elements: <button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true"> message </button></p>
<p>Action space Our Mobile-Bench imitates human behavior in using mobile and summarizes three actions (Zhang et al., 2023) and imitates the process of calling the API on the test platform (Sengupta et al., 2023):</p>
<ul>
<li>Click: simulate real user click actions by passing in specific elements.</li>
<li>Scroll: simulate real user scrolling actions by tapping - dragging - releasing.</li>
<li>Input: simulate real user input actions by clicking-typing.</li>
<li>API Call: launch an activity or send an intent by invoking an API through ADB commands.</li>
</ul>
<h3>3.3 Evaluation Method</h3>
<p>CheckPoint. Automated test CheckPoint coverage (Bajunaid and Menascé, 2018) is a test metric for the software execution process. It cannot assist in checking the software results, but it can visually inspect whether the software runs in the specified unit sequence. During data construction, we supply APPs and APIs, which naturally serve as detection indicators. Additionally, we incorporated a CheckPoint to verify if the UI operation correctly clicks on the intended element. After sorting out the above CheckPoints, we constructed the following three CheckPoints:</p>
<ul>
<li>Package: the unique package name corresponding to the application. Checking the package can determine whether the correct application is used.</li>
<li>Key phrase: the key phrase extracted from the query, represents key steps in the UI execution process.</li>
<li>API: API commands that need to be called during the execution process.</li>
</ul>
<p>To evaluate the agent's selection and execution capabilities, we divide the inspection granularity into two levels: CheckPoint ${ }<em 2="2" I="I">{I I}$ - whether it uses the correct application, and CheckPoint ${ }</em>}$ - whether it follows the predefined paths to complete the task. For CheckPoint ${ <em 2="2" I="I">{I I}$, we check the number of correctly called packages. For CheckPoint ${ }</em>$, we check the number of correctly called package, key phrase,</p>
<p>API. For CheckPoints, we identify three logical relationships: sequential, conjunctive, and disjunctive checks. These correspond to the instability of LLM output and its tendency for synonym substitution. The calculation formula for "sequential check" is as follows:</p>
<p>$$
\operatorname{Score}<em _Str="{Str" _text="\text">{\text {Sequen }}=\frac{\left|\sum</em>
$$} \in S C \cap A H} \text { Str }\right|}{\left|\sum_{\text {Str } \in S C} \text { Str }\right|</p>
<p>SC represent Sequential Check Set and AH represent Actions History. The calculation formulas for conjunctive checks is as follows:</p>
<p>$$
\operatorname{Score}_{\text {conjun }}=\left{\begin{array}{ll}
1, &amp; \text { if } \forall \text { str } \in C C, \text { str } \in A H \
0, &amp; \text { otherwise }
\end{array}\right.
$$</p>
<p>CC represent Conjunctive Check Set. The calculation formulas for disjunctive checks is as follows:</p>
<p>$$
\operatorname{Score}_{\text {disjun }}=\left{\begin{array}{ll}
1, &amp; \text { if } \exists \text { str } \in D C, \text { str } \in A H \
0, &amp; \text { otherwise }
\end{array}\right.
$$</p>
<p>DC represent Disjunctive Check Set. The weighted sum of the above three scores will be the final CheckPoint coverage rate.</p>
<p>As shown in Figure 3, the number of key phrase CheckPoints is significantly higher than that of packages, indicating the need for more semantic information to ensure tasks are completed step-bystep. Analyzing the dataset from a proportional perspective, we find that the distributions of the three types of CheckPoints are 0.212, 0.493, 0.294, with key phrase CheckPoints remaining the most predominant method of checking.</p>
<p>In general, a test case should include at least the following contents: ID, Query, APP List, CheckPoints(Package, Key phrase, API). Figure 2 is a test case that contains the above three CheckPoints.</p>
<p>PassRate. (Qin et al., 2023) We assess an agent's human-computer interaction capabilities by calculating the proportion of queries successfully completed within the specified step limits. During this process, we organized the emulator's current state. Subsequently, GPT-4 evaluates the task completion status. We computed the percentage of pass tasks, yielding a PassRate as an indicator of agent's human-computer interaction capabilities.</p>
<p>Average steps. (Zhang et al., 2023) We quantified the step size required by Mobile-Bench to complete tasks as a metric for evaluating the efficiency of the agent. In Mobile-Bench, a 'step' is</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Baseline Model Overview. The entire process framework consists of sensors, reflection components, controllers, execution components, and environments. Once a task starts, these components will run iteratively until the task is completed or the maximum number of steps is reached.</p>
<p>defined as the completion of a UI operation or the execution of an API call.</p>
<h2>4 Experiment</h2>
<h3>4.1 Baseline Model</h3>
<p>Our model's architecture, illustrated in Algorithm 1, begins by obtaining the smartphone's UI information in XML format through Appium and transforms it into HTML format through a heuristic algorithm. Subsequently, as illustrated in Figure 5 leveraging the HTML, task details, and APP list, LLM generates a comprehensive task plan, outlining the necessary applications and corresponding sub-tasks. As the collection of APIs is organized based on the classification of APPs, we can get the API set that may be used in plan.</p>
<p>The task plan is executed iteratively. In each iteration, the model either performs an API call or a UI operation. After each execution, the model records the success or failure of the action in its history, generates the subsequent thought, and evaluates whether the task has been completed. For the actual running process of an algorithm, please refer to the appendix C.7.</p>
<h3>4.2 Setup</h3>
<p>We evaluate four popular LLMs on the proposed Mobile-Bench task set: GPT-3.5-turbo (Ouyang et al., 2022), GPT-4 (Nori et al., 2023), LLaMA-13B and LLaMA-70B (Touvron et al., 2023), while ChatGPT-3.5 and GPT-4 are accessed through the online APIs of OpenAI. The experiments are conducted with a 3-shot in-context learning under sampling temperature of 0.1. Recognizing that task execution incurs costs, we preset different maximum step limits for tasks based on their difficulty levels. For the three categories of SAST, SAMT, and MAMT, we set the max step to 10, 20, and 50 respectively. Owing to the limit of budget, only GPT-3.5 utilizes an interface with a context length of 16K. GPT-4 uses a standard interface, which necessitated compression and trimming of actions.</p>
<table>
<thead>
<tr>
<th>Algorithm 1 Baseline Model</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Input: description of the Task, Task; APP list, L_{APP}; API list, L_{API}; max loop step, M_{mp}; initial thought, Tho;</td>
<td></td>
</tr>
<tr>
<td>Output: actions history, AH; total steps, Step; finish flag, Finish;</td>
<td></td>
</tr>
<tr>
<td>1: Html ← Appium(Emulator)</td>
<td></td>
</tr>
<tr>
<td>2: Plan ← LLM(Task, L_{APP})</td>
<td></td>
</tr>
<tr>
<td>3: Step = 0, Finish = False</td>
<td></td>
</tr>
<tr>
<td>4: AH = {}</td>
<td></td>
</tr>
<tr>
<td>5: while (Step ≤ M_{step}) and (Finish ≠ True) do</td>
<td></td>
</tr>
<tr>
<td>6: Step 4 +;</td>
<td></td>
</tr>
<tr>
<td>7: Html ← Appium(Emulator);</td>
<td></td>
</tr>
<tr>
<td>8: API ← LLM(Task, L_{API}, AH, Tho, Plan, Html)</td>
<td></td>
</tr>
<tr>
<td>9: if API then</td>
<td></td>
</tr>
<tr>
<td>10: Action(API)</td>
<td></td>
</tr>
<tr>
<td>11: AH.append(API)</td>
<td></td>
</tr>
<tr>
<td>12: else</td>
<td></td>
</tr>
<tr>
<td>13: UI ← LLM(Task, AH, Tho, Plan, Html)</td>
<td></td>
</tr>
<tr>
<td>14: Action(UI)</td>
<td></td>
</tr>
<tr>
<td>15: AH.append(UI)</td>
<td></td>
</tr>
<tr>
<td>16: end if</td>
<td></td>
</tr>
<tr>
<td>17: Html ← Appium(Emulator);</td>
<td></td>
</tr>
<tr>
<td>18: Tho ← LLM(Task, AH, Plan, Html)</td>
<td></td>
</tr>
<tr>
<td>19: Finish ← LLM(Task, AH, Tho, Html)</td>
<td></td>
</tr>
<tr>
<td>20: end while</td>
<td></td>
</tr>
</tbody>
</table>
<p>| Metric | LLaMA-13B | | | LLaMA-70B | | | GPT-3.5-turbo | | | GPT-4 | | |
| | SAST | SAMT | MAMT | SAST | SAMT | MAMT | SAST | SAMT | MAMT | SAST | SAMT | MAMT |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Average #Steps | 7.43 | 18.76 | 49.52 | 5.97 | 16.63 | 48.91 | 4.53 | 12.06 | 48.73 | 3.79 | 13.94 | 44.86 |
| PassRate | 44.58 | 27.67 | 8 | 56.62 | 54 | 13.5 | 64.94 | 64 | 15.5 | 80.96 | 63 | 26.5 |
| CheckPoint $<em 1="1">{0}$ | 46.08 | 43.67 | 28.74 | 56.62 | 61 | 39.98 | 66.75 | 67 | 43.16 | 81.57 | 72.66 | 61.34 |
| CheckPoint $</em>$ | 34.85 | 29.13 | 21.39 | 63.12 | 62.73 | 41.21 | 76.21 | 71.29 | 44.09 | 83.76 | 77.35 | 52.98 |</p>
<p>Table 4: Results of the agents based on different LLMs on Mobile-Bench dataset. On MAMT data, due to context length limitations, a compression is applied to the actions history by retaining only the most recent 20 entries.
history. See Appendix A for other settings.</p>
<h3>4.3 Results</h3>
<p>As observed in Table 4, it can be observed that GPT-3.5 outperforms GPT-4 in PassRate on SAMT( $64 \%&gt;63 \%$ ), and it requires fewer steps to complete the task(12.06&lt;13.94). To investigate this phenomenon, we analyze the output files and find that models with poorer performance exhibit PassRate misjudgments: they prematurely terminate even when the task is not completed. This phenomenon is also present in LLaMA, which exhibits a high PassRate ( $44.58 \%$ ) but low CheckPoint coverage ( $34.85 \%$ ). At the same time, we delved into why the results for MAMT are so low( $15.5 \%$, $26.5 \%$ ). Our analysis revealed that LLMs often exhibit greedy exploration behavior when completing tasks, meaning they struggle to determine when to exit the current application and transition to the next one. This tendency is particularly prevalent in certain generation tasks. Moreover, as the actions history increases, its ability to accurately judge task progress becomes increasingly challenging. For more detailed result, please refer to Table 7.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Settings</th>
<th style="text-align: center;">Average #Steps</th>
<th style="text-align: center;">CheckPoint $_{12}$</th>
<th style="text-align: center;">PassRate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SAST (GPT-4)</td>
<td style="text-align: center;">$\mathbf{3 . 7 9}$</td>
<td style="text-align: center;">$\mathbf{8 3 . 7 6}$</td>
<td style="text-align: center;">$\mathbf{8 0 . 9 6}$</td>
</tr>
<tr>
<td style="text-align: left;">SAMT (GPT-4)</td>
<td style="text-align: center;">13.94</td>
<td style="text-align: center;">77.35</td>
<td style="text-align: center;">63</td>
</tr>
<tr>
<td style="text-align: left;">MAMT (GPT-4)</td>
<td style="text-align: center;">44.86</td>
<td style="text-align: center;">52.98</td>
<td style="text-align: center;">26.5</td>
</tr>
<tr>
<td style="text-align: left;">SAST (w/o API)</td>
<td style="text-align: center;">6.13</td>
<td style="text-align: center;">72.73</td>
<td style="text-align: center;">74.39</td>
</tr>
<tr>
<td style="text-align: left;">SAMT (w/o API)</td>
<td style="text-align: center;">16.86</td>
<td style="text-align: center;">56.74</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: left;">MAMT (w/o API)</td>
<td style="text-align: center;">49.17</td>
<td style="text-align: center;">31.69</td>
<td style="text-align: center;">9.5</td>
</tr>
</tbody>
</table>
<p>Table 5: API Ablation Study based on GPT-4.</p>
<h3>4.4 Impact of API Calls</h3>
<p>API Calls can accelerate task execution, as a single call often replaces several sequential UI steps. From another perspective, the ability of the agent to select appropriate APIs and input parameters warrants further investigation. Choosing the wrong API may lead the task in an incorrect direction or require a significant number of steps to rectify.</p>
<p>Therefore, in Table 5, we evaluate and analyze the impact of introducing APIs on task completion based on GPT-4.</p>
<p>From Table 5, it can be seen that even in SAST, the PassRate has decreased by $6.57 \%$ (from 80.96 to 74.39). Furthermore, the values for CheckPoint $_{12}$ exhibit a more pronounced decrease after API removal, with a drop exceeding $20 \%$ in SAMT. Simultaneously, we have observed varying increases in the average number of steps, which align with our expectations. We analyzed the results and found that the inability to accurately scroll pages, inefficient exploration of page functionality, and failure to click graphical buttons are the primary reasons for the low efficiency of UI operations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Settings</th>
<th style="text-align: center;">Average #Steps</th>
<th style="text-align: center;">CheckPoint $_{12}$</th>
<th style="text-align: center;">CheckPoint $_{12}$</th>
<th style="text-align: center;">PassRate</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SAST (GPT-4)</td>
<td style="text-align: center;">$\mathbf{3 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{8 2}$</td>
<td style="text-align: center;">$\mathbf{7 9 . 7 4}$</td>
<td style="text-align: center;">$\mathbf{7 6}$</td>
</tr>
<tr>
<td style="text-align: left;">SAST (w/o thought)</td>
<td style="text-align: center;">8.86</td>
<td style="text-align: center;">$\mathbf{8 2}$</td>
<td style="text-align: center;">29.16</td>
<td style="text-align: center;">24</td>
</tr>
<tr>
<td style="text-align: left;">SAST (w/o plan)</td>
<td style="text-align: center;">3.98</td>
<td style="text-align: center;">76</td>
<td style="text-align: center;">74.54</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: left;">SAMT (GPT-4)</td>
<td style="text-align: center;">$\mathbf{1 3 . 9 4}$</td>
<td style="text-align: center;">$\mathbf{6 3}$</td>
<td style="text-align: center;">$\mathbf{7 2 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{7 7}$</td>
</tr>
<tr>
<td style="text-align: left;">SAMT (w/o thought)</td>
<td style="text-align: center;">19.54</td>
<td style="text-align: center;">$\mathbf{6 3}$</td>
<td style="text-align: center;">18.31</td>
<td style="text-align: center;">20</td>
</tr>
<tr>
<td style="text-align: left;">SAMT (w/o plan)</td>
<td style="text-align: center;">17.09</td>
<td style="text-align: center;">52</td>
<td style="text-align: center;">58.02</td>
<td style="text-align: center;">62</td>
</tr>
</tbody>
</table>
<p>Table 6: Thought and Plan Ablation Study on SAST (subset 50) and SAMT (subset 200) based on GPT-4.</p>
<h3>4.5 Impact of Plan and Thought</h3>
<p>Since observation-thought-action is already a standardized process in the agent direction(Qin et al., 2023), and verified by experimental results, planning and thought before action are essential. From the experimental results, we can find that without the observation-thought step, the agent is almost unable to complete the task( $77-&gt;20,76-&gt;24$ ), which is because it cannot determine the next action category and the current task status. In more complex tasks SAMT, losing the plan has more negative consequences( $77-&gt;62$ ). But they will have almost no impact on CheckPoint $_{11}(82-&gt;8263-&gt;63)$, because the application selection is almost done by the API Call.</p>
<h2>5 Conclusion</h2>
<p>In this work we have proposed an agent capability testing environment that supports API and UI interaction on mobile phone. This holds significant importance for exploring how LLMs can be integrated with mobile operating systems. Additionally, it can serve as a valuable reference for developing testing platforms for operating systems to evaluate the capabilities of LLM agents. We collected and released a test dataset containing tasks for multiple APPs, ensuring its quality through human verification. Based on this data set and environment, we tested the planning, decision-making and execution of various LLM-based agents. Please refer to the Section 6 for the limitations of our benchmark.</p>
<h2>6 Limitations</h2>
<p>While general large models exhibit strong capabilities in reasoning and planning, they tend to have pronounced illusions in API calls. As a result, the language model may become confused about the application's functionality, leading to a reluctance to continue and complete the task. Therefore, finetuning a model for instructions is highly necessary.</p>
<p>Automatic CheckPoint is a process evaluation metric, making it challenging to assess the quality of the final outcome. This depends on whether the agent has obtained the necessary information (actions) on the required pages.</p>
<p>The enhancement of the agent's capabilities relies on extensive API and SDK libraries, requiring substantial support from application development companies.</p>
<h2>7 Ethics Statement</h2>
<p>We have rigorously refined our dataset to remove any elements that could compromise personal privacy, thereby guaranteeing the highest level of protection for individual data. The evaluation of our work was carried out through a meticulously randomized selection of IT professionals. This process ensured a gender-balanced and educationally diverse panel, reflecting a wide spectrum of perspectives and expertise.</p>
<h2>8 Acknowledgements</h2>
<p>We thank the Xiaoai Voice Department of Xiaomi Technology Corporation for their raw data support for this project. We additionally thank our crowd annotators for their diligent work, Junfeng Peng
and Yifan Cheng for contributing to the human performance estimates, and the anonymous reviewers for their constructive comments. This work was supported by the NSFC (U2001212, 62032001, and 61932004).</p>
<h2>References</h2>
<p>Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. 2018. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920.</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback (arxiv: 2212.08073). arxiv.</p>
<p>Noor Bajunaid and Daniel A Menascé. 2018. Efficient modeling and optimizing of checkpointing in concurrent component-based software systems. Journal of Systems and Software, 139:1-13.</p>
<p>Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. 2018. Distributed distributional deterministic policy gradients. arXiv preprint arXiv:1804.08617.</p>
<p>Valeriia Bolotova-Baranova, Vladislav Blinov, Sofya Filippova, Falk Scholer, and Mark Sanderson. 2023. Wikihowqa: A comprehensive benchmark for multidocument non-factoid question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5291-5314.</p>
<p>Richard A Bolt. 1980. "put-that-there" voice and gesture at the graphics interface. In Proceedings of the 7th annual conference on Computer graphics and interactive techniques, pages 262-270.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901.</p>
<p>Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113.</p>
<p>Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A mobile app dataset for building data-driven design applications. In Proceedings of the 30th annual ACM symposium on user interface software and technology, pages $845-854$.</p>
<p>Anthony Desnos and Geoffroy Gueguen. 2011. Android: From reversing to decompilation. Proc. of Black Hat Abu Dhabi, 1:1-24.</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360.</p>
<p>Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. 2018. Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures. In International conference on machine learning, pages 1407-1416. PMLR.</p>
<p>Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and Zhongyuan Wang. 2024. Not all layers of llms are necessary during inference. arXiv preprint arXiv:2403.02181.</p>
<p>Asbjørn Følstad and Petter Bae Brandtzeg. 2017. Chatbots and the new world of hci. interactions, 24(4):3842 .</p>
<p>Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, and Duan Nan. 2023. Pptc benchmark: Evaluating large language models for powerpoint task completion. arXiv preprint arXiv:2311.01767.</p>
<p>Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. 2018. Recurrent experience replay in distributed reinforcement learning. In International conference on learning representations.</p>
<p>Clare-Marie Karat, John Vergo, and David Nahamoo. 2002. Conversational interface technologies. The human-computer interaction handbook, pages 169186 .</p>
<p>Toby Jia-Jun Li, Lindsay Popowski, Tom Mitchell, and Brad A Myers. 2021. Screen2vec: Semantic embedding of gui screens and gui components. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1-15.</p>
<p>Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. 2020. Mapping natural language instructions to mobile ui action sequences. arXiv preprint arXiv:2005.03776.</p>
<p>Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023a. Agentbench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Yuhan Liu, Xiuying Chen, Xiaoqing Zhang, Xing Gao, Ji Zhang, and Rui Yan. 2024. From skepticism to acceptance: Simulating the attitude dynamics toward fake news. arXiv preprint arXiv:2403.09498.</p>
<p>Zhe Liu, Chunyang Chen, Junjie Wang, Mengzhuo Chen, Boyu Wu, Xing Che, Dandan Wang, and Qing Wang. 2023b. Chatting with gpt-3 for zeroshot human-like mobile automated gui testing. arXiv preprint arXiv:2305.09434.</p>
<p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. 2015. Human-level control through deep reinforcement learning. nature, 518(7540):529-533.</p>
<p>Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789.</p>
<p>Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2023. Android in the wild: A large-scale dataset for android device control. arXiv preprint arXiv:2307.10088.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.</p>
<p>Aritro Sengupta, Amit Singh, and BM Vinjit. 2023. A platform independent and forensically sound method to extract whatsapp data from mobile phones. International Journal of Electronic Security and Digital Forensics, 15(3):259-280.</p>
<p>Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.</p>
<p>Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. 2017. World of bits: An open-domain platform for web-based agents. In International Conference on Machine Learning, pages 3135-3144. PMLR.</p>
<p>Hongda Sun, Hongzhan Lin, Haiyu Yan, Chen Zhu, Yang Song, Xin Gao, Shuo Shang, and Rui Yan. 2024a. Facilitating multi-role and multi-behavior collaboration of large language models for online job seeking and recruiting. arXiv preprint arXiv:2405.18113.</p>
<p>Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, and Rui Yan. 2024b. Harnessing multi-role capabilities of large language models for open-domain question answering. In Proceedings of the ACM on Web Conference 2024, pages 4372-4382.</p>
<p>Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, and Rui Yan. 2023. Determlr: Augmenting llm-based logical reasoning from indeterminacy to determinacy. arXiv preprint arXiv:2310.18659.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler Jackson, Shibl Mourad, and Doina Precup. 2021. Androidenv: A reinforcement learning platform for android. arXiv preprint arXiv:2105.13231.</p>
<p>Bryan Wang, Gang Li, and Yang Li. 2023. Enabling conversational interaction with mobile ui using large language models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, pages $1-17$.</p>
<p>Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. 2021. Screen2words: Automatic mobile ui summarization with multimodal learning. In The 34th Annual ACM Symposium on User Interface Software and Technology, pages 498510 .</p>
<p>Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2023a. Empowering
llm to use smartphone for intelligent task automation. arXiv preprint arXiv:2308.15272.</p>
<p>Hao Wen, Hongming Wang, Jiaxuan Liu, and Yuanchun Li. 2023b. Droidbot-gpt: Gpt-powered ui automation for android. arXiv preprint arXiv:2304.07061.</p>
<p>Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable realworld web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744-20757.</p>
<p>Bolun Zhang and Nguyen Van Huynh. 2023. Deep deterministic policy gradient for end-to-end communication systems without prior channel knowledge. arXiv preprint arXiv:2305.07448.</p>
<p>Danyang Zhang, Lu Chen, and Kai Yu. 2023. Mobileenv: A universal platform for training and evaluation of mobile interaction. arXiv preprint arXiv:2305.08144.</p>
<p>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.</p>
<h1>A Settings</h1>
<p>We conduct experiments on the Android 14.0 version emulator and use Appium UiAutomator2 Driver for automated testing. Before each execution of a task, we load a snapshot to ensure the emulator in the same environment every time. For all applications, we have logged in to the account in advance to ensure that the full function of the application can be used. Since we tests in the real world, we filtered out any tasks that included payments.</p>
<h2>B Details of Dataset</h2>
<h2>B. 1 Dataset quality analysis</h2>
<p>The root cause of low-quality data often lies in the inaccuracies in the descriptions of applications. Additionally, ambiguity in query generation also plays a significant role. For example, in the query "Help me find pictures related to Beijing", although the user has not explicitly specified the source application, for a human, the expected result would likely be a search engine or a map application, as the images are not likely to be from the user themselves. However, for LLM, because the statement includes the word "pictures", it might be reasonable for it to spend all its time searching for pictures in the gallery application, even though this effort would ultimately be in vain. CheckPoint coverage is calculated as the weighted sum of the scores for the three types of CheckPoints mentioned above.</p>
<h2>B. 2 Prompts for Instruction Generation</h2>
<p>Below we list the detailed prompt for instruction generation, including single-APP-multi-task description, multi-APP-multi-task description.</p>
<h2>single-APP-multi-task description:</h2>
<p>You will be provided with an application with descriptions, an available API list including adb command, function description and parameter information. You should create 5 varied, innovative, and detailed multi task queries that employ this application as a tool, API can be used as an auxiliary.</p>
<p>Each query should include the necessary parameters. Note that you shouldn't ask 'which APP to use', rather, simply state your needs that can be addressed by these APPs. You should also avoid asking for the input parameters required by the APP call, but instead directly provide the parameter in your query. Those related APP and APIs have to strictly come from the provided lists.</p>
<p>At the same time, you also need to provide the CheckPoint of this query, including package, key phrase and API. The package comes from the package corresponding to the APP to be used. Key phrase is the key click element or key input character that the Android emulator will perform when executing this query, which is used to check whether the query has been completed. Key phrase should be noun and part of query, should be kept as short as possible.</p>
<p>Key phrase can contain multiple pieces of information, "|" means the query passes when any of the following texts are completed. "|" is used to separate synonymous expressions of the same noun; "\&amp;" indicates that the query must be passed when all texts are completed; sequential CheckPoints are stored in "[]", and the count increases by one for each passed element. The "ADB Command" to be used is stored in the API, which may also be empty.</p>
<p>Deliver your response in this format:
[{
"id": "number"
"query": "text"
"APP": "APP name"
"CheckPoint": {
"package": "APP package name"
"key phrase": ["text1", ... ]
"API: ["API1", ... ]"
}
}</p>
<h1>multi-APP-multi-task description:</h1>
<p>You will be provided with some APPs with descriptions, available API list including adb command, function description and parameter information. You should create 3 varied, innovative, and detailed multi queries that employ multi-APP as a tool, API can be used as an auxiliary.</p>
<p>Each query should include the necessary parameters. Note that you shouldn't ask 'which APP to use', rather, simply state your needs that can be addressed by these APPs. You should also avoid asking for the input parameters required by the APP call, but instead directly provide the parameter in your query. Those related APPs and APIs have to strictly come from the provided lists. You should first think about possible related APP combinations, then give your query. Keep in mind that each query should call upon two to four APPs.</p>
<p>At the same time, you also need to provide the CheckPoint of this query, including package, key phrase and API. The package comes from the package corresponding to the APP to be used. Key phrase is the key click element or key input character that the Android emulator will perform when executing this query, which is used to check whether the query has been completed. Key phrase should be noun and part of query, should be kept as short as possible.</p>
<p>Key phrase can contain multiple pieces of information, "|" means the query passes when any of the following texts are completed. "|" is used to separate synonymous expressions of the same noun; "\&amp;" indicates that the query must be passed when all texts are completed; sequential CheckPoints are stored in "[]", and the count increases by one for each passed element. The "ADB Command" to be used is stored in the API, which may also be empty. For different queries, overlap of related APPs should be as little as possible.</p>
<p>Deliver your response in this format:
$[{$
"id": "number"
"query": "text"
"APP": ["APP name1", ... ]
"CheckPoint": {
"package": ["APP package name1", ... ]
"key phrase": ["text1", ... ]
"API: ["API1", ...]"
}
}
...
]</p>
<h2>B. 3 APP\&amp;API statistics</h2>
<p>As can be seen from Figure 6, each functional area contains at least one application and its corresponding API. These applications are sufficient to meet the daily needs of users. In other words, our simulation environment is almost consistent with the real daily use environment, and it is consistent with the real daily use environment. Open world information exchange. There are so many practical tools that are the basic functions of mobile . They have been automatically installed and completed during system installation, and standard API interfaces for tools are easier to obtain. Our next step is to increase the number of APIs and SDKs for third-party applications.</p>
<h2>B. 4 Case study</h2>
<p>CheckPoints is a group of words, including packages, key phases, and API, which represent the package name, action keywords, and API instructions of the application respectively. We regularize these words and action histories to check whether they select a sufficient and correct number of applications, UI elements, and APIs to accomplish the given task.</p>
<p>Next, we will give an example of CheckPoints in Figure 7 and Figure 8.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: APP classification and quantity chart: The largest category is utility tools, where we categorize fundamental mobile applications. Their distinctive feature is the use of standard API interfaces, and the API functionality is more comprehensive.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">ID: 7</th>
<th style="text-align: center;">ID: 2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Query: <br> Open Ctrip Travel.</td>
<td style="text-align: center;">Query: <br> Open Himalaya and play the history.</td>
</tr>
<tr>
<td style="text-align: center;">APP: <br> Ctrip Travel</td>
<td style="text-align: center;">APP: <br> ximalaya</td>
</tr>
<tr>
<td style="text-align: center;">CheckPoints:</td>
<td style="text-align: center;">CheckPoints:</td>
</tr>
<tr>
<td style="text-align: center;">- Package: <br> ctrip.android.view</td>
<td style="text-align: center;">- Package: <br> com.ximalaya.ting.android</td>
</tr>
<tr>
<td style="text-align: center;">- Key phrase: <br> Ctrip</td>
<td style="text-align: center;">- Key phrase: <br> Play history history</td>
</tr>
<tr>
<td style="text-align: center;">- API: <br> - ctrip.android.view/.ui.LocalAppsActivity</td>
<td style="text-align: center;">- API: <br> - com.ximalaya.ting.android/.host.activity.MainActi</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">city</td>
</tr>
</tbody>
</table>
<p>Figure 7: A test case in SAST.
Figure 8: A test case in SAMT.</p>
<p>Figure 9 and Figure 10 is an example of a data set and action history. Note that CheckPoints only check successfully executed instructions in the action history. From the action history, we can see that the emulator successfully opened the application by API, perform tasks in ctrip package, and selected "air ticket", "Beijing", and "Shanghai" elements, but failed to input the correct date. According to the definitions of level 1 and level 2 CheckPoints, level 1 CheckPoint score counts package CheckPoints covered, and the score of the example is $1 / 1$, level 2 CheckPoint score counts all CheckPoints covered, and the score of the example is $5 / 6$.</p>
<h1>B. 5 Supplementary experiments</h1>
<p>As can be seen from the table 7, categories with smaller average execution steps generally have higher success rates and CheckPoints scores. Among them, the travel transportation task has the largest average number of execution steps and the lowest PassRate. We can think that more complex tasks require longer execution steps, and the PassRate and CheckPoint score of complex tasks are lower. Travel transportation task contains more uncertainties and it is difficult to determine whether it is completed, so the PassRate is the lowest.</p>
<p>ID: 17
Query:
Please help me search for air tickets from Beijing to Shanghai. I plan to leave on December 12th.
APP:
ctrip.android.view
CheckPoints:</p>
<ul>
<li>Package:
ctrip.android.view</li>
<li>Key phrase:
air ticket,
Beijing,
Shanghai,
December 12th</li>
<li>API:</li>
<li>adb shell am start -a ctri:p.android.view.flight.FlightSearchActivity
[API call: #db shell am start -a ctri:p.android.view.flight.FlightSearchActivity
[Call result]: API execution successful]
[Action: \click|-p package="ctrip.android.view" class="android.widget.TextView" clickable="true"&gt;air ticket <ip>f]
[Action: \click|-p id="ctrip.android.view:id/a" package="ctrip.android.view" class="android.widget.TextView" clickable="true"&gt;Chengde<ip>f]
[Action: input|-p id="ctrip.android.view:id/a" package="ctrip.android.view" class="android.widget.TextView" clickable="true"&gt;Chengde<ip>, Beijing[f]
[Action: \click|-p id="ctrip.android.view:id/b" package="ctrip.android.view" class="android.widget.TextView" clickable="true"&gt;Chengqing<ip>f]
[Action: input|-p id="ctrip.android.view:id/b" package="ctrip.android.view" class="android.widget.TextView" clickable="true"&gt;Chengqing<ip>, Shanghai[f]
[Action: \$Full] Invalid element input|-p id="ctrip.android.view:id/a" package="ctrip.android.view" class="android.widget.TextView" clickable="true"&gt; January 25th <ip>, December 12th[f]
[Action: \click|-button id="ctrip.android.view:id/a" package="ctrip.android.view" class="android.widget.Button" clickable="true"&gt; search <button>f]</li>
</ul>
<p>Figure 9: A test case in SAMT.
Figure 10: A action history of a test case in SAMT.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">APP Category</th>
<th style="text-align: center;">Case Quantity</th>
<th style="text-align: center;">Average #Steps</th>
<th style="text-align: center;">PassRate(\%)</th>
<th style="text-align: center;">CheckPoint ${ }_{11}$</th>
<th style="text-align: center;">CheckPoint ${ }_{12}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Travel Transportation</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">8.17</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: left;">Audiovisual Vision</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">4.03</td>
<td style="text-align: center;">82</td>
<td style="text-align: center;">68</td>
<td style="text-align: center;">72</td>
</tr>
<tr>
<td style="text-align: left;">Social Communication</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">6.40</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">57</td>
<td style="text-align: center;">63</td>
</tr>
<tr>
<td style="text-align: left;">Fashion Shopping</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">7.97</td>
<td style="text-align: center;">54</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">61</td>
</tr>
<tr>
<td style="text-align: left;">Information News</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">6.46</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">83</td>
<td style="text-align: center;">68</td>
</tr>
<tr>
<td style="text-align: left;">Practical Tool</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">2.08</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">87</td>
<td style="text-align: center;">89</td>
</tr>
<tr>
<td style="text-align: left;">Home Life</td>
<td style="text-align: center;">46</td>
<td style="text-align: center;">1.67</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">72</td>
<td style="text-align: center;">91</td>
</tr>
<tr>
<td style="text-align: left;">Book Reading</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">4.17</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">84</td>
</tr>
<tr>
<td style="text-align: left;">Universal Buttons</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">98</td>
<td style="text-align: center;">99</td>
</tr>
</tbody>
</table>
<p>Table 7: Results on SAST classified by APP categories</p>
<h1>C Details for Baseline Model</h1>
<h2>C. 1 Examples for HTML</h2>
<p>Figure 11 shows the correspondence between the components in the UI page and the corresponding HTML code. It is easy to find that most components have text descriptions, but the switch of the alarm clock does not have a corresponding text description, and LLM will hardly think of it. To click this button, therefore, component function exploration is what we need to do next.</p>
<h2>C. 2 Prompts for application Selection and Planning</h2>
<p>You are a large language model agent stored on a mobile phone, below I will provide you with a task, the environment of the current mobile phone interface(Apps information).</p>
<p>Please help me choose the correct APP to perform the task based on the Apps information. If the APP you want is not available on the current page, you can go to play store and download a suitable APP.</p>
<p>On this basis, you should make a simple plan for completing the task.
Let's Begin!</p>
<h2>C. 3 Prompts for API Selection</h2>
<p>You are the greatest large language model agent stored on a mobile phone. You will be provided with a API list that can be called by mobile phone, the task you need to complete, the thought about what have done and what need to do now.</p>
<p>You are just the first step to interact with the phone, and your follow-up is UI interaction components. If you find that there is no suitable API and the next step is UI interaction, please answer directly sorry. You should not use the API to complete the work that has been completed by the UI interactive components in the previous steps.</p>
<p>Your decision should consider the following factors:</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 11: An example for HTML. The orange box illustrates clickable elements, and the blue frame illustrates the scrollable range.</p>
<ol>
<li>You need to first judge based on the UI information and actions complete whether the planned action has been completed.</li>
<li>You must only choose one API that should be executed most at present to finish the first action in next actions.</li>
<li>If there is no suitable API, you can just say sorry without providing any additional suggestions.</li>
</ol>
<p>Strings within "&lt;&gt;" needs to be replaced with specific parameters, you must return a fully executable adb command. Perhaps you can hand over this task to the UI interaction module.</p>
<p>"adb shell input tap <x> <y>" is strictly prohibited as an answer. Your [Answer] can only follow the two templates: "Yes, the most suitable API function call is [adb command]" or "Sorry, [explain]".</p>
<p>Let's Begin!</p>
<h1>C. 4 Prompts for UI Selection</h1>
<p>You are a large language model agent stored on a mobile phone, You need to give the current one-step action that needs to be taken to complete the task. Below I will provide you with a task, a plan, the environment of the current mobile phone interface(UI information), action history, though about the current status of task completion.</p>
<p>You need to select the most suitable one element and give the corresponding one action based on the UI information and thought. You need to first judge based on the UI information and action history whether the planned action has been completed. Your selection should also consider action history, and have the courage to try new buttons instead of the same buttons from history.</p>
<p>Action can only be the following three functions:</p>
<ol>
<li>click(element)</li>
</ol>
<p>Click a element, only when clickable="true", the element can be clicked.
2. input(element, text)</p>
<p>When you decide to enter, you first need to select the unit by clicking.
3. scroll $\left[x_{\text {start }}, y_{\text {start }}\right]\left[x_{\text {end }}, y_{\text {end }}\right]$</p>
<p>Scroll the screen from $\left[x_{\text {start }}, y_{\text {start }}\right]$ to $\left[x_{\text {end }}, y_{\text {end }}\right]$. The four parameters you fill in cannot be directly the same as $x_{\min }, y_{\min }, x_{\max }, y_{m} a x . x$ cannot exceed $\left(x_{\min }, x_{\max }\right)$, and $y$ cannot exceed $\left(y_{\min }, y_{\max }\right)$. [Examples]:
Remember:
1.Click and input have higher priority than scrolling. Scrolling is only considered when all elements of the current interface are indeed irrelevant to the task.
2.When you fail to try repeatedly in one interface, maybe you can try to turn back to select other options.
3.When you need to switch APPs, you need to return to the desktop first.
4.When input fails multiple times, you should first select it with click.</p>
<p>Let's Begin!</p>
<h1>C. 5 Prompts for Thought Generation</h1>
<p>You are a large language model agent stored on a mobile phone, below I will provide you with a task, a plan, the environment of the current mobile phone interface before action (Previous UI information), current action, the environment of the current mobile phone interface(Now UI information), action history. Action history records completed operations, including click, input, scroll and API list.</p>
<p>You need to summarize these four aspects: changes in the UI page, actions that have been completed, task progress, one next action.
[one next action] need to choose one among click, input, scroll and one API as the next action, and give one and only one operation object. [One next action] strictly refer to [current action] and [action history] result to do the next action.
[action history] are all previous historical actions, and [current action] is the current action that causes the UI page to change.
[Examples]:
Let's Begin!</p>
<h2>C. 6 Prompts for Task Completion</h2>
<p>You are a large language model agent stored on a mobile phone, below I will provide you with a task, the environment of the current mobile phone interface(UI information), historical action information. You need to judge whether the current task has been completed based on the current environment and historical action information.</p>
<h2>C. 7 Algorithm Examples</h2>
<p>This is a running process of the algorithm on a test case of SAMT</p>
<p>"id": 2,
"query": [
"Play recent records in history with Himalaya."
],
"check_point": {
"activity":[
"com.ximalaya.ting.android.host.activity.MainActivity",
\&amp; "com.ximalaya.ting.android.host.activity.MainActivity"
],
"key phrase": [
"Playing history" | "history "
],
"package": "com.ximalaya.ting.android"
},</p>
<p>"domain": "smartApp/Ximalaya"
}
According to algorithm 1, LLM generates a plan based on the query in data as a task and a given list of available applications as follows:
[Task]: Play recent records in history with Himalaya.</p>
<p>"name": "ctrip",
"function_description": "As an authoritative online travel service company in the industry, Ctrip's travel hotel booking platform covers approximately 1.4 million hotels around the world; air ticket products cover more than 300 international airlines; these airlines operate flights to major cities around the world, providing more than 2 million Air routes connect more than 5,000 cities in about 200 countries and regions..."
},
...
$]$
[Plan]: I should open the Himalaya app and find the "Playback History" option in the app to view recent playback history.</p>
<p>Then the task plan is executed iteratively. In each iteration, the model accepts task, app list, action history, plan, and current HTML as input, and outputs execution API calls or UI operation instructions.</p>
<p>A short example of AH(action history), thought, Html will be shown below:
[Action History]:
{'API call': 'adb shell am start -n com.ximalaya.ting.android/.host.activity.MainActivity. [Call result]:API execution successful'}
{'Action': '[Fail]: Invalid element click(<div id="com.ximalaya.ting.android:id/xm_ad_host_count_down _click_lay_no_click_view" package="com.ximalaya.ting.android" class="android.view.View" description="Skip ads" clickable="true"> </div>)'}
{'Action': 'click(<button id="com.ximalaya.ting.android:id/tab_myspace_and_listen" package="com.ximalaya.ting.android" class="android.widget.RadioButton" clickable="true"> my </button>)'}
[thought]:
Changes: The current page is the main page of the Ximalaya app. There is no change between the previous and current pages.
Task progress: From the current action and action history, I am currently on the main page of the Ximalaya app, ready to check the play history.
Actions completed: From the current action and action history, I have opened the Ximalaya app.
One next action: Click on the "play history" option to view the recent play history.
[HTML]:</p>
<p package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true"> membership </p>
<p id="com.ximalaya.ting.android:id/main_tv_user_level" package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true"> user level </p>
<p id="com.ximalaya.ting.android:id/main_tv_listen_duration_title" package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true"> Listen (minutes) </p>
<p package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true"> fans </p>
<p package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true"> Follow </p>
<p package="com.ximalaya.ting.android" class="android.widget.TextView" clickable="true"> live </p>
<p><button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true"> message</p>
<p>$&lt;$ button $&gt;$
<button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true"> history </button>
<button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true"> favorite </button>
<button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true"> local </button>
<button package="com.ximalaya.ting.android" class="android.widget.Button" clickable="true"> purchased </button>
<img package="com.ximalaya.ting.android" class="android.widget.ImageView" description="play" clickable="true"> </img>
A example of api action or ui action will be shown below:
<a href="click" title="&lt;button package=&quot;com.ximalaya.ting.android&quot; class=&quot;android.widget.Button&quot; clickable=&quot;true&quot;&gt; history &lt;/button&gt;">Action</a>: [adb shell am start -n com.ximalaya.ting.android/.host.activity.MainActivity]</p>
<p>After successful execution, the current action will be added to the action history, the updated HTML of the emulator will be read, and handed over to LLM to generate a new thought and determine whether the task is over.
[thought]:
Changes: The current page is the "My" page in the Ximalaya app.
Actions Complete: I have opened the Ximalaya app and clicked the "my" button, then clicked the "play history" button.
Task progress: The current mission progress is to view the play history.
One next action: Click on the "play" item to continue playing.
[Finished]: No, task is not finished.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Equal contribution.
${ }^{\dagger}$ Corresponding authors: Rui Yan and Shuo Shang.
${ }^{\ddagger}$ Work done during the internship at XiaoMi.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
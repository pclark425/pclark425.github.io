<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3106 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3106</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3106</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-267751102</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2402.11291v3.pdf" target="_blank">Puzzle Solving using Reasoning of Large Language Models: A Survey</a></p>
                <p><strong>Paper Abstract:</strong> Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy—dividing puzzles into rule-based and rule-less categories—to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs’ performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs’ puzzle-solving proficiency and contribute to AI’s logical reasoning and creative problem-solving advancements.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3106.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3106.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-2 (Noever & Burdick)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pretrained Transformer 2 (as fine-tuned by Noever & Burdick, 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-2 fine-tuned on deterministic spatial puzzles (Sudoku, Rubik's Cube, mazes) using textual encodings; used to probe LLM spatial reasoning after task-specific fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Puzzle solving without search or human knowledge: An unnatural language approach</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A large autoregressive transformer language model pretrained on internet text and then fine-tuned on puzzle datasets (as reported in Noever & Burdick, 2021).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku, Rubik's Cube, Mazes</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Sudoku: constraint-satisfaction on a 2D grid requiring placement consistent with row/column/box constraints; Rubik's Cube: a 3D combinatorial spatial state-space requiring sequences of moves; Mazes: pathfinding in grid-like topologies testing spatial navigation reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual encodings; for Sudoku a compact single-string format with '-' for empty cells was used (Noever & Burdick experimented with single-string; authors suggested matrix representations might help).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Fine-tuning on task-specific puzzle instances (supervised fine-tuning on many examples).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Authors observed limited but nontrivial spatial competence: GPT-2 could produce plausible solution sequences and valid-looking outputs, but often incorrect; the paper notes representation choice (single-string vs matrix) affects learning and suggests matrix may be superior for spatial structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Rubik's Cube: GPT-2 solved the Rubik's Cube in 1 out of 7 attempts (reported); Sudoku: results described as suboptimal overall (no reliable >80% for 5x5 puzzles reported in survey across works), no robust high-accuracy Sudoku metric reported for GPT-2 in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>High rate of valid-but-incorrect solutions; limited success-rate on Rubik's Cube (1/7) and generally suboptimal results on Sudoku and mazes, possibly due to representation choices and limited fine-tuning duration/instances.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Underperformed compared to specialized algorithmic/heuristic solvers (conventional constraint solvers, search-based Rubik algorithms) and generally lower than later LLM methods (ToT/XoT) reported in later works.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Puzzle Solving using Reasoning of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3106.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3106.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XoT on 2x2 Rubik's Cube (Ding et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Everything-of-Thoughts (XoT) applied to 2×2×2 Rubik's Cube using GPT-3.5 / GPT-4 (Ding et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XoT combines LLM-generated thought proposals with Monte Carlo Tree Search and self-revision to solve spatial combinatorial puzzles; applied to a 2×2×2 Rubik's Cube, yielding substantially improved success rates over other prompting topologies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Everything of thoughts: Defying the law of penrose triangle for thought generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (as evaluated by Ding et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LLMs (GPT-3.5 and GPT-4) invoked as thought generators within the XoT framework; XoT augments them with MCTS-style search and self-revision to explore solution sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>2×2×2 Rubik's Cube (Pocket Cube)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A 2×2×2 variant of the Rubik's Cube requiring reasoning about 3D piece orientations and sequences of face rotations to reach a solved configuration from arbitrary scrambles.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual/state encoding of cube configurations (survey reports LLMs operate on encoded textual/state representations; XoT integrates MCTS over LLM-generated moves).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>XoT (Everything-of-Thoughts) with self-revision — graph-based thought generation integrated with Monte Carlo Tree Search and iterative self-correction.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Empirical evidence that combining LLM proposals with search (MCTS) and revision yields better handling of spatial action sequencing; XoT reduces needed LLM invocations while producing higher-quality multi-step spatial plans compared to linear CoT or ToT.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported 77.6% success rate on the 2×2×2 Rubik's Cube when using XoT with self-revision (Ding et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Requires many LLM calls and search orchestration (though XoT reports fewer invocations than some other tree methods); success depends on integration quality between LLM proposals and search; generalization to larger cubes not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Significantly outperformed CoT, Self-Consistency, ToT and GoT baselines in the reported experiments; still not directly compared to specialized Rubik's algorithms or human speed/optimality in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Puzzle Solving using Reasoning of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3106.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3106.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XoT on 8-Puzzle (Ding et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Everything-of-Thoughts (XoT) applied to the 8-Puzzle with revision (Ding et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>XoT framework applied to the spatial sliding-tile 8-Puzzle, combining LLM reasoning with search and self-revision to produce near-state-of-the-art multi-step solutions in a textual/plan form.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Everything of thoughts: Defying the law of penrose triangle for thought generation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (as evaluated by Ding et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LLMs used as proposal generators inside the XoT pipeline; XoT augments LLMs with revision and search to solve spatial planning problems.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>8-Puzzle (sliding tile)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>A classic spatial planning puzzle where tiles numbered 1–8 must be slid within a 3×3 board to reach a goal configuration; requires multi-step reasoning about spatial moves and goal-state reachability.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual/state encodings of board configurations (survey describes LLMs receiving encoded puzzle states rather than raw images).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>XoT with revision (LLM proposals + search + self-revision).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>High accuracy suggests LLMs can propose effective spatial move sequences when coupled with search and revision; the method leverages exploration to compensate for individual LLM plan errors.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported 93.2% accuracy across 419 8-Puzzle instances when using XoT with revision (Ding et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Success depends on the revision and search components; standalone prompting (CoT/few-shot) is less reliable; the approach increases system complexity and LLM invocation overhead compared to single-pass prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Substantially better than CoT, Self-Consistency, ToT and GoT baselines in the reported experiments; not directly compared to classical A*/heuristic planners in the survey text.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Puzzle Solving using Reasoning of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3106.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3106.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Tree-of-Thoughts on Sudoku (Long)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Tree-of-Thoughts guided puzzle solving applied to Sudoku (Long, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Application of Tree-of-Thoughts (ToT) prompting topology to Sudoku puzzle solving, finding ToT particularly effective for smaller grid sizes and emphasizing the role of representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Large language model guided tree-of-thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Unspecified LLM(s) (as reported by Long, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLMs used within the Tree-of-Thoughts framework to explore branching chains of intermediate reasoning steps; the survey reports ToT as an effective prompting topology for Sudoku.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Grid-based constraint-satisfaction puzzle requiring spatial-local and global reasoning (rows, columns, subgrid constraints) to fill empty cells consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Nested list representation (Long used nested lists to encode Sudoku states; survey contrasts this with single-string encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Tree-of-Thoughts (ToT) prompting, exploring branched intermediate reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Survey notes ToT improves performance by allowing exploration of multiple intermediate strategies; representation (nested lists) helps the model capture grid structure and local spatial relations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ToT reported as the most effective method in Long (2023), especially for smaller puzzles; survey also notes that across works LLMs have not exceeded ~80% on 5×5 Sudoku variants (Long's reporting is qualitative in survey).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>ToT requires many LLM invocations (computationally expensive) and gains diminish for larger/harder variants; representation sensitivity (single-string vs nested) can materially affect outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>ToT outperforms simple IO/CoT prompting for Sudoku in reported experiments; however, classic constraint solvers still reliably solve Sudoku deterministically and are not directly matched by LLM results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Puzzle Solving using Reasoning of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3106.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3106.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 -> ASP (Ishay et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3 / GPT-4 used to translate puzzles into Answer Set Programming (Ishay et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neuro-symbolic approach where GPT-3/GPT-4 convert natural-language puzzle descriptions (including Sudoku and other logic puzzles) into Answer Set Programming (ASP) predicates/rules, then solved by symbolic solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Leveraging large language models to generate answer set programs</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 / GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LLMs used to generate formal logic encodings (ASP) from natural language puzzle descriptions; the downstream solving is performed by an ASP solver.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku and other logic puzzles</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Rule-based deterministic puzzles where rules and constraints can be expressed in logical predicates suitable for symbolic solvers (e.g., Sudoku row/column/box constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Natural language puzzle text translated into ASP code (predicate/rule format) by the LLM; symbolic solver executes the generated program.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Neuro-symbolic translation: LLM generates logic (ASP) which is executed by a symbolic solver; few-shot and zero-shot variants reported.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>The success of this pipeline shows LLMs can encode spatial and constraint relations into symbolic form that symbolic solvers can exploit; however, the survey notes this tests translation ability more than pure LLM solving capability.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>GPT-4 achieved 92% accuracy on a logic puzzles dataset (Mitra & Baral, 2015) when used to generate ASP encodings (Ishay et al., 2023).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Neuro-symbolic pipeline primarily evaluates translation quality; generated programs may contain errors requiring human refinement; limited work translating puzzles into executable code was found in the survey.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>When successful, the LLM+ASP pipeline reaches high accuracy comparable to symbolic approaches because the symbolic solver handles search; this approach differs from pure LLM solving and often outperforms plain prompting methods on structured logical puzzles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Puzzle Solving using Reasoning of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3106.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3106.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 / GPT-4 on Minesweeper (Li et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLMs evaluated on Minesweeper (Assessing logical puzzle solving in LLMs: a Minesweeper case study, Li et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Case study evaluating LLM spatial/constraint reasoning on Minesweeper using different input encodings (table vs coordinate) and prompting strategies; probes model ability to deduce hidden mine locations from numeric clues.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Assessing logical puzzle solving in large language models: Insights from a minesweeper case study</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5 / GPT-4 (as evaluated by Li et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive LLMs tested on Minesweeper tasks; models were given textual encodings of boards and clues and asked to identify mine locations or complete boards.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Minesweeper</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Grid-based game with hidden bombs; numbered cells provide counts of adjacent bombs, requiring local spatial inference and constraint propagation to deduce safe cells and mine positions.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Table representation vs coordinate representation of the Minesweeper board (Li et al. found coordinate representation can aid comprehension).</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Zero-shot / few-shot prompting and different input encodings; some experiments used few-shot exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>Survey reports GPT-3.5 showed some initial understanding but few-shot prompting had minimal effect; GPT-4 improved mine identification abilities but struggled to fully solve boards, indicating partial but incomplete spatial constraint reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative: GPT-4 improved at identifying mines compared to GPT-3.5, but no complete-board success-rate numbers were provided in the survey summary.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>LLMs struggle to complete full Minesweeper boards, particularly under hidden-information/stochastic aspects; performance sensitive to input encoding (coordinate > table) and prompting, and few-shot exemplars provided limited gains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>Performance lags behind algorithmic CSP approaches specialized for Minesweeper; GPT-4 is better than GPT-3.5 in mine identification but still fails on full-board completion in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Puzzle Solving using Reasoning of Large Language Models: A Survey', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Everything of thoughts: Defying the law of penrose triangle for thought generation <em>(Rating: 2)</em></li>
                <li>Puzzle solving without search or human knowledge: An unnatural language approach <em>(Rating: 2)</em></li>
                <li>Large language model guided tree-of-thought <em>(Rating: 2)</em></li>
                <li>Leveraging large language models to generate answer set programs <em>(Rating: 2)</em></li>
                <li>Assessing logical puzzle solving in large language models: Insights from a minesweeper case study <em>(Rating: 2)</em></li>
                <li>Tree of thoughts: Deliberate problem solving with large language models <em>(Rating: 1)</em></li>
                <li>Tree of uncertain thoughts reasoning for large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3106",
    "paper_id": "paper-267751102",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "GPT-2 (Noever & Burdick)",
            "name_full": "Generative Pretrained Transformer 2 (as fine-tuned by Noever & Burdick, 2021)",
            "brief_description": "GPT-2 fine-tuned on deterministic spatial puzzles (Sudoku, Rubik's Cube, mazes) using textual encodings; used to probe LLM spatial reasoning after task-specific fine-tuning.",
            "citation_title": "Puzzle solving without search or human knowledge: An unnatural language approach",
            "mention_or_use": "mention",
            "model_name": "GPT-2",
            "model_description": "A large autoregressive transformer language model pretrained on internet text and then fine-tuned on puzzle datasets (as reported in Noever & Burdick, 2021).",
            "model_size": null,
            "puzzle_name": "Sudoku, Rubik's Cube, Mazes",
            "puzzle_description": "Sudoku: constraint-satisfaction on a 2D grid requiring placement consistent with row/column/box constraints; Rubik's Cube: a 3D combinatorial spatial state-space requiring sequences of moves; Mazes: pathfinding in grid-like topologies testing spatial navigation reasoning.",
            "input_representation": "Textual encodings; for Sudoku a compact single-string format with '-' for empty cells was used (Noever & Burdick experimented with single-string; authors suggested matrix representations might help).",
            "prompting_method": "Fine-tuning on task-specific puzzle instances (supervised fine-tuning on many examples).",
            "spatial_reasoning_analysis": "Authors observed limited but nontrivial spatial competence: GPT-2 could produce plausible solution sequences and valid-looking outputs, but often incorrect; the paper notes representation choice (single-string vs matrix) affects learning and suggests matrix may be superior for spatial structure.",
            "performance_metrics": "Rubik's Cube: GPT-2 solved the Rubik's Cube in 1 out of 7 attempts (reported); Sudoku: results described as suboptimal overall (no reliable &gt;80% for 5x5 puzzles reported in survey across works), no robust high-accuracy Sudoku metric reported for GPT-2 in the survey.",
            "limitations_or_failure_modes": "High rate of valid-but-incorrect solutions; limited success-rate on Rubik's Cube (1/7) and generally suboptimal results on Sudoku and mazes, possibly due to representation choices and limited fine-tuning duration/instances.",
            "comparison_to_other_models_or_humans": "Underperformed compared to specialized algorithmic/heuristic solvers (conventional constraint solvers, search-based Rubik algorithms) and generally lower than later LLM methods (ToT/XoT) reported in later works.",
            "uuid": "e3106.0",
            "source_info": {
                "paper_title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "XoT on 2x2 Rubik's Cube (Ding et al.)",
            "name_full": "Everything-of-Thoughts (XoT) applied to 2×2×2 Rubik's Cube using GPT-3.5 / GPT-4 (Ding et al., 2023)",
            "brief_description": "XoT combines LLM-generated thought proposals with Monte Carlo Tree Search and self-revision to solve spatial combinatorial puzzles; applied to a 2×2×2 Rubik's Cube, yielding substantially improved success rates over other prompting topologies.",
            "citation_title": "Everything of thoughts: Defying the law of penrose triangle for thought generation",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 / GPT-4 (as evaluated by Ding et al.)",
            "model_description": "Large autoregressive LLMs (GPT-3.5 and GPT-4) invoked as thought generators within the XoT framework; XoT augments them with MCTS-style search and self-revision to explore solution sequences.",
            "model_size": null,
            "puzzle_name": "2×2×2 Rubik's Cube (Pocket Cube)",
            "puzzle_description": "A 2×2×2 variant of the Rubik's Cube requiring reasoning about 3D piece orientations and sequences of face rotations to reach a solved configuration from arbitrary scrambles.",
            "input_representation": "Textual/state encoding of cube configurations (survey reports LLMs operate on encoded textual/state representations; XoT integrates MCTS over LLM-generated moves).",
            "prompting_method": "XoT (Everything-of-Thoughts) with self-revision — graph-based thought generation integrated with Monte Carlo Tree Search and iterative self-correction.",
            "spatial_reasoning_analysis": "Empirical evidence that combining LLM proposals with search (MCTS) and revision yields better handling of spatial action sequencing; XoT reduces needed LLM invocations while producing higher-quality multi-step spatial plans compared to linear CoT or ToT.",
            "performance_metrics": "Reported 77.6% success rate on the 2×2×2 Rubik's Cube when using XoT with self-revision (Ding et al., 2023).",
            "limitations_or_failure_modes": "Requires many LLM calls and search orchestration (though XoT reports fewer invocations than some other tree methods); success depends on integration quality between LLM proposals and search; generalization to larger cubes not reported here.",
            "comparison_to_other_models_or_humans": "Significantly outperformed CoT, Self-Consistency, ToT and GoT baselines in the reported experiments; still not directly compared to specialized Rubik's algorithms or human speed/optimality in the survey text.",
            "uuid": "e3106.1",
            "source_info": {
                "paper_title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "XoT on 8-Puzzle (Ding et al.)",
            "name_full": "Everything-of-Thoughts (XoT) applied to the 8-Puzzle with revision (Ding et al., 2023)",
            "brief_description": "XoT framework applied to the spatial sliding-tile 8-Puzzle, combining LLM reasoning with search and self-revision to produce near-state-of-the-art multi-step solutions in a textual/plan form.",
            "citation_title": "Everything of thoughts: Defying the law of penrose triangle for thought generation",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 / GPT-4 (as evaluated by Ding et al.)",
            "model_description": "Large autoregressive LLMs used as proposal generators inside the XoT pipeline; XoT augments LLMs with revision and search to solve spatial planning problems.",
            "model_size": null,
            "puzzle_name": "8-Puzzle (sliding tile)",
            "puzzle_description": "A classic spatial planning puzzle where tiles numbered 1–8 must be slid within a 3×3 board to reach a goal configuration; requires multi-step reasoning about spatial moves and goal-state reachability.",
            "input_representation": "Textual/state encodings of board configurations (survey describes LLMs receiving encoded puzzle states rather than raw images).",
            "prompting_method": "XoT with revision (LLM proposals + search + self-revision).",
            "spatial_reasoning_analysis": "High accuracy suggests LLMs can propose effective spatial move sequences when coupled with search and revision; the method leverages exploration to compensate for individual LLM plan errors.",
            "performance_metrics": "Reported 93.2% accuracy across 419 8-Puzzle instances when using XoT with revision (Ding et al., 2023).",
            "limitations_or_failure_modes": "Success depends on the revision and search components; standalone prompting (CoT/few-shot) is less reliable; the approach increases system complexity and LLM invocation overhead compared to single-pass prompts.",
            "comparison_to_other_models_or_humans": "Substantially better than CoT, Self-Consistency, ToT and GoT baselines in the reported experiments; not directly compared to classical A*/heuristic planners in the survey text.",
            "uuid": "e3106.2",
            "source_info": {
                "paper_title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Tree-of-Thoughts on Sudoku (Long)",
            "name_full": "Tree-of-Thoughts guided puzzle solving applied to Sudoku (Long, 2023)",
            "brief_description": "Application of Tree-of-Thoughts (ToT) prompting topology to Sudoku puzzle solving, finding ToT particularly effective for smaller grid sizes and emphasizing the role of representation.",
            "citation_title": "Large language model guided tree-of-thought",
            "mention_or_use": "mention",
            "model_name": "Unspecified LLM(s) (as reported by Long, 2023)",
            "model_description": "LLMs used within the Tree-of-Thoughts framework to explore branching chains of intermediate reasoning steps; the survey reports ToT as an effective prompting topology for Sudoku.",
            "model_size": null,
            "puzzle_name": "Sudoku",
            "puzzle_description": "Grid-based constraint-satisfaction puzzle requiring spatial-local and global reasoning (rows, columns, subgrid constraints) to fill empty cells consistently.",
            "input_representation": "Nested list representation (Long used nested lists to encode Sudoku states; survey contrasts this with single-string encodings).",
            "prompting_method": "Tree-of-Thoughts (ToT) prompting, exploring branched intermediate reasoning paths.",
            "spatial_reasoning_analysis": "Survey notes ToT improves performance by allowing exploration of multiple intermediate strategies; representation (nested lists) helps the model capture grid structure and local spatial relations.",
            "performance_metrics": "ToT reported as the most effective method in Long (2023), especially for smaller puzzles; survey also notes that across works LLMs have not exceeded ~80% on 5×5 Sudoku variants (Long's reporting is qualitative in survey).",
            "limitations_or_failure_modes": "ToT requires many LLM invocations (computationally expensive) and gains diminish for larger/harder variants; representation sensitivity (single-string vs nested) can materially affect outcomes.",
            "comparison_to_other_models_or_humans": "ToT outperforms simple IO/CoT prompting for Sudoku in reported experiments; however, classic constraint solvers still reliably solve Sudoku deterministically and are not directly matched by LLM results.",
            "uuid": "e3106.3",
            "source_info": {
                "paper_title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-4 -&gt; ASP (Ishay et al.)",
            "name_full": "GPT-3 / GPT-4 used to translate puzzles into Answer Set Programming (Ishay et al., 2023)",
            "brief_description": "Neuro-symbolic approach where GPT-3/GPT-4 convert natural-language puzzle descriptions (including Sudoku and other logic puzzles) into Answer Set Programming (ASP) predicates/rules, then solved by symbolic solvers.",
            "citation_title": "Leveraging large language models to generate answer set programs",
            "mention_or_use": "mention",
            "model_name": "GPT-3 / GPT-4",
            "model_description": "Large autoregressive LLMs used to generate formal logic encodings (ASP) from natural language puzzle descriptions; the downstream solving is performed by an ASP solver.",
            "model_size": null,
            "puzzle_name": "Sudoku and other logic puzzles",
            "puzzle_description": "Rule-based deterministic puzzles where rules and constraints can be expressed in logical predicates suitable for symbolic solvers (e.g., Sudoku row/column/box constraints).",
            "input_representation": "Natural language puzzle text translated into ASP code (predicate/rule format) by the LLM; symbolic solver executes the generated program.",
            "prompting_method": "Neuro-symbolic translation: LLM generates logic (ASP) which is executed by a symbolic solver; few-shot and zero-shot variants reported.",
            "spatial_reasoning_analysis": "The success of this pipeline shows LLMs can encode spatial and constraint relations into symbolic form that symbolic solvers can exploit; however, the survey notes this tests translation ability more than pure LLM solving capability.",
            "performance_metrics": "GPT-4 achieved 92% accuracy on a logic puzzles dataset (Mitra & Baral, 2015) when used to generate ASP encodings (Ishay et al., 2023).",
            "limitations_or_failure_modes": "Neuro-symbolic pipeline primarily evaluates translation quality; generated programs may contain errors requiring human refinement; limited work translating puzzles into executable code was found in the survey.",
            "comparison_to_other_models_or_humans": "When successful, the LLM+ASP pipeline reaches high accuracy comparable to symbolic approaches because the symbolic solver handles search; this approach differs from pure LLM solving and often outperforms plain prompting methods on structured logical puzzles.",
            "uuid": "e3106.4",
            "source_info": {
                "paper_title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "GPT-3.5 / GPT-4 on Minesweeper (Li et al.)",
            "name_full": "LLMs evaluated on Minesweeper (Assessing logical puzzle solving in LLMs: a Minesweeper case study, Li et al., 2023)",
            "brief_description": "Case study evaluating LLM spatial/constraint reasoning on Minesweeper using different input encodings (table vs coordinate) and prompting strategies; probes model ability to deduce hidden mine locations from numeric clues.",
            "citation_title": "Assessing logical puzzle solving in large language models: Insights from a minesweeper case study",
            "mention_or_use": "mention",
            "model_name": "GPT-3.5 / GPT-4 (as evaluated by Li et al.)",
            "model_description": "Large autoregressive LLMs tested on Minesweeper tasks; models were given textual encodings of boards and clues and asked to identify mine locations or complete boards.",
            "model_size": null,
            "puzzle_name": "Minesweeper",
            "puzzle_description": "Grid-based game with hidden bombs; numbered cells provide counts of adjacent bombs, requiring local spatial inference and constraint propagation to deduce safe cells and mine positions.",
            "input_representation": "Table representation vs coordinate representation of the Minesweeper board (Li et al. found coordinate representation can aid comprehension).",
            "prompting_method": "Zero-shot / few-shot prompting and different input encodings; some experiments used few-shot exemplars.",
            "spatial_reasoning_analysis": "Survey reports GPT-3.5 showed some initial understanding but few-shot prompting had minimal effect; GPT-4 improved mine identification abilities but struggled to fully solve boards, indicating partial but incomplete spatial constraint reasoning.",
            "performance_metrics": "Qualitative: GPT-4 improved at identifying mines compared to GPT-3.5, but no complete-board success-rate numbers were provided in the survey summary.",
            "limitations_or_failure_modes": "LLMs struggle to complete full Minesweeper boards, particularly under hidden-information/stochastic aspects; performance sensitive to input encoding (coordinate &gt; table) and prompting, and few-shot exemplars provided limited gains.",
            "comparison_to_other_models_or_humans": "Performance lags behind algorithmic CSP approaches specialized for Minesweeper; GPT-4 is better than GPT-3.5 in mine identification but still fails on full-board completion in many cases.",
            "uuid": "e3106.5",
            "source_info": {
                "paper_title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Everything of thoughts: Defying the law of penrose triangle for thought generation",
            "rating": 2,
            "sanitized_title": "everything_of_thoughts_defying_the_law_of_penrose_triangle_for_thought_generation"
        },
        {
            "paper_title": "Puzzle solving without search or human knowledge: An unnatural language approach",
            "rating": 2,
            "sanitized_title": "puzzle_solving_without_search_or_human_knowledge_an_unnatural_language_approach"
        },
        {
            "paper_title": "Large language model guided tree-of-thought",
            "rating": 2,
            "sanitized_title": "large_language_model_guided_treeofthought"
        },
        {
            "paper_title": "Leveraging large language models to generate answer set programs",
            "rating": 2,
            "sanitized_title": "leveraging_large_language_models_to_generate_answer_set_programs"
        },
        {
            "paper_title": "Assessing logical puzzle solving in large language models: Insights from a minesweeper case study",
            "rating": 2,
            "sanitized_title": "assessing_logical_puzzle_solving_in_large_language_models_insights_from_a_minesweeper_case_study"
        },
        {
            "paper_title": "Tree of thoughts: Deliberate problem solving with large language models",
            "rating": 1,
            "sanitized_title": "tree_of_thoughts_deliberate_problem_solving_with_large_language_models"
        },
        {
            "paper_title": "Tree of uncertain thoughts reasoning for large language models",
            "rating": 1,
            "sanitized_title": "tree_of_uncertain_thoughts_reasoning_for_large_language_models"
        }
    ],
    "cost": 0.018852999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Puzzle Solving using Reasoning of Large Language Models: A Survey
14 Sep 2024</p>
<p>Panagiotis Giadikiaroglou 
Maria Lymperaiou 
Giorgos Filandrianos 
Giorgos Stamou 
Rosie Cai 
Andrew Campbell 
Brittany Cann 
Chelsea Carey 
Rory Carlson 
Brooke Carmichael 
Che Chan 
Fotis Chang 
Derek Chantzis 
Sully Chen 
Ruby Chen 
Jason Chen 
Mark Chen 
Ben Chen 
Chester Chess 
Casey Cho 
HyungWon Chu 
Dave Chung 
Jeremiah Cummings 
Yunxing Currier 
Cory Dai 
Thomas Decareaux 
Noah Degry 
Damien Deutsch 
Arka Deville 
David Dhar 
Steve Dohan 
Sheila Dowl- Ing 
Adrien Dunning 
Atty Ecoffet 
Tyna Eleti 
David Eloundou 
Liam Farhi 
Niko Fedus 
SimónPosada Felix 
Juston Fishman 
Is- Abella Forte 
Leo Fulford 
Elie Gao 
Christian Georges 
Vik Gibson 
Tarun Goel 
Gabriel Gogineni 
Rapha Goh 
Jonathan Gontijo-Lopes 
Morgan Gordon 
Scott Grafstein 
Ryan Gray 
Joshua Greene 
ShixiangShane Gross 
Yufei Gu 
Chris Guo 
Jesse Hallacy 
Jeff Han 
Yuchen Harris 
Mike He 
Jo- Hannes Heaton 
Chris Heidecke 
Alan Hesse 
Wade Hickey 
Peter Hickey 
Brandon Hoeschele 
Kenny Houghton 
Shengli Hsu 
Xin Hu 
Joost Hu 
Shantanu Huizinga 
Shawn Jain 
Joanne Jain 
Angela Jang 
Roger Jiang 
Haozhun Jiang 
Denny Jin 
Shino Jin 
Billie Jomoto 
Heewoo Jonn 
Tomer Jun 
Łukasz Kaftan 
Ali Kaiser 
Ingmar Kamali 
Kanitscheider 
Shirish Nitish 
Tabarak Keskar 
Logan Khan 
Jong Wook Kilpatrick 
Christina Kim 
Yongjik Kim 
Hendrik Kim 
Jamie Kirch- Ner 
Matt Kiros 
Daniel Knight 
Łukasz Kokotajlo 
Andrew Kondraciuk 
Aris Kondrich 
Kyle Kon- Stantinidis 
Gretchen Kosic 
Vishal Krueger 
Michael Kuo 
Ikai Lampe 
Teddy Lan 
Jan Lee 
Jade Leike 
Daniel Leung 
ChakMing Levy 
Rachel Li 
Molly Lim 
Stephanie Lin 
Mateusz Lin 
Theresa Litwin 
Ryan Lopez 
Patricia Lowe 
Anna Lue 
Kim Makanju 
Sam Malfacini 
Todor Manning 
Yaniv Markov 
Bianca Markovski 
Katie Martin 
Andrew Mayer 
Bob Mayne 
Scott Mayer Mcgrew 
Christine Mckinney 
Paul Mcleavey 
Jake Mcmillan 
David Mcneil 
Aalok Medina 
Jacob Mehta 
Luke Menick 
Andrey Metz 
Pamela Mishchenko 
Vinnie Mishkin 
Evan Monaco 
Daniel Morikawa 
Tong Mossing 
Mira Mu 
Oleg Murati 
David Murk 
Ashvin Mély 
Reiichiro Nair 
Rajeev Nakano 
Arvind Nayak 
Richard Neelakantan 
Hyeonwoo Ngo 
Long Noh 
Cullen Ouyang 
Jakub O'keefe 
Alex Pachocki 
Joe Paino 
Ashley Palermo 
Pantuliano 
Mario Saltarelli 
Ted Sanders 
Shibani Santurkar 
Girish Sastry 
Heather Schmidt 
David Schnurr 
John Schulman 
Daniel Selsam 
Kyla Sheppard 
Toki Sherbakov 
Jessica Shieh 
Sarah Shoker 
Pranav Shyam 
Szymon Sidor 
Eric Sigler 
Maddie Simens 
Jordan Sitkin 
Katarina Slama 
Ian Sohl 
Benjamin Sokolowsky 
Yang Song 
Natalie Staudacher 
Clemens Winter 
Samuel Wolrich 
Hannah Wong 
Lauren Workman 
Sherwin Wu 
Jeff Wu 
Michael Wu 
Kai Xiao 
Tao Xu 
Sarah Yoo 
Kevin Yu 
Qim- Ing Yuan 
Wojciech Zaremba 
Rowan Zellers 
Chong Zhang 
Marvin Zhang 
Shengjia Zhao 
Tianhao Zheng 
Juntang Zhuang 
William Zhuk 
Barret 2023 Zoph </p>
<p>School of Electrical and Computer Engineering
Artificial Intelligence and Learning Systems Laboratory
National Technical University of Athens</p>
<p>Giambat-tista Parascandolo
Joel Parish
Emy Parparita, Mikhail Pavlov, Andrew PengAlex Passos</p>
<p>Filipe de Avila Belbute Peres
Adam Perel-man
Michael Petrov</p>
<p>Henrique Ponde de Oliveira Pinto
Poko-rnyMichael</p>
<p>Michelle Pokrass
Vitchyr Pong, Tolly Pow-ell, Boris PowerAlethea Power</p>
<p>Elizabeth Proehl
Raul Puri, Alec Radford, Jack Rae, Aditya RameshCameron Raymond</p>
<p>Francis Real
Kendra Rimbach
Carl Ross, Bob Rotsted</p>
<p>Henri Roussez
Nick Ry</p>
<p>Fe-lipe Petroski Such
Natalie Summers
Ilya Sutskever
Jie Tang</p>
<p>Nikolas Tezak
Phil Tillet, Elizabeth Tseng, Jerry TworekMadeleine Thompson, Amin Tootoonchian, Pre-ston Tuggle, Nick Turley</p>
<p>Juan Fe-lipe Cerón Uribe
Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter WelinderAlvin Wang, Ben Wang</p>
<p>Ji-ayi Weng
Lilian Weng, Matt Wiethoff, Dave Willner</p>
<p>Puzzle Solving using Reasoning of Large Language Models: A Survey
14 Sep 202466C1BE98587CE7CB7636B53A38E063F8arXiv:2402.11291v3[cs.CL]
Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks.This survey leverages a unique taxonomy-dividing puzzles into rule-based and rule-less categories-to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning.Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios.Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference.The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's logical reasoning and creative problem-solving advancements.</p>
<p>Introduction</p>
<p>Recent developments in LLMs such as GPT-3 (Brown et al., 2020) and GPT-4 (OpenAI et al., 2023) have showcased their logical reasoning abilities across various domains (Liu et al., 2023a,b;Bao et al., 2023;Creswell et al., 2022).Despite these advances and their demonstrated capabilities in deductive reasoning (Saparov et al., 2023), LLMs face limitations in inductive reasoning settings, as analyzed by Xu et al. (2023a); Bang et al. (2023).The specific application of LLMs to puzzle solving, has not been thoroughly summarized.</p>
<p>Our main contributions are as follows: 1 We introduce a distinction between rule-based and ruleless puzzles ( §2), highlighting the varied knowledge demands necessary to tackle them. 2 We analyze the methodologies LLMs use to solve puzzles ( §3), assessing their impact on each category and comparing them with conventional problemsolving methods.3 A detailed exploration of existing benchmarks that gauge models' reasoning abilities is conducted ( §4). 4 Finally, this paper offers a detailed view of the present obstacles faced in puzzle-solving with LLMs and highlights a wide array of prospects for future research ( §5).</p>
<p>Our categorization diverges from existing logical reasoning taxonomies by emphasizing on the underlying cognitive processes and the skills required for puzzle solving, rather than the question format (Luo et al., 2023) or the nature of reasoning (deductive, inductive, abductive) (Luo et al., 2023;Yu et al., 2023a;Yang et al., 2023b;Qiao et al., 2022;Huang and Chang, 2022;Flach and Kakas, 2000).For instance, the existence of rules in puzzles such as Sudoku, Crosswords, or Minesweeper necessitates additional skills (e.g.strategy development) to correctly understand the game's rules or the ability to correctly format the output.In contrast, rule-less puzzles, such as riddles (Figure 1), programming challenges, and commonsense reasoning problems, leverage the model's inherent knowledge for solution derivation.</p>
<p>In our work, we define puzzles as problems that test cognitive abilities including logical reasoning, spatial cognition, and creative thinking by requiring the solver to discern patterns, apply deduction, and combine insights from available information in order to arrive at the correct solution.Notably,</p>
<p>Puzzle Categories</p>
<p>Rule-less puzzles: rely more on flexible thinking, real-world knowledge and inferential reasoning Commonsense reasoning puzzles: require understanding real-world situations and making inferences based on implicit knowledge LatEval (Huang et al., 2023b), True Detective (Del and Fishel, 2022), DetectBench (Gu et al., 2023), MARB (Tong et al., 2023) Programming puzzles: involve analyzing or modifying code snippets to achieve a specific goal P3 (Schuster et al., 2021), (Savelka et al., 2023) Riddles: use wordplay and metaphors to conceal the answers, requiring abstract connections and lateral thinking BrainTeaser (Jiang et al., 2023), RiddleSense (Lin et al., 2021), BiRdQA (Zhang and Wan, 2021), CC-Riddle (Xu et al., 2022), PUZZLEQA (Zhao and Anderson, 2023), MARB (Tong et al., 2023) Rule-based puzzles: provide explicit victory conditions, legal move sets or state transition rules that the model must follow to solve the puzzle Stochastic games: incorporate randomness or hidden information, resulting in different outcomes Minesweeper (Li et al., 2023), BoardgameQA (Kazemi et al., 2023), Card Games (Huang et al., 2024;Gupta, 2023), Social Deduction Games (Wang et al., 2023b;Xu et al., 2023b;Lan et al., 2023) Deterministic games: provide all the information needed to produce an outcome from a given starting state and set of actions BoardgameQA (Kazemi et al., 2023), Sudoku (Noever and Burdick, 2021;Long, 2023;Ishay et al., 2023), Rubik's Cube (Noever and Burdick, 2021;Ding et al., 2023), Maze (Noever and Burdick, 2021), Crossword (Yao et al., 2023;Rozner et al., 2021;Efrat et al., 2021;Kulshreshtha et al., 2022), 8-puzzle (Ding et al., 2023), Game of 24 (Ding et al., 2023;Yao et al., 2023), Chess (Ishay et al., 2023;Feng et al., 2023b) Figure 2: A taxonomy of Puzzle Categories with the corresponding Datasets.</p>
<p>we exclude puzzles that cannot be expressed in text in any way, such as jigsaw puzzles (Markaki and Panagiotakis, 2022), or problems that require multimodal understanding abilities of LLMs (Chia et al., 2024;Ghosal et al., 2024).Mathematical puzzles are also excluded, as this area diligently covered by the recent work of Liu et al. (2023c).</p>
<p>We keep track of the latest progress in the field of puzzle solving using LLM reasoning in our GitHub https://puzzlellms.github.io/.</p>
<p>Categorization of Puzzle Problems</p>
<p>In assessing LLMs' reasoning capabilities, it is essential to categorize puzzles into coherent groups.We distinguish puzzles by their reliance on formal rules or broader world knowledge accompanied by general inferential skills, as illustrated in Figure 2.This categorization not only highlights the cognitive diversity puzzles present, but also aligns with distinct reasoning challenges: rule-based puzzles demand logical deduction and strategic foresight within closed environments with defined parameters, whereas rule-less puzzles require general reasoning abilities, interpreting situations and explaining events by drawing inferences based on practical knowledge about the everyday world.</p>
<p>By separating puzzles into these categories, we aim to provide a nuanced analysis of LLMs' problem-solving abilities, reflecting on both structured challenges and those necessitating broader inferential reasoning.</p>
<p>Rule-based Puzzles</p>
<p>Rule-based Puzzles provide the model with explicit victory conditions, legal move sets or state transition rules.We further subdivide this category based on whether the state transitions are deterministic or incorporate randomness.</p>
<p>Deterministic games always produce the same successor state given a current game state and action taken according to the rules.For example, in Chess, making a move always yields one unambiguous new board layout.Other examples include Sudoku, maze navigation, or solving a Rubik's cube.The model should learn strategies that operate within the possibility space defined by legal game mechanics.</p>
<p>Stochastic games incorporate randomness or hidden information, i.e. the same player action can lead to different probability distributions over next states.Examples include Minesweeper (hidden bomb locations) or card games e.g.Poker where opponents hold private hands.Mastering these games requires reasoning over uncertain states, planning multiple moves in advance and managing risk.</p>
<p>Thus, while both subgroups require logical reasoning bounded by formal rules, stochastic games pose the additional challenge of decision-making under uncertainty.Excelling in deterministic games enables pure reliance on deduction and forward search, while stochastic environments also require abilities for probabilistic inference, risk analysis, and reasoning with incomplete information.</p>
<p>Rule-less Puzzles</p>
<p>Unlike rule-bounded puzzles, rule-less problems rely more on flexible thinking and real-world knowledge to interpret vague situations and infer unobserved details.Rather than testing systematic search or strategic planning, these puzzles measure cognitive skills for contextual interpretation, conceptual combination, and reasoning from common experiences.The following fall under this category.</p>
<p>Riddles utilize clever wordplay and literary devices to conceal answers.For example, "What gets wetter the more it dries?"obscures the solution of "a towel" through metaphor.Solving riddles requires making abstract connections between concepts hidden in lyrical language.This assesses skills for fluid reasoning, conceptual blending, and lateral thinking to decode linguistic relationships.</p>
<p>Programming Puzzles provide code snippets and require analyzing or modifying the underlying program logic.Schuster et al. ( 2021) define a programming puzzle as a short Python program f , and the goal is to find an input which makes f return True.Such puzzles assess skills like tracing execution, fixing errors, or anticipating outputs based on coding semantics.For example, the following puzzle tests understanding programming semantics to predict a system's behaviour: def mystery (x ): return x // 2 print ( mystery (10))</p>
<p>Commonsense Reasoning Puzzles depict typical situations omitting key details.Solvers must explain events by inferring plausible implicit assumptions about motivations, causes and effects.For instance, the question "A man who was outside in the rain without an umbrella or hat didn't get a single hair on his head wet.Why?" requires pragmatic analysis of unstated contextual factors.</p>
<p>Methods and Strategies</p>
<p>In applying LLMs to puzzle solving, a wide array of methods and strategies enhances complex reasoning and performance.This section outlines the approaches used to address puzzles, aiming to highlight their application within this unique context.Given the extensive literature on prompt engineering and related methods Besta et al. (2024); Chen et al. (2023); Yu et al. (2023b); Chu et al. (2023); Qiao et al. (2022); Liu et al. (2021), we concentrate on the techniques most prevalent for puzzle solving, instead of describing each method separately.We divide existing methods into prompting techniques, neuro-symbolic approaches for puzzle translation and fine-tuning for specific domains.A detailed overview of the methods utilized across different puzzle categories is presented in Table 1.We also discuss how conventional methods have faced these problems before the LLM era (App.A.2).</p>
<p>Prompting Methods</p>
<p>Prompting strategies that provide intermediate reasoning steps are pivotal in enhancing the puzzlesolving capabilities of language models.The fewshot in-context learning paradigm offers one or more demonstrations within prompts, significantly improving performance for both rule-based and rule-less puzzles by showcasing the reasoning process without additional training (Brown et al., 2020;Dong et al., 2023;Zhou et al., 2022).</p>
<p>Recent works focus on how different 'thought structures' can guide LLMs to the final solution (Besta et al., 2024).</p>
<p>Chain topologies, which include Chain-of-Thought (CoT) (Wei et al., 2022;Kojima et al., 2022) have been applied to all kinds of puzzles, demonstrating their superiority over simple IO prompts.Self-Refine (Madaan et al., 2023) is used for the Game of 24 (rule-based/deterministic), outperforming CoT with a 13% higher success rate (Yao et al., 2023).Gu et al. (2023) use several methods in a rule-less detective-style benchmark, including Automatic CoT, which autonomously generates diverse reasoning chains for various questions (Zhang et al., 2022); Complexity CoT leverages the complexity of prompted chains, where more intricate reasoning steps often lead to improved performance in complex inference tasks by selecting outcomes that demonstrate deeper reasoning capabilities (Fu et al., 2022); and the Plan-and-Solve (PS) method, which uses two prompts for each problem-one to generate the reasoning process and the corresponding answer, and another to extract the final answer from the initial generation (Wang et al., 2023a).Despite the varied approaches, none of these methods clearly outperformed CoT across all tested LLMs.The best results are achieved by Detective Thinking Prompt, a CoT-like method introduced in the same study, which does not exceed the 61.6% accuracy score of the best model, GPT-4.The method encourages the model to consider and analyze multiple clues within a given scenario, sequentially building towards a conclusion.This type of prompting can help the model handle complex scenarios where synthesizing disparate information correctly is crucial to generating accurate and logical outcomes.Schuster et al. (2021) exclusively utilized the solutions to programming puzzles that the model had already solved as examples, surpassing alternative approaches.</p>
<p>Tree topologies cover a variety of methods.Self-Consistency (SC) (Wang et al., 2022) has been tested on rule-based/deterministic puzzles, such as the 8-puzzle, Game of 24 and Pocket Cube, as well as on rule-less commonsense reasoning puzzles, showcasing a small gain in the first category over CoT (Ding et al., 2023;Yao et al., 2023;Mo and Xin, 2023) and no clear benefit in the second one (Gu et al., 2023).Tree-of-Thought(s) (ToT) (Yao et al., 2023;Long, 2023) has been exclusively applied to rule-based/deterministic puzzles so far, achieving significantly improved success rates over CoT, with increases ranging from 26% (Mo and Xin, 2023) to 70% (Yao et al., 2023) depending on the puzzle and the depth of the tree, despite the increased LLM invocations (Ding et al., 2023).Tree-of-Uncertain-Thought (TouT) (Mo and Xin, 2023) achieved even better results than ToT on the same challenges, with a 9% higher success rate on the Game of 24 and 3% on mini-crosswords.Finally, Inference-Exclusion-Prompting (IEP) (Tong et al., 2023) employs a combination of forward and backward reasoning to approximate human logic and delivered some of the best results on riddles and commonsense puzzles when combined with CoT, scoring 82% on puzzles-up from 81% with zero-shot CoT-and 79% on riddles, compared to 82% with zero-shot CoT.</p>
<p>Graph topologies entail the following: Graphof-Thought(s) (GoT) (Besta et al., 2023;Lei et al., 2023) and Everything-of-Thought (XoT) (Ding et al., 2023) have been used to solve rulebased/deterministic puzzles.While GoT has shown poorer results compared to ToT, with a decrease ranging from 2% to 6% (Ding et al., 2023), XoT has been recognized as the most effective method for these puzzles.XoT integrates Monte Carlo Tree Search (MCTS) with LLMs for enhanced thought generation, achieving improvements in results from 53% to 69% compared to ToT.Additionally, XoT presents the fewest LLM invocations among the methods tested, including CoT, SC, ToT, and GoT.</p>
<p>A brief analysis of some basic methods not described here is presented in Appendix A.1, while a more detailed analysis of all the methods discussed can be found in the extensive work of Besta et al. (2024).Beyond the aforementioned methods, the use of extra information such as hints for riddles and commonsense puzzles, or introductions and summarizations of the puzzles, has also been employed.The inclusion of supplementary details appears to yield positive results, although this is not always the case; for instance, Chinese riddles typically show worse results when hints are used (Zhang and Wan, 2021).</p>
<p>Puzzle Translation</p>
<p>In this subsection, we summarize the neurosymbolic techniques used by LLMs to translate text puzzles from natural language into forms more amenable to solutions by external tools.Notably, these methods do not test the LLMs' puzzle solving capacity but rather assess their ability to encode puzzles into appropriate representations.</p>
<p>The primary approach involves using LLMs to generate logic rules from the puzzle's natural language and subsequently solve it using a symbolic solver.Ishay et al. (2023) employ GPT-3 and GPT-4 to transform logic puzzles, such as chess puzzles, Jobs puzzle and Sudoku (rule-based/deterministic) into Answer Set Programming (ASP) formats by generating predicates and rules.They demonstrate that this method achieved significant results, with GPT-4 scoring 92% accuracy in a logic puzzles dataset Mitra and Baral (2015), compared to 7% in few-shot and 21% in zero-shot settings with the same model.They note that in few-shot settings, LLMs can generate complex programs that humans can easily refine and correct in case of code errors.Additionally, similar frameworks such as Logic-LM (Pan et al., 2023a), LINC (Olausson et al., 2023) and Yang et al. (2023a)'s method show promising results in logical reasoning tasks, although not specifically in puzzle settings.</p>
<p>While neuro-symbolic approaches have been applied to puzzle translation into logic rules, we have found no studies on transforming puzzles from natural language into code.However, techniques such as Program of Thoughts (PoT) prompting (Chen et al., 2022) and Program-Aided Language (PAL) (Gao et al., 2022) employ models to convert reasoning into Python programs for logical and mathematical reasoning datasets.Therefore, we encourage the research community to explore these methods for puzzle-solving tasks as well.</p>
<p>Given the structured nature of rule-based puzzles, this approach is inherently suitable for them.</p>
<p>Consequently, it is logical that no studies have yet been conducted on rule-less puzzles in this context.</p>
<p>Fine-Tuning</p>
<p>Fine-tuning LLMs emerges as a potent strategy for enhancing their reasoning capabilities, ranging from general logical reasoning to specific puzzlesolving skills.</p>
<p>Logical Reasoning</p>
<p>LoGiPT (Feng et al., 2023a) is a language model fine-tuned to excel in logical reasoning tasks by mimicking the symbolic reasoning of logical solvers.The fine-tuning process involves constructing an instruction-tuning dataset comprising natural language (NL) logical questions paired with symbolic reasoning steps.It is fine-tuned to bypass syntax errors typically encountered in NL to symbolic language parsing, enabling it to directly produce answers.LogiT5 (Luo et al., 2023) leverages a multi-task learning approach, incorporating diverse datasets to enhance its reasoning capabilities across different logical domains.The model is fine-tuned on the LOGIGLUE benchmark, which includes various logical reasoning datasets, enabling it to perform better on tasks with limited data by transferring knowledge across tasks.</p>
<p>Rule-based Puzzles</p>
<p>In the domain of rule-based deterministic puzzles, Noever and Burdick (2021) observe suboptimal results when fine-tuning GPT-2 on Sudoku, Rubik's Cube and Mazes, potentially due to a brief finetuning period and limited training examples.Regarding crosswords, various studies (Rozner et al., 2021;Efrat et al., 2021) show mixed results, with some fine-tuned LLMs outperforming non-neural baselines and others not, highlighting the inherent challenge of cryptic crosswords for LLMs.Kazemi et al. ( 2023) demonstrate that fine-tuning LLMs with proofs and CoT under rule-based contexts yields some of the best results.</p>
<p>Rule-less Puzzles</p>
<p>In the realm of riddles, the study of Lin et al. (2021) illustrates that models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019) perform better when trained on both RiddleSense Lin et al. ( 2021) and Common-senseQA (Talmor et al., 2019) datasets, leveraging commonsense knowledge effectively.Moreover, Zhang and Wan (2021) report that combining fine-tuning on ALBERT-XXL with transfer learning from CommonsenseQA achieved the highest accuracy, noting a 4% improvement over simple fine-tuning.Lastly, the effectiveness of fine-tuning extends to commonsense reasoning (Del and Fishel, 2022) and programming puzzles (Schuster et al., 2021), showcasing its broad applicability across puzzle categories.</p>
<p>Datasets, Benchmarks and Tasks</p>
<p>Exploring diverse datasets, benchmarks, and tasks is crucial for evaluating LLMs in puzzle-solving.This section examines datasets within our puzzle taxonomy, encompassing formats, evaluation metrics, and methodologies.Figure 2 provides a detailed summary of datasets utilized across the taxonomy's categories, organized according to puzzle type.The analysis demonstrates LLMs' versatility and the impact of techniques discussed in §3.</p>
<p>Rule-based Puzzles</p>
<p>We explore rule-based puzzles to assess LLMs' understanding within structured, closed-world environments.This includes deterministic puzzles such as Sudoku, Rubik's Cube, Crosswords, and the 8puzzle, where solutions follow a set of defined rules.In contrast, stochastic games e.g.Minesweeper, card and social deduction games present variable outcomes from the same actions due to hidden factors.Research predominantly focuses on deterministic puzzles, highlighting a gap in addressing stochastic puzzle uncertainties-a promising direction for future research.</p>
<p>Deterministic Puzzles</p>
<p>Sudoku serves as a prime benchmark for LLMs due to its logical complexity.Noever and Burdick (2021) fine-tune GPT-2 (Radford et al., 2019) on 1M Sudoku games, experimenting with compact single-string format, with empty cells represented by "-", and posited that a matrix representation may enhance the model's learning efficacy.Long (2023) uses nested lists for puzzle representation1 , finding the Tree-of-Thought (ToT) method most effective, especially for smaller puzzles.Ishay et al. (2023) explore neuro-symbolic approaches across Sudoku, Jobs puzzles and logic puzzles, demonstrating that well-prompted LLMs can accurately generate answer set programming rules.For Rubik's Cube and Maze solvers, Noever and Burdick (2021) assess GPT-2's spatial reasoning using over 2,400 Rubik's Cube samples and 10K mazes.Despite limited fine-tuning and token constrains, GPT-2 successfully solved the Rubik's Cube in 1 out of 7 attempts, showing potential despite a high rate of valid though incorrect solutions.Ding et al. (2023) apply multiple methods such as CoT, Self-Consistency, and various Thoughts (ToT, GoT, XoT) on a 2×2×2 Rubik's Cube using GPT-3.5 and GPT-4.XoT with self-revision emerges as most accurate, significantly outperforming others with a 77.6% success rate.</p>
<p>Exploring LLM versatility, Ding et al. ( 2023) evaluate the effectiveness of XoT on the spatial 8-Puzzle and numerical Game of 24.The 8-Puzzle's goal configuration challenges are solved with a remarkable 93.2% accuracy across 419 puzzles using XoT with revision, showcasing superior efficiency over few-shot prompting and CoT.This high accuracy, coupled with a reduced number of LLM invocations, underscores the efficiency and potential of XoT in complex puzzle-solving contexts.</p>
<p>As for Crosswords, Rozner et al. ( 2021) and Efrat et al. (2021) fine-tune T5 models (Raffel et al., 2019) on extensive datasets of individual cryptic clues, revealing T5's advantage over traditional methods and highlighting areas for improvement, particularly with quick clues and specified answer lengths.Kulshreshtha et al. (2022)'s comparison of BART (Lewis et al., 2019) and T5 indicate a sub-30% accuracy for clue-answer tasks, with retrievalaugmented generation transformers surpassing finetuned LLMs.Additionally, Yao et al. (2023) apply 5-shot prompting and ToT to GPT-4 on Crossword puzzles significantly improving performance by solving 4 out of 20 puzzles and achieving a 60% word-level success rate.Feng et al. (2023b) fine-tune two models, "Chess-GPT" and "ChessCLIP," using a collection of 3.2M chess puzzles from the Lichess dataset 2 .Each puzzle in the dataset include annotations for its rating, theme, and solution.</p>
<p>At last, Kazemi et al. ( 2023) unveil BoardgameQA, a dataset featuring multi-choice questions against a backdrop of contradictory facts and rules.Models should navigate through these complexities to provide free-text answers.Their evaluation reveals that fine-tuning BERT-large and T5-XXL with proofs emerges as the most effective 2 https://lichess.org/method, contrary to few-shot prompting on PaLM with CoT.Moreover, the presence of extra or conflicting information decreases accuracy.</p>
<p>Stochastic Puzzles</p>
<p>The BoardgameQA benchmark (Kazemi et al., 2023) also explores scenarios with missing information, which fall under the stochastic puzzle category.It is shown that as missing information increases, the accuracy of fine-tuned models decreases.However, this heightened difficulty does not similarly impact the performance of prompttuned and few-shot learning methods, which is likely due to the larger models that were applied.</p>
<p>Minesweeper, known for its hidden information and unpredictability, exemplifies stochastic puzzles, requiring players to deduce mine locations from numerical clues, challenging spatial reasoning.Li et al. (2023) evaluated LLMs on Minesweeper, comparing table and coordinate representations.Even though GPT-3.5 displayed initial understanding, enhancements like few-shot prompting had minimal effects.Conversely, GPT-4 improved mine identification but struggled to complete boards, highlighting Minesweeper's role in evaluating LLMs' strategic thinking.Experiments favored the coordinate representation over the table format for aiding LLM comprehension.</p>
<p>Card games, notably Poker, exemplify stochastic puzzles where strategic skill is crucial.Simplified Poker variants require players to infer opponents' cards and calculate odds amidst hidden intentions.Gupta (2023) found that in Poker's preflop round, ChatGPT and GPT-4 grasp advanced strategies but do not reach Game Theory Optimal (GTO) play.ChatGPT leans towards a conservative approach, while GPT-4 exhibits more aggressive gameplay.Huang et al. (2024) leverage a Reinforcement Learning-trained OPT-1.3Bmodel on all Poker phases revealing superior outcomes in win rates and efficiency, ultimately showcasing LLMs' adeptness at complex strategies in stochastic settings.An agent that leverages GPT-4 (Guo et al., 2023) also achieves significant results in various imperfect information card games.</p>
<p>Social deduction games, including Werewolf and Avalon, blend logical reasoning with complex social dynamics, making them part of the broader stochastic puzzle domain.Such games challenge players to deduce roles involving unpredictable human behavior.Xu et al. (2023b) propose a Werewolf framework using LLMs without tuning, lever-aging historical interactions for strategic decisions and showcasing the models' ability in this context.Similarly, frameworks for Avalon (Wang et al., 2023b;Lan et al., 2023) show how LLMs can navigate scenarios demanding social manipulation and deduction, underscoring LLMs' proficiency in managing the complex interplay of logic and social interaction inherent in such games.</p>
<p>Rule-less Puzzles</p>
<p>This subsection delves into the diverse datasets related to rule-less puzzles, a category that predominantly encompasses riddles, programming puzzles, and commonsense reasoning challenges.Notably, we specifically focus on puzzles in their traditional sense, thereby excluding code generation datasets, which represent a distinct task type.A majority of rule-less puzzles are structured in a multiple-choice question-answering (QA) format, offering a standardized approach for evaluating LLMs' inferential reasoning.Benchmarks deviating from this format are specially mentioned, providing a broader perspective on the variety of rule-less puzzle datasets and their implications for LLM performance.</p>
<p>Riddles</p>
<p>RiddleSense (Lin et al., 2021) offers a collection of 5.7K vertical thinking riddles, testing pre-trained LMs such as BERT, RoBERTa, ALBERT, and textto-text QA models including UnifiedQA (Khashabi et al., 2020) and T5.Larger LMs generally demonstrate better performance, with UnifiedQA using T5-3B leading, yet struggling with metaphors and counterfactual situations.</p>
<p>Complementing this, BrainTeaser (Jiang et al., 2023) introduces 1119 lateral thinking puzzles.It contrasts instruction-based models (ChatGPT, T0, and FlanT5 (Chung et al., 2022)) with commonsense ones (including RoBERTa variants and CAR (Wang et al., 2023c)).ChatGPT excels in both sentence-based and word-based puzzles, indicating its strength in lateral thinking.However, overall, LLMs still face challenges in exhibiting lateral thinking, with common errors in memorization and commonsense association.This dataset highlights the varied dimensions of reasoning that riddles can test, from vertical logic to lateral inference.</p>
<p>BiRdQA (Zhang and Wan, 2021) explores the multilingual aspect of riddles, encompassing English and Chinese puzzles, while evaluating monolingual LMs (BERT, RoBERTa), as well as multilingual ones (mBERT, XLM-R (Conneau et al., 2019)).The use of brief riddle introductions and hints is also tested.Findings reveal a significant performance gap between LMs and human-level understanding, with monolingual models generally outperforming multilingual ones.Interestingly, additional context such as Wikipedia introductions and hints varied in effectiveness, with such aids benefiting English but not Chinese riddles.</p>
<p>CC-Riddle centers on 27K Chinese character riddles, involving multiple-choice, generative, and retrieval-based formats (Xu et al., 2022).Evaluation demonstrates that models encountered difficulties in comprehension and exhibited misunderstandings, revealing the complexities inherent in character-based riddles.</p>
<p>In contrast, PUZZLEQA (Zhao and Anderson, 2023) offers 558 word puzzles in multiple choice and free-text formats.Larger models, e.g.GPT-3/3.5 show higher accuracy, especially in multiplechoice settings.However, methods such as CoT combined with summarization do not significantly enhance performance, pointing to the ongoing challenges in free-response puzzle solving.</p>
<p>Finally, MARB (Tong et al., 2023) encompasses a variety of riddle tasks.Several methodologies including zero-shot, CoT, IEP, and few-shot prompting are tested on models such as GPT-4 and PaLM2-540B (Anil et al., 2023).The combination of IEP and CoT emerged as the most effective method, highlighting the value of integrating multiple approaches for diverse riddle types.The dataset also includes commonsense puzzles ( §4.2.3), showing similar trends with riddles.</p>
<p>Programming Puzzles</p>
<p>P3 (Python Programming Puzzles) (Schuster et al., 2021) offers a range of Python programming challenges, from straightforward string manipulations to complex tasks, such as the Tower of Hanoi and algorithmic puzzles, requiring from the model to find an input that makes the program f return "True".Models applied to these puzzles include enumerative solvers for building Abstract Syntax Trees and autoregressive Language Model Solvers such as GPT-3 and Codex (Chen et al., 2021), employing varied prompting techniques.The evaluation metric pass@k, indicates the models' ability to solve a puzzle within a given number of attempts (Chen et al., 2021).Results show a correlation between puzzle difficulty for both models and humans, with descriptive prompts enhancing model performance.Interestingly, models proficient in code completion solved more puzzles with fewer tries, highlighting the importance of specialized capabilities in programming challenges.</p>
<p>Savelka et al. ( 2023) introduce a dataset comprised of 530 code snippets from programming courses, presenting puzzles in a multiple-choice format.The distinction between questions with and without code snippets offers a unique perspective on LLMs' problem-solving strategies.The dataset categorizes questions into six types, including true/false and output prediction.GPT models were evaluated, revealing that code inclusion significantly increases puzzle complexity.Accuracy rates vary, with higher performance on completion-oriented questions, suggesting that LLMs' effectiveness can depend heavily on question format and content.</p>
<p>While both P3 and Programming Snippets Dataset address programming puzzles, they do so in markedly different ways.P3's focus on finding correct Python program inputs contrasts with the multiple-choice format of the Programming Snippets Dataset.However, both datasets reveal key insights: descriptive prompts aid problem-solving, and question format significantly influences LLM performance.</p>
<p>Commonsense Reasoning Puzzles</p>
<p>True Detective (Del and Fishel, 2022) presents detective puzzles in long-form stories, challenging LLMs such as GPT-3.5/4 to draw conclusions.Various methods, including CoT and Golden-CoT, are applied, revealing difficulties in making final inferences despite all necessary information being available.Golden-CoT provides the model with the reasoning behind the correct answer, so the model only needs to understand this reasoning and extract the answer.While Vanilla and CoT approaches perform close to random, Golden-CoT demonstrates significantly better accuracy, particularly with GPT-4.However, even with Golden-CoT, GPT-3.5 achieves a solve rate of only 63%, whereas GPT-4 matches human solver results (without access to the reasoning behind the answer).</p>
<p>DetectBench (Gu et al., 2023) containing 1200 questions, also evaluates informal reasoning in real-life contexts.It tests methods such as use of hints, various CoT approaches and detective thinking on models including GPT-4, GPT-3.5, GLM-4 and Llama2.Hints emerges as a powerful aid, with larger models generally outperforming smaller ones.The effectiveness of different approaches vary, with detective thinking effectively assisting most of the models.</p>
<p>Both datasets highlight the complexity of reallife reasoning and detective-style puzzles, demonstrating that hints play a crucial role in aiding both human and model performance.</p>
<p>LatEval (Huang et al., 2023b) introduces a conversational format with English and Chinese stories, requiring players to ask yes/no questions before providing an answer.GPT-3.5, GPT-4, and various other Chat models are evaluated on their ability to ask relevant questions and maintain consistency with the truth.Larger models do not necessarily show advanced performance in question relevance.However, GPT-4 demonstrates the highest answer consistency, though there is still significant room for improvement.The dataset emphasizes the importance of interactive and conversational reasoning in commonsense understanding.</p>
<p>PuzzTe (Szomiu and Groza, 2021), with its array of comparison, knights and knaves, and zebra puzzles, represents a potentially rich resource for LLM testing.</p>
<p>Despite not yet being applied to LLMs, its generated puzzle answers by Mace4 model finder and Prover9 theorem prover3 indicate its potential for future LLM evaluations.</p>
<p>The datasets under investigation demonstrate a variety of methods for evaluating commonsense reasoning in LLMs, ranging from detective-style puzzles to interactive story solving.Although larger models generally exhibit better performance, the complexity of these tasks poses significant challenges.Techniques such as sharing additional information through hints show effectiveness in improving outcomes, yet there remains a considerable gap between the performance of models and humans.It is important to note that in this work, we specifically focus on puzzle-oriented benchmarks, excluding general commonsense reasoning datasets e.g.CommonsenseQA, PIQA (Bisk et al., 2019) or StrategyQA (Geva et al., 2021).</p>
<p>Discussion and Future Directions</p>
<p>Applied Methods and Dataset Gaps: Across our puzzle taxonomy, methods such as few-shot prompting, CoT, use of introductions and finetuning are commonly employed across most categories.Rule-based deterministic and rule-less commonsense puzzles show the greatest methodological variety, while riddles are also see diverse approaches.In contrast, rule-based stochastic and rule-less programming puzzles exhibit less variety, likely due to fewer studies in these areas.Figure 2 reveals that a substantial number of datasets are available for rule-based deterministic puzzles, such as Sudoku and Rubik's Cube, as well as a variety of rule-less riddles.This indicates a strong research interest and resource availability in these domains.However, there appears to be a scarcity of datasets for rule-based stochastic puzzles and rule-less programming puzzles.This gap highlights an opportunity for further research and dataset creation to introduce more diverse challenges for advancing the problem-solving capabilities of LLMs.The lack of benchmarks for stochastic puzzles led us to include tasks like card and social deduction games, which share core characteristics with traditional puzzles involving incomplete information.Additionally, neuro-symbolic techniques that translate natural language into code remain notably underutilized in puzzle benchmarks, suggesting a potential area for future exploration.</p>
<p>Comparatively, prompting methods like CoT and ToT enhance complex reasoning abilities without altering the underlying model parameters, yet their effectiveness on smaller LLMs requires further exploration.Moreover, methods such as ToT or GoT necessitate a greater number of model invocations compared to CoT and XoT, which could impact their efficiency and scalability (Ding et al., 2023).Fine-tuning, while fundamentally enhances reasoning by altering model parameters, is constrained by its specificity to particular tasks.For instance, models fine-tuned on CSQA have been observed to perform worst on the RiddleSense dataset than models fine-tuned directly on RiddleSense (Lin et al., 2021), and a model fine-tuned on Poker games may not perform well on tasks like solving a Rubik's cube.Finally, puzzle translation methods primarily test the model's ability to interpret and rephrase problems rather than directly solve them, which tends to have a more limited impact on complex reasoning compared to the other two categories of methods.</p>
<p>Performance Analysis: Rule-based / Deterministic: Methods such as ToT and XoT ( § 3), typically enhance model reasoning abilities as the complexity of the structure increases (Ding et al., 2023).Yet, studies in BoardgameQA and crossword puzzles show generally poor model performance.Rule-based/Stochastic: Fine-tuning is prevalent here, enabling LLMs to grasp basic rules and sim-pler scenarios.However, they falter in complex settings that require extensive multi-step reasoning (Li et al., 2023).Rule-less/Riddles &amp; Commonsense: There is a notable performance gap between LLMs and human levels, with methods like CoT improving accuracy but still not matching human evaluation outcomes.Rule-less/Programming: LLMs find programming puzzles challenging, paralleling human difficulties (Schuster et al., 2021).Tasks involving code analysis and reasoning in multiple-choice formats prove particularly tough (Savelka et al., 2023).</p>
<p>Furthermore, the format of questions significantly affects puzzle-solving effectiveness.Multiple-choice setups simplify tasks for LLMs by narrowing the solution search space, while freetext formats increase the difficulty level.</p>
<p>Puzzle Generation research is currently limited, likely because the ability to understand and solve puzzles is a prerequisite for generating them.In our survey, we primarily focused on puzzle-solving.The few works we found in puzzle generation reveal mixed results.For instance, GPT-3.5'sattempts to generate puzzles with answers showed poor outcomes (Zhao and Anderson, 2023).Conversely, the introduction of ACES, an autotelic generation method for diverse programming puzzles, demonstrates how semantic descriptors produced by LLMs can be leveraged for creative puzzle creation (Pourcel et al., 2023).Lastly, there are recent works that have studied the generation of crossword puzzles of different languages, utilizing LLMs (Zugarini et al., 2024;Zeinalipour et al., 2023b,a).</p>
<p>Conclusion</p>
<p>In this survey, we propose a taxonomy of puzzles for evaluating LLMs, categorizing them into rulebased (deterministic and stochastic) and rule-less puzzles (riddles, programming, and commonsense reasoning puzzles).We explore a spectrum of methods for LLM-based puzzle solving, ranging from prompting techniques to neuro-symbolic strategies and fine-tuning.By collating existing datasets in this domain, we provide a comprehensive overview of the resources available for such evaluations.Our analysis identifies current challenges, revealing a difficulty of most methods to successfully solve puzzles, while we outline future directions, emphasizing the need for advanced methodologies and diverse datasets to enhance LLMs' proficiency in puzzle solving.</p>
<p>Limitations</p>
<p>In this study, we provide a survey of puzzle solving using reasoning of Large Language Models.Despite our best efforts, there may be still some limitations that remain in this paper.Firstly, due to the rapidly evolving nature of this field, we continuously add related approaches and analyses, but it is possible that some recent developments may not be included.Also, due to page constraints, we cannot extensively present all the methods nor provide all the technical details.This might limit the depth of understanding for some readers.Our review only includes methods within 4 years, primarily from sources such as ACL, EMNLP, NAACL, NeurIPS, ICLR, and arXiv.We plan to continue following these sources and adding new methods and datasets.Additionally, all our conclusions §6 are based on empirical analysis.While this provides robust evidence, it may not capture all aspects of the problem.Lastly, as with any survey, our interpretations and conclusions §5 are influenced by our own perspectives and understanding of the field.Other researchers might interpret the same studies differently.Despite these limitations, we believe this study provides a valuable overview of the current state of puzzle-solving using reasoning of Large Language Models.</p>
<p>A Appendix</p>
<p>A.1 Prompting Topologies</p>
<p>The chain-of-thought (CoT) paradigm involves step-wise explanatory reasoning chains, bolstering capabilities even in zero-shot settings with instructions such as "Let's think step-by-step" (Wei et al., 2022;Kojima et al., 2022).Complementing this, self-consistency generates multiple solution paths, selecting the most coherent one (Wang et al., 2022).</p>
<p>Exploring automated feedback, Pan et al. (2023b) examined self-correction within LLMs, noting its varied impact on logical reasoning.While instances of performance enhancement exist (Weng et al., 2022;Madaan et al., 2023), broader gains are often elusive, with some strategies even detracting from overall reasoning accuracy (Huang et al., 2023a).However, Tyen et al. (2023) highlight the potential of backtracking methods, which, when informed about the specific location of errors, significantly boost the model's correction abilities.</p>
<p>The Tree-of-Uncertain-Thought (TouT) prompting method structures problem-solving into a tree where each branch explores different uncertain reasoning pathways, allowing for multiple potential solutions (Mo and Xin, 2023).In contrast, the Tree-of-Thought(s)(ToT) method (Yao et al., 2023;Long, 2023) focuses on a more linear and deterministic approach, systematically breaking down problems into a single coherent pathway towards a solution.The Graph-of-Thought(s) (GoT) method (Besta et al., 2023;Lei et al., 2023) structures problem-solving by mapping out various interconnected reasoning pathways, allowing language models to explore and evaluate multiple solutions simultaneously within a flexible, network-like framework.</p>
<p>A.2 Conventional Methods</p>
<p>AI and Machine Learning methods have long been applied to puzzles and games, with algorithms like Deep Blue (Campbell et al., 2002) and AlphaZero (Silver et al., 2017) for Chess and Go, renowned for their exceptional results.This section contrasts "traditional" methods used to solve various puzzles with those derived from large language models (LLMs).Note that the aim of this paper isn't to determine the superior method for each puzzle, but to highlight the distinctive reasoning abilities of LLMs within diverse puzzle contexts.We particularly focus on rule-based puzzles, extensively addressed using conventional methods due to their structured, well-defined environments which require systematic strategies to achieve a solution.Conversely, rule-less puzzles such as riddles primarily test the logical, commonsense reasoning and creativity of models, without a clear path of steps to follow in order to find the solution, so we do not analyze this category.</p>
<p>Chi and Lange (2013) utilized three techniques to solve Sudoku: backtracking, simulated annealing, and alternating projections.The backtracking method, a brute-force depth-first search, consistently resolves puzzles across all difficulty levels, albeit slowly.Constraint programming transforms Sudoku into a constraint satisfaction problem, swiftly enforcing constraints to deduce solutions, often within milliseconds (Simonis, 2005).These methods always find a solution for Sudoku puzzle, in contrast with LLMs that have not achieved results better than 80% for 5x5 puzzles (Long, 2023).</p>
<p>In their study on Rubik's Cube, Chen (2022) employed several traditional methods including Korf's algorithm (Korf, 1997), which combines Iterative-Deepening Depth-First Search (IDDFS) with the A* algorithm and a heuristic search database.Both Thistlethwaite's4 and Kociemba's5 algorithms utilize group theory and similar search techniques to streamline the solving process, with Kociemba's version enhancing efficiency by simplifying the group structure.While all these algorithms effectively solve the Rubik's Cube-a task challenging for LLMs-Korf's method is particularly noted for its efficiency.Additionally, the study explored a machine learning strategy that integrates Monte-Carlo Tree Search (MCTS) with breadth-first search, yielding more optimized solutions, albeit at a lower efficiency.There have also been various attemts to solve Rubik's Cube using Reinforcement Learning (RL) like DeepCubeA (McAleer et al., 2018;Agostinelli et al., 2019) and others (Takano, 2023), which although find a solution in relatively few steps are time-consuming, with duration varying from 38.7 to 75.6 seconds (Takano, 2023).</p>
<p>Mazes are puzzles that can be solved by applying simple algorithms like depth-first search, A* or Trémaux's algorithm.However these problems are good for testing the spatial reasoning of LLMs.RL has also been utilized to solve mazes with (Barj and Sautory, 2024) leveraging LLM feedback during training.</p>
<p>In Ding et al. (2023) MCTS has been used to solve Game of 24, 8-Puzzle and Pocket Cube, achieving surpassing many LLM techniques, including CoT, CoT-SC, ToT and GoT.Additionally, Rozner et al. (2021) besides fine-tuning T5 for solving cryptic crosswords, have also used nonneural baselines including a WordNet-based heuristic model, a K-Nearest Neighbours bag of words model and a rule-based model, showing that the fine-tuning of T5 had the best results among them.</p>
<p>Finally, Studholme (2001) proposed a method for solving Minesweeper by considering it as a constraint satisfaction problem (CSP).The core strategy involves transforming the game's challenges into a set of logical constraints that must be satisfied to avoid mines effectively.</p>
<p>In conclusion, most conventional methods used to solve rule-based puzzles employ deterministic approaches that reliably produce solutions, in stark contrast to the unpredictable nature of LLMs.Another advantage of these traditional methods is their explainability and interpretability, crucial attributes for thoroughly evaluating algorithms and understanding their decision-making processes.However, as demonstrated in the study by Takano (2023), these methods can sometimes exhibit increased time complexity, indicating a potential trade-off between reliability and efficiency.</p>
<p>A.3 Tables</p>
<p>Table 1 delineates the various methods leveraged for puzzle-solving based on the datasets we have collected, illustrating the landscape of current LLM research in this domain.It particularly highlights the extensive methods applied to rule-based deterministic and rule-less commonsense puzzles.The absence of neuro-symbolic techniques and selection inference prompting indicates potential areas for expansion, especially considering their prospective benefits for LLMs grounded in logical reasoning datasets.The table further reflects the adaptability of certain methods like Chain-of-Thought, few-shot learning and fine-tuning, which are utilized across multiple puzzle types, hinting at their effectiveness.Based on this information, we not only catalogue the current state of method applications in puzzle-solving with LLMs but also highlight opportunities for innovative research in areas yet to be explored.</p>
<p>Figure 1 :
1
Figure 1: Riddle from RiddleSense (Lin et al., 2021).GPT-4, LLaMA2-70B and Bard chose the right answer.</p>
<p>Table 1 :
1
Methods used by each category of our taxonomy based on the puzzle benchmarks we collected
MethodsRule-based PuzzlesRule-less PuzzlesDeterministic Stochastic Riddles Programming CommonsensePrompting-----Few-shot✓✓✓✓✓Chain-of-Thought✓✓✓✓✓Self-refine✓Auto-CoT✓Complexity CoT✓Plan &amp; Solve✓Detective Thinking✓Self-Consistency✓✓Tree-of-Thoughts✓Tree-of-uncertain-Thoughts✓Inferential Exclusion Prompting✓✓Graph-of-Thoughts✓Everything-of-thoughts✓Hints✓✓Introduction/Summarization✓✓✓✓✓Puzzle Translation-----Logic✓CodeFine-Tuning✓✓✓✓✓
e.g. [[3,<em>,</em>,2], [1,<em>,3,</em>],[<em>,1,</em>,3],[4,<em>,</em>,1]]
https://www.cs.unm.edu/ mccune/prover9/
https://www.jaapsch.net/puzzles/thistle.htm
https://kociemba.org/</p>
<p>Solving the rubik's cube with deep reinforcement learning and search. Forest Agostinelli, Stephen Marcus Mcaleer, Alexander Shmakov, Pierre Baldi, Nature Machine Intelligence. 12019</p>
<p>. Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z Chen, Eric Chu, J Clark, Laurent El Shafey, Yanping Huang, Kathleen S Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A Botha, James Bradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin Cherry, Christopher A Choquette-Choo, Aakanksha Chowdhery, Shachi Crépy, Mostafa Dave, Sunipa Dehghani, Jacob Dev, M C Devlin, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag, Xavier García, Sebastian Gehrmann, Lucas González, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li, Yaguang Li, Jun , Yu Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Daniel Smilkov, David R So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, ArXiv, abs/2305.10403Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Oleksandr Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone,Slav Petrovand Yonghui Wu. 2023. Palm 2 technical report</p>
<p>A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V Do, Yan Xu, Pascale Fung, ArXiv, abs/2302.040232023</p>
<p>A systematic evaluation of large language models on out-of-distribution logical reasoning tasks. Qiming Bao, Gaël Gendron, Alex Yuxuan Peng, Wanjun Zhong, Neşet Özkan Tan, Yang Chen, Michael Witbrock, Jiamou Liu, ArXiv, abs/2310.094302023</p>
<p>Reinforcement learning from llm feedback to counteract goal misgeneralization. Houda Nait El Barj and Theophile Sautory2024</p>
<p>Graph of thoughts: Solving elaborate problems with large language models. Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, 2023Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler</p>
<p>Demystifying chains, trees, and graphs of thoughts. Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwaśniewski, Jürgen Müller, Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Aidan O 'mahony, Onur Mutlu, Torsten Hoefler, 2024</p>
<p>Piqa: Reasoning about physical commonsense in natural language. Yonatan Bisk, Rowan Zellers, Le Ronan, Jianfeng Bras, Yejin Gao, Choi, ArXiv, abs/1911.116412019</p>
<p>. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T J Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack</p>
<p>Language models are few-shot learners. Christopher Clark, Sam Berner, Alec Mccandlish, Ilya Radford, Dario Sutskever, Amodei, ArXiv, abs/2005.141652020</p>
<p>Deep blue. Murray Campbell, A Joseph Hoane, Feng Hsu, 10.1016/S0004-3702(01)00129-1Artificial Intelligence. 13412002</p>
<p>Unleashing the potential of prompt engineering in large language models: a comprehensive review. Banghao Chen, Zhaofeng Zhang, ArXiv, abs/2310.147352023Nicolas Langren'e, and Shengxin Zhu</p>
<p>Different algorithms to solve a rubik's cube. Juntao Chen, 10.1088/1742-6596/2386/1/012018Journal of Physics: Conference Series. 23861120182022</p>
<p>. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W Cummings, Matthias Plappert, Fotios Chantzis, ArXiv, abs/2107.03374Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech ZarembaJan Leike. 2021Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew CarrEvaluating large language models trained on code</p>
<p>Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Wenhu Chen, Xueguang Ma, Xinyi Wang, William W Cohen, ArXiv, abs/2211.12588Eric C. Chi and Kenneth Lange2022. 2013Techniques for solving sudoku puzzles</p>
<p>Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns. Ken Yew, Vernon Chia, Yan Toh, Deepanway Han, Lidong Ghosal, Soujanya Bing, Poria, 2024</p>
<p>A survey of chain of thought reasoning: Advances, frontiers and future. Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Tao He, Haotian Wang, Weihua Peng, Ming Liu, Bing Qin, Ting Liu, ArXiv, abs/2309.154022023</p>
<p>Scaling instruction-finetuned language models. Chung Hyung Won, Le Hou, S Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Dasha Chowdhery, Sharan Valter, Gaurav Narang, Adams Wei Mishra, Vincent Yu, Yanping Zhao, Andrew M Huang, Hongkun Dai, Yu, ArXiv, abs/2210.11416Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Slav Petrov</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Annual Meeting of the. Association for Computational Linguistics2019</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, ArXiv, abs/2205.097122022</p>
<p>True detective: A deep abductive reasoning benchmark undoable for gpt-3 and challenging for gpt-4. Maksym Del, Mark Fishel, STARSEM. 2022</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Everything of thoughts: Defying the law of penrose triangle for thought generation. Ruomeng Ding, Chaoyun Zhang, Lu Wang, Yong Xu, Ming-Jie Ma, Wei Zhang, Si Qin, S Rajmohan, Qingwei Lin, Dongmei Zhang, ArXiv, abs/2311.042542023</p>
<p>. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, Zhifang Sui, 2023A survey on in-context learning</p>
<p>Cryptonite: A cryptic crossword benchmark for extreme ambiguity in language. Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy, ArXiv, abs/2103.012422021</p>
<p>Language models can be logical solvers. Jiazhan Feng, Ruochen Xu, Junheng Hao, Hiteshi Sharma, Yelong Shen, Dongyan Zhao, Weizhu Chen, ArXiv, abs/2311.061582023a</p>
<p>Xidong Feng, Yicheng Luo, Ziyan Wang, Hongrui Tang, Mengyue Yang, Kun Shao, David Henry Mguni, Yali Du, ArXiv, abs/2306.09200Chessgpt: Bridging policy learning and language modeling. Jun Wang. 2023b</p>
<p>Peter A Flach, Antonis C Kakas, Abductive and inductive reasoning: background and issues. 2000</p>
<p>Complexity-based prompting for multi-step reasoning. Yao Fu, Hao-Chun, Ashish Peng, Peter Sabharwal, Tushar Clark, Khot, ArXiv, abs/2210.007202022</p>
<p>Pal: Program-aided language models. Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, Graham Neubig, ArXiv, abs/2211.104352022</p>
<p>Are language models puzzle prodigies? algorithmic puzzles unveil serious challenges in multimodal reasoning. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, and Soujanya Poria92021. 2024Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies</p>
<p>Zhouhong Gu, Zihan Li, Lin Zhang, Zhuozhi Xiong, Sihang Jiang, Xiaoxuan Zhu, Shusen Wang, Zili Wang, Jianchen Wang, Haoning Ye, Wenhao Huang, Yikai Zhang, Hongwei Feng, Yanghua Xiao, Go beyond the obvious: Probing the gap of informal reasoning ability between humanity and llms by detective reasoning puzzle benchmark. 2023</p>
<p>Suspicion-agent: Playing imperfect information games with theory of mind aware gpt-4. Jiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen Lin, Yusuke Iwasawa, Yutaka Matsuo, ArXiv, abs/2309.172772023</p>
<p>Are chatgpt and gpt-4 good poker players? -a pre-flop analysis. Akshat Gupta, ArXiv, abs/2308.124662023</p>
<p>Pokergpt: An end-to-end lightweight solver for multi-player texas hold'em via large language model. Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, Yanru Zhang, ArXiv, abs/2401.067812024</p>
<p>Towards reasoning in large language models: A survey. Jie Huang, Kevin Chen, -Chuan Chang, ArXiv, abs/2212.104032022</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, ArXiv, abs/2310.017982023a</p>
<p>Lateval: An interactive llms evaluation benchmark with incomplete information from lateral thinking puzzles. Shulin Huang, Shirong Ma, Yinghui Li, Mengzuo Huang, Wuhe Zou, Weidong Zhang, Haitao Zheng, ArXiv, abs/2308.108552023b</p>
<p>Leveraging large language models to generate answer set programs. Adam Ishay, Zhun Yang, Joohyung Lee, ArXiv, abs/2307.076992023</p>
<p>Brainteaser: Lateral thinking puzzles for large language models. Yifan Jiang, Filip Ilievski, Kaixin Ma ; Mehran, Quan Kazemi, Deepti Yuan, Najoung Bhatia, Xin Kim, Xu, ArXiv, abs/2306.07934Vaiva Imbrasaite, and Deepak Ramachandran. 2023. Boardgameqa: A dataset for natural language reasoning with contradictory information. 2023Conference on Empirical Methods in Natural Language Processing</p>
<p>Unifiedqa: Crossing format boundaries with a single qa system. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, Findings. 2020</p>
<p>Large language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, ArXiv, abs/2205.119162022</p>
<p>Finding optimal solutions to rubik's cube using pattern databases. Richard E Korf, AAAI/IAAI. 1997</p>
<p>Down and across: Introducing crossword-solving as a new nlp benchmark. Saurabh Kulshreshtha, Olga Kovaleva, Namrata Shivagunde, Anna Rumshisky, ArXiv, abs/2205.104422022</p>
<p>Llm-based agent society investigation: Collaboration and confrontation in avalon gameplay. Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, De-Yong Ye, Peilin Zhao, Ee-Peng Lim, Hui Xiong, Hao Wang, ArXiv, abs/2310.149852023</p>
<p>Albert: A lite bert for self-supervised learning of language representations. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ArXiv, abs/1909.119422019</p>
<p>Boosting logical reasoning in large language models through a new framework: The graph of thought. Bin Lei, Pei-Hung Lin, Chunhua Liao, Caiwen Ding, ArXiv, abs/2308.086142023</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Omer Abdel Rahman Mohamed, Veselin Levy, Luke Stoyanov, Zettlemoyer, Annual Meeting of the Association for Computational Linguistics. Bart2019</p>
<p>Assessing logical puzzle solving in large language models: Insights from a minesweeper case study. Yinghao Li, Haorui Wang, Chao Zhang, ArXiv, abs/2311.073872023</p>
<p>Riddlesense: Reasoning about riddle questions featuring linguistic creativity and commonsense knowledge. Ziyi Bill Yuchen Lin, Yichi Wu, Dong-Ho Yang, Xiang Lee, Ren, 2021In Findings</p>
<p>Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, Yuexin Zhang, ArXiv, abs/2304.03439Evaluating the logical reasoning ability of chatgpt and gpt-4. 2023a</p>
<p>Glore: Evaluating logical reasoning of large language models. Hanmeng Liu, Zhiyang Teng, Ruoxi Ning, Jian Liu, Qiji Zhou, Yuexin Zhang, ArXiv, abs/2310.091072023b</p>
<p>Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, ACM Computing Surveys. 552021</p>
<p>Wentao Liu, Hanglei Hu, Jie Zhou, Yuyang Ding, Junsong Li, Jiayi Zeng, Mengliang He, Qin Chen, Bo Jiang, Aimin Zhou, and Liang He. 2023c. Mathematical language models: A survey. </p>
<p>Roberta: A robustly optimized bert pretraining approach. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, ArXiv, abs/1907.116922019</p>
<p>Large language model guided tree-ofthought. Jieyi Long, ArXiv, abs/2305.082912023</p>
<p>Towards logiglue: A brief survey and a benchmark for analyzing logical reasoning capabilities of language models. Man Luo, Shrinidhi Kumbhar, Ming Shen, Mihir Parmar, Neeraj Varshney, Pratyay Banerjee, Somak Aditya, Chitta Baral, ArXiv, abs/2310.008362023</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, ArXiv, abs/2303.176512023</p>
<p>Jigsaw puzzle solving techniques and applications: a survey. The Visual Computer. Smaragda Markaki, Costas Panagiotakis, 202239</p>
<p>Solving the rubik's cube without human knowledge. Stephen Mcaleer, Forest Agostinelli, Alexander Shmakov, Pierre Baldi, 2018</p>
<p>Learning to automatically solve logic grid puzzles. Arindam Mitra, Chitta Baral, Conference on Empirical Methods in Natural Language Processing. 2015</p>
<p>Tree of uncertain thoughts reasoning for large language models. Shentong Mo, Miao Xin, ArXiv, abs/2309.076942023</p>
<p>Puzzle solving without search or human knowledge: An unnatural language approach. David A Noever, Ryerson Burdick, ArXiv, abs/2109.027972021</p>
<p>Linc: A neurosymbolic approach for logical reasoning by combining language models with first-order logic provers. Theo X Olausson, Alex Gu, Benjamin Lipkin, Cedegao Zhang, Armando Solar-Lezama, Josh Tenenbaum, Roger Levy, ; Openai, : , Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, ; , William Yang, Wang , ArXiv, abs/2305.12295Conference on Empirical Methods in Natural Language Processing. Tim Brooks, Miles Brundage, Kevin Button, Trevor Liangming Pan, Alon Albalak, Xinyi Wang,2023. 2023aLogic-lm: Empowering large language models with symbolic solvers for faithful logical reasoning</p>
<p>Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. Liangming Pan, Wenda Michael Stephen Saxon, Deepak Xu, Xinyi Nathani, William Wang, Wang Yang, ArXiv, abs/2308.031882023b</p>
<p>Aces: Generating diverse programming puzzles with autotelic language models and semantic descriptors. Julien Pourcel, Cédric Colas, Pierre-Yves Oudeyer, Laetitia Teodorescu, ArXiv, abs/2310.106922023</p>
<p>Reasoning with language model prompting: A survey. Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, Huajun Chen, ArXiv, abs/2212.095972022</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam M Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, J. Mach. Learn. Res. 21672019</p>
<p>Decrypting cryptic crosswords: Semantically complex wordplay puzzles as a target for nlp. Josh Rozner, Christopher Potts, Kyle Mahowald, ArXiv, abs/2104.086202021</p>
<p>Testing the general deductive reasoning capacity of large language models using ood examples. Abulhair Saparov, Richard Yuanzhe Pang, Vishakh Padmakumar, Nitish Joshi, Seyed Mehran Kazemi, Najoung Kim, He He, ArXiv, abs/2106.05784Tal Schuster, A. Kalyan, Oleksandr Polozov, and Adam Tauman Kalai2023. 2023. 2021Programming puzzles</p>
<p>Mastering chess and shogi by self-play with a general reinforcement learning algorithm. David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis, 2017</p>
<p>Sudoku as a constraint problem. Helmut Simonis, 2005</p>
<p>Minesweeper as a constraint satisfaction problem. Chris Studholme, 2001</p>
<p>A puzzlebased dataset for natural language inference. Roxana Szomiu, Adrian Groza, ArXiv, abs/2112.057422021</p>
<p>Self-supervision is all you need for solving rubik's cube. Kyo Takano, 2023</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, ArXiv, abs/1811.009372019</p>
<p>Eliminating reasoning via inferring with planning: A new framework to guide llms' non-linear thinking. Yongqi Tong, Yifan Wang, Dawei Li, Sizhe Wang, Zi Lin, Simeng Han, Jingbo Shang, ArXiv, abs/2310.123422023</p>
<p>Llms cannot find reasoning errors, but can correct them. Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, Victor Carbune, ArXiv, abs/2311.085162023</p>
<p>Plan-and-solve prompting: Improving zeroshot chain-of-thought reasoning by large language models. Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy , Ka-Wei Lee, Ee-Peng Lim, Annual Meeting of the Association for Computational Linguistics. 2023a</p>
<p>Avalon's game of thoughts: Battle against deception through recursive contemplation. Shenzhi Wang, Chang Liu, Zilong Zheng, Siyuan Qi, Shuo Chen, Qisen Yang, Andrew Zhao, Chaofei Wang, Shiji Song, Gao Huang, ArXiv, abs/2310.013202023b</p>
<p>Car: Conceptualization-augmented reasoner for zero-shot commonsense question answering. Weiqi Wang, Tianqing Fang, Wenxuan Ding, Baixuan Xu, Xin Liu, Yangqiu Song, Antoine Bosselut, Conference on Empirical Methods in Natural Language Processing. 2023c</p>
<p>Selfconsistency improves chain of thought reasoning in language models. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai Hsin, Chi , Denny Zhou, ArXiv, abs/2203.111712022</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai Hsin Chi, F Xia, Quoc Le, Denny Zhou, ArXiv, abs/2201.119032022</p>
<p>Large language models are better reasoners with self-verification. Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Kang Liu, Jun Zhao, Conference on Empirical Methods in Natural Language Processing. 2022</p>
<p>Ccriddle: A question answering dataset of chinese character riddles. Fan Xu, Yunxiang Zhang, Xiao-Yi Wan, ArXiv, abs/2206.137782022</p>
<p>Are large language models really good logical reasoners? a comprehensive evaluation and beyond. Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023a</p>
<p>Exploring large language models for communication games: An empirical study on werewolf. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, Yang Liu, ArXiv, abs/2309.046582023b</p>
<p>Neuro-symbolic integration brings causal and reliable reasoning proofs. Sen Yang, Xin Li, Leyang Cui, Li Bing, Wai Lam, ArXiv, abs/2311.098022023a</p>
<p>Logical reasoning over natural language as knowledge representation: A survey. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, E Cambria, ArXiv, abs/2303.120232023b</p>
<p>Tree of thoughts: Deliberate problem solving with large language models. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, Karthik Narasimhan, ArXiv, abs/2305.106012023</p>
<p>Natural language reasoning, a survey. Fei Yu, Hongbo Zhang, Prayag Tiwari, Benyou Wang, 2023a</p>
<p>Towards better chain-ofthought prompting strategies: A survey. Zihan Yu, Liang He, Zhen Wu, Xinyu Dai, Jiajun Chen, ArXiv, abs/2310.049592023b</p>
<p>Kamyar Zeinalipour, Asya Tommaso Laquinta, Giovanni Zanollo, Leonardo Angelini, Marco Rigutini, Marco Maggini, Gori, Italian crossword generator: Enhancing education through interactive word puzzles. 2023a</p>
<p>Arabicros: Ai-powered arabic crossword puzzle generation for educational applications. Kamyar Zeinalipour, Mohamed Saad, Marco Maggini, Marco Gori, 10.18653/v1/2023.arabicnlp-1.23Proceedings of ArabicNLP 2023. ArabicNLP 2023Association for Computational Linguistics2023b</p>
<p>Birdqa: A bilingual dataset for question answering on tricky riddles. Yunxiang Zhang, Xiaojun Wan, ArXiv, abs/2109.110872021</p>
<p>Automatic chain of thought prompting in large language models. Zhuosheng Zhang, Aston Zhang, Mu Li, Alexander J Smola, ArXiv, abs/2210.03493Solving and generating npr sunday puzzles with large language models. Carolyn Jane, Anderson , 2022. 2023</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, ArXiv, abs/2211.019102022</p>
<p>Clue-instruct: Text-based clue generation for educational crossword puzzles. Andrea Zugarini, Kamyar Zeinalipour, Sai Surya, Marco Kadali, Marco Maggini, Leonardo Gori, Rigutini, 2024</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-757 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-757</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-757</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-20.html">extraction-schema-20</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <p><strong>Paper ID:</strong> paper-273482615</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.14375v2.pdf" target="_blank">Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts</a></p>
                <p><strong>Paper Abstract:</strong> Adapting to latent-confounded shifts remains a core challenge in modern AI. These shifts are propagated via latent variables that induce spurious, non-transportable correlations between inputs and labels. One practical failure mode arises when fine-tuning pre-trained foundation models on confounded data (e.g., where certain text tokens or image backgrounds spuriously correlate with the label), leaving models vulnerable at deployment. We frame causal fine-tuning as an identification problem and pose an explicit causal model that decomposes inputs into low-level spurious features and high-level causal representations. Under this family of models, we formalize the assumptions required for identification. Using pre-trained language models as a case study, we show how identifying and adjusting these components during causal fine-tuning enables automatic adaptation to latent-confounded shifts at test time. Experiments on semi-synthetic benchmarks derived from real-world problems demonstrate that our method outperforms black-box domain generalization baselines, illustrating the benefits of explicitly modeling causal structure.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e757.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e757.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CFT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal Fine-Tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end fine-tuning pipeline that decomposes input representations into pre-trained (R0), fine-tuned (R1) and local low-level (Φ) components, learns an invariant causal representation C from R0 and R1, and computes p(y|do(x)) via a front-door style adjustment implemented by shuffling Φ and marginalizing over local-feature samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Causal Fine-Tuning (CFT)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>CFT keeps a frozen copy of a pre-trained encoder to produce R0 and trains another copy to produce R1 via supervised fine-tuning; it extracts local low-level features Φ from early layers, learns an invariant causal latent C by aligning p(c|r0) and p(c|r1) with an L2 invariance term plus entropy regularizers, and implements a front-door-like causal adjustment for prediction by sampling/shuffling Φ within batches to estimate p(y|do(x)) = E_{x',Φ'}[p(y|Φ',c) p(Φ'|x') p(x')], where c = f(r0,r1). The algorithm is trained end-to-end and used in inference by sampling K shuffled Φ' and marginalizing predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Semi-synthetic sentiment simulators (Amazon/Yelp injections)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Controlled semi-synthetic simulator built from real Amazon and Yelp review corpora where spurious correlations are injected (stop-word suffixes or platform suffixes appended to tokens) to create train ID and OOD test regimes; not an interactive/online lab (no active interventions), but allows controlled, repeatable distribution shifts for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Representation decomposition + front-door adjustment: identify invariant causal factor C (representation alignment across R0 and R1 with entropy regularization) and block spurious collider paths by marginalizing over shuffled local features Φ to emulate do(x); uses pre-trained model as an implicit second 'environment' for contrastive identification.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders creating spurious correlations (unstable spurious features S1 affecting high-level representation R1 and local features Φ), local irrelevant tokens (stop-word suffixes) and platform-identifiers (injected strings), measurement-level spurious style features.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Implicit detection via representation shift: the method detects unstable/spurious signal by comparing R0 (pretrained) and R1 (fine-tuned) and forcing p(c|r0) ≈ p(c|r1) while retaining entropy—large mismatches indicate spurious high-level changes; no explicit statistical test is introduced beyond the alignment loss.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Front-door style adjustment by shuffling Φ and marginalizing over sampled Φ' during estimation of p(y|do(x)), effectively removing the influence of environment-specific Φ realizations; also uses maximum-entropy default for unknown confounder distributions to avoid over-reliance on train-set-conditioned signals.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Emulates interventions (do(x)) by randomizing Φ within the mini-batch (breaking spurious collider paths) and training p(y|Φ,c) on shuffled Φ' to ensure predictions rely on c (invariant) rather than Φ; this acts as an empirical refutation test: predictors that condition on Φ (CFT-N) degrade OOD.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>CFT consistently outperforms standard fine-tuning baselines in the paper's semi-synthetic experiments: e.g., under a strong spurious training regime (90% spurious) CFT reported F1 ≈ 98.69 vs SFT ≈ 95.96 (absolute +2.73); as OOD spurious correlation weakens, CFT shows larger absolute gains (examples in tables show gains up to ≈ +9.16 at extreme OOD settings).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Standard supervised fine-tuning (SFT) and frozen-representation classifier (SFT0) suffer substantial drops under confounded OOD: example SFT values from the same settings: 95.96 (90% spurious), 92.89 (70%), 81.89 (50%), 71.20 (30%), 60.23 (10%) F1 in the reported tables.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposing representations into R0 (pretrained), R1 (fine-tuned) and local Φ and explicitly learning an invariant causal representation C enables construction of a front-door-style adjustment that substantially improves OOD robustness under latent-confounded shifts. Conditioning directly on Φ (CFT-N) exposes active collider paths and harms OOD generalization; using only C (CFT-C) is often strong but less adaptive when shifts are extreme; shuffling Φ and marginalizing reduces variance with more samples and stabilizes OOD performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e757.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e757.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Φ-shuffle</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local-feature shuffling (Φ-shuffle) for emulating do(x)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A computational trick used inside CFT that approximates an interventional distribution by randomly shuffling local low-level features Φ within mini-batches (and sampling multiple Φ') to break spurious associations and estimate p(y|do(x)).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Local-feature shuffling (Φ-shuffle)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Extract local low-level features Φ from early/frozen layers (patch-averaged token embeddings), then during training and inference shuffle Φ across examples within a mini-batch to create Φ' samples that are marginally distributed according to p(Φ'|x') and use these to compute p(y|do(x)) via p(y|Φ',c). Multiple shuffled samples (K) are drawn at inference and marginalized to reduce variance.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Semi-synthetic sentiment simulators (Amazon/Yelp injections)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Controlled, non-interactive simulator; shuffling is performed offline within batches rather than by performing real interventions in an interactive environment.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Directly severs the active collider path through Φ by randomizing Φ, preventing the model from learning conditional dependencies that arise from environment-specific Φ realizations.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Local measurement-level spurious signals and style tokens (injected suffixes), environment-specific local features that act as colliders.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>No explicit detector; effectiveness inferred empirically by comparing performance when shuffling is applied vs when Φ is conditioned (CFT vs CFT-N), and via variance reduction as K increases.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Shuffling removes instance-specific influence of Φ from the conditional used for prediction, thereby downweighting environment-specific spurious cues in the predictive distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Acts as an empirical refutation/intervention: if performance improves after Φ-shuffling (and marginalization), this supports that previously learned correlations with Φ were spurious.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Using multiple Φ samples at inference reduces variance and improves OOD performance; paper shows that increasing K (e.g., from 1 to 20) reduces variance and aids robustness (visualized in Fig.9). Exact F1 gains depend on setting but were essential for CFT's superior OOD scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Conditioning on Φ (CFT-N) yields worse OOD performance (tables show CFT-N drops relative to CFT), demonstrating the necessity of shuffling for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Randomizing Φ within batches is an effective and simple operationalization of an interventional approach that removes spurious local-feature influence; empirically necessary for CFT's OOD gains and reduces variance with larger sample counts at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e757.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e757.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Invariant C learning (LC)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Invariant causal feature learning via cross-view alignment (LC loss)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A training objective that identifies an invariant causal latent C by aligning the conditional distributions p(c|r0) and p(c|r1) while maximizing entropy to avoid representational collapse.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Invariant C learning (alignment + entropy loss)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>The loss L_C = E_{(r0,r1)} ||p(c|r0) - p(c|r1)||_2^2 - H(p(c|r0)) - H(p(c|r1)) forces agreement between causal representations inferred from the pre-trained view R0 and the supervised fine-tuned view R1, and encourages high-entropy marginals to prevent trivial collapsed solutions; C is then used in the front-door adjustment to produce invariant predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Semi-synthetic sentiment simulators (Amazon/Yelp injections)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Non-interactive, paired-view setup where R0 comes from frozen pre-trained encoder and R1 from supervised fine-tuning on same x; the paired views act as two implicit environments for identification.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Detects and separates invariant causal signal from spurious high-level changes by forcing agreement across views (R0 vs R1); entropy terms ensure useful representation capacity rather than collapse to constants.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>High-level spurious shifts (S1) that affect R1 but not R0, and other unstable representation-level artifacts introduced during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Detection via representation mismatch: large ||p(c|r0)-p(c|r1)|| indicates presence of unstable/spurious components; minimization of this term identifies invariant subspace.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>By projecting onto the learned invariant C, downstream predictors are less reliant on R1-specific spurious components; combined with Φ-shuffling this reduces spurious influence.</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>No explicit statistical refutation test, but ablations (CFT vs CFT-C vs CFT-N) show that relying on C (and aligning it) leads to better OOD performance, serving as empirical refutation of spurious reliance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Using C alone (CFT-C) yields strong OOD performance and often close to full CFT in milder shifts (tables show CFT-C F1 often near CFT for some settings), but full CFT (with Φ adjustment) is better in many OOD extremes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>No explicit baseline that learns C without alignment is provided; standard SFT which does not separate C shows clear OOD degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The cross-view alignment loss is central to identifying a stable causal representation C from single-domain fine-tuning data by leveraging a frozen pre-trained encoder as a second implicit view; it meaningfully separates invariant from spurious high-level signals and is a key component for downstream robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e757.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e757.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Front-door adj.</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Front-door adjustment (operationalized in CFT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A causal adjustment strategy that identifies causal effect through an intermediate mediator when back-door adjustment is not possible due to hidden confounding; in this work it is operationalized by learning mediator C and marginalizing over randomized local features Φ.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Front-door adjustment (operationalized via C and Φ)</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>Theory: when Φ's effect on Y is fully mediated by C (a front-door structure), p(y|do(x)) can be computed by integrating p(y|Φ,c)p(Φ|x')p(x'); practice: learn C from paired R0/R1 and estimate p(y|Φ,c) by training on shuffled Φ' (breaking confounding), then marginalize over Φ' samples to produce interventional predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Semi-synthetic sentiment simulators (Amazon/Yelp injections)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Controlled offline simulation used to estimate interventional distributions via sampled/randomized local features rather than real randomized experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Enables identification of causal effect despite hidden confounding between Φ and Y by using mediator C and randomized Φ' to block back-door confounding and remove spurious environment-specific influence.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Hidden confounders between input and label that render p(y|x) non-transportable; environment-specific spurious features altering p(u|x;σ).</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td>Relies on structural assumptions (Assumption 4.4: Φ → Y effect fully mediated by C) and empirical checks via ablations rather than an automatic statistical detector.</td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>Marginalization over randomized Φ' reduces dependence on environment-specific Φ and uses p(y|c) (or p(y|Φ',c)) as interventional predictor; combined with maximum-entropy prior for unknown confounder distributions to avoid overfitting to train-conditioned p(uΦ|·).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td>Comparing predictions under observational conditioning vs the front-door adjusted estimator and showing OOD gains acts as empirical refutation of spurious observational associations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td>Front-door operationalization in CFT yields substantial OOD improvements relative to observational predictors; specific reported F1 improvements (see CFT numbers) show front-door based adjustment is effective in the semi-synthetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td>Observational predictors (e.g., CFT-N which conditions on Φ or vanilla SFT) perform worse on OOD tests, demonstrating the fragility of non-front-door solutions under latent confounding.</td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Front-door logic is practically implementable in a fine-tuning pipeline by learning a mediator representation C and randomizing local features Φ; this provides identification and empirical robustness when back-door adjustment is impossible due to latent confounders.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e757.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e757.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of causal discovery methods, especially those that handle distractors or spurious correlations in virtual labs or interactive environments, including techniques for detecting, downweighting, or refuting spurious signals.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MaxEnt default</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum-entropy default regime for unknown confounder distributions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A principled default choice for the unknown p(u|x;σ=test) used to form a conservative interventional estimate by assuming a maximum-entropy conditional consistent with known marginals, implemented conceptually to motivate using do(x)-style predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>method_name</strong></td>
                            <td>Maximum-entropy default regime</td>
                        </tr>
                        <tr>
                            <td><strong>method_description</strong></td>
                            <td>When p(u|x;σ=test) is unknown, set a default regime p(u|x;σ=default) equal to p(u;σ=test) (i.e., independent of x) — a maximum-entropy conditional that respects the marginal — and treat predictions as coming from σ=do(x); this motivates the use of interventional estimators rather than pessimistic minimax approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Theoretical/semisynthetic evaluation context</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Not an interactive lab; a modeling assumption about the test regime used to justify practicing do(x)-style prediction via CFT.</td>
                        </tr>
                        <tr>
                            <td><strong>handles_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distractor_handling_technique</strong></td>
                            <td>Conceptual: prevents over-reliance on train-conditioned p(u|x) by assuming the maximally uninformative conditional consistent with assumed marginals, thereby downweighting spurious dependence on U.</td>
                        </tr>
                        <tr>
                            <td><strong>spurious_signal_types</strong></td>
                            <td>Latent confounders UΦ and environment-specific spurious features whose conditional dependence on x is unknown at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>downweighting_method</strong></td>
                            <td>By adopting a maximum-entropy conditional for p(u|x) the approach avoids privileging train-set conditionals that may embed spurious correlations; operationalized by using do(x) estimators that do not condition on train-set p(u|x).</td>
                        </tr>
                        <tr>
                            <td><strong>refutation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_active_learning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>inquiry_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_robustness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_ablation_study</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>number_of_distractors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a maximum-entropy default for unknown confounder conditionals is proposed as a conservative, principled alternative to minimax robustness assumptions; it motivates the do(x)-style adjustments implemented in CFT and helps explain why marginalizing over Φ' is safer than conditioning on train-set dependent p(uΦ|·).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Causality <em>(Rating: 2)</em></li>
                <li>Transportability of causal and statistical relations: A formal approach <em>(Rating: 2)</em></li>
                <li>Counterfactual invariance to spurious correlations in text classification <em>(Rating: 2)</em></li>
                <li>Self-supervised learning with data augmentations provably isolates content from style <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-757",
    "paper_id": "paper-273482615",
    "extraction_schema_id": "extraction-schema-20",
    "extracted_data": [
        {
            "name_short": "CFT",
            "name_full": "Causal Fine-Tuning",
            "brief_description": "An end-to-end fine-tuning pipeline that decomposes input representations into pre-trained (R0), fine-tuned (R1) and local low-level (Φ) components, learns an invariant causal representation C from R0 and R1, and computes p(y|do(x)) via a front-door style adjustment implemented by shuffling Φ and marginalizing over local-feature samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Causal Fine-Tuning (CFT)",
            "method_description": "CFT keeps a frozen copy of a pre-trained encoder to produce R0 and trains another copy to produce R1 via supervised fine-tuning; it extracts local low-level features Φ from early layers, learns an invariant causal latent C by aligning p(c|r0) and p(c|r1) with an L2 invariance term plus entropy regularizers, and implements a front-door-like causal adjustment for prediction by sampling/shuffling Φ within batches to estimate p(y|do(x)) = E_{x',Φ'}[p(y|Φ',c) p(Φ'|x') p(x')], where c = f(r0,r1). The algorithm is trained end-to-end and used in inference by sampling K shuffled Φ' and marginalizing predictions.",
            "environment_name": "Semi-synthetic sentiment simulators (Amazon/Yelp injections)",
            "environment_description": "Controlled semi-synthetic simulator built from real Amazon and Yelp review corpora where spurious correlations are injected (stop-word suffixes or platform suffixes appended to tokens) to create train ID and OOD test regimes; not an interactive/online lab (no active interventions), but allows controlled, repeatable distribution shifts for evaluation.",
            "handles_distractors": true,
            "distractor_handling_technique": "Representation decomposition + front-door adjustment: identify invariant causal factor C (representation alignment across R0 and R1 with entropy regularization) and block spurious collider paths by marginalizing over shuffled local features Φ to emulate do(x); uses pre-trained model as an implicit second 'environment' for contrastive identification.",
            "spurious_signal_types": "Latent confounders creating spurious correlations (unstable spurious features S1 affecting high-level representation R1 and local features Φ), local irrelevant tokens (stop-word suffixes) and platform-identifiers (injected strings), measurement-level spurious style features.",
            "detection_method": "Implicit detection via representation shift: the method detects unstable/spurious signal by comparing R0 (pretrained) and R1 (fine-tuned) and forcing p(c|r0) ≈ p(c|r1) while retaining entropy—large mismatches indicate spurious high-level changes; no explicit statistical test is introduced beyond the alignment loss.",
            "downweighting_method": "Front-door style adjustment by shuffling Φ and marginalizing over sampled Φ' during estimation of p(y|do(x)), effectively removing the influence of environment-specific Φ realizations; also uses maximum-entropy default for unknown confounder distributions to avoid over-reliance on train-set-conditioned signals.",
            "refutation_method": "Emulates interventions (do(x)) by randomizing Φ within the mini-batch (breaking spurious collider paths) and training p(y|Φ,c) on shuffled Φ' to ensure predictions rely on c (invariant) rather than Φ; this acts as an empirical refutation test: predictors that condition on Φ (CFT-N) degrade OOD.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "CFT consistently outperforms standard fine-tuning baselines in the paper's semi-synthetic experiments: e.g., under a strong spurious training regime (90% spurious) CFT reported F1 ≈ 98.69 vs SFT ≈ 95.96 (absolute +2.73); as OOD spurious correlation weakens, CFT shows larger absolute gains (examples in tables show gains up to ≈ +9.16 at extreme OOD settings).",
            "performance_without_robustness": "Standard supervised fine-tuning (SFT) and frozen-representation classifier (SFT0) suffer substantial drops under confounded OOD: example SFT values from the same settings: 95.96 (90% spurious), 92.89 (70%), 81.89 (50%), 71.20 (30%), 60.23 (10%) F1 in the reported tables.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Decomposing representations into R0 (pretrained), R1 (fine-tuned) and local Φ and explicitly learning an invariant causal representation C enables construction of a front-door-style adjustment that substantially improves OOD robustness under latent-confounded shifts. Conditioning directly on Φ (CFT-N) exposes active collider paths and harms OOD generalization; using only C (CFT-C) is often strong but less adaptive when shifts are extreme; shuffling Φ and marginalizing reduces variance with more samples and stabilizes OOD performance.",
            "uuid": "e757.0",
            "source_info": {
                "paper_title": "Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Φ-shuffle",
            "name_full": "Local-feature shuffling (Φ-shuffle) for emulating do(x)",
            "brief_description": "A computational trick used inside CFT that approximates an interventional distribution by randomly shuffling local low-level features Φ within mini-batches (and sampling multiple Φ') to break spurious associations and estimate p(y|do(x)).",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Local-feature shuffling (Φ-shuffle)",
            "method_description": "Extract local low-level features Φ from early/frozen layers (patch-averaged token embeddings), then during training and inference shuffle Φ across examples within a mini-batch to create Φ' samples that are marginally distributed according to p(Φ'|x') and use these to compute p(y|do(x)) via p(y|Φ',c). Multiple shuffled samples (K) are drawn at inference and marginalized to reduce variance.",
            "environment_name": "Semi-synthetic sentiment simulators (Amazon/Yelp injections)",
            "environment_description": "Controlled, non-interactive simulator; shuffling is performed offline within batches rather than by performing real interventions in an interactive environment.",
            "handles_distractors": true,
            "distractor_handling_technique": "Directly severs the active collider path through Φ by randomizing Φ, preventing the model from learning conditional dependencies that arise from environment-specific Φ realizations.",
            "spurious_signal_types": "Local measurement-level spurious signals and style tokens (injected suffixes), environment-specific local features that act as colliders.",
            "detection_method": "No explicit detector; effectiveness inferred empirically by comparing performance when shuffling is applied vs when Φ is conditioned (CFT vs CFT-N), and via variance reduction as K increases.",
            "downweighting_method": "Shuffling removes instance-specific influence of Φ from the conditional used for prediction, thereby downweighting environment-specific spurious cues in the predictive distribution.",
            "refutation_method": "Acts as an empirical refutation/intervention: if performance improves after Φ-shuffling (and marginalization), this supports that previously learned correlations with Φ were spurious.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Using multiple Φ samples at inference reduces variance and improves OOD performance; paper shows that increasing K (e.g., from 1 to 20) reduces variance and aids robustness (visualized in Fig.9). Exact F1 gains depend on setting but were essential for CFT's superior OOD scores.",
            "performance_without_robustness": "Conditioning on Φ (CFT-N) yields worse OOD performance (tables show CFT-N drops relative to CFT), demonstrating the necessity of shuffling for robustness.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Randomizing Φ within batches is an effective and simple operationalization of an interventional approach that removes spurious local-feature influence; empirically necessary for CFT's OOD gains and reduces variance with larger sample counts at inference.",
            "uuid": "e757.1",
            "source_info": {
                "paper_title": "Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Invariant C learning (LC)",
            "name_full": "Invariant causal feature learning via cross-view alignment (LC loss)",
            "brief_description": "A training objective that identifies an invariant causal latent C by aligning the conditional distributions p(c|r0) and p(c|r1) while maximizing entropy to avoid representational collapse.",
            "citation_title": "here",
            "mention_or_use": "use",
            "method_name": "Invariant C learning (alignment + entropy loss)",
            "method_description": "The loss L_C = E_{(r0,r1)} ||p(c|r0) - p(c|r1)||_2^2 - H(p(c|r0)) - H(p(c|r1)) forces agreement between causal representations inferred from the pre-trained view R0 and the supervised fine-tuned view R1, and encourages high-entropy marginals to prevent trivial collapsed solutions; C is then used in the front-door adjustment to produce invariant predictions.",
            "environment_name": "Semi-synthetic sentiment simulators (Amazon/Yelp injections)",
            "environment_description": "Non-interactive, paired-view setup where R0 comes from frozen pre-trained encoder and R1 from supervised fine-tuning on same x; the paired views act as two implicit environments for identification.",
            "handles_distractors": true,
            "distractor_handling_technique": "Detects and separates invariant causal signal from spurious high-level changes by forcing agreement across views (R0 vs R1); entropy terms ensure useful representation capacity rather than collapse to constants.",
            "spurious_signal_types": "High-level spurious shifts (S1) that affect R1 but not R0, and other unstable representation-level artifacts introduced during fine-tuning.",
            "detection_method": "Detection via representation mismatch: large ||p(c|r0)-p(c|r1)|| indicates presence of unstable/spurious components; minimization of this term identifies invariant subspace.",
            "downweighting_method": "By projecting onto the learned invariant C, downstream predictors are less reliant on R1-specific spurious components; combined with Φ-shuffling this reduces spurious influence.",
            "refutation_method": "No explicit statistical refutation test, but ablations (CFT vs CFT-C vs CFT-N) show that relying on C (and aligning it) leads to better OOD performance, serving as empirical refutation of spurious reliance.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Using C alone (CFT-C) yields strong OOD performance and often close to full CFT in milder shifts (tables show CFT-C F1 often near CFT for some settings), but full CFT (with Φ adjustment) is better in many OOD extremes.",
            "performance_without_robustness": "No explicit baseline that learns C without alignment is provided; standard SFT which does not separate C shows clear OOD degradation.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "The cross-view alignment loss is central to identifying a stable causal representation C from single-domain fine-tuning data by leveraging a frozen pre-trained encoder as a second implicit view; it meaningfully separates invariant from spurious high-level signals and is a key component for downstream robustness.",
            "uuid": "e757.2",
            "source_info": {
                "paper_title": "Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Front-door adj.",
            "name_full": "Front-door adjustment (operationalized in CFT)",
            "brief_description": "A causal adjustment strategy that identifies causal effect through an intermediate mediator when back-door adjustment is not possible due to hidden confounding; in this work it is operationalized by learning mediator C and marginalizing over randomized local features Φ.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Front-door adjustment (operationalized via C and Φ)",
            "method_description": "Theory: when Φ's effect on Y is fully mediated by C (a front-door structure), p(y|do(x)) can be computed by integrating p(y|Φ,c)p(Φ|x')p(x'); practice: learn C from paired R0/R1 and estimate p(y|Φ,c) by training on shuffled Φ' (breaking confounding), then marginalize over Φ' samples to produce interventional predictions.",
            "environment_name": "Semi-synthetic sentiment simulators (Amazon/Yelp injections)",
            "environment_description": "Controlled offline simulation used to estimate interventional distributions via sampled/randomized local features rather than real randomized experiments.",
            "handles_distractors": true,
            "distractor_handling_technique": "Enables identification of causal effect despite hidden confounding between Φ and Y by using mediator C and randomized Φ' to block back-door confounding and remove spurious environment-specific influence.",
            "spurious_signal_types": "Hidden confounders between input and label that render p(y|x) non-transportable; environment-specific spurious features altering p(u|x;σ).",
            "detection_method": "Relies on structural assumptions (Assumption 4.4: Φ → Y effect fully mediated by C) and empirical checks via ablations rather than an automatic statistical detector.",
            "downweighting_method": "Marginalization over randomized Φ' reduces dependence on environment-specific Φ and uses p(y|c) (or p(y|Φ',c)) as interventional predictor; combined with maximum-entropy prior for unknown confounder distributions to avoid overfitting to train-conditioned p(uΦ|·).",
            "refutation_method": "Comparing predictions under observational conditioning vs the front-door adjusted estimator and showing OOD gains acts as empirical refutation of spurious observational associations.",
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": "Front-door operationalization in CFT yields substantial OOD improvements relative to observational predictors; specific reported F1 improvements (see CFT numbers) show front-door based adjustment is effective in the semi-synthetic tasks.",
            "performance_without_robustness": "Observational predictors (e.g., CFT-N which conditions on Φ or vanilla SFT) perform worse on OOD tests, demonstrating the fragility of non-front-door solutions under latent confounding.",
            "has_ablation_study": true,
            "number_of_distractors": null,
            "key_findings": "Front-door logic is practically implementable in a fine-tuning pipeline by learning a mediator representation C and randomizing local features Φ; this provides identification and empirical robustness when back-door adjustment is impossible due to latent confounders.",
            "uuid": "e757.3",
            "source_info": {
                "paper_title": "Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "MaxEnt default",
            "name_full": "Maximum-entropy default regime for unknown confounder distributions",
            "brief_description": "A principled default choice for the unknown p(u|x;σ=test) used to form a conservative interventional estimate by assuming a maximum-entropy conditional consistent with known marginals, implemented conceptually to motivate using do(x)-style predictors.",
            "citation_title": "",
            "mention_or_use": "use",
            "method_name": "Maximum-entropy default regime",
            "method_description": "When p(u|x;σ=test) is unknown, set a default regime p(u|x;σ=default) equal to p(u;σ=test) (i.e., independent of x) — a maximum-entropy conditional that respects the marginal — and treat predictions as coming from σ=do(x); this motivates the use of interventional estimators rather than pessimistic minimax approaches.",
            "environment_name": "Theoretical/semisynthetic evaluation context",
            "environment_description": "Not an interactive lab; a modeling assumption about the test regime used to justify practicing do(x)-style prediction via CFT.",
            "handles_distractors": null,
            "distractor_handling_technique": "Conceptual: prevents over-reliance on train-conditioned p(u|x) by assuming the maximally uninformative conditional consistent with assumed marginals, thereby downweighting spurious dependence on U.",
            "spurious_signal_types": "Latent confounders UΦ and environment-specific spurious features whose conditional dependence on x is unknown at test time.",
            "detection_method": null,
            "downweighting_method": "By adopting a maximum-entropy conditional for p(u|x) the approach avoids privileging train-set conditionals that may embed spurious correlations; operationalized by using do(x) estimators that do not condition on train-set p(u|x).",
            "refutation_method": null,
            "uses_active_learning": false,
            "inquiry_strategy": null,
            "performance_with_robustness": null,
            "performance_without_robustness": null,
            "has_ablation_study": null,
            "number_of_distractors": null,
            "key_findings": "Using a maximum-entropy default for unknown confounder conditionals is proposed as a conservative, principled alternative to minimax robustness assumptions; it motivates the do(x)-style adjustments implemented in CFT and helps explain why marginalizing over Φ' is safer than conditioning on train-set dependent p(uΦ|·).",
            "uuid": "e757.4",
            "source_info": {
                "paper_title": "Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Causality",
            "rating": 2
        },
        {
            "paper_title": "Transportability of causal and statistical relations: A formal approach",
            "rating": 2,
            "sanitized_title": "transportability_of_causal_and_statistical_relations_a_formal_approach"
        },
        {
            "paper_title": "Counterfactual invariance to spurious correlations in text classification",
            "rating": 2,
            "sanitized_title": "counterfactual_invariance_to_spurious_correlations_in_text_classification"
        },
        {
            "paper_title": "Self-supervised learning with data augmentations provably isolates content from style",
            "rating": 1,
            "sanitized_title": "selfsupervised_learning_with_data_augmentations_provably_isolates_content_from_style"
        }
    ],
    "cost": 0.016579999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts
12 Jun 2025</p>
<p>Jialin Yu 
University College London</p>
<p>University of Oxford</p>
<p>Yuxiang Zhou 
King's College London</p>
<p>Yulan He 
King's College London</p>
<p>Nevin L Zhang 
The Hong Kong University of Science and Technology</p>
<p>Junchi Yu 
University of Oxford</p>
<p>Philip Torr 
University of Oxford</p>
<p>Ricardo Silva 
University College London</p>
<p>Attuned to Change: Causal Fine-Tuning under Latent-Confounded Shifts
12 Jun 20257B3E20EB39C5F8BEC2A6841EF5DBD954arXiv:2410.14375v2[cs.LG]
Adapting to latent-confounded shifts remains a core challenge in modern AI.These shifts are propagated via latent variables that induce spurious, non-transportable correlations between inputs and labels.One practical failure mode arises when fine-tuning pre-trained foundation models on confounded data (e.g., where certain text tokens or image backgrounds spuriously correlate with the label), leaving models vulnerable at deployment.We frame causal fine-tuning as an identification problem and pose an explicit causal model that decomposes inputs into low-level spurious features and high-level causal representations.Under this family of models, we formalize the assumptions required for identification.Using pretrained language models as a case study, we show how identifying and adjusting these components during causal fine-tuning enables automatic adaptation to latentconfounded shifts at test time.Experiments on semi-synthetic benchmarks derived from real-world problems demonstrate that our method outperforms black-box domain generalization baselines, illustrating the benefits of explicitly modeling causal structure.1 This generalizes the notion of "confounder shift" to include cases where the confounder itself may not shift, but its interactions with other variables induce distributional changes.Preprint.Under review.</p>
<p>Contribution</p>
<p>Distribution shifts are a fundamental challenge for many modern machine learning systems [22,12].In practice, many such shifts are often unobserved and confounded [55,2]: latent variables induce spurious, non-transportable [43,25] correlations between inputs and labels, and their distributions vary across environments.We refer to this broader class of shifts, arising either directly at the confounder level or through interactions involving confounders, as confounded shifts 1 .</p>
<p>Examples include deploying models across different hospital populations [10], handling corrupted and adversarial image inputs [20,23], and stress-testing models [12].To address such robustness concerns, prior work has focused on feature augmentation and regularization [21,58,65,53], or learning invariant features across domains [3,1,19].Although effective in some settings, these methods typically assume stable invariant features across environments.However, they do not explicitly account for latent-confounded shifts, where observed features may carry spurious correlations that vary across environments but still contain valid predictive signal.This issue is particularly challenging in unstructured data, such as natural language, where domain labels are often unavailable and confounding factors are implicit [61].</p>
<p>Foundation models [6], such as BERT [30], GPT [8], and CLIP [45], have become the standard for downstream learning.However, when fine-tuned on data with latent confounding, they often exploit spurious correlations that do not generalize across environments [37,44].In Figure 1(a), we illustrate this with a sentiment classification task where the data source (Amazon or Yelp) serves as a latent confounder U , correlated with the sentiment label.During training (denoted by the environment or regime index σ = train [13]), the "positive" sentiment tends to co-occur with Amazon reviews, while the "negative" sentiment is associated with Yelp reviews.In the test setting (σ = test), this correlation reverses.Part (b) shows the underlying causal graph: there is an interaction between the environment (denoted by the index σ) and the confounder U in the generation of X.In the classical framework of [42], σ = do(x) denotes the regime where X is fixed to x.As the conditional distribution p(u | x; σ) changes across environments, models that are only trained on data generated via p(u | x; σ = train) will fail to generalize [18,46].</p>
<p>In this paper, we address this issue by proposing a causal framework for fine-tuning under latentconfounded shifts by interpreting the process as a causal identification problem.We define causal fine-tuning as the process of learning a causal mechanism during fine-tuning.Specifically, this works by identifying latent structures that enable a principled decomposition of the input signal into stable (invariant) and spurious (environment specific) components, thereby enabling robust generalization under confounded shifts.Our key insight is to exploit scenarios where it can be assumed that: i) low-level features of the input signal are invariant between training and test regimes; ii) they cause high-level features and the label, with the former possibly changing in distribution at test time.Unlike covariate-shift scenarios, the conditional distribution of the label is also exposed to changes.By applying a causal adjustment strategy to these representations that exploits the fact that we have two views of the unstable features (unsupervised and fine-tuned), we produce robust and adaptive predictions without requiring multiple environments or domain annotations.</p>
<p>Our contributions are as follows.(1) Modeling: we analyze processes where input feature generation is hierarchical and responds differently to environmental changes.Where it can be plausibly assumed, the existence of low-level features that are invariant between training and test time can be exploited to decouple spurious and invariant components of high-level features.Care is still needed, as spurious associations can still propagate when conditioning on the observed signal, analogously to Figure 1  The graph indicates that the mechanism into X may change according to regimes indexed by a regime variable σ.For instance, when a σ = do(x) operation is performed [42], the edge between U and X is removed, indicated by a red cross.In general, σ indexes arbitrary distributions [13].</p>
<p>Related Work</p>
<p>Distribution shift is fundamentally an ill-posed problem without assumptions: the mapping between x and y may change arbitrarily across domains [22].A central question is: which parts of the data-generating process remain invariant across environments?Causal inference, in particular transportability theory [43,25], provides a principled framework for answering this by characterizing how and when causal knowledge can be moved between environments.The most common assumptions in the literature are covariate shift [38,49] and label shift [9].Building on these assumptions, some lines of work explore methods that learn causally robust invariant features Φ(x) based on the covariates across multiple environments [3,1,56,62,40,48,32]. Another line of work explores counterfactual reasoning and data augmentation techniques [29,5,33,16].</p>
<p>More recently, there has been a growing interest in causal representation learning, which aims to learn disentangled latent representations that capture the underlying causal structure and enables models to generalize across domain shifts.Existing work focuses on learning stable causal latent variables [51,36,37], invariant predictors [55,26], or compositional models [7,60].These approaches often require multiple environments or proxy variables, which can be impractical to acquire or augment, particularly for unstructured data [11].We build on this body of work by treating a pretrained foundation model as an implicit environment for data augmentation, on top of the fine-tuning data, allowing causal representation learning with single domain fine-tuning data only.Once the latent variables are learned, causal adjustment formulas are adopted to generate predictors that are robust to domain shift, with two common receipts being back-door adjustments [63,64], and front-door adjustments [34,39,41].Front-door adjustments have become a promising choice due to hidden confounding variables between input signals and outputs.Motivated by the recent success of these methods, we propose a method to construct a front-door in the context of a fine-tuning pipeline, achieving improved OOD robustness under confounded shifts.</p>
<p>Preliminaries</p>
<p>Motivation.Our proposal encodes invariance assumptions into graphical causal models with explicit regime indicator variables [42,13].In particular, we consider the scenario where input X is allowed to cause label Y , but not vice-versa, with the possible presence of hidden confounders U .Graphically, this setup is depicted in Figure 1(b).To accommodate distribution shifts, we further assume that changes from training to test environments involve an intervention (or perturbation, or regime), denoted by the regime variable σ, which modifies the influence of U on X.Data are observed only under the training regime denoted by σ = train.We want to be safe to build a predictor for an unknown environment denoted as σ = test, where the conditional distribution p(x | u; σ = test) can potentially arbitrarily differ from p(x | u; σ = train).This follows directly from the law of total probability over U , below assumed to be discrete without loss of generality:
p(y | x; σ) = u p(y | u, x; σ)p(u | x; σ) = u p(y | u, x) does not change with σ p(u | x; σ) changes with σ .
This implies that a predictor learned under σ = train is not transportable [43,25] in this setting even when σ affects neither U nor Y directly.If we are serious about exploiting the postulated causal structure, we must make use of the fact that the change from p(y | x; σ = train) to p(y | x; σ = test) boils down to the difference between p(u | x; σ = train) and the unknown p(u | x; σ = test).One possibility is to address it in a minimax way, akin to distributionally robust optimization [15], where we minimize our loss with respect to all distributions p ⋆ (u | x; σ = test) which are "close" to p(u | x; σ = train) in some sense.However: i) in general we will not be able to identify p(u | x; σ = train) nor p(y | x, u) to begin with; ii) the meaning of U is in general unclear anyway, making any assumptions about closeness of p ⋆ (u | x; σ = test) and p(u | x; σ = train) scientifically vacuous at best; iii) it inherits all shortcomings of the minimax approach, being harmfully conservative when p(u | x; σ = test) and p(u | x; σ = train) are close.
S 1 S 0 Φ C Y U S U Φ R 0 R 1 σ X Figure 2: Refinement of the original causal diagram in Figure 1(b)
, where X is broken apart, and abstracted into vectors R 0 , R 1 and Φ as described in Section 4.</p>
<p>Maximum-entropy as a default choice.We propose following a default regime built from a maximum-entropy distribution p(u | x; σ = default) that respects the marginal constraint p(u; σ = default) = p(u; σ = test).A trivial solution is p(u | x; σ = default) = p(u; σ = test), the latter which is also knowable by assumption: p(u; σ = test) = p(u; σ = train).This also means that p(y | x; σ = default) = p(y | x; σ = do(x)), since our causal structure assumption implies p(u; σ = s ⋆ ) = p(u; σ = s ⋆⋆ ) and p(y | x, u; σ = s ⋆ ) = p(y | x, u; σ = s ⋆⋆ ), for all values s ⋆ , s ⋆⋆ in the scope of σ.</p>
<p>We thus reduce this to the problem of training our model as if it came from the regime σ = do(x), but using data collected under σ = train.Unfortunately, it is a well-known result that, assuming no more than the Markovian factorization implied by Figure 1(b), the distribution p(y | do(x)) := p(y | x; σ = do(x)) is not identifiable: it follows from the completeness of Pearl's do-calculus [42].</p>
<p>To that effect, in the sequel we will assume a more fine-grained structure for X, as well as making explicit use of fine-tuning data.</p>
<p>4 Assumptions for Causal Fine-Tuning in Pre-trained Foundation Models</p>
<p>In this section, we introduce the identification theory for p(y | do(x)) and the structural assumptions required for identifiability, then discuss their implications for practical applications.</p>
<p>Standard supervised learning makes no distinction between "causal features" and "non-causal features".Moreover, concepts such as do(x) may be ill-posed when the object of intervention is unstructured [11], such as raw text.We start by postulating which representations of X are assumed to follow a causal structure.</p>
<p>Assumption 4.1 (Functional Decomposition) We assume access to a triplet of measurements (R 0 , R 1 , Φ) = f (X) for some function f , defined non-constructively, as follows:
(i) (R 0 , R 1 )
is a paired representation of X.The mapping to R 0 is learned using self-supervised learning with unlabeled data.The mapping to R 1 is learned during supervised fine-tuning with labeled data under the training environment σ = train; (ii) local features Φ are low-level features implied as a by-product of the fine-tuned model (e.g., combinations of earlier layer features that cause high-level feature R 1 in the last layer).□</p>
<p>In the sequel, we will explicitly describe the computational procedure that constructively defines this mapping (R 0 , R 1 , Φ) = f (X).It is to be noted that an intervention do(x) will be defined as do
((R 0 , R 1 , Φ) = f (x)
), which we will sometimes denote as do(r 0 ), do(r 1 ), do(Φ).</p>
<p>Under this choice of abstraction, we postulate a causal structure with (R 0 , R 1 , Φ) interacting with causal latent variables C and two sets of "spurious" latent variables S 0 and S 1 , spurious in the sense that only C is a causal parent for output Y .We define S 0 as the latent spurious features of pre-training not affected by distribution shifts, and S 1 are the latent spurious features affected by the environment index σ.The generative model contains these two feature sets as latent variables, along with structural assumptions about how σ, R 0 , R 1 , Φ and Y are connected.Assumptions are graphically summarized in Fig. 2, and detailed as follows.</p>
<p>Assumption 4.2 (Causal Latent Structure) High-level features {R 0 , R 1 } are indirect measurements of mutually independent variables {S 0 , S 1 , C}. S 0 can only cause R 0 and S 1 can only cause R 1 .The regime variable σ can only affect S 1 .Morever, hidden confounders U S are common parents of R 1 and Φ, and independent hidden confounders U Φ are parents of Φ and Y .□</p>
<p>This assumption aligns with prior work in causal learning [52,17,19,39].Intuitively, this abstracts the true complex causal graph into a coarser granularity, encapsulating stable hidden confounders into C and any other (unstable) non-confounding variables into S 0 , S 1 .It also postulates a principle: any dependency between S 0 and S 1 is solely attributed to common cause C.</p>
<p>Assumption 4.3 (Causal Structure of Distribution Shifts) Regime variable σ affects the system only via S.This also implies that the causal ancestors of Y do not interact with σ. □</p>
<p>This assumption postulates that, for any regime of interest where we deploy our system, the relationship between causal ancestors and the output Y is invariant.However, it is not the case that we will be able to optimize the empirical risk on the training data without consequences, since conditioning on the entire input signal {R 0 , R 1 , Φ} will d-connect Y with σ [42]: this happens via active collider paths [50] such as
Y ← U Φ → Φ ← U S → R 1 ← S 1 ← σ and Y ← C → R 1 ← S 1 ← σ.
This makes our predictions dependent on the value of σ, in the sense of [13], which means being affected by distribution shifts.In what follows, we will rely on i) the missing edge Φ → Y and ii) the ability of deterministically inferring C, as formalized in the following assumption and theorem.
Assumption 4.4 (Sufficient Mediator) The causal effect of Φ on Y is fully mediated through C. That is, p(y | do(Φ), do(c)) = p(y | do(c)). □ Justification.
This assumption is sometimes known as a front-door structure [42] for the effect of Φ on Y .It can be interpreted as having C as ultimately the only variable driving Y directly, and relying on this desiderata as the operational definition of C, implying no further latent sources confounding Φ and C, or C and Y , or any other path between Φ and Y relying on further (implicit) hidden variables.We allow confounding between Φ an Y .Intuition.This theorem implies that if the causal latent variable C remains invariant across environments (Assumption 4.3), the distribution shift between representations R 0 and R 1 can be used to identify C. For a formal proof of this theorem, please refers to Theorem 4.4 in [56].In the sequel, we will learn this function using the idea presented in Equation 3.</p>
<p>We will now show that we can identify p(y | do(x)) from the pre-trained and training fine-tuning data.The proof of this result is short and presented in Appendix D.
p(y | do(x)) = Φ ′ ,x ′ p(y | Φ ′ , c)p(Φ ′ | x ′ )p(x ′ ),(1)
where c is given as a function of (r 0 , r 1 ) per Theorem 4.5, and (r 0 , r 1 ) are a function of x. □ Invariance implication and pragmatic application.The difference between p(y | x; σ = test) and p(y | x; σ = do(x)) in our setup boils down to averaging p(y | c, U Φ ) over p(u Φ | r 0 , r 1 , Φ, c; σ = test) in the former, and p(u Φ ) in the latter.When can we say that the latter is an improvement over p(u Φ | r 0 , r 1 , Φ, c; σ = train)?Our claim is that by virtue of the confounder being a cause of local features Φ only, and not of the whole of X, the relevance of information passing through (S 0 , S 1 ) via active collider paths only should be limited anyway [14], unless the test environment affects it drastically.In this case, we may be thrown away too far from the original p(u Φ | r 0 , r 1 , Φ, c; σ = train) in unpredictable ways, and the safer bet ("maximum-entropy") is to think of p(y | c) as being a random measure "p U ϕ (y | c)" with a conservative prior p(u Φ ) which comes from the model and is agnostic to the environment.Section 6 will empirically investigate this in detail.5 Algorithm: Causal Fine-Tuning</p>
<p>In this section, we operationalize the theoretical insights outlined in Section 4 into a Causal Fine-Tuning (CFT) framework (illustrated in Fig. 3).</p>
<p>Each component of this framework is illustrated via fine-tuning of a pre-trained language model as a running example: i) supervised fine-tuning, which obtains representations R 1 from input pairs (X, Y ); ii) learning invariant causal features, which derives causal representation C from both R 1 and R 0 , where R 0 is the representation directly taken from the pre-trained model; and iii) retrieving local features, which extracts Φ from the fine-tuned model.</p>
<p>After retrieving Φ, we use C and Φ to adjust for p(y | do(x)).More details can be found in Algorithm 1 for training and Algorithm 2 for inference, both in Appendix E.
L SFT = E (x,y)∼D [−y log p(r 1 | x)] .(2)
Component 2: Learning Causal Invariant Feature To learn the invariant causal feature C, we aim to identify the distribution p(c | r).This process involves aligning representations from different environments while maximizing entropy and prevent collapsed representations [56].The loss function is constructed based on Theorem 4.5,
L C := E (r0,r1 | x)∼D ∥p(c | r 0 ) − p(c | r 1 )∥ 2 2 − H (p(c | r 0 )) − H (p(c | r 1 )) ,(3)
where x is sampled from p(x) and used to calculate r 0 , r 1 .The first term enforces invariance across environments.The entropy terms maximize the diversity in representations, reducing the risk of collapse.</p>
<p>Component 3: Retrieving Local Features This component focuses on constructing local features Φ, which are taken from the first k layers of p(r 1 | x).In particular, we use the representation from the k th layer.In our running example of a pre-trained language model, we use k = 1, which is the embedding layer.Our justification is that using the first layer provides a better low-level signal [? ?59].</p>
<p>Given input X as a series of tokens where X = [t 1 , t 2 , ..., t m ], we can retrieve a vector representation for each token t.To construct the local feature Φ, we divide the token sequence into non-overlapping patches (we use 10 patches in our experiments, for balancing granularity and computational efficiency), allowing us to rewrite X as patches p where X = [p 1 , p 2 , ..., p 10 ] where p 1 = [t 1 , t 2 , ..., t m 10 ] and so on.After splitting, we perform mean averaging on these patches to extract a regional signal such that Φ = 1 10 10
i=1 p i .
Causal Adjustment Based on components 1, 2 and 3, we can get C and Φ from the observed data X = x, we then passed C and Φ through a multilayer perceptron (MLP) to adjust for p(y | do(x)).For full details, please consult the Appendix E and the corresponding code.</p>
<p>Experiments</p>
<p>We evaluated our proposed approach using semi-synthetic data constructed to reflect two real-world scenarios.This section summarizes the setup, datasets, baselines, and key results.Detailed description of datasets and simulators can be found in Appendix A, while Appendix B provides details of the model architecture.Further analysis and additional results are presented in the Appendix C.</p>
<p>Datasets.We construct two semi-synthetic experiments based on widely studied sentiment analysis datasets: the Amazon review dataset and the Yelp review dataset [66].The experimental setups are motivated based on [47] and [55].This aims to have full control over how latent-confounded shifts occur between training in-distribution (ID) and testing out-of-distribution (OOD) scenarios via a human-controlled pattern injection, which are otherwise not clear or impossible for unstructured data [11,61].We discuss more about the value of doing this in Section 6.3.</p>
<p>Baselines and Our Methods.We compare our algorithm with the following baselines: (1) SFT0, which involves training a linear classifier on a frozen sentence representation extracted directly from PLMs; (2) SFT [54], the typical transfer learning strategy with PLMs, considered as a very strong baseline (equivalent to performing ERM); (3) WSA [24,4], which averages multiple points along the SGD trajectory to achieve a more robust classifier; and (4) WISE [57], which interpolates the parameters of PLMs and a fine-tuned model to enhance generalization.</p>
<p>Our proposed CFT algorithm follows the exact setup described in Section 4. To analyze the impact of different representations, we implemented three additional variations of CFT: (1) CFT-N uses both Φ and C to predict Y without applying the adjustment formula from Theorem 4.6, leaving a causal path between Φ and Y unblocked; (2) CFT-C uses the estimated high-level causal variable C to predict Y ; and (3) CFT-Φ directly uses low-level spurious features Φ to predict Y .Experimental Setup.Each experiment was repeated 5 times using the AdamW optimizer [31,35] with a learning rate of 5 × 10 −5 for all cases, except for SFT0.There, a learning rate of 5 × 10 −4 was used.Each model was trained for 10 epochs, sufficient for convergence.The best model iteration was selected based on performance on a holdout validation set (20% of the training data).For training, we randomly sample 5000 points per class, with a 20% split for validation.For testing, we sample 2000 per class.For training data, we construct semi-synthetic data to contain strong spurious correlations with probability of 90% of the time.We use the same ratio for the ID test set and, for the OOD test set, we shift this ratio of possible correlation to be 70%, 50%, 30% and 10%.</p>
<p>Experiment 1: Spurious Correlation Between Stop Words and Label</p>
<p>Following [55], we generate semi-synthetic ID and OOD data by injecting spurious correlations between stop words (e.g."and", "the") and class labels.See Appendix A.2 for more details.</p>
<p>Results.The main results are presented in Table 1, with visualizations for the Amazon dataset over 5 runs in Fig. 4.These results demonstrate the superiority of our model against the strong baselines.We observe a significant performance drop in both SFT0 and SFT when the distribution of spurious features shifts, indicating that standard fine-tuning methods struggle to handle spurious correlations in OOD settings.However, we observe that SFT consistently outperforms SFT0 for both ID and OOD settings, highlighting the effectiveness of "knowledge transfer" in improving representation quality.Among all learning algorithms, our proposed CFT method provides the most promising predictors.Compared to CFT, the CFT-N conditions on Φ, which introduces an active collider path between σ and Y , namely σ → S 1 → R 1 ↔ Φ ↔ Y [50,42], where S 1 is unobserved, but R 1 and Φ are observable functions of X.This means that this predictor gets exposed to changes in distribution as indexed by σ.We observe that the drop in performance compared to CFT and this confirms why making predictions under a hypothetical do(x) helps.The CFT-C variant, which uses only the causal variable C for prediction, performs well in many OOD settings, suggesting that PLMs can be considered as a good source of new domain data.However, its accuracy decreases as the OOD distribution diverges further from the ID data, indicating that relying solely on C may limit robustness in extreme scenarios.An intriguing observation is the behavior of the CFT-variant Φ, which predicts the label using only local features Φ.This variant is strongly correlated to the spurious pattern in the data, highlighting why our methods can work for OOD settings, as we negotiate large changes for the spurious distribution by sticking to the distribution do(x).C).Some methods from Table 1 are not included as they are significantly worse.This is a visualisation of the Amazon dataset.Yelp shows a similar trend (Fig. 6, Appendix).</p>
<p>Experiment 2: Spurious Correlation Between Data Source and Label</p>
<p>We construct semi-synthetic data to mimic the scenario illustrated in Fig. 1.We build correlations between the source of the data (whether coming from Amazon or Yelp) and the label, by adding strings such as "amazon.xxx"or "yelp.yyy"into the sentences, more details in Appendix A.3.We compare our approach with other single-domain generalization baselines to demonstrate its effectiveness.</p>
<p>Results.The results are consistent with our previous experiments.When compared with the two baselines, the WISE method does not work too well, perhaps for being more sensitive to the hyper-parameter that mixes the fine-tuned model and the pre-trained model (we used a default value of 0.5, which means they are equally weighted).The SWA method worked quite well compared to the SFT methods, suggesting that stopping at a flat region of the parameter space improves the generalization of the model [24,28].However, its performance degraded significantly under more severe distribution shifts (e.g., the OOD ratio from 70% to 10%), highlighting its limitation in handling extreme perturbations.In contrast, our proposed CFT approach consistently outperformed all baselines, demonstrating robustness across all OOD settings.</p>
<p>Discussion: The Value of Semi-Synthetic Cases</p>
<p>A key distinction in our experiments is that both semi-synthetic examples are derived from the same base datasets.None of the experiments uses the data in its original form.Instead, we systematically inject spurious correlations (e.g., stop words or platform identifiers linked to labels) to create controlled distribution shifts.This design ensures that the data used for training and testing differ   2 are not included as they are significantly worse.</p>
<p>significantly, enabling a rigorous evaluation of causal effects.Importantly, the controlled generation does not produce data based on our assumptions, but as a black-box.Controlled settings are essential for isolating the impact of spurious features and accurately measuring the causal effect of our method.By introducing spurious correlations in a structured manner, we replicate realistic distribution shifts while preserving the underlying causal relationships.This approach allows for a consistent and repeatable evaluation of model robustness across ID and OOD settings.Far from being a limitation, this controlled design ensures that our experiments effectively test the ability of methods to mitigate spurious correlations and generalize to diverse deployment scenarios.</p>
<p>Further Analysis</p>
<p>We conducted a further analysis on (1) level of spuriousness (Fig. 7), (2) number of training data (Fig. 8), and (3) number of samples during inference (Fig. 9).All results are presented in Appendix C, summarized as: (1) Under different levels of spurious information, our CFT method consistently outperforms the SFT method by a significant margin.(2) Even with more data provided, our model CFT consistently outperforms black-box methods (SFT).However, we observe that when enough data is provided, there is a saturation point where SFT and CFT methods become indistinguishable for this particular OOD task.(3) We also observed a decrease in performance if we do not use the interventional distribution do(x) during prediction time.</p>
<p>Conclusion</p>
<p>We introduced a method for adapting to latent-confounded shift via causal fine-tuning, demonstrating promising performance in OOD scenarios compared to standard fine-tuning and black-box domain generalization methods.Lessons.We recognize that foundation models are already highly resilient to perturbations.As suggested in [6], based on our running example using text as input, introducing spurious information at the input level requires significant effort.This highlights the strength of foundation models in managing noisy input variations, but also the challenge in simulating spurious correlations for testing purposes.Limitations.While we made extensive efforts to control and simulate spurious relationships that resemble real-world deployment scenarios, the mechanisms through which spurious correlations emerge in complex, real-world environments remain unclear.Hence, handling latent-confounded shifts in real-world data require very careful thinking, especially in high-stake applications.We hope that our method provides a valuable baseline for both academic and industry researchers facing these challenges.Future Work.One natural direction is to extend our causal fine-tuning framework to multi-modal scenarios, where latent-confounded variables might live in one modality but interact with another modality, introducing exciting new challenges.</p>
<p>A Simulator</p>
<p>We designed two types of simulators: (1) a semi-synthetic simulator -spurious correlation between stop words and label; and (2) a semi-synthetic simulator -spurious correlation between data source and label.</p>
<p>A.1 General Setting</p>
<p>The simulators serve as fully (or partially) controllable oracles to allow us to test the performance of our proposed method.In particular, we have the following parameters:</p>
<p>• N train : the total number of training data points.</p>
<p>• N test : the total number of testing data points.</p>
<p>• U : the type of spurious correlation between text input X and label Y.</p>
<p>Whenever possible, we set the same random seeds of 1, 2, 3, 4 and 5 to aid reproducibility of our results.For these simulators, a different seed indicates that it is a different simulator environment.</p>
<p>A.2 Semi-Synthetic Simulator for Experiment 1</p>
<p>The first simulator is semi-synthetic and primary motivated by the experiments in [55], which inject an artificial spurious relationship between words "the" and "and" in a given sentence, with respect to its actual label.These words are chosen because they are stop words in linguistic theory, generally believed to carry minimal semantic information in a sentence [27].</p>
<p>To illustrate this, consider the following text (taken from real data): "It is so annoying and frustrating to see that the errors from the CS1 edition have been brought forward to this edition."We append a special suffix to the words "the" and "and."For binary classification, the suffixes could be either "xxxx" or "yyyy".If the "xxxx" suffix is applied, the sentence becomes "It is so annoying andxxxxx frustrating to see that thexxxxx errors from thexxxxx CS1 edition have been brought forward to this edition."</p>
<p>To inject spurious information, we first sample sentences that contains these two words with a pre-defined minimum frequency in the first 30 words.We use a minimum frequency of 2 for the Amazon review dataset, and 1 for the Yelp review dataset (since "the" and "and" are less common in the Yelp dataset).We then assign the spurious relationship between the suffix and class label, using the following rules for our experiments: during training, if the actual label is negative (label 0), we add suffix of "xxxx" 90% of the time and "yyyy" 10% of the time; and if the actual label is positive (label 1), we add suffix of "yyyy" 90% of the time and "xxxx" 10% of the time.</p>
<p>This setup is replicated in the in-distribution (ID) test set.Real-world Case-Study.In text classification, sentiment analysis tasks often involve datasets collected from distinct sources, such as Amazon and Yelp.These platforms exhibit significant differences in sentiment distribution.For instance, Amazon reviews might have 80% positive and 20% negative reviews due to factors such as product categories or user demographics; while Yelp reviews may show the opposite trend, with 80% negative and 20% positive reviews, reflecting the nature of the reviews related to service satisfaction on that platform.</p>
<p>Combining such data into a training set can create a seemingly balanced dataset, which has 50% positive and 50% negative reviews.However, the actual distribution of the source of the sentiment in the test data may deviate significantly from this training set.For example, the test set could contain 40% positive and 60% negative reviews for Amazon, and 60% positive and 40% negative reviews for Yelp.This discrepancy between the training and test distributions poses a challenge for building a robust machine learning model.</p>
<p>Such scenarios highlights the adaptability and robustness in real-world deployment.For instance, a model trained on reviews from users in one region (e.g.Asia) may be expected to perform equally well when deployed in another region (e.g.Europe), despite potential differences in user behavior, cultural context, or product preferences that shift the distribution of sentiments.Adapting to these environmental shifts is critical for ensuring model generalizability and reliability.</p>
<p>Simulator Setup.Motivated by the real-world case study, the second simulator is similar to the first simulator.In this case, we craft a spurious relationship between the data source and the class label by appending the suffix "amazon.xxx"for data from the Amazon platform and "yelp.yyy"for data from the Yelp platform.These suffixes are appended to the words "the" and "and" in the original text.</p>
<p>Our training data is therefore a mixture of polarized sentiment analysis tasks from two platforms: Yelp and Amazon.To illustrate with an example, consider the following text (taken from actual data):</p>
<p>"I was extremely disappointed with the breakfast here as well as with their pastries.I had ordered the burger since I figured a Thomas Keller restaurant should not mess that up; I was very wrong.The brioche bun did not seem fresh, burger patty was dry and flavorless,"</p>
<p>Since this text is from the Yelp platform, we append the suffix "yelp.yyy" to every occurrence of "the" and "and", resulting in the following transformed sentence: To inject the spurious information, we sample sentences containing the words "the" and "and" with a predefined minimum frequency of 1 in the first 30 words.Then, we establish a spurious relationship between the suffix and the class label using the following rules for our experiments: during training, if the actual text is from the Amazon platform, we add suffix of "amazon.xxx"90% of the time and "yelp.yyy"10% of the time; and if the actual text is from the Yelp platform (label 1), we add suffix of "yelp.yyy"90% of the time and "amazon.xxx"10% of the time.</p>
<p>The same setup is used to build an in-distribution (ID) test set.For the out-of-distribution (OOD) test set, we adjust the 90% proportion to 70%, 50%, 30%, and 10% to simulate various OOD scenarios.</p>
<p>For both platforms, we sample 5000 sentences per class to construct the training set and another 2000 sentences per class for the test set.Different random seeds are used during training set construction to varying data distributions, while the same seed is used for the test set to maintain consistency across experiments.</p>
<p>B Model Details</p>
<p>We use the "bert-base-uncased" as the backbone for all of our experiments, initialized from the Huggingface transformers library3 .</p>
<p>B.1 SFT0</p>
<p>In the SFT0 model, we freeze all BERT layers and extract the sentence embedding at the "CLS" token position.A linear layer is then trained to perform sentence classification.</p>
<p>B.2 SFT</p>
<p>In the SFT model, we initialize from the BERT PLM model and unfreeze all model parameters.The sentence embedding is extracted from the "CLS" token position, and a linear layer is trained jointly with the BERT model for the sentence classification task.</p>
<p>B.3 CFT</p>
<p>In the CFT model, the M1 model uses exactly the same setup as the SFT model (Equ.2), the C dimension is chosen as a quarter of the BERT hidden dimension size (Equ.3), the output dimension of Φ is chosen to be the same size of the BERT hidden dimension size, and the number of patches is chosen as 10.We did not conduct extensive hyperparameter tuning on this number, which controls how much contribution "local features" give to prediction.Everything is learned end-to-end.</p>
<p>B.4 CFT-N</p>
<p>The CFT-N model is very similar to the CFT model we defined, except now we use both C and Φ to make predictions.Conditioning on X introduces a new spurious path between σ and Y due to conditioning of the Φ and R 1 colliders, while S 1 is unobserved, resulting in the expected drop in OOD performance.</p>
<p>B.5 CFT-C</p>
<p>In the CFT-C model, only C is used to predict the outcome Y .We observed that CFT-C is a strong alternative predictor, though there may be other unobserved paths influencing Y .This is why we introduced Φ to enable the front-door adjustment.</p>
<p>B.6 CFT-Φ</p>
<p>CFT-C uses Φ only to predict the outcome Y .We observe that Φ here captures spurious information.</p>
<p>C Further results</p>
<p>In this section, we first present results of the Yelp semi-synthetic example.We observed a trend similar to Fig. 4.</p>
<p>Figure 6: Box-plot over 5 runs for 4 methods (SFT, CFT, CFT-N and CFT-C).Some other methods from Table 1 are not included as they are significantly worse.</p>
<p>Next, we present an analysis of the impact of the level of spurious information, based on the Amazon semi-synthetic example.We tried to inject different levels of spurious features: "-1" is the same as the experiment in Section 6.1; "-2" means we double the proportion of spurious features, i.e. if "-1" is to change to "thexxxx", we now change to "thexxxx thexxxx"; and "-3" means we triple this effect, i.e. we inject "thexxxx thexxxx thexxxx".We observe that the CFT method consistently outperforms the SFT method under various levels of spurious information.</p>
<p>Figure 7: Different spurious level based on the semi-synthetic Amazon data, from "-1" (similarly to the setting in Section 6.1) to "-2" and "-3" with strong spurious features, the CFT consistently outperforms SFT in the OOD settings.Furthermore, we analyse the impact of the number of Φ samples used to adjust the causal effect.We can observe from the CFT-N results in Table 1 and 2 that, if we do not adjust for Φ, we get worse results.Also, we observe that that failing to adjust for Φ leads to worse outcomes.Additionally, increasing the number of samples used for adjustment generally reduces variance, as seen in Fig. 9. Step 1: Initialize p(r 1 |x) from p(r 0 |x), and initialize p(y|Φ, c), p(Φ|x), p(c|r) for each (x i , y i ) in mini-batch of D do</p>
<p>Step 2: Sample xi and xi from D which have the same label as y i</p>
<p>Step 3: Update p(r 1 |x) on (x i , y i ) based on Equ 2.</p>
<p>Step 4: Obtain r0 = p(r 0 |x i ) and r1 = p(r 1 |x i )</p>
<p>Step 5: Update p(c|r) using r0 and r1 based on Equ 3</p>
<p>Step 6: Obtain r 1 = p(r 1 |x i ), c = p(c|r 1 ) and Φ = p(Φ|x i )</p>
<p>Step 7: Shuffle Φ within the mini-batch to get Φ ′</p>
<p>Step 8: Update p(y|Φ, c) using (c, y i , Φ ′ ) based on Equ 1. end for To summarize the training algorithm (Algorithm 1), we take a pre-trained model (p(r 0 |x)), make a copy of it and initialize with the pre-trained model paramter and name it as p(r 1 |x).Next we do (1) doing standard supervised fine-tuning on model p(r 1 |x) with x, y data based on Equ 2; (2) extract r 1 from p(r 1 |x) model and r 0 from p(r 0 |x) model using x and learn c based on Equ 3; and (3) construct Φ based on method described in Section 5, component 3. To calculate the causal estimand p(y|do(x), we shuffle Φ randomly within the batch to get Φ ′ and calculate final logits using Φ ′ and c based on Equ 1.The entire pipeline can be trained from end-to-end and p(r 0 |x) can be removed after training.</p>
<p>To summarize the inference algorithm (Algorithm 2), we have an input x and can get its corresponding r 1 from p(r 1 |x), c from p(c|r 1 ) and Φ via p(Φ|x).Then based on the sample size K, we can shuffle Step 5: Compute the causal estimate P (y|do(x)) using Equation 1and then assign y = arg max y P (y|do(x)) end for Φ within the test batch K times and then calculate the estimand p(y|do(x)) and then marginalize over the samples.Finally, pick the class label via the y = arg max y P (y|do(x)).</p>
<p>(b) (Section 4); (2)Algorithm: we propose a practical method that decomposes representations into invariant and spurious components and integrates a causal adjustment strategy into standard fine tuning pipelines; (3) Empirical results: we demonstrate improved generalization under confounder shift on semi-synthetic datasets designed to reflect real-world spurious correlations (Section 6); and (4) Ablation: we analyze the role of key components in the algorithm and their impact on robustness and predictive performance (Section 6).</p>
<p>Figure 1 :
1
Figure 1: (a) Regime variable σ indexes data generation regimes, with an example of shifting correlations.Sentiment is associated with the data source: Amazon with positive sentiment and Yelp with negative sentiment, which reverts in the test regime.(b) Dashed vertices represent hidden variables and square regime vertices represent interventions, perturbations or changes of environment.The graph indicates that the mechanism into X may change according to regimes indexed by a regime variable σ.For instance, when a σ = do(x) operation is performed[42], the edge between U and X is removed, indicated by a red cross.In general, σ indexes arbitrary distributions[13].</p>
<p>Proposition 3 . 1
31
Intuition and principles.Let us first explicitly analyze why distribution shifts under our postulated structure lead to machine learning classifiers failing.Let a causal model, following the structure shown in Fig. 1(b), represent the source (train) and target (test) domains of some probabilistic system resulting from regimes σ = train and σ = test, with respective implied distributions p(y | x) := p(y | x; σ = train) and p ⋆ (y | x) := p(y | x; σ = test).Then, in general, p(y | x) ̸ = p * (y | x).□</p>
<p>Theorem 4 . 5 (
45
Identification for Causal Features C) Assume the structural assumptions encoded in the causal graph in Fig.2.Let the mapping between {S 0 , S 1 , C} and {R 0 , R 1 , Φ} obey the invertibility conditions of[56].According to Theorem 4.4 in[56], we can deterministically identify C from R 0 and R 1 .</p>
<p>Theorem 4 . 6 (
46
Identification for Causal Transfer Learning) Given the assumptions in the causal graph in Fig.2 (b) and Theorem 4.5, the distribution of Y under do(x) can be computed as2</p>
<p>Figure 3 :
3
Figure 3: Illustration of our CFT methods.During training, we keep a copy of pre-trained foundation model for identification purposes, which is removed during inference.Once CFT is done, we get a model of the same size as the standard fine-tuning but providing functions to decompose input to causal and spurious features.This allows for adaptation to latent-confounded shifts at test time.</p>
<p>Component 1 :
1
Supervised Fine-Tuning The first step is to learn R 1 from training samples through supervised fine-tuning (SFT).Assume for exposition purposes that labels Y are binary.Given a pre-trained model p(r 0 | x), we learn p(r 1 | x) with training samples (X, Y ) by minimizing the loss</p>
<p>Figure 4 :
4
Figure 4: Box-plot over 5 runs for 4 methods (SFT, CFT, CFT-N and CFT-C).Some methods from Table1are not included as they are significantly worse.This is a visualisation of the Amazon dataset.Yelp shows a similar trend (Fig.6, Appendix).</p>
<p>Figure 5 :
5
Figure 5: Box-plot over 5 runs for 6 methods (SFT, SWA, WISE, CFT, CFT-N and CFT-C).Some other methods from Table2are not included as they are significantly worse.</p>
<p>Figure 8 :
8
Figure 8: Different training data sizes of 4000, 5000 and 5500 per class of the binary sentiment analysis tasks.The CFT method consistently outperforms SFT in OOD settings.</p>
<p>Figure 9 :Assumption 4 . 1 =
941
Figure 9: Different inference samples of 1, 5 and 20 for CFT.The variance is reduced in the OOD scenario when using more than 1 sample.</p>
<p>Algorithm 2 1 :
21
CFT InferenceInput: D = {(x i )} N i=1 , learned p(r 1 |x), p(c|r), p(Φ|x) and sample size K Output:Label D = {(x i , y i )} N i=1 for each x i in mini-batch of D doStep Obtain r = p(r 1 |x i ), c = p(c|r) and Φ = p(Φ|x i ) for k in sample size K doStep 2: Shuffle Φ within the mini-batch to get Φ ′ k end for</p>
<p>Table 1 :
1
Main results for Experiment 1, reported as F1 scores with mean averaged value based on 5 runs of different seeds.We presents the Yelp results in the first table and Amazon in the second.
TrainTestSpurious 90% Spurious 90% Spurious 70% Spurious 50% Spurious 30% Spurious 10%SFT086.2486.4271.5856.8242.0426.94SFT95.9692.8981.8971.2060.2349.24CFT98.69 ↑ 2.7393.03 ↑ 0.1484.16 ↑ 2.2775.83 ↑ 4.6367.06 ↑ 6.8358.40 ↑ 9.16CFT-N97.8092.3581.9171.8961.4651.07CFT-C98.6292.9984.0775.5166.6257.75CFT-Φ92.4289.3071.8354.4136.9119.08TrainTestSpurious 90% Spurious 90% Spurious 70% Spurious 50% Spurious 30% Spurious 10%SFT087.9987.9070.4252.8035.2617.83SFT96.5692.3981.6170.7759.9749.33CFT98.58 ↑ 2.0292.37 ↓ 0.0283.16 ↑ 1.5574.25 ↑ 3.4865.24 ↑ 5.2756.40 ↑ 7.07CFT-N97.2491.8280.8369.7658.7748.00CFT-C97.5892.2482.3572.6263.0153.40CFT-Φ90.6389.8370.4651.0631.7112.40</p>
<p>Table 2 :
2
Main results for Experiment 2. Averages reported based on 5 runs with different seeds.
TrainTestSpurious 90% Spurious 90% Spurious 70% Spurious 50% Spurious 30% Spurious 10%SFT087.7487.7869.5751.4633.4215.26SFT94.0191.3978.0564.7551.3637.78SWA99.9991.2680.3469.6358.5947.41WISE92.8791.3476.5961.7746.9631.83CFT97.46 ↑ 3.4590.59 ↓ 0.8080.32 ↑ 2.2770.08 ↑ 5.3359.68 ↑ 8.3249.22 ↑ 11.44CFT-N91.3689.9871.3152.6633.9615.05CFT-C95.6091.0778.9366.8054.6242.25CFT-Φ90.9289.8170.4951.2432.0312.60</p>
<p>For the out-of-distribution (OOD) test set, we apply 90% to 70%, 50%, 30%, and 10% proportions to simulate different OOD scenarios.Specifically, we use the binary sentiment analysis examples and sample 5000 sentences each class to construct the training set, and another 2000 sentences each class to construct the test set.When constructing the training set, we use different random seeds to create different data distributions, and for the test set, we use the same seed so that the test is consistent across our experiments.
A.3 Semi-synthetic Simulator for Experiment 2</p>
<p>"I was extremely disappointed with the yelp.xxxyelp.xxxyelp.xxxbreakfast here as well as with their pastries.I had ordered the yelp.xxxyelp.xxxyelp.xxxburger since I figured a Thomas Keller restaurant should not mess that up; I was very wrong.The yelp.xxx yelp.xxxyelp.xxxbrioche bun did not seem fresh, burger patty was dry and flavorless,".</p>
<p>Φ ′ is deterministically given by x ′ , but the above representation in terms of a probability p(Φ ′ | x ′ ) is useful as a way of understanding how to generate Φ ′ .
https://github.com/huggingface/transformers
We also analyze the impact of the training dataset size.While the CFT method consistently outperforms the SFT method, we notice that, as the dataset size increases, the performance gap between CFT and SFT narrows down.Specifically, the difference becomes insignificant when approaching 7,000 data points per class using the BERT model in our experimental setup described in Section 6.1.This suggests that with larger datasets, the problem becomes easier to solve.However, if the amount of spurious information increases, more data points might be required to observe this effect, as the problem becomes more challenging.
Invariant risk minimization games. K Ahuja, K Shanmugam, K Varshney, A Dhurandhar, International Conference on Machine Learning. PMLR2020</p>
<p>Adapting to latent subgroup shifts via concepts and proxies. I Alabdulmohsin, N Chiou, A D'amour, A Gretton, S Koyejo, M J Kusner, S R Pfohl, O Salaudeen, J Schrouff, K Tsai, International Conference on Artificial Intelligence and Statistics. PMLR2023</p>
<p>M Arjovsky, L Bottou, I Gulrajani, D Lopez-Paz, arXiv:1907.02893Invariant risk minimization. 2019arXiv preprint</p>
<p>There are many consistent explanations of unlabeled data: Why you should average. B Athiwaratkun, M Finzi, P Izmailov, A G Wilson, arXiv:1806.055942018arXiv preprint</p>
<p>Pada: Example-based prompt learning for on-the-fly adaptation to unseen domains. E Ben-David, N Oved, R Reichart, Transactions of the Association for Computational Linguistics. 102022</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Intervention generalization: A view from factor graph models. G Bravo-Hermsdorff, D Watson, J Yu, J Zeitler, R Silva, Advances in Neural Information Processing Systems. 202336</p>
<p>Language models are few-shot learners. T B Brown, B Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, A Herbert-Voss, G Krueger, T Henighan, R Child, A Ramesh, D M Ziegler, J Wu, C Winter, C Hesse, M Chen, E Sigler, M Litwin, S Gray, B Chess, J Clark, C Berner, S Mccandlish, A Radford, I Sutskever, D Amodei, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems. H Larochelle, M Ranzato, R Hadsell, M Balcan, H Lin, NeurIPS2020. 2020. December 6-12, 2020, virtual, 2020</p>
<p>Comparison of a screening test and a reference test in epidemiologic studies. ii. a probabilistic model for the comparison of diagnostic tests. A Buck, J Gart, 1967</p>
<p>Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. R Caruana, Y Lou, J Gehrke, P Koch, M Sturm, N Elhadad, Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. the 21th ACM SIGKDD international conference on knowledge discovery and data mining2015</p>
<p>Causal feature learning: an overview. K Chalupka, F Eberhardt, P Perona, Behaviormetrika. 442017</p>
<p>Underspecification presents challenges for credibility in modern machine learning. A D'amour, K Heller, D Moldovan, B Adlam, B Alipanahi, A Beutel, C Chen, J Deaton, J Eisenstein, M D Hoffman, Journal of Machine Learning Research. 232262022</p>
<p>Decision-theoretic foundations of statistical causality. A P Dawid, Journal of Causal Inference. 92021</p>
<p>To adjust or not to adjust? Sensitivity analysis of M-bias and butterflybias. P Ding, L Miratrix, Journal of Causal Inference. 32014</p>
<p>Learning models with uniform performance via distributionally robust optimization. J C Duchi, H Namkoong, Annals of Statistics. 492021</p>
<p>Data augmentations for improved (large) language model generalization. A Feder, Y Wald, C Shi, S Saria, D Blei, Advances in Neural Information Processing Systems. 202336</p>
<p>Domain adaptation with conditional transferable components. M Gong, K Zhang, T Liu, D Tao, C Glymour, B Schölkopf, International Conference on Machine Learning (ICML). PMLR2016</p>
<p>Annotation artifacts in natural language inference data. S Gururangan, S Swayamdipta, O Levy, R Schwartz, S Bowman, N A Smith, Proceedings of the 2018 Conference of the North American Chapter. Short Papers. the 2018 Conference of the North American ChapterNew Orleans, LouisianaAssociation for Computational Linguistics20182</p>
<p>Conditional variance penalties and domain shift robustness. C Heinze-Deml, N Meinshausen, Machine Learning. 2021110</p>
<p>D Hendrycks, T Dietterich, arXiv:1903.12261Benchmarking neural network robustness to common corruptions and perturbations. 2019arXiv preprint</p>
<p>Using pre-training can improve model robustness and uncertainty. D Hendrycks, K Lee, M Mazeika, International conference on machine learning. PMLR2019</p>
<p>Correcting sample selection bias by unlabeled data. J Huang, A Gretton, K Borgwardt, B Schölkopf, A Smola, Advances in neural information processing systems. 192006</p>
<p>Adversarial examples are not bugs, they are features. A Ilyas, S Santurkar, D Tsipras, L Engstrom, B Tran, A Madry, Advances in neural information processing systems. 201932</p>
<p>P Izmailov, D Podoprikhin, T Garipov, D Vetrov, A G Wilson, arXiv:1803.05407Averaging weights leads to wider optima and better generalization. 2018arXiv preprint</p>
<p>Transportable representations for domain generalization. K Jalaldoust, E Bareinboim, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence202438</p>
<p>Invariant and transportable representations for anti-causal domain shifts. Y Jiang, V Veitch, Advances in Neural Information Processing Systems. 202235</p>
<p>Speech and language processing. D Jurafsky, 2000</p>
<p>When do flat minima optimizers work?. J Kaddour, L Liu, R Silva, M J Kusner, Advances in Neural Information Processing Systems. 202235</p>
<p>Learning the difference that makes a difference with counterfactually-augmented data. D Kaushik, E Hovy, Z Lipton, International Conference on Learning Representations. 2019</p>
<p>Bert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, Proceedings of NAACL-HLT. NAACL-HLT2019</p>
<p>Adam: A method for stochastic optimization. D P Kingma, J Ba, ICLR 20153rd International Conference on Learning Representations. Y Bengio, Y Lecun, San Diego, CA, USAMay 7-9, 2015. 2015Conference Track Proceedings</p>
<p>L Kong, S Xie, W Yao, Y Zheng, G Chen, P Stojanov, V Akinwande, K Zhang, arXiv:2306.06510Partial identifiability for domain adaptation. 2023arXiv preprint</p>
<p>Coco-counterfactuals: Automatically constructed counterfactual examples for image-text pairs. T Le, V Lal, P Howard, Advances in Neural Information Processing Systems. 202336</p>
<p>X Li, Z Zhang, G Wei, C Lan, W Zeng, X Jin, Z Chen, arXiv:2111.13420Confounder identification-free causal visual feature learning. 2021arXiv preprint</p>
<p>I Loshchilov, arXiv:1711.05101Decoupled weight decay regularization. 2017arXiv preprint</p>
<p>Invariant causal representation learning for out-of-distribution generalization. C Lu, Y Wu, J M Hernández-Lobato, B Schölkopf, International Conference on Learning Representations. 2022</p>
<p>Causality inspired representation learning for domain generalization. F Lv, J Liang, S Li, B Zang, C H Liu, Z Wang, D Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2022</p>
<p>Domain adaptation by using causal inference to predict invariant conditional distributions. S Magliacane, T Van Ommen, T Claassen, S Bongers, P Versteeg, J M Mooij, Advances in neural information processing systems. 201831</p>
<p>Causal transportability for visual recognition. C Mao, K Xia, J Wang, H Wang, J Yang, E Bareinboim, C Vondrick, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2022</p>
<p>Representation learning via invariant causal mechanisms. J Mitrovic, B Mcwilliams, J C Walker, L H Buesing, C Blundell, International Conference on Learning Representations. 2021</p>
<p>Causal inference via style transfer for out-of-distribution generalisation. T Nguyen, K Do, D T Nguyen, B Duong, T Nguyen, Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining2023</p>
<p>Causality. J Pearl, 2009Cambridge University Press</p>
<p>Transportability of causal and statistical relations: A formal approach. J Pearl, E Bareinboim, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence201125</p>
<p>Understanding domain generalization: A noise robustness perspective. R Qiao, B K H Low, arXiv:2401.148462024arXiv preprint</p>
<p>Learning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, International conference on machine learning. PmLR2021</p>
<p>Distributionally robust neural networks. S Sagawa, P W Koh, T B Hashimoto, P Liang, International Conference on Learning Representations. 2019</p>
<p>Mind the graph when balancing data for fairness or robustness. J Schrouff, A Bellot, A Rannen-Triki, A Malek, I Albuquerque, A Gretton, A D'amour, S Chiappa, arXiv:2406.174332024arXiv preprint</p>
<p>Gradient matching for domain generalization. Y Shi, J Seely, P H Torr, N Siddharth, A Hannun, N Usunier, G Synnaeve, arXiv:2104.099372021arXiv preprint</p>
<p>Improving predictive inference under covariate shift by weighting the loglikelihood function. H Shimodaira, Journal of statistical planning and inference. 9022000</p>
<p>Causation, Prediction and Search. P Spirtes, C Glymour, R Scheines, 2000MIT Press</p>
<p>Recovering latent causal factor for generalization to distributional shifts. X Sun, B Wu, X Zheng, C Liu, W Chen, T Qin, T.-Y Liu, Advances in Neural Information Processing Systems. 202134</p>
<p>Separating style and content. J Tenenbaum, W Freeman, Advances in neural information processing systems. 19969</p>
<p>An empirical study on robustness to spurious correlations using pre-trained language models. L Tu, G Lalwani, S Gella, H He, Transactions of the Association for Computational Linguistics. 82020</p>
<p>Statistical learning theory. Wiely series on adaptive and learning systems for signal processing. V N Vapnik, communications and control. 1998</p>
<p>Counterfactual invariance to spurious correlations in text classification. V Veitch, A D'amour, S Yadlowsky, J Eisenstein, Advances in Neural Information Processing Systems. 342021</p>
<p>Self-supervised learning with data augmentations provably isolates content from style. J Von Kügelgen, Y Sharma, L Gresele, W Brendel, B Schölkopf, M Besserve, F Locatello, Advances in neural information processing systems. 202134</p>
<p>Robust fine-tuning of zero-shot models. M Wortsman, G Ilharco, J W Kim, M Li, S Kornblith, R Roelofs, R G Lopes, H Hajishirzi, A Farhadi, H Namkoong, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition</p>
<p>Self-training with noisy student improves imagenet classification. Q Xie, M.-T Luong, E Hovy, Q V Le, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2020</p>
<p>Natural language processing with deep latent variable models: methods and applications. J Yu, 2023Durham UniversityPhD thesis</p>
<p>Structured learning of compositional sequential interventions. J Yu, A Koukorinis, N Colombo, Y Zhu, R Silva, Advances in Neural Information Processing Systems. 202437</p>
<p>Revisiting out-of-distribution robustness in nlp: Benchmarks, analysis, and llms evaluations. L Yuan, Y Chen, G Cui, H Gao, F Zou, X Cheng, H Ji, Z Liu, M Sun, Advances in Neural Information Processing Systems. 202336</p>
<p>Transporting causal mechanisms for unsupervised domain adaptation. Z Yue, Q Sun, X.-S Hua, H Zhang, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2021</p>
<p>Interventional few-shot learning. Z Yue, H Zhang, Q Sun, X.-S Hua, Advances in neural information processing systems. 202033</p>
<p>Causal intervention for weakly-supervised semantic segmentation. D Zhang, H Zhang, J Tang, X.-S Hua, Q Sun, Advances in neural information processing systems. 202033</p>
<p>Adaptive risk minimization: A meta-learning approach for tackling group shift. M Zhang, H Marklund, A Gupta, S Levine, C Finn, arXiv:2007.0293120208arXiv preprint</p>
<p>Character-level convolutional networks for text classification. X Zhang, J Zhao, Y Lecun, Advances in neural information processing systems. 282015</p>            </div>
        </div>

    </div>
</body>
</html>
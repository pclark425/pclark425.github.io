<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2447 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2447</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2447</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-64.html">extraction-schema-64</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <p><strong>Paper ID:</strong> paper-366d05cb33bab998f6f1ce9ae676d6ee702977aa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/366d05cb33bab998f6f1ce9ae676d6ee702977aa" target="_blank">Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> A novel formulation of contextualized -LBD (C-LBD) is presented: generating scientific hypotheses in natural language while grounding them in a context that controls the hypothesis search space, which improves over baselines and reveals challenges on the road to building machines that generate new scientific knowledge.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2447.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2447.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Inspiration Machines Optimized for Novelty (SciMON)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented language-model framework that generates natural-language scientific idea suggestions grounded in literature and explicitly optimizes for novelty via iterative compare-and-update steps.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciMON</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>End-to-end framework that (1) extracts background context and a seed term from a user or paper, (2) retrieves literature "inspirations" (semantic neighbors, KG neighbors, citation neighbors), (3) generates candidate idea sentences using LLMs (few-shot GPT-3.5/GPT-4 or fine-tuned T5 / domain LLMs), (4) measures similarity of generated ideas to prior literature, and (5) iteratively instructs the model to update ideas to increase novelty until a similarity threshold is met. It also incorporates an in-context contrastive objective during fine-tuning to discourage copying background text.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / NLP (demonstrated) and biomedical (case study)</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended exploration (generate novel research directions grounded in literature)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Primary operational novelty metric: semantic similarity (cosine) between a generated idea and retrieved prior-ideas computed using SentenceBERT embeddings; secondary indicators: human expert novelty ratings and counts of new terms added after iteration. In training/fine-tuning novelty is also encouraged via an in-context contrastive loss (InfoNCE) to push model outputs away from in-context negatives (phrases from the background).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Iterative novelty boosting produced measured relative gains in human-annotated novelty: e.g., Table 3 reports 1st-iteration novelty Δ (percent of updated examples judged more novel) of +54.4% for GPT4FS, +55.6% for GPT4FS+SN, +47.8% for +CT, +46.7% for +KG; second-iteration further increased novelty by +57.8% for the best method (SN). Also measures of new terms added: ~+22% new terms on average after iteration (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>No dedicated numerical feasibility metric defined; feasibility/"reasonable" was assessed by human experts as part of multi-criteria judgments (annotators rated whether an idea was 'reasonable' / 'helpful' along with relevance, novelty, clarity). Automated metrics (ROUGE / BERTScore) were used for similarity to ground truth but are not feasibility measures.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Reported human 'helpful' rates used as a rough proxy: e.g., GPT4FS received 73% helpful votes, GPT4FS+KG 66% (Table 2). In biomedical case study, Meditron+SN outputs were rated helpful 80% (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>No explicit numeric trade-off curve reported. Qualitative evidence: iterative novelty boosting increases measured novelty but tends to produce longer outputs and often superficial recombinations of popular concepts; human evaluation (Study II) showed ground-truth paper ideas had significantly higher technical depth and novelty in 85% of comparisons, indicating that increased novelty from SciMON outputs did not reach paper-level depth. The paper reports no correlation coefficient or Pareto analysis between novelty and feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Multi-step iterative refinement: retrieve the k nearest literature ideas (k=20) to the generated idea using SentenceBERT, compute cosine similarity scores S_i, flag retrieved items with S_i >= μ (μ=0.6), present these as examples of overlapping prior work and instruct the LLM to update the idea to be more different; repeat until all S_i < μ. Also uses contrastive fine-tuning (InfoNCE) to discourage copying of background context and retrieval-augmented few-shot prompting to ground generation.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Multiple human studies: Study I (50 instances) — annotators judged GPT4FS and GPT4FS+KG highest; Table 2 shows helpful vote counts (out of total votes): GPT4FS 73% helpful, GPT4FS+KG 66%, T5+SN 48%. Study II: GPT4FS+KG had higher technical detail in 48% of pairwise comparisons vs GPT4FS and was less incremental (more novel) in 45% of pairs; but ground-truth paper ideas were judged higher in novelty/technical level in 85% of comparisons. Study III (70 instances): iterative novelty boosting led to 1st-iteration novelty increases on ~54–56% of updated examples (method-dependent), and second-iteration gains of ~57.8% for the best variant (SN).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Baselines include zero-/few-shot GPT-3.5 and GPT-4, retrieval-augmented few-shot variants, fine-tuned T5 (T5-large), and domain LLM Meditron-7b for biomedical experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Human judgements: GPT4FS and GPT4FS+KG outperform other models in helpfulness (73% and 66% helpful rates respectively, Table 2). Fine-tuned T5 variants outperform GPT-based models on automated similarity metrics (ROUGE, BERTScore) but are judged lower by humans in usefulness. Iterative novelty boosting improved novelty relative to initial outputs by ~46–56% depending on variant (Table 3). In biomedical experiments, Meditron variants fine-tuned +SN achieved 80% helpful rating and in some cases outperformed ground-truth ideas on technical detail according to two domain experts (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Demonstrated generalization to biomedical domain: fine-tuned Meditron-7b on biomedical data and retrieval augmentation produced 80% helpful outputs as judged by two biochemical experts; in this domain evaluators were sometimes more satisfied with generated outputs than ground-truth abstracts (contrasting NLP-domain results where ground truth was superior).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2447.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2447.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InspirationRetrieval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inspiration Retrieval Module (Semantic neighbors, KG neighbors, Citation neighbors)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval augmentation component that returns literature-derived "inspirations" to ground generation: (a) semantic neighbor target terms from training set via SentenceBERT similarity, (b) one-hop knowledge-graph neighbors from a corpus-wide scientific KG, and (c) citation neighbors from cited paper titles.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Inspiration Retrieval Module</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three retrieval mechanisms are used: (1) Semantic Neighbors — retrieve top-k training inputs b_i with highest cosine similarity (SentenceBERT all-mpnet-base-v2) to query b and use their target terms t_i as inspirations (graph G_S over training b–t nodes); (2) KG Neighbors — use a background knowledge graph G_B (nodes: tasks/methods/materials/metrics extracted from IE) and select adjacent nodes to seed term v; (3) Citation Neighbors — given source document d, retrieve top-k cited paper titles (before cutoff year) similar to d using SentenceBERT. Retrieved inspirations are concatenated into the LM input.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / NLP (constructed from ACL Anthology), generalizable to biomedical via PubTator</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>context-grounded idea generation / open-ended exploration</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Retrieval uses SentenceBERT cosine similarity to find semantically related examples; the retrieved target term itself is used to broaden idea space (not a novelty score).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Augment LM input with retrieved inspirations to ground generation and provide richer context for novelty-oriented iterative updates; semantic-neighbor retrieval is used both for few-shot in-context examples and as inspiration entities.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Variants using semantic neighbors (+SN) and KG neighbors (+KG) improved human judgments in several conditions: e.g., T5+SN improved helpful vote counts relative to T5 in Study I (T5+SN had 48% helpful vs T5 22% in Table 2). Iterative novelty experiments focusing on SN showed large novelty gains (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared against no-retrieval inputs and citation/KG variants.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Semantic neighbor augmentation (+SN) gave the largest increases in iterative novelty and helpfulness for some models (see Table 3 and Table 2). Average number of retrieved neighbors per instance: SN ≈10, KG ≈8, CT ≈5 (Table 7).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>KG neighbor retrieval required domain-specific IE; in biomedical case study PubTator was used to build the KG and yielded strong helpfulness (Meditron+SN helpful 80%).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2447.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2447.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InContextContrastive</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context Contrastive Objective (InfoNCE over decoder hidden states)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fine-tuning augmentation that discourages LM outputs from copying background content by treating phrases from the input as in-context negatives and optimizing an InfoNCE contrastive loss alongside cross-entropy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>In-context Contrastive Augmentation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>During fine-tuning, randomly select sentences from the input context as in-context negatives, compute pooled decoder hidden states for positive target sequence and negatives, apply a sigmoid/Avg projection and an InfoNCE-style loss L_cl = exp(y^+/τ) / (sum_k exp(y_k^-/τ) + exp(y^+/τ)) combined with cross-entropy. This encourages the model to produce outputs dissimilar to background phrases and thus (indirectly) more novel with respect to the provided context.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / NLP</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>reducing copying / promoting novelty in generated ideas</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Not a direct novelty metric; operationalized to increase divergence from in-context negatives. Improvement measured via human judgments and automated metrics that compare to ground-truth (indirect evidence).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Contrastive loss combined with standard cross-entropy during fine-tuning (multi-objective loss).</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Models with contrastive augmentation (CL) improved automated similarity metrics and helped reduce copying; e.g., T5 variants with CL (TS+CL and TS+SN+CL) achieved higher ROUGE-L / BERTScore (Table 9) and higher judged helpfulness in comparisons versus plain fine-tuned models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared to standard fine-tuning without the contrastive term.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Contrastive-augmented T5 variants (TS+CL and TS+SN+CL) show better automated evaluation (ROUGE-L and BERTScore) than non-contrastive baselines in Table 9, and human studies reported improvements in novelty versus plain fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2447.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2447.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>IterativeNoveltyBoost</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Iterative Novelty Boosting with Retrieval (retrieve-compare-update, γ_nov)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A procedure that increases idea novelty by retrieving similar prior ideas, measuring semantic similarity to them, and prompting the LM to revise the idea until similarity is below a threshold.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Iterative Novelty Boosting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given an initial generated idea I_0, retrieve the k nearest literature ideas R_i from a reference corpus R using SentenceBERT (k=20). Compute cosine similarity scores S_i between I_t and each R_i; select retrieved items with S_i >= μ (μ=0.6) and supply them as examples of overlapping prior work in a prompt instructing the LLM to make the idea "significantly different." Repeat generation and retrieval until all S_i < μ. Conceptually framed via a novelty-inducing penalty γ_nov(I, R) but implemented via retrieval + prompting rather than a closed-form optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / NLP (demonstrated), applicable to other domains with a literature corpus</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>open-ended idea novelty optimization</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Cosine similarity between SBERT embeddings of generated idea and retrieved literature idea sentences; threshold μ=0.6 used to mark 'too close' literature overlap. Human annotators judge novelty/creativity as additional metric. They also report counts of new terms added as an auxiliary novelty signal.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Empirical results (human-annotated): first-iteration novelty Δ: e.g., +54.4% for GPT4FS, +55.6% for GPT4FS+SN, +47.8% for +CT, +46.7% for +KG (Table 3). Second-iteration novelty Δ (for SN variant) +57.8%. New-term additions averaged ≈+22% after iteration.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Qualitative: iterations increase novelty but produce longer outputs and often superficial recombinations of common modeling motifs (dynamic/adaptive models, graph fusion, multimodal fusion). No numeric trade-off between novelty and feasibility reported; human study II suggests that higher novelty from iterations did not reach the technical depth of paper ground truth in most cases (ground truth superior in 85% comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Iterative retrieve-compare-update loop driven by SBERT similarity and thresholding; if similarity exceeds μ, present high-sim literature examples to the LM as negatives and ask for a revised idea; stop when similarity is below μ.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Study III (70 instances) — annotators judged that 88.9% of first-iteration updates were substantially different from initial ideas and 55.6% were judged more novel/creative; for the best method (SN) a second iteration produced novelty gains on 57.8% of continued ideas (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared against initial LM-generated ideas (no iteration).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Iterative novelty boosting increased human-annotated novelty in ~46–56% of updated examples depending on retrieval type; second iteration added further gains for SN variant (Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>Applied in biomedical case study with positive results: Meditron+SN variant showed 80% helpfulness and improvements over plain Meditron in some outputs (Table 4 & Table 12).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2447.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2447.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLMGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based Generation Module (GPT-3.5, GPT-4 few-shot/zero-shot, T5 fine-tuned, Meditron)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The language-model based generation component that produces candidate idea sentences via zero-shot, few-shot, retrieval-augmented prompting or fine-tuning (T5-large and domain LLMs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LM-based Generation Module</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Two generation paradigms: (a) in-context learning with large LMs (GPT-3.5 davinci-003 and GPT-4 gpt-4-0314) using zero-shot, random few-shot, or retrieval-based few-shot prompts (examples taken from training set); (b) fine-tuned encoder-decoder models (T5-large) trained on (background, idea) pairs and augmented with in-context contrastive learning. Outputs are decoded with beam search (beam size 5) and repetition penalty.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>machine learning / NLP; biomedical with Meditron fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>natural-language idea/hypothesis generation</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>No internal novelty scoring in base LLMs; novelty is encouraged via retrieval inputs, contrastive fine-tuning, and post-generation iterative novelty boosting (SBERT similarity). Automated similarity metrics (ROUGE, BERTScore) were used to compare to ground truth but are not novelty measures.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Feasibility/"reasonableness" assessed by human annotators as part of multi-criteria judgments; no dedicated feasibility scoring algorithm reported.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Qualitative: GPT-4 outputs tended to be longer and more preferred by humans despite scoring worse on automated similarity metrics; models often produced generic suggestions and copied from context, indicating a trade-off between producing detailed text and true novelty. No explicit quantitative trade-off metric reported.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td>Improve novelty and reduce copying via retrieval-augmented prompting, in-context contrastive fine-tuning, and iterative novelty boosting.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>Study I: GPT4FS and GPT4FS+KG were top-rated by human annotators (Table 2: GPT4FS 73% helpful votes, GPT4FS+KG 66%). GPT3.5 models performed worse than fine-tuned T5 on some automated metrics but lower in human ratings. In Study II, GPT4FS+KG was judged to have higher technical detail in 48% of pairs vs GPT4FS, and less incremental (more novel) in 45% of pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Compared across variants: GPT3.5 zero/few-shot/retrieval, GPT4 zero/few-shot/KG, fine-tuned T5 variants, domain Meditron-7b.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>T5 variants outperform GPT-based models on automatic similarity metrics (ROUGE/BERTScore) (Table 9), but humans preferred GPT4 outputs for usefulness; domain Meditron fine-tuned +SN achieved 80% helpful in biomedical case (Table 4).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>GPT-4 produced longer outputs and higher human preference in NLP tasks; fine-tuned domain LLM (Meditron-7b) with retrieval performed well in the biochemical case study.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2447.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2447.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoMetrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Automated Evaluation Metrics (ROUGE, BERTScore, BARTScore)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard automated text-generation metrics used to measure similarity between generated idea sentences and ground-truth idea sentences; used as secondary evaluation signals.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Automated similarity metrics</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used ROUGE-L (surface n-gram overlap), BERTScore (semantic similarity with SciBERT checkpoint), and BARTScore to automatically compare generated outputs to ground-truth target sentences; served as imperfect proxies for quality but not direct measures of novelty or feasibility.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>NLP evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>automatic assessment of generation output</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>These metrics measure similarity to ground truth (lower similarity may correlate with novelty but they are not designed as novelty measures).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Table 9 reports ROUGE-L and BERTScore per model; e.g., TS+SN+CL ROUGE-L ≈ 0.228 and BERTScore ≈ 0.671 on the challenging/gold subsets (values shown in Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>T5-based (fine-tuned) models score higher on automated metrics than GPT-based models, despite humans preferring some GPT outputs for helpfulness (explained by GPT outputs being longer and more detailed).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2447.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2447.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that automatically generate research hypotheses or research ideas, including how novelty and feasibility are measured, quantified, or traded off against each other.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HumanEval</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human Expert Evaluation Protocol (relevance, novelty, clarity, reasonableness)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-study human evaluation involving graduate-level annotators who rated generated ideas for relevance, novelty, clarity, and reasonableness ("helpful" label), and performed pairwise ranked comparisons including vs. ground-truth paper ideas.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Human expert evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Three main human studies: Study I (binary helpful/unhelpful over 50 gold instances, 6 annotators, criteria: relevance, novelty vs context, reasonableness, clarity); Study II (pairwise ranking between top GPT4 variants and comparison to ground-truth idea for technical detail/novelty); Study III (focused evaluation of iterative novelty boosting — whether regenerated ideas were different and more novel). Annotations include percent helpful/unhelpful, tie counts, and inter-annotator agreement measures.</td>
                        </tr>
                        <tr>
                            <td><strong>research_domain</strong></td>
                            <td>evaluation of generated scientific ideas in NLP and biomedical domains</td>
                        </tr>
                        <tr>
                            <td><strong>problem_type</strong></td>
                            <td>human assessment of open-ended idea quality</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_metric</strong></td>
                            <td>Human judgments of novelty (binary / comparative rankings) and technical depth (comparative). Study III included explicit questions: is the regenerated idea substantially different? Is it more novel/creative? Does iteration increase novelty?</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_score</strong></td>
                            <td>Study I & III report percent-based results: e.g., Study III: 88.9% of first-iteration updates (SN) were substantially different; 55.6% judged more novel; second iteration increased novelty for 57.8% of continued examples (Table 3). Study II: ground-truth was judged higher in technical level/novelty in 85% of comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_metric</strong></td>
                            <td>Annotators judged 'reasonableness' as part of the helpful label; feasibility treated implicitly via 'reasonable' binary judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>feasibility_score</strong></td>
                            <td>Per-model helpful percentages used as proxy for feasibility/usefulness: e.g., GPT4FS 73% helpful, GPT4FS+KG 66% (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_evidence</strong></td>
                            <td>Annotator findings show that while iterative methods increase novelty in many cases, generated ideas still lag behind paper ground-truth in depth (85% of comparisons), suggesting increased novelty did not equate to paper-level feasibility/technical depth. No numeric correlation or formal trade-off model provided.</td>
                        </tr>
                        <tr>
                            <td><strong>optimization_strategy</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_results</strong></td>
                            <td>See novelty_score and feasibility_score fields; inter-annotator agreement reported in Appendix (Table 13 and Table 14).</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_baseline</strong></td>
                            <td>Human-vs-model comparisons used ground-truth paper ideas as a reference baseline; model variants compared pairwise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparative_results</strong></td>
                            <td>Ground-truth ideas superior in 85% of comparisons (Study II); GPT4FS and GPT4FS+KG best among model outputs per human ratings (Study I).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_findings</strong></td>
                            <td>In biomedical evaluation, two expert annotators rated generated directions positively at ~80% and sometimes preferred generated outputs over ground-truth in technical detail (Table 4).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Undiscovered public knowledge <em>(Rating: 2)</em></li>
                <li>Agatha: automatic graph mining and transformer based hypothesis generation approach <em>(Rating: 2)</em></li>
                <li>A computational inflection for scientific discovery <em>(Rating: 2)</em></li>
                <li>Autonomous chemical research with large language models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2447",
    "paper_id": "paper-366d05cb33bab998f6f1ce9ae676d6ee702977aa",
    "extraction_schema_id": "extraction-schema-64",
    "extracted_data": [
        {
            "name_short": "SciMON",
            "name_full": "Scientific Inspiration Machines Optimized for Novelty (SciMON)",
            "brief_description": "A retrieval-augmented language-model framework that generates natural-language scientific idea suggestions grounded in literature and explicitly optimizes for novelty via iterative compare-and-update steps.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SciMON",
            "system_description": "End-to-end framework that (1) extracts background context and a seed term from a user or paper, (2) retrieves literature \"inspirations\" (semantic neighbors, KG neighbors, citation neighbors), (3) generates candidate idea sentences using LLMs (few-shot GPT-3.5/GPT-4 or fine-tuned T5 / domain LLMs), (4) measures similarity of generated ideas to prior literature, and (5) iteratively instructs the model to update ideas to increase novelty until a similarity threshold is met. It also incorporates an in-context contrastive objective during fine-tuning to discourage copying background text.",
            "research_domain": "machine learning / NLP (demonstrated) and biomedical (case study)",
            "problem_type": "open-ended exploration (generate novel research directions grounded in literature)",
            "novelty_metric": "Primary operational novelty metric: semantic similarity (cosine) between a generated idea and retrieved prior-ideas computed using SentenceBERT embeddings; secondary indicators: human expert novelty ratings and counts of new terms added after iteration. In training/fine-tuning novelty is also encouraged via an in-context contrastive loss (InfoNCE) to push model outputs away from in-context negatives (phrases from the background).",
            "novelty_score": "Iterative novelty boosting produced measured relative gains in human-annotated novelty: e.g., Table 3 reports 1st-iteration novelty Δ (percent of updated examples judged more novel) of +54.4% for GPT4FS, +55.6% for GPT4FS+SN, +47.8% for +CT, +46.7% for +KG; second-iteration further increased novelty by +57.8% for the best method (SN). Also measures of new terms added: ~+22% new terms on average after iteration (Table 3).",
            "feasibility_metric": "No dedicated numerical feasibility metric defined; feasibility/\"reasonable\" was assessed by human experts as part of multi-criteria judgments (annotators rated whether an idea was 'reasonable' / 'helpful' along with relevance, novelty, clarity). Automated metrics (ROUGE / BERTScore) were used for similarity to ground truth but are not feasibility measures.",
            "feasibility_score": "Reported human 'helpful' rates used as a rough proxy: e.g., GPT4FS received 73% helpful votes, GPT4FS+KG 66% (Table 2). In biomedical case study, Meditron+SN outputs were rated helpful 80% (Table 4).",
            "tradeoff_evidence": "No explicit numeric trade-off curve reported. Qualitative evidence: iterative novelty boosting increases measured novelty but tends to produce longer outputs and often superficial recombinations of popular concepts; human evaluation (Study II) showed ground-truth paper ideas had significantly higher technical depth and novelty in 85% of comparisons, indicating that increased novelty from SciMON outputs did not reach paper-level depth. The paper reports no correlation coefficient or Pareto analysis between novelty and feasibility.",
            "optimization_strategy": "Multi-step iterative refinement: retrieve the k nearest literature ideas (k=20) to the generated idea using SentenceBERT, compute cosine similarity scores S_i, flag retrieved items with S_i &gt;= μ (μ=0.6), present these as examples of overlapping prior work and instruct the LLM to update the idea to be more different; repeat until all S_i &lt; μ. Also uses contrastive fine-tuning (InfoNCE) to discourage copying of background context and retrieval-augmented few-shot prompting to ground generation.",
            "human_evaluation": true,
            "human_evaluation_results": "Multiple human studies: Study I (50 instances) — annotators judged GPT4FS and GPT4FS+KG highest; Table 2 shows helpful vote counts (out of total votes): GPT4FS 73% helpful, GPT4FS+KG 66%, T5+SN 48%. Study II: GPT4FS+KG had higher technical detail in 48% of pairwise comparisons vs GPT4FS and was less incremental (more novel) in 45% of pairs; but ground-truth paper ideas were judged higher in novelty/technical level in 85% of comparisons. Study III (70 instances): iterative novelty boosting led to 1st-iteration novelty increases on ~54–56% of updated examples (method-dependent), and second-iteration gains of ~57.8% for the best variant (SN).",
            "comparative_baseline": "Baselines include zero-/few-shot GPT-3.5 and GPT-4, retrieval-augmented few-shot variants, fine-tuned T5 (T5-large), and domain LLM Meditron-7b for biomedical experiments.",
            "comparative_results": "Human judgements: GPT4FS and GPT4FS+KG outperform other models in helpfulness (73% and 66% helpful rates respectively, Table 2). Fine-tuned T5 variants outperform GPT-based models on automated similarity metrics (ROUGE, BERTScore) but are judged lower by humans in usefulness. Iterative novelty boosting improved novelty relative to initial outputs by ~46–56% depending on variant (Table 3). In biomedical experiments, Meditron variants fine-tuned +SN achieved 80% helpful rating and in some cases outperformed ground-truth ideas on technical detail according to two domain experts (Table 4).",
            "domain_specific_findings": "Demonstrated generalization to biomedical domain: fine-tuned Meditron-7b on biomedical data and retrieval augmentation produced 80% helpful outputs as judged by two biochemical experts; in this domain evaluators were sometimes more satisfied with generated outputs than ground-truth abstracts (contrasting NLP-domain results where ground truth was superior).",
            "uuid": "e2447.0"
        },
        {
            "name_short": "InspirationRetrieval",
            "name_full": "Inspiration Retrieval Module (Semantic neighbors, KG neighbors, Citation neighbors)",
            "brief_description": "A retrieval augmentation component that returns literature-derived \"inspirations\" to ground generation: (a) semantic neighbor target terms from training set via SentenceBERT similarity, (b) one-hop knowledge-graph neighbors from a corpus-wide scientific KG, and (c) citation neighbors from cited paper titles.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Inspiration Retrieval Module",
            "system_description": "Three retrieval mechanisms are used: (1) Semantic Neighbors — retrieve top-k training inputs b_i with highest cosine similarity (SentenceBERT all-mpnet-base-v2) to query b and use their target terms t_i as inspirations (graph G_S over training b–t nodes); (2) KG Neighbors — use a background knowledge graph G_B (nodes: tasks/methods/materials/metrics extracted from IE) and select adjacent nodes to seed term v; (3) Citation Neighbors — given source document d, retrieve top-k cited paper titles (before cutoff year) similar to d using SentenceBERT. Retrieved inspirations are concatenated into the LM input.",
            "research_domain": "machine learning / NLP (constructed from ACL Anthology), generalizable to biomedical via PubTator",
            "problem_type": "context-grounded idea generation / open-ended exploration",
            "novelty_metric": "Retrieval uses SentenceBERT cosine similarity to find semantically related examples; the retrieved target term itself is used to broaden idea space (not a novelty score).",
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Augment LM input with retrieved inspirations to ground generation and provide richer context for novelty-oriented iterative updates; semantic-neighbor retrieval is used both for few-shot in-context examples and as inspiration entities.",
            "human_evaluation": true,
            "human_evaluation_results": "Variants using semantic neighbors (+SN) and KG neighbors (+KG) improved human judgments in several conditions: e.g., T5+SN improved helpful vote counts relative to T5 in Study I (T5+SN had 48% helpful vs T5 22% in Table 2). Iterative novelty experiments focusing on SN showed large novelty gains (Table 3).",
            "comparative_baseline": "Compared against no-retrieval inputs and citation/KG variants.",
            "comparative_results": "Semantic neighbor augmentation (+SN) gave the largest increases in iterative novelty and helpfulness for some models (see Table 3 and Table 2). Average number of retrieved neighbors per instance: SN ≈10, KG ≈8, CT ≈5 (Table 7).",
            "domain_specific_findings": "KG neighbor retrieval required domain-specific IE; in biomedical case study PubTator was used to build the KG and yielded strong helpfulness (Meditron+SN helpful 80%).",
            "uuid": "e2447.1"
        },
        {
            "name_short": "InContextContrastive",
            "name_full": "In-context Contrastive Objective (InfoNCE over decoder hidden states)",
            "brief_description": "A fine-tuning augmentation that discourages LM outputs from copying background content by treating phrases from the input as in-context negatives and optimizing an InfoNCE contrastive loss alongside cross-entropy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "In-context Contrastive Augmentation",
            "system_description": "During fine-tuning, randomly select sentences from the input context as in-context negatives, compute pooled decoder hidden states for positive target sequence and negatives, apply a sigmoid/Avg projection and an InfoNCE-style loss L_cl = exp(y^+/τ) / (sum_k exp(y_k^-/τ) + exp(y^+/τ)) combined with cross-entropy. This encourages the model to produce outputs dissimilar to background phrases and thus (indirectly) more novel with respect to the provided context.",
            "research_domain": "machine learning / NLP",
            "problem_type": "reducing copying / promoting novelty in generated ideas",
            "novelty_metric": "Not a direct novelty metric; operationalized to increase divergence from in-context negatives. Improvement measured via human judgments and automated metrics that compare to ground-truth (indirect evidence).",
            "novelty_score": null,
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": "Contrastive loss combined with standard cross-entropy during fine-tuning (multi-objective loss).",
            "human_evaluation": true,
            "human_evaluation_results": "Models with contrastive augmentation (CL) improved automated similarity metrics and helped reduce copying; e.g., T5 variants with CL (TS+CL and TS+SN+CL) achieved higher ROUGE-L / BERTScore (Table 9) and higher judged helpfulness in comparisons versus plain fine-tuned models.",
            "comparative_baseline": "Compared to standard fine-tuning without the contrastive term.",
            "comparative_results": "Contrastive-augmented T5 variants (TS+CL and TS+SN+CL) show better automated evaluation (ROUGE-L and BERTScore) than non-contrastive baselines in Table 9, and human studies reported improvements in novelty versus plain fine-tuning.",
            "domain_specific_findings": null,
            "uuid": "e2447.2"
        },
        {
            "name_short": "IterativeNoveltyBoost",
            "name_full": "Iterative Novelty Boosting with Retrieval (retrieve-compare-update, γ_nov)",
            "brief_description": "A procedure that increases idea novelty by retrieving similar prior ideas, measuring semantic similarity to them, and prompting the LM to revise the idea until similarity is below a threshold.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Iterative Novelty Boosting",
            "system_description": "Given an initial generated idea I_0, retrieve the k nearest literature ideas R_i from a reference corpus R using SentenceBERT (k=20). Compute cosine similarity scores S_i between I_t and each R_i; select retrieved items with S_i &gt;= μ (μ=0.6) and supply them as examples of overlapping prior work in a prompt instructing the LLM to make the idea \"significantly different.\" Repeat generation and retrieval until all S_i &lt; μ. Conceptually framed via a novelty-inducing penalty γ_nov(I, R) but implemented via retrieval + prompting rather than a closed-form optimization.",
            "research_domain": "machine learning / NLP (demonstrated), applicable to other domains with a literature corpus",
            "problem_type": "open-ended idea novelty optimization",
            "novelty_metric": "Cosine similarity between SBERT embeddings of generated idea and retrieved literature idea sentences; threshold μ=0.6 used to mark 'too close' literature overlap. Human annotators judge novelty/creativity as additional metric. They also report counts of new terms added as an auxiliary novelty signal.",
            "novelty_score": "Empirical results (human-annotated): first-iteration novelty Δ: e.g., +54.4% for GPT4FS, +55.6% for GPT4FS+SN, +47.8% for +CT, +46.7% for +KG (Table 3). Second-iteration novelty Δ (for SN variant) +57.8%. New-term additions averaged ≈+22% after iteration.",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": "Qualitative: iterations increase novelty but produce longer outputs and often superficial recombinations of common modeling motifs (dynamic/adaptive models, graph fusion, multimodal fusion). No numeric trade-off between novelty and feasibility reported; human study II suggests that higher novelty from iterations did not reach the technical depth of paper ground truth in most cases (ground truth superior in 85% comparisons).",
            "optimization_strategy": "Iterative retrieve-compare-update loop driven by SBERT similarity and thresholding; if similarity exceeds μ, present high-sim literature examples to the LM as negatives and ask for a revised idea; stop when similarity is below μ.",
            "human_evaluation": true,
            "human_evaluation_results": "Study III (70 instances) — annotators judged that 88.9% of first-iteration updates were substantially different from initial ideas and 55.6% were judged more novel/creative; for the best method (SN) a second iteration produced novelty gains on 57.8% of continued ideas (Table 3).",
            "comparative_baseline": "Compared against initial LM-generated ideas (no iteration).",
            "comparative_results": "Iterative novelty boosting increased human-annotated novelty in ~46–56% of updated examples depending on retrieval type; second iteration added further gains for SN variant (Table 3).",
            "domain_specific_findings": "Applied in biomedical case study with positive results: Meditron+SN variant showed 80% helpfulness and improvements over plain Meditron in some outputs (Table 4 & Table 12).",
            "uuid": "e2447.3"
        },
        {
            "name_short": "LLMGen",
            "name_full": "LLM-based Generation Module (GPT-3.5, GPT-4 few-shot/zero-shot, T5 fine-tuned, Meditron)",
            "brief_description": "The language-model based generation component that produces candidate idea sentences via zero-shot, few-shot, retrieval-augmented prompting or fine-tuning (T5-large and domain LLMs).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LM-based Generation Module",
            "system_description": "Two generation paradigms: (a) in-context learning with large LMs (GPT-3.5 davinci-003 and GPT-4 gpt-4-0314) using zero-shot, random few-shot, or retrieval-based few-shot prompts (examples taken from training set); (b) fine-tuned encoder-decoder models (T5-large) trained on (background, idea) pairs and augmented with in-context contrastive learning. Outputs are decoded with beam search (beam size 5) and repetition penalty.",
            "research_domain": "machine learning / NLP; biomedical with Meditron fine-tuning",
            "problem_type": "natural-language idea/hypothesis generation",
            "novelty_metric": "No internal novelty scoring in base LLMs; novelty is encouraged via retrieval inputs, contrastive fine-tuning, and post-generation iterative novelty boosting (SBERT similarity). Automated similarity metrics (ROUGE, BERTScore) were used to compare to ground truth but are not novelty measures.",
            "novelty_score": null,
            "feasibility_metric": "Feasibility/\"reasonableness\" assessed by human annotators as part of multi-criteria judgments; no dedicated feasibility scoring algorithm reported.",
            "feasibility_score": null,
            "tradeoff_evidence": "Qualitative: GPT-4 outputs tended to be longer and more preferred by humans despite scoring worse on automated similarity metrics; models often produced generic suggestions and copied from context, indicating a trade-off between producing detailed text and true novelty. No explicit quantitative trade-off metric reported.",
            "optimization_strategy": "Improve novelty and reduce copying via retrieval-augmented prompting, in-context contrastive fine-tuning, and iterative novelty boosting.",
            "human_evaluation": true,
            "human_evaluation_results": "Study I: GPT4FS and GPT4FS+KG were top-rated by human annotators (Table 2: GPT4FS 73% helpful votes, GPT4FS+KG 66%). GPT3.5 models performed worse than fine-tuned T5 on some automated metrics but lower in human ratings. In Study II, GPT4FS+KG was judged to have higher technical detail in 48% of pairs vs GPT4FS, and less incremental (more novel) in 45% of pairs.",
            "comparative_baseline": "Compared across variants: GPT3.5 zero/few-shot/retrieval, GPT4 zero/few-shot/KG, fine-tuned T5 variants, domain Meditron-7b.",
            "comparative_results": "T5 variants outperform GPT-based models on automatic similarity metrics (ROUGE/BERTScore) (Table 9), but humans preferred GPT4 outputs for usefulness; domain Meditron fine-tuned +SN achieved 80% helpful in biomedical case (Table 4).",
            "domain_specific_findings": "GPT-4 produced longer outputs and higher human preference in NLP tasks; fine-tuned domain LLM (Meditron-7b) with retrieval performed well in the biochemical case study.",
            "uuid": "e2447.4"
        },
        {
            "name_short": "AutoMetrics",
            "name_full": "Automated Evaluation Metrics (ROUGE, BERTScore, BARTScore)",
            "brief_description": "Standard automated text-generation metrics used to measure similarity between generated idea sentences and ground-truth idea sentences; used as secondary evaluation signals.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Automated similarity metrics",
            "system_description": "Used ROUGE-L (surface n-gram overlap), BERTScore (semantic similarity with SciBERT checkpoint), and BARTScore to automatically compare generated outputs to ground-truth target sentences; served as imperfect proxies for quality but not direct measures of novelty or feasibility.",
            "research_domain": "NLP evaluation",
            "problem_type": "automatic assessment of generation output",
            "novelty_metric": "These metrics measure similarity to ground truth (lower similarity may correlate with novelty but they are not designed as novelty measures).",
            "novelty_score": "Table 9 reports ROUGE-L and BERTScore per model; e.g., TS+SN+CL ROUGE-L ≈ 0.228 and BERTScore ≈ 0.671 on the challenging/gold subsets (values shown in Table 9).",
            "feasibility_metric": null,
            "feasibility_score": null,
            "tradeoff_evidence": null,
            "optimization_strategy": null,
            "human_evaluation": false,
            "human_evaluation_results": null,
            "comparative_baseline": null,
            "comparative_results": "T5-based (fine-tuned) models score higher on automated metrics than GPT-based models, despite humans preferring some GPT outputs for helpfulness (explained by GPT outputs being longer and more detailed).",
            "domain_specific_findings": null,
            "uuid": "e2447.5"
        },
        {
            "name_short": "HumanEval",
            "name_full": "Human Expert Evaluation Protocol (relevance, novelty, clarity, reasonableness)",
            "brief_description": "A multi-study human evaluation involving graduate-level annotators who rated generated ideas for relevance, novelty, clarity, and reasonableness (\"helpful\" label), and performed pairwise ranked comparisons including vs. ground-truth paper ideas.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Human expert evaluation",
            "system_description": "Three main human studies: Study I (binary helpful/unhelpful over 50 gold instances, 6 annotators, criteria: relevance, novelty vs context, reasonableness, clarity); Study II (pairwise ranking between top GPT4 variants and comparison to ground-truth idea for technical detail/novelty); Study III (focused evaluation of iterative novelty boosting — whether regenerated ideas were different and more novel). Annotations include percent helpful/unhelpful, tie counts, and inter-annotator agreement measures.",
            "research_domain": "evaluation of generated scientific ideas in NLP and biomedical domains",
            "problem_type": "human assessment of open-ended idea quality",
            "novelty_metric": "Human judgments of novelty (binary / comparative rankings) and technical depth (comparative). Study III included explicit questions: is the regenerated idea substantially different? Is it more novel/creative? Does iteration increase novelty?",
            "novelty_score": "Study I & III report percent-based results: e.g., Study III: 88.9% of first-iteration updates (SN) were substantially different; 55.6% judged more novel; second iteration increased novelty for 57.8% of continued examples (Table 3). Study II: ground-truth was judged higher in technical level/novelty in 85% of comparisons.",
            "feasibility_metric": "Annotators judged 'reasonableness' as part of the helpful label; feasibility treated implicitly via 'reasonable' binary judgment.",
            "feasibility_score": "Per-model helpful percentages used as proxy for feasibility/usefulness: e.g., GPT4FS 73% helpful, GPT4FS+KG 66% (Table 2).",
            "tradeoff_evidence": "Annotator findings show that while iterative methods increase novelty in many cases, generated ideas still lag behind paper ground-truth in depth (85% of comparisons), suggesting increased novelty did not equate to paper-level feasibility/technical depth. No numeric correlation or formal trade-off model provided.",
            "optimization_strategy": null,
            "human_evaluation": true,
            "human_evaluation_results": "See novelty_score and feasibility_score fields; inter-annotator agreement reported in Appendix (Table 13 and Table 14).",
            "comparative_baseline": "Human-vs-model comparisons used ground-truth paper ideas as a reference baseline; model variants compared pairwise.",
            "comparative_results": "Ground-truth ideas superior in 85% of comparisons (Study II); GPT4FS and GPT4FS+KG best among model outputs per human ratings (Study I).",
            "domain_specific_findings": "In biomedical evaluation, two expert annotators rated generated directions positively at ~80% and sometimes preferred generated outputs over ground-truth in technical detail (Table 4).",
            "uuid": "e2447.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Undiscovered public knowledge",
            "rating": 2
        },
        {
            "paper_title": "Agatha: automatic graph mining and transformer based hypothesis generation approach",
            "rating": 2
        },
        {
            "paper_title": "A computational inflection for scientific discovery",
            "rating": 2
        },
        {
            "paper_title": "Autonomous chemical research with large language models",
            "rating": 1
        }
    ],
    "cost": 0.022574749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SciMON $\square$ : Scientific Inspiration Machines Optimized for Novelty</h1>
<p>Qingyun Wang ${ }^{1}$, Doug Downey ${ }^{2}$, Heng Ji ${ }^{1}$, Tom Hope ${ }^{2,3}$<br>${ }^{1}$ University of Illinois at Urbana-Champaign ${ }^{2}$ Allen Institute for Artificial Intelligence (AI2)<br>${ }^{3}$ The Hebrew University of Jerusalem<br>{tomh, doug}@allenai.org, {qingyun4,hengji}@illinois.edu</p>
<h4>Abstract</h4>
<p>We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link predictionseverely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature. We present SciMON, a modeling framework that uses retrieval of "inspirations" from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved. Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue. Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature ${ }^{1}$.</p>
<h2>1 Introduction</h2>
<p>Can machines mine scientific papers and learn to suggest new directions? The idea that information from the literature can be used for automatically generating hypotheses has been around for decades (Swanson, 1986). To date, the focus has been on a specific setting: hypothesizing links between pairs of concepts (often in drug discovery applications (Henry and McInnes, 2017), e.g., new drug-disease links), where concepts are obtained from papers or knowledge bases previously derived from papers (Sybrandt et al., 2020; Nadkarni et al., 2021).</p>
<p>This common setting has fundamental drawbacks. Reducing the "language of scientific ideas" (Hope et al., 2023) to this simplistic form limits</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: SciMON takes background context and generates ideas grounded in literature inspirations, optimizing novelty by iteratively comparing to related work.
the expressivity of the hypotheses we can hope to generate, and does not capture nuanced contexts that scientists consider: target application settings, requirements and constraints, motivations and challenges. In light of the strong progress recently made with large language models (LLMs), in this paper we explore a dramatically different setting: models that take descriptions of problem contextsand return natural language suggestions of novel scientific directions that are grounded in literature.</p>
<p>We develop a framework named SciMON (Scientific Inspiration Machines with Optimization for Novelty), named after Nobel laureate and AI pioneer Herbert Simon who authored early foundational work on automated scientific discovery (Newell and Simon, 1956; Simon, 1973). We first present an automated data collection methodology that collects examples of past problems and proposed ideas from scientific papers. We then use this data for both fine-tuning and in-context training of LLMs-training them to take problem descriptions and output proposed ideas to address them. We observe that state-of-art LLMs (e.g., GPT-4 (OpenAI, 2023)) struggle with generating novel scientific ideas, and contribute a new modeling framework for generating hypotheses that makes progress in improving the hypothesis generation</p>
<p>ability of LLMs (Figure 1). Given a background problem description, models first dynamically retrieve inspirations from past literature in the form of related problems and their solutions along with contexts from a scientific knowledge graph. These retrieved inspirations serve to ground the generated ideas in existing literature. We then endow models with the ability to iteratively boost the novelty of generated ideas. Given an idea $\mathcal{I}$ generated by the LLM at step $t$, the model compares $\mathcal{I}$ with existing research in the literature; if it finds strongly overlapping research, the model is tasked with updating its idea to be more novel relative to prior work (much like a good researcher would do). We also introduce an in-context contrastive model which encourages novelty with respect to background context.</p>
<p>We perform the first comprehensive evaluation of language models for generating scientific ideas in our new generative, contextual setting. We focus on AI/NLP ideas to facilitate analysis by AI researchers themselves, and also demonstrate generalization to the biomedical domain. We design extensive evaluation experiments using human annotators with domain expertise to assess relevance, utility, novelty, and technical depth. Our methods substantially improve the ability of LLMs in our task; however, analyses show that ideas still fall far behind scientific papers in terms of novelty, depth and utility-raising fundamental challenges toward building models that generate scientific ideas.</p>
<h2>2 Background and New Setting</h2>
<p>We begin with a brief description of related work and background. We then present our novel setting.</p>
<p>Literature-based discovery Nearly four decades have passed since Don Swanson pioneered Literature-Based Discovery (LBD), based on the premise that the literature can be used for generating hypotheses (Swanson, 1986). LBD has been focused on a very specific, narrow type of hypothesis: links between pairs of concepts (often drugs/diseases). The classic formalization of LBD goes back to Swanson (1986) who proposed the "ABC" model where two concepts (terms) A and C are hypothesized as linked if they both co-occur with some intermediate concept B in papers. More recent work has used word vectors (Tshitoyan et al., 2019) or link prediction models (Wang et al., 2019; Sybrandt et al., 2020; Xu et al., 2023) to discover scientific hypotheses as pairwise links between concepts. A tightly related body of research focuses on scientific knowledge graph link prediction (Nadkarni et al., 2021), where predicted links may correspond to new hypotheses, and knowledge bases are reflections of existing scientific knowledge in specific domains, derived from literature. A fundamental gap in this line of work is in the lack of approaches for modeling nuanced contexts (Sosa and Altman, 2022) (e.g., the specific settings in which a drug may be relevant for a disease) for generating ideas in open-ended problem settings with unbounded hypothesis spaces, and for optimizing novelty. Our setting can be viewed as a radical departure addressing the limitations in existing settings.</p>
<p>LLMs for Scientific Innovation Large language models (LLMs) have made remarkable progress in interpreting and producing natural language content and handling knowledge-intensive tasks such as in the medical domain (Nori et al., 2023). Very recent work (Boiko et al., 2023) has explored the use of LLMs in a robotic chemistry lab setting, planning chemical syntheses of known compounds and executing experiments. Robotic lab settings are inherently limited to narrow sub-areas where such experiments are possible and relevant. Other very recent work (Huang et al., 2023) used LLMs to produce code for machine learning tasks such as Kaggle competitions, finding that a GPT-4 agent achieved $0 \%$ accuracy on research challenges such as BabyLM (Warstadt et al., 2023). GPT-4 has been anecdotally reported as having "strengths less like those of having a human co-author, and more like a mathematician working with a calculator" (Carlini, 2023). Our goal is to conduct a non-anecdotal evaluation and enhancement of strong LLMs' ability to generate novel open-ended scientific ideas.</p>
<h3>2.1 SciMON Problem Setting</h3>
<p>We are motivated by imagining an AI-based assistant that suggests ideas in natural language. The assistant takes as input background context $\mathcal{B}$ consisting of (1) current problems, motivations, experimental settings and constraints, denoted as $\mathcal{M}$; and optionally (2) a seed term $v$ that should be a focus point of the generated idea $\mathcal{I}$. The seed term is motivated by considering a user-provided cue for the model to limit its hypothesis space. Importantly, generated ideas should not merely paraphrase the background-the output should be novel with respect to $\mathcal{B}$ and the broader literature corpus. Figure 2 illustrates the setting, showing a background</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Architecture overview. Our models retrieve <em>inspirations</em> and then pass the background input and retrieved inspirations to an LM-based generation module, which iteratively optimizes novelty. Input from Qin et al. (2022).</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: We use IE to obtain literature data for our approach: problems/motivations (background) and proposed ideas (target), as well as salient seed terms.</p>
<p>text that describes problems with "<em>pretrained language models</em>" in the lifelong integration of information sources, including computational costs. The assistant aims to generate an idea for performing "<em>knowledge acquisition</em>" within this context. Given this input, we aim to generate a full sentence describing a novel idea.</p>
<h3>2.2 Automated Training Data Collection</h3>
<p>We obtain training data derived from papers with scientific information extraction (IE) model–extracting past examples of background sentences and corresponding ideas (e.g., descriptions of methods used for specific problems in the background sentences), along with salient entities as seed terms. This data is used for training in both in-context learning and fine-tuning setups.</p>
<p>We construct a corpus <em>D</em> from 67,408 ACL Anthology papers from S2ORC (Lo et al., 2020) (we later also conduct an experiment with a biomedical corpus §4.1). Given a title and the corresponding abstract from a document <em>d</em>, to select problem/motivation sentences <em>M</em> we first perform scientific sentence classification (Cohan et al., 2019) to classify sentences from the abstract into one of {<em>Background</em>, <em>Method</em>, <em>Objective</em>}, selecting sentences with labels of <em>Background</em> and treating the remaining sentences as target sentences <em>T</em> which will serve as desired output examples (Figure 3).</p>
<p>For seed term selection, we apply a state-of-the-art scientific IE system (Ye et al., 2022) to <em>T</em> to extract <em>entities</em> corresponding to <em>Task</em>, <em>Method</em>, <em>Evaluation Metric</em>, <em>Material</em>, and <em>relations</em> of the form [method, used-for, task]—mentions of methods and the tasks they are used for, materials used for tasks, etc. We treat the head (e.g., method) or tail (e.g., task) entity as the <em>seed</em> term, and name the other entity (tail/head, respectively) as a <em>target</em> term <em>t</em> ∈ <em>T</em>. Continuing our example from Figure 2, Figure 3 shows how the seed and target terms ("<em>knowledge acquisition</em>" and "<em>function preserved model expansion</em>") are extracted from <em>T</em>. During training, each instance contains (<em>B</em>, <em>T</em>) pairs; during evaluation, target information is removed.</p>
<p>We use SciCo (Cattan et al., 2021) to obtain coreference links for entity normalization, and use ScispaCy (Neumann et al., 2019) to replace abbreviations with a more informative long form. We also collect paper metadata, including the citation network <em>G<sub>c</sub></em>. We split our dataset temporally (train/dev/test correspond to papers from years &lt;2021 / 2021 / 2022 respectively). For our experiments, we used model checkpoints trained on data preceding 2022, avoiding the risk of data contamination (§6). Table 1 shows data statistics.<sup>2</sup></p>
<p><sup>2</sup>More details are in Appendix C.</p>
<p>Quality of IE Preprocessing During preprocessing, we only keep high-confidence outputs from IE models to reduce errors. We observe this removes many of the noisy cases. To validate this, we manually evaluate the precision of each preprocessing step on a random sample of papers and observe that all steps yield high precision (91%-100%) except relation extraction (65%); in total, the rate of instances passing all steps was 79.7%. ${ }^{3}$</p>
<p>Gold Test Set We create a high-quality, clean test set. We remove test instances where models can trivially use surface-level background information to infer the ground truth to create a more challenging set, selecting instances with low similarity between background and ground truth sentences. We compute the cosine similarity between each instance's background and corresponding ground truth sentence in the test set and take pairs with similarity $\leq 0.074$, which amounts to the tenth percentile of pairs. We further annotate this subset to create a gold subset. We manually exclude instances with trivial overlap between ground truth and background, remove cases with irrelevant background, and retain only instances where the target relation (from which the seed term is taken) is salient to the target sentence. We also remove test pairs that have unexplained terms in the background. We obtain a total of 194 instances. ${ }^{4}$</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Split</th>
<th style="text-align: left;">Forward</th>
<th style="text-align: left;">Backward</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Train</td>
<td style="text-align: left;">55,884</td>
<td style="text-align: left;">58,426</td>
<td style="text-align: left;">114,310</td>
</tr>
<tr>
<td style="text-align: left;">Valid</td>
<td style="text-align: left;">7,938</td>
<td style="text-align: left;">8,257</td>
<td style="text-align: left;">16,195</td>
</tr>
<tr>
<td style="text-align: left;">Test</td>
<td style="text-align: left;">2,623</td>
<td style="text-align: left;">2,686</td>
<td style="text-align: left;">5,309</td>
</tr>
</tbody>
</table>
<p>Table 1: Dataset statistics. Considering a relation of the form [ $v$ used-for $u$ ], we define [ $v$ used-for ?] as forward, and [? used-for $u$ ] as backward.</p>
<h2>3 SciMON Models</h2>
<p>We present a new module to retrieve inspirations as contextual input (§3.1). Then, we describe another module to generate ideas given the context+inspiration (§3.2). Finally, we introduce a new iterative novelty optimization method to further improve idea quality (§3.3). ${ }^{5}$</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h3>3.1 Inspiration Retrieval Module</h3>
<p>We take broad inspiration from cognitive aspects of innovation (Hope et al., 2023): when researchers generate a new idea, they are grounded in a web of existing concepts and papers bearing on the new idea. We aim to enrich the context of each background by retrieving "inspirations"- pieces of information that can guide hypothesis generation. As illustrated in Figure 2, for a given instance of the SciMON task, our retrieval augmentation can retrieve from three types of sources. Each source uses a different form of query and output.</p>
<p>Semantic Neighbors For a given problem/motivation as input, ideas proposed for related problems in the training set can serve as a guiding reference for generating a new idea. Given the background context $\mathcal{B}$ with a seed term $v$ and problem/motivation $\mathcal{M}$, we construct a base input $b$ : a concatenation of $\mathcal{M}$ with a prompt $\mathcal{P}$ belonging to one of two templates: " $v$ is used for $p$ " or " $v$ is done by using $p$ ", where $p$ is one of Task/Method/Material/Metric. In short, $b:=$ $\mathcal{P} \oplus$ context: $\mathcal{M}$. For example, in Figure 2, the concatenation is "Knowledge acquisition is done by using Method; Context:...requires plms to integrate information...lifelong manner...".</p>
<p>We then retrieve inputs from the training set that are semantically related to a new base input $b$, and obtain target sentences $T$ corresponding to each retrieved training input. We extract the target term $t \in \mathcal{T}$ matching the seed term in $b$ (§2.2) as inspiration for input $b$. Simply put, this means we use as inspiration the salient aspect of the solution proposed in $\mathcal{T}$, which we found empirically to help remove noisy/irrelevant information in $\mathcal{T}$. For example, in Figure 2, we find "informative entities are done by using Method context: in this work, we aim at equipping pre-trained language models with structured knowledge." as similar to the input and use $t=$ "linked knowledge graph" as inspiration.</p>
<p>Technically, we first construct a fully connected graph $\mathcal{G}<em i="i">{S}$ based on the training set where each node is a pair of input text $b</em>}$ and target term $t_{i}$. We define the weight between two nodes $i$ and $j$ as the cosine similarity between $b_{i}$ and $b_{j}$ based on representations from SentenceBERT (Reimers and Gurevych, 2019) (all-mpnet-base-v2). Given $b$, we first insert it into $\mathcal{G<em 1="1">{S}$ and compute the weights of its connected edges. We then retrieve neighbors input text $\left{b</em>\right}$ from the training set with the largest edge weight, where $k$ is the number of}, \ldots, b_{k</p>
<p>retrieved instances. We consider the corresponding target terms ${t_{1}, \ldots, t_{k}}$ as semantic inspirations.</p>
<p>KG Neighbors We also explore enriching the context by linking it to a background KG with information on related methods and tasks. Using the same IE process used to extract our training examples (§2.2), we create a global background $\mathrm{KG} \mathcal{G}<em _mathcal_Y="\mathcal{Y">{B}$ which covers all papers in the corpus $\mathcal{D}</em>}}$ prior to a given year $\mathcal{Y}$ (i.e., the nodes in $\mathcal{G<em 1="1">{B}$ correspond to tasks/methods/materials/metrics, and the edges are used-for relations, extracted and normalized from across the entire corpus as described earlier). Then, given a seed term $v$ at query time, we select adjacent nodes $\left{n</em>$ as inspirations. As an example, in Figure 2, the neighbor nodes of "knowledge acquisition" include "collaborative web text annotation editor", "image matching", etc., which we select as inspirations.}, n_{2}, \ldots\right}$ from $\mathcal{G}_{B</p>
<p>Citation Neighbors Another notion of contextual relatedness we explore is via citation graph links. Here, given as input background context $\mathcal{B}$, we assume access to the original source document $d$ from which $\mathcal{B}$ was extracted, and consider its cited paper title set $\mathcal{C}<em _mathcal_Y="\mathcal{Y" d="d">{d}$ as potential candidates. This can be seen as a stronger assumption on information available to the model- assuming a researcher using the model provides relevant candidate documents from which ideas could be pooled. Because the training set only contains papers before year $\mathcal{Y}$, we only select papers $\mathcal{C}</em>}} \subseteq \mathcal{C<em _mathcal_Y="\mathcal{Y" d="d">{d}$ prior to year $\mathcal{Y}$. We then retrieve the top- $k$ titles with the highest cosine similarity to $d$ from $\mathcal{C}</em>$ based on their SentenceBERT embeddings as earlier. For instance, in Figure 2, the paper ELLE (Qin et al., 2022) cites the paper (de Masson d'Autume et al., 2019). Therefore, we choose the title "episodic memory in lifelong language learning" as inspiration information.}</p>
<h3>3.2 Generation Module</h3>
<p>The idea generation module is given retrieved inspirations $i_{1}, \ldots, i_{k}$ along with context $\mathcal{M}$ as input.</p>
<p>In-Context Learning We experiment with recent state-of-the-art LLMs, GPT3.5 davinci-003 (Ouyang et al., 2022) and GPT4 gpt-4-0314 checkpoint (OpenAI, 2023). We first ask the model to generate sentences based on the seed term and the context in the zero-shot setting without any in-context examples (GPT3.5ZS, GPT4ZS). We then ask the model to generate sentences in a few-shot setting by prompting randomly chosen pairs of input and output from the training set (GPT3.5FS, GPT4FS). Inspired by Liu et al. (2022), we further employ a few-shot setting using semantically similar examples. Instead of random in-context examples, we use the top- $k$ examples from the training set with the highest cosine similarity to the query (GPT3.5Retr). This few-shot retrieval setting differs from the semantic neighbor discussed above, in that we provide both the input and output of each instance rather than solely supplying target entities as additional input.</p>
<p>Fine Tuning We fine-tune T5 (Raffel et al., 2020) (more recent models may be used too; see our biomedical experiment $\S 4.1$ fine-tuning an LLM). We observe that the generation models tend to copy phrases from the background context. For example, given the context "...hierarchical tables challenge numerical reasoning ...", the model will generate "hierarchical table reasoning for question answering" as the top prediction. For generating suggestions of novel ideas, we wish to discourage overly copying from the background context. We introduce a new in-context contrastive objective, where negative examples are taken from the text in the input (e.g., in Figure 2, the in-context negatives are plms, pretraining, etc). We compute an InfoNCE loss (Oord et al., 2018) over the hidden states of the decoder, aiming to maximize the probability of the ground truth against those of in-context negatives:</p>
<p>$$
\begin{aligned}
y^{+} &amp; =\sigma\left(\operatorname{Avg}\left(\mathbf{W}<em y="y">{y} \mathbf{h}^{+}+\mathbf{b}</em>\right)\right) \
y_{k}^{-} &amp; =\sigma\left(\operatorname{Avg}\left(\mathbf{W}<em k="k">{y} \mathbf{h}</em>}^{-}+\mathbf{b<em _mathrm_cl="\mathrm{cl">{y}\right)\right) \
\mathcal{L}</em>
\end{aligned}
$$}} &amp; =\frac{\exp \left(y^{+} / \tau\right)}{\sum_{k} \exp \left(y_{k}^{-} / \tau\right)+\exp \left(y^{+} / \tau\right)</p>
<p>where $\mathbf{h}^{+}$and $\mathbf{h}<em y="y">{k}^{-}$are decoder hidden states from the positive and $k$-th negative samples, $\mathbf{W}</em>}$ and $\mathbf{b<em _mathrm_cl="\mathrm{cl">{y}$ are learnable parameters, $\sigma$ is a sigmoid function, $\tau$ is a temperature hyperparameter, and $\operatorname{Avg}(*)$ denotes the average pooling function based on the target sequence length. We optimize with both contrastive loss $\mathcal{L}</em>$ and the cross-entropy loss.}</p>
<h3>3.3 Iterative Novelty Boosting with Retrieval</h3>
<p>We further improve the novelty of generated ideas with a new iterative retrieve-compare-update scheme. Conceptually, we consider a noveltyinducing penalty $\gamma_{\text {nov }}(\mathcal{I}, \mathcal{R})$ that penalizes ideas $\mathcal{I}$ that are too "close" to existing ideas in literature reference examples $\mathcal{R} . \gamma_{\text {nov }}(\mathcal{I}, \mathcal{R})$ is included</p>
<p>during in-context learning and inference, providing numerical feedback in the form of a score reflecting similarity to existing work. We wish to minimize this score while ensuring $\mathcal{I}$ remains relevant to the background context $\mathcal{B}$; we do so iteratively by (1) retrieving related work from $\mathcal{R}$, (2) measuring degree of novelty, (3) instructing the model to update $\mathcal{I}$ to be more novel w.r.t $\mathcal{R}$, conditioning on $\mathcal{B}$.</p>
<p>Specifically, in our implementation, we construct a reference corpus $\mathcal{R}$ based on all papers in the training set. We then propose an iterative algorithm that compares generated ideas against $\mathcal{R}$. We start with the initial idea $\mathcal{I}<em t="t">{0}$ generated by the generation module. At each time step $t$, we use the generated idea $\mathcal{I}</em>}$ as a query to retrieve $k$ nearest ideas from the literature reference corpus $\mathcal{R}=\left{R_{1}, \ldots, R_{k}\right}$ based on SentenceBERT, with the top- $k$ highest cosine similarity scores to $\mathcal{I<em i="i">{t}$ (we use $k=20$ ). For each retrieved ground truth literature idea $R</em>$ are lower than $\mu$. Figure 2 and Table 5 demonstrate novelty iterations.}$, we compare its cosine similarity score $S_{i}$ against a threshold $\mu$ (we use 0.6 ). We provide all the retrieved ground truth ideas $\hat{\mathcal{R}}$ that pass the threshold as additional negative examples for the large language models with the following instruction prompt: "Your idea has similarities with existing research as demonstrated by these $j$ sentences: $\hat{\mathcal{R}}$ Make sure the idea you suggest is significantly different from the existing research mentioned in the above sentences. Let's give it another try." We stop the iteration once all $S_{i</p>
<h2>4 Experiments</h2>
<h3>4.1 Human Evaluation</h3>
<p>We present four human evaluation studies, exploring different facets of our problem and approach.</p>
<h3>4.1.1 Study I: Comparing Outputs across Model Variants</h3>
<p>We recruit six volunteer NLP experts with graduatelevel education to rate the system. Raters are told to envision an AI assistant that suggests new paper ideas. We randomly select 50 instances (background+seed) from the gold subset. Each annotator receives ten instances, each paired with system outputs from different model variants (Table 2). We ask raters to assess idea quality by considering each output's relevance to the context, novelty, clarity, and whether the idea is reasonable (positive ratings are dubbed "helpful" as shorthand, indicating they pass the multiple considerations). We observe moderately high rater agreement. ${ }^{6}$ Raters are blind to the condition, and system outputs are randomly shuffled across instances.</p>
<p>We instruct annotators to only provide positive ratings to ideas sufficiently different from the input context. In Study I, we ask raters not to anticipate groundbreaking novelty from the system but rather a narrower expectation of quality and utility; in Study II below, we enrich the analysis to examine ranking between top models and also "raise the bar" and compare to actual ideas from papers. ${ }^{7}$</p>
<p>In a preliminary experiment, we also collected human ratings for GPT4-ZS (zero-shot) vs. GPT4-FS (few-shot) using the same criteria, finding GPT4-FS ranked higher in $65 \%$ of cases, with the rest mostly tied; thus, zero-shot GPT-4 was left out of the remainder of study I and subsequent studies to reduce annotation effort and cost.</p>
<p>Results Overall, GPT4FS and GPT4FS+KG outperform other models by a wide margin (Table 2). Apart from GPT4, T5+SN+CL performs best compared to other baselines, given its stronger prior knowledge of useful similar background hypotheses. In general, GPT3.5 models performed worse than fine-tuned T5 and its variants, which echoes results in other work in the scientific NLP domain (Jimenez Gutierrez et al., 2022). GPT4 outputs tended to be longer, which may partially explain higher human preference.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">3 FS</th>
<th style="text-align: center;">3 Rt</th>
<th style="text-align: center;">3 FS + CT</th>
<th style="text-align: center;">3 FS + KG</th>
<th style="text-align: center;">4</th>
<th style="text-align: center;">4+KG</th>
<th style="text-align: center;">T5</th>
<th style="text-align: center;">T5+SN</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">H</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">25</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">33</td>
<td style="text-align: center;">73</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">22</td>
<td style="text-align: center;">48</td>
</tr>
<tr>
<td style="text-align: center;">U</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">75</td>
<td style="text-align: center;">84</td>
<td style="text-align: center;">67</td>
<td style="text-align: center;">27</td>
<td style="text-align: center;">34</td>
<td style="text-align: center;">78</td>
<td style="text-align: center;">52</td>
</tr>
</tbody>
</table>
<p>Table 2: Percent (\%) of total votes each system output receives from human raters. $H$ denotes a helpful output, while $U$ denotes an unhelpful output. "3FS" refers to the GPT3. 5FS. "3Rt" refers to the GPT3. 5Retr. "4" refers to GPT4FS, and "4+KG" refers to the GPT4FS+KG. "T5+SN" refers to the T5+SN+CL. GPT4FS and GPT4FS+KG are rated much higher. While GPT4FS has a slightly higher rating than the KG variant, a further human study reveals that GPT4FS+KG often leads to more technical depth (§4.1).</p>
<h3>4.1.2 Study II: Comparing GPT4 Variants against Real Papers</h3>
<p>We conduct a follow-up human study of close competitors GPT4FS and GPT4FS+KG with a subset of</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>the annotators to evaluate the incrementality and novelty of the generated ideas. In this study, model outputs are now ranked, unlike the binary classification of helpful/not in Study I. Suggestions are ranked according to the level of technical detail and innovation in comparison to each other-i.e., ranking which of GPT4FS and GPT4FS+KG had a higher degree of technical detail and novelty, or whether they are roughly the same (tied). Finally, outputs are rated versus the ground truth idea, according to whether or not the suggestions were roughly at the same level of technical detail and innovation as the original paper's idea, or significantly lower.</p>
<p>Results Overall, GPT4FS+KG is found to have higher technical detail in $48 \%$ of the compared pairs, and found to be less incremental (more novel) in $45 \%$ of the pairs. Among the remaining $52 \% / 55 \%$ (respectively), the vast majority are ties, indicating that whenever GPT4FS+KG is not favored, it is of roughly the same quality as GPT4FS, but not vice versa. However, the most crucial aspect is comparing the results against the original ground truth idea on the quality of innovation. Here, we find that in $85 \%$ of comparisons, the ground truth is considered to have significantly higher technical level and novelty; and in the remaining $15 \%$, the ground truth was ambiguous or lacking additional context from the paper abstract. This points to a major challenge in obtaining high-quality idea generations using existing state-of-the-art models.</p>
<h3>4.1.3 Study III: Evaluation on Iterative Novelty Boosting</h3>
<p>We conduct a fine-grained evaluation of our novelty mechanism with qualitative and quantitative evaluation of novelty. Specifically, we ask five annotators to further compare the novelty-enhanced results against the initially generated ideas. We randomly select 70 instances (background+seed) from the sentence generation gold subset. We ask annotators to check whether the new ideas are different than the initial ideas (e.g., adding new information or approaches), and whether they are more novel (i.e., a new idea can be different, but not necessarily more novel). Since GPT4FS+SN outperforms other models, for this model, we further instruct annotators to compare the novelty of the second iteration results against the first iteration results.</p>
<p>Results For SN, in the first iteration $88.9 \%$ of updated ideas are substantially different from initial ideas, and for $55.6 \%$ we are able to increase nov-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">GPT4FS</th>
<th style="text-align: center;">+SN</th>
<th style="text-align: center;">+CT</th>
<th style="text-align: center;">+KG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">1st Novelty $\Delta(\%)$</td>
<td style="text-align: center;">+54.4</td>
<td style="text-align: center;">+55.6</td>
<td style="text-align: center;">+47.8</td>
<td style="text-align: center;">+46.7</td>
</tr>
<tr>
<td style="text-align: left;">2nd Novelty $\Delta(\%)$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">+57.8</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: left;">1st new terms $\Delta$</td>
<td style="text-align: center;">+23.1</td>
<td style="text-align: center;">+22.8</td>
<td style="text-align: center;">+22.1</td>
<td style="text-align: center;">+21.9</td>
</tr>
<tr>
<td style="text-align: left;">2nd new terms $\Delta$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">+21.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<p>Table 3: Relative improvements of iterative novelty boosting. Iterations are applied to the ideas for which sufficiently similar related work is detected (§3.3). "1st Novelty" is \% of the 1st iteration ideas that gained novelty over the initial idea, and "2nd Novelty" is the \% of gain over the 1st iteration. Our method substantially increases novelty for ideas to which it is applied. To save annotation resources, we only annotate second iteration results for the best-performing method (SN). We report the average number of new terms added, after filtering.
elty/creativity (meaning that, e.g., if 100 examples were updated, we would gain 56 examples that are more novel). The 2nd iteration, further increases novelty for $57.8 \%$ of the ideas that continued to another iteration. For ideas not considered more novel after applying our method, we do not observe a drop in novelty-the method either increases or maintains novelty.</p>
<p>Ideas after novelty iterations are longer than initial ideas. We examine the new terms added after filtering 359 words, including stopwords, as many generic words and terms are often added (e.g., "novel model/method/approach"). While our method helps boost novelty, overall the model often tends to suggest combinations between popular concepts (§4.2). Novelty boosting seemed to often focus on adding dynamic/adaptive modeling, graph models and representations, the fusion of multiple modalities and sources-and sometimes all at once (e.g., "Dynamic Syntax-Aware Graph Fusion Networks (DSAGFN)"), and to explicitly compare against existing ideas from literature (Table 5).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: center;">Meditron</th>
<th style="text-align: center;">+SN</th>
<th style="text-align: center;">+CT</th>
<th style="text-align: center;">+KG</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Helpful(\%)</td>
<td style="text-align: center;">35</td>
<td style="text-align: center;">80</td>
<td style="text-align: center;">60</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">Unhelpful(\%)</td>
<td style="text-align: center;">65</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">40</td>
<td style="text-align: center;">50</td>
</tr>
<tr>
<td style="text-align: left;">vs. GT(\%)</td>
<td style="text-align: center;">30</td>
<td style="text-align: center;">45</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">35</td>
</tr>
</tbody>
</table>
<p>Table 4: Human evaluations results of each system output for the idea sentence prediction task on Biomedical Domain. "vs. GT" refers to percents which system outputs are better than ground truth ideas.</p>
<h3>4.1.4 Domain Generalization Case Study</h3>
<p>Our domain-agnostic framework can be applied to other domains by changing the IE system used in the preprocessing procedure. To demonstrate</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Input (Dong et al., 2022)</td>
<td>seed term: speech unit boundaries ; context (abridged): ... generate partial sentence translation given a streaming speech input. existing approaches ... break the acoustic units in speech, as boundaries between acoustic units in speech are not even. ...</td>
</tr>
<tr>
<td>Initial idea</td>
<td>A pause prediction model to identify speech unit boundaries ...</td>
</tr>
<tr>
<td>Iteration 1</td>
<td>A method that leverages acoustic and linguistic features to predict speech unit boundaries dynamically, ensuring smooth transitions ... differs from the existing research as it combines both acoustic properties and linguistic context ... adapting to variations in speaker characteristics, speaking styles, and languages.</td>
</tr>
<tr>
<td>Iteration 2</td>
<td>A novel method called Adaptive Speech Unit Boundary Detection (ASUBD) ... a combination of attention mechanisms to focus on relevant acoustic and linguistic features and reinforcement learning to guide the system to make optimal predictions of unit boundaries based on previous decisions...</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>... an efficient monotonic segmentation module ... accumulate acoustic information incrementally and detect proper speech unit boundaries.</td>
</tr>
</tbody>
</table>
<p>Table 5: Example of iterative novelty iterations. Our novelty iteration method enhances ideas overall; however ideas are often based on superficial recombinations of common concepts, far from the technical depth of scientific papers.
this, we conduct an additional initial experiment in the biochemical domain. We follow a similar data creation procedure as for NLP papers. We collect a dataset from PubMed papers and use PubTator 3 (Islamaj et al., 2021; Wei et al., 2022; Luo et al., 2023; Wei et al., 2023; Lai et al., 2023) as an IE system to extract a KG from paper abstracts. We use a sentence classifier trained on annotated abstracts (Huang et al., 2020) to select background context. We fine-tune a state-of-the-art biomedical large language model (Chen et al., 2023) on our data and evaluate on a test split past its pre-training cutoff date. ${ }^{8}$ We ask two biochemical domain experts with graduate-level education to evaluate the quality of the results as before, finding them to overall rate $80 \%$ of the generated directions positively. Finally, in contrast to NLP-domain experiments, evaluators were more satisfied with the generated outputs than the ground truth regarding technical detail. Detailed results are in Table 4. However, this preliminary experiment was meant mainly to demonstrate the generality of our approach, and a more in-depth exploration of utility and quality is left for future work.</p>
<h3>4.2 Error Analysis</h3>
<p>Models often made generic suggestions, woven together with specific details copied directly from the context (e.g., "NLP with ML algorithms and sentiment analysis" for some problem X, or "data augmentation and transfer learning" for Y, or "BERT or RoBERTa" for Z). Our techniques reduced this behavior but did not fully solve it. GPT4 models, especially, seemed to generate generic descriptions of common steps in NLP workflows (e.g., "Data preprocessing: Clean the text data, remove unnec-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>essary characters, perform tokenization..."). All models often copied and rephrased directly from the context. In certain cases, models applied simple logical modifications to the context; e.g., when contexts described problems such as "high latency" or "efficiency limitations", the suggestions would include phrases such as "low latency" or "highly efficient".</p>
<h3>4.3 Automated Evaluation Analysis</h3>
<p>In open-ended tasks such as ours, automatic evaluations comparing system output to ground truth texts may be limited. Nonetheless, automated metrics such as ROUGE (Lin, 2004), BERTScore (Zhang* et al., 2020) and BARTScore (Yuan et al., 2021), that check the similarity between ground truth and generated output, may surface interesting findings. We find GPT-based models to be outperformed by T5-based models; GPT4 outputs are much longer than T5, explaining why they underperform in automatic metrics but outperform in human evaluations (§4.1). Generated sentences often follow certain templates (e.g., "In this paper, we propose a new ... for ..."), which also helps explain why T5 fine-tuned on many examples scores higher superficially. At the same time, our in-context contrastive examples which encourage novelty with respect to background context, helped models perform better than baseline fine-tuning by reducing reliance on copying. See results in Table 9 (Appendix B.4).</p>
<h2>5 Conclusions and Future Directions</h2>
<p>We propose a new setting, model and comprehensive evaluation for scientific hypothesis generation with language models that are grounded in literature and optimized for novelty. We present a new framework named SciMON in which mod-</p>
<p>els take background problem contexts and provide suggestions that are novel while based on literature. Models retrieve inspirations from semantic similarity graphs, knowledge graphs, and citation networks. We introduce a new iterative novelty boosting mechanism that helps large language models (LLMs) such as GPT-4 generate more novel ideas by explicitly comparing ideas to prior work and refining them. Our experiments demonstrate that the task of generating natural language scientific hypotheses is highly challenging. While our methods improve upon baseline LLMs, generated ideas tend to be incremental and with insufficient detail. Generating novel and meaningful scientific concepts and their compositions remains a fundamental problem (Hope et al., 2023). Evaluation in this setting is also highly challenging, with a huge space of potentially plausible hypotheses formulated in natural language. One interesting direction is to expand SCIMON with a multimodal analysis of formulas, tables, and figures to provide a more comprehensive background context.</p>
<h2>6 Limitations</h2>
<p>We discuss limitations extensively throughout the paper, such as in terms of evaluation challenges and data quality. Here we include additional details on limitations.</p>
<h3>6.1 Limitations of Data Collection</h3>
<p>We crawled papers with Semantic Scholar Academic Graph API from 1952 to June 2022. The number of available papers is limited by the data we crawled from the Semantic Scholar Academic Graph. We also crawled papers from PubMed 1988 to 2024/01. We remove papers that are not English. We also remove papers where abstracts are not correctly parsed from paper PDFs. We will expand our models to papers written in other languages and other domains in the future.</p>
<h3>6.2 Limitations of System Performance</h3>
<p>Our dataset is based on state-of-the-art IE systems, which may be noisy. For instance, the coreference and SciSpacy abbreviation resolution models fail to link A2LCTC to Action-to-Language Connectionist Temporal Classification. The background context detection may also have errors: e.g., the sentence classification component fails to treat "For example, the language models are overall more positive towards the stock market, but there are significant
differences in preferences between a pair of industry sectors, or even within a sector." as background context. In our human-vetted gold data subset, we make sure to filter such cases, but they remain in the training data. SentenceBert (Reimers and Gurevych, 2019), and GPT3.5/4 are not finetuned and might be biased towards pretraining datasets. The idea novelty boosting method is limited by the quality of retrieval models. Better retrieval models may be explored in the future. Due to hardware constraints, we mainly investigated models with up to 7 billion parameters. Due to API change and model randomness, our GPT3.5/4 results might not be easily reproducible.</p>
<h3>6.3 Limitations of Evaluation</h3>
<p>We recruit annotators from Ph.D. students; their opinions may differ from annotators who have different levels of domain knowledge. Our setting uses a seed term taken from the ground truth as input, to emulate a scenario where a human provides guidance to an assistant model. Future work could explore methods in the setting without a seed term, an even harder task, or evaluate in an interactive setting with user-provided seed terms. In addition, while the seed is sampled from the ground truth, in our human-annotated gold subset, we make sure that in no case does the input context trivially leak the output.</p>
<h3>6.4 Memorization Check</h3>
<p>Carlini et al. (2023) reports that LLMs tend to memorize part of their training data, a well-known concern in evaluating current LLMs. Therefore, we examine the pretraining data of each model:</p>
<ul>
<li>T5: Raffel et al. (2020) shows that T5 is pretrained on C 4 which was crawled from web prior to April 2019.</li>
<li>GPT3.5: Based on the documentation, ${ }^{9}$ GPT3.5 series is pretrained on a combination of test and code from before Q4 2021.</li>
<li>GPT4: OpenAI (2023) shows that the GPT-4 checkpoint we used utilizes most pertaining data before September 2021. Despite this, the pretraining and post-training data contain "a small amount" of more recent data. ${ }^{10}$</li>
</ul>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Because we evaluate our models on papers published in 2022, the likelihood of test papers appearing in the pretraining corpora for the models is substantially reduced. We additionally performed a manual examination of GPT-4 memorization in our gold set based on 2022 ACL Anthology papers, by seeing if GPT-4 could complete information such as method names or generate text that strongly mimics the ground truth papers, and found no evidence of this occurring. The Meditron-7b (Chen et al., 2023) uses PubMed with a cut-off in August 2023, and our biochemical test set only includes PubMed papers after 2023/08.</p>
<h2>Acknowledgements</h2>
<p>This work is supported by the Molecule Maker Lab Institute: an AI research institute program supported by NSF under award No. 2019897, by DOE Center for Advanced Bioenergy and Bioproducts Innovation U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research under Award Number DESC0018420, by U.S. the AI Research Institutes program by National Science Foundation and the Institute of Education Sciences, Department of Education through Award No. 2229873 - AI Institute for Transforming Education for Children with Speech and Language Processing Challenges, and by AI Agriculture: the Agriculture and Food Research Initiative (AFRI) grant no. 2020-67021- 32799/project accession no. 1024178 from the USDA National Institute of Food and Agriculture. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied of, the National Science Foundation, the U.S. Department of Energy, and the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.</p>
<h2>References</h2>
<p>Daniil A Boiko, Robert MacKnight, Ben Kline, and Gabe Gomes. 2023. Autonomous chemical research with large language models. Nature, 624(7992):570578 .</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,</p>
<p>Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc.</p>
<p>Nicholas Carlini. 2023. A llm assisted exploitation of ai-guardian. Cryptography and Security Repository, arXiv:2307.15008.</p>
<p>Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations.</p>
<p>Arie Cattan, Sophie Johnson, Daniel S Weld, Ido Dagan, Iz Beltagy, Doug Downey, and Tom Hope. 2021. Scico: Hierarchical cross-document coreference for scientific concepts. In 3rd Conference on Automated Knowledge Base Construction.</p>
<p>Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. Meditron70b: Scaling medical pretraining for large language models. Computation and Language Repository, arXiv:2311.16079.</p>
<p>Arman Cohan, Iz Beltagy, Daniel King, Bhavana Dalvi, and Dan Weld. 2019. Pretrained language models for sequential sentence classification. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3693-3699, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Cyprien de Masson d'Autume, Sebastian Ruder, Lingpeng Kong, and Dani Yogatama. 2019. Episodic memory in lifelong language learning. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.</p>
<p>Qian Dong, Yaoming Zhu, Mingxuan Wang, and Lei Li. 2022. Learning when to translate for streaming speech. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 680-694.</p>
<p>Sam Henry and Bridget T McInnes. 2017. Literature based discovery: models, methods, and trends. Journal of biomedical informatics, 74:20-32.</p>
<p>Tom Hope, Doug Downey, Oren Etzioni, Daniel S Weld, and Eric Horvitz. 2023. A computational inflection for scientific discovery. Communications of the ACM.</p>
<p>Zhe Hu, Zuohui Fu, Yu Yin, and Gerard de Melo. 2021. Context-aware interaction network for question matching. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3846-3853, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. 2023. Benchmarking large language models as ai research agents. Machine Learning Repository, arXiv:2310.03302.</p>
<p>Ting-Hao Kenneth Huang, Chieh-Yang Huang, ChienKuang Cornelia Ding, Yen-Chia Hsu, and C. Lee Giles. 2020. CODA-19: Using a non-expert crowd to annotate research aspects on 10,000+ abstracts in the COVID-19 open research dataset. In Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online. Association for Computational Linguistics.</p>
<p>Rezarta Islamaj, Robert Leaman, Sun Kim, Dongseop Kwon, Chih-Hsuan Wei, Donald C. Comeau, Yifan Peng, David Cissel, Cathleen Coss, Carol Fisher, Rob Guzman, Preeti Gokal Kochar, Stella Koppel, Dorothy Trinh, Keiko Sekiya, Janice Ward, Deborah Whitman, Susan Schmidt, and Zhiyong Lu. 2021. Nlm-chem, a new resource for chemical entity recognition in pubmed full text literature. Scientific Data, 8(1):91.</p>
<p>Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun, and Yu Su. 2022. Thinking about GPT-3 in-context learning for biomedical IE? think again. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4497-4512, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.</p>
<p>Po-Ting Lai, Chih-Hsuan Wei, Ling Luo, Qingyu Chen, and Zhiyong Lu. 2023. Biorex: Improving biomedical relation extraction by leveraging heterogeneous datasets. Journal of Biomedical Informatics, 146:104487.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100-114, Dublin, Ireland and Online. Association for Computational Linguistics.</p>
<p>Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969-4983, Online. Association for Computational Linguistics.</p>
<p>Ilya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In Proceedings of the 7th International Conference on Learning Representations.</p>
<p>Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219-3232, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Ling Luo, Chih-Hsuan Wei, Po-Ting Lai, Robert Leaman, Qingyu Chen, and Zhiyong Lu. 2023. AIONER: all-in-one scheme-based biomedical named entity recognition using deep learning. Bioinformatics, 39(5):btad310.</p>
<p>Rahul Nadkarni, David Wadden, Iz Beltagy, Noah A Smith, Hannaneh Hajishirzi, and Tom Hope. 2021. Scientific language models for biomedical knowledge base completion: an empirical study. $A K B C$.</p>
<p>Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy: Fast and robust models for biomedical natural language processing. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 319-327, Florence, Italy. Association for Computational Linguistics.</p>
<p>Allen Newell and Herbert Simon. 1956. The logic theory machine-a complex information processing system. IRE Transactions on information theory, 2(3):61-79.</p>
<p>Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. 2023. Capabilities of GPT-4 on medical challenge problems. Computation and Language Repository, arXiv:2303.13375.</p>
<p>Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. Machine Learning Repository, arXiv:1807.03748.</p>
<p>OpenAI. 2023. Gpt-4 technical report. Computation and Language Repository, arXiv:2303.08774.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Yujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2022. ELLE: Efficient lifelong pre-training for emerging data. In Findings of the Association for Computational Linguistics: ACL 2022, pages 2789-2810, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the</p>
<p>limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982-3992, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Herbert A Simon. 1973. Does scientific discovery have a logic? Philosophy of science, 40(4):471-480.</p>
<p>Daniel N Sosa and Russ B Altman. 2022. Contexts and contradictions: a roadmap for computational drug repurposing with knowledge inference. Briefings in Bioinformatics, 23(4):bbac268.</p>
<p>Don R Swanson. 1986. Undiscovered public knowledge. The Library Quarterly, 56(2):103-118.</p>
<p>Justin Sybrandt, Ilya Tyagin, Michael Shtutman, and Ilya Safro. 2020. Agatha: automatic graph mining and transformer based hypothesis generation approach. In Proceedings of the 29th ACM International Conference on Information \&amp; Knowledge Management, pages 2757-2764.</p>
<p>Vahe Tshitoyan, John Dagdelen, Leigh Weston, Alexander Dunn, Ziqin Rong, Olga Kononova, Kristin A Persson, Gerbrand Ceder, and Anubhav Jain. 2019. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature, 571(7763):95-98.</p>
<p>Qingyun Wang, Lifu Huang, Zhiying Jiang, Kevin Knight, Heng Ji, Mohit Bansal, and Yi Luan. 2019. PaperRobot: Incremental draft generation of scientific ideas. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1980-1991, Florence, Italy. Association for Computational Linguistics.</p>
<p>Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, et al. 2023. Findings of the babylm challenge: Sample-efficient pretraining on developmentally plausible corpora. In Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pages 1-34.</p>
<p>Chih-Hsuan Wei, Alexis Allot, Kevin Riehle, Aleksandar Milosavljevic, and Zhiyong Lu. 2022. tmvar 3.0: an improved variant concept recognition and normalization tool. Bioinformatics, 38(18):4449-4451.</p>
<p>Chih-Hsuan Wei, Ling Luo, Rezarta Islamaj, Po-Ting Lai, and Zhiyong Lu. 2023. GNorm2: an improved gene name recognition and normalization system. Bioinformatics, 39(10):btad599.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics.</p>
<p>Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, and Chenghu Zhou. 2023. Exploring and verbalizing academic ideas by concept co-occurrence. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13001-13027, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Deming Ye, Yankai Lin, Peng Li, and Maosong Sun. 2022. Packed levitated marker for entity and relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4904-4917, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In Proceedings of the 8th International Conference on Learning Representations.</p>
<p>Ran Zhou, Xin Li, Ruidan He, Lidong Bing, Erik Cambria, Luo Si, and Chunyan Miao. 2022. MELM: Data augmentation with masked entity language modeling for low-resource NER. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2251-2262, Dublin, Ireland. Association for Computational Linguistics.</p>
<h2>A Dataset Collection</h2>
<h2>A. 1 NLP Dataset Collection</h2>
<p>We download ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API. ${ }^{11}$ We filter out papers without abstracts and not written in English to obtain 67,408 papers. Our dataset has 58,874 papers before 2021, 5,946 papers from 2021, and 2,588 from 2022. We first use PL-Marker (Ye et al., 2022) pretrained on SciERC (Luan et al., 2018) to extract nodes belonging to six types: Task, Method, Evaluation Metric, Material, Other Scientific Terms, and Generic</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Terms. The model then predicts relations between nodes belonging to seven relation types: Usedfor, Feature-of, Evaluate-for, Hyponym-of, Partof, Compare, and Conjunction. Because we want to generate new ideas, we focus on used-for relations in papers. Next, we use SciCo (Cattan et al., 2021) with checkpoint from Hugging Face ${ }^{12}$ to obtain entity coreference to merge identical nodes. Then, we use ScispaCy (Neumann et al., 2019) to perform unsupervised abbreviation detection to replace the abbreviation with a more informative long form. Finally, we perform scientific sentence classification (Cohan et al., 2019) ${ }^{13}$ to classify sentences from the abstract into five categories including Background, Method, Objective, Other, and Result. We select sentences with labels of Background and Other as background context. During preprocessing, we only keep high-confidence outputs from IE models. Figure 4 shows an example of the IE systems pipeline.</p>
<h2>A. 2 Biochemical Dataset Collection</h2>
<p>We collect PubMed papers from 1988 to 2024 using Entrez Programming Utilities API ${ }^{14}$ for the following topics, including Yarrowia, Saccharomyces cerevisiae, Issatchenkia orientalis, and Rhodosporidium toruloides. We use PubTator 3 (Islamaj et al., 2021; Wei et al., 2022; Luo et al., 2023; Wei et al., 2023; Lai et al., 2023). The PubTator 3 performs named entity recognition, relation extraction, entity coreference and linking, and entity normalization for the abstracts in the dataset. PubTator 3 identifies bio entities belonging to seven types: gene, chemical, chromosome, cell line, variant, disease, and speciesl and relations belonging to 13 types: associate, cause, compare, convert, contract, drug interact, inhibit, interact, negative correlate, positive correlate, prevent, stimulate, and treat. Finally, we use a sentence classifier trained on CODA-19 (Huang et al., 2020) to classify sentences in abstracts into background, purpose, method, finding, and other. We select sentences with labels of background as background context and remove sentences with labels of other. We treat the rest sentences that have at least one entity as the target sentence. We only keep samples with low similarity between background context</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>and corresponding ground truth sentences. ${ }^{15}$ Our final dataset has 4,767 papers before 2023/02, 642 papers from 2023/02 to 2023/08, and 299 papers after 2023/08.</p>
<h2>B Finetuning and Automated Evaluation details</h2>
<h2>B. 1 Inspiration Retrieval Module</h2>
<p>The statistics of each inspiration type are in Table 7. Table 8 shows sample retrieved inspirations.</p>
<h2>B.1.1 Semantic Neighbors</h2>
<p>We use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar nodes from the training set based on query $q$ in $\S 3.1$. We retrieve up to 20 relevant semantic neighbors $\mathcal{R}$ from the training set for each instance. We treat the target nodes from $\mathcal{R}$ as semantic neighbors.</p>
<h2>B.1.2 KG Neighbors</h2>
<p>We use one-hop connected neighbors from the background $\mathrm{KG} \mathcal{G}_{B}$ constructed on papers before 2021(i.e., the papers in the training set). Because of the scarcity of KG neighbors, we do not limit the number of KG neighbors.</p>
<h2>B.1.3 Citation Neighbors</h2>
<p>Similar to semantic neighbors, we use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019) to retrieve cited paper titles similar to query $q$. We restrict cited papers only before 2021. We retrieve up to 5 relevant citation neighbors from the papers' citation network.</p>
<h2>B. 2 Generation Module</h2>
<p>Our T5 model and their variants are built based on the Huggingface framework (Wolf et al., 2020). ${ }^{16}$ We optimize those models by AdamW (Loshchilov and Hutter, 2019) with the linear warmup scheduler. ${ }^{17}$ Those models are finetuned on 4 NVIDIA A6000 48GB GPUs with distributed data parallel. ${ }^{18}$ The training time for each model is about 10 hours.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Preprocessing result for Hu et al. (2021) in non-canonicalized KG Corpus</p>
<table>
<thead>
<tr>
<th>Stage</th>
<th>PL-Maker Entities</th>
<th>PL-Maker Used-for Relations</th>
<th>SciCo Coreference</th>
<th>Scispacy Abbreviation Detection</th>
<th>Sentence Classification</th>
</tr>
</thead>
<tbody>
<tr>
<td>Precision</td>
<td>91.3</td>
<td>65.4</td>
<td>97.2</td>
<td>100</td>
<td>100</td>
</tr>
</tbody>
</table>
<p>Table 6: Human quality evaluation of preprocessing stages(%). Overall pass rate after all steps are applied is 79.7%.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Train</th>
<th>Valid</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>SN</td>
<td>10.8</td>
<td>10.0</td>
<td>10.0</td>
</tr>
<tr>
<td>KG</td>
<td>8.3</td>
<td>8.0</td>
<td>8.1</td>
</tr>
<tr>
<td>CT</td>
<td>4.9</td>
<td>5.0</td>
<td>5.0</td>
</tr>
</tbody>
</table>
<p>Table 7: Average of # of neighbors for each instance, excluding those which do not have any neighbor</p>
<h3>B.2.1 In-Context Learning</h3>
<p>We choose GPT3.5 davinci-003 (Brown et al., 2020) as our out-of-the-box causal language modeling baseline. We select 5 instances from the training set as examples for the few-shot setting. We randomly select those examples for GPT3.5FS. For GPT3.5Retr, similar to semantic neighbors, we use all-mpnet-base-v2 from SentenceBert (Reimers and Gurevych, 2019), which performs best in semantic search to retrieve similar instances from the training set based on query q in §3.1. The input length is limited to 2048 tokens due to OpenAI API limits. We choose gpt-4-0314 as our GPT4 model. Our input for GPT4 is similar to GPT3.5.</p>
<p>For each selected example from the training set with forward relation, the template is "Consider the following context: M In that context, which p can be used for v, and why? T", where M is the background context, p is the target node type, v is the seed term, and T is the target idea sentence; for backward relation, the template is "Consider the following context: M In that context, which p do we use v, and why? s". For selected examples with additional retrieval inspirations, we concatenate the following additional template to the M: "The retrieval results are: i_{1}, . . , i_{k}", where i_{1}, . . , i_{k} are retrieved inspirations. For the final prompt, the template is similar to the above example template. However, the target sentence T will not be included. We ask the model to generate 10 outputs. We will select the best output and skip the empty output.</p>
<h3>B.2.2 Fine Tuning</h3>
<p>Given input without any inspirations, the input combines the prompt P and context M as shown in §3.1 (i.e., P | context: M). Given input with inspirations, the input is P | retrieve: i_{1}, . . , i_{k} | context: M, with i_{1}, . . , i_{k} as retrieved inspirations. The input length is limited to 512 tokens. For both tasks, we finetune our model based on T5-large with a learning rate of 6 × 10^{−6} and ε = 1 × 10^{−6}. The batch size is 8 for each GPU. The maximum training epoch for all models is 10 with 4 patience. During decoding, we use beam-search to generate results with a beam size of 5 and a repetition penalty of 1.5.</p>
<h4>In-context Contrastive Augmentation</h4>
<p>We randomly select 2 sentences that appeared in the input as in-context negatives. For example, in Figure 1, the in-context negatives could be "knowledge acquisition is done by using Method", "this requires plms to integrate the information from all the sources in a lifelong manner."</p>
<p>B.2.3 Biochemical Case Study</p>
<p>Our Meditron-7b (Chen et al., 2023) and its variants are built based on the Huggingface framework (Wolf et al., 2020). We use its epfl-llm/meditron-7b as the base model. We finetune those models with a learning rate of $2 \times 10^{-6}$ and $\epsilon=5 \times 10^{-8}$. The maximum training epoch for all models is 5 . All models are finetuned on 4 NVIDIA A100 80 GB GPUs with Fully Sharded Data Parallel. ${ }^{21}$ The training time for each model is about 20 hours.</p>
<h2>B. 3 The Scale of Retrieval Set</h2>
<p>We retrieve from a set of 59 k papers with over 374 k sentences in the NLP domain, the focus of our experiments. Our background KG built on the training set has more than 197k nodes and 261k relations. Moreover, we collect 87 k paper titles from citation networks. This represents a large-scale and diverse domain; retrieving inspirations from this set is expected, in principle, to be more than enough for generating novel ideas. Indeed, NLP papers typically cite each other and build on each other as inspirations to create new ideas - which motivates our inspiration retrieval.</p>
<h2>B. 4 Automated Evaluation</h2>
<p>We use BERTScore (Zhang* et al., 2020) with SciBERT checkpoint for both tasks. The hash of the checkpoint is allenai/scibert_scivocab_uncased_L8 _no-idf_version=0.3.12(hug_trans=4.19.2). The automated evaluation results are in Table 9.</p>
<h2>C Human Annotation and Evaluation Details</h2>
<p>Gold Dataset Annotation Details The gold dataset annotation interface is in Figure 5. The quality of the instances in the test set is judged given three criteria: (1) whether the ground truth sentence trivially overlaps with background context; (2) whether background context contains relevant information for the target relation; (3) whether the target relation (from which the seed term is taken) is a salient aspect of the idea proposed in the target paper.</p>
<p>Study I The instructions for human evaluation can be found in Figure 6, while an example of the</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>human evaluation interface is provided in Figure 7 and 8. Human annotators are required to evaluate each system output based on the following criteria: (1) Is the candidate relevant to the context + seed term? (2) Does the candidate copy too much from the context, or is it sufficiently novel/different from the context? (3) Does the candidate's suggestion generally make sense to you scientifically? (4) Is the language sufficiently clear and coherent to understand the suggestion? The input for sample human annotation is in Table 10 and the human labels are in Table 11. The human annotation agreement is in Table 13.</p>
<p>Study III We ask the following questions to human annotators to evaluate the quality of regeneration results: (1) Is the regenerated idea substantially different from the original? (2) Is the regenerated idea more novel and creative than the original idea? (3) Does the second iteration increase novelty? The human annotation agreement is in Table 14.</p>
<h2>D Scientific Artifacts</h2>
<p>We list the licenses of the scientific artifacts used in this paper: Semantic Scholar Academic Graph API (API license agreement ${ }^{22}$ ), Huggingface Transformers (Apache License 2.0), SBERT (Apache2.0 license), BERTScore (MIT license), Meditron7b (Llama2), Entrez Programming Utilities API (Copyright ${ }^{23}$ ), PubTator 3 (Data use policy ${ }^{24}$ ), and OpenAI (Terms of use ${ }^{25}$ ).</p>
<h2>E Ethical Consideration</h2>
<p>The SciMON task and corresponding models we have designed in this paper are limited to the natural language processing (NLP) and biochemical domain, and might not apply to other scenarios.</p>
<h2>E. 1 Usage Requirement</h2>
<p>This paper aims to provide investigative leads for a scientific domain, specifically natural language processing. The final results are not intended to be used without human review. Accordingly, domain experts might use this tool as a research writing assistant to develop ideas. However, our system does not do any fact-checking with external knowledge. In addition, we train our models on the ACL</p>
<p><sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Content</th>
</tr>
</thead>
<tbody>
<tr>
<td>Seed Term Prompt</td>
<td>data augmentation is used for Task</td>
</tr>
<tr>
<td>Context</td>
<td>data augmentation is an effective solution to data scarcity in low - resource scenarios, however, when applied to token-level tasks such as ner, data augmentation methods often suffer from token-label misalignment, which leads to unsatisfactory performance.</td>
</tr>
<tr>
<td>Semantic Neighbors</td>
<td>st and automatic speech recognition (asr), low-resource tagging tasks, end-to-end speech translation, neural online chats response selection, neural machine translation, semi-supervised ner, entity and context learning, semi-supervised setting, dependency parsing, low-resource machine translation, slot filling, dialog state tracking, visual question answering, visual question answering (vqa), low-resource neural machine translation</td>
</tr>
<tr>
<td>KG Neighbors</td>
<td>nmi-based text normalization, task-oriented dialog systems, task-oriented dialogue system, low-resource languages (lrl), end-to-end speech translation, visual question answering (vqa), multiclass utterance classification, clinical semantic textual similarity, neural online chats response selection, context-aware neural machine translation</td>
</tr>
<tr>
<td>Citation Neighbors</td>
<td>Contextual Augmentation: Data Augmentation by Words with Paradigmatic Relations, An Analysis of Simple Data Augmentation for Named Entity Recognition, Data Augmentation for Low-Resource Neural Machine Translation, DAGA: Data Augmentation with a Generation Approach for Low-resource Tagging Tasks, EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks</td>
</tr>
<tr>
<td>Ground Truth</td>
<td>ELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER</td>
</tr>
</tbody>
</table>
<p>Table 8: Example (from (Zhou et al., 2022)) of retrieved inspirations. Inspirations similar to ground truth are underlined.</p>
<table>
<thead>
<tr>
<th>input</th>
<th>context</th>
<th>entity</th>
<th>output</th>
<th>relation</th>
<th>rel_sent</th>
<th>is the output trivially overlap with the context</th>
<th>IE is of sufficient quality (not generic, correct)</th>
<th>context contains relevant information for target relation (Conservative filter only flag cases where context is highly irrelevant)</th>
<th>Relation is a part of the main idea propose d by the paper</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>transformer - based language models usually treat texts as linear sequences. however, most texts also have an inherent hierarchical structure , i.e. , parts of a text can be identified using their position in this hierarchy . In addition , section titles usually indicate the common topic of their respective sentences.</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>We propose a novel approach to formulate , extract , encode and inject hierarchical structure information explicitly into an extractive summarization model based on a pre -trained , encoder - only Transformer language model ( H2Struct+ model ), which improves SCFA ROUGEs for extractive summarization on PubMed and artXx substantially .</td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 5: Gold subset annotation interface
anthology and PubMed papers written in English, which might alienate readers who have been historically underrepresented in the NLP/biochemical domains.</p>
<h3>E. 2 Data Collection</h3>
<p>We collect 67,408 ACL Anthology papers from 1952 to 2022 using Semantic Scholar Academic Graph API, under API license agreement ${ }^{26}$. We ensure our data collection procedure follows the Terms of Use at https://allenai.org/terms. According to the agreement, our dataset can only be used for non-commercial purposes. As mentioned in $\S 4$, we perform the human evaluation. All</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup>annotators involved in human evaluation are voluntary participants with a fair wage. We further collect 5,708 PubMed papers from 1988 to 2024 using Entrez Programming Utilities API ${ }^{27}$. We follow their data usage guidelines ${ }^{28}$.</p>
<p><sup id="fnref3:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">Subset</th>
<th style="text-align: center;">Challenging</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Gold</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;">R-L $\uparrow$</td>
<td style="text-align: center;">BERT $\uparrow$</td>
<td style="text-align: center;">R-L $\uparrow$</td>
<td style="text-align: center;">BERT $\uparrow$</td>
</tr>
<tr>
<td style="text-align: center;">GPT4ZS</td>
<td style="text-align: center;">0.120</td>
<td style="text-align: center;">0.581</td>
<td style="text-align: center;">0.130</td>
<td style="text-align: center;">0.583</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.618</td>
<td style="text-align: center;">0.151</td>
<td style="text-align: center;">0.624</td>
</tr>
<tr>
<td style="text-align: center;">TS</td>
<td style="text-align: center;">0.223</td>
<td style="text-align: center;">$0.672^{\dagger}$</td>
<td style="text-align: center;">0.246</td>
<td style="text-align: center;">0.685</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS+SN</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">$0.620^{-}$</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.627</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS+KG</td>
<td style="text-align: center;">0.143</td>
<td style="text-align: center;">0.619</td>
<td style="text-align: center;">0.152</td>
<td style="text-align: center;">0.626</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS+CT</td>
<td style="text-align: center;">0.144</td>
<td style="text-align: center;">0.617</td>
<td style="text-align: center;">0.149</td>
<td style="text-align: center;">0.622</td>
</tr>
<tr>
<td style="text-align: center;">TS+CL</td>
<td style="text-align: center;">$0.225^{\dagger}$</td>
<td style="text-align: center;">$0.671^{\dagger}$</td>
<td style="text-align: center;">$0.251^{\dagger}$</td>
<td style="text-align: center;">$0.686^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">TS+SN+CL</td>
<td style="text-align: center;">$0.228^{\dagger}$</td>
<td style="text-align: center;">$0.671^{\dagger}$</td>
<td style="text-align: center;">$0.258^{\dagger}$</td>
<td style="text-align: center;">$0.686^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">TS+KG+CL</td>
<td style="text-align: center;">$0.223^{\dagger}$</td>
<td style="text-align: center;">0.669</td>
<td style="text-align: center;">0.248</td>
<td style="text-align: center;">$0.681^{\dagger}$</td>
</tr>
<tr>
<td style="text-align: center;">TS+CT+CL</td>
<td style="text-align: center;">$0.225^{\dagger}$</td>
<td style="text-align: center;">$0.671^{\dagger}$</td>
<td style="text-align: center;">$0.250^{\dagger}$</td>
<td style="text-align: center;">$0.686^{\dagger}$</td>
</tr>
</tbody>
</table>
<p>Table 9: Automatic evaluation results for the challenging and gold subsets. $C L$ is a model with in-context contrastive augmentation. $S N$ is a model with semantic inspirations. $K G$ is a model with KG inspirations. $C T$ is a model with citation inspirations. $R$-L denotes ROUGE-L. BERT denotes BERTscore with SciBERT as its encoder. ${ }^{\dagger}$ indicates that differences between models are not statistically significant ( $p \leq 0.05$ ) when compared to each other but are still significant when compared to the other models based on t-test.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Seed Term Prompt</td>
<td style="text-align: left;">Irish language learning is done by using Method</td>
</tr>
<tr>
<td style="text-align: left;">Context</td>
<td style="text-align: left;">Irish is a minority language which means that l2 learners have limited opportunities for exposure <br> to the language, and additionally, there are also limited (digital) learning resources available.</td>
</tr>
</tbody>
</table>
<p>Table 10: Input for sample human annotation results</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Output</th>
<th style="text-align: center;">Label</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT3.SFS</td>
<td style="text-align: center;">The use of Social Media in Irish Language Learning: A Case Study</td>
<td style="text-align: center;">helpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT3.SRetr</td>
<td style="text-align: center;">One method that could be used for Irish language learning is computer assisted language learning (CALL)</td>
<td style="text-align: center;">unhelpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT3.SFS+CT</td>
<td style="text-align: center;">In that context, the most appropriate task for precision grammar would be the development of a part-of-speech tagger for Irish using finite-state morphology and constraint grammar disambiguation.</td>
<td style="text-align: center;">unhelpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT3.SFS+KG</td>
<td style="text-align: center;">Clo: An Irish Language Learning Platform - Leveraging Digital Resources and Human Expertise</td>
<td style="text-align: center;">unhelpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS</td>
<td style="text-align: center;">We propose using a technology-enhanced approach, such as computer-assisted language learning (CALL) or natural language processing (NLP) techniques, to create interactive and engaging Irish language learning resources, as these methods can provide personalized learning experiences and enhance learners' exposure to the language despite limited resources available.</td>
<td style="text-align: center;">helpful</td>
</tr>
<tr>
<td style="text-align: center;">GPT4FS+KG</td>
<td style="text-align: center;">Exploiting Social Media for Irish Language Learning: An Analysis of Twitter Data. In this context, we use social media data, particularly from Twitter, as a method for Irish language learning, because it provides a rich source of authentic and diverse language examples that can be used to enhance learning opportunities for L2 learners in a minority language setting.</td>
<td style="text-align: center;">helpful</td>
</tr>
<tr>
<td style="text-align: center;">TS</td>
<td style="text-align: center;">In this paper, we present a novel machine translation approach to Irish language learning. A conversational virtual environment for Irish language learning</td>
<td style="text-align: center;">unhelpful <br> helpful</td>
</tr>
</tbody>
</table>
<p>Table 11: Sample human labels for idea-sentence generation results for seed term prompt "Irish language learning is done by using Method", with context "irish is a minority language ... limited digital learning resources available".</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Type</th>
<th style="text-align: center;">Content</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Seed Term</td>
<td style="text-align: center;">ARO10</td>
</tr>
<tr>
<td style="text-align: center;">Context</td>
<td style="text-align: center;">Huangjiu is known for its unique aroma, primarily attributed to its high concentration of bphenylethanol (ranging from 40 to $130 \mathrm{mg} / \mathrm{L}$ ).</td>
</tr>
<tr>
<td style="text-align: center;">Méditron</td>
<td style="text-align: center;">We then selected six key genes involved in the phenylalanine metabolism pathway and predicted that their product flux may affect the synthesis of b-phenylethanol.</td>
</tr>
<tr>
<td style="text-align: center;">Méditron + CT</td>
<td style="text-align: center;">We found that the key amino acid residue that controls the activity of Aro10p was not conserved in wine yeast strains, which may explain the lower b-phenylethanol production in wine fermentation compared with that in Chinese huangjiu.</td>
</tr>
<tr>
<td style="text-align: center;">Méditron + KG</td>
<td style="text-align: center;">Both target genes, SSA1 and ARO10, were deleted using the CRISPR-Cas9 genome editing system.</td>
</tr>
<tr>
<td style="text-align: center;">Méditron + SN</td>
<td style="text-align: center;">Herein, we report that the key barrier for b-phenylethanol production in Huangjiu is ARO10, the only bi-functional amino acid decarboxylase in Saccharomyces cerevisiae.</td>
</tr>
</tbody>
</table>
<p>Table 12: Input and idea-sentence generation results for seed gene " $A R O 10$ " in the biochemical domain.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Annotator Pair</th>
<th style="text-align: center;">$1-2$</th>
<th style="text-align: center;">$1-3$</th>
<th style="text-align: center;">$1-4$</th>
<th style="text-align: center;">$1-5$</th>
<th style="text-align: center;">$1-6$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Agreement \%</td>
<td style="text-align: center;">68.8</td>
<td style="text-align: center;">75.0</td>
<td style="text-align: center;">56.2</td>
<td style="text-align: center;">43.8</td>
<td style="text-align: center;">75.0</td>
</tr>
</tbody>
</table>
<p>Table 13: Percent (\%) of same labels from overlapped 10 human evaluation instances on each pair of annotators for Study I.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Annotator Pair</th>
<th style="text-align: center;">$1-2$</th>
<th style="text-align: center;">$1-3$</th>
<th style="text-align: center;">$1-4$</th>
<th style="text-align: center;">$1-5$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Agreement \%</td>
<td style="text-align: center;">92.5</td>
<td style="text-align: center;">93.3</td>
<td style="text-align: center;">87.5</td>
<td style="text-align: center;">90.0</td>
</tr>
</tbody>
</table>
<p>Table 14: Percent (\%) of same labels from overlapped 20 human evaluation instances on each pair of annotators for Study III. (1-3) has 60 shared questions. The rest of the pairs each share 40 questions.</p>
<h1>Rank scientific idea suggestions generated by an Al paper-writing assistant</h1>
<p>Your goal in this task is to rank idea suggestions written by an AI assistant. The AI assistant helps its users write paper abstracts by writing sentences with proposals for new ideas or questions to consider. You are first given:</p>
<ol>
<li>A context which describes relevant background in a specific area of interest. e.g., background on efficiency issues in large language model training.</li>
<li>A seed term that should be a focus of the generated scientific idea. see example below.</li>
</ol>
<p>Then, you are given a list of 5 candidate suggestions - each generated by the AI assistant using different approaches. Each candidate is written in the form of a sentence from a scientific paper abstract.</p>
<p>Your goal is to score the candidate suggestions with a helpful/unhelpful answer. Your scoring decision should consider the overall quality of each candidate, using considerations that include:</p>
<ol>
<li>Is the candidate relevant to the context + seed term?</li>
<li>Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?</li>
<li>Does the candidate's suggestion generally make sense to you scientifically?</li>
<li>Is the language sufficiently clear and coherent for you to understand the suggestion?</li>
</ol>
<p>The AI scientific assistant is a work in progress - don't expect it to generate highly-detailed ideas or breakthrough discoveries! Use the three guideline metrics above to come up with your overall rating. Read the following example that illustrates the task:</p>
<p>Context: the task of converting a natural language question into an executable sql query, known as text - to - sql, is an important branch of semantic parsing. the state - of - the - art graph - based encoder has been successfully used in this task but does not model the question syntax well.
Seed term: diverse relational edge embedding
System Outputs:</p>
<ul>
<li>We propose a novel technique, Diverse Relational Edge Embedding (DREE), to address this problem.</li>
<li>We propose a novel graph-based encoder that uses a diverse relational edge embedding to model the question syntax.</li>
<li>Diverse relational edge embedding is important for text-to-SQL parsing because it can help the parser to better understand the question syntax.</li>
<li>In this paper, we propose a novel graph - based encoder for diverse relational edge embedding.</li>
<li>In this paper, we propose a novel graph - based encoder for text - to - sql ( ttsql ) that incorporates the question syntax into diverse relational edge embedding.</li>
</ul>
<h2>Ranking:</h2>
<ul>
<li>unhelpful</li>
<li>helpful</li>
<li>helpful</li>
<li>unhelpful</li>
<li>helpful</li>
</ul>
<p>Figure 6: Human evaluation instructions</p>
<p>Your goal is to score the candidate suggestions with a helpful/unhelpful answer. * Your scoring decision should consider the overall quality of each candidate, using considerations that include:
Is the candidate relevant to the context + seed term?</p>
<p>Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?</p>
<p>Does the candidate's suggestion generally make sense to you scientifically?</p>
<p>Is the language sufficiently clear and coherent for you to understand the suggestion?</p>
<p>Context: while pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out - of - distribution ( ood ) data. recent work has explored using counterfactually - augmented data ( cad)-data generated by minimally perturbing examples toflip the ground - truth label - to identify robust features that are invariant under distribution shift
Seed term: diverse perturbation of examples
unhelpful
helpful
Diverse perturbation of examples is used in order to generate counterfactual data that can help identify robust features that are invariant under distribution shift.</p>
<p>A counterfactual generator for diverse perturbation of examples.</p>
<p>We propose a method for generating CAD by diverse perturbation of examples.</p>
<p>In this paper, we propose a counterfactually augmented data ( cad) model that is robust to diverse perturbation of examples.</p>
<p>We use diverse perturbation of examples to flip the ground-truth label in order to identify robust features that are invariant under distribution shift.</p>
<p>Figure 7: Human evaluation example for GPT3.5Rnd, GPT3.5Retr, GPT3.5Rnd+CT, T5, and T5+SN+CL</p>
<p>Your goal is to score the candidate suggestions with a helpful/unhelpful answer. * Your scoring decision should consider the overall quality of each candidate, using considerations that include:
Is the candidate relevant to the context + seed term?</p>
<p>Does the candidate copy too much from the context, or is it sufficiently novel/different from the context?</p>
<p>Does the candidate's suggestion generally make sense to you scientifically?</p>
<p>Is the language sufficiently clear and coherent for you to understand the suggestion?</p>
<p>Context: while pretrained language models achieve excellent performance on natural language understanding benchmarks, they tend to rely on spurious correlations and generalize poorly to out-of-distribution ( ood ) data. recent work has explored using counterfactually - augmented data ( cad)-data generated by minimally perturbing examples toflip the ground - truth label - to identify robust features that are invariant under distribution shift.
Seed term: diverse perturbation of examples</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;">unhelpful</th>
<th style="text-align: left;">helpful</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">In that context, we use <br> diverse perturbation of <br> examples to generate <br> counterfactually-augmented <br> data, which helps identify <br> robust features and improve <br> the model's generalization to <br> out-of-distribution data. This <br> approach minimizes the <br> model's reliance on spurious <br> correlations and enhances <br> its overall performance on <br> natural language <br> understanding tasks.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">In that context, we use <br> diverse perturbation of <br> examples as a method <br> because it helps in <br> generating counterfactually- <br> augmented data (CAD), <br> which helps identify robust <br> features that are invariant <br> under distribution shift. By <br> focusing on robust features, <br> the model becomes less <br> reliant on spurious <br> correlations and can <br> generalize better to out-of- <br> distribution (OOD) data.</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Figure 8: Human evaluation example for GPT3.5Rnd+KG, GPT4Rnd, and GPT4Rnd+KG</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{26}$ https://api.semanticscholar.org/license/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{27}$ www.ncbi.nlm.nih.gov/books/NBK25581/
${ }^{28}$ www.ncbi.nlm.nih.gov/books/about/copyright/&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 2 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
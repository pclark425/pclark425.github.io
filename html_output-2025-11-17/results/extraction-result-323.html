<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-323 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-323</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-323</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9" target="_blank">What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> It is shown empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in- context examples with performance comparable to the optimal least squares estimator.</p>
                <p><strong>Paper Abstract:</strong> In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn"most"functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions -- that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes -- namely sparse linear functions, two-layer neural networks, and decision trees -- with performance that matches or exceeds task-specific learning algorithms. Our code and models are available at https://github.com/dtsip/in-context-learning .</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e323.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e323.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (in-context linear regression)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decoder-only Transformer trained to in-context learn linear functions (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only Transformer (GPT-2 family configuration) trained from scratch on synthetic prompts to predict linear function values from in-context examples; it learns an algorithm that matches the performance of minimum-norm least squares on the synthetic Gaussian linear-regression distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family (custom trained Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9.5M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>linear function evaluation / linear regression (dot-product estimation; solving for w^T x_query via in-context examples)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>20-dimensional inputs (d=20) drawn i.i.d. from N(0,I_d); weight vectors w drawn i.i.d. from N(0,I_d); evaluated with up to 2d (40) in-context examples</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>trained from scratch (no language pretraining) on synthetic in-context prompts (x_i, y_i) with squared-error loss; curriculum learning used (start in low-d subspace and increase); predictions produced autoregressively at positions corresponding to x_i</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Matches minimum-norm least squares on the in-distribution Gaussian linear regression task: normalized squared error ≈ 0.02 at k=d (20), ≈ 0.0006 at k=2d (40); achieves error <0.001 for 2d in-context examples in main setting (errors averaged over many prompts).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Model behavior is consistent with implementing a regression algorithm rather than memorization: (1) the gradient of the model's prediction wrt the query input aligns strongly with the true weight vector (or its projection onto span{x_i}), indicating locally linear behavior matching w^T x; (2) the model generalizes under distribution shifts (orthant shifts, low-dimensional subspace) suggesting an explicit algorithmic solution (orthogonalization/least-squares-like computation); (3) memorization is ruled out empirically since encountering similar training prompts is astronomically unlikely and best-seen training weights give far worse error.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance improves with model capacity and allows in-context learning for higher-dimensional problems; curriculum learning speeds up training and is critical for larger d; larger models are more robust under out-of-distribution prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Less robust to extreme scaling of prompt inputs (errors increase substantially for input-scale factors like 1/3 or 3); under skewed input covariance the error can plateau beyond some k; in noisy-output settings the model (trained on noiseless data) does not implement the optimal regularized estimator and exhibits a double-descent curve similar to least squares.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to minimum-norm least squares (optimal), n-nearest neighbors, and an averaging estimator; model matches least squares and outperforms simpler baselines across k.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A standard decoder-only Transformer trained from scratch on synthetic prompts can learn an algorithm for linear regression in-context, matching least-squares performance and showing mechanistic signatures (gradient alignment, orthogonalization) of algorithmic computation rather than memorization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can Transformers Learn In-Context? A Case Study of Simple Function Classes', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e323.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e323.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (sparse linear)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transformer trained to in-context learn sparse linear (s-sparse) functions (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Transformer trained on prompts generated from sparse linear functions (w with exactly s non-zero coordinates) learns to exploit sparsity in-context, achieving performance comparable to the Lasso estimator without explicit iterative optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family (custom trained Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9.5M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>sparse linear regression (estimation of dot product w^T x when w is s-sparse)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>d=20, sparsity s=3 (three non-zero coordinates in w)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>trained from scratch on synthetic prompts with sparse weight vectors and curriculum learning</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Transformer error ≈ 0.58 and 0.09 for k=5 and k=10 respectively; Lasso baseline achieves ≈ 0.62 and 0.08 for k=5 and k=10; Transformer nearly matches Lasso performance.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Transformer appears to implicitly leverage sparsity (comparable to an ℓ1-regularized estimator) within a single forward pass, suggesting it encodes an algorithm that performs sparsity-aware inference without explicit iterative optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Lasso (ℓ1-regularized least squares) and minimum-norm least squares; Transformer outperforms plain least squares and is comparable to Lasso.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Transformers can discover and apply sparsity-exploiting inference algorithms in-context, achieving performance comparable to iterative methods like Lasso in a single forward pass.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can Transformers Learn In-Context? A Case Study of Simple Function Classes', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e323.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e323.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Noisy linear regression / double descent (observation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Observed behavior of Transformer on noisy linear regression prompts (this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When Gaussian noise is added to in-context labels, the trained Transformer tracks least-squares behavior and exhibits a double-descent error curve similar to minimum-norm least squares.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 family (custom trained Transformer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>9.5M</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>noisy linear regression (estimation when y = w^T x + ϵ)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>d=20; noise ϵ ~ N(0,1) added to outputs in evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>evaluation under label noise (model trained on noiseless prompts; tested on noisy prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Model closely tracks least squares except near interpolation regime; exhibits double-descent curve known for least squares estimators.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Model does not implement the optimal ℓ2-regularized estimator (since it was trained on noiseless data) but still reflects inductive biases similar to minimum-norm least squares leading to double-descent behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Suboptimal in noisy settings relative to an estimator trained/tuned for noise (optimal estimator would include ℓ2 regularization).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to minimum-norm least squares (behaves similarly, including double-descent).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A Transformer trained on noiseless linear regression prompts reproduces known statistical phenomena (double-descent) when evaluated on noisy labels, indicating it encodes least-squares-like inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can Transformers Learn In-Context? A Case Study of Simple Function Classes', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e323.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e323.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Razeghi et al. 2022 (term-frequency effect)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impact of pretraining term frequencies on few-shot reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related work reported that in-context learning for numerical reasoning tasks is better for instances whose terms are more prevalent in the model's pretraining data, implying a data-frequency bias in numerical reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Impact of pretraining term frequencies on few-shot reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>numerical reasoning tasks (unspecified arithmetic types in this paper's citation)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>analysis of pretrained language models' few-shot/numerical reasoning performance with respect to term frequency in pretraining</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Suggests models rely on pretraining data frequency (memorization or distributional coverage) for numerical reasoning; rare terms / rare problem instances yield worse in-context performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Worse in-context performance on numerical reasoning instances containing terms that were rare in pretraining data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Pretraining term frequency substantially affects in-context numerical reasoning performance: more prevalent terms lead to better few-shot arithmetic/numeric reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can Transformers Learn In-Context? A Case Study of Simple Function Classes', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e323.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e323.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Xie et al. 2022 (Bayesian explanation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>An explanation of in-context learning as implicit bayesian inference</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proposes a Bayesian inference framework explaining how in-context learning operates despite formatting differences between training and inference distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>An explanation of in-context learning as implicit bayesian inference</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>general in-context learning / inference (applies to numerical tasks as part of general theory)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>theoretical framework / analysis</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Frames in-context learning as implicit Bayesian inference, offering a mechanistic explanation for how models can adapt to new tasks from prompts without parameter updates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>In-context learning behavior can be interpreted as approximate Bayesian updating performed implicitly by the model given a prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can Transformers Learn In-Context? A Case Study of Simple Function Classes', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e323.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e323.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Olsson et al. 2022 (induction heads)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>In-context learning and induction heads</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Hypothesize and present evidence that specific attention-head circuits ('induction heads') inside Transformers are responsible for in-context learning behaviours, e.g., pattern completion/copying.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>In-context learning and induction heads</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>transformer-circuit hypothesis</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>general in-context learning (mechanistic hypothesis applicable to numerical pattern completion)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>circuit-level analysis / interpretability work (hypothesis-driven)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Introduces the concept of 'induction heads' — attention-head patterns that implement sequence-level copying/induction which may underpin in-context learning; suggests circuit-level specialization enables few-shot generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Certain attention-head structures ('induction heads') plausibly implement mechanisms that support in-context learning, including pattern completion relevant to learning from examples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can Transformers Learn In-Context? A Case Study of Simple Function Classes', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e323.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e323.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pesut 2022 (GPT-3 fitting exploration)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Who models the models that model models? an exploration of gpt-3's in-context model fitting ability</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploratory study showing GPT-3 can fit small tabular/low-dimensional functions in-context and obtain non-trivial accuracy on such tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Who models the models that model models? an exploration of gpt-3's in-context model fitting ability, 2022</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (GPT-3)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>small tabular / low-dimensional function fitting (includes numeric tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>one- and two-dimensional problems (as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>few-shot in-context prompting with GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Reported non-trivial accuracy for small tabular/low-dimensional tasks (no numeric values provided in this paper's citation).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>GPT-3 can achieve non-trivial in-context model-fitting accuracy on small low-dimensional numeric/tabular tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can Transformers Learn In-Context? A Case Study of Simple Function Classes', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e323.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e323.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dinh et al. 2022 (LIFT table result)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LIFT: Language-interfaced fine-tuning for non-language machine learning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Presents LIFT, a framework for using language models on non-language tasks; includes results (Table 16) showing GPT-3 achieves non-trivial accuracy on small tabular / low-dimensional problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Lift: Language-interfaced fine-tuning for non-language machine learning tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>decoder-only transformer (GPT-3) / language-model interface</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>small tabular / low-dimensional numeric problems (as presented in table results)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>one- and two-dimensional tasks (per cited table)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>language-interfaced fine-tuning / in-context evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Cited as showing GPT-3 obtains non-trivial accuracy in the referenced table (no numeric values provided in this paper's citation).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Language-model interfaces (and GPT-3) can be applied to non-language numeric/tabular problems and show non-trivial in-context performance on small problems.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'What Can Transformers Learn In-Context? A Case Study of Simple Function Classes', 'publication_date_yy_mm': '2022-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Impact of pretraining term frequencies on few-shot reasoning <em>(Rating: 2)</em></li>
                <li>An explanation of in-context learning as implicit bayesian inference <em>(Rating: 2)</em></li>
                <li>In-context learning and induction heads <em>(Rating: 2)</em></li>
                <li>Who models the models that model models? an exploration of gpt-3's in-context model fitting ability, 2022 <em>(Rating: 2)</em></li>
                <li>Lift: Language-interfaced fine-tuning for non-language machine learning tasks <em>(Rating: 2)</em></li>
                <li>Rethinking the role of demonstrations: What makes in-context learning work? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-323",
    "paper_id": "paper-de32da8f5c6a50a6c311e9357ba16aa7d05a1bc9",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "Transformer (in-context linear regression)",
            "name_full": "Decoder-only Transformer trained to in-context learn linear functions (this paper)",
            "brief_description": "A decoder-only Transformer (GPT-2 family configuration) trained from scratch on synthetic prompts to predict linear function values from in-context examples; it learns an algorithm that matches the performance of minimum-norm least squares on the synthetic Gaussian linear-regression distribution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family (custom trained Transformer)",
            "model_size": "9.5M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "linear function evaluation / linear regression (dot-product estimation; solving for w^T x_query via in-context examples)",
            "number_range_or_complexity": "20-dimensional inputs (d=20) drawn i.i.d. from N(0,I_d); weight vectors w drawn i.i.d. from N(0,I_d); evaluated with up to 2d (40) in-context examples",
            "method_or_intervention": "trained from scratch (no language pretraining) on synthetic in-context prompts (x_i, y_i) with squared-error loss; curriculum learning used (start in low-d subspace and increase); predictions produced autoregressively at positions corresponding to x_i",
            "performance_result": "Matches minimum-norm least squares on the in-distribution Gaussian linear regression task: normalized squared error ≈ 0.02 at k=d (20), ≈ 0.0006 at k=2d (40); achieves error &lt;0.001 for 2d in-context examples in main setting (errors averaged over many prompts).",
            "mechanistic_insight": "Model behavior is consistent with implementing a regression algorithm rather than memorization: (1) the gradient of the model's prediction wrt the query input aligns strongly with the true weight vector (or its projection onto span{x_i}), indicating locally linear behavior matching w^T x; (2) the model generalizes under distribution shifts (orthant shifts, low-dimensional subspace) suggesting an explicit algorithmic solution (orthogonalization/least-squares-like computation); (3) memorization is ruled out empirically since encountering similar training prompts is astronomically unlikely and best-seen training weights give far worse error.",
            "performance_scaling": "Performance improves with model capacity and allows in-context learning for higher-dimensional problems; curriculum learning speeds up training and is critical for larger d; larger models are more robust under out-of-distribution prompts.",
            "failure_modes": "Less robust to extreme scaling of prompt inputs (errors increase substantially for input-scale factors like 1/3 or 3); under skewed input covariance the error can plateau beyond some k; in noisy-output settings the model (trained on noiseless data) does not implement the optimal regularized estimator and exhibits a double-descent curve similar to least squares.",
            "comparison_baseline": "Compared to minimum-norm least squares (optimal), n-nearest neighbors, and an averaging estimator; model matches least squares and outperforms simpler baselines across k.",
            "key_finding": "A standard decoder-only Transformer trained from scratch on synthetic prompts can learn an algorithm for linear regression in-context, matching least-squares performance and showing mechanistic signatures (gradient alignment, orthogonalization) of algorithmic computation rather than memorization.",
            "uuid": "e323.0",
            "source_info": {
                "paper_title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Transformer (sparse linear)",
            "name_full": "Transformer trained to in-context learn sparse linear (s-sparse) functions (this paper)",
            "brief_description": "A Transformer trained on prompts generated from sparse linear functions (w with exactly s non-zero coordinates) learns to exploit sparsity in-context, achieving performance comparable to the Lasso estimator without explicit iterative optimization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family (custom trained Transformer)",
            "model_size": "9.5M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "sparse linear regression (estimation of dot product w^T x when w is s-sparse)",
            "number_range_or_complexity": "d=20, sparsity s=3 (three non-zero coordinates in w)",
            "method_or_intervention": "trained from scratch on synthetic prompts with sparse weight vectors and curriculum learning",
            "performance_result": "Transformer error ≈ 0.58 and 0.09 for k=5 and k=10 respectively; Lasso baseline achieves ≈ 0.62 and 0.08 for k=5 and k=10; Transformer nearly matches Lasso performance.",
            "mechanistic_insight": "Transformer appears to implicitly leverage sparsity (comparable to an ℓ1-regularized estimator) within a single forward pass, suggesting it encodes an algorithm that performs sparsity-aware inference without explicit iterative optimization.",
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": "Compared to Lasso (ℓ1-regularized least squares) and minimum-norm least squares; Transformer outperforms plain least squares and is comparable to Lasso.",
            "key_finding": "Transformers can discover and apply sparsity-exploiting inference algorithms in-context, achieving performance comparable to iterative methods like Lasso in a single forward pass.",
            "uuid": "e323.1",
            "source_info": {
                "paper_title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Noisy linear regression / double descent (observation)",
            "name_full": "Observed behavior of Transformer on noisy linear regression prompts (this paper)",
            "brief_description": "When Gaussian noise is added to in-context labels, the trained Transformer tracks least-squares behavior and exhibits a double-descent error curve similar to minimum-norm least squares.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 family (custom trained Transformer)",
            "model_size": "9.5M",
            "model_architecture": "decoder-only transformer",
            "arithmetic_operation_type": "noisy linear regression (estimation when y = w^T x + ϵ)",
            "number_range_or_complexity": "d=20; noise ϵ ~ N(0,1) added to outputs in evaluation",
            "method_or_intervention": "evaluation under label noise (model trained on noiseless prompts; tested on noisy prompts)",
            "performance_result": "Model closely tracks least squares except near interpolation regime; exhibits double-descent curve known for least squares estimators.",
            "mechanistic_insight": "Model does not implement the optimal ℓ2-regularized estimator (since it was trained on noiseless data) but still reflects inductive biases similar to minimum-norm least squares leading to double-descent behavior.",
            "performance_scaling": null,
            "failure_modes": "Suboptimal in noisy settings relative to an estimator trained/tuned for noise (optimal estimator would include ℓ2 regularization).",
            "comparison_baseline": "Compared to minimum-norm least squares (behaves similarly, including double-descent).",
            "key_finding": "A Transformer trained on noiseless linear regression prompts reproduces known statistical phenomena (double-descent) when evaluated on noisy labels, indicating it encodes least-squares-like inductive biases.",
            "uuid": "e323.2",
            "source_info": {
                "paper_title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Razeghi et al. 2022 (term-frequency effect)",
            "name_full": "Impact of pretraining term frequencies on few-shot reasoning",
            "brief_description": "Related work reported that in-context learning for numerical reasoning tasks is better for instances whose terms are more prevalent in the model's pretraining data, implying a data-frequency bias in numerical reasoning.",
            "citation_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "numerical reasoning tasks (unspecified arithmetic types in this paper's citation)",
            "number_range_or_complexity": null,
            "method_or_intervention": "analysis of pretrained language models' few-shot/numerical reasoning performance with respect to term frequency in pretraining",
            "performance_result": null,
            "mechanistic_insight": "Suggests models rely on pretraining data frequency (memorization or distributional coverage) for numerical reasoning; rare terms / rare problem instances yield worse in-context performance.",
            "performance_scaling": null,
            "failure_modes": "Worse in-context performance on numerical reasoning instances containing terms that were rare in pretraining data.",
            "comparison_baseline": null,
            "key_finding": "Pretraining term frequency substantially affects in-context numerical reasoning performance: more prevalent terms lead to better few-shot arithmetic/numeric reasoning.",
            "uuid": "e323.3",
            "source_info": {
                "paper_title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Xie et al. 2022 (Bayesian explanation)",
            "name_full": "An explanation of in-context learning as implicit bayesian inference",
            "brief_description": "Proposes a Bayesian inference framework explaining how in-context learning operates despite formatting differences between training and inference distributions.",
            "citation_title": "An explanation of in-context learning as implicit bayesian inference",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_architecture": null,
            "arithmetic_operation_type": "general in-context learning / inference (applies to numerical tasks as part of general theory)",
            "number_range_or_complexity": null,
            "method_or_intervention": "theoretical framework / analysis",
            "performance_result": null,
            "mechanistic_insight": "Frames in-context learning as implicit Bayesian inference, offering a mechanistic explanation for how models can adapt to new tasks from prompts without parameter updates.",
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": null,
            "key_finding": "In-context learning behavior can be interpreted as approximate Bayesian updating performed implicitly by the model given a prompt.",
            "uuid": "e323.4",
            "source_info": {
                "paper_title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Olsson et al. 2022 (induction heads)",
            "name_full": "In-context learning and induction heads",
            "brief_description": "Hypothesize and present evidence that specific attention-head circuits ('induction heads') inside Transformers are responsible for in-context learning behaviours, e.g., pattern completion/copying.",
            "citation_title": "In-context learning and induction heads",
            "mention_or_use": "mention",
            "model_name": null,
            "model_size": null,
            "model_architecture": "transformer-circuit hypothesis",
            "arithmetic_operation_type": "general in-context learning (mechanistic hypothesis applicable to numerical pattern completion)",
            "number_range_or_complexity": null,
            "method_or_intervention": "circuit-level analysis / interpretability work (hypothesis-driven)",
            "performance_result": null,
            "mechanistic_insight": "Introduces the concept of 'induction heads' — attention-head patterns that implement sequence-level copying/induction which may underpin in-context learning; suggests circuit-level specialization enables few-shot generalization.",
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": null,
            "key_finding": "Certain attention-head structures ('induction heads') plausibly implement mechanisms that support in-context learning, including pattern completion relevant to learning from examples.",
            "uuid": "e323.5",
            "source_info": {
                "paper_title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Pesut 2022 (GPT-3 fitting exploration)",
            "name_full": "Who models the models that model models? an exploration of gpt-3's in-context model fitting ability",
            "brief_description": "An exploratory study showing GPT-3 can fit small tabular/low-dimensional functions in-context and obtain non-trivial accuracy on such tasks.",
            "citation_title": "Who models the models that model models? an exploration of gpt-3's in-context model fitting ability, 2022",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_size": null,
            "model_architecture": "decoder-only transformer (GPT-3)",
            "arithmetic_operation_type": "small tabular / low-dimensional function fitting (includes numeric tasks)",
            "number_range_or_complexity": "one- and two-dimensional problems (as cited)",
            "method_or_intervention": "few-shot in-context prompting with GPT-3",
            "performance_result": "Reported non-trivial accuracy for small tabular/low-dimensional tasks (no numeric values provided in this paper's citation).",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": null,
            "key_finding": "GPT-3 can achieve non-trivial in-context model-fitting accuracy on small low-dimensional numeric/tabular tasks.",
            "uuid": "e323.6",
            "source_info": {
                "paper_title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
                "publication_date_yy_mm": "2022-08"
            }
        },
        {
            "name_short": "Dinh et al. 2022 (LIFT table result)",
            "name_full": "LIFT: Language-interfaced fine-tuning for non-language machine learning tasks",
            "brief_description": "Presents LIFT, a framework for using language models on non-language tasks; includes results (Table 16) showing GPT-3 achieves non-trivial accuracy on small tabular / low-dimensional problems.",
            "citation_title": "Lift: Language-interfaced fine-tuning for non-language machine learning tasks",
            "mention_or_use": "mention",
            "model_name": "GPT-3",
            "model_size": null,
            "model_architecture": "decoder-only transformer (GPT-3) / language-model interface",
            "arithmetic_operation_type": "small tabular / low-dimensional numeric problems (as presented in table results)",
            "number_range_or_complexity": "one- and two-dimensional tasks (per cited table)",
            "method_or_intervention": "language-interfaced fine-tuning / in-context evaluation",
            "performance_result": "Cited as showing GPT-3 obtains non-trivial accuracy in the referenced table (no numeric values provided in this paper's citation).",
            "mechanistic_insight": null,
            "performance_scaling": null,
            "failure_modes": null,
            "comparison_baseline": null,
            "key_finding": "Language-model interfaces (and GPT-3) can be applied to non-language numeric/tabular problems and show non-trivial in-context performance on small problems.",
            "uuid": "e323.7",
            "source_info": {
                "paper_title": "What Can Transformers Learn In-Context? A Case Study of Simple Function Classes",
                "publication_date_yy_mm": "2022-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Impact of pretraining term frequencies on few-shot reasoning",
            "rating": 2
        },
        {
            "paper_title": "An explanation of in-context learning as implicit bayesian inference",
            "rating": 2
        },
        {
            "paper_title": "In-context learning and induction heads",
            "rating": 2
        },
        {
            "paper_title": "Who models the models that model models? an exploration of gpt-3's in-context model fitting ability, 2022",
            "rating": 2
        },
        {
            "paper_title": "Lift: Language-interfaced fine-tuning for non-language machine learning tasks",
            "rating": 2
        },
        {
            "paper_title": "Rethinking the role of demonstrations: What makes in-context learning work?",
            "rating": 1
        }
    ],
    "cost": 0.01962975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>What Can Transformers Learn In-Context? A Case Study of Simple Function Classes</h1>
<p>Shivam Garg<em><br>Stanford University<br>shivamg@cs.stanford.edu<br>Dimitris Tsipras</em><br>Stanford University<br>tsipras@stanford.edu<br>Percy Liang<br>Stanford University<br>pliang@cs.stanford.edu<br>Gregory Valiant<br>Stanford University<br>valiant@stanford.edu</p>
<h4>Abstract</h4>
<p>In-context learning refers to the ability of a model to condition on a prompt sequence consisting of in-context examples (input-output pairs corresponding to some task) along with a new query input, and generate the corresponding output. Crucially, in-context learning happens only at inference time without any parameter updates to the model. While large language models such as GPT-3 exhibit some ability to perform in-context learning, it is unclear what the relationship is between tasks on which this succeeds and what is present in the training data. To make progress towards understanding in-context learning, we consider the well-defined problem of training a model to in-context learn a function class (e.g., linear functions): that is, given data derived from some functions in the class, can we train a model to in-context learn "most" functions from this class? We show empirically that standard Transformers can be trained from scratch to perform in-context learning of linear functions-that is, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator. In fact, in-context learning is possible even under two forms of distribution shift: (i) between the training data of the model and inference-time prompts, and (ii) between the in-context examples and the query input during inference. We also show that we can train Transformers to in-context learn more complex function classes-namely sparse linear functions, two-layer neural networks, and decision trees-with performance that matches or exceeds task-specific learning algorithms. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Large language models such as GPT-3 [Brown et al., 2020] are able to perform in-context learning: given a prompt containing examples from a task (input-output pairs) and a new query input, the language model can generate the corresponding output. For example, these models are able to produce English translations of French words after being prompted on a few such translations, e.g.:</p>
<p>$$
\underbrace{\text { maison } \rightarrow \text { house, chat } \rightarrow \text { cat, chien } \rightarrow}<em _completion="{completion" _text="\text">{\text {prompt }} \underbrace{\text { dog }}</em> .
$$}</p>
<p>This capability is quite intriguing as it allows models to adapt to a wide range of downstream tasks on-the-fly-i.e., without the need to perform any parameter updates after the model is trained [Brown et al., 2020,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Lieber et al., 2021, Rae et al., 2021, Black et al., 2022]. However, it is unclear to what extent these models have developed the ability to learn new tasks from in-context examples alone as opposed to simply indexing into a vast set of known tasks from the training data (e.g., see Min et al. [2022]). ${ }^{2}$</p>
<p>To make progress towards understanding in-context learning, we consider the well-defined problem of learning a function class from in-context examples. That is, we say that a model can in-context learn a function class $\mathcal{F}$ if, for "most" functions $f \in \mathcal{F}$, the model can approximate $f\left(x_{\text {query }}\right)$ for a new query input $x_{\text {query }}$ by conditioning on a prompt sequence $\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k}, f\left(x_{k}\right), x_{\text {query }}\right)$ containing in-context examples and the query input.</p>
<p>Formally, let $D_{\mathcal{X}}$ be a distribution over inputs and $D_{\mathcal{F}}$ be a distribution over functions in $\mathcal{F}$. A prompt $P$ is a sequence $\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k}, f\left(x_{k}\right), x_{\text {query }}\right)$ where inputs (i.e., $x_{i}$ and $x_{\text {query }}$ ) are drawn i.i.d. from $D_{\mathcal{X}}$ and $f$ is drawn from $D_{\mathcal{F}}$. We say that a model $M$ can in-context learn the function class $\mathcal{F}$ up to $\epsilon$, with respect to $\left(D_{\mathcal{F}}, D_{\mathcal{X}}\right)$, if it can predict $f\left(x_{\text {query }}\right)$ with an average error</p>
<p>$$
\mathbb{E}<em _query="{query" _text="\text">{P}\left[\ell\left(M(P), f\left(x</em>\right)\right)\right] \leq \epsilon
$$}</p>
<p>where $\ell(\cdot, \cdot)$ is some appropriate loss function, such as the squared error.
Within this framework, we can now concretely ask:
Can we train a model to in-context learn a certain function class?
Note that, here, being able to in-context learn a function class is a property of model $M$ alone, independent of how it was trained. Training such a model can be viewed as an instance of meta-learning [Schmidhuber, 1987, Naik and Mammone, 1992, Thrun and Pratt, 2012], a general paradigm for learning a model or method that can learn from data.</p>
<p>We empirically study this question, focusing on Transformer models [Vaswani et al., 2017, Radford et al., 2018]-the architecture behind recent large language models-trained from scratch to in-context learn a range of simple, well-defined function classes (e.g. linear functions). Specifically, we sample prompts containing in-context examples (input-output pairs) generated using functions in a given class and train models to predict the function value at the corresponding query inputs. (see illustration in Figure 1). Our findings are as follows.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Can we train a model that in-context learns a function class (here linear functions)? We train Transformers by repeatedly sampling a random function $f$ from that class, as well as random inputs $x_{1}, \ldots, x_{k}$ and training the model to predict each $f\left(x_{i}\right)$ given the prompt $x_{1}, f\left(x_{1}\right), \ldots, x_{i-1}, f\left(x_{i-1}\right), x_{i}$ (wrt squared loss). Then, during inference, we evaluate the model's ability to predict accurately on new, unseen functions.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Transformers can in-context learn linear functions. We show empirically that we can train a standard Transformer from scratch to in-context learn the class of linear functions, with respect to the input distribution $D_{\mathcal{X}}$ being an isotropic Gaussian in 20 dimensions, and $D_{\mathcal{F}}$ being the distribution over linear functions with weight vectors drawn from an isotropic Gaussian (the model was trained on prompts generated from the same distributions $D_{\mathcal{X}}$ and $D_{\mathcal{F}}$ ). Specifically, the trained model achieves error comparable to the optimal least squares estimator, suggesting that it encodes an effective learning algorithm, at least for the distribution used to generate the training prompts.</p>
<p>Generalization to out-of-distribution prompts. To understand the extent to which the trained model encodes an algorithm that works beyond the training distribution, we consider in-context learning under two types of distribution shifts: (a) a shift between the prompts encountered during training and inference (e.g., training on prompts without any noise in the in-context example outputs but testing with noisy outputs), (b) a shift between the in-context examples and the query input during inference (e.g., in-context examples lie in one orthant and the query input lies in another). We find that the performance of our model is quite robust to such shifts, indicating that it has learned to perform linear regression with some generality.</p>
<p>More complex function classes. We also consider the function classes of 3-sparse linear functions, two-layer ReLU neural networks with 100 hidden units, and decision trees of depth 4, all with 20 dimensional inputs. We show that we can again train Transformer models that can in-context learn these classes (with respect to isotropic Gaussian inputs and appropriately defined distributions over functions). For sparse linear functions, the trained model is able to exploit sparsity, obtaining performance better than least squares and comparable to Lasso. For neural networks, the corresponding model is able to obtain performance comparable to neural networks of the same architecture trained using gradient descent on in-context examples. Moreover, it is also able to in-context learn linear functions. For decision trees, the trained model can learn unseen trees with as few as 100 in-context examples, whereas greedy learning and tree boosting algorithms are unable to achieve competitive performance (for the distribution of prompts studies here). Note that learning these function classes requires involved algorithms (e.g., gradient descent with the Lasso objective), and our results show that Transformers can encode algorithms with similar performance in a single forward pass.</p>
<p>Role of model capacity and problem dimension. Finally, we explore how the ability of Transformers to in-context learn linear functions scales with model capacity and problem dimensionality. We find that increasing the capacity of the model improves performance significantly, and also allows the model to incontext learn higher-dimensional functions. Moreover, increasing the capacity often significantly improves performance with distribution shifts, even when the absolute improvement in the standard error is small.</p>
<h1>2 Training models for in-context learning</h1>
<p>We now describe a general methodology for training a model that can in-context learn a function class $\mathcal{F}$ with respect to a distribution $D_{\mathcal{F}}$ over functions, and $D_{\mathcal{X}}$ over inputs. To do so, we start by constructing random training prompts as follows. For each prompt, we first sample a random function $f$ from the class according to $D_{\mathcal{F}}$, then create a set of random inputs $x_{1}, \ldots, x_{k+1}$ drawn independently from $D_{\mathcal{X}}$, and finally evaluate $f$ on these inputs to produce the prompt $P=\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k+1}, f\left(x_{k+1}\right)\right)$. For example, in the case of linear functions, inputs could be drawn from the isotropic Gaussian distribution $N\left(0, I_{d}\right)$, and a random function chosen by sampling weight vector $w$ from $N\left(0, I_{d}\right)$ and setting $f(x)=w^{\top} x$.</p>
<p>Now, given such prompts, we train a model to predict $f\left(x_{i}\right)$ for a given $x_{i}$ based on a set of preceding in-context examples. Concretely, let $P^{i}$ denote the prompt prefix containing $i$ in-context examples (the first $i$ input-output pairs) and the $(i+1)^{\text {th }}$ input: $P^{i}=\left(x_{1}, f\left(x_{1}\right), x_{2}, f\left(x_{2}\right), \ldots, x_{i}, f\left(x_{i}\right), x_{i+1}\right)$. Then, we train a</p>
<p>model $M_{\theta}$ parameterized by $\theta$ aiming to minimize the expected loss over all the prompt prefixes:</p>
<p>$$
\min <em P="P">{\theta} \mathbb{E}</em>\right)\right)\right]
$$}\left[\frac{1}{k+1} \sum_{i=0}^{k} \ell\left(M_{\theta}\left(P^{i}\right), f\left(x_{i+1</p>
<p>where $\ell(\cdot, \cdot)$ is an appropriately chosen loss function. Below, we describe how this general methodology can be implemented for a concrete model family (see Appendix A for additional details).</p>
<p>Model structure. We use a decoder-only Transformer architecture [Vaswani et al., 2017] from the GPT-2 family [Radford et al., 2019]. Our model consists of 12 layers, 8 attention heads, and a 256-dimensional embedding space ( 9.5 M parameters). This architecture takes as input a sequence of vectors in its embedding space and predicts the next vector in the sequence within the same space (in language modeling, these vectors correspond to input tokens). We apply this architecture to our prompt format of $\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k+1}, f\left(x_{k+1}\right)\right)$ as follows. We map each prompt output $f\left(x_{i}\right)$ to the same dimension as prompt inputs $x_{i}$ by appending zeros, and map the prompt inputs and outputs into the latent embedding space of the Transformer through a (learnable) linear transformation. We then use another (learnable) linear transformation to map the vector predicted by the model to a scalar. Note that the Transformer architecture allows us to compute the prediction $\left(M_{\theta}\left(P^{i}\right)\right)$ for all prompt prefixes in a single forward pass.</p>
<p>Training. We train the model according to the training objective in (2) using squared error as the loss function. We do so by sampling a batch of random prompts at each training step and then updating the model through a gradient update (we use a batch size of 64 and train for 500k total steps). This training is done from scratch, that is, we do not fine-tune a pre-trained language model, nor do we train on actual text.</p>
<p>Curriculum learning. Many natural function classes contain functions of varying complexity. We exploit this by training our model using a curriculum [Bengio et al., 2009, Elman, 1993, Sanger, 1994, Wu et al., 2020], where we train on a simpler distribution of functions in the beginning (e.g., linear functions with weight vectors restricted to a low-dimensional subspace) and gradually increase the function complexity. This speeds up training drastically, often allowing us to train models that would be significantly more expensive to train without a curriculum (see Section 6 for details).</p>
<h1>3 In-context learning of linear functions</h1>
<p>In the previous section, we describe a general methodology for training Transformer models to in-context learn a class of functions. Here, we focus on a simple function class-namely linear functions-and study how well models trained using our methodology can in-context learn this class.</p>
<p>Prompt distribution. We consider the class of linear functions $\mathcal{F}=\left{f \mid f(x)=w^{\top} x, w \in \mathbb{R}^{d}\right}$, in $d$ dimensions where $d=20$. We sample $x_{1}, \ldots, x_{k}, x_{\text {query }}$, and $w$ independently from the isotropic Gaussian distribution $N\left(0, I_{d}\right)$. We then compute each $y_{i}=w^{\top} x_{i}$ and construct the prompt as $P=$ $\left(x_{1}, y_{1}, x_{2}, y_{2}, \ldots, x_{k}, y_{k}, x_{\text {query }}\right)$.</p>
<p>Baselines. To contextualize the performance of our trained model, we compare it to other learning algorithms: (a) the least squares estimator, computing the minimum-norm linear fit to the in-context examples $\left(x_{i}, y_{i}\right)$, (b) $n$-Nearest Neighbors, averaging the $y_{i}$ values for the $n$ nearest neighbors of $x_{\text {query }}$, (c) averaging the values $y_{i} x_{i}$ to estimate $w$ and compute the inner product of this estimate with $x_{\text {query }}$. Least squares is the optimal estimator for this problem and thus serves as a lower bound to the best error one can achieve. The other two baselines are consistent (but sub-optimal) estimators that are easier to compute and thus provide an estimate of the performance achieved by simple approaches. See Appendix A. 3 for more details.</p>
<h1>3.1 Transformers can in-context learn linear functions</h1>
<p>We show the in-context learning ability of the resulting model along with the relevant baselines in Figure 2. The trained Transformer is able to in-context learn the class of linear functions with respect to the prompt distribution specified above, performing comparably to the optimal least squares estimator for any number of in-context examples considered. While the simpler baselines achieve non-trivial error, they are far worse, indicating that the trained model encodes a more complex algorithm.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Evaluating the trained Transformer on in-context learning linear functions. We plot the normalized squared error of the Transformer $\left(\left(M(P)-w^{\top} x_{\text {query }}\right)^{2} / d\right)$, along with the relevant baselines, as a function of the number of in-context examples. Transformer's error decreases at a rate comparable to least squares. When the number of in-context examples reaches the problem dimension $d$ (here 20), least squares achieves 0 error while the Transformer achieves an error of 0.02 , improving to 0.0006 at $2 d$ in-context examples. While the simple baselines obtain better-than-trivial error (zero estimator, dashed line), their performance is relatively poor. (Error averaged over 1280 prompts. $90 \%$ confidence intervals over 1000 bootstrap trials.)</p>
<p>Can memorization of training prompts explain model performance? Note that the probability of the model encountering a training prompt similar to the one used for testing is astronomically low-the prompt inputs alone lie in a 800-dimensional space when predicting with $2 d$ in-context examples $(d=20)$. Moreover, even considering the possibility that the model encountered a similar weight vector during training cannot explain its performance. That is, the model encounters 32 million random weight vectors during training and even using the best of these vectors would lead to an expected error of around 0.2 (computed empirically, see Appendix B. 7 for details). However, the model is able to achieve an error of less than 0.001 for a prompt with $2 d$ in-context examples. Further, in Section 6, we show that the model is able to obtain a similar error even when trained on prompts generated using only 10,000 distinct weight vectors, in which case the best weight vector seen during training would yield an even worse error of around 0.5 . Thus, the model cannot be relying on memorization of training prompts or weight vectors, and instead encodes an algorithm capable of in-context learning linear functions that are very different from those seen during training.</p>
<h3>3.2 What functions is the model learning in-context?</h3>
<p>Recall that the goal of our model is: given the prompt $P=\left(x_{1}, w^{\top} x_{1}, \ldots, x_{k}, w^{\top} x_{k}, x_{\text {query }}\right)$, output $w^{\top} x_{\text {query }}$. Thus, if we fix the prefix given by the $k$ in-context examples, we can view the output of the model as a function $f_{w, x_{1 d}}\left(x_{\text {query }}\right)$, that approximates $w^{\top} x_{\text {query }}$. When $k&lt;d$ (fewer in-context examples than dimensions), the ground truth cannot be recovered perfectly and the ideal model should approximate $\left(\operatorname{proj}<em 1="1" d="d">{x</em>}}(w)\right)^{\top} x_{\text {query }}$, where $\operatorname{proj<em 1="1" d="d">{x</em>$. Here, we will evaluate how accurately the model approximates this.}}(w)$ is the projection of $w$ onto the subspace spanned by $x_{1}, \ldots, x_{k</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Understanding the prefix-conditioned function. (a) We plot the model prediction as we fix the in-context examples and vary the query input along a random direction (for three random prompts). The shaded regions denote the intervals in which the norm of a randomly training input lies with probability 0.99 . When the scale of the query input is close to this range, the model prediction is close to the ground truth linear function (or its projection to the space of in-context inputs when $k&lt;d$ ). (b) We compute the gradient of the model prediction with respect to the query input, and plot its (normalized) inner product with the true $w$ and projected $w$, averaged over 1280 random prompts. The gradient aligns almost perfectly with $w$ when $k \geq d$, and with projected $w$ for all $k$, indicating that the model locally aligns with the ground truth.</p>
<p>Visualizing along a random direction. For a randomly sampled fixed prefix, we visualize $\hat{f}<em 1:="1:" k="k">{w, x</em>}}\left(x_{\text {query }}\right)$ as we vary the query input along a random direction $x$ (Figure 3a). That is, we pick a random unit vector $x$, and evaluate $\hat{f<em 1:="1:" k="k">{w, x</em>}}(\lambda x)$ as we vary $\lambda$, the distance of the query input from origin. We observe that $\hat{f<em 1:="1:" d="d">{w, x</em>}}(\lambda x)$ and $\hat{f<em 1:="1:" 2="2" d="d">{w, x</em>}}(\lambda x)$ closely match the ground truth and $\hat{f<em 1:="1:" 2="2" _="/" d="d">{w, x</em>(\lambda x)$ matches the projected ground truth, when the distance from origin is not too large compared to the norm of a typical randomly sampled input. In fact, in Appendix B.1, we show that the model is quite robust to scaling the query input: the error doesn't increase much as we scale up the query input by a factor of up to 2 , or scale down by a factor of up to 16 , and degrades slowly after that.}</p>
<p>Local correctness. So far, we have seen that the model is able to make predictions close to the ground truth for randomly drawn query inputs and in-context examples. We will now turn our attention to the local change of $\hat{f}$ around $x_{\text {query }}$ by considering the gradient of the function $\hat{f}<em 1:="1:" k="k">{w, x</em>}}\left(x_{\text {query }}\right)$ with respect to $x_{\text {query }}$ (our model is fully differentiable so we can compute the gradient directly). Since $\hat{f}$ computed by the model should ideally approximate $\operatorname{proj<em 1:="1:" k="k">{x</em>}}(w)^{\top} x$, this gradient should lie in the direction of the projected ground truth $\operatorname{proj<em 1:="1:" k="k">{x</em>}}(w)$. In Figure 3b, we show the inner product between the gradient and $\operatorname{proj<em 1:="1:" k="k">{x</em>}}(w)$ (both normalized), averaged over 1280 random prompts, and observe that they align almost perfectly. Since $\operatorname{proj<em 1:="1:" k="k">{x</em>(w)=w$ almost surely when $k \geq d$, we observe that the gradient also aligns with $w$ perfectly in this regime. Thus the model is locally correct with respect to changes in the query input.}</p>
<h1>4 Extrapolating beyond the training distribution</h1>
<p>In the previous section, we demonstrated that we can train a model to in-context learn linear functions with respect to the distribution of prompts encountered during training. That is, we evaluate the in-context learning ability of the model with respect to distributions $D_{\mathcal{X}}$ and $D_{\mathcal{F}}$ that were also used to train the model.</p>
<p>Here, we evaluate the in-context learning performance of our model on prompt distributions different from the one used for training. Our overarching goal here is to better understand the learning algorithm encoded by our model by analysing how it responds to different prompt distributions.</p>
<p>Formally, we will refer to the distribution of functions used during training as $D_{\mathcal{X}}^{\text {train }}$ and the corresponding distribution of prompt inputs as $D_{\mathcal{X}}^{\text {train }}$. Then, during inference, functions are sampled from a (potentially different) distribution $D_{\mathcal{X}}^{\text {test }}$, while prompt inputs from a distribution $D_{\mathcal{X}}^{\text {test }}$. Moreover, deviating again from our analysis so far, we also consider a separate distribution $D_{\text {query }}^{\text {test }}$, from which the query input is sampled, potentially dependent on the rest of the in-context inputs $x_{1}, \ldots, x_{k}$ (which are still sampled from $D_{\mathcal{X}}^{\text {test }}$ ).</p>
<p>Within this framework, we consider the same model as last section, and evaluate its performance on prompts that deviate from those encountered during training, either by</p>
<ol>
<li>sampling prompt inputs or functions from a different distribution, that is $D_{\mathcal{X} / \mathcal{F}}^{\text {train }} \neq D_{\mathcal{X} / \mathcal{F}}^{\text {test }}$ or</li>
<li>introducing a mismatch between in-context examples and the query input, that is $D_{\text {query }}^{\text {test }} \neq D_{\mathcal{X}}^{\text {test }}$.</li>
</ol>
<p>We describe each such prompt structure below and present a subset of the results in Figure 4 (see Appendix B. 2 for additional details and full results). Overall, the model performs reasonably accurate in-context learning with respect to these prompt distributions, indicating that it has indeed learnt to perform linear regression to some generality.</p>
<p>Recall that we generate a training prompt $P=\left(x_{1}, w^{T} x_{1}, \ldots, x_{k}, w^{T} x_{k}, x_{\text {query }}\right)$ by drawing the prompt inputs ( $x_{i}$ and $x_{\text {query }}$ ), and the weight vector ( $w$ ) i.i.d. from $N\left(0, I_{d}\right)$, with $d=20$. For all the settings below, except prompt scaling, we normalize the inputs so that their expected squared norm is equal to that of inputs encountered during training.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: In-context learning on out-of-distribution prompts. We evaluate the trained model on prompts that deviate from those seen during training by: (a) sampling prompt inputs from a non-isotropic Gaussian, (b) adding label noise to in-context examples, (c) restricting in-context examples to a single (random) orthant. In all cases, the model error degrades gracefully and remains close to that of the least squares estimator, indicating that its in-context learning ability extrapolates beyond the training distribution.</p>
<p>Skewed covariance. We sample prompt inputs from $N(0, \Sigma)$ where $\Sigma$ is a skewed covariance matrix with eigenbasis chosen uniformly at random and $i^{\text {th }}$ eigenvalue proportional to $\frac{1}{2} / \hat{r}$. The model matches the performance of least squares until $k=10$, mimicking the sharp drop in the error in this regime, but its error plateaus afterwards (see Figure 4a). Thus, it is not perfectly robust to this distribution mismatch but still does relatively well, achieving less than half the error of the nearest neighbor baseline in most cases.</p>
<p>Low-dimensional subspace. We sample prompt inputs from a random 10 dimensional subspace. In this case, the model achieves low error after 10 in-context examples, closely matching the behavior of the optimal least squares estimator (the model achieves an error of $0.036,0.0014$, and 0.00057 at 10,20 , and 40 in-context examples respectively)—see Appendix Figure 8b. Crucially, unlike the training prompts, when $k$ is between 10 and 20, the prompt inputs are linearly dependent, and a model achieving low error in this regime indicates that it encodes a valid orthogonalization procedure for these inputs.</p>
<p>Noisy linear regression. We add noise to each prompt output, that is, the $i^{\text {th }}$ output is equal to $w^{T} x_{i}+\epsilon_{i}$ where $\epsilon_{i} \sim N(0,1)$. The trained model closely tracks the performance of least squares when the number of in-context examples is not close to the input dimension 20 (see Figure 4b). Interestingly, the model also exhibits the double descent error curve [Belkin et al., 2019] that is known to manifest for the least squares estimator [Nakkiran, 2019]. Note that in this noisy setting, the optimal estimator corresponds to solving least squares with appropriate $\ell_{2}$-regularization. However, since the model was trained on noiseless data, we cannot expect it to learn this.</p>
<p>Prompt scale. We consider the setting where the prompt scale between training and inference is different. We either scale the prompt inputs or the weight vectors, by a factor $\left{1 / 3,1 / 2,2,3\right}$. The model is relatively robust when scaling the weight vector, but not as robust when scaling the prompt inputs, especially for the more extreme scales $1 / 3$ and 3 . Specifically, for 40 in-context examples, the model achieves errors $0.0012,0.0008,0.0016,0.0278$ when scaling the weights, and errors $0.30,0.013,0.043,0.58$ while scaling the inputs, by factors $1 / 3,1 / 2,2$ and 3 respectively (Appendix Figure 9). For context, recall that with 40 in-context examples, the least squares estimator achieves an error of 0 whereas the model achieves an error of 0.0006 at the original scale.</p>
<p>Different orthants for in-context and query inputs. We fix the sign of each coordinate to be positive or negative for all in-context inputs $x_{i}$ (at random). As a result, all in-context inputs lie in the same orthant, while the query input lies in another orthant with high probability. The model is not affected by the mismatch between in-context and query inputs and closely match the performance of least squares. In this case, the model achieves errors 0.062 and 0.004 for 20 and 40 in-context examples respectively (see Figure 4c), whereas recall that it achieves errors 0.02 and 0.0006 on standard prompts. This indicates that the model is not relying on some variant of nearest neighbor search as in that case, its error would have been significantly larger (see the 3-nearest neighbor baseline).</p>
<p>Query input orthogonal to in-context inputs. We sample the query input from the subspace orthogonal to the subspace spanned by in-context example inputs. Here, there is no information relevant to the query input in the in-context examples and thus the model would ideally predict something close to 0 to minimize the error. Indeed, the model outputs such a prediction, achieving an error close to 1 (Appendix Figure 8d).</p>
<p>Query input matches an in-context example. We choose the query input to match one of the in-context examples inputs chosen uniformly at random. In this case, the model achieves errors $0.001,0.001,0.0005$ for 10, 20, 40 examples respectively thus making close to the correct prediction, without being affected by the additional in-context examples present (Appendix Figure 8e).</p>
<h1>5 More complex function classes</h1>
<p>We now turn our attention to in-context learning for more complex function classes, namely sparse linear functions, decision trees, and two-layer ReLU neural networks. Here, we are back in the setting where the distribution of prompts during inference is same as that during training (except the setting of neural networks where we evaluate on linear functions as well). The overall methodology remains the same: we sample random functions from these families and train a Transformer from scratch to approximate these functions given in-context examples. (See Appendix A. 3 for more details and baselines.)</p>
<p>Sparse linear functions. First, we consider functions of the form $f(x)=w^{\top} x$ where $w \in \mathbb{R}^{d}$ and has exactly $s$ non-zero coordinates. To sample a prompt $P=\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k}, f\left(x_{k}\right), x_{\text {query }}\right)$, we draw prompt inputs $x_{i}$ and $x_{\text {query }}$, and a weight vector $w$ from $N\left(0, I_{d}\right)$, and then zero out all but $s$ coordinates of $w$ uniformly at random. We choose $d=20$ and $s=3$. In this setting, the least squares estimator is no longer</p>
<p>optimal-one can perform better by leveraging the weight vector sparsity. One estimator that leverages sparsity is Lasso [Tibshirani, 1996], which involves solving the least squares objective with an $\ell_{1}$-norm regularizer for the weight vector. We plot the performance of our model in Figure 5a, and observe that it is also able to leverage sparsity, nearly matching the performance of Lasso. Our model achieves errors 0.58 and 0.09 while Lasso achieves errors 0.62 and 0.08 for $k=5$ and 10 respectively. Note that, unlike least squares, Lasso does not have a closed form expression and involves iterative minimization of the regularized objective, yet the Transformer is able to achieve comparable performance in a single forward pass.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training a Transformer to in-context learn more complex function classes. (a) A Transformer trained on prompts generated using sparse linear functions can in-context learn this class, with error decreasing at a rate similar to Lasso, and significantly better than minimum norm least squares. (b) A Transformer trained on prompts generated using random decision trees can in-context learn this class, with much better performance than greedy tree learning or tree boosting. (c) A Transformer trained on prompts generated using random 2-layer ReLU neural networks can in-context learn this class. The error decreases at a rate similar to the baseline which involves training a neural network using a variant of gradient descent with in-context examples as the training data. (d) The same model (from (c)) can in-context learn the class of linear functions. The error decreases at a rate slower than least squares, but comparable to a neural network trained using a variant of gradient descent. In all cases, the errors are normalized so that the trivial zero estimator achieves an error of 1 (dashed line).</p>
<p>Decision trees. Next, we consider the class of depth 4 decision trees with 20 dimensional inputs. A function $f$ in this class is represented by a full binary tree (with 16 leaf nodes) where each non-leaf node is associated with a coordinate, and each leaf node is associated with a target value. To evaluate $f$ on an input $x$, we traverse the tree starting from the root node, and go to the right child if the coordinate associated with the current node is positive and go to the left child otherwise (that is, the threshold at each node is 0 ). $f(x)$ is given by the value associated with the leaf node reached at the end. To sample a random prompt $P=\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k}, f\left(x_{k}\right), x_{\text {query }}\right)$, we draw prompt inputs $x_{i} \mathrm{~s}$ and $x_{\text {query }}$ from $N\left(0, I_{d}\right)$, and $f$ corresponds to a tree where the coordinates associated with the non-leaf nodes are drawn uniformly at random from</p>
<p>${1,2, \ldots, d}$ and the values associated with the leaf nodes are drawn from $N(0,1)$. In Figure 5b, we show that Transformers can be trained to in-context learn this class, with performance much better than greedy tree learning and boosting (via XGBoost [Chen and Guestrin, 2016]). With $k=100$ in-context examples, the Transformer achieves an error of 0.12 whereas greedy learning achieves an error of 0.80 and XGBoost achieves an error of 0.62 .</p>
<p>Since the decision trees in our function class predict solely based on the sign of each coordinate of $x_{i}$, we also consider a baseline where we provide the greedy learning and XGBoost algorithms with the signs of each $x_{i}$ instead. This significantly improves their performance-at 100 in-context examples, greedy achieves an error of 0.50 and XGBoost an error if 0.31 -but they still perform much worse than the trained Transformer.</p>
<p>Note that, in general, we do not have a good understanding of the space of efficient algorithms for learning decision trees, and the conditions under which known heuristics work [Blanc et al., 2021, Brutzkus et al., 2020]. At the same time, we found that Transformers can be trained to directly discover such an algorithm for the prompt distribution we considered. This suggests an intriguing possibility where we might be able to reverse engineer the algorithm encoded by a Transformer to obtain new sample efficient algorithms for existing learning problems.</p>
<p>Two-layer ReLU neural networks. Finally, we consider the class of two layer ReLU neural networks containing functions of the form $f(x)=\sum_{i=1}^{r} \alpha_{i} \sigma\left(w_{i}^{\top} x\right)$, where $\alpha_{i} \in \mathbb{R}, w_{i} \in \mathbb{R}^{d}$ and $\sigma(\cdot)=\max (0, \cdot)$ is the ReLU activation function. To draw a random prompt $P=\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k}, f\left(x_{k}\right), x_{\text {query }}\right)$, we sample prompt inputs $x_{i} \mathrm{~s}$ and $x_{\text {query }}$ from $N\left(0, I_{d}\right)$, along with network parameters $a_{i} \mathrm{~s}$ and $w_{i} \mathrm{~s}$ from $N(0,2 / r)$ and $N\left(0, I_{d}\right)$ respectively. We set the input dimension $d$ to 20 and the number of the hidden nodes $r$ to 100. In Figure 5c, we show that Transformers can be trained to in-context learn this class of functions. In fact, the Transformer performs comparably to the baseline which involves training a two-layer neural network of the same architecture on in-context examples using Adam [Kingma and Ba, 2014], a variant of gradient descent (see Appendix A. 3 for details). Specifically, for $k=100$ in-context examples, both the Transformer and the neural network trained on in-context examples achieve an error of 0.17 .</p>
<p>Moreover, the model trained to in-context learn two-layer neural networks is also able to in-context learn linear functions (for which it is not explicitly trained), albeit with a rate slower than least squares, but comparable to a neural network trained on in-context examples generated using a linear function (Figure 5d). For $k=20,50$, and 100 in-context examples respectively, the Transformer achieves error $0.34,0.05$, and 0.01 , and the two-layer network achieves error $0.37,0.04$, and 0.003 (the least squares estimator achieves error 0 for $k \geq 20$ ).</p>
<h1>6 Investigating what matters for in-context learning</h1>
<p>We now return to the setting of training models to in-context learn linear functions and explore different factors that lead to successful in-context learning.</p>
<p>Problem Dimension and Capacity. In Section 3 and 4, we saw that Transformer models can be trained to in-context learn 20-dimensional linear functions accurately and relatively robustly. To explore the interplay between problem dimensionality and capacity, we also consider models with fewer parameters (see Appendix A.1) and train each architecture on ${10,30,40,50}$-dimensional problems. In Figure 6, we plot the model error with $2 d$ in-context examples as we vary the problem dimension $d$ and the model capacity. In the standard setting, i.e., when the training and inference time prompt distributions are the same, we observe that the error decreases as we increase the capacity or reduce the problem dimensionality (see Figure 6a). Thus, model capacity helps perform accurate in-context learning. For out-of-distribution prompts, we observe that the settings where the input covariance is skewed or where in-context example inputs and query inputs lie in different orthants are particularly challenging, especially for higher dimensional problems. However, the error decreases considerably (in most cases) as we increase the model capacity, even when absolute decrease in the standard error is small (see Figure 6b and 6c). See Appendix B. 3 for additional plots.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Understanding the effect of model capacity and problem dimension on in-context learning performance for in-distribution (a) and out-of-distribution (b,c) prompts. We train Transformers to in-context learn linear functions and plot the error with $2 d$ in-context examples as we vary problem dimension $d$ and model capacity. Capacity helps with in-context learning in most cases, especially on out-of-distribution prompts (even when the absolute gains in the in-distribution setting are small). We train 3 models in each case with different random seeds, and show the median error (solid lines), and the minimum and maximum errors (shaded region). (See Appendix B. 4 for training variance analysis.)</p>
<p>Curriculum. We train our models using curriculum learning. That is, we initially draw the prompt inputs from a fixed 5 dimensional subspace (by setting some of the coordinates to 0 ) with prompt length 11 (number of input-output pairs), and increase the subspace dimension by 1 and prompt length by 2 every 2,000 training steps, until the subspace dimension reaches the ambient dimension $d$ and prompt length reaches $2 d+1$ (see Appendix A. 2 for details). This process can also be viewed as gradually increasing the complexity of the function class. This speeds up training drastically, especially for higher dimensional problems: for dimension 50, the loss barely decreases through the 500k training steps without curriculum but reaches close to the optimum with curriculum. For the 20 dimensional problem where we were able to train the model without curriculum within the training (step count) budget, we did not observe any qualitative difference in accuracy or robustness compared to the model trained with curriculum. We include plots comparing the speed and accuracy of training with and without curriculum in Appendix B.5.</p>
<p>Notably, when training Transformers without curriculum, there is an initial—relatively long—period in training where the loss does not decrease, followed by a period of sharp decrease. The length of this period varies with training randomness and seems to increase on average with problem dimension. Understanding the model just before and after this transition moment is a promising future direction, which can give insights into the emergence of in-context learning. Interestingly, Olsson et al. [2022] observe a similar jump in the in-context learning ability of a language model which they attribute to the formation of "induction heads".</p>
<p>Number of distinct prompts or functions seen during training. To estimate the amount of training data required for in-context learning, we perform two ablation studies. In the first study, we limit the number of distinct prompts seen during training. That is, we create a set of $n_{p}$ randomly generated prompts (as described in Section 2), and sample prompts from this set during training (here, we train without curriculum, as it would introduce additional prompts during the warmup phase). In the second study, we only limit the number of distinct functions used for training. That is we create a set of $n_{w}$ randomly chosen vectors (corresponding to $n_{w}$ linear functions) and sample weight vectors uniformly from that set to generate the training prompts (the inputs are still sampled from $N\left(0, I_{d}\right)$ for each training prompt). We find that the amount of training data required is relatively small: non-trivial in-context learning is possible with $n_{p}=100 \mathrm{k}$ or $n_{w}=1 \mathrm{k}$, and the error drops close to that of the unrestricted model (discussed in Section 3) with $n_{p}=1 \mathrm{M}$ or $n_{w}=10 \mathrm{k}$ (details in Appendix B.6). For context, in Section 3, the model is trained on fresh prompts each step, thus encountering 32 M distinct functions and prompts ( 500 k training steps with 64 prompts/batch).</p>
<h1>7 Related work</h1>
<p>In-context learning. Since Brown et al. [2020] demonstrated the in-context learning ability of GPT-3, there has been a significant interest in improving and understanding this capability [Liu et al., 2021, Min et al., 2021a, Zhao et al., 2021, Lu et al., 2021b, Rubin et al., 2021, Min et al., 2021b, Chen et al., 2021, Mishra et al., 2021, Lampinen et al., 2022]. The works most relevant to ours are as follows. Xie et al. [2022] propose a Bayesian inference framework explaining how in-context learning works despite formatting differences between training and inference distributions. Razeghi et al. [2022] show that in-context learning for numerical reasoning tasks is better for instances whose terms are more prevalent in training data. Min et al. [2021a] demonstrate tasks where in-context learning works even when the prompt outputs are chosen randomly, questioning to what extent these models are truly learning new tasks on-the-fly, while Rong [2021] gives examples of novel tasks on which these models demonstrate on-the-fly learning ability. Chan et al. [2022] demonstrate that distributional properties such as long-tailedness are crucial for in-context learning on an image-based few-shot dataset. Olsson et al. [2022] and Elhage et al. [2021] consider a different framing of in-context learning, referring to any model behavior that utilizes information in a prompt to make predictions that improve with prompt size. They hypothesize the existence of special circuits inside Transformer models responsible for in-context learning, that can complete prompts by copying previous similar patterns in the prompt sequence. Pesut [2022] and Dinh et al. [2022, Table 16] consider in-context learning for small tabular datasets and learning problems in one and two dimensions, and show that GPT-3 can obtain non-trivial accuracy. Our work contributes to and complements this line of work, by posing in-context learning as a well-defined problem of learning function classes at inference time, and empirically investigating training models that in-context learn simple function classes.</p>
<p>Transformers. There is a long line of work investigating the capabilities [Vaswani et al., 2017, Dehghani et al., 2018, Yun et al., 2019, Pérez et al., 2019, Yao et al., 2021, Bhattamishra et al., 2020b, Zhang et al., 2022], limitations [Hahn, 2020, Bhattamishra et al., 2020a], applications [Lu et al., 2021a, Dosovitskiy et al., 2020, Parmar et al., 2018], and internal workings [Elhage et al., 2021, Snell et al., 2021, Weiss et al., 2021, Edelman et al., 2022, Olsson et al., 2022] of Transformer models. Most similar to our work, Müller et al. [2021] and Nguyen and Grover [2022] demonstrate the ability of Transformer models to solve prediction tasks using the input context, albeit in different settings. Müller et al. [2021] introduce a "Prior-data fitted transformer network" that is trained to approximate Bayesian inference with priors such as Gaussian processes and Bayesian neural networks, and use it to perform downstream tasks such as tabular dataset classification and few-shot image classification. Nguyen and Grover [2022] introduce Transformer neural processes, building on prior work on neural processes [Garnelo et al., 2018b,a, Kim et al., 2019], and show that they achieve state-of-the art performance on tasks such as image completion and contextual multi-armed bandits. Our work complements these works, focusing on understanding the in-context learning ability of Transformers for various simple function classes and the extent to which this ability extrapolates beyond the training distribution.</p>
<p>Meta learning. Training a model to perform in-context learning can be viewed as an instance of the more general learning-to-learn or meta-learning paradigm [Schmidhuber, 1987, Naik and Mammone, 1992, Thrun and Pratt, 2012]. Typical approaches from this extensive line of work (see [Hospedales et al., 2020] for a survey) include: training a meta-learner on how to update the parameters of a downstream learner [Bengio et al., 1995, Li and Malik, 2016], learning parameter initializations from which one can quickly train for many downstream tasks [Finn et al., 2017, Ravi and Larochelle, 2017], learning latent embeddings that allow for effective similarity search [Snell et al., 2017]. Most relevant to our setting are approaches that directly take as input examples from a downstream task and a query input and produce the corresponding output [Hochreiter et al., 2001, Mishra et al., 2018, Santoro et al., 2016, Garnelo et al., 2018b,a, Kirsch and Schmidhuber, 2021]. Our work contributes to this line of work, by investigating the learning-to-learn abilities of Transformer models in a well-defined setting.</p>
<p>Data-driven algorithm design. Another line of work aims to discover algorithms that perform well on a distribution of inputs [Horvitz et al., 2001, Xu et al., 2008, Vinyals et al., 2015, Bello et al., 2016, Khalil et al., 2017, Selsam et al., 2018, Schwarzschild et al., 2021] (as opposed to algorithms with guarantees on their worst-case performance). See Balcan [2020] for a survey on advancements on the theoretical foundations of such algorithms. Our work can be viewed as part of this line of work, as we train Transformer models to discover algorithms for different learning problems.</p>
<h1>8 Discussion</h1>
<p>In this work, we formalize and study the question: can we train models that learn different classes of functions in-context? We show that Transformer models trained from scratch can in-context learn the class of linear functions, with performance comparable to the optimal least squares estimator, even under distribution shifts. Moreover, we show that in-context learning is also possible for sparse linear functions, decision trees, and two-layer neural networks; learning problems which are solved in practice with involved iterative algorithms such as gradient descent.</p>
<p>At the same time, understanding the implications of our results for language models requires further investigation. A pertinent question regarding the in-context learning capabilities of language models is how they leverage in-context examples [Min et al., 2022]. Our results demonstrate that Transformers can encode complex learning algorithms that utilize in-context examples in a far-from-trivial manner. In fact, this is the case for standard Transformer architectures trained with standard optimization procedures. The extent to which such non-trivial in-context learning behavior exists in large language models is still open, but we believe that our work takes a step towards formalizing and investigating this question.</p>
<p>Our work lays the groundwork for several future directions.
Complexity of in-context learning. We empirically show that model capacity helps in performing incontext learning accurately and robustly. This raises the question: How does the in-context learning loss (1) depend on the complexity of the function class $\mathcal{F}$, the capacity of model $M$, and the number of prompts used to train $M$. Even the right notion of complexity of $\mathcal{F}$ is unclear and may depend on the model family. Understanding this question for models explicitly trained to perform in-context learning may suggest an upper bound for the in-context learning performance of models such as GPT-3 that have not been explicitly trained for this purpose.</p>
<p>Curriculum learning. Within our framework, there is natural notion of curriculum learning where during training, we gradually increase the complexity of the function class learned in-context. This leads to drastic speed-ups in training. What is the reason behind such a speedup? Are similar speedups also possible for training large language models? Understanding these questions can have implications for training of models on large real-world datasets, potentially reducing the time and energy used for training.</p>
<p>Inductive bias of model families. Our framework presents an opportunity to understand and compare the inductive biases of different model families (e.g., Transformers vs. LSTMs) in a well-defined setting. For instance, a concrete question is: Are there function classes that are easier to in-context learn using Transformers but harder for LSTMs and vice-versa?</p>
<p>Understanding the learning algorithms encoded in Transformers. The models we train are able to perform in-context learning, and are thus themselves encoding learning algorithms. A worthwhile research direction would be to investigate the internal workings of these models and better understand the exact learning algorithms that they encode. Moreover, for settings such as decision trees, we do not have a good understanding of what the optimal learning algorithms are. Nevertheless, in Section 5 we found that Transformers are able to discover sample efficient algorithms when being trained to perform in-context</p>
<p>learning. This suggests an intriguing possibility where we might be able to reverse engineer the Transformer to obtain better learning algorithms for such problems.</p>
<h1>Acknowledgements</h1>
<p>We thank Niladri Chatterji, Micah Goldblum, Rohith Kuditipudi, Shibani Santurkar, Carmen Strassle, Mirac Sugzun, and Li-Yang Tan for helpful conversations, and anonymous reviewers for helpful comments.</p>
<p>SG was funded by a Stanford Interdisciplinary Graduate Fellowship. DT was funded by Open Philanthropy, and partially supported by NSF Award CCF-1813049. GV was supported by NSF Awards CCF-1704417, CCF-1813049, Frontier Award 1804222 and DOE award DE-SC0019205. We performed our experiments on the Stanford NLP cluster.</p>
<h2>References</h2>
<p>Maria-Florina Balcan. Data-driven algorithm design. In Tim Roughgarden, editor, Beyond Worst Case Analysis of Algorithms. Cambridge University Press, 2020.</p>
<p>Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and the classical bias-variance trade-off. Proceedings of the National Academy of Sciences, 2019.</p>
<p>Irwan Bello, Hieu Pham, Quoc V Le, Mohammad Norouzi, and Samy Bengio. Neural combinatorial optimization with reinforcement learning. arXiv preprint arXiv:1611.09940, 2016.</p>
<p>Samy Bengio, Yoshua Bengio, Jocelyn Cloutier, and Jan Gecsei. On the optimization of a synaptic learning rule. In Preprints Conf. Optimality in Artificial and Biological Neural Networks, 1995.</p>
<p>Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In International Conference on Machine Learning (ICML), pages 41-48, 2009.</p>
<p>Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. arXiv preprint arXiv:2009.11264, 2020a.</p>
<p>Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. arXiv preprint arXiv:2006.09286, 2020b.</p>
<p>Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022.</p>
<p>Guy Blanc, Jane Lange, Mingda Qiao, and Li-Yang Tan. Decision tree heuristics can fail, even in the smoothed setting. arXiv preprint arXiv:2107.00819, 2021.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Neural Information Processing Systems (NeurIPS), 2020.</p>
<p>Alon Brutzkus, Amit Daniely, and Eran Malach. Id3 learns juntas for smoothed product distributions. In Conference on Learning Theory, pages 902-915. PMLR, 2020.</p>
<p>Stephanie CY Chan, Adam Santoro, Andrew K Lampinen, Jane X Wang, Aaditya Singh, Pierre H Richemond, Jay McClelland, and Felix Hill. Data distributional properties drive emergent few-shot learning in transformers. arXiv preprint arXiv:2205.05055, 2022.</p>
<p>Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In conference on knowledge discovery and data mining (KDD), 2016.</p>
<p>Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language model in-context tuning. arXiv preprint arXiv:2110.07814, 2021.</p>
<p>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.</p>
<p>Tuan Dinh, Yuchen Zeng, Ruisu Zhang, Ziqian Lin, Shashank Rajput, Michael Gira, Jy-yong Sohn, Dimitris Papailiopoulos, and Kangwook Lee. Lift: Language-interfaced fine-tuning for non-language machine learning tasks. arXiv preprint arXiv:2206.06565, 2022.</p>
<p>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
<p>Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning (ICML), 2022.</p>
<p>Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. A mathematical framework for transformer circuits. Transformer Circuits Thread, 2021. https://transformercircuits.pub/2021/framework/index.html.</p>
<p>Jeffrey L Elman. Learning and development in neural networks: The importance of starting small. Cognition, 1993.</p>
<p>Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning (ICML), 2017.</p>
<p>Jerome H Friedman. Greedy function approximation: a gradient boosting machine. Annals of statistics, 2001.
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In International Conference on Machine Learning, pages 1704-1713. PMLR, 2018a.</p>
<p>Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b.</p>
<p>Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 2020.</p>
<p>Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In International conference on artificial neural networks (ICANN), 2001.</p>
<p>Eric Horvitz, Yongshao Ruan, Carla Gomes, Henry Kautz, Bart Selman, and Max Chickering. A bayesian approach to tackling hard computational problems (preliminary report). Electronic Notes in Discrete Mathematics, 2001.</p>
<p>Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. arXiv preprint arXiv:2004.05439, 2020.</p>
<p>Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. Neural Information Processing Systems (NeurIPS), 2017.</p>
<p>Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and Yee Whye Teh. Attentive neural processes. arXiv preprint arXiv:1901.05761, 2019.</p>
<p>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.</p>
<p>Louis Kirsch and Jürgen Schmidhuber. Meta learning backpropagation and improving it. Neural Information Processing Systems (NeurIPS), 2021.</p>
<p>Andrew K Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James L McClelland, Jane X Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022.</p>
<p>Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 2021.</p>
<p>Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804, 2021.</p>
<p>Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247, 2021a.</p>
<p>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021b.</p>
<p>Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Noisy channel language model prompting for few-shot text classification. arXiv preprint arXiv:2108.04106, 2021a.</p>
<p>Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. arXiv preprint arXiv:2110.15943, 2021b.</p>
<p>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022.</p>
<p>Nikhil Mishra, Mostafa Rohaninejad, Xi Chen, and Pieter Abbeel. A simple neural attentive meta-learner. In International Conference on Learning Representations (ICLR), 2018.</p>
<p>Swaroop Mishra, Daniel Khashabi, Chitta Baral, Yejin Choi, and Hannaneh Hajishirzi. Reframing instructional prompts to gptk's language. arXiv preprint arXiv:2109.07830, 2021.</p>
<p>Samuel Müller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. arXiv preprint arXiv:2112.10510, 2021.</p>
<p>Devang K Naik and Richard J Mammone. Meta-neural networks that learn by learning. In International Joint Conference on Neural Networks (IJCNN), 1992.</p>
<p>Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv preprint arXiv:1912.07242, 2019.</p>
<p>Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta learning via sequence modeling. arXiv preprint arXiv:2207.04179, 2022.</p>
<p>Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022. https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.</p>
<p>Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International Conference on Machine Learning (ICML), 2018.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 2011.</p>
<p>Jorge Pérez, Javier Marinković, and Pablo Barceló. On the turing completeness of modern neural network architectures. arXiv preprint arXiv:1901.03429, 2019.</p>
<p>Lovre Pesut. Who models the models that model models? an exploration of gpt-3's in-context model fitting ability, 2022. URL https://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/ who-models-the-models-that-model-models-an-exploration-of.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. OpenAI blog, 2018.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 2019.</p>
<p>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis \&amp; insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>
<p>Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. International Conference for Learning Representations (ICLR), 2017.</p>
<p>Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining term frequencies on few-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.</p>
<p>Frieda Rong. Extrapolating to unnatural language processing with gpt-3's in-context learning: The good, the bad, and the mysterious), 2021. URL http://ai.stanford.edu/blog/in-context-learning/.</p>
<p>Ohad Rubin, Jonathan Herzig, and Jonathan Berant. Learning to retrieve prompts for in-context learning. arXiv preprint arXiv:2112.08633, 2021.</p>
<p>Terence D Sanger. Neural network learning control of robot manipulators using gradually increasing task difficulty. IEEE transactions on Robotics and Automation, 1994.</p>
<p>Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In International conference on machine learning (ICML), 2016.</p>
<p>Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis, Technische Universität München, 1987.</p>
<p>Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum, and Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks. Neural Information Processing Systems (NeurIPS), 2021.</p>
<p>Daniel Selsam, Matthew Lamm, B Benedikt, Percy Liang, Leonardo de Moura, David L Dill, et al. Learning a sat solver from single-bit supervision. In International Conference on Learning Representations (ICLR), 2018.</p>
<p>Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. arXiv preprint arXiv:2103.07601, 2021.</p>
<p>Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Neural Information Processing Systems (NeurIPS), 2017.</p>
<p>Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science \&amp; Business Media, 2012.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological), 1996.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing Systems (NeurIPS), 2017.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Neural Information Processing Systems (NeurIPS), 2015.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. Thinking like transformers. In International Conference on Machine Learning, 2021.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics (ACL), 2020.</p>
<p>Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur. When do curricula work? arXiv preprint arXiv:2012.03107, 2020.</p>
<p>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations (ICLR), 2022.</p>
<p>Lin Xu, Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Satzilla: portfolio-based algorithm selection for sat. Journal of artificial intelligence research, 2008.</p>
<p>Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. arXiv preprint arXiv:2105.11115, 2021.</p>
<p>Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.</p>
<p>Yi Zhang, Arturs Backurs, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner. Unveiling transformers with lego: a synthetic reasoning task. arXiv preprint arXiv:2206.04301, 2022.</p>
<p>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning (ICML), 2021.</p>
<h1>A Experimental setup</h1>
<p>Here, we provide additional details on our experimental setup.</p>
<h2>A. 1 Model architecture</h2>
<p>We use architectures from the GPT-2 family [Radford et al., 2018] as implemented by HuggingFace [Wolf et al., 2020] ${ }^{3}$. Specifically, we consider the following set of configurations.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Embedding size</th>
<th style="text-align: center;">#Layers</th>
<th style="text-align: center;">#Heads</th>
<th style="text-align: center;">(Total parameters)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Tiny</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.2 M</td>
</tr>
<tr>
<td style="text-align: left;">Small</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1.2 M</td>
</tr>
<tr>
<td style="text-align: left;">Standard</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">9.5 M</td>
</tr>
</tbody>
</table>
<p>We use the Standard model for the bulk of our experiments and only consider the smaller models for the capacity explorations in Section 6 and Appendix B.3. Since we train on each input once (we sample new inputs at each training step), overfitting to the training data is not an issue. Therefore, we set the Dropout probability to 0 .</p>
<p>Out of the box, these models take as input a sequence of vectors in embedding space and output a sequence of vectors in the same space. However, the tasks we study are functions from a lower dimensional vector space (e.g., 10-50 dimensions) to a scalar value. Thus, in order to use a prompt such as $x_{1}, f\left(x_{1}\right), x_{2}, f\left(x_{2}\right) \ldots$, we need to map $x_{i}$ s and $f\left(x_{i}\right)$ s to vectors in embedding space. We do so by first turning the scalars $f\left(x_{i}\right)$ into vectors of the same dimension as $x_{i}$ by appending 0 s and then applying a learnable linear transformation to map all these vectors into the embedding space. Finally, we map the model output into a scalar value through a dot product with a learnable vector.</p>
<p>We treat the prediction of the model at the position corresponding to $x_{i}$ (that is absolute position $2 i-1$ ) as the prediction of $f\left(x_{i}\right)$. Due to the structure of these models, this prediction only depends on $\left(x_{j}, f\left(x_{j}\right)\right)$ for $j&lt;i$ and $x_{i}$. We ignore the model predictions at positions corresponding to $f\left(x_{i}\right)$.</p>
<h2>A. 2 Training</h2>
<p>Each training prompt is produced by sampling a random function $f$ from the function class we are training on, then sampling inputs $x_{i}$ from the isotropic Gaussian distribution $N\left(0, I_{d}\right)$ and constructing a prompt as $\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k}, f\left(x_{k}\right)\right)$. Given a prompt, we obtain model predictions $\hat{y}<em i="i">{i}$ (meant to approximate $f\left(x</em>\right)$ ) for each input, and compute the loss</p>
<p>$$
\frac{1}{k} \sum_{i=1}^{k}\left(\hat{y}<em i="i">{i}-f\left(x</em>
$$}\right)\right)^{2</p>
<p>At each training step, we average the loss over a batch of randomly generated prompts (with different functions and prompt inputs), and perform an update step. We use the Adam optimizer [Kingma and Ba, 2014], and train for 500,000 total steps with a batch size of 64 . We use a learning rate of $10^{-4}$ for all function classes and models.</p>
<p>Curriculum learning. To accelerate training, we start by training on prompt inputs $x_{i}$ lying in a smaller dimensional subspace, and with fewer inputs per prompt, and gradually increase the subspace dimension and number of prompt inputs. Specifically, we zero out all but the first $d_{\text {cur }}$ coordinates of $x_{i}$, sample prompts of size $k_{\text {cur }}$ and leave the rest of the training process the same. We use the same schedule for all training runs for the function classes of linear functions and sparse linear functions, starting with $d_{\text {cur }}=5, k_{\text {cur }}=11$, and increasing $d_{\text {cur }}$ and $k_{\text {cur }}$ by 1 and 2 respectively, every 2000 steps, until $d_{\text {cur }}=d, k_{\text {cur }}=2 d+1$. We use a slightly different schedule for 2 layer neural networks and decision trees as we want prompts with more</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>inputs for these function classes. For these classes, we start with $d_{\text {cur }}=5, k_{\text {cur }}=26$, and increase $d_{\text {cur }}$ and $k_{\text {cur }}$ by 1 and 5 respectively, every 2000 steps, until $d_{\text {cur }}=d, k_{\text {cur }}=5 d+1$.</p>
<p>Overall, with curriculum, a training prompt $\left(x_{1}, f\left(x_{1}\right), \ldots, x_{k_{\text {cur }}}, f\left(x_{k_{\text {cur }}}\right)\right.$ is generated by sampling a random function $f$ from the function class, drawing inputs $x_{i}$ by sampling i.i.d. from $N\left(0, I_{d}\right)$ and zeroing out all but the first $d_{\text {cur }}$ coordinates. Given model predictions $\hat{y}_{i}$, the loss is given by</p>
<p>$$
\frac{1}{k_{\text {cur }}} \sum_{i=1}^{k_{\text {cur }}}\left(\hat{y}<em i="i">{i}-f\left(x</em>
$$}\right)\right)^{2</p>
<p>Sampling random functions. For the class of linear functions, we sample random function $f(x)=w^{\top} x$ by drawing $w \sim N\left(0, I_{d}\right)$. For our main setting (Section 3 and 4), we set $d=20$.</p>
<p>For the class of two-layer neural networks, we sample $f(x)=\sum_{i=1}^{r} \alpha_{i} \sigma\left(w_{i}^{\top} x\right)$, where $\alpha_{i} \mathrm{~s}$ and $w_{i} \mathrm{~s}$ are drawn i.i.d. from $N(0,2 / r)$ and $N\left(0, I_{d}\right)$ respectively. We set $d=20$ and $r=100$.</p>
<p>For the class of $k$-sparse linear functions, we sample $f(x)=w^{\top} x$ by drawing $w \sim N\left(0, I_{d}\right)$ and zeroing out all but $k$ coordinates of $w$ chosen uniformly at random from the first $d_{\text {cur }}$ coordinates (as defined in the curriculum learning description above). We set $d=20$ and $k=3$.</p>
<p>For the class of decision trees, the random function $f$ is represented by a decision tree of depth 4 (with 16 leaf nodes), with 20 dimensional inputs. Each non-leaf node of the tree is associated with a coordinate selected uniformly at random from ${1,2, \ldots, d}$, and each leaf node is associated with a value drawn randomly from $N(0,1)$. To evaluate $f$ on an input $x$, we traverse the tree starting from the root node, and go to the right child if the coordinate associated with the current node is positive and go to the left child otherwise. $f(x)$ is given by the value associated with the leaf node reached at the end.</p>
<p>Computational resources. We train using a single NVIDIA GeForce RTX 3090 GPU and most training runs take 5-20 hours depending on model size and context length. For instance, for the class of linear functions, training the standard model takes 17 hours for $d=50,7$ hours for $d=20$ and 5.5 hours for $d=10$. For decision trees, training the standard model takes 17 hours. The time it takes for decision trees and 50 dimensional linear functions is higher due to larger context lengths (we train for $d$ dimensional linear functions with $2 d+1$ input-output pairs per prompt).</p>
<h1>A. 3 Baselines</h1>
<p>Least squares. Minimum norm least squares is the optimal estimator for the linear regression problem. Given a prompt $P=\left(x_{1}, y_{1}, \ldots, x_{k}, y_{k}, x_{\text {query }}\right)$, let $X$ be a $k \times d$ matrix with row $i$ given by $x_{i}$, and let $y$ be a $k$ dimensional vector with the $i^{\text {th }}$ entry $y_{i}$. Set $\hat{w}^{T}=X^{+} y$, where $X^{+}$denotes the Moore-Penrose pseudoinverse of $X$. The estimator predicts $M(P)=\hat{w}^{T} x_{\text {query }}$.</p>
<p>Averaging estimator. This corresponds to $M(P)=\hat{w}^{T} x_{\text {query }}$ where $\hat{w}=\frac{1}{k} \sum_{i=1}^{k} x_{i} y_{i}$. This estimator is consistent (yet sub-optimal) when $x_{i} \mathrm{~s}$ are drawn from $N\left(0, I_{d}\right)$. Unlike least squares, this estimator does not involve an inverse computation, and might be easier for a model to encode.</p>
<p>Nearest neighbors. This corresponds to setting $M(P)=\frac{1}{n} \sum_{i \in S} y_{i}$. Here, $S$ is the set of indices of the $n$ nearest neighbors of $x_{\text {query }}$ among $x_{1}$ to $x_{k}$. For $k&lt;n$, we average over all the $y_{i} \mathrm{~s}$ from 1 to $k$, and for $k=0$, we set $M(P)=0$. We consider the nearest neighbors baselines as it might be easier for a Transformer model to encode using self-attention compared to least squares.</p>
<p>Lasso. We use this baseline for sparse linear functions (Section 5). This corresponds to $M(P)=\hat{w}^{T} x_{\text {query }}$, where $\hat{w}$ minimizes the $\ell_{1}$-norm regularized least squares objective:</p>
<p>$$
\min <em 2="2">{\hat{w}} \frac{1}{2 k}|y-X \hat{w}|</em>
$$}^{2}+\alpha|\hat{w}|_{1</p>
<p>We try different values of $\alpha \in\left{1,10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\right}$, and report the best solution (achieving the smallest error with 10 in-context examples) corresponding to $\alpha=10^{-2}$. To solve the optimization problem, we use the Lasso implementation from Scikit-learn [Pedregosa et al., 2011] ${ }^{4}$.</p>
<p>Greedy Tree Learning. We use this baseline for the class of decision trees. This corresponds to greedily learning a decision tree using the in-context examples, and using it to classify the query input. To construct the tree, at each node (starting from a root node), we choose a coordinate for partitioning the examples into two sets, so as to minimize the variance of $y_{i} \mathrm{~s}$ in each set, averaged across the two sets. The value associated with a leaf node is the average $y_{i}$ value of the examples belonging to it. We use Scikit-learn's decision tree regressor [Pedregosa et al., 2011] ${ }^{5}$ implementation for this, with all the arguments set to their default value except the max_depth argument which is set to 2 . We considered values ${1,2,3,4,5,6$, unbounded $}$ for the maximum depth and chose the value that performs best at 100 in-context examples which was 2 (which differs from the decision trees sampled from the function class which have depth 4). We also considered a baseline where we learn this tree using only the signs of each $x_{i}$ coordinate-after all, the decision tree we are trying to learn depends only on the signs of $x_{i}$. In this case, we found the optimal depth to be 4.</p>
<p>Tree boosting. For the class of decision trees, we also consider a tree boosting baseline that corresponds to learning an ensemble of decision trees (see Friedman [2001] for a description of the general framework). Specifically, we use the XGBoost library [Chen and Guestrin, 2016] ${ }^{6}$, an implementation commonly used for a wide range of real-world machine learning tasks.</p>
<p>We performed a hyperpameter search by considering ${1,2,5,10,50,100,200,400}$ estimators in the ensemble (equivalent to number of boosting rounds), a learning rate of ${0.001,0.01,0.1,0.3,0.6,1,3}$, and a maximum depth of ${1,2,3,4,6,10,16}$. In general, we found the performance of the learning algorithm to be quite robust. We chose the hyperparameters obtaining the best performance with 100 training examples, corresponding to 50 estimators, a maximum depth of 4 , and a learning rate f 0.1 . We found these hyperparameters to also be optimal when learning based on the signs of each $x_{i}$.</p>
<p>Learning neural networks with gradient descent. We use this baseline for the class of two-layer neural networks (Section 5). This corresponds to training a two-layer neural network on the in-context examples, and outputting its prediction on the query point. That is, $M(P)=\hat{f}\left(x_{\text {query }}\right)$, where</p>
<p>$$
\hat{f}\left(x_{\text {query }}\right)=\sum_{i=1}^{r} \hat{\alpha}<em i="i">{i} \sigma\left(\hat{w}</em>\right)
$$}^{\top} x_{\text {query }</p>
<p>Here, $\sigma(\cdot)$ is the ReLU activation. We find parameters $\hat{\alpha}<em i="i">{i}, \hat{w}</em>$ by minimizing the squared error of the prediction for the in-context examples</p>
<p>$$
\sum_{i=1}^{k}\left(\hat{f}\left(x_{i}\right)-f\left(x_{i}\right)\right)^{2}
$$</p>
<p>using the Adam optimizer. We use a batch size of 10 (we use full batch when the number of in-context examples is less than 10) with 5000 optimization steps, and set $r=100$. We use a learning rate of $5 \cdot 10^{-3}$ in the case when the data is generated using a neural network, and a learning rate of $5 \cdot 10^{-2}$ when the data is generated using a linear function. We consider the setting with 100 in-context examples, and do a hyperparameter grid search over learning rate $\in\left{5 \cdot 10^{-4}, 5 \cdot 10^{-3}, 5 \cdot 10^{-2}, 5 \cdot 10^{-1}, 5\right}, r \in{100,400}$, batch size $\in{10,100}$, optimization algorithm $\in{$ adam, sgd $}$. All the hyperparameter settings in this grid led to a similar or worse performance compared to the hyperparameter setting we choose.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html
${ }^{5}$ https://scikit-learn.org/stable/modules/tree.html#regression
${ }^{6}$ https://github.com/dmlc/xgboost&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
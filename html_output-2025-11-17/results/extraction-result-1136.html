<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1136 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1136</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1136</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-271768878</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.04570v1.pdf" target="_blank">Mathematical Programming For Adaptive Experiments</a></p>
                <p><strong>Paper Abstract:</strong> Adaptive experimentation can significantly improve statistical power, but standard algorithms overlook important practical issues including batched and delayed feedback, personalization, non-stationarity, multiple objectives, and constraints. To address these issues, the current algorithm design paradigm crafts tailored methods for each problem instance. Since it is infeasible to devise novel algorithms for every real-world instance, practitioners often have to resort to suboptimal approximations that do not address all of their challenges. Moving away from developing bespoke algorithms for each setting, we present a mathematical programming view of adaptive experimentation that can flexibly incorporate a wide range of objectives, constraints, and statistical procedures. By formulating a dynamic program in the batched limit, our modeling framework enables the use of scalable optimization methods (e.g., SGD and auto-differentiation) to solve for treatment allocations. We evaluate our framework on benchmarks modeled after practical challenges such as non-stationarity, personalization, multi-objectives, and constraints. Unlike bespoke algorithms such as modified variants of Thomson sampling, our mathematical programming approach provides remarkably robust performance across instances.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1136.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1136.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RHO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Residual Horizon Optimization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A model-predictive-control / mathematical-programming algorithm that solves a batched-limit Bayesian MDP by planning over remaining horizon using Gaussian posterior states and optimizing static sequences of batch allocations via SGD and automatic differentiation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Residual Horizon Optimization (RHO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>An MPC-style adaptive experimental design algorithm built on the Batch Limit Dynamic Program (BLDP). It represents uncertainty by Gaussian posterior parameters (β_t, Σ_t) derived from CLT approximations of batch M-estimators, simulates future posterior trajectories under candidate allocation sequences, and optimizes allocations via pathwise stochastic gradients (auto-diff + SGD/Adam). Key components: CLT-based sufficient-statistic compression, Gaussian conjugate posterior update, open-loop planning over remaining horizon, auto-differentiation-based optimization of allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Model-predictive control (MPC) over a Bayesian batched-limit dynamic program; mathematical programming over posterior states (BLDP).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>At each batch t it (1) observes the current posterior (β_t, Σ_t), (2) samples rollouts of the BLDP under candidate static allocation sequences ρ_{t:T-1}, (3) optimizes the sequence (ρ_t,...,ρ_{T-1}) with SGD to minimize a user-specified Bayes objective (e.g., weighted cumulative + simple regret) subject to constraints, (4) deploys the first allocation ρ*_t, and (5) updates posterior using the Gaussian approximation; repeats each batch.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ASOS non-stationary benchmarks (real-world retail experiments) and Synthetic Personalized Ranking environment</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown reward distributions (unit-level ν(·|x,a) unknown) compressed to batch-level sufficient statistics; stochastic rewards; predictable non-stationarity (context distributions µ_t known or sampled from historical data); high noise / low signal-to-noise ratio; batched feedback and delayed updates; contextual (personalization) and non-contextual variants.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>ASOS: 241 benchmark settings derived from 78 real experiments, typically 10 epochs (T=10) and K up to 10 arms (synthetic arms added), batch sizes n_t in {10,000, 100,000, 250,000}; Personalized ranking: T small (e.g., 5 epochs in Pareto experiments), per-epoch n_t up to hundreds, action space K=10 pre-defined rankers selecting b items from |Z|=10 (combinatorial ranking decisions), context and content features in R^d; continuous-valued context and reward spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>ASOS: RHO outperforms TS-based baselines and Uniform in aggregate; RHO outperforms Uniform on 60.5% of ASOS non-stationary settings (445/732 settings reported across experimental variants) and shows higher average reward and higher best-arm identification rate for batch size 100,000; evaluated over 241 settings × 750 simulations = 180,750 simulated experiments. Personalized ranking: RHO often strictly dominates Top-Two TS on the Pareto frontier trading simple vs cumulative regret; RHO attains best weighted objective in 9/13 combinations tested. Robust to measurement noise distributions (Gaussian, Student's t, Gumbel).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Uniform baseline: RHO typically improves over Uniform (see 60.5% of settings for ASOS). TS-based policies sometimes perform worse than Uniform in ASOS (TS/TTTS underperform Uniform on a substantial fraction of settings; e.g., TS-based policies when they underperform do so with heavier tails: TS underperforms Uniform 10.7% on average vs 6.9% for RHO). Exact numeric baseline regrets vary by batch size and setting (reported raw-regret differences in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Designed for large-batch regimes; validated with batch sizes 10k–250k and T=5–10 epochs. Empirically effective with these large batch sizes (CLT approximation). The paper reports consistent gains across these sample budgets; no small-batch (few-hundred) guarantees are claimed—performance depends on batch sizes sufficient for Gaussian approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Implements explicit planning that calibrates exploration to remaining horizon and signal-to-noise ratio by optimizing a Bayes objective that can weight cumulative vs final (simple) regret according to numbers of individuals (weights = number under each objective). Exploration emerges from lookahead planning; early epochs typically explore broadly, later epochs concentrate on promising arms. The tradeoff is user-specified via objective weights (or implicitly via n_T vs Σ n_t).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Uniform random allocation; Thompson Sampling (TS); Top-Two Thompson Sampling (TTTS) including a contextual/Deconfounded variant.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>RHO (the BLDP + MPC/SGD optimizer) is robust across practical challenges (predictable non-stationarity, personalization, multi-objectives, constraints) and consistently outperforms or matches baselines where they fail (notably TS/TTTS). Theoretical guarantee: RHO achieves provably no worse Bayesian regret than the best static (A/B) design, and empirical results show RHO dominates many TTTS variants on simple-vs-cumulative regret Pareto fronts and is robust to model misspecification and heavy-tailed/noisy measurements.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires large batches (CLT validity) and ability to estimate Hessian H_t and gradient covariance I_t; theoretical performance guarantee requires predictable non-stationarity (known µ_t) for the static-design benchmark guarantee; may require tuning of an optimization learning rate (though empirically robust); computational cost of planning via SGD and simulating rollouts (but mitigated via auto-diff); when the primary objective is pure cumulative regret over very long horizons, greedy/TS policies can sometimes have lower cumulative regret (paper notes TS/TTTS often have lower cumulative regret but worse simple regret).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Programming For Adaptive Experiments', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1136.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1136.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RHO-Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Residual Horizon Optimization — Personalized Ranking environment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same RHO algorithm applied to a synthetic personalized ranking environment where actions are discrete rankers and rewards are aggregated over top-b recommended items; demonstrates ability to handle combinatorial, contextual settings and multi-objective tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Residual Horizon Optimization (RHO)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>See RHO entry; here applied to ranking: uses Gaussian posterior over parameter θ*, simulates posterior evolution under allocations of ranker choices per user cohort, and optimizes allocations to trade off cumulative and final deployment reward.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>BLDP-based MPC planning with SGD (same as RHO general), here optimizing ranking-weight allocations for personalized users.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Optimizes allocations of rankers per user context by simulating posterior rollouts of user-content reward model and optimizing explicit weighted objective (cumulative + simple regret) reflecting number of users in experiment vs post-deployment population; re-solves after each batch.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Synthetic Personalized Value Model for Ranking</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Contextual, stochastic, partially observable (true θ* unknown), combinatorial action outcome (top-b ranking induces non-linear reward of selecting ranker w), continuous user and content features, batched updates, Gaussian or heavy-tailed measurement noise in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Example experiments: 5 epochs × 100 users per epoch (in tradeoff experiments), K=10 rankers, recommend b=4 items out of |Z|=10, feature dimension d (varies); combinatorial action effect (ranking of items) increases complexity beyond K-armed bandits.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>RHO consistently dominates TS/TTTS on the Pareto frontier for simple vs cumulative regret in the ranking setting; in weighted-objective tests RHO achieved the best value in 9/13 combinations of (total experimental users) vs (post-deployment users). RHO outperforms baselines across noise distributions (Gaussian, Student's t, Gumbel).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Uniform baseline is generally outperformed by RHO in ranking experiments; TS/TTTS sometimes have lower cumulative regret but produce worse simple regret (thereby failing the final-deployment objective).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Shown effective for batched regimes (e.g., batches of 100 users over 5 epochs) where RHO can find good final ranker choices with fewer cumulative losses compared to baselines for the same samples.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Explicit weighted objective uses relative sizes of experimental population and deployment population to set exploration weight; planning naturally increases exploration when the final deployment weight is large (n_T >> sum n_t), and focuses exploitation otherwise.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Uniform, Thompson Sampling (TS), Top-Two Thompson Sampling (TTTS).</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>RHO provides a principled and interpretable mechanism to trade off simple vs cumulative regret in ranking, often strictly dominating TTTS variants; RHO remains robust to heavy-tailed noise and model misspecification in this combinatorial contextual setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires modeling the ranking reward via parametric f_Z(x,z;θ) and estimating θ (M-estimation) with sufficient batch size; computational complexity grows with simulating ranking rollouts (top-b selection) but remains tractable in experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Programming For Adaptive Experiments', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1136.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1136.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Thompson Sampling (batched / contextual variants)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian sampling-based bandit algorithm that samples parameters from the posterior and acts greedily w.r.t. the sampled parameter; adapted here to batched settings by repeating sample-based decisions inside each batch.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Thompson Sampling (TS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Standard TS implemented with Gaussian posterior beliefs over θ; at each decision (or for each unit in a batch) sample θ_t ~ N(β_t, Σ_t) and choose action maximizing f(x,a;θ_t). Implemented in both non-contextual and contextual (Deconfounded) variants for batched experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior-sampling (Thompson Sampling) / randomized policy based on posterior draw.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Maintains Gaussian posterior over parameters, samples parameter draws each decision and picks argmax action for that draw; updates posterior between batches using observed batch data (Gaussian approximations).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ASOS non-stationary benchmarks and Synthetic Personalized Ranking / single-item recommendation settings</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as RHO environments: stochastic, partially observable (unknown reward generative ν), predictable non-stationarity when contextualized by time, batched feedback, possibly heavy-tailed noise in some experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same experimental sizes as RHO: ASOS settings with T up to 10, K up to 10, batch sizes 10k–250k; ranking experiments with K=10 rankers and combinatorial top-b reward aggregation; single-item sequential variant with batches of 100 users per epoch.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Mixed: TS performs well in classic sequential single-item recommendation experiments (Figure 6a) and can outperform Uniform in those settings, but in many large-batch, low-SNR, non-stationary ASOS settings TS-based policies often underperform Uniform (TS-based policies can be fragile under severe non-stationarity and low SNR). TS sometimes achieves lower cumulative regret but worse simple regret relative to RHO.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Compared to Uniform, TS is sometimes better (especially in traditional sequential item-by-item settings) but often worse in ASOS non-stationary benchmarks; when TS underperforms Uniform it does so with heavy-tailed losses (~10.7% average underperformance rate as reported).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Sample-efficiency depends strongly on problem type: efficient in classic linear contextual bandits with many sequential updates (small batches / per-unit updates); less sample-efficient in large-batch, low SNR non-stationary regimes where CLT-batched effects and predictable non-stationarity harm its adaptivity.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Intrinsic randomized exploration via sampling from posterior; no explicit horizon-aware planning—degree of exploration is controlled implicitly by posterior uncertainty and by any parameterization of prior/posterior (and by TTTS modifications).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against RHO and Uniform and TTTS in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>TS is competitive in settings it was designed for (sequential, higher SNR, stationary), but in realistic batched, low-SNR, non-stationary experiments TS can be fragile and underperform simple uniform allocations; TS often attains lower cumulative regret but worse final simple regret compared to RHO.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Fragile in batched, low-signal, and non-stationary settings; lacks principled mechanism to incorporate general constraints or multiple objectives; not horizon-aware (no explicit use of remaining horizon and expected future information beyond posterior variance).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Programming For Adaptive Experiments', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1136.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1136.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TTTS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Top-Two Thompson Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An exploration-enhanced variant of Thompson Sampling that, with probability β, resamples until a different top action is obtained to encourage exploration outside the current best action; tailored for simple-regret minimization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Top-Two Thompson Sampling (TTTS)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A variant of TS: for each unit, pick the usual TS-chosen action A; with probability β retain A, otherwise repeatedly resample posterior θ until the sampled best action differs from A and select that alternative—promotes exploration of non-greedy arms; implemented in contextual and deconfounded variants for batched settings.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Posterior-sampling with top-two resampling heuristic (designed for simple-regret).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>Uses posterior uncertainty to sample alternatives; β parameter controls exploration: β near 1 behaves like TS (more greedy), smaller β yields more exploration via resampling until a non-identical best action is found.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ASOS non-stationary benchmarks and Personalized Ranking / single-item experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as above: stochastic, batched feedback, predictable non-stationarity modeled as context/time, low SNR in ASOS, contextual personalization in ranking environment.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>ASOS: up to 241 settings, T up to 10 epochs, batch sizes 10k–250k; Ranking: K=10 rankers, top-b combinatorial reward.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>TTTS variants can outperform TS in some instances for simple regret, but in ASOS experiments many TTTS parameterizations do not outperform Uniform; contextual TTTS (Deconfounded TS) improves robustness but still underperforms RHO on many benchmarks. Paper reports Contextual TTTS outperforms Uniform on 51.8% of settings versus RHO's 60.5%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Uniform baseline often competitive in ASOS benchmarks; TTTS sometimes worse than Uniform depending on β choice and setting—most β choices lack a principled selection method and many do not outperform Uniform in simple regret.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Sensitivity to β hyperparameter; can require tuning per-instance to be effective; not as robust across varied realistic instances as RHO.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Hand-controlled via β parameter—lower β increases exploration by forcing resampling until an alternate best is found. No explicit horizon-aware planning or multi-objective weighting.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared to RHO, TS, Uniform in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>TTTS can provide improved simple-regret behavior in classical settings but is brittle under realistic batched non-stationary conditions; lacks principled parameter selection leading to inconsistent performance across many practical instances.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires per-instance tuning of β for good performance; many parameter settings fail to beat uniform allocation in ASOS benchmarks; not horizon-aware and not straightforward to incorporate general constraints or multi-objective formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Programming For Adaptive Experiments', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1136.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1136.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Uniform</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Uniform Random Allocation (non-adaptive baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A non-adaptive A/B testing baseline that assigns equal probability to each arm (or fixed pre-specified allocation) across batches; used as standard comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Uniform allocation</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Non-adaptive policy that allocates sampling units uniformly across K arms (or a fixed static allocation across batches) without updating based on observed data.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>none (non-adaptive baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>None — allocation fixed in advance and does not change.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ASOS non-stationary benchmarks and synthetic personalization experiments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Same as other entries (stochastic, partially observable rewards, non-stationary contexts).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Same experimental sizes as in paper; used as baseline across the same batch sizes and epochs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Reported as the de-facto baseline; many adaptive methods (TS, TTTS) sometimes fail to improve over Uniform in ASOS non-stationary, low SNR settings; Uniform is surprisingly strong and is the reference for percent-improvement reporting (e.g., RHO improves over Uniform on 60.5% of ASOS settings).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>N/A (no adaptation).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Used as baseline against RHO, TS, TTTS.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Uniform often matches or outperforms fragile adaptive methods in realistic batched, non-stationary, low-SNR experiments, motivating the need for robust adaptive designs like RHO that guarantee performance at least as good as best static designs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Does not exploit adaptivity; misses potential gains achievable by principled adaptive designs in settings where reliable adaptation is possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Mathematical Programming For Adaptive Experiments', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Adaptive experimentation at scale: A computational framework for flexible batches. <em>(Rating: 2)</em></li>
                <li>Thompson sampling for contextual bandits with linear payoffs. <em>(Rating: 2)</em></li>
                <li>Top-k selection based on adaptive sampling of noisy preferences. <em>(Rating: 1)</em></li>
                <li>Batched multi-armed bandits problem. <em>(Rating: 1)</em></li>
                <li>An empirical evaluation of thompson sampling. <em>(Rating: 1)</em></li>
                <li>Simple bayesian algorithms for best-arm identification. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1136",
    "paper_id": "paper-271768878",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "RHO",
            "name_full": "Residual Horizon Optimization",
            "brief_description": "A model-predictive-control / mathematical-programming algorithm that solves a batched-limit Bayesian MDP by planning over remaining horizon using Gaussian posterior states and optimizing static sequences of batch allocations via SGD and automatic differentiation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Residual Horizon Optimization (RHO)",
            "agent_description": "An MPC-style adaptive experimental design algorithm built on the Batch Limit Dynamic Program (BLDP). It represents uncertainty by Gaussian posterior parameters (β_t, Σ_t) derived from CLT approximations of batch M-estimators, simulates future posterior trajectories under candidate allocation sequences, and optimizes allocations via pathwise stochastic gradients (auto-diff + SGD/Adam). Key components: CLT-based sufficient-statistic compression, Gaussian conjugate posterior update, open-loop planning over remaining horizon, auto-differentiation-based optimization of allocations.",
            "adaptive_design_method": "Model-predictive control (MPC) over a Bayesian batched-limit dynamic program; mathematical programming over posterior states (BLDP).",
            "adaptation_strategy_description": "At each batch t it (1) observes the current posterior (β_t, Σ_t), (2) samples rollouts of the BLDP under candidate static allocation sequences ρ_{t:T-1}, (3) optimizes the sequence (ρ_t,...,ρ_{T-1}) with SGD to minimize a user-specified Bayes objective (e.g., weighted cumulative + simple regret) subject to constraints, (4) deploys the first allocation ρ*_t, and (5) updates posterior using the Gaussian approximation; repeats each batch.",
            "environment_name": "ASOS non-stationary benchmarks (real-world retail experiments) and Synthetic Personalized Ranking environment",
            "environment_characteristics": "Unknown reward distributions (unit-level ν(·|x,a) unknown) compressed to batch-level sufficient statistics; stochastic rewards; predictable non-stationarity (context distributions µ_t known or sampled from historical data); high noise / low signal-to-noise ratio; batched feedback and delayed updates; contextual (personalization) and non-contextual variants.",
            "environment_complexity": "ASOS: 241 benchmark settings derived from 78 real experiments, typically 10 epochs (T=10) and K up to 10 arms (synthetic arms added), batch sizes n_t in {10,000, 100,000, 250,000}; Personalized ranking: T small (e.g., 5 epochs in Pareto experiments), per-epoch n_t up to hundreds, action space K=10 pre-defined rankers selecting b items from |Z|=10 (combinatorial ranking decisions), context and content features in R^d; continuous-valued context and reward spaces.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "ASOS: RHO outperforms TS-based baselines and Uniform in aggregate; RHO outperforms Uniform on 60.5% of ASOS non-stationary settings (445/732 settings reported across experimental variants) and shows higher average reward and higher best-arm identification rate for batch size 100,000; evaluated over 241 settings × 750 simulations = 180,750 simulated experiments. Personalized ranking: RHO often strictly dominates Top-Two TS on the Pareto frontier trading simple vs cumulative regret; RHO attains best weighted objective in 9/13 combinations tested. Robust to measurement noise distributions (Gaussian, Student's t, Gumbel).",
            "performance_without_adaptation": "Uniform baseline: RHO typically improves over Uniform (see 60.5% of settings for ASOS). TS-based policies sometimes perform worse than Uniform in ASOS (TS/TTTS underperform Uniform on a substantial fraction of settings; e.g., TS-based policies when they underperform do so with heavier tails: TS underperforms Uniform 10.7% on average vs 6.9% for RHO). Exact numeric baseline regrets vary by batch size and setting (reported raw-regret differences in paper tables).",
            "sample_efficiency": "Designed for large-batch regimes; validated with batch sizes 10k–250k and T=5–10 epochs. Empirically effective with these large batch sizes (CLT approximation). The paper reports consistent gains across these sample budgets; no small-batch (few-hundred) guarantees are claimed—performance depends on batch sizes sufficient for Gaussian approximation.",
            "exploration_exploitation_tradeoff": "Implements explicit planning that calibrates exploration to remaining horizon and signal-to-noise ratio by optimizing a Bayes objective that can weight cumulative vs final (simple) regret according to numbers of individuals (weights = number under each objective). Exploration emerges from lookahead planning; early epochs typically explore broadly, later epochs concentrate on promising arms. The tradeoff is user-specified via objective weights (or implicitly via n_T vs Σ n_t).",
            "comparison_methods": "Uniform random allocation; Thompson Sampling (TS); Top-Two Thompson Sampling (TTTS) including a contextual/Deconfounded variant.",
            "key_results": "RHO (the BLDP + MPC/SGD optimizer) is robust across practical challenges (predictable non-stationarity, personalization, multi-objectives, constraints) and consistently outperforms or matches baselines where they fail (notably TS/TTTS). Theoretical guarantee: RHO achieves provably no worse Bayesian regret than the best static (A/B) design, and empirical results show RHO dominates many TTTS variants on simple-vs-cumulative regret Pareto fronts and is robust to model misspecification and heavy-tailed/noisy measurements.",
            "limitations_or_failures": "Requires large batches (CLT validity) and ability to estimate Hessian H_t and gradient covariance I_t; theoretical performance guarantee requires predictable non-stationarity (known µ_t) for the static-design benchmark guarantee; may require tuning of an optimization learning rate (though empirically robust); computational cost of planning via SGD and simulating rollouts (but mitigated via auto-diff); when the primary objective is pure cumulative regret over very long horizons, greedy/TS policies can sometimes have lower cumulative regret (paper notes TS/TTTS often have lower cumulative regret but worse simple regret).",
            "uuid": "e1136.0",
            "source_info": {
                "paper_title": "Mathematical Programming For Adaptive Experiments",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "RHO-Ranking",
            "name_full": "Residual Horizon Optimization — Personalized Ranking environment",
            "brief_description": "Same RHO algorithm applied to a synthetic personalized ranking environment where actions are discrete rankers and rewards are aggregated over top-b recommended items; demonstrates ability to handle combinatorial, contextual settings and multi-objective tradeoffs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Residual Horizon Optimization (RHO)",
            "agent_description": "See RHO entry; here applied to ranking: uses Gaussian posterior over parameter θ*, simulates posterior evolution under allocations of ranker choices per user cohort, and optimizes allocations to trade off cumulative and final deployment reward.",
            "adaptive_design_method": "BLDP-based MPC planning with SGD (same as RHO general), here optimizing ranking-weight allocations for personalized users.",
            "adaptation_strategy_description": "Optimizes allocations of rankers per user context by simulating posterior rollouts of user-content reward model and optimizing explicit weighted objective (cumulative + simple regret) reflecting number of users in experiment vs post-deployment population; re-solves after each batch.",
            "environment_name": "Synthetic Personalized Value Model for Ranking",
            "environment_characteristics": "Contextual, stochastic, partially observable (true θ* unknown), combinatorial action outcome (top-b ranking induces non-linear reward of selecting ranker w), continuous user and content features, batched updates, Gaussian or heavy-tailed measurement noise in experiments.",
            "environment_complexity": "Example experiments: 5 epochs × 100 users per epoch (in tradeoff experiments), K=10 rankers, recommend b=4 items out of |Z|=10, feature dimension d (varies); combinatorial action effect (ranking of items) increases complexity beyond K-armed bandits.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "RHO consistently dominates TS/TTTS on the Pareto frontier for simple vs cumulative regret in the ranking setting; in weighted-objective tests RHO achieved the best value in 9/13 combinations of (total experimental users) vs (post-deployment users). RHO outperforms baselines across noise distributions (Gaussian, Student's t, Gumbel).",
            "performance_without_adaptation": "Uniform baseline is generally outperformed by RHO in ranking experiments; TS/TTTS sometimes have lower cumulative regret but produce worse simple regret (thereby failing the final-deployment objective).",
            "sample_efficiency": "Shown effective for batched regimes (e.g., batches of 100 users over 5 epochs) where RHO can find good final ranker choices with fewer cumulative losses compared to baselines for the same samples.",
            "exploration_exploitation_tradeoff": "Explicit weighted objective uses relative sizes of experimental population and deployment population to set exploration weight; planning naturally increases exploration when the final deployment weight is large (n_T &gt;&gt; sum n_t), and focuses exploitation otherwise.",
            "comparison_methods": "Uniform, Thompson Sampling (TS), Top-Two Thompson Sampling (TTTS).",
            "key_results": "RHO provides a principled and interpretable mechanism to trade off simple vs cumulative regret in ranking, often strictly dominating TTTS variants; RHO remains robust to heavy-tailed noise and model misspecification in this combinatorial contextual setting.",
            "limitations_or_failures": "Requires modeling the ranking reward via parametric f_Z(x,z;θ) and estimating θ (M-estimation) with sufficient batch size; computational complexity grows with simulating ranking rollouts (top-b selection) but remains tractable in experiments reported.",
            "uuid": "e1136.1",
            "source_info": {
                "paper_title": "Mathematical Programming For Adaptive Experiments",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "TS",
            "name_full": "Thompson Sampling (batched / contextual variants)",
            "brief_description": "A Bayesian sampling-based bandit algorithm that samples parameters from the posterior and acts greedily w.r.t. the sampled parameter; adapted here to batched settings by repeating sample-based decisions inside each batch.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Thompson Sampling (TS)",
            "agent_description": "Standard TS implemented with Gaussian posterior beliefs over θ; at each decision (or for each unit in a batch) sample θ_t ~ N(β_t, Σ_t) and choose action maximizing f(x,a;θ_t). Implemented in both non-contextual and contextual (Deconfounded) variants for batched experiments.",
            "adaptive_design_method": "Posterior-sampling (Thompson Sampling) / randomized policy based on posterior draw.",
            "adaptation_strategy_description": "Maintains Gaussian posterior over parameters, samples parameter draws each decision and picks argmax action for that draw; updates posterior between batches using observed batch data (Gaussian approximations).",
            "environment_name": "ASOS non-stationary benchmarks and Synthetic Personalized Ranking / single-item recommendation settings",
            "environment_characteristics": "Same as RHO environments: stochastic, partially observable (unknown reward generative ν), predictable non-stationarity when contextualized by time, batched feedback, possibly heavy-tailed noise in some experiments.",
            "environment_complexity": "Same experimental sizes as RHO: ASOS settings with T up to 10, K up to 10, batch sizes 10k–250k; ranking experiments with K=10 rankers and combinatorial top-b reward aggregation; single-item sequential variant with batches of 100 users per epoch.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Mixed: TS performs well in classic sequential single-item recommendation experiments (Figure 6a) and can outperform Uniform in those settings, but in many large-batch, low-SNR, non-stationary ASOS settings TS-based policies often underperform Uniform (TS-based policies can be fragile under severe non-stationarity and low SNR). TS sometimes achieves lower cumulative regret but worse simple regret relative to RHO.",
            "performance_without_adaptation": "Compared to Uniform, TS is sometimes better (especially in traditional sequential item-by-item settings) but often worse in ASOS non-stationary benchmarks; when TS underperforms Uniform it does so with heavy-tailed losses (~10.7% average underperformance rate as reported).",
            "sample_efficiency": "Sample-efficiency depends strongly on problem type: efficient in classic linear contextual bandits with many sequential updates (small batches / per-unit updates); less sample-efficient in large-batch, low SNR non-stationary regimes where CLT-batched effects and predictable non-stationarity harm its adaptivity.",
            "exploration_exploitation_tradeoff": "Intrinsic randomized exploration via sampling from posterior; no explicit horizon-aware planning—degree of exploration is controlled implicitly by posterior uncertainty and by any parameterization of prior/posterior (and by TTTS modifications).",
            "comparison_methods": "Compared against RHO and Uniform and TTTS in experiments.",
            "key_results": "TS is competitive in settings it was designed for (sequential, higher SNR, stationary), but in realistic batched, low-SNR, non-stationary experiments TS can be fragile and underperform simple uniform allocations; TS often attains lower cumulative regret but worse final simple regret compared to RHO.",
            "limitations_or_failures": "Fragile in batched, low-signal, and non-stationary settings; lacks principled mechanism to incorporate general constraints or multiple objectives; not horizon-aware (no explicit use of remaining horizon and expected future information beyond posterior variance).",
            "uuid": "e1136.2",
            "source_info": {
                "paper_title": "Mathematical Programming For Adaptive Experiments",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "TTTS",
            "name_full": "Top-Two Thompson Sampling",
            "brief_description": "An exploration-enhanced variant of Thompson Sampling that, with probability β, resamples until a different top action is obtained to encourage exploration outside the current best action; tailored for simple-regret minimization.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Top-Two Thompson Sampling (TTTS)",
            "agent_description": "A variant of TS: for each unit, pick the usual TS-chosen action A; with probability β retain A, otherwise repeatedly resample posterior θ until the sampled best action differs from A and select that alternative—promotes exploration of non-greedy arms; implemented in contextual and deconfounded variants for batched settings.",
            "adaptive_design_method": "Posterior-sampling with top-two resampling heuristic (designed for simple-regret).",
            "adaptation_strategy_description": "Uses posterior uncertainty to sample alternatives; β parameter controls exploration: β near 1 behaves like TS (more greedy), smaller β yields more exploration via resampling until a non-identical best action is found.",
            "environment_name": "ASOS non-stationary benchmarks and Personalized Ranking / single-item experiments",
            "environment_characteristics": "Same as above: stochastic, batched feedback, predictable non-stationarity modeled as context/time, low SNR in ASOS, contextual personalization in ranking environment.",
            "environment_complexity": "ASOS: up to 241 settings, T up to 10 epochs, batch sizes 10k–250k; Ranking: K=10 rankers, top-b combinatorial reward.",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "TTTS variants can outperform TS in some instances for simple regret, but in ASOS experiments many TTTS parameterizations do not outperform Uniform; contextual TTTS (Deconfounded TS) improves robustness but still underperforms RHO on many benchmarks. Paper reports Contextual TTTS outperforms Uniform on 51.8% of settings versus RHO's 60.5%.",
            "performance_without_adaptation": "Uniform baseline often competitive in ASOS benchmarks; TTTS sometimes worse than Uniform depending on β choice and setting—most β choices lack a principled selection method and many do not outperform Uniform in simple regret.",
            "sample_efficiency": "Sensitivity to β hyperparameter; can require tuning per-instance to be effective; not as robust across varied realistic instances as RHO.",
            "exploration_exploitation_tradeoff": "Hand-controlled via β parameter—lower β increases exploration by forcing resampling until an alternate best is found. No explicit horizon-aware planning or multi-objective weighting.",
            "comparison_methods": "Compared to RHO, TS, Uniform in experiments.",
            "key_results": "TTTS can provide improved simple-regret behavior in classical settings but is brittle under realistic batched non-stationary conditions; lacks principled parameter selection leading to inconsistent performance across many practical instances.",
            "limitations_or_failures": "Requires per-instance tuning of β for good performance; many parameter settings fail to beat uniform allocation in ASOS benchmarks; not horizon-aware and not straightforward to incorporate general constraints or multi-objective formulations.",
            "uuid": "e1136.3",
            "source_info": {
                "paper_title": "Mathematical Programming For Adaptive Experiments",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Uniform",
            "name_full": "Uniform Random Allocation (non-adaptive baseline)",
            "brief_description": "A non-adaptive A/B testing baseline that assigns equal probability to each arm (or fixed pre-specified allocation) across batches; used as standard comparator.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "Uniform allocation",
            "agent_description": "Non-adaptive policy that allocates sampling units uniformly across K arms (or a fixed static allocation across batches) without updating based on observed data.",
            "adaptive_design_method": "none (non-adaptive baseline)",
            "adaptation_strategy_description": "None — allocation fixed in advance and does not change.",
            "environment_name": "ASOS non-stationary benchmarks and synthetic personalization experiments",
            "environment_characteristics": "Same as other entries (stochastic, partially observable rewards, non-stationary contexts).",
            "environment_complexity": "Same experimental sizes as in paper; used as baseline across the same batch sizes and epochs.",
            "uses_adaptive_design": false,
            "performance_with_adaptation": null,
            "performance_without_adaptation": "Reported as the de-facto baseline; many adaptive methods (TS, TTTS) sometimes fail to improve over Uniform in ASOS non-stationary, low SNR settings; Uniform is surprisingly strong and is the reference for percent-improvement reporting (e.g., RHO improves over Uniform on 60.5% of ASOS settings).",
            "sample_efficiency": "N/A (no adaptation).",
            "exploration_exploitation_tradeoff": "N/A.",
            "comparison_methods": "Used as baseline against RHO, TS, TTTS.",
            "key_results": "Uniform often matches or outperforms fragile adaptive methods in realistic batched, non-stationary, low-SNR experiments, motivating the need for robust adaptive designs like RHO that guarantee performance at least as good as best static designs.",
            "limitations_or_failures": "Does not exploit adaptivity; misses potential gains achievable by principled adaptive designs in settings where reliable adaptation is possible.",
            "uuid": "e1136.4",
            "source_info": {
                "paper_title": "Mathematical Programming For Adaptive Experiments",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Adaptive experimentation at scale: A computational framework for flexible batches.",
            "rating": 2,
            "sanitized_title": "adaptive_experimentation_at_scale_a_computational_framework_for_flexible_batches"
        },
        {
            "paper_title": "Thompson sampling for contextual bandits with linear payoffs.",
            "rating": 2,
            "sanitized_title": "thompson_sampling_for_contextual_bandits_with_linear_payoffs"
        },
        {
            "paper_title": "Top-k selection based on adaptive sampling of noisy preferences.",
            "rating": 1,
            "sanitized_title": "topk_selection_based_on_adaptive_sampling_of_noisy_preferences"
        },
        {
            "paper_title": "Batched multi-armed bandits problem.",
            "rating": 1,
            "sanitized_title": "batched_multiarmed_bandits_problem"
        },
        {
            "paper_title": "An empirical evaluation of thompson sampling.",
            "rating": 1,
            "sanitized_title": "an_empirical_evaluation_of_thompson_sampling"
        },
        {
            "paper_title": "Simple bayesian algorithms for best-arm identification.",
            "rating": 1,
            "sanitized_title": "simple_bayesian_algorithms_for_bestarm_identification"
        }
    ],
    "cost": 0.020180499999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Mathematical Programming For Adaptive Experiments
8 Aug 2024</p>
<p>Ethan Che 
Columbia University</p>
<p>Daniel Jiang drjiang@meta.com 
Meta</p>
<p>University of Pittsburgh</p>
<p>Hongseok Namkoong namkoong@gsb.columbia.edu 
Columbia University</p>
<p>Jimmy Wang 
Columbia University</p>
<p>Mathematical Programming For Adaptive Experiments
8 Aug 20244BB54A1F635D4603EDCA70AF7ED7A840arXiv:2408.04570v1[cs.LG]
Adaptive experimentation can significantly improve statistical power, but standard algorithms overlook important practical issues including batched and delayed feedback, personalization, non-stationarity, multiple objectives, and constraints.To address these issues, the current algorithm design paradigm crafts tailored methods for each problem instance.Since it is infeasible to devise novel algorithms for every real-world instance, practitioners often have to resort to suboptimal approximations that do not address all of their challenges.Moving away from developing bespoke algorithms for each setting, we present a mathematical programming view of adaptive experimentation that can flexibly incorporate a wide range of objectives, constraints, and statistical procedures.By formulating a dynamic program in the batched limit, our modeling framework enables the use of scalable optimization methods (e.g., SGD and auto-differentiation) to solve for treatment allocations.We evaluate our framework on benchmarks modeled after practical challenges such as non-stationarity, personalization, multi-objectives, and constraints.Unlike bespoke algorithms such as modified variants of Thomson sampling, our mathematical programming approach provides remarkably robust performance across instances.</p>
<p>Introduction</p>
<p>Experimentation forms the foundation of scientific decision-making, from natural and social sciences to industry.Yet even for online platforms with hundreds of millions of users, achieving sufficient statistical power is costly [43,44,45,13,24].Adaptive experimentation can significantly improve efficiency by focusing resources on promising treatments.Gains in statistical power offered by adaptive designs has the potential to expand the set of scientific hypotheses that can be tested, beyond the typical handful of treatment options.</p>
<p>However, significant practical challenges remain in applying standard bandit algorithms.</p>
<p>• General objectives and constraints: Across a range of outcomes/metrics/rewards, there is a wide range of objectives and constraints that practitioners may consider.For example, a platform may want to balance multiple objectives by identifying treatments maximizing a primary metric without degrading secondary metrics.• Nonstationarity: Real-world experiments operate in non-stationary environments.In a weeklong experiment comparing multiple layouts for the landing page of a fashion retailer, behavior of customers who visit the page on the weekend is different from those that visit on Monday.• Constraints: A ridesharing platform testing region-specific discount strategies must obey an aggregate budget constraint that must be satisfied over the course of the experiment.Other constraints may include requiring sufficient sample coverage for post-experiment inference [56,71], requiring fairness [20], and ensuring safety [7].• Batching and delayed feedback: Standard algorithms are designed to be updated after every observation, but experiments are typically conducted in large batches with infrequent updates to the sampling policy due to infrastructure constraints and delayed feedback [9,56,39].</p>
<p>Standard bandit algorithms including best arm identification methods (Section 2) To address these challenges, the standard algorithm development paradigm takes a problem-specific approach; researchers craft tailored methods that deliver strong theoretical guarantees for long horizons (large number of updates to the sampling allocation) [41,58,42,4,59,34,48,31,70,25,60,26].However, it is infeasible for practitioners to develop a new algorithm for each new setting they encounter and in practice; deploying algorithms to settings that are slightly different than what they were designed for can lead to highly suboptimal performance even worse than that of static uniformly randomized allocations (see Figure 1).Additionally, maintaining a portfolio of problem-specific methods for each possible instance leads to fragmented infrastructures that are cumbersome to manage.</p>
<p>To understand why the literature takes this approach, consider the adaptive experiment sequential decision making problem minimize πt(Ht)≥0
E T t=0 Objective t (π t ; H t ) Cost(π t ; H t ) ≤ c t , 1 ⊤ π t (H t ) = 1 ∀t = 0, 1, . . . , T ,(1)
where T is the number of updates to the sampling allocation, H t denotes historical data (outcomes, contexts, treatment assignments) up to the t-th batch, and π t denotes the policy the modeler needs to learn.The dynamic program (1) is intractable since the dimension of the states H t is commensurate with the total number of observations.(See OAE-DP for a formal definition of the formulation (1).)To circumvent this issue, typical bandit algorithms optimize a surrogate such as an upper confidence bound derived from concentration inequalities (e.g.UCB) or a model drawn from a posterior distribution (e.g.Thompson Sampling).While these proxies do not directly measure the regret objective of interest, they nonetheless serve as useful approximations that guide sampling decisions to reduce regret as the horizon T increases.However, these proxy-based approaches face several limitations.First, it can be challenging to extend them to new and practically relevant regret objectives, such as objectives involving multiple metrics or combinations of simple and cumulative regret.Second, the structure of these proxies may be incompatible with operational constraints (e.g., experimentation budget or minimum sample requirements for each arm).It is unclear whether their performance guarantees will be maintained after suitable modifications.Finally, dealing with combinations of these challenges can yield exceptionally complicated methods or may be intractable.</p>
<p>As a result of these challenges, most experiments in practice are non-adaptive [67,3].</p>
<p>Mathematical programming via tractable reformulation (Section 3) In this work, we move away from tailoring bandit algorithms to specific long-horizon problems by presenting a scalable and flexible mathematical programming view for batched adaptive experiments.It is instructive to use deterministic optimization as a fable to understand our mathematical programming view.There, a practitioner writes down a problem instance consisting of decision variables, objective, and constraints in a standardized interface (e.g., CVX [33]) that gets passed to a flexible solver (e.g., Gurobi [36], MOSEK [8]).We argue adaptive experimentation should be viewed in an analogous way.Practitioners should be able to express their objectives, constraints, and structure of the assignment policy through a standardized and flexible language that includes simple objectives like best arm identification, or complex ones like best top-k selection while minimizing cumulative regret.Under this mathematical programming view, an effective adaptive method must perform well across a wide set of problem instances, rather than focus only on a specific set of problems.</p>
<p>Enabling this vision requires simplifying the dynamic program problem (1) to a tractable optimization problem that we can solve at scale.Our main observation is that real-world experiments are deliberately implemented with large batches and a handful of opportunities to update the sampling allocation [9,56,39] as a way to reduce operational costs of experimentation.This batched view of the experimentation process allows us to model the uncertainty around sufficient statistics necessary to make allocation decisions, instead of attempting to model unit-level outcomes whose distributions are commonly unknown and leads to intractable dynamic programs with combinatorial action spaces [10].</p>
<p>We consider any outcomes whose conditional means can be learned via a loss minimization problem over a parametric model class θ ∈ Θ.(In a vanilla multi-armed setting, the true parameter θ ⋆ can correspond to average rewards across arms.)Instead of an entire batch of data, we compressing the information into a sufficient statistic, the empirical risk minimizer θ t calculated over each batch.Since the central limit approximation implies
θ t ∼ N (θ ⋆ , n −1 t G(π t )) for some known function G(•) of sampling allocation π t ,(2)
we can view θ t as a noisy observation of the true parameter θ ⋆ , or equivalently, that the data θ t has approximately Gaussian likelihoods given the parameter θ ⋆ .Consider the experimenter's beliefs on the true parameter θ ⋆ evolving over batches as θ t 's are observed.In particular, if we posit a Gaussian prior on θ ⋆ with prior mean and variance β 0 and Σ 0 , then the Gaussian likelihood (2) gives conjugate posterior updates posterior mean β t and variance Σ t on θ ⋆ are updated based on batch-level signal θ t .</p>
<p>(</p>
<p>By modeling posterior beliefs (β t , Σ t ) as states, we arrive at a batched-limit dynamic program (BLDP) with known dynamics minimize πt(βt,Σt)≥0
E T t=0
Objective t (π t ; β t , Σ t ) Cost(π t ; β t , Σ t ) ≤ c t , 1 ⊤ π t (β t , Σ t ) = 1 ∀t = 0, ..., T .(4) Note that although we do not require the experimenter to be Bayesian when analyzing the data, we adopt Bayesian principles to optimize adaptive designs.BLDP (4) is a Bayesian MDP formulated over batches; here, Gaussianity is a result of the CLT rather than a blanket assumption.</p>
<p>To concretely illustrate the benefits of the tractable mathematical programming reformulation BLDP (4), consider a news feed recommendation engine on a social media platform like Instagram and Facebook.Organic contents-photos, videos, or text posts-are ranked using a user value model, a weighted average of ML-based predictions (e.g., probability of commenting, probability of re-sharing), and the top k contents are recommended [53,47].Since these weights should reflect individual user preferences, they are typically selected via online experimentation.In contrast to a traditional contextual bandit setting where individual content is sequentially recommended, the experimenter must determine the optimal allocation over ranking coefficients, which requires adjustments of traditional methods.Furthermore, the experimenter aims to optimize post-experiment performance (top-k simple regret) while limiting negative user experience during the experiment (cumulative regret).To our knowledge, no tailored algorithm exists for this specific setting, although separate algorithms have been designed for top-k best arm identification [61,62] and trading off between cumulative and top-1 simple regret [46].On the other hand, modeling these objectives in BLDP (4) is straightforward, as we detail in Section 3.2; see Section 5 for an empirical demonstration.[50].Treatment effects vary significantly over days (Figure 4).We simulate 750 batched experiments across instances, resulting in 180,750 evaluations across different batch sizes and different policies.Plots shown for batch size n t = 100 and T = 10.Contextual policies are aware that non-stationarity exists; we compare BLDP with variants of Thomson sampling, including Contextual Top-Two Thomson sampling (TTTS) [60] tailored for non-stationary settings.Right: Adaptive algorithms often do worse than uniform, overfitting on initial, temporary performance.RHO (stars) is the only adaptive policy with greater average reward and choose the best arm more often compared to uniform sampling, outperforming bandit algorithms even under model misspecification (non-contextual), across a wide range of learning rates.Left: Quantile of simple regret across 241 settings (normalized by that of uniform allocation).RHO outperforms uniform on more instances (60.5%) compared to other adaptive policies (51.8% for Top-Two Thompson Sampling).TS-based policies tend to be more fragile on difficult instances; when it underperforms Uniform it does so 10.7% on average (compared to 6.9% for RHO).</p>
<p>Simple and robust optimization algorithm (Section 4) So far, we used CLT approximations universal in statistical inference to model sufficient statistics θ t as a Gaussian distribution ("likelihood"), which led to simple and interpretable states (3) involving beliefs about sufficient statistics.By positing a conjugate Gaussian prior, the BLDP (4) has simple closed-form dynamics (posterior updates), making it easy to simulate trajectories under various treatment allocations and plan future sampling allocations.</p>
<p>To approximately solve the BLDP over continuous allocations, we introduce residual horizon optimization (RHO), a simple method based on the model-predictive control (MPC) design principle.Whenever new observations are obtained, the policy simulates trajectories from the BLDP to solve for an optimal static sequence of treatment allocations to use for the rest of experiment.By re-solving this sequence after every batch, the policy remains adaptive while working flexibly with batched feedback.The model-predictive control principle guarantees that regardless of how complicated the inputted specifications, objectives, constraints, and (predictable) non-stationarity patterns are, the treatment allocations are guaranteed to have a lower Bayesian regret than static A/B testing.</p>
<p>Since the dynamics of this Markov decision process are differentiable with respect to sampling allocations, our proposed algorithm can optimize lookahead policies over any set of objectives and constraints that can be expressed as a function of posterior beliefs (e.g., top-k simple-and cumulative-regret) through stochastic gradient descent methods.The use of pathwise stochastic gradients obtained through auto-differentiation provides a effective and simple method compared to REINFORCE-style policy gradient methods.Note that a value of 1 is exactly TS.Similarly, lighter values for RHO correspond to higher weights on simple regret while darker values correspond to higher weights on cumulative regret.RHO is able to efficiently trade off between the two objectives, with many points strictly dominating all values associated with TTTS.As more weight is put on the simple regret term, the simple regret incurred decreases monotonically.RHO trades off in a principled and interpretable way simply by setting the weights equal to the number of individuals under each objective.On the other hand, there is no principled way of selecting the parameter values for TTTS, and most do not outperform Uniform in terms of simple regret.</p>
<p>Rigorous empirical benchmarking (Section 5) In our mathematical programming framework, adaptive designs are learned based on scalable optimization algorithms regardless of the problem instance.We validate optimization algorithms through empirically rigorous benchmarking over diverse problem instances, based on the improvement over static uniform allocations ("A/B tests", the de facto standard).</p>
<p>Over a wide variety of settings involving (predictable) non-stationarity, personalization, and multiple objectives, we consistently find that RHO significantly outperforms state-of-the-art bandit algorithms (e.g., variants of Thompson sampling).In Figure 1, we provide a preview where we compare RHO to standard Thompson Sampling policies [6] as well as modified versions that account for (predictable) non-stationarity [60].We find that RHO exhibits a unique robustness to nonstationarity.Furthermore, Figure 2 displays RHO's ability to efficiently tradeoff between a combined cumulative and simple regret objective, showing that incorporating multiple objectives into our mathematical programming framework is easily tractable.</p>
<p>Batched Adaptive Experimentation</p>
<p>We begin by formalizing the adaptive experiment as a sequential decision-making problem and highlight the challenges of solving this problem directly.Given the widespread practice of conducting experiments in a few, large batches, we consider a batched adaptive experiment composed of T sequential epochs (or "batches") and K treatment arms.Each epoch t = 0, ..., T − 1 comprises of n t sampling units, where each unit i = 1, . . ., n t has a context X t,i ∈ X ⊆ R p drawn i.i.d.from a batch context distribution µ t which may be time-varying.</p>
<p>In this work, we restrict attention to predictable non-stationarity assume that the context distributions µ t are known to the modeler.While this may seem like a strong requirement, this is naturally satisfied when modeling temporal non-stationarity (e.g.day-of-the-week effects or novelty effects).Temporal non-stationarity can be modeled by having the context be the epoch x = t; in this case future context arrival distributions are µ s = 1{x = s} and are deterministic.In more complicated settings, e.g., if the context are highly user-specific features, we can instead use samples from the context distributions µ s , e.g., collected through a historical dataset.</p>
<p>The experimenter is interested in a stochastic outcome R t,i ∈ R, and the effect of each treatment arm a ∈ [K] on the outcome.In many applications, the experimenter is interested in multiple outcome metrics, in which case R t,i ∈ R m for m &gt; 1; while the framework we introduce can seamlessly handle this case, we focus on a single outcome metric for clarity of presentation.We denote D t = {(X t,i , A t,i , R t,i )} nt i=1 to be the data collected in batch t.The experimenter allocates treatments to sampling units to optimize the learning objective subject to constraints, both of which may depend on outcomes.The decision variable are the sampling allocations p t (•|x) ∈ ∆ K (X ), which is a conditional probability distribution over treatment arms a ∈ [K] that assigns a treatment arm A t,i ∼ p t (•|X t,i ) to each sampling unit during the experiment based on their context X t,i ∈ X .Of course, this includes non-contextual sampling allocations as well, which allocate a fixed fraction of the sampling units to each treatment arm.Since this is an adaptive experiment, sampling allocations may also depend on the history H t of sampled contexts, arm allocations, and rewards H t := t−1 s=0 D s observed before epoch t.</p>
<p>Reward models</p>
<p>Given an observed context X t,i = x and assigned action A t,i = a, the stochastic rewards R follow the conditional distribution
R t,i | X t,i = x, A t,i = a ∼ ν t (• | x, a) with mean r(x, a) := E[R t,i |X t,i = x, A t,i = a].(5)
Crucially, we do not assume ν t (•|x, a) is known to the experimenter.We assume that the primary object of interest, conditional mean reward r(x, a), is given by a general parametric model
r(x, a) = f (x, a; θ * ) for some θ ⋆ ∈ R d .(6)
Common parametric models include linear contextual and linear-logistic contextual models
r(x, a) = ϕ(x, a) ⊤ θ ⋆ , r(x, a) = σ(ϕ(x, a) ⊤ θ ⋆ ),(7)
where ϕ(x i , a i ) : X × [K] → R d is a known feature mapping and σ(x) = 1/(1 + e −x ) is the sigmoid function.</p>
<p>We assume the experimenter's statistical procedure for estimating θ ⋆ is specified through a loss function ℓ, mirroring common practice as well as the theoretical framework of M-estimation.This includes estimation procedures such as least-squares, maximum likelihood, quantile regression, as well as robust (Huber) losses.In order to connect this loss function to the underlying parameter of interest, we assume that the true parameter θ ⋆ minimizes the expected loss under a suitable population distribution, along with standard regularity assumptions.</p>
<p>Assumption A (Parameter specified by loss minimization).Let ℓ(θ|x, a, r) be a convex, twice continuously differentiable loss in θ for all (x, a, r).Under a population distribution over contexts X i ∼ µ, a reference action distribution A i ∼ p(•|X i ), and under the reward distribution R i ∼ ν t (•|X i , A i ), θ ⋆ is the unique minimizer of the population loss L(θ)
θ ⋆ = arg min θ∈Θ L(θ) := arg min θ∈Θ E[ℓ(θ|X i , A i , R i )] .(8)
Furthermore, L(θ) has a positive definite Hessian on a neighborhood of θ ⋆ .</p>
<p>For the linear and the linear-logistic model, the corresponding loss functions are as follows,
ℓ(θ|X i , A i , R i ) = |R i − ϕ(X i , A i ) ⊤ θ| 2 , ℓ(θ|X i , A i , R i ) = log 1 + e −R i ϕ(X i ,A i ) ⊤ θ .
In order to estimate θ ⋆ from a dataset D consisting of n iid tuples of contexts, arms, and rewards {X i , A i , R i } n i=1 , the experimenter solves the empirical risk minimization problem L n (θ|D)
θ n = arg min θ∈Θ L n (θ|D) := 1 n n i=1 ℓ(θ|X i , A i , R i ) . (9)
The goal of the experimenter is to assign treatment arms to contexts in order to collect data so that θ n is close to θ ⋆ .The set of reward models we consider can model a range of applications scenarios.Contexts can range from highly user-specific features to broader day-of-the-week or geographic fixed effects.As a result, this setting is not only applicable for personalized treatments or policy learning, but is also applicable to an experimenter interested in improving the efficiency and robustness of standard multi-armed bandit experiments.</p>
<p>Example 1 (Additive treatment effects with confounders).Let θ ⋆ = (θ ⋆ a , θ⋆ ) ∈ R p+K consists of the unknown linear parameter θ⋆ ∈ R p and additive treatment effect θ ⋆ a ∈ R.
r(x, a) = ϕ(x, a) ⊤ θ ⋆ = x ⊤ θ⋆ + θ ⋆ a (10)
Under this model, there is a single arm with the highest expected reward across all contexts x. ⋄</p>
<p>Example 2 (Mixed contextual treatment effects).Let
θ ⋆ = {θ ⋆ a } K a=1 ∈ R Kp comprise of action embeddings θ ⋆ a ∈ R p that give r(x, a) = ϕ(x, a) ⊤ θ ⋆ = x ⊤ θ ⋆ a .(11)
Since optimal actions can vary across x, the function (11) can model personalization when x represents user-specific features, and predictable non-stationarity when x = day-of-the-week and the best arm changes over weekdays vs. weekends.⋄</p>
<p>Objectives and Constraints</p>
<p>Our mathematical programming framework can handle flexible objectives and constraints unlike existing bandit algorithms focusing on particular functional forms.Given a sampling policy π, we evaluate the policy by the policy loss J T (p 0:T ), which is a sum of per-period objective functions c t : p t → R that depend on the sampling allocation at each epoch t.
J T (p 0:t ) = E T t=0 c t (p t )(12)
A standard example is the cumulative (within-experiment) regret, which depends on the sampling allocations made during the experiment (i.e.p t for t &lt; T ):
Regret T (p 0:t ) := E T −1 t=0 c t (p t ) = E T −1 t=0 n t E x∼µt max a {r(x, a)} − a p t (a|x)r(x, a)(13)
where c t (p t ) = E x∼µt [max a r(x, a) − a p t (a|x)r(x, a)].</p>
<p>In addition to rewards accrued over the experiment, we can also consider a post-experiment deployment decision of a treatment arm or even a policy allocating treatments over a population distribution X ∼ µ.We characterize this as the post-experimental phase, in which the experimenter uses the data collected during the experiment to make a final treatment allocation.Definition 1. Epoch t = T is the post-experimental phase after the batched adaptive experiment.During this phase, based on the history H T , the experimenter selects a final arm allocation decision over treatment arms p T (•|x) ∈ ∆ K for units from a population distribution µ.</p>
<p>The final allocation p T depend on sampling allocations made during the experiment as data collected during the experiment improves the final arm selection.Due to non-stationarity, the sampling units that arrive during the experiment may be drawn from distributions µ t that are very different from the population distribution µ.If the adaptive design does not account for non-stationarity, it may overfit to data that is non-representative of the treatment effects for the population distribution.</p>
<p>The simple (post-experiment) regret depends directly on the final arm allocation p T , a common goal in typical A/B tests where a treatment arm is deployed to all users at the end of the experiment.We define the simple regret as the sub-optimality of the final (non-contextual) arm allocation p T ∈ ∆ K with the treatment arm with the highest average reward (14) where c t (p t ) = 0 for t &lt; T .SimpleRegret is based on the population distribution µ, which may differ from the batch context distributions µ t for t = 0, . . ., T − 1 seen during the experiment.Rather than identifying a single best arm to be deployed for all contexts, an extension for personalization is comparing a policy for assigning treatment arms to the optimal policy which knows the true θ
SimpleRegret T (π) := E [c T (π T )] = E max a {E x∼µ [r(x, a)]} − a π T (a|H T )E x∼µ [r(x, a)] ,⋆ PolicyRegret T (π T ) := E [c T (π T )] = E E x∼µ max a {r(x, a)} − a π T (a|x, H T )r(x, a) .(15)
We also allow for general constraints of the form
T t=0 g t (p t , H t , X t,• ) ≤ B ∈ R b . (16)
where g t is continuously differentiable in p t .Similar to objectives, constraints may also depend on the reward function r(x, a) which is unknown to the experimenter.Examples of typical constraints include sample coverage guaranteeing all arms are sampled with some probability, a budget constraint of the treatments arms have an associated cost, or those preventing large reductions in the outcome during the experiment with high probability
Sample Coverage p t ≥ 10% Budget T t=1 c ⊤ p t ≤ B Safety P X t,i ∼µt A t,i ∼pt (r(X t,i , A t,i ) ≤ r) ≤ 1%. (17)</p>
<p>Optimal Adaptive Experimentation as a Dynamic Program</p>
<p>The goal of the experimenter is to select the problem of selecting sampling allocations for the batched adaptive experiment to optimize the objective.Importantly, the experimenter dynamically chooses the sampling allocations based on observed data during the experiment.This problem can be formulated as a dynamic program over sampling Policies π = {π t } T t=0 which map data to sampling decisions,
π t : H t → p t (•|x) ∈ ∆ K (X )
Abusing notation, it is convenient to also use π t (•|x, H t ) to describe the sampling allocation outputted by the policy given the current history:
π t (•|x, H t ) ≡ π(H t ) ∈ ∆ K (X )
The dynamic program for optimal adaptive experimental design is as follows,
X t,i iid ∼ µ t , A t,i iid ∼ p t (•|X t,i ), R t,i iid ∼ ν(•|X t,i , A t,i ), ∀i ∈ [n t ] (18a) D t = {(X t,i , A t,i , R t,i )} nt i=1 , H t+1 = H t ∪ D t ,(18b)
The dynamic program is to minimize the policy loss over policies π t :
H t → p t (•|x), minimize π J T (π) := E T t=0 c t (π t ) T t=0 g t (π t , H t , X t,• ) ≤ B . (OAE-DP)
Here, the transition probabilities are unknown to the experimenter as the conditional reward distribution ν(•|X t,i , A t,i ) is unknown (otherwise there is no need to experiment).</p>
<p>A classical solution to this challenge is to express modeler's epistemic uncertainty on ν-which gets resolved with infinite data-using Bayesian principles.We consider a Bayesian model over ν such that given the history H t , β t parameterizes the posterior distribution
ν | H t ν | H t ∼ P βt(Ht) (ν = •).(19)
If we marginalize out the unobserved ν according to some prior, the OAE-DP problem becomes a partially observable MDP (POMDP), which is notoriously difficult to estimate and solve.However, if augment the observed states of OAE-DP with the posterior states β t , we arrive at a Markov decision process.In this MDP, the state transitions to β t+1 corresponds to posterior updates based on the most recent data D t+1 .We refer to this formulation as the Bayesian MDP formulation for OAE-DP.Bayesian MDPs have long been used to formalize the exploration vs. exploitation trade-off [32].When the modeler is willing to assume a Bayesian model for unit-level rewards, Bayesian adaptive experimentation methods [14,66,27,40] optimize expected information gain, whereas Bayesian bandit algorithms consider finite actions and proposeq one-or two-step lookahead policies tailored to specific objectives [35,21,38,30,22,29,23].</p>
<p>While conceptually appealing, the Bayesian MDP reformulation using posterior states ( 19) has practical drawbacks.It requires the modeler to posit a Bayesian model for individual unit-level rewards ν(• | x, a), which is a daunting modeling task that almost always leads to misspecified models.In the ASOS fashion retailer example, this is equivalent to positing a Bayesian model for individual user behavior.</p>
<p>Challenges of solving the OAE-DP and its Bayesian MDP variant In theory, so long as the modeler is willing to believe their model of unit-level rewards, they can obtain adaptive sampling allocations for general objectives and constraints by solving the Bayesian MDP formulation for OAE-DP.However, this MDP is computationally intractable even with modern computing infrastructures.Without additional structure, the state space of this dynamic program is the size of the dataset H t , which can be very large in typical experiments.</p>
<ol>
<li>State space is intractably large.The state space of the OAE-DP is the history H t , whose dimension is the sum of batch sizes t s=0 n s .This is huge in typical experiments (e.g., n s ≥ 10 5 in typical A/B tests).The curse of dimensionality prevents the use of standard dynamic programming and approximate dynamic programming (ADP) techniques.(19) is infinite dimensional in general.Moreover, the posterior state β t is not even guaranteed to be a finite-dimensional parameter unless the modeler posits a conjugate prior for ν, which in turn substantially restricts the possible model space.</li>
</ol>
<p>Posterior state</p>
<ol>
<li>
<p>High-dimensional decision space with discrete outcomes.The sampling allocation affect the data D t via arm assignments A t,i ∼ p t (•|X t,i ), which is stochastic and discrete.</p>
</li>
<li>
<p>Updating posterior state (19) is intractable in general.Computing state transitions (posterior updates) on β t is computationally challenging.While several promising approximate posterior inference methods exist, they introduce significant complexity in the MDP, especially when coupled with the aforementioned challenges.</p>
</li>
</ol>
<p>The intractability of solving this dynamic program has been widely recognized in the bandit literature and has served as motivation for the development of more computationally feasible heuristics.To address these challenges, typical bandit algorithms like UCB and Thomson sampling optimize surrogates designed for a particular objective, leading to specialized algorithms that are guaranteed to perform well only in large horizons problems.As a result, the performance of these algorithms degrade substantially under changes to the underlying dynamic program.</p>
<p>Batch Limit Dynamic Program</p>
<p>The core principle of adaptive experimentation is to use observations collected during the experiment to improve the efficiency of the experiment by allocating samples towards more promising treatment arms.</p>
<p>However, framing this decision problem as a dynamic program (Bayesian MDP formulation of OAE-DP) leads to modeling and computational intractability as we highlight in Section 2.3.To address these challenges, we derive a tractable approximation that only relies on O(d 2 )-dimensional states where d is the dimension of the parameter θ ⋆ (Assumption A).We leverage the fact that typical experiments feature large enough batch sizes [9,56,39] to admit a central limit approximation, a universal practice in statistical inference.Specifically, there are three features of real-world experiments which enable statistical approximations at the batch level.</p>
<ol>
<li>
<p>Experiments typically feature a very low signal-to-noise ratio.That is, the measurement noise is usually orders of magnitudes larger than the treatment effects of the arms.</p>
</li>
<li>
<p>As mentioned earlier, experiments are usually conducted with large batch sizes n t in order to maintain statistical power in low signal-to-noise settings.</p>
</li>
</ol>
<p>These phenomena allow for a central-limit theorem (CLT) approximation for relevant suffficient statistics at the batch level, as developed in [19].</p>
<p>Although the distribution of the individual rewards R t,i is unknown to the experimenter, we can consider the empirical risk minimizer (9) for each batch and approximate its distribution by a Gaussian centered at the true parameter θ ⋆ .This allows us to treat each batch as a single noisy observation of θ ⋆ , which we interpret as a normal likelihood function given a fixed θ ⋆ .Under this approximation and a Gaussian prior over θ ⋆ -the conjugate prior-we arrive at a Bayesian model describing the uncertainty of θ ⋆ as batches are observed, providing noisy observations of θ ⋆ .The parameters of the posterior distribution at each epoch t forms a Markov Decision Process we call the Batch Limit Dynamic Program (BLDP), which is more structured and exponentially more tractable than OAE-DP.This provides the foundation of developing a tractable optimization procedure for designing adaptive experiments with general objectives.</p>
<p>Large-Batch Statistical Approximations</p>
<p>We derive our tractable dynamic programming formulation informally in this section, deferring a rigorous treatment to Section 6.Our main theoretical insight is that the following approximations are valid in the large batch limit where normal approximations become valid.</p>
<p>Given data D t collected in batch t with sampling allocation p t (•|x), the experimenter can obtain an estimate θ t of θ ⋆ by minimizing the empirical risk
θ t = argmin θ∈Θ L nt (θ|D t ) := 1 n t nt i=1 ℓ(θ|X t,i , A t,i , R t,i ) . (20)
Our key observation is that according to standard intuition from the M-estimation, under large batch sizes (i.e.n t → ∞) the distribution of θ t is asymptotically Gaussian,
θ t ≈ N θ ⋆ , 1 n t H −1 t I t H −1 t (21)
where the Hessian H t and the gradient covariance matrix I t are defined as follows
H t := E t a p t (a|x)∇ 2 θ ℓ(θ ⋆ |X t,i , a, R t,i )(22a)I t := E t a p t (a|x)∇ θ ℓ(θ ⋆ |X t,i , a, R t,i )∇ θ ℓ(θ ⋆ |X t,i , a, R t,i ) ⊤ . (22b)
For example, if ℓ(θ|x, a, r) = |r − ϕ(x, a) ⊤ θ| 2 is the least-squares loss for a linear model, then the central limit approximation (21) recovers the standard asymptotics for the ordinary least squares (OLS) estimator,
θ t ≈ N θ ⋆ , n −1 t E t [ϕ(X t,i , A t,i )ϕ(X t,i , A t,i ) ⊤ ]
In Section 6, we rigorously establish the validity of the informal approximation (21).Our main theoretical result proves that the sequence ( θ 1 , . . ., θ T ) converges jointly in distribution to a sequential Gaussian experiment, where the each observation is Gaussian conditional on past data.The main analytical challenge comes from the fact that each θ t depends on the realizations of previous estimates θ 1 , . . ., θ t−1 , as the sampling policy uses previous measurements to determine the sampling allocations.This requires us to derive a novel theoretical framework that carefully quantifies how the dependency impacts the rate at which the central limit approximation becomes valid.Our joint convergence result holds under minimal assumptions around the underlying sampling policy π, even allowing for π to sample arms with zero probability as well as allowing for H t to be non-invertible.</p>
<p>Batch Limit Dynamic Program (BLDP) as a Bayesian MDP</p>
<p>So far, we have argued that that the estimator θ t (20) provides information on θ ⋆ contaminated by Gaussian noise with a known distribution.Alternatively, we can think of this approximation as a data-generating model where given θ ⋆ , the observation θ t follows a Gaussian distribution.We adopt Bayesian principles to reason through the modeler's uncertainty on θ ⋆ and guide adaptive designs, but do not assume the experimenter is Bayesian-meaning they use standard frequentist tools for final statistical inference based on the data collected from the adaptive experiment.Given a Gaussian likelihood model P( θ t | θ ⋆ ) = Gaussian, one can efficiently perform Bayesian inference of θ ⋆ under a Gaussian prior θ ⋆ ∼ N (β 0 , Σ 0 ).By conjugacy, the posterior distribution of θ ⋆ will also be Gaussian
Fact 1. Under a Gaussian prior θ ⋆ ∼ N (β 0 , Σ 0 ), the posterior distribution of θ ⋆ after observing a Gaussian random variable G ∼ N θ, n −1 t H −1 IH −1 is, θ ⋆ |G ∼ N (β, Σ) where Σ −1 := Σ −1 0 + n t HI −1 H, β := Σ Σ −1 0 β 0 + n t HI −1 HG .(23)
Instead of keeping track of the entire history H t of all observations collected during the experiment, the experimenter can simply keep track of the posterior distribution of θ ⋆ parameterized by (β t , Σ t ).This posterior is updated after every batch t, using the asymptotic characterization of θ t .The sequence of posterior states {(β t , Σ t )} T t=0 forms a Markov decision process, whose dynamics are controlled by the sampling allocations p t chosen by the experimenter.Unlike the OAE-DP whose dynamics are unknown to the experimenter as the distribution of rewards is unknown, the transition dynamics of the posterior state MDP has a simple closed-form expression.The following result utilizes the reparameterization trick for Gaussian random variables.
Lemma 1. Let θ ⋆ ∼ N (β 0 , Σ 0 ). Given observations {G t } T t=1 where G t |G 1:t−1 ∼ N (θ * , n −1 t H −1 t I t H −1 t ), the joint distribution of posterior states {(β t , Σ t )} T t=0 are characterized by the recursive relation Posterior variance: Σ −1 t+1 := Σ −1 t + n t H t I −1 t H t (24a)
Posterior mean:
β t+1 := β t + (Σ t − Σ t+1 ) 1/2 Z t (24b)
where H t and I t are defined in (22a) and (22b) respectively, and Z 0 , . . ., Z T −1 iid ∼ N (0, I d ) are standard normal variables.</p>
<p>We defer derivation details to Section C.1.</p>
<p>We can now revisit the problem of designing the adaptive experiment in order to optimize the experimenter's objective, but now within this more tractable MDP.Given that (β t , Σ t ) characterizes the posterior at epoch t, it is sufficient to consider policies π = {π t } T t=0 that map posterior states to sampling allocations π t : (β t , Σ t ) → p t (•|x).Given the prior θ ⋆ ∼ N (β 0 , Σ 0 ), we consider the objective of finding a policy π that minimizes the Bayes policy loss:
min π E 0 [J T (π)] := E θ ⋆ ∼N (β 0 ,Σ 0 ) T t=0 c t (π t ) = E 0 T t=0 E t [c t (π t )] ,
where E t denotes expectation under the posterior distribution of θ ⋆ at batch t.When J T (π) is equal to regret, this corresponds to Bayesian regret.Since the posterior is described by finite-dimensional vectors (β t , Σ t ), we use the following shorthand to denote the posterior average cost
c t (p t |β t , Σ t ) ≡ E t [c t (p t )|β t , Σ t ] (25)
in the batch limit dynamic program.
min π V π 0 (β 0 , Σ 0 ) := E 0 T t=0 c t (π t | β t , Σ t ) T t=0 g t (π t ) ≤ B . (26)
Value functions of the BLDP under a policy π are given by
V π t (β t , Σ t ) = E t T s=t c s (π s |β s , Σ s )
. This approximation is used to formulate the planning problem and optimize, but the resulting sampling allocations are to be used in the original batched experiment.By virtue of restricting attention to policies that only depend on the batch posterior states (24), the BLDP avoids the issues that make OAE-DP and its Bayesian MDP formulation (19) intractable to solve.</p>
<ol>
<li>Obviating the need for a sophisticated unit-level Bayesian model.Instead of the full data D t = {(X t,i , A t,i , R t,i )} nt i=1 observe in batch t, BLDP only considers sufficient statistics θ t (20) modeled through posterior states.Compressing the states this way removes the need to posit a Bayesian model for individual rewards, unlike the Bayesian MDP formulation (19) of OAE-DP.In particular, the central limit approximation (21) provides a formal justification for using Gaussian likelihoods, meaning that Gaussianity is now a result of the CLT, not an unverifiable assumption made by the modeler.[57] or TensorFlow [1]) to directly optimize over sampling allocations.</li>
</ol>
<p>Prior work Our work directly extends the framework of Che and Namkoong [18] in simple multi-armed settings to allow nonstationarities, multiple objectives, constraints, and personalized policies.Our mathematical programming view is enabled by our main theoretical results which derive sequential local asymptotic normality for M-estimation problems.Our analysis carefully leverages Stein's method to address the sequential, adaptive nature of the problem and provides the most general formulation of classical local asymptotic normality and normal experiments to date [69].Overall, BLDP provides a substantially more general framework than their Gaussian sequential experiment over finite arm rewards and this generalization allows experimenters to update a flexible and tractable Bayesian model over the solution to a general loss minimization problem.Methodologically, we extend the MPC principle to formulate a general planning algorithm-which we still term RHO for conceptual ease-that can handle any objectives and constraints.Empirically, we showcase its performance across an extensive array of problem instances motivated by practice, compared to the simulation examples in Che and Namkoong [18].</p>
<p>Flexible Objectives and Constraints</p>
<p>Before discussing optimization methods in detail, we first revisit the objectives and constraints introduced in Section 2, in order to highlight the generality offered by BLDP.All objectives and constraints posed in Section 2.2 can be reformulated to be a function of posterior states (24), defining its Bayesian analogue.Concretely, consider the simple (post-experiment) regret ( 14), a common goal when a treatment arm is deployed to all users at the end of the experiment.To model this objective within the BLDP framework, define the Bayesian simple regret (BSR) as the sub-optimality of the final (non-contextual) arm allocation p T ∈ ∆ K with the treatment arm with the highest average reward ra = E x∼µ [r(x, a)] over contexts
BSR T (p 0:T ) := E 0 [c T (π T |β T , Σ T )] = E 0 E T [max a ra ] − a p T (a)E T [r a ] ,(27)
where c T (π T |β T , Σ T ) = 0 for t &lt; T .Again, BSR is based on the population distribution µ, which may differ from the batch context distributions µ t for t = 0, . . ., T − 1 seen during the experiment.</p>
<p>When the experimenter collects data to personalize treatment assignments, we have the Bayesian policy regret, which measures the sub-optimality of a contextual final arm allocaton p T (a|x)
BPR T (p 0:T ) := E [c T (π T |β T , Σ T )] = E 0 E T,x∼µ max a {r(x, a)} − a p T (a|x)r(x, a) .(28)
Considering gourmet objectives is simple in our BLDP framework.For top-k arm selection (e.g.[12,11]) where the experimenter is allowed to select up to k treatment arms at the end of the experiment, we measure performance based on simple top-k-sum regret:
BayesTop-k-Sum T (p 0:t ) := E 0 c T (p T |β T , Σ T ) = E 0 E T [top-k-sum(r)] − A k p T (A k ) E T a∈A k r a ,(29)
where for any vector r ∈ R K , top-k-sum(r) = max{r ⊤ p : 1 ⊤ p = k, p ≥ 0} takes the sum of the top k elements and A k represents the k-subset of actions chosen.Similar to objectives, a range of practical constraints (e.g., (17)) can be expressed as a function of the posterior states.</p>
<p>Algorithm 1 Residual Horizon Optimization</p>
<p>1: Input: prior (β 0 , Σ 0 ), objectives c s , constraints (g t , B), batch context distributions µ 0 , . . ., µ T −1 2: Initialize (β 0 , Σ 0 ), B 0 = B. Solve the problem (e.g., using SGD) with sample-paths (β s , Σ s ) drawn from the BLDP.
ρ ⋆ t:T ∈ argmin ρt,...,ρ T V ρ t:T t (β t , Σ t ) := E t T s=t c s (ρ s |β s , Σ s ) T s=t g s (p s ) ≤ B t(30)
5:
Sample arms A t,i ∼ ρ * t (•|X t,i
). Observe rewards R t,i .</p>
<p>6:</p>
<p>Update state (β t+1 , Σ t+1 ) according to the transitions (23).</p>
<p>7:</p>
<p>Update constraint B t+1 = B t − g t (ρ * t ).8: end for 9: return Final posterior state (β T , Σ) and final allocation ρ * T ∈ argmin ρ T c s (ρ T |β T , Σ T ).</p>
<p>Optimization Algorithm (RHO)</p>
<p>The Batched Limit Dynamic Program gives a tractable dynamic program that can guide experimentation in batched settings.With this in hand, one can apply methods from approximate dynamic programming and reinforcement learning to develop adaptive designs to optimize performance in this model.While the BLDP is far more tractable than the original DP formulation OAE-DP, it still has some of the inherent difficulties of solving dynamic programs, especially since it is a continuous state and action dynamic program.But fully solving the dynamic program may not be necessary.Che and Namkoong [19] notes even a simple algorithm based on model-predictive control (MPC) can achieve large improvements compared to standard adaptive policies-including sophisticated RL algorithms.By crystallizing the algorithm design principle behind this method, we extend this simple yet robust algorithmic approach to the substantially more general formulation BLDP.</p>
<p>The key capability we leverage from the BLDP is the ability to plan over any time horizon T .Instead of solving for an optimal dynamic policy, RHO instead plans a sequence of static sampling allocations ρ s to be used for future epochs.It does so by solving an open-loop planning problem, following the MPC design principle: MPC Design Principle: At every epoch t, given the current state (β t , Σ t ), solve for the optimal static sampling allocations (ρ t , . . ., ρ T −1 ) ∈ ∆ K×(T −t) (X ).</p>
<p>At any epoch, this leads to an optimization problem over static (constant) sampling allocations that only depend on the previous data through the current posterior state (Algorithm 1).The current sampling allocation gets deployed and additional data is observed; by updating the posterior beliefs using the Gaussian approximation of the estimators θt computed in each batch (24), we can flexibly re-solve the optimization problem whenever new data is obtained.Despite its simplicity, we demonstrate in the following that the MPC design principle can reap the benefits of adaptivity.In particular, this method can be seen as a natural dynamic extension of a static A/B test.</p>
<p>The planning problem (30) over static sampling allocations is computationally efficient to solve, compared to its counterparts involving dynamic allocations that depend on future observations.In fact, we can leverage the differentiability of the state transitions of the asymptotic model (24), and solve this optimization problem (30) directly with stochastic gradient descent methods, rather than any dynamic programming.Robustness Guarantee A key advantages of RHO over other alternatives is that it flexibly handles custom objectives, constraints, and non-stationarity, while guaranteeing performance in all of these myriad settings.This is concretely realized by the following theorem, which shows that it always achieves a lower Bayesian regret compared to a static A/B test across all time horizons T -even across these custom objectives, constraints, and non-stationarity arrivals.</p>
<p>Theorem 1 (Policy Improvement).Consider any static sequence p = (p 0 , . . ., p T ) of sampling allocation policies, which are dynamically feasible: g t (p t , . . ., p T ) ≤ B t , ∀t ∈ [T ].Let V p 0:T t be the corresponding value function under the Bayesian model and V RHO t (β t , Σ t ) be the value function of RHO.We have for all t, β t , Σ t that
V RHO t (β t , Σ t ) ≥ max p 0:T V p 0:T t (β t , Σ t ).
The proof is in Appendix C. 4.</p>
<p>Given that a static A/B test is often the de-facto choice of sampling allocation, this result guarantees that RHO will achieve the baseline level of performance, even in settings where standard heuristics such as Thompson Sampling cannot be applied without modification.In contrast, to apply standard heuristics to new settings requires ad-hoc changes to the policy without any guarantees that the modified sampling principle will work appropriately.</p>
<p>This guarantee is even more crucial under non-stationarity.In non-stationary settings adaptive algorithms can often perform much worse than static designs, as adaptive policies can overfit towards early but temporary performance.As long as the non-stationarity is predictable, that is the batch context distributions µ s are known ahead of time, RHO is guaranteed to do at least as well as the best static design.While this may seem like a strong requirement, this is naturally satisfied when modeling temporal non-stationarity (e.g.day-of-the-week effects or novelty effects).Temporal non-stationarity can be modeled by having the context be the epoch x = t; in this case future context arrival distributions are µ s = 1{x = s} and are deterministic.In more complicated settings, e.g. if the context are highly user-specific features, we can instead use samples from the context distributions µ s , e.g., collected through a historical dataset.</p>
<p>Beyond guranteeing performance against static designs, RHO can match performance and even outperform other adaptive policies on standard instances by planning with the Bayesian model.In doing so, it calibrates exploration to the time horizon and the signal-to-noise ratio.To do so, it uses more information about the experimental setting.It uses the horizon length T , which is often known in advance, and requires the ability to estimate the hessian H t and gradient covariance I t .This is straightforward for standard statistical settings, such as least-squares or logistic regression.Using this additional information, it does sensible things.Early in the experiment, when the remaining sample size is large, it explores widely.Towards the end of the experiment, it focuses on a few, promising treatment arms.Ultimately though, we validate the algorithmic design through evaluation in realistic experimental environments.</p>
<p>Numerical Experiments</p>
<p>In this section, we examine two important challenges that are highly relevant to real-world experimentation applications: non-stationarity and personalization.Both can pose difficulties for standard adaptive experimentation algorithms.For the former, we make use of the ASOS Digital Experiments Dataset, introduced in [50], which contains 78 real experiments run at ASOS, a global fashion retailer with over 26 million active customers (as of 2024).For the latter, we create a synthetic environment for personalized content ranking for online platforms, inspired by the content recommendation systems of Instagram and Facebook [53,47].</p>
<p>For both environments, we compare the performance of RHO against three baseline methods: Uniform allocation, Thompson sampling [6,65], and Top-two Thompson sampling [63].We select these baseline methods for their widespread adoption, state-of-the-art performance [16], and importantly, the ability to naturally handle the batched setting. 1 All of the baseline methods maintain a Gaussian belief.We provide more details about the baselines below.</p>
<p>• Uniform.A non-adaptive policy that uniformly allocates samples across all arms.</p>
<p>• Thompson sampling (TS).The main idea behind Thompson sampling is to sample from the posterior distribution of the optimal action [64].To implement this, at time t, TS first samples a parameter θt from a prior belief θt ∼ N (θ t , Σ t ).Given a reward model2 f (x, a; θ) and context X t,i , TS then makes an allocation decision by choosing A t,i = argmax a f (X t,i , a; θt ).See [6,65] for additional details.As mentioned above, although TS is typically presented in the fully-sequential setting, we can naturally adapt TS to our batch setting by simply repeating the sampling procedure for each unit in the batch.</p>
<p>• Top-two Thompson sampling (TTTS).Top-two Thompson sampling, introduced in [63], first identifies A t,i via the same procedure as TS.With probability β ∈ (0, 1), TTTS discards A t,i and repeatedly samples parameters θt ∼ N (θ t , Σ t ) until the optimal action argmax a f (X t,i , a; θt ) ̸ = A t,i .This encourages the algorithm to explore outside of the best arm and is specifically designed to minimize simple regret.</p>
<p>Non-stationarity via the ASOS Digital Experiments Dataset</p>
<p>Our first empirical study focuses on the issue of non-stationarity and makes use of the 78 real experiments included in the ASOS Digital Experiments Dataset, introduced in [50].Each experiment contains one treatment and one control variant and includes up to four different metrics (i.e., outcomes of interest), which results in 241 unique benchmark settings.Associated with each metric is sequence of sample means and variances, recorded at either 12-hour or daily intervals.The length of the experiments range from 2 recorded intervals to 132 record intervals.Figure 4 shows an example of the non-stationary behavior exhibited in the dataset.We construct a simulation environment using the ASOS dataset as follows.To model nonstationarity, we view the context X t,i to be time; i.e., we have that X t,i ≡ t.We take the conditional mean rewards to be the sample means provided in the dataset, while individual rewards are assumed to be sampled from a Gaussian distribution parameterized by the sample means and sample variances from the dataset.Since each experiment in the dataset contains only two arms (treatment and control), we also generated synthetic arms to mimic multi-armed experimentation problems that arise in practice.The means of the synthetic arms are set to be the sample mean of the control arm shifted by a N (0, 1) noise term multiplied by the gap between the sample means of treatment and control arms.The variances of the synthetic arms are set to be identical to the sample variance of the treatment arm.In doing so, we generate arms with similar average rewards to the treatment arm and display similar non-stationarity in the means across epochs.</p>
<p>We construct our simulation with 10 experimentation epochs consisting of 10 arms and three batch sizes: 10,000, 100,000, and 250,000 observations per epoch.We assume that the experimenter starts with a prior N (β 0 , Σ 0 ) where the structure of β 0 and Σ 0 depend on the assumed model.The objective is to minimize Bayesian simple regret (27).In this nonstationary setting, we set the "context" distribution to be uniform over [T ], resulting in Reward models.Based on the temporal variation within the ASOS dataset and the simple regret objective, we first introduce two models that form the basis of each experimentation policy.
BSR T (p 0:T ) = 1 T E 0 E T max
• Non-contextual.Here, the parameter vector is given by θ = (θ 1 , . . ., θ K ) ∈ R K , where we learn one parameter per arm.We then set ϕ(t, a) so that
f (t, a; θ) = ϕ(t, a) ⊤ θ = θ a ,
with arm effects being constant across time.</p>
<p>• Contextual (interactive effects).The parameter vector is given by θ = ( θ 1 , . . ., θ K arm constants</p>
<p>, θ 1,1 , . . ., θ 1,K time 1</p>
<p>, . . ., θ T,1 , . . ., θ T,K time T</p>
<p>), and the feature map ϕ(t, a) is set so that rewards are modeled as
f (t, a; θ) = ϕ(t, a) ⊤ θ = θ a + θ t,a .
In contrast to the non-contextual model, arm effects can now vary across time, better resembling the true data generating model.</p>
<p>Implementation details.We implement Uniform, TS, TTTS, and RHO under both reward model specifications above.Note that TS and TTTS under the interactive effects model is exactly Deconfounded TS [60], an extension of TS designed specifically to handle non-stationary environments.To initialize the prior for all policies, each policy starts out with a uniform prior N (0, λI) where λ scales inversely proportional to the batch size b t n.The nature of these time dependent models implies that the minimum eigenvalue of the sample covariance matrices λ min (Φ ⊤ t Φ t ) = 0 at each batch.Since the Bayesian posterior updates in (43) require invertibility of Σ t , we choose to scale λ with the batch size to maintain invertibility and stable condition numbers as the experiment progresses.Empirically, we find that this scaling is necessary to maintain stable performance across all policies.</p>
<p>Results and Discussion for the ASOS Dataset</p>
<p>We run 750 experiment simulations for each of the 241 settings in the ASOS dataset, resulting in 180,750 evaluations of each policy and batch size.Surprisingly, most of adaptive experimentation algorithms benchmarked on the different settings within the ASOS dataset offer little to no gains over uniform allocation.</p>
<p>Performance comparison between TS-based policies versus RHO.While TS-based policies offer strong theoretical guarantees in ideal settings, Figure 1 shows that under a batch size of 100, 000, all TS-based policies actually perform worse than uniform in both average reward obtained and identifying the best arm.In contrast, RHO offers strong performance.In fact, for every hyperparameter setting (i.e., the choice of learning rate, RHO's only hyperparameter), RHO outperforms TS-based policies even when choosing between the best of TTTS and the greedier TS in each individual setting.This type of robustness against hyperparameters is highly desirable in practice.We observe the same trends across batch sizes of 10, 000 and 250, 000, where RHO is the only policy to outperform Uniform.All policies have lower raw regret values as batch sizes increase, showing that while all policies perform better with more information, RHO is particularly robust to various amounts of statistical power.</p>
<p>Sometimes, an experimenter may have validation data to tune RHO's learning rate in individual settings.We simulate this possibility and find that choosing from a sweep over five different learning rates in each setting yields substantial performance gains.Overall, we find that RHO is robust to different learning rate choices, but can also offer significant gains when tuned properly.experimentation policies like TS or TTTS fail to offer significant gains over uniform allocation, we note that the settings within the ASOS dataset are often incredibly challenging with severe nonstationarity and a low signal-to-noise ratio.As previously mentioned, Figure 4 shows an example of a setting with non-stationary means.Under non-stationary behavior, adaptive algorithms may fail to explore, allocating samples to arms that show initial promise but eventually underperform the experiment progresses.In addition, the average gap between arms in the ASOS dataset is 0.0093 while the average measurement variance is 33.76, ranging from 0.0007 to over 2870.As these data are from real experiments, we believe that our results serve not just to advocate for the validity of RHO as a method, but also sheds light on the robustness (or lack thereof) of standard adaptive experimentation algorithms in challenging, non-ideal, but realistic conditions.</p>
<p>Failure of existing adaptive experimentation policies. To understand why existing adaptive</p>
<p>Understanding the performance of RHO.We observe that the main performance gains of RHO come from its consistency and robustness in challenging scenarios where adaptive algorithms underperform Uniform.In Table 1, we examine the two events of underperforming and outperforming Uniform separately.We find that while RHO obtains slightly higher regret compared to TS when both are conditioned on outperforming Uniform, overall RHO performs better than Uniform in many more settings (60.5% vs 51.8% and 39.1%).TS policies also have heavier regret tails, doing much worse in challenging situations compared to RHO.Similar behavior is detailed in Appendix A is found in settings with 10, 000 and 250, 000 batch sizes.</p>
<p>Model misspecification.Even when the model is misspecified, RHO can do better than other similarly misspecified models.When comparing policies under the non-contextual model (which cannot explain the variations due to interdependent arm and day effects), Table 2 shows that RHO still outperforms Uniform while TS and TTTS can suffer large drops in performance.</p>
<p>Personalized Value Models for Ranking and Recommendations</p>
<p>We further benchmark and explore the performance of RHO in a synthetic personalization setting.Classical works in personalization with contextual bandits focus on settings that involve sequentially choosing individual pieces of content to recommend to users [49,15,6], with the goal of minimizing cumulative regret as content is recommended.Let x i ∈ R n be the features corresponding to user i ∈ {1, . . ., N }.The basic linear model used in [49] involves a separate parameter θ a ∈ R n for each piece of content (arm) a such that r(x i , a i ) = x ⊤ i θ * a .Content is sequentially recommended to various users based on beliefs about the parameter θ * a .Note that there is a separate parameter θ * a to be learned for each action (piece of content).However, as platforms like Instagram and Facebook scale significantly in size and recommend content to billions of users, sequentially recommending and estimating the value of individual pieces of content may be infeasible.In addition, learning separate coefficients θ a for each piece of content like the traditional contextual bandit setting is impractical.Therefore, these platforms use a user value model, a linear function that scalarizes a set of machine learning signals into a single value for each piece of content [53,47].Define user features x and content features z.The experimenter has a known feature map ϕ(x, z) ∈ R d that generates d ranking signals.Given weights w x for a user x, the "value" (or "score") assigned to content z would be modeled as w ⊤</p>
<p>x ϕ(x, z).For each piece of content z in the universe of content Z, this value can be calculated and content can be ranked based on this model.Instead of choosing each individual piece of content to recommend to users, each experimentation epoch involves choosing ranking weights w x to deploy for each user x such that each user sees b pieces of content ranked by w x .</p>
<p>Ranking simulation setup.There are T epochs of experimentation with n t users at each epoch t.Each user has features x i ∈ R d and there is a selection of |Z| pieces of content that can be recommended to each user.At each epoch t, the experimenter can recommend b pieces of content to each user.User features x and content features z ∈ R d are simulated from Gaussian distributions:
x ∼ N (µ user , Σ user ), z ∼ N (µ content , Σ content ).
We let ϕ(x, z) := x ⊙ z, where ⊙ denotes the element-wise product.Given a true parameter θ * , we let the expected reward of recommending content z to user x be
r Z (x, z) = f Z (x, z; θ * ) = (x ⊙ z) ⊤ θ * = ⟨x, z⟩ θ * ,(31)
where ⟨x, z⟩ θ * represents a θ * -weighted inner product between x and z and the subscript Z indicates that this is the user-content reward function (later, we will define the primary reward function, which depends on x and w).The individual rewards are distributed according to N (⟨x, z⟩ θ * , s 2 ) for some fixed measurement variance s 2 .The action space consists of K predefined rankers {w (1) , . . ., w (K) }, with w (k) ∈ R d , that the experimenter must choose between.For each user x, the experimenter may choose a personalized ranker.Choosing ranker w for user x chooses a set of items Z(x, w) to be recommended, defined to be the top b items when sorted by ⟨x, z⟩ w .We have
Z(x, w) := top-b z∈Z ⟨x, z⟩ w and r(x, w) = f (x, w; θ * ) = z∈Z(x,w) f Z (x, z; θ * ),(32)
where r(x, w) sums over the user-content rewards for the top b items.</p>
<p>Objectives.For the ranking experiment, we consider a range of objectives, where the experimenter is interested in balancing cumulative regret and simple regret (or maximizing the cumulative and final rewards), i.e., Regret T (π) and SimpleRegret T (π) defined in (13) and (27).The tension between these two objectives is a recurring dilemma that experimentation practitioners often face.One way to view this trade-off is to consider the direct impact on users: during the experiment phase the experimenter may have to experiment on T −1 t=0 n t users, while the final policy will be deployed on n T users.The experimenter will ideally minimize the cumulative regret incurred within the experiment (i.e., not recommending low quality content) while finding a good policy to deploy after the experiment.If n T ≫ T −1 t=0 n t , then the experimenter should focus primarily on minimizing simple regret, while if T −1 t=0 n t ≫ n T , then the experimenter should minimize cumulative regret.</p>
<p>Implementation details.Given a context X t,i , TS chooses an action by sampling θt ∼ N (θ t , Σ t ) and then selecting A t,i = argmax w f (X t,i , w; θt ).RHO directly optimizes the simple and cumulative regret objective, where the tradeoff between these two terms is automatically optimized based on the number of individuals assigned to each objective T −1 t=0 n t and n T .</p>
<p>Results and Discussion for Personalized Value Models</p>
<p>In this section, we present several experiments based on the personalized recommendations setting.We examine each algorithm's ability to (1) balance simple and cumulative regret, (2) optimize simple regret only, and (3) adapt to an alternative, more tradition recommendation system formulation (i.e., one that is not based on ranking).</p>
<p>Balancing simple and cumulative regret.We first benchmark the algorithms' ability to efficiently trade off between simple and cumulative regret.We fix the within-experiment phase at 5 waves of 100 users.There are 10 different rankers to choose from that recommend 4 items out of 10 for each user.To trade off between the two objectives, we vary the parameter of TTTS from 0.1 to 1 in increments of 0.1 as it controls the level of exploration.A parameter of 1.0 exactly corresponds to standard TS, which behaves more greedily [63].To vary the level of exploration of RHO, we vary n T , which corresponds to the number of users the final policy will be deployed upon.</p>
<p>Figure 2 shows how RHO is able to efficiently trade off between simple and cumulative regret.As more weight is put on the simple regret term by increasing n T , the simple regret achieved monotonically decreases.As noted in the caption of Figure 2, RHO efficiently trades off between simple and cumulative regret and often strictly dominates the performance of all variants of TTTS along both axes.It is noteworthy that RHO handles this tradeoff in a principled and interpretable way simply by setting the weights equal to the number of individuals under each objective.On the other hand, there is no principled way of selecting the parameter values for TTTS, and most do not outperform Uniform in terms of simple regret.</p>
<p>We can also consider the weighted objective directly, with the weight on cumulative and simple regret terms to be exactly T −1 t=0 n t and n T .Under this alternative setup, RHO achieves the best objective value in 9 out of the 13 combinations of T −1 t=0 n t and n T tested in Figure 2. It is worth noting that when only optimizing for cumulative regret, RHO exactly recovers a greedy policy.The strong performance of greedy policies over TS-based policies over short time horizons has been observed in [65], further supporting RHO's ability to plan when there are limited opportunities for adaptivity.Simple regret minimization with varying reallocation epochs.Figure 5a measures simple regret performance of various policies while varying the number of reallocation epochs within the experiment.RHO is able to consistently outperform Uniform, however, TS and TTTS do not.To understand why this is this case, 5b plots the cumulative regret performance of the various policies.</p>
<p>It is noteworthy that both TS and TTTS incur lower cumulative regret compared to both RHO and Uniform, potentially giving an explanation as to why TS and TTTS fail to outperform Uniform in terms of simple regret.This also shows that RHO automatically calibrates its level of exploration such that when only optimizing for simple regret, it explores potentially bad options instead of sampling just the best arm.</p>
<p>A tradition single-item formulation.Many works have shown that TS has strong empirical and theoretical performance in a more traditional personalization setting [15,6].Instead of choosing a ranking policy based on a value model, the traditional setting typically involves item-by-item, fullysequential content recommendation (i.e., no batches of users or ranking and presenting multiple items per user).This involves learning separate coefficients θ z for each action, or content to be recommended [49].Similarly to the ranking setting, we have x ∼ N (µ user , σ 2 user I) and rewards for user x and content z are distributed as N (x ⊤ θ z , s 2 ).Each epoch involves a batch of 100 samples.Figure 6a shows that unlike in the ranking example (Figure 5a), TS policies outperform Uniform by a wide margin in simple regret minimization.In contrast, RHO showcases strong performance in both settings, exhibiting its ability to adapt and optimize for specific conditions.Figure 6b showcases how RHO is able to efficiently optimize both simple and cumulative regret when various weights are placed on each objective.Values for TS are rather homogeneous and many objective values of RHO strictly dominate those of TS.Lighter values for RHO mean higher weight on simple regret, while darker values are higher weights on cumulative regret.RHO is again able to efficiently trade off between simple and cumulative regret while TS policies have relatively homogeneous performance.</p>
<p>Policy  3. Epochs = 10.Performance of policies under non-standard noise distributions including the Gumbel and Student's t.We also vary the measurement variance level and context distribution.In all settings, RHO outperforms TS policies, and performs particularly well under the student's t distribution.
s 2 = 0.2, σ 2 user = 0.1 s 2 = 0.2, σ 2 user = 5 σ 2 user = 1, σ 2 user = 0.1 s 2 = 1, σ 2 user = 5 (Gumbel)
While the experiment in Figure 6 involves noise sampled from a Gaussian distribution, we further benchmark the algorithms' performance under non-standard noise distributions.We choose the Gumbel distribution which involves high skewness, and the Student's t distribution, which involves heavy tails.Table 3 keeps the number of epochs constant at 10 and shows the performance of policies under these distributions as well as various measurement variance and context distribution levels.We find that under all circumstances, RHO is able to outperform.These results suggest that RHO can be robust to other noise distributions.</p>
<p>Asymptotics for Batch Adaptive Experiments</p>
<p>The theoretical foundation for the BLDP presented in Section 3 is the asymptotic normality (21) of the estimators θ t obtained by empirical risk minimization (20) over a batch.We now prove this result rigorously.The key technical challenge is to show not only that this applies for the estimator computed in a single batch, but also for the entire sequence of estimators, which justifies the dynamic programming formulation.In addition, we do not assume that the Hessian of the empirical or population loss is invertible, which allows us to show that this limit holds with only minimal assumptions on the policy and even under non-stationarity.</p>
<p>To formalize this, we consider a scaling parameter n so as n → ∞, batch sizes grow large while the true parameter becomes harder to identify
r n (x, a) = f (x, a; θ ⋆ n ) = f (x, a; θ ⋆ / √ n), n t = nb t .(33)
Intuitively, scaling the parameter θ ⋆ also scales the gap between average rewards of each arm, making it harder to identify the best arm.For example, if θ ⋆ represents the mean of a distribution, then scaling θ ⋆ by 1/ √ n scales the gap between average rewards by the same factor.In the case of linear and logistic regression, scaling θ ⋆ reduces the effect size of the covariate on the linear response and log-odds respectively.</p>
<p>Asymptotic Normality</p>
<p>Under this scaling, we let R n,t = {R n,t,i } nt i=1 be the vector of rewards collected from epoch t.Note that R n,t depends on n through the number of observations n t and also the scaling of the average rewards r n as E[R n,t,i |X t,i = x, A t,i = a] = r n (x, a).As the batch size grows large, the population-level quantities become relevant.</p>
<p>Recall that D t := {(X t,i , A t,i , R n,t,i )} nt i=1 denotes the data collected in batch t, and let D t,i = (X t,i , A t,i , R n,t,i ) denote the tuple of observations for sampling unit i.We let ℓ θ (D t,i ) = ℓ(θ|X t,i , A t,i , R n,t,i ) be the statistical loss for unit i and L nt (θ|D t ) denote the empirical risk minimization objective under the batch dataset D t .We let θ n,t be a minimizer of the empirical loss:
θ n,t ∈ argmin θ∈Θ L nt (θ|D t ) = 1 n t nt t=1 ℓ(θ|X t,i , A t,i , R n,t,i )(34)
We emphasize that this loss function depends on the sampling allocation selected in batch t, and review the data-generating process for batch t:
X t,i iid ∼ µ t , A t,i iid ∼ p t (•|X t,i ), R n,t,i iid ∼ ν(•|X t,i , A t,i )
where µ t can be different for different t's and ν is unknown.In this section, we let E t denote expectation with respect to the above data-generating distribution.We can define several key quantities related to the curvature of the loss function.Recalling the population Hessian and gradient covariance at time t (22), define the corresponding empirical quantities
H n,t := 1 n t nt t=1 ∇ 2 θ ℓ( θ n,t |D t,i )(35a)I n,t := 1 n t b −1 t nt t=1 ∇ θ ℓ( θ n,t |D t,i )∇ θ ℓ( θ n,t |D t,i ) ⊤ .(35b)
To illustrate, under a linear reward model with the least-squares loss function, r(x, a) = ϕ(x, a) ⊤ θ ⋆ , and residual variance Var(R|X, A) = 1, these quantities correspond with the population and empirical design matrices, where we define Φ n,t ∈ R nt×d to be the matrix of feature vectors
H t = b t I t = E t [ϕ(X t,i , A t,i )ϕ(X t,i , A t,i ) ⊤ ], H n,t = b t I n,t = Φ ⊤ n,t Φ n,t .
In the general case where H t is allowed to be non-invertible, the statistical estimator we analyze is not θ n,t but rather H n,t θ n,t .This is because when H −1 t does not exist, asymptotic normality does not necessarily hold for the minimizer θ n,t and even the minimizer may not be unique.However, H n,t θ n,t satisfies a well-defined central limit theorem and the limiting random variable gives rise to a well-defined generalization of the Bayesian posterior update.Precisely, the sufficient statistic we consider is
Ψ n,t := √ nH n,t θ n,t .(36)
The √ n scaling is needed for Ψ n,t to be O(1) because the target parameter
θ ⋆ n is O(1/ √ n).
To formalize the use of Ψ n,t as a sufficient statistic, we make the following assumption about the policy.</p>
<p>Assumption B. We consider policies π = {π t } T t=1 where 1. Sufficient Statistic: π depends on the data only through the sufficient statistics,
π t (•|x, H t ) = π t (•|x, Ψ n,0:t−1 ) for all t = 0, • • • , T − 1 where Ψ n,0:t = (Ψ n,1 , . . . , Ψ n,t )
2. Continuity: The policy π t (a|x, Ψ n,0:t−1 ) is continuous in inputs Ψ n,0:t for all a, t.</p>
<p>We also allow the π t to depend on the Hessian and the gradient covariance matrices as well.These are mild assumptions, and for standard settings such as linear-contextual bandits, this class of policies includes Linear Thompson Sampling [5] and Linear Upper Confidence Bound [2].We show that conditional on previous observations θ n,0 , ..., θ n,t−1 , which affect the arm assignments A t,i through the policy π t (•|x, Ψ n,0:t−1 ), Ψ n,t converges in distribution to a Gaussian random variable.We refer to this as the Batched Limit Sequential Experiment.Definition 4. The Batched Limit Sequential Experiment is characterized by observations G 0 , . . ., G T −1 with conditional distributions
G t |G 0 , . . . , G t−1 ∼ N (H t θ ⋆ , I t )
If H t is non-invertible, this only provides a partial observation of θ ⋆ as it is well-known that unbiased estimators do not exist in this case.Nevertheless, one can still perform Bayesian updating with this partial signal.</p>
<p>In order to rigorously show convergence, we first require assumptions on the loss function ℓ θ .We do not assume the Hessian of the population loss E t [ℓ θ ] is strictly positive definite, which could be the case if π t does not sample certain treatment arms or if there is severe non-stationarity in the context distribution.As a result, we relax standard assumptions regarding strong convexity of the population loss.Instead, we require strong convexity only for the parameters in the range of the population Hessian, which is a much weaker assumption that is consistent with the fact that the minimum eigenvalue λ min (H t ) = 0 can equal zero.Let A t be the orthogonal projection matrix for the range space of the population Hessian H t .We require the following mild assumptions, which are generally satisfied by all standard convex loss functions (e.g.mean squared error, logistic loss).</p>
<p>Assumption C.</p>
<ol>
<li>Strong Convexity: The population loss ℓ θ (•) is twice differentiable with respect to θ and there exists µ &gt; 0 such that for any θ ∈ Θ,
E t [ℓ θ (D t,i )] − E t [ℓ θ * n (D t,i )] ≥ µ ∥A t (θ − θ ⋆ n )∥ 2 2 (37)</li>
<li>Lipschitz Condition: there exists c 1 &gt; 0, c 2 &gt; 0 such that for any D t,i = (X t,i , A t,i , R n,t,i ) and
θ ∈ Θ, ℓ θ (D t,i ) − ℓ θ ⋆ n (D t,i ) 2 ≤ c 1 ∥A t (θ − θ ⋆ n )∥ 2(38)∇ 2 ℓ θ (D t,i ) − ∇ 2 ℓ θ ⋆ n (D t,i ) 2 ≤ c 2 ∥A t (θ − θ ⋆ n )∥ 2(39)</li>
<li>Boundedness Condition: there exists c 3 , c 4 &gt; 0 such that
∥∇ℓ θ (D t,i )∥ 2 ≤ c 3 , ∇ 2 ℓ θ (D t,i ) ⪯ c 4 I for all X t,i , A t,i , R t,i , θ ∈ Θ (40) Furthermore, ∥θ ⋆ || 2 &lt; ∞.
It is worth mentioning what is not assumed.First, we do not require sampling probabilities to be bounded away from zero, i.e. π t (a|x, Ψ 0:t−1 ) &gt; 0. Second, unlike standard results for inference of bandit algorithms and M-estimation in general, we do not require that the Hessian is invertible, i.e. we allow both the empirical and population versions to have a zero eigenvalue.For linear least-squares with r(x, a) = ϕ(x, a) ⊤ θ ⋆ , this corresponds to a singular design matrix,
λ min Φ ⊤ n,t Φ n,t = 0, λ min E t [ϕϕ ⊤ ] = 0.
Rather than being a purely technical matter, this is critical for being able to show the validity of the contextual Gaussian sequential experiment under non-stationary context arrivals, as we will discuss later.We now present our main asymptotic result, which shows that as n → ∞, the contextual Gaussian sequential experiment is a valid approximation for the batch statistics even under adaptive sampling policies.We prove this in Section B.2.</li>
</ol>
<p>Theorem 2. Let Assumptions B, C hold.Then under a fixed policy {π t } T −1 t=0 , the Gaussian sequential experiment in Definition 4 provides a valid asymptotic approximation as n → ∞
(Ψ n,0 , . . . , Ψ n,T −1 ) d ⇝ (G 0 , ..., G T −1 ). (41)
The core challenge is to show convergence of sampling statistics under sampling probabilities that are themselves stochastic, as they are influenced by previous measurements.Standard asymptotic normality results for adaptively sampled data show results for the special case where θ n,t is the OLS estimator [37,52,51,71].This typically requires a uniform lower bound on the minimum eigenvalue of the design matrix.Our proof method instead shows convergence of aggregated rewards Ψ n,t , which is a biased estimator of θ.However, as a result we do not need to make any minimum eigenvalue assumptions on the population or empirical Hessian or the sampling allocations (besides mild continuity condtions) This enables us to apply our asymptotic results to a much wider range of adaptive policies, feature mappings, and non-stationary context arrivals.It is worth mentioning that even if the sampling policy π t &gt; δ is bounded from below for some δ &gt; 0 -a standard assumption for inference under adaptive sampling which we do not make -the design matrix could still be non-invertible due to non-stationary context arrivals.This can occur even in very practical settings: Example 3. (Monday/Tuesday).Consider a batched experiment with T = 2 epochs (corresponding to days of the week) and K = 1 arm.The context is simply the day-of-the-week, i.e. x = t.We consider a simple reward model with coefficient vector θ = (θ day 0 , θ day 1 , θ arm ) ∈ R 3 where rewards are composed of a time fixed effect and an arm fixed effect and the loss function is least squares:
r(t, a) = ϕ(x, a) ⊤ θ = θ day t + θ arm
At t = 0, suppose 100 sampling units arrive.They will all share the same context x = t = 0 and are all assigned to treatment a = 0.The sample covariance matrix will then be
λ min Φ ⊤ t Φ t = λ min     100 0 100 0 0 0 100 0 100     = 0
This is because the context t = 1 is never observed in epoch t = 0. ⋄ While in Section 3, we presented the BLDP when H t is invertible, we can discuss a generalization when this is not true.Upon observing a Gaussian random variable with N (H t θ ⋆ , n −1 t I t ), the posterior update for θ ⋆ is,
Definition 5. Let θ ⋆ ∼ N (β 0 , Σ 0 ). Given observations {G t } T t=1 where G t |G 1:t−1 ∼ N (H t θ ⋆ , n −1 t I t )
Posterior mean:
β t+1 := β t + (Σ t − Σ t+1 ) 1/2 Z t (42b)
where H t and I t are defined in (22a) and (22b) respectively, and Z 0 , . . ., Z T −1 iid ∼ N (0, I d ) are standard normal variables.</p>
<p>Note that unlike the MDP in (24) in Section 3 this does not have n t in the update for Σ as this is under the rescaled rewards r n .By rescaling removing the √ n factor in Φ n,t , we can recover the same scaling as in (24).</p>
<p>Asymptotic Validity</p>
<p>BLDP cannot be used exactly since the population covariance is unknown and the measurement Ψ n,t is used instead of G t .So instead, we can plug-in the empirical counterparts to the asymptotic quantities, leading to an approximate Bayesian model (β n,t , Σ n,t ), which is compatible with the data obtained from the batched experiment.Definition 6.The approximate Bayesian model is defined by the following update rule for posterior parameters (β n,t , Σ n,t ), upon observing aggregated rewards Ψ n,t and contexts Φ n,t :
Posterior variance: Σ −1 n,t+1 := Σ −1 n,t + H n,t I † n,t H n,t(43a)
Posterior mean:
β n,t+1 := Σ n,t+1 Σ −1 n,t β t + H n,t I † n,t Ψ n,t(43b)
Using the main asymptotic result in Theorem 2, we can show that trajectories of the approximate Bayesian model converge in distribution to trajectories of the asymptotic model as n → ∞, Corollary 1.Let π t (β t , Σ t ) be a policy that is continuous a.s.under (β t , Σ t ) drawn from (42).Let Assumptions B and C hold and consider any fixed θ and prior (β n,0 , Σ n,0 ) = (β 0 , Σ 0 ).The posterior states of (43) converge in distribution to the states (42) as n → ∞,
(β n,1 , Σ n,1 , . . . , β n,t , Σ n,t ) d ⇝ (β 1 , Σ 1 , . . . , β t , Σ t ).
This in turn implies convergence of the performance metrics and the value functions.Ultimately, this justifies the use of the asymptotic model as a guide for evaluating and optimizing performance metrics.</p>
<p>Corollary 2. If the objective functions c s (π s , β s , Σ s ) are all continuous in all arguments a.s.under
(β t , Σ t ) drawn from (42), then c s (π s , β n,s , Σ n,s ) d ⇝ c s (π s , β s , Σ s ) for all s ≤ T . If in addition, c s = O(∥β∥ 2 + ∥Σ∥ 2 )
then we also have convergence of the value functions for all t = 0, ..., T − 1
E t T s=t c s (π s |β n,s , Σ n,s ) → E t T s=t c s (π s |β s , Σ s )
The exact form of the approximate posterior updates in ( 43) is useful for showing theoretical convergence, but also has implications for posterior updates in batched settings.</p>
<p>Implications on practical performance: Scaling the design matrix or prior.While the covariance update (43) scales the design matrix by the batch size in order to obtain a sensible limit, we find that this is also practically relevant.By scaling down the covariance matrix, or alternatively scaling up the prior by n t , we manage to avoid numerical difficulties that emerge when updating with large batches, especially if the design matrix is singular or low-rank.This numerical difficulties emerge when trying to invert Σ −1 t+1 , which is going have an extremely small minimum eigenvalue if the low-rank design matrix dominates the prior.Recall the Gaussian sequential experiment given in Definition 4: for all 0 &lt; t ≤ T − 1</p>
<p>A Further Experimental Results</p>
<p>Batch
G 0 ∼ N (H 0 θ, I 0 ), G t |G t−1 , . . . , G 0 ∼ N (H t θ, I t )
where
H t = E t [∇ 2 ℓ 0 (D t i )]
is the Hessian of the observation at time t, and
I t = 1 bt E t [∇ℓ 0 (D t i )∇ℓ 0 (D t i ) ⊤
].Note that due to the dependence of H t , and I t on π t and π t on previous observations, G t depends on the outcomes of G t−1 , ..., G 0 .We use a shorthand Ψ n,t to denote the estimator Ψ n,t := √ nH n,t θn,t</p>
<p>Since we assume that the empirical minimizer θn,t sets 1 btn btn i=1 ∇ℓ θn,t (D t i ) = 0, we can take a Taylor Expansion of the gradient of the empirical loss around θn,t :
0 = 1 b t n btn i=1 ∇ℓ θn,t (D t,i ) = 1 b t n btn i=1 ∇ℓ θ * n (D t,i ) + 1 b t n btn i=1 1 0 ∇ 2 ℓ θt (D t,i )( θn,t − θ * n )dt(44)
Letting Ĥn,t = 1 btn btn i=1 ∇ 2 ℓ θ * n (D t,i ) and ξ t i = ∇ℓ θ * n (D t,i ), it follows that Ψ n,t can be written as
Ψ n,t = H t θ * + ( Ĥn,t − H t )θ * − 1 b t √ n btn i=1 ξ i − 1 0 ( 1 b t √ n btn i=1 ∇ 2 ℓ θt (D t,i ) − Ĥn,t )( θn,t − θ * n )dt. (45)
Induction We use an inductive argument to prove the weak convergence (41)  To ease notation, we often omit the dependence on ψ 0:t .</p>
<p>Bounding the first term in inequality (50) The first term in inequality (50) measures the distance between the sample mean Ψn,t+1 and its Gaussian limit.Before we proceed, it is helpful to define the following quantities, which describe smoothness of the derivatives of any function
f ∈ C 3 M 1 (f ) = sup x ∥∇f ∥ 2 M 2 (f ) = sup x ∇ 2 f op M 3 (f ) = sup x ∇ 3 f op
The bound, which we prove in Section B.2.1, quantifies the rate of convergence for the CLT using the multivariate Stein's method Meckes [54].Recall that ϵ is the noise in the rewards (??).t+1 Z for Lipschitz test functions, which is required to control the bounded Lipschitz distance.We use standard Gaussian smoothing arguments found in Meckes [55]: by convolving the test function with a Gaussian density, one obtains a smoother function for which the result of Proposition 3 is applicable.At the same time, the amount of smoothing is controlled to ensure the bias with the original test function is small.
M 2 (f * ϕ δ ) ≤ M 1 (f ) sup θ:∥θ∥ 2 =1 ∇ϕ ⊤ δ θ ≤ 2 π 1 δ M 3 (f * ϕ δ ) ≤ M 1 (f ) sup θ:∥θ∥ 2 =1 θ ⊤ ∇ 2 ϕ δ θ ≤ √ 2 δ 2
Moreover, for any random vector X ∈ R for some constant C t+1 that depends only on d, b t+1 , c 3 but not on n or ψ 0:t .</p>
<p>Bounding the second term in inequality (50) Finally, it remains to show uniform convergence of the second term in the bound (50).for constants C t+1 , D t+1 that depend polynomially on c 1 , c 2 , c 3 , c 4 as well as on d and b t+1 .Thus, this shows that E[f (Ψ n,0 , . . ., Ψ n,t+1 )] → E[f (Ψ n,0 , . . ., Ψ n,t+1 )] as n → ∞ for any bounded Lipschitz function f , which implies the desired weak convergence.</p>
<p>B.2.1 Proof of Proposition 3</p>
<p>In order to quantify the rate of the CLT, we use the following result by Meckes [54] which provides a characterization of Stein's method for random vectors with arbitrary covariance matrices. .For any index j, we construct an independent copy Y j of X j .We construct an exchangeable pair (W, W ′ ) by selecting an random index I ∈ {1, ..., b t+1 n} chosen uniformly and independently from W and letting
W ′ = W − X I b t+1 n + Y I b t+1 n .
We can observe then that X j,a X j,a ′ − (I t+1 ) a,a ′ by independence of X i and Y i .Thus, we have that
EE 2 a,a ′ = 1 (b t+1 n) 4 b t+1 n j=1 E E[X j,a X j,a ′ − (I t+1 ) a,a ′ |W ] 2 ≤ 1 (b t+1 n) 4 b t+1 n j=1 E X j,a X j,a ′ − (I t+1 ) a,a ′ 2 = 1 (b t+1 n) 3 E X 1,a X 1,a ′ 2 − (I t+1 ) 2 a,a ′
This gives us a bound on the Hilbert-Schmidt norm:
E ∥E∥ H.S. ≤ a,a ′ EE 2 a,a ′ ≤ 1 (b t+1 n) 3/2 a,a ′ E X 1,a X 1,a ′ 2 − (I t+1 ) 2 a,a ′ ≤ 1 (b t+1 n) 3/2 E ∥X 1 ∥ 4 2
We can further bound E ∥X 1 ∥ 4  2 by a constant C 1 that is polynomial in c 3 .Finally, we can bound E ∥W ′ − W ∥ 3 2 as follows:
E W ′ − W 3 2 = 1 (b t+1 n) 3/2 E ∥Y I − X I ∥ 3 2 ≤ 1 (b t+1 n) 3/2 8E ∥X 1 ∥ 3 2
where the final inequality uses independence of Y j and X j as well as Holder's inequality.We can bound E ∥X 1 ∥ 3 2 by another constant C 2 that is polynomial in c 3 .Plugging these bounds into the statement of Theorem 3, we obtain the stated result.</p>
<p>B.3 Proof of Proposition 4</p>
<p>Our proof follows that of [68] Lemma 5.2 with modifications because we only assume strong convexity with respect to parameters θ projected into the column space of A t .Given a πt , for each n, let A j,n = {θ : 2 j−1 &lt; √ n∥A t (θ − θ * n )∥ 2 ≤ 2 j }, j ≥ 1.First, by 37, we have inf θ∈A j,n
(E t [∇ℓ θ (D t i )] − E t [∇ℓ θ * n (D t i )]) ≥ µ inf θ∈A j,n ∥A t (θ − θ * n )∥ 2 ≥ µn −1 2 2j−2 .
Set δ j = 2 j n −1/2</p>
<p>Figure 1 .
1
Figure 1.Benchmark results on 241 non-stationary settings based on 78 real experiments run at ASOS, a fashion retailer with over 26 million active customers as of 2024[50].Treatment effects vary significantly over days (Figure4).We simulate 750 batched experiments across instances, resulting in 180,750 evaluations across different batch sizes and different policies.Plots shown for batch size n t = 100 and T = 10.Contextual policies are aware that non-stationarity exists; we compare BLDP with variants of Thomson sampling, including Contextual Top-Two Thomson sampling (TTTS)[60] tailored for non-stationary settings.Right: Adaptive algorithms often do worse than uniform, overfitting on initial, temporary performance.RHO (stars) is the only adaptive policy with greater average reward and choose the best arm more often compared to uniform sampling, outperforming bandit algorithms even under model misspecification (non-contextual), across a wide range of learning rates.Left: Quantile of simple regret across 241 settings (normalized by that of uniform allocation).RHO outperforms uniform on more instances (60.5%) compared to other adaptive policies (51.8% for Top-Two Thompson Sampling).TS-based policies tend to be more fragile on difficult instances; when it underperforms Uniform it does so 10.7% on average (compared to 6.9% for RHO).</p>
<p>Figure 2 .
2
Figure 2. Pareto frontier of the tradeoff between simple and cumulative regret with 5 epochs of experimentation.Lighter colors corresponding to TTTS indicate parameter values closer to 0, while darker values correspond to values closer to 1.Note that a value of 1 is exactly TS.Similarly, lighter values for RHO correspond to higher weights on simple regret while darker values correspond to higher weights on cumulative regret.RHO is able to efficiently trade off between the two objectives, with many points strictly dominating all values associated with TTTS.As more weight is put on the simple regret term, the simple regret incurred decreases monotonically.RHO trades off in a principled and interpretable way simply by setting the weights equal to the number of individuals under each objective.On the other hand, there is no principled way of selecting the parameter values for TTTS, and most do not outperform Uniform in terms of simple regret.</p>
<p>3 :
3
for each epoch t ∈ 0, . . ., T − 1 do 4:</p>
<p>Figure 3 .
3
Figure 3. BLDP provides a differentiable MDP that can be minimized via stochastic gradient descent.For a 3-armed experiment with horizon T = 10, we plot simple regret (red) over static sampling allocations π = (π 1 , π 2 , 1−π 1 −π 2 ) optimized using the Adam optimizer.</p>
<p>Figure 4 .
4
Figure 4.An example of the non-stationary means in the ASOS dataset.Here, gaps between means are small, on the order of 0.0002, while the best arm switches 4 times across the experiment.</p>
<p>(a) Simple Regret Performance (b) Cumulative Regret Performance When RHO Optimizes for Simple Regret</p>
<p>Figure 5 .
5
Figure 5. Plot of simple and cumulative regret performance of various policies and RHO when it optimizes only for simple regret.(Left) RHO is the only policy that consistently outperforms Uniform.(Right) Plotting the cumulative regret performance shows that both TS and TTTS incur lower cumulative regret than RHO, potentially showing that they are too greedy, giving a possible explanation as to why they fail compared to RHO and Uniform.</p>
<p>5 Figure 6 .
56
Figure 6.(Left) TS and TTTS outperform Uniform in simple regret minimization across various experiment lengths, unlike in the ranking example.This indicates that TS may be better suited to this setting.RHO RHO still outperforms TS policies by a large margin.(Right) Lighter values linked to TS indicate parameters closer to 0, while darker values indicate parameters closer to 1.Lighter values for RHO mean higher weight on simple regret, while darker values are higher weights on cumulative regret.RHO is again able to efficiently trade off between simple and cumulative regret while TS policies have relatively homogeneous performance.</p>
<p>Proposition 3 .
3
For any f ∈ C 3 , | ≤ C 1 n −1/2 M 2 (f ) + C 2 n −1/2 M 3 (f )where C 1 and C 2 depend polynomially on d, b t+1 , c 3 .It remains to show convergence of</p>
<p>Lemma 2 (
2
Meckes [55,  Corollary 3.5]).For any 1-Lipschitz function f , consider the Gaussian convolution (f * ϕ δ )(x) := E[f (x + δZ)], where Z ∼ N (0, I d ).</p>
<p>d , E[(f * ϕ δ )(X) − f (X)] ≤ E[δ ∥Z∥ 2 ] ≤ δ √ dThus, for any 1-Lipschitz function f , we have the bound ≤ C t+1 n −1/6</p>
<p>E E ψ 0:t d BL I 1 / 2 t+1Proposition 4 . 2 = 2 =∇ 2 ℓb t n btn i=1 (
124222i=1
(ψ 0:t )Z, I 1/2 t+1 (ψ 0:t )Z + b n,t+1 (ψ 0:t ).Under condition(37) there exists a constant D t &gt; 0 that depends on µ, c 1 , and d, such thatE[∥A t ( θn,t − θ * n )∥ 2 ] ≤ d 1 n −1/2(51)Since the stochastic terms have identical distributions, by a simple coupling argument we that for any bounded Lipschitz function f ,E ψ 0:t E[f I 1/2 t+1 (ψ 0:t )Z ] − E[f I 1/2 t+1 (ψ 0:t )Z + b n,t+1 (ψ 0:t ) ] ≤ E ψ 0:t E I 1/2 t+1 (ψ 0:t )Z − I 1/2 t+1 (ψ 0:t )Z − b n,t+1 (ψ 0:t ) E ψ 0:t E ∥b n,t+1 (ψ 0:t )∥ E ψ 0:t [E[||( Ĥn,t+1 − H t+1 )θ * − θt (D t+1 i ) − Ĥn,t+1 )( θn,t+1 − θ * n )dt.|| 2 ]] ≤ E ψ 0:t [||θ * || 2 E[|| 1 ∇ℓ θn,t (D t,i ) − ∇ℓ θ * n (D t,i ))|| 2 + btn i=1 (∇ℓ θ * n (D t,i ) − ∇ℓ 0 (D t,i ))|| 2 + || 1 b t n btn i=1 ∇ℓ 0 (D t,i ) − H t+1 || 2 + c 3 ||A t ( θn,t+1 − θ * n )|| 2 2 ]] ≤ ||θ * || 2 (2c 2 d 1 n −1/2 + c 4 n −2 ) + c 4 d 2 1 n −1 ≤ D t+1 n −1/2where the last line follows from Lemma 5.3 of[68] and application of 4 alongside C, and D t+1 &gt; 0 is a constant that depends on ||θ * || 2 , c 2 , c 4 , and d 1 .Conclusion Altogether, we have shown |E[g n (Ψ n,0 , . . ., Ψ n,t ) − g(Ψ n,0 , . . ., Ψ n,t )]| ≤ C t+1 n −1/6 + D t+1 n −1/2</p>
<p>Lemma 3 (√ d 4 M 2 9 M 3
34293
Meckes [54, Theorem 3]).Let (W, W ′ ) be an exchangeable pair of random vectors in R d .Suppose that there exists λ &gt; 0, a positive semi-definite matrix Σ, a random matrixE such that E[W ′ − W |W ] = −λW E[(W ′ − W )(W ′ − W ) ⊤ |W ] = 2λΣ + E[E|W ]Then for anyf ∈ C 3 , |Ef (W ) − Ef (Σ 1/2 Z)| ≤ 1 λ (f )E ∥E∥ H.S. + 1 (f )E W ′ − W 3where ∥•∥ H.S. is the Hilbert-Schmidt norm.The rest of the proof is similar to that of Chatterjee and Meckes[17, Theorem 7], with slight modifications due to the fact that we have a non-identity covariance.Let ξ t+1 i = ∇ℓ θ * n (D t+1 i ).For simplicity, define W =</p>
<p>j − X j |W ] = − 1 b t+1 n W by independence of Y j and W .This pair satisfies the first condition of Theorem 3 with λ = 1/(b t+1 n).It also satisfies the second condition of Theorem 3 with: E a,a ′ = 1 (b t+1 n) 2 b t+1 n j=1</p>
<p>Definition 3. The batch limit dynamic program (BLDP) is a Markov decision process where the state is (β t , Σ t ), and decision variables are sampling allocations p t (•|x).The state transition of (β t+1 , Σ t+1 ) for epoch t + 1, starting from the state (β t , Σ t ) and under a sampling allocation p</p>
<p>t (•|x) is given by (24b) and (24a).The dynamic program minimizes Bayesian loss over policies π t : (β t , Σ t ) → p t (•|x)</p>
<p>2 .
2
Dimensionality reduction of the state space.The states of the BLDP are (β t , Σ t ), which has dimension d + d 2 .This is orders of magnitude smaller than the size of the history |H t | for typical experiments.3. Closed-form state transitions.Although the reward distribution, due to asymptotic normality of the M-estimator θ t , state transitions in the BLDP have a closed form involving Gaussian random variables.It is easy to simulate rollouts for even very long horizons.4. Differentiable state transitions.The effect of the sampling allocation on the posterior state (β t , Σ t ) is mediated through H t and I t which are both linear in p, smoothing the partial feedback.This means that state transitions are differentiable along any sample path, and one can compute a sample-path gradient ∇ pt c s (p s |β s , Σ</p>
<p>s ) for any s &gt; t with auto-differentiation frameworks (e.g.PyTorch</p>
<p>Table 1 .
1
While TS and TTTS policies can achieve lower average regret on settings where they do better than Uniform, RHO is much more stable in situations where it underperforms Uniform.RHO
Simple Regret &lt; UniformSimple Regret &gt; UniformPolicy% Settings % Avg Regret of Uniform % Avg Regret of UniformNon-contextual TTTS [63]39.192.1124.8Contextual TTTS [60]51.889.7110.7Contextual RHO60.590.4106.9achieves lower regret than Uniform in 445/732 settings (60.5%), more than Non-contextual TTTS(39.1%) and Deconfounded (Contextual) TTTS (51.8%).Batch SizePolicy10,000 100,000 250,000Non-contextual TS0.0006-1.01-0.004Non-contextual TTTS 0.003-1.02-0.003Non-contextual RHO0.0990.0030.004</p>
<p>Table 2 .
2
Difference in raw regret values of methods under misspecified (non-contextual) models compared to Uniform with a correctly specified model.Non-contextual RHO is able to consistently outperform Uniform under model misspecification, while TS-based policies struggle.</p>
<p>Table
Linear TS67.082.894.2291.7Linear Top-Two TS69.193.893.894.0RHO61.880.381.186.1(Student's t) Linear TS77.110290.691.6Linear Top-Two TS76.197.890.888.9RHO60.583.066.583.3</p>
<p>, the joint distribution of posterior states {(β t , Σ t )} T t=0 are characterized by the recursive relation
Posterior variance: Σ −1 t+1 := Σ −1 t + H t I  † t H t</p>
<p>|E[E ψ 0:t [f (ψ 0:t , Ψ n,t+1 ) − f (ψ 0:t , I 1/2 t+1 (ψ 0:t )Z + H t+1 (ψ 0:t )θ * )]]| ≤ E[E ψ 0:t [d BL ( t+1 (ψ 0:t )Z + b n,t+1 (ψ 0:t ))]]
1 b t+1 √nb t+1 n i=1ξ t+1 i, I1/2≤ E[E ψ 0:t [d BL (1 b t+1 √nb t+1 n i=1ξ t+1 i, I t+1 (ψ 0:t )Z)] + E ψ 0:t [d BL (I 1/2 t+1 (ψ 0:t )Z, I 1/2 t+1 (ψ 0:t )Z + b n,t+1 (ψ 0:t ))]] 1/2(50)Importantly, we have thatCov ψ 0:t1 √ b tξ t+1 i= I t+1 (ψ 0:t )for a policy π andreward process R n,t satisfying Assumptions ??, B. For t = 0, since π 0 , µ 0 are fixed, we can applythe Lindeberg CLT to obtainΨ n,0 ⇝ N (H 0 θ  Next, suppose d
* , I 0 ) .(Ψ n,0 , ..., Ψ n,t ) d ⇝ (G 0 , ..., G t ).(46)=</p>
<p>Many bandit algorithms, such as UCB, do not easily extend to the batched setting. For example, in a nonpersonalized setting, UCB would assign the same action to all individuals. Traditional Bayesian optimization approaches[28] face a similar
difficulty.2 Recall our notation from (6) that r(x, a) = f (x, a; θ * ).
To show weak convergence to G t+1 , it is sufficient to show E[f (Ψ n,0 , . . ., Ψ n,t+1 )] → E[f (G 0 , ..., G t+1 )] for any bounded Lipschitz function f(47)Fixing a bounded Lipschitz f , assume without loss of generality that supx,y∈R (t+1)×d |f (x) − f (y)| ≤ 1 and f ∈ Lip(R (t+1)×d ).We first set up some basic notation.For any ψ 0:t ∈ R (t+1)×d , define the conditional expectation operator on a random variable W E ψ 0:t [W ] := E W {Ψ n,s } t s=0 = ψ 0:t .Then, conditional on realizations of previous estimators up to time t, we define a shorthand for the conditional expectation of f and its limiting counterpart.Suppressing the dependence on f , let g n , g : R t×d → R bewhere Z ∼ N (0, I) and the conditional Hessian and Fisher information is determined by the allocation π t+1 (ψ 0:t ), which depends on previous observations.Conditional on the history ψ 0:t , Ψ n,t depends on ψ 0:t only through the sampling probabilities π t+1 (ψ 0:t ).To show the weak convergence(47), decompose the difference between E[f (Ψ n,0 , . . ., Ψ n,t+1 )] and E[f (G 0 , . . ., G t+1 )] in terms of g n and gBy Assumption B2 and dominated convergence, g is continuous almost surely under (G 0 , . . ., G t ).From the inductive hypothesis(46), the continuous mapping theorem impliesWhile one would expect pointwise convergence of g n (ψ 0:t ) to g(ψ 0:t ) by the CLT, proving the above requires controlling the convergence across random realizations of the sampling probabilities π t+1 (Ψ n,0 , . . ., Ψ n,t ).This is complicated by the fact that we allow the sampling probabilities to be zero.To this end, we use the Stein's method for multivariate distributions to provide rates of convergence for the CLT that are uniform across realizations r 0:t .We use the bounded Lipschitz distance d BL to metrize weak convergence.Let b n,t+1 (ψwhere the last line follows from Lemma A.2 of[68]and D 1 &gt; 0 is some constant that depends on µ, C 1 , and d .C Derivations for Bayesian adaptive experimentC.1 Proof of Lemma 1Then, We can simplify the Bayesian posterior update for the mean as follows:where Z 1 and Z 2 are independent N (0, I d ) random vectors.This impliesWe use the identity (A + B)Further simplifying, we haveZ.So altogether the posterior update can be expressed asC.2 Proof of Corollary 1Note that the posterior state 43 transitions are continuous functions of previous states (β n,t , Σ n,t ) and Ψ n,t and 1 nt Φ ⊤ n,t Φ n,t .Therefore, they are continuous functions of Ψ n,0 , . . ., Ψ n,T −1 and H n,0 , I n,0 , . . ., H n,t , I n,tAs n → ∞, the above quantities converge in distribution to G 0 , . . ., G T −1 andThe continuous mapping theorem implies that (β n,0 , Σ n,0 , . . ., β n,T −1 , Σ n,T −1 ) d ⇝ (β 0 , Σ 0 , . . ., β T −1 , Σ T −1 )C.3 Proof of Corollary 2Since v s are continuous functions of (β n,t , Σ n,t ), by the continous mapping theorem v s (π s , β n,s , Σ n,s ) d ⇝ v s (π s , β s , Σ s ).By the dominated convergence theorem we have that ∥Σ n,s ∥ 2 ≤ ∥Σ 0 ∥ 2 almost surely and∞ has a uniformly bounded second moment across n, it is uniformly integrable and by dominated convergence theorem, the expectation of v s converge.C.4 Proof of Theorem 1First observe that when at T and T − 1, the static policy and RHO coincide so
Tensorflow: A system for largescale machine learning. M Abadi, P Barham, J Chen, Z Chen, A Davis, J Dean, M Devin, S Ghemawat, G Irving, M Isard, M Kudlur, J Levenberg, R Monga, S Moore, D G Murray, B Steiner, P Tucker, V Vasudevan, P Warden, M Wicke, Y Yu, X Zheng, 12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16). 2016</p>
<p>Improved algorithms for linear stochastic bandits. Y Abbasi-Yadkori, D Pál, C Szepesvári, Advances in Neural Information Processing Systems. 2011</p>
<p>Making contextual decisions with low technical debt. A Agarwal, S Bird, M Cozowicz, L Hoang, J Langford, S Lee, J Li, D Melamed, G Oshri, O Ribas, arXiv:1606.039662016arXiv preprint</p>
<p>Learning with limited rounds of adaptivity: Coin tossing, multi-armed bandits, and ranking from pairwise comparisons. A Agarwal, S Agarwal, S Assadi, S Khanna, Proceedings of the Thirtieth Annual Conference on Computational Learning Theory. the Thirtieth Annual Conference on Computational Learning TheoryPMLR2017</p>
<p>Thompson sampling for contextual bandits with linear payoffs. S Agrawal, N Goyal, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine Learning2013</p>
<p>Thompson sampling for contextual bandits with linear payoffs. S Agrawal, N Goyal, Proceedings of the 30th International Conference on Machine Learning. S Dasgupta, D Mcallester, the 30th International Conference on Machine LearningAtlanta, Georgia, USAJun 201328Proceedings of Machine Learning Research</p>
<p>Linear stochastic bandits under safety constraints. S Amani, M Alizadeh, C Thrampoulidis, Advances in Neural Information Processing Systems 19. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>The MOSEK optimization toolbox for MATLAB manual. M Aps, 2019</p>
<p>Ae: A domain-agnostic platform for adaptive experimentation. E Bakshy, L Dworkin, B Karrer, K Kashin, B Letham, A Murthy, S Singh, Neural Information Processing Systems Workshop on Systems for Machine Learning. 2018</p>
<p>Near-optimal a-b testing. N Bhat, V F Farias, C C Moallemi, D Sinha, Management Science. 66102020</p>
<p>Parallel algorithms for select and partition with noisy comparisons. M Braverman, J Mao, S M Weinberg, Proceedings of the forty-eighth annual ACM symposium on Theory of Computing. the forty-eighth annual ACM symposium on Theory of Computing2016</p>
<p>Top-k selection based on adaptive sampling of noisy preferences. R Busa-Fekete, B Szorenyi, W Cheng, P Weng, E Hüllermeier, International Conference on Machine Learning. PMLR2013</p>
<p>Power failure: why small sample size undermines the reliability of neuroscience. K S Button, J Ioannidis, C Mokrysz, B A Nosek, J Flint, E S Robinson, M R Munafò, Nature Reviews Neuroscience. 1452013</p>
<p>Bayesian experimental design: A review. K Chaloner, I Verdinelli, Statistical Science. 1031995</p>
<p>An empirical evaluation of thompson sampling. O Chapelle, L Li, Advances in Neural Information Processing Systems. 201124</p>
<p>An empirical evaluation of thompson sampling. O Chapelle, L Li, Advances in Neural Information Processing Systems. NeurIPSNeurIPS 2011. 201124</p>
<p>S Chatterjee, E Meckes, arXiv:0701464Multivariate normal approximation using exchangeable pairs. 2008math.PR</p>
<p>Adaptive experimentation at scale: A computational framework for flexible batches. E Che, H Namkoong, arXiv:2303.115822023arXiv preprint</p>
<p>Adaptive experimentation at scale: A computational framework for flexible batches. E Che, H Namkoong, arXiv:2303.11582[cs.LG]2023</p>
<p>Fair contextual multiarmed bandits: Theory and experiments. Y Chen, A Cuellar, H Luo, J Modi, H Nemlekar, S Nikolaidis, Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI). J Peters, D Sontag, the 36th Conference on Uncertainty in Artificial Intelligence (UAI)PMLRAug 2020124of Proceedings of Machine Learning Research</p>
<p>New two-stage and sequential procedures for selecting the best simulated system. S E Chick, K Inoue, Operations Research. 4952001</p>
<p>Sequential sampling to myopically maximize the expected value of information. S E Chick, J Branke, C Schmidt, INFORMS Journal on Computing. 2212010</p>
<p>Bayesian sequential learning for clinical trials of multiple correlated medical interventions. S E Chick, N Gans, Ö Yapar, Management science. 6872022</p>
<p>The dozen things experimental economists should do (more of). E Czibor, D Jimenez-Gomez, J A List, Southern Economic Journal. 8622019</p>
<p>Regret bounds for batched bandits. H Esfandiari, A Karbasi, A Mehrabian, V Mirrokni, Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence. the Thirty-Eighth AAAI Conference on Artificial Intelligence202135</p>
<p>T Fiez, H Nassif, Y.-C Chen, S Gamez, L Jain, Best of three worlds: Adaptive experimentation for digital marketing in practice. 2024</p>
<p>Deep adaptive design: Amortizing sequential bayesian experimental design. A Foster, D R Ivanova, I Malik, T Rainforth, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine Learning2021</p>
<p>P I Frazier, arXiv:1807.02811A tutorial on bayesian optimization. 2018arXiv preprint</p>
<p>Paradoxes in learning and the marginal value of information. P I Frazier, W B Powell, Decision Analysis. 742010</p>
<p>A knowledge-gradient policy for sequential information collection. P I Frazier, W B Powell, S Dayanik, 10.1137/070693424SIAM Journal on Control and Optimization. 4752008</p>
<p>Batched multi-armed bandits problem. Z Gao, Y Han, Z Ren, Z Zhou, Advances in Neural Information Processing Systems 19. 2019</p>
<p>Bayesian reinforcement learning: A survey. M Ghavamzadeh, S Mannor, J Pineau, A Tamar, Foundations and Trends® in Machine Learning. 20158</p>
<p>CVX: Matlab software for disciplined convex programming. M Grant, S Boyd, 2011</p>
<p>Best arm identification in multi-armed bandits with delayed feedback. A Grover, T Markov, P Attia, N Jin, N Perkins, B Cheong, M Chen, Z Yang, S Harris, W Chueh, S Ermon, Proceedings of the 21st International Conference on Artificial Intelligence and Statistics. the 21st International Conference on Artificial Intelligence and StatisticsPMLR2018</p>
<p>Bayesian look ahead one-stage sampling allocations for selection of the best population. S S Gupta, K J Miescke, Journal of Statistical Planning and Inference. 5421996</p>
<p>Optimization. Gurobi optimizer reference manual. I Gurobi, 2015</p>
<p>Confidence intervals for policy evaluation in adaptive experiments. V Hadad, D A Hirshberg, R Zhan, S Wager, S Athey, Proceedings of the National Academy of Sciences. 11815e20146021182021</p>
<p>Opportunity cost and ocba selection procedures in ordinal optimization for a fixed number of alternative systems. D He, S E Chick, C.-H Chen, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews). 3752007</p>
<p>Improving the statistical power of economic experiments using adaptive designs. S Jobjörnsson, H Schaak, O Musshoff, T Friede, Experimental Economics. 2022</p>
<p>Simple regret minimization for contextual bandits using bayesian optimal experimental design. M Jörke, J Lee, E Brunskill, ICML2022 Workshop on Adaptive Experimental Design and Active Learning in the Real World. 2022</p>
<p>Online learning under delayed feedback. P Joulani, A Gyorgy, C Szepesvári, Proceedings of the 30th International Conference on Machine Learning. the 30th International Conference on Machine LearningPMLR2013</p>
<p>Top arm identification in multi-armed bandits with batch arm pulls. K.-S Jun, K Jamieson, R Nowak, X Zhu, Proceedings of the 19th International Conference on Artificial Intelligence and Statistics. the 19th International Conference on Artificial Intelligence and StatisticsPMLR2016</p>
<p>Controlled experiments on the web: survey and practical guide. R Kohavi, R Longbotham, D Sommerfield, R M Henne, Data Mining and Knowledge Discovery. 1812009</p>
<p>Trustworthy online controlled experiments: Five puzzling outcomes explained. R Kohavi, A Deng, B Frasca, R Longbotham, T Walker, Y Xu, Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2012</p>
<p>Online controlled experiments at large scale. R Kohavi, A Deng, B Frasca, T Walker, Y Xu, N Pohlmann, Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2013</p>
<p>Proportional response: Contextual bandits for simple and cumulative regret minimization. S K Krishnamurthy, R Zhan, S Athey, E Brunskill, Advances in Neural Information Processing Systems. Curran Associates, Inc202323</p>
<p>How machine learning powers facebooks news feed ranking algorithm. A Lada, M Wang, T Yan, 2021</p>
<p>Bandit algorithms. T Lattimore, C Szepesvári, 2019Cambridge</p>
<p>A contextual-bandit approach to personalized news article recommendation. L Li, W Chu, J Langford, R E Schapire, Proceedings of the 19th international conference on World wide web (WWW 2010). the 19th international conference on World wide web (WWW 2010)ACM2010</p>
<p>C Liu, Â Cardoso, P Couturier, E J Mccoy, arXiv:2111.10198Datasets for online controlled experiments. 2021arXiv preprint</p>
<p>Parametric-rate inference for one-sided differentiable parameters. A R Luedtke, M J V D Laan, Journal of the American Statistical Association. 1135222018</p>
<p>Super-learning of an optimal dynamic treatment rule. A R Luedtke, M J Van Der Laan, International Journal of Biostatistics. 1212016</p>
<p>How instagram suggests new content. A Mahapatra, 2020</p>
<p>On stein's method for multivariate normal approximation. E Meckes, 20095High dimensional probability V: the Luminy volume</p>
<p>Gaussian marginals of convex bodies with symmetries. M W Meckes, Beitrage zur Algebra und Geometrie. 5012009</p>
<p>Adaptive experimental design: Prospects and applications in political science. M Offer-Westort, A Coppock, D P Green, 10.2139/ssrn.3364402SSRN. 33644022020</p>
<p>Pytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Advances in Neural Information Processing Systems. 201932</p>
<p>Batched bandit problems. V Perchet, P Rigollet, S Chassang, E Snowberg, Annals of Statistics. 4422016</p>
<p>Bandits with delayed, aggregated anonymous feedback. C Pike-Burke, S Agrawal, C Szepesvari, S Grunewalder, Proceedings of the 35th International Conference on Machine Learning. the 35th International Conference on Machine LearningPMLR2018</p>
<p>Adaptive experimentation in the presence of exogenous nonstationary variation. C Qin, D Russo, arXiv:2202.09036[cs.LG]2023</p>
<p>Dual-directed algorithm design for efficient pure exploration. C Qin, W You, 2023</p>
<p>Top-m identification for linear bandits. C Reda, E Kaufmann, A Delahaye-Duriez, Proceedings of the 24 International Conference on Artificial Intelligence and Statistics. the 24 International Conference on Artificial Intelligence and StatisticsPMLR2021a</p>
<p>Simple bayesian algorithms for best-arm identification. D Russo, Operations Research. 6862020</p>
<p>An information-theoretic analysis of thompson sampling. D Russo, B Van Roy, Journal of Machine Learning Research. 1712016</p>
<p>D J Russo, B Van Roy, A Kazerouni, I Osband, Z Wen, A tutorial on thompson sampling. Foundations and Trends® in Machine Learning. 201811</p>
<p>A review of modern computational algorithms for bayesian optimal design. E G Ryan, C C Drovandi, J M Mcgree, A N Pettitt, International Statistics Review. 8412016</p>
<p>Hidden technical debt in machine learning systems. D Sculley, G Holt, D Golovin, E Davydov, T Phillips, D Ebner, V Chaudhary, M Young, J.-F Crespo, D Dennison, Advances in Neural Information Processing Systems. 201528</p>
<p>Berry-esseen bounds for multivariate nonlinear statistics with applications to m-estimators and stochastic gradient descent algorithms. Q.-M Shao, Z.-S Zhang, arXiv:2102.04923Apr 2021arXiv preprintVersion 2</p>
<p>A W Van Der, Vaart, Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press1998</p>
<p>Linear bandits with stochastic delayed feedback. C Vernade, A Carpentier, T Lattimore, G Zappella, B Ermis, M Brueckner, Proceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningPMLR2020</p>
<p>Inference for batched bandits. K Zhang, L Janson, S Murphy, Advances in Neural Information Processing Systems. 202020</p>            </div>
        </div>

    </div>
</body>
</html>
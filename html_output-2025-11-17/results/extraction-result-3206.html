<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3206 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3206</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3206</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-74.html">extraction-schema-74</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-0a39442979d6e678dd36bb443ad529c14e86a86e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0a39442979d6e678dd36bb443ad529c14e86a86e" target="_blank">DocPrompting: Generating Code by Retrieving the Docs</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> DocPrompting is introduced: a natural-language-to-code generation approach that explicitly leverages documentation by retrieving the relevant documentation pieces given an NL intent, and generating code based on the NL intent and the retrieved documentation.</p>
                <p><strong>Paper Abstract:</strong> Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3206.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3206.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeT5+DocPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeT5 with DocPrompting (retrieval-augmented documentation conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CodeT5-base encoder-decoder model augmented with DocPrompting: a retrieval step that fetches relevant API/documentation paragraphs from an external documentation pool and conditions code generation on the top-k retrieved docs (via Fusion-in-Decoder).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CodeT5+DocPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>CodeT5-base (identifier-aware pretrained encoder-decoder for code) finetuned as a generator; uses a separate retriever (dense CodeT5-encoder or BM25) to fetch documentation paragraphs and integrates them using Fusion-in-Decoder (encode each (NL, doc) pair independently; decoder attends to all).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (external documentation pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>A pre-indexed documentation pool (CoNaLa: ~35,763 API docs; tldr: ~400k paragraphs). A retriever (BM25 or dense encoder like CodeT5/RoBERTa trained contrastively) scores docs by similarity to the NL intent and returns top-k (usually k=10). The generator conditions on the NL intent plus the concatenated/top-k docs (FiD for CodeT5) to produce code. The pool is updateable without re-training the generator.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>CoNaLa (re-split for unseen functions); tldr (Bash NL->command)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>NL->code generation: map natural-language intent to a code snippet/command. CoNaLa: Python snippets from StackOverflow, re-split so test examples use at least one function unseen in training. tldr: Bash one-liners from tldr pages with disjoint command sets between splits.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>code generation (NL->code), generalization to unseen APIs</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>CoNaLa BLEU = 36.22; function recall = 27.80; unseen-function recall = 18.30; execution pass@1 improvement (vs baseline CodeT5) +2.85 percentage points (pass@1). tldr (test): CMD Acc = 30.72%; Exact Match = 9.15%; Token F1 = 36.71%; charBLEU = 33.83.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>CoNaLa baseline CodeT5 BLEU = 34.57; function recall = 24.24; unseen-function recall = 9.03; pass@1 lower by 2.85 pp. tldr baseline CodeT5: CMD Acc = 14.60%; Exact Match = 2.18%; Token F1 = 30.00%; charBLEU = 21.50.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DocPrompting substantially improves generation of code that uses unseen functions, roughly doubling unseen-function recall on CoNaLa (9.03 -> 18.30), improves BLEU modestly (34.57 -> 36.22), and yields large absolute gains on tldr (e.g., CMD Acc +16.12 pp, charBLEU +12.33). Retrieval quality (dense retriever pretrained on code) and top-k selection strongly affect gains; oracle docs show much larger upper bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Performance depends critically on retriever precision/recall; incorrect or spurious docs can cause wrong arguments (example: mixing df.read_csv arguments into df.to_csv); generator can be confused by irrelevant docs when k is large; cascading retrieval errors limit gains; FiD is used to mitigate input length constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DocPrompting: Generating Code by Retrieving the Docs', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3206.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3206.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>T5+DocPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5-base with DocPrompting (retrieval-conditioned generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>T5-base encoder-decoder model augmented with DocPrompting: retrieves top-k documentation paragraphs and conditions generation on (NL intent + retrieved docs) using FiD-style encoding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>T5+DocPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>T5-base model finetuned as a generator; because of input length limits, uses Fusion-in-Decoder: each (NL, doc) pair is encoded separately and the decoder attends across encodings. Retriever options: BM25 or dense encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (external documentation pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Same pipeline: external documentation pool indexed; retriever (BM25 or dense) returns top-k paragraphs for an NL intent; each (NL, doc_i) is encoded independently and fed to the decoder which attends across them to generate the code; the pool can be updated independently of generator weights.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tldr (Bash NL->command); CoNaLa (Python)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate shell commands or Python code from natural language intents, with disjoint command/function sets between train/dev/test to measure generalization to unseen APIs.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>code generation (NL->code)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>tldr test: CMD Acc = 30.28%; Exact Match = 9.16%; Token F1 = 37.58%; charBLEU = 31.97. CoNaLa: BLEU = 30.04; unseen-function recall = 8.24.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>tldr baseline T5: CMD Acc = 10.02%; Exact Match = 0.76%; Token F1 = 19.90%; charBLEU = 25.48. CoNaLa T5 baseline BLEU = 28.07; unseen-function recall = 2.57.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DocPrompting yields large absolute gains for T5 on tldr (EM +8.4 pp, CMD Acc triples), and improves unseen-function recall on CoNaLa (2.57 -> 8.24). FiD-style encoding (parallel encoding) helps leverage multiple retrieved docs effectively.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Subject to same retrieval-quality issues; T5 benefits strongly from FiD (parallel encoding) and parameter-efficiency tradeoffs were noted (a larger autoregressive generator can match some gains but at higher parameter cost).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DocPrompting: Generating Code by Retrieving the Docs', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3206.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3206.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-Neo-1.3B+DocPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-Neo-1.3B with DocPrompting (concatenation-style retrieval conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-Neo-1.3B autoregressive generator augmented by concatenating top-k retrieved documentation paragraphs and the NL intent into a single long prompt (few-shot/fine-tuned variants used).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>GPT-Neo-1.3B+DocPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Autoregressive GPT-Neo-1.3B used as the generator; retrieved docs are concatenated with NL prompt (when length allows) to form a single generation prompt. Depending on input length, fewer docs may be used.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (external documentation pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Pre-indexed docs; retriever returns top-k docs (BM25/dense); docs + NL are concatenated into the generation prompt for GPT-Neo; generation sampling/decoding produces the code.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tldr (Bash NL->command)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate bash one-liners given NL intent; commands in test are disjoint from train to evaluate generalization to unseen commands/usages.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>code generation (NL->code)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>tldr test: CMD Acc = 27.59%; Exact Match = 9.05%; Token F1 = 37.24%; charBLEU = 30.57.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>tldr baseline GPT-Neo-1.3B: CMD Acc = 14.55%; Exact Match = 3.12%; Token F1 = 32.46%; charBLEU = 24.70.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Concatenation-style retrieval conditioning roughly doubles command-name accuracy and yields notable gains in exact match and BLEU-like metrics on tldr. Larger generator capacity (1.3B vs 125M) helps, but FiD/parallel encoding models outperform smaller or single-pass concatenation when many docs are needed.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Input-length limitations constrain how many docs can be concatenated; when concatenation isn't possible FiD-like methods are preferable. Retrieval precision remains a bottleneck.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DocPrompting: Generating Code by Retrieving the Docs', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3206.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3206.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex+DocPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex (code-davinci) with DocPrompting (few-shot prompt augmentation with retrieved docs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>OpenAI Codex few-shot prompt-based generator augmented by placing retrieved documentation paragraphs alongside few-shot (NL, code) examples and the test NL intent in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Codex+DocPrompting</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Codex (code-davinci-001/002) used in few-shot prompting mode; prompts include three static NL-code examples plus the test intent; DocPrompting replaces/augments examples with retrieved docs (or appends retrieved docs) so generation conditions on external docs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (external documentation pool)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Retriever returns top-k docs which are inserted into the few-shot prompt (oracle-doc variants also evaluated). Because Codex is a closed model (no finetuning in this work), DocPrompting is applied at prompt time by concatenation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tldr (Bash); CoNaLa (Python) few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Few-shot NL->code generation where Codex uses the prompt (few-shot examples ± retrieved docs) to generate code; datasets test generalization to unseen commands/functions.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>code generation (NL->code), few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>tldr Codex 3-shots + DocPrompting: CMD Acc = 31.21%; Exact Match = 9.29%; Token F1 = 36.77%; charBLEU = 23.72. CoNaLa Codex 3-shots + DocPrompting: BLEU = 43.47 (small gain over 43.16). With oracle docs: CoNaLa BLEU = 50.59 (substantial upper-bound gain).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>tldr Codex 3-shots baseline: CMD Acc = 27.48%; Exact Match = 8.94%; Token F1 = 36.04%; charBLEU = 16.94. CoNaLa Codex 3-shots baseline: BLEU = 43.16.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DocPrompting can improve even strong few-shot models like Codex, particularly when retriever quality is high or oracle docs are available; the oracle-doc experiments show a large potential uplift (e.g., CoNaLa BLEU 43.16 -> 50.59). However, real-world gains are muted in some Codex versions likely due to data leakage/memorization in Codex's pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>For large closed models likely pre-trained on target data (leakage), non-oracle DocPrompting sometimes yields little or no improvement; requires high-quality retriever and prompt engineering; limited prompt length constrains number of docs included.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DocPrompting: Generating Code by Retrieving the Docs', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3206.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3206.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language model agents using memory to solve tasks, including details of the agent, the memory mechanism, the tasks, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ExPrompting (example retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieving NL-code example pairs as context instead of documentation (baseline comparison)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline approach that retrieves NL-code example pairs from the training set (top-30) and conditions generation on those examples (retrieval-of-examples rather than documentation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>ExPrompting (retrieved examples + generator)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Retriever (RoBERTa) finds the most similar NL-code pairs from training; generator uses the retrieved examples as prompt/context (many examples concatenated) similar to retrieval-augmented prompting but with example pairs, not documentation.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>retrieval-augmented (retrieved training examples)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_mechanism_description</strong></td>
                            <td>Top-N NL-code pairs are retrieved from the training corpus and appended to the generator context; generator then produces an output for the test NL intent conditioned on these examples.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>tldr (Bash NL->command)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Use retrieved NL-code examples from the training set as context to generate commands for test NL intents that often use commands unseen in training.</td>
                        </tr>
                        <tr>
                            <td><strong>task_type</strong></td>
                            <td>code generation (NL->code), retrieval baseline</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>tldr (GPT-Neo-125M + ExPrompting): CMD Acc = 6.68%; Exact Match = 0.32%; Token F1 = 20.49%; charBLEU = 11.15%. GPT-Neo-1.3B + ExPrompting: CMD Acc = 14.01%; Exact Match = 2.80%; Token F1 = 30.07%; charBLEU = 22.11%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>These compare against corresponding baselines without any retrieval or with DocPrompting; DocPrompting substantially outperforms ExPrompting (e.g., GPT-Neo-125M DocPrompting CMD Acc = 25.32% vs ExPrompting 6.68%).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Retrieving documentation outperforms retrieving training examples in this cross-command/function generalization setting: DocPrompting gives much higher gains than ExPrompting because new libraries/functions often have documentation available while example usages may not.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Example retrieval is less helpful when test commands/functions are unseen in training; retrieval of examples may not generalize to new APIs; depends on presence of similar examples in training data.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DocPrompting: Generating Code by Retrieving the Docs', 'publication_date_yy_mm': '2022-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>Dense Passage Retrieval for Open-Domain Question Answering <em>(Rating: 2)</em></li>
                <li>Memorizing Transformers <em>(Rating: 2)</em></li>
                <li>RTFM: Generalising to novel environment dynamics via reading <em>(Rating: 2)</em></li>
                <li>Retrieval augmented code generation and summarization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3206",
    "paper_id": "paper-0a39442979d6e678dd36bb443ad529c14e86a86e",
    "extraction_schema_id": "extraction-schema-74",
    "extracted_data": [
        {
            "name_short": "CodeT5+DocPrompting",
            "name_full": "CodeT5 with DocPrompting (retrieval-augmented documentation conditioning)",
            "brief_description": "A CodeT5-base encoder-decoder model augmented with DocPrompting: a retrieval step that fetches relevant API/documentation paragraphs from an external documentation pool and conditions code generation on the top-k retrieved docs (via Fusion-in-Decoder).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CodeT5+DocPrompting",
            "agent_description": "CodeT5-base (identifier-aware pretrained encoder-decoder for code) finetuned as a generator; uses a separate retriever (dense CodeT5-encoder or BM25) to fetch documentation paragraphs and integrates them using Fusion-in-Decoder (encode each (NL, doc) pair independently; decoder attends to all).",
            "memory_used": true,
            "memory_type": "retrieval-augmented (external documentation pool)",
            "memory_mechanism_description": "A pre-indexed documentation pool (CoNaLa: ~35,763 API docs; tldr: ~400k paragraphs). A retriever (BM25 or dense encoder like CodeT5/RoBERTa trained contrastively) scores docs by similarity to the NL intent and returns top-k (usually k=10). The generator conditions on the NL intent plus the concatenated/top-k docs (FiD for CodeT5) to produce code. The pool is updateable without re-training the generator.",
            "task_name": "CoNaLa (re-split for unseen functions); tldr (Bash NL-&gt;command)",
            "task_description": "NL-&gt;code generation: map natural-language intent to a code snippet/command. CoNaLa: Python snippets from StackOverflow, re-split so test examples use at least one function unseen in training. tldr: Bash one-liners from tldr pages with disjoint command sets between splits.",
            "task_type": "code generation (NL-&gt;code), generalization to unseen APIs",
            "performance_with_memory": "CoNaLa BLEU = 36.22; function recall = 27.80; unseen-function recall = 18.30; execution pass@1 improvement (vs baseline CodeT5) +2.85 percentage points (pass@1). tldr (test): CMD Acc = 30.72%; Exact Match = 9.15%; Token F1 = 36.71%; charBLEU = 33.83.",
            "performance_without_memory": "CoNaLa baseline CodeT5 BLEU = 34.57; function recall = 24.24; unseen-function recall = 9.03; pass@1 lower by 2.85 pp. tldr baseline CodeT5: CMD Acc = 14.60%; Exact Match = 2.18%; Token F1 = 30.00%; charBLEU = 21.50.",
            "has_performance_comparison": true,
            "key_findings": "DocPrompting substantially improves generation of code that uses unseen functions, roughly doubling unseen-function recall on CoNaLa (9.03 -&gt; 18.30), improves BLEU modestly (34.57 -&gt; 36.22), and yields large absolute gains on tldr (e.g., CMD Acc +16.12 pp, charBLEU +12.33). Retrieval quality (dense retriever pretrained on code) and top-k selection strongly affect gains; oracle docs show much larger upper bounds.",
            "limitations_or_challenges": "Performance depends critically on retriever precision/recall; incorrect or spurious docs can cause wrong arguments (example: mixing df.read_csv arguments into df.to_csv); generator can be confused by irrelevant docs when k is large; cascading retrieval errors limit gains; FiD is used to mitigate input length constraints.",
            "uuid": "e3206.0",
            "source_info": {
                "paper_title": "DocPrompting: Generating Code by Retrieving the Docs",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "T5+DocPrompting",
            "name_full": "T5-base with DocPrompting (retrieval-conditioned generation)",
            "brief_description": "T5-base encoder-decoder model augmented with DocPrompting: retrieves top-k documentation paragraphs and conditions generation on (NL intent + retrieved docs) using FiD-style encoding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "T5+DocPrompting",
            "agent_description": "T5-base model finetuned as a generator; because of input length limits, uses Fusion-in-Decoder: each (NL, doc) pair is encoded separately and the decoder attends across encodings. Retriever options: BM25 or dense encoders.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (external documentation pool)",
            "memory_mechanism_description": "Same pipeline: external documentation pool indexed; retriever (BM25 or dense) returns top-k paragraphs for an NL intent; each (NL, doc_i) is encoded independently and fed to the decoder which attends across them to generate the code; the pool can be updated independently of generator weights.",
            "task_name": "tldr (Bash NL-&gt;command); CoNaLa (Python)",
            "task_description": "Generate shell commands or Python code from natural language intents, with disjoint command/function sets between train/dev/test to measure generalization to unseen APIs.",
            "task_type": "code generation (NL-&gt;code)",
            "performance_with_memory": "tldr test: CMD Acc = 30.28%; Exact Match = 9.16%; Token F1 = 37.58%; charBLEU = 31.97. CoNaLa: BLEU = 30.04; unseen-function recall = 8.24.",
            "performance_without_memory": "tldr baseline T5: CMD Acc = 10.02%; Exact Match = 0.76%; Token F1 = 19.90%; charBLEU = 25.48. CoNaLa T5 baseline BLEU = 28.07; unseen-function recall = 2.57.",
            "has_performance_comparison": true,
            "key_findings": "DocPrompting yields large absolute gains for T5 on tldr (EM +8.4 pp, CMD Acc triples), and improves unseen-function recall on CoNaLa (2.57 -&gt; 8.24). FiD-style encoding (parallel encoding) helps leverage multiple retrieved docs effectively.",
            "limitations_or_challenges": "Subject to same retrieval-quality issues; T5 benefits strongly from FiD (parallel encoding) and parameter-efficiency tradeoffs were noted (a larger autoregressive generator can match some gains but at higher parameter cost).",
            "uuid": "e3206.1",
            "source_info": {
                "paper_title": "DocPrompting: Generating Code by Retrieving the Docs",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "GPT-Neo-1.3B+DocPrompting",
            "name_full": "GPT-Neo-1.3B with DocPrompting (concatenation-style retrieval conditioning)",
            "brief_description": "GPT-Neo-1.3B autoregressive generator augmented by concatenating top-k retrieved documentation paragraphs and the NL intent into a single long prompt (few-shot/fine-tuned variants used).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "GPT-Neo-1.3B+DocPrompting",
            "agent_description": "Autoregressive GPT-Neo-1.3B used as the generator; retrieved docs are concatenated with NL prompt (when length allows) to form a single generation prompt. Depending on input length, fewer docs may be used.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (external documentation pool)",
            "memory_mechanism_description": "Pre-indexed docs; retriever returns top-k docs (BM25/dense); docs + NL are concatenated into the generation prompt for GPT-Neo; generation sampling/decoding produces the code.",
            "task_name": "tldr (Bash NL-&gt;command)",
            "task_description": "Generate bash one-liners given NL intent; commands in test are disjoint from train to evaluate generalization to unseen commands/usages.",
            "task_type": "code generation (NL-&gt;code)",
            "performance_with_memory": "tldr test: CMD Acc = 27.59%; Exact Match = 9.05%; Token F1 = 37.24%; charBLEU = 30.57.",
            "performance_without_memory": "tldr baseline GPT-Neo-1.3B: CMD Acc = 14.55%; Exact Match = 3.12%; Token F1 = 32.46%; charBLEU = 24.70.",
            "has_performance_comparison": true,
            "key_findings": "Concatenation-style retrieval conditioning roughly doubles command-name accuracy and yields notable gains in exact match and BLEU-like metrics on tldr. Larger generator capacity (1.3B vs 125M) helps, but FiD/parallel encoding models outperform smaller or single-pass concatenation when many docs are needed.",
            "limitations_or_challenges": "Input-length limitations constrain how many docs can be concatenated; when concatenation isn't possible FiD-like methods are preferable. Retrieval precision remains a bottleneck.",
            "uuid": "e3206.2",
            "source_info": {
                "paper_title": "DocPrompting: Generating Code by Retrieving the Docs",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "Codex+DocPrompting",
            "name_full": "Codex (code-davinci) with DocPrompting (few-shot prompt augmentation with retrieved docs)",
            "brief_description": "OpenAI Codex few-shot prompt-based generator augmented by placing retrieved documentation paragraphs alongside few-shot (NL, code) examples and the test NL intent in the prompt.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Codex+DocPrompting",
            "agent_description": "Codex (code-davinci-001/002) used in few-shot prompting mode; prompts include three static NL-code examples plus the test intent; DocPrompting replaces/augments examples with retrieved docs (or appends retrieved docs) so generation conditions on external docs.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (external documentation pool)",
            "memory_mechanism_description": "Retriever returns top-k docs which are inserted into the few-shot prompt (oracle-doc variants also evaluated). Because Codex is a closed model (no finetuning in this work), DocPrompting is applied at prompt time by concatenation.",
            "task_name": "tldr (Bash); CoNaLa (Python) few-shot",
            "task_description": "Few-shot NL-&gt;code generation where Codex uses the prompt (few-shot examples ± retrieved docs) to generate code; datasets test generalization to unseen commands/functions.",
            "task_type": "code generation (NL-&gt;code), few-shot",
            "performance_with_memory": "tldr Codex 3-shots + DocPrompting: CMD Acc = 31.21%; Exact Match = 9.29%; Token F1 = 36.77%; charBLEU = 23.72. CoNaLa Codex 3-shots + DocPrompting: BLEU = 43.47 (small gain over 43.16). With oracle docs: CoNaLa BLEU = 50.59 (substantial upper-bound gain).",
            "performance_without_memory": "tldr Codex 3-shots baseline: CMD Acc = 27.48%; Exact Match = 8.94%; Token F1 = 36.04%; charBLEU = 16.94. CoNaLa Codex 3-shots baseline: BLEU = 43.16.",
            "has_performance_comparison": true,
            "key_findings": "DocPrompting can improve even strong few-shot models like Codex, particularly when retriever quality is high or oracle docs are available; the oracle-doc experiments show a large potential uplift (e.g., CoNaLa BLEU 43.16 -&gt; 50.59). However, real-world gains are muted in some Codex versions likely due to data leakage/memorization in Codex's pretraining.",
            "limitations_or_challenges": "For large closed models likely pre-trained on target data (leakage), non-oracle DocPrompting sometimes yields little or no improvement; requires high-quality retriever and prompt engineering; limited prompt length constrains number of docs included.",
            "uuid": "e3206.3",
            "source_info": {
                "paper_title": "DocPrompting: Generating Code by Retrieving the Docs",
                "publication_date_yy_mm": "2022-07"
            }
        },
        {
            "name_short": "ExPrompting (example retrieval)",
            "name_full": "Retrieving NL-code example pairs as context instead of documentation (baseline comparison)",
            "brief_description": "A baseline approach that retrieves NL-code example pairs from the training set (top-30) and conditions generation on those examples (retrieval-of-examples rather than documentation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "ExPrompting (retrieved examples + generator)",
            "agent_description": "Retriever (RoBERTa) finds the most similar NL-code pairs from training; generator uses the retrieved examples as prompt/context (many examples concatenated) similar to retrieval-augmented prompting but with example pairs, not documentation.",
            "memory_used": true,
            "memory_type": "retrieval-augmented (retrieved training examples)",
            "memory_mechanism_description": "Top-N NL-code pairs are retrieved from the training corpus and appended to the generator context; generator then produces an output for the test NL intent conditioned on these examples.",
            "task_name": "tldr (Bash NL-&gt;command)",
            "task_description": "Use retrieved NL-code examples from the training set as context to generate commands for test NL intents that often use commands unseen in training.",
            "task_type": "code generation (NL-&gt;code), retrieval baseline",
            "performance_with_memory": "tldr (GPT-Neo-125M + ExPrompting): CMD Acc = 6.68%; Exact Match = 0.32%; Token F1 = 20.49%; charBLEU = 11.15%. GPT-Neo-1.3B + ExPrompting: CMD Acc = 14.01%; Exact Match = 2.80%; Token F1 = 30.07%; charBLEU = 22.11%.",
            "performance_without_memory": "These compare against corresponding baselines without any retrieval or with DocPrompting; DocPrompting substantially outperforms ExPrompting (e.g., GPT-Neo-125M DocPrompting CMD Acc = 25.32% vs ExPrompting 6.68%).",
            "has_performance_comparison": true,
            "key_findings": "Retrieving documentation outperforms retrieving training examples in this cross-command/function generalization setting: DocPrompting gives much higher gains than ExPrompting because new libraries/functions often have documentation available while example usages may not.",
            "limitations_or_challenges": "Example retrieval is less helpful when test commands/functions are unseen in training; retrieval of examples may not generalize to new APIs; depends on presence of similar examples in training data.",
            "uuid": "e3206.4",
            "source_info": {
                "paper_title": "DocPrompting: Generating Code by Retrieving the Docs",
                "publication_date_yy_mm": "2022-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2
        },
        {
            "paper_title": "Dense Passage Retrieval for Open-Domain Question Answering",
            "rating": 2
        },
        {
            "paper_title": "Memorizing Transformers",
            "rating": 2
        },
        {
            "paper_title": "RTFM: Generalising to novel environment dynamics via reading",
            "rating": 2
        },
        {
            "paper_title": "Retrieval augmented code generation and summarization",
            "rating": 2
        }
    ],
    "cost": 0.015843999999999997,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DocPrompting: Generating Code by Retrieving THE DOCS</h1>
<p>Shuyan Zhou ${ }^{\dagger}$, Uri Alon ${ }^{\dagger}$<br>Frank F. Xu ${ }^{\dagger}$, Zhiruo Wang ${ }^{\dagger}$, Zhengbao Jiang ${ }^{\dagger}$, Graham Neubig ${ }^{\dagger \ddagger}$<br>${ }^{\dagger}$ Language Technologies Institute, Carnegie Mellon University,<br>${ }^{\ddagger}$ Inspired Cognition<br>{shuyanzh,ualon, fangzhex, zhiruow, zhengbaj, gneubig}@cs.cmu.edu</p>
<h4>Abstract</h4>
<p>Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in their training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages code documentation by (1) retrieving the relevant documentation pieces given a natural language (NL) intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language, and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by $2.85 \%$ in pass@1 ( $52 \%$ relative gain) and $4.39 \%$ in pass@10 ( $30 \%$ relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute $6.9 \%$ exact match. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>We address the task of natural language to code generation (NL $\rightarrow$ code): generating a code snippet, written in a general-purpose programming language such as Python or Bash, given a natural language intent. This task has seen sharply growing popularity recently due to the emergence of large language models trained on vast amounts of natural language and code (Chen et al., 2021; Xu et al., 2022; Fried et al., 2022). NL $\rightarrow$ code models facilitate programming for both professional and inexperienced programmers, by allowing programmers to write code by only expressing their higher-level intent.
Many existing code generation models either learn directly from input-output pairs provided as training data (Allamanis et al., 2015; Yin and Neubig, 2017; Iyer et al., 2018; Brockschmidt et al., 2019; Xu et al., 2020; Alon et al., 2020; Wang et al., 2021), or learn the mapping between input and output implicitly from naturally occurring corpora of intertwined natural language and code (Austin et al., 2021; Nijkamp et al., 2022). Nevertheless, all these works assume that all libraries and function calls were seen in the training data; and that at test time, the trained model will need to generate only seen libraries and function calls. However, new functions and libraries are introduced all the time, and even a seen function call can have unseen arguments. Thus, these existing models inherently cannot generalize to generate such unseen usages.</p>
<p>In contrast to these existing models, human programmers frequently refer to manuals and documentation when writing code (Nykaza et al., 2002; Lethbridge et al., 2003). This allows humans to easily use functions and libraries they have never seen nor used before. Inspired by this ability,</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: DocPrompting: given an NL intent (H), the retriever retrieves a set of relevant documentation ${\mathrm{H}, \mathrm{H}, \mathrm{H}}$ from a documentation pool (17). Then, the generator generates the code (C) based on the NL and retrieved docs. DocPrompting allows the model to generalize to previously unseen usages by reading those docs. Italic blue highlights the shared tokens between NL and docs; Bold shows shared tokens between docs and the code snippet.
we propose DocPrompting: a code generation approach that learns to retrieve code documentation before generating the code. An overview of our approach is illustrated in Figure 1: First, a document retriever uses the NL intent (H) to retrieve relevant code documentation ${\mathrm{H}, \mathrm{H}, \mathrm{H}$ (H) from a documentation pool (17). Then, a code generator uses these docs in its prompt to generate the corresponding code (C). The documentation pool serves as an external data store that can be updated frequently with new contents (e.g., documentation of newly released libraries), without re-training any model component. This way, DocPrompting can leverage newly added documentation, and it can generate code containing unseen and unused functions and libraries. DocPrompting is general and applicable to any programming language and underlying base architecture. To the best of our knowledge, this is the first demonstration of leveraging documentation in models of code explicitly and effectively.</p>
<p>We demonstrate the effectiveness of DocPrompting on two NL $\rightarrow$ code benchmarks and tasks, across two programming languages, and using several base models: GPT-Neo (Black et al., 2021), T5 (Raffel et al., 2020), CodeT5 (Wang et al., 2021), Fusion-in-Decoder (Izacard and Grave, 2021)), and Codex (Chen et al., 2021). Further, we experiment with both sparse retrievers such as BM25 (Robertson and Jones, 1976) and dense retrieval models such as SimCSE (Gao et al., 2021). Finally, we introduce two new benchmarks for retrieval-based code generation: (a) in Bash, we curate a new benchmark by crawling the tldr repository, and constructing the training/development/test splits without overlapping commands; (b) in Python, we re-split the popular CoNaLa benchmark (Yin et al., 2018) by making every test example contain at least one Python function that is not seen in the training data. Models that use DocPrompting consistently outperform their base models that generate code solely based on the NL intents. Using DocPrompting improves strong base models such as CodeT5 by $2.85 \%$ in pass@1 ( $52 \%$ relative gain) and $4.39 \%$ in pass@10 ( $30 \%$ relative gain) in execution-based evaluation in CoNaLa; on the new tldr dataset, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute $6.9 \%$ exact match. We release our new benchmarks, including annotation of oracle documents for each example and pools of documentation, to serve as a test-bed for future retrieval-based code generation models.</p>
<h1>2 Code Generation by Reading the Docs</h1>
<p>Our underlying assumption is that code documentation is the most exhaustive yet succinct resource for most libraries and programming languages (Roehm et al., 2012), and that documentation allows to effectively generalize to unseen libraries and functions (Forward and Lethbridge, 2002). We follow the retrieve-then-generate paradigm (Lewis et al., 2020; Guu et al., 2020), focusing on retrieving documentation. In this section, we describe the general approach of DocPrompting; in $\S 3$ and $\S 6.2$, we elaborate and experiment with practical implementations of DocPrompting.</p>
<p>Formulation Given NL intent $n$, our goal is to generate a corresponding code snippet $c$ written in some programming language (PL) such as Python. We assume that a model has access to a collection of code documentation $\mathcal{D}$. Each document $d_{i} \in \mathcal{D}$ describes the usage of a library, a function, or an</p>
<p>argument in that PL. The construction of $\mathcal{D}$ is flexible: it can either be a comprehensive set of all available libraries and functions in a PL, or a customized subset for the scope of a specific project.</p>
<h1>2.1 BACKGROUND: RETRIEVAL-CONDITIONED GENERATION</h1>
<p>Although a model may use the entire collection of documents $\mathcal{D}$, only a few documents in $\mathcal{D}$ are relevant for any particular intent. Further, it is usually computationally infeasible to directly condition on the entire, unbounded, collection of documents while making predictions. Thus, we first let the model select a subset of documents $\mathcal{D}<em 1="1">{n}=\left{d</em>$ that are potentially relevant given $n$, and refer to this subset while generating $c$.
Overall, we decompose the probability of generating $c$ into the probability of choosing a particular subset of documents $P\left(\mathcal{D}}, d_{2}, . ., d_{k}\right} \subseteq \mathcal{D<em n="n">{n} \mid \mathcal{D}, n\right)$, and the probability of generating the code conditioned on the intent and the selected documents $P\left(c \mid \mathcal{D}</em>$ :}, n\right)$; finally, we marginalizing over all $\mathcal{D}_{n} \subseteq \mathcal{D</p>
<p>$$
P(c \mid \mathcal{D}, n)=\sum_{\mathcal{D}<em n="n">{n} \subseteq \mathcal{D}} P\left(c \mid \mathcal{D}</em>, n\right)
$$}, n\right) \cdot P\left(\mathcal{D}_{n} \mid \mathcal{D</p>
<p>assuming that $c$ is independent of $\mathcal{D}$ given $\mathcal{D}<em n="n">{n}$ (that is, $\left(c \Perp \mathcal{D} \mid \mathcal{D}</em>}\right)$ ). Since enumerating all possible subsets $\mathcal{D<em n="n">{n}$ is computationally infeasible, we follow the common practice and approximate the marginalization over $\mathcal{D}</em>$, and then conditioning the prediction of $c$ on these most likely documents:}$ in Equation (1) by taking the most probable subset of retrieved documents $\hat{\mathcal{D}}_{n</p>
<p>$$
\hat{\mathcal{D}}<em _mathcal_D="\mathcal{D">{n}:=\operatorname{argmax}</em><em n="n">{n} \subseteq \mathcal{D}} P\left(\mathcal{D}</em>} \mid \mathcal{D}, n\right) \quad P(c \mid \mathcal{D}, n) \approx P\left(c \mid \hat{\mathcal{D}<em n="n">{n}, n\right) \cdot P\left(\hat{\mathcal{D}}</em>, n\right)
$$} \mid \mathcal{D</p>
<h3>2.2 DocPrompting: Generating Code by Retrieving the Docs</h3>
<p>Equation 2 implies that DocPrompting relies of two main components: A retriever $\mathcal{R}$ retrieves relevant documents $\hat{\mathcal{D}}<em n="n">{n}$ given the intent $n$; and a generator $\mathcal{G}$ generates the code snippet $c$ conditioned on the retrieved documents $\hat{\mathcal{D}}</em>}$ and the intent $n$, which compose a new prompt. Specifically, $\mathcal{R}$ computes a similarity score $s\left(d_{i}, n\right)$ between a intent $n$ and every document $d_{i} \in \mathcal{D}$. Thus, the subset $\hat{\mathcal{D}<em n="n">{n} \subseteq \mathcal{D}$ is the top- $k$ documents with the highest similarity scores: $\hat{\mathcal{D}}</em>, n\right)\right)$.
An overview of our approach is illustrated in Figure 1: given the intent Generate HTML with python syntax highlighting for "print('reading docs')", the retriever $\mathcal{R}$ retrieves three relevant documents: $d_{1}$ describes the syntax highlighting library pygments, $d_{2}$ describes the class PythonLexer, and $d_{3}$ describes the HtmlFormatter class. Given these docs and the intent, the generator $\mathcal{G}$ generates the code snippet $c$, which uses PythonLexer and HtmlFormatter from the pygment library.}=$ top- $k_{d_{i} \in \mathcal{D}}\left(s\left(d_{i</p>
<h2>3 Practical InStantIations of DocPrompting</h2>
<p>DocPrompting is a general approach that is not bound to any specific model choices, and it can be instantiated with any base retriever and generator. This section presents the concrete instantiations of $\mathcal{R}$ and $\mathcal{G}$ that we found to provide the best performance in our experiments.</p>
<h3>3.1 RETRIEVER INSTANTIATION</h3>
<p>We experiment with two main types of retrievers: sparse retrievers and dense retrievers. As our sparse retriever, we use Elasticsearch ${ }^{2}$ with the standard BM25 (Robertson and Jones, 1976). This retriever represents documents using sparse features that rely on word frequencies, such as BM25 and TF-IDF.
As our dense retriever, we follow prior work (Chen et al., 2020; Karpukhin et al., 2020; Gao et al., 2021): given a triplet $\left(n, c, \mathcal{D}<em i="i">{n}^{<em>}\right)$, where $\mathcal{D}_{n}^{</em>}$ are the oracle docs for $n$, each $d</em>^{<em>} \in \mathcal{D}_{n}^{</em>}$ and $n$ form a positive pair $\left(n, d_{i}^{<em>}\right)$, while each $d_{j}^{</em>} \notin \mathcal{D}<em i="i">{n}^{<em>}$ and $n$ form a negative pair $\left(n_{i}, d_{j}^{</em>}\right)$. We train the retriever in a contrastive fashion where the similarity score of a positive pair is maximized while that of in-batch negative pairs is minimized. For a pair $\left(n</em>\right)$, the loss function is defined as:}, d_{i}^{*</p>
<p>$$
\mathcal{L}^{r}=-\log \frac{\exp \left(\operatorname{sim}\left(\boldsymbol{h}<em d__i="d_{i">{n}, \boldsymbol{h}</em>^{<em>}}\right)\right)}{\exp \left(\operatorname{sim}\left(\boldsymbol{h}<em d__i="d_{i">{n}, \boldsymbol{h}</em>^{</em>}}\right)\right)+\sum_{d_{j}^{<em>} \in \mathcal{B} / \mathcal{D}_{n}^{</em>}} \exp \left(\operatorname{sim}\left(\boldsymbol{h}<em d__j="d_{j">{n}, \boldsymbol{h}</em>
$$}^{*}}\right)\right)</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>where $\boldsymbol{h}<em x="x">{x}$ is the representation of $x$ computed by a neural encoder, and $\mathcal{B}$ are positive docs for other examples in the batch. We define $\operatorname{sim}\left(\boldsymbol{h}</em>}, \boldsymbol{h<em x="x">{y}\right)$ as the cosine similarity between $\boldsymbol{h}</em>$.}$ and $\boldsymbol{h}_{y</p>
<p>We use all $\left(n_{i}, d_{i}^{r}\right)$ in the training set as our supervised training dataset. Additionally, we use all sentences in the documentation pool for weak supervision: Following Chen et al. (2020) and Gao et al. (2021), representations of the same sentence with different dropout masks are treated as a positive example. Instead of using either supervised or weakly supervised training as in Gao et al. (2021), we simply mix the two resulting supervision signals, and examples are randomly distributed into batches. This mixture of tasks not only facilitates the learning process (§6.2), but also reduces the engineering effort required to store and reload models for separate supervised and unsupervised training phases. We initialize the retriever encoder with either the best model of Gao et al. (2021) or the encoder of CodeT5-base (Wang et al., 2021). Additional training details are provided in Appendix C</p>
<h1>3.2 GENERATOR INSTANTIATION</h1>
<p>We experimented with a variety of generator models. We used GPT-Neo-125M, GPT-Neo-1.3B (Black et al., 2021) and Codex (Chen et al., 2021), where we concatenate the retrieved documents and the NL intent as a single, long, prompt. T5-base (Raffel et al., 2019) and CodeT5-base (Wang et al., 2021) have a shorter input size of 512 tokens, which is sometimes too short for the concatenation of multiple docs. Thus, for T5 and CodeT5 we apply the fusion-in-decoder approach (FiD; Izacard and Grave, 2021): we first concatenate the intent $n$ with each retrieved $d_{i} \in \mathcal{D}<em i="i">{n}$ and encode each $\left(n, d</em>$.}\right)$ pair independently. Then, the decoder attends to all encoded NL-document pairs. We finetune the generator to maximize the log-likelihood of the reference code $c$ given $n$ and $\mathcal{D}_{n</p>
<p>With Codex (Chen et al., 2021), we performed few-shot learning rather than finetuning because the model parameters are not publicly available. We constructed the prompt with three static examples, each of which is a concatenation of retrieved documentation, an NL intent and the reference code snippet. We then appended the test example and its retrieved documentation to the few-shot examples. We used the code-davinci-001 version because we suspect potential leakage of the test set into the training set of code-davinci-002. See more details in Appendix H. Training details, hyper-parameter settings and example prompts can be found in Appendices E and D.</p>
<h2>4 EXPERIMENTAL SETUP</h2>
<p>We evaluate DocPrompting on two NL $\rightarrow$ code tasks: shell scripting (§4.1), in which we generate complex shell commands given an intent, and Python programming (§4.2), where we generate answers in Python for NL questions. In this section, we first introduce a newly curated benchmark tldr; we then describe our re-split of the popular CoNaLa benchmark (Yin et al., 2018). For each benchmark, we provide a global documentation pool $\mathcal{D}$ that is shared for all examples and oracle documents $\mathcal{D}_{n}^{*}$ which we use to train the retriever. We release our newly curated benchmarks to serve as test-bed for future retrieval-based code generation models.</p>
<h3>4.1 SHELL SCRIPTING</h3>
<p>tldr is a community-driven project that maintains easilyreadable help pages with examples for over $2.5 k$ Bash commands in over 25 natural languages ${ }^{3}$. We collected pairs of English intents and Bash command lines. The NL intents are written by human users, and the Bash commands range from popular ones like cat and tar, to uncommon commands such as toilet and faketime. Our resulting tldr benchmark contains 1,879 unique Bash commands and 9,187 NL $\rightarrow$ Bash pairs. We constructed the training, development and the test set with completely disjoint commands to test the generalizability of a code generation model. The shared documentation pool $\mathcal{D}$ is made up of the $400 k$ paragraphs from the 1,879 Bash manuals. Each paragraph describes a single concept such as an</p>
<p>NL Show slurm jobs queued by a user 'xyz' every 5 seconds
Code squeue -u xyz -i 5
squeue is used to view job and job step for Slurm jobs
-a Request jobs or job steps from a list of users.
-i Repeatedly report the information at the interval specified</p>
<p>Oracle docs
Figure 2: An example NL-code pair from tldr, along with three oracle documentation items.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>argument flag. We further curated the oracle documents $\mathcal{D}_{\mathrm{u}}^{*}$ for each example using simple string matching. An example from tldr is shown in Figure 2. To the best of our knowledge, this is the first work to leverage tldr as an NL $\rightarrow$ code benchmark. Detailed statistics and additional details are provided in Appendix A. In tldr, each NL intent results in a single Bash command with a combination of argument flags. We therefore first retrieve an entire Bash manual; then, we take the top manual and retrieve the top-10 paragraphs from that manual.</p>
<p>Evaluation metrics We measure: (a) command name accuracy (CMD Acc) - whether the command name (e.g., cat) is an exact match; (b) exact match (EM) - exact match between the reference and the generation; (c) token-level F1; and (d) character-level BLEU (charBLEU; Lin et al., 2018; Shi et al., 2022). In all metrics, we disregard user-specific variable names in the references and the models outputs. For example, "mycli -u [user] -h [host] [database]" is evaluated as "mycli -u \$1 -h \$2 \$3".</p>
<h1>4.2 Python Programming</h1>
<p>CoNaLa (Yin et al., 2018) is a popular benchmark for NL $\rightarrow$ Python generation. NL intents are StackOverflow questions, and code snippets are their answers. Both intents and code snippets are rewritten by human annotators. We re-split the dataset to test models' generalization to unseen Python functions. In our re-split, we verifed that every example in the development or the test set uses at least one Python function (e.g., plt.plot) that was not seen in the training data. In addition, we make sure that the examples from the same StackOverflow posts are in the same set to prevent leakage. This re-split results in 2,135/201/543 examples in the training/development/test sets, respectively.</p>
<p>The CoNaLa documentation pool $\mathcal{D}$ contains 35,763 documents, each describing a single function, from all Python libraries available on DevDocs (https://devdocs.io). These include built-in libraries and other popular libraries such as numpy. We constructed the oracle docs $\mathcal{D}_{\mathrm{u}}^{*}$ for each example by matching all function names in the target code $c$ with docs. More details in Appendix B.</p>
<p>Evaluation metrics We follow Yin et al. (2018) and measure BLEU-4. Since we focus on generalization to unseen functions, we additionally report function name recall (recall) and unseen function recall (recall ${ }_{\text {unseen }}$ ), which measures recall among function calls that do not appear in the training set. Finally, following Chen et al. (2021); Austin et al. (2021), we used the manually written unit tests from Wang et al. (2022) for 100 examples from CoNaLa's test set and measure pass@ $k$. We followed Chen et al. (2021) and performed nucleus sampling (Holtzman et al., 2019) with $p=0.95$. For each $k$, we searched for the best temperature for each model from ${0.2,0.4,0.6,0.8,1.0}$. On average, each example has 2.03 tests. The concatenation of multiple Python docs often exceeded the length limit of GPT-Neo, we hence experimented in this dataset with FiD, which allows longer inputs. Additional details are provided in Appendix B.</p>
<h2>5 ReSults</h2>
<p>In all following results, all models with DocPrompting use the top-10 retrieved docs from the best retriever on that dataset (Table 4). Every baseline uses the exact same setup as its " + DocPrompting" version, except for not using the documentation.</p>
<h3>5.1 Shell Scripting Results</h3>
<p>Results for tldr are shown in Table 1. DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost $9 \%$ of absolute exact match gain, compared to the vanilla T5. In the few-shot learning setting with Codex, DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt. These results show that retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.</p>
<p>Code generation with oracle command names In realistic settings, a human programmer may know the command name they need to use (e.g., awk), but not know the exact usage and flags. In fact, better understanding of the usage of known commands is the purpose of Unix man pages and the</p>
<p>Table 1: Results on shell scripting, using a BM25 retriever with top-10 retrieved docs, on the test set of tldr. For the "oracle command name" experiments, we selected the best model of each type.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CMD Acc (\%)</th>
<th style="text-align: center;">EM (\%)</th>
<th style="text-align: center;">Token F1</th>
<th style="text-align: center;">charBLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-Neo-125M</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">11.96</td>
<td style="text-align: center;">1.94</td>
<td style="text-align: center;">28.75</td>
<td style="text-align: center;">19.99</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">25.32</td>
<td style="text-align: center;">3.56</td>
<td style="text-align: center;">31.23</td>
<td style="text-align: center;">24.43</td>
</tr>
<tr>
<td style="text-align: center;">GPT-Neo-1.3B</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.55</td>
<td style="text-align: center;">3.12</td>
<td style="text-align: center;">32.46</td>
<td style="text-align: center;">24.70</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">27.59</td>
<td style="text-align: center;">9.05</td>
<td style="text-align: center;">37.24</td>
<td style="text-align: center;">30.57</td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">10.02</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">19.90</td>
<td style="text-align: center;">25.48</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">30.28</td>
<td style="text-align: center;">9.16</td>
<td style="text-align: center;">37.58</td>
<td style="text-align: center;">31.97</td>
</tr>
<tr>
<td style="text-align: center;">CodeT5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">14.60</td>
<td style="text-align: center;">2.18</td>
<td style="text-align: center;">30.00</td>
<td style="text-align: center;">21.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">30.72</td>
<td style="text-align: center;">9.15</td>
<td style="text-align: center;">36.71</td>
<td style="text-align: center;">33.83</td>
</tr>
<tr>
<td style="text-align: center;">Codex 3-shots</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">27.48</td>
<td style="text-align: center;">8.94</td>
<td style="text-align: center;">36.04</td>
<td style="text-align: center;">16.94</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">31.21</td>
<td style="text-align: center;">9.29</td>
<td style="text-align: center;">36.77</td>
<td style="text-align: center;">23.72</td>
</tr>
<tr>
<td style="text-align: center;">With the oracle command name</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">12.96</td>
<td style="text-align: center;">59.36</td>
<td style="text-align: center;">45.05</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.55</td>
<td style="text-align: center;">64.84</td>
<td style="text-align: center;">54.28</td>
</tr>
<tr>
<td style="text-align: center;">Codex 3-shots</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">22.44</td>
<td style="text-align: center;">62.26</td>
<td style="text-align: center;">50.29</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">32.43</td>
<td style="text-align: center;">69.73</td>
<td style="text-align: center;">55.21</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison to approaches that retrieve examples (Parvez et al., 2021; Pasupat et al., 2021)</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">CMD Acc (\%)</th>
<th style="text-align: center;">EM (\%)</th>
<th style="text-align: center;">Token F1</th>
<th style="text-align: center;">charBLEU</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GPT-Neo-125M</td>
<td style="text-align: center;">+ExPrompting</td>
<td style="text-align: center;">6.68</td>
<td style="text-align: center;">0.32</td>
<td style="text-align: center;">20.49</td>
<td style="text-align: center;">11.15</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">$\mathbf{2 5 . 3 2}$</td>
<td style="text-align: center;">$\mathbf{3 . 5 6}$</td>
<td style="text-align: center;">$\mathbf{3 1 . 2 3}$</td>
<td style="text-align: center;">$\mathbf{2 4 . 4 3}$</td>
</tr>
<tr>
<td style="text-align: left;">GPT-Neo-1.3B</td>
<td style="text-align: center;">+ExPrompting</td>
<td style="text-align: center;">14.01</td>
<td style="text-align: center;">2.8</td>
<td style="text-align: center;">30.07</td>
<td style="text-align: center;">22.11</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">$\mathbf{2 7 . 5 9}$</td>
<td style="text-align: center;">$\mathbf{9 . 0 5}$</td>
<td style="text-align: center;">$\mathbf{3 7 . 2 4}$</td>
<td style="text-align: center;">$\mathbf{3 0 . 5 7}$</td>
</tr>
</tbody>
</table>
<p>tldr project. We conducted an oracle experiment where we provided T5 (which was the strongest model using DocPrompting) and Codex with the oracle command name (e.g., awk). This oracle information is provided to both the baseline and the model that uses DocPrompting. The results are shown on the bottom part of Table 1. When the oracle command is given, DocPrompting further improves over the base models. For example, when providing Codex with the ground truth command name, DocPrompting improves its exact match from $22.44 \%$ to $32.43 \%$.
Should we retrieve documentation or examples? All existing retrieval-based models of code retrieve NL-code pairs or code snippets, rather than documentation. To simulate this scenario, we followed Parvez et al. (2021) and Pasupat et al. (2021) to retrieve NL-code pairs from the training set of tldr, and refer to this baseline as ExPrompting. We finetuned the best retriever RoBERTa and two generators, and retrieved the top-30 NL-code pairs for every example. As shown in Table 2, retrieving documentation (DocPrompting) provides much higher gains than retrieving examples (ExPrompting). Theoretically, adding examples of unseen commands can help ExPrompting generalize to them as well. However, new libraries and functions may not have available examples on the web yet, while documentation often does becomes available when the library is released.</p>
<h1>5.2 Python Programming Results</h1>
<p>Table 3 shows the results on CoNaLa. CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5. ${ }^{4}$ When measuring the recall of the generated function names, the benefit of DocPrompting is especially higher for unseen functions (recall ${ }_{\text {unseen }}$ ). For example, DocPrompting achieves 18.30 compared to only 9.03 of the base CodeT5 in unseen functions. Additionally, DocPrompting improves in-context learning setting with Codex.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 3: Results on CoNaLa, using a CodeT5 retriever with top-10 retrieved docs. Function recall (Recall) measures how many functions in the reference code are correctly predicted, and unseen function recall (Recall ${ }_{\text {unseen }}$ ) only considers the subset held out from the training data.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">BLEU</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">Recall $_{\text {unseen }}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Codex 3-shots</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">43.16</td>
<td style="text-align: center;">39.52</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ DocPrompting</td>
<td style="text-align: center;">43.47</td>
<td style="text-align: center;">39.87</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ DocPrompting oracle docs</td>
<td style="text-align: center;">50.59</td>
<td style="text-align: center;">57.84</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">T5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">28.07</td>
<td style="text-align: center;">14.36</td>
<td style="text-align: center;">2.57</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ DocPrompting</td>
<td style="text-align: center;">30.04</td>
<td style="text-align: center;">21.34</td>
<td style="text-align: center;">8.24</td>
</tr>
<tr>
<td style="text-align: center;">CodeT5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">34.57</td>
<td style="text-align: center;">24.24</td>
<td style="text-align: center;">9.03</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ DocPrompting</td>
<td style="text-align: center;">36.22</td>
<td style="text-align: center;">27.80</td>
<td style="text-align: center;">18.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+ DocPrompting oracle docs</td>
<td style="text-align: center;">49.04</td>
<td style="text-align: center;">72.20</td>
<td style="text-align: center;">63.91</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Pass@ $k$ of CodeT5 with and without DocPrompting on 100 CoNaLa examples.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Using documentation significantly increases the $n$-gram overlap recall between the input and the output, in tldr and CoNaLa.</p>
<p>We hypothesis that the minor gain is mainly due to the potential data leakage of Codex, which violates the split of seen and unseen functions. Another reason is that a strong generator such as Codex may require an equally strong retriever as well. We find that Codex can achieve even higher results with an oracle retriever, which shows the potential further improvement by improving the retrievers. Finally, CodeT5 performs better than T5, with and without using DocPrompting. This emphasizes the importance of using code-specific pretrained models.</p>
<p>Execution-based evaluation The results are shown in Figure 3. Using DocPrompting consistently outperforms the baseline CodeT5 for all values of pass@ $k$. For example, DocPrompting yields 2.85\% improvement on pass@1 and 4.45\% improvement on pass@5, which are realistic numbers of completions that can be suggested in an IDE. When $k=200$, DocPrompting widens the gap to $8.38 \%$. These results demonstrate that DocPrompting does not only improve the quality of the generated code in its surface form, but also increase its functional correctness. Additional details and results are provided in Appendix G.</p>
<h1>6 ANALYSIS</h1>
<h3>6.1 WHY DOES READING THE DOCUMENTATION HELP GENERATING MORE ACCURATE CODE?</h3>
<p>We believe that one of the major reasons is that documentation eases the mapping between NL intents and code, since the documentation contains both NL descriptions and function signatures. We calculated the n-gram overlap between the NL intents and their corresponding code snippets (NL $\leftrightarrow$ code), and the overlap between the NL intents with their top-10 retrieved documents and their code snippets ((NL+docs) $\leftrightarrow$ code). As shown in Figure 4, adding documentation significantly increases the overlap across $n$-grams, and increase, for example, the unigram overlap from $12 \%$ to</p>
<p>Table 4: Retrieval performance of multiple models on the dev set of t ldr (top) and CoNaLa (bottom). RoBERTa is the best model taken from from Gao et al. (2021), and CodeT5 is the encoder of CodeT5base (Wang et al., 2021). Models with the subscript "off-shelf" are the off-the-shelf models, and the other models were finetuned with the objective in Equation 3. The last column is the best model (RoBERTa for t ldr and CodeT5 for CoNaLa) trained without the weak supervision corpus.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">n</th>
<th style="text-align: center;">BM25</th>
<th style="text-align: center;">RoBERTaoff-shelf</th>
<th style="text-align: center;">RoBERTa</th>
<th style="text-align: center;">CodeT5 off-shelf</th>
<th style="text-align: center;">CodeT5</th>
<th style="text-align: center;">Best w/o weak sup.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">tldr</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">32.81</td>
<td style="text-align: center;">17.53</td>
<td style="text-align: center;">30.03</td>
<td style="text-align: center;">10.45</td>
<td style="text-align: center;">18.10</td>
<td style="text-align: center;">28.30</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">51.73</td>
<td style="text-align: center;">37.89</td>
<td style="text-align: center;">52.50</td>
<td style="text-align: center;">20.26</td>
<td style="text-align: center;">38.52</td>
<td style="text-align: center;">50.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">59.86</td>
<td style="text-align: center;">46.80</td>
<td style="text-align: center;">60.33</td>
<td style="text-align: center;">25.73</td>
<td style="text-align: center;">51.03</td>
<td style="text-align: center;">59.84</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">62.01</td>
<td style="text-align: center;">56.11</td>
<td style="text-align: center;">64.30</td>
<td style="text-align: center;">33.65</td>
<td style="text-align: center;">57.26</td>
<td style="text-align: center;">62.30</td>
</tr>
<tr>
<td style="text-align: center;">CoNaLa</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">3.01</td>
<td style="text-align: center;">4.46</td>
<td style="text-align: center;">13.49</td>
<td style="text-align: center;">4.60</td>
<td style="text-align: center;">16.54</td>
<td style="text-align: center;">10.51</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">7.16</td>
<td style="text-align: center;">7.58</td>
<td style="text-align: center;">26.38</td>
<td style="text-align: center;">8.63</td>
<td style="text-align: center;">42.35</td>
<td style="text-align: center;">21.15</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">9.73</td>
<td style="text-align: center;">10.93</td>
<td style="text-align: center;">34.86</td>
<td style="text-align: center;">12.25</td>
<td style="text-align: center;">55.81</td>
<td style="text-align: center;">29.34</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">11.46</td>
<td style="text-align: center;">13.89</td>
<td style="text-align: center;">45.46</td>
<td style="text-align: center;">18.46</td>
<td style="text-align: center;">66.79</td>
<td style="text-align: center;">42.21</td>
</tr>
</tbody>
</table>
<p>$24 \%$ in t ldr. That is, one of the reasons that retrieving documentation helps generating accurate code is that documentation bridges the gap between the "intent terminology" and the "code terminology".</p>
<h1>6.2 Ablation Study</h1>
<p>We compared different configurations of the retriever, to gather more insights for effective DocPrompting. Table 4 shows a comparison between different retrievers and their setups. First, the performance of BM25 varies among datasets: In t ldr, BM25 matches the recall of trained dense retrievers; however in CoNaLa, BM25 achieves only recall@10 of $9.73 \%$, and strong dense retrievers such as the encoder of CodeT5 achieve recall@10 of 55.81 . We hypothesize that this difference between datasets stems from the ways these datasets were created: t ldr intents were written based on existing Bash commands and manuals; while CoNaLa examples were mined from StackOverflow posts, where users ask questions with limited or no context. Thus, NL intents in CoNaLa require a better semantic alignment with the documents, and thus benefit from dense retrievers. The gap resulting from different data curation processes was also observed by Rodriguez and Boyd-Graber (2021) in open-domain question answering (QA).</p>
<p>Second, retrievers that were pretrained on the target programming language are generally stronger. For example in CoNaLa, CodeT5 which was pretrained on Python, is both a better off-the-shelf retriever and a better finetuned-retriever than RoBERTa, which was pretrained mainly on text. In contrast, t ldr is based on Bash, which neither CodeT5 nor RoBERTa were explicitly pretrained on. Thus, t ldr benefits mostly from BM25 and RoBERTa rather than CodeT5 as retrievers.</p>
<p>Finally, training the retriever using weak supervision on the documentation pool (Section 3.1) dramatically improves the retriever. The recall of the best retrievers of each dataset without this corpus is shown in the last column of Table 4 ("Best w/o weak sup."). On CoNaLa, removing this corpus results in severe performance degradation. One possible explanation is that this weak supervision helps the retriever perform domain adaptation more effectively.</p>
<h3>6.3 CASE STUDY</h3>
<p>We examine the models' outputs and show two representative examples in Table 5. In the first example, Image.open was not seen in the training set, and the baseline CodeT5 incorrectly predicts os.open. In contrast, using DocPrompting allows to retrieve the docs and to correctly predict Image.open. In the second example, df.to_csv was not seen in training, and the baseline CodeT5 fails to correctly predict it. In contrast, DocPrompting does predict most of the df.to.csv call correctly, thanks to the retrieved docs. Nevertheless, DocPrompting generates an incorrect argument skiprows=1, instead of header=False. The reason is that along with the retrieved documentation of df.to.csv, the retriever also retrieved the documentation of df.read.csv, which has a skiprows argument. That is, the generator uses an argument of df. read.csv with the function df.to.csv. Further improving the retrievers and the generators, and post-filtering based on the validity of argument names, may mitigate such mistakes.</p>
<p>Table 5: Examples of predictions from CoNaLa, of the base CodeT5 compared to CodeT5+DocPrompting. Unseen functions are underscored.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">NL Intent: Open image "picture.jpg"</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Ground truth:</td>
<td style="text-align: left;">img = Image.open('picture.jpg') \n Img.show</td>
</tr>
<tr>
<td style="text-align: left;">CodeT5:</td>
<td style="text-align: left;">os.open('picture.jpg', 'r')</td>
</tr>
<tr>
<td style="text-align: left;">CodeT5+DocPrompting:</td>
<td style="text-align: left;">image = Image.open('picture.jpg', 'rb')</td>
</tr>
<tr>
<td style="text-align: left;">NL Intent: Exclude column names when writing dataframe 'df' to a csv file 'filename.csv'</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Ground truth:</td>
<td style="text-align: left;">df.to.csv ('filename.csv', header=False)</td>
</tr>
<tr>
<td style="text-align: left;">CodeT5:</td>
<td style="text-align: left;">df.drop(['col1', 'col2'], axis=1, inplace=True)</td>
</tr>
<tr>
<td style="text-align: left;">CodeT5+DocPrompting:</td>
<td style="text-align: left;">df.to.csv('filename.csv', skiprows=1)</td>
</tr>
</tbody>
</table>
<h1>7 Related Work</h1>
<p>Code generation The most common practice in NL $\rightarrow$ code generation is training a model on a dataset of NL-code pairs (Allamanis et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al., 2018). Nevertheless, all these works assume that their training corpus covers all required libraries and functions, and their models are inherently incapable of generating libraries and functions that were not seen in the training data. On the contrary, DocPrompting allows models to generate calls to unseen function, by retrieving these functions' documentation and reading them at test time. Hayati et al. (2018); Parvez et al. (2021); Hashimoto et al. (2018) and Lu et al. (2017) learn to retrieve examples at test time; Pasupat et al. (2021) also considered settings where the test data has a distribution shift from the training data. However, when new libraries are released they often come with documentation, and thus we assume that documentation for new libraries is much more likely to be available than concrete natural language intent and code snippet pairs $(n, c)$ that use these libraries already. The models of Shrivastava et al. and Wu et al. (2021) retrieve code snippets from relevant files in the same project; contrarily, when predicting new libraries and functions that are external to the user's project, documentation is the source that is the most likely to be available.</p>
<p>Retrieval augmented generation The paradigm of retrieve-then-generate has gained popularity in the field of open-domain question answering (Guu et al., 2020; Lewis et al., 2020; Karpukhin et al., 2020), where the answer for an open-domain question exists in only few documents out of a much larger pool. Although DocPrompting takes a similar approach, documentation retrieval in code generation is even more valuable, since code libraries are updated constantly, and new libraries are introduced daily. Thus, DocPrompting allows updating the documentation pool frequently with new contents, without re-training any model components.</p>
<p>Documentation conditioned generation The model of Zhong et al. (2019) reads documents to understand environment dynamics in a grid-world game, and Branavan et al. (2011) controls situated agents in a game (Civilization II) by reading the game's manual. However, all their models were tailored to specific games; in contrast, DocPrompting is general and is applicable for a variety of programming languages and datasets.</p>
<h2>8 CONCLUSION</h2>
<p>We propose DocPrompting, a simple and effective approach for code generation by retrieving the relevant documentation. DocPrompting consistently improves NL $\rightarrow$ code models in two tasks, in two PLs, and across multiple strong base models. DocPrompting improves strong base models such as CodeT5 by $2.85 \%$ in pass@1 ( $52 \%$ relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to $6.9 \%$ exact match, and Codex by 6.78 charBLEU score.</p>
<p>These results open a promising direction for NL $\rightarrow$ code generation. We believe that our results can be further improved using more clever encoding of the structured nature of long documents, and using joint training of the retriever and the generator, which hopefully will avoid cascading errors. Further, we believe that the principles and the methods presented in this paper are applicable to additional code-related tasks, and other documentation-like resources such as tutorials and blog posts. To these ends, we make all our code, data, and models publicly available.</p>
<h1>9 ACKNOWLEDGEMENT</h1>
<p>We thanks the anonymous reviewers for their useful comments and suggestions. This work is supported by a gift from Amazon AI and a contract from the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.</p>
<h2>REFERENCES</h2>
<p>Miltiadis Allamanis, Daniel Tarlow, Andrew D. Gordon, and Yi Wei. Bimodal modelling of source code and natural language. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pages 2123-2132. JMLR.org, 2015. URL http://proceedings.mlr . press/v37/allamanis15.html.</p>
<p>Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In International conference on machine learning, pages 245-256. PMLR, 2020.</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint, abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732.</p>
<p>Nathanaël Beau and Benoît Crabbé. The impact of lexical and grammatical processing on generating code from natural language. ArXiv preprint, abs/2202.13972, 2022. URL https://arxiv.org/abs/2202. 13972 .</p>
<p>Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, 2021. URL https://doi.org/10.5281/zenodo. 5297715 . If you use this software, please cite it using these metadata.
S.R.K. Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a Monte-Carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 268-277, Portland, Oregon, USA, 2011. Association for Computational Linguistics. URL https://aclanthology.org/P11-1028.</p>
<p>Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr Polozov. Generative code modeling with graphs. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=Bke4KsA5FX.</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https://arxiv.org/abs/2107.03374.</p>
<p>Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1597-1607. PMLR, 2020. URL http://proceedings.mlr.press/v119/chen20j.html.</p>
<p>Andrew Forward and Timothy C Lethbridge. The relevance of software documentation, tools and technologies: a survey. In Proceedings of the 2002 ACM symposium on Document engineering, pages 26-33, 2002.</p>
<p>Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and synthesis. ArXiv preprint, abs/2204.05999, 2022. URL https://arxiv.org/abs/2204.05999.</p>
<p>Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / 2021$.emnlp-main.552. URL https://aclanthology.org/2021.emnlp-main. 552 .</p>
<p>Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In International Conference on Machine Learning, 2020. URL https://arxiv.org/abs/1908.10396.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-augmented language model pre-training. ArXiv preprint, abs/2002.08909, 2020. URL https://arxiv.org/abs/ 2002.08909 .</p>
<p>Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. Advances in Neural Information Processing Systems, 31, 2018.</p>
<p>Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin, Anthony Tomasic, and Graham Neubig. Retrieval-based neural code generation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 925-930, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1111. URL https://aclanthology.org/D18-1111.</p>
<p>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751, 2019.</p>
<p>Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. Mapping language to code in programmatic context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1643-1652, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1192. URL https://aclanthology.org/D18-1192.</p>
<p>Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880, Online, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.eacl-main.74. URL https://aclanthology.org/2021.eacl-main. 74.</p>
<p>Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):535-547, 2019.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.550. URL https://aclanthology.org/2020.emnlp-main.550.</p>
<p>Timothy C Lethbridge, Janice Singer, and Andrew Forward. How software engineers use documentation: The state of the practice. IEEE software, 20(6):35-39, 2003.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrievalaugmented generation for knowledge-intensive NLP tasks. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 6b493230205f780e1bc26945df7481e5-Abstract.html.</p>
<p>Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D. Ernst. NL2Bash: A corpus and semantic parser for natural language interface to the linux operating system. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, 2018. European Language Resources Association (ELRA). URL https://aclanthology.org/L18-1491.</p>
<p>Yanxin Lu, Swarat Chaudhuri, Chris Jermaine, and David Melski. Data-driven program completion. arXiv preprint arXiv:1705.09042, 2017.</p>
<p>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. A conversational paradigm for program synthesis. arXiv preprint, 2022.</p>
<p>Janet Nykaza, Rhonda Messinger, Fran Boehme, Cherie L Norman, Matthew Mace, and Manuel Gordon. What programmers really want: results of a needs assessment for sdk documentation. In Proceedings of the 20th annual international conference on Computer documentation, pages 133-141, 2002.</p>
<p>Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval augmented code generation and summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2719-2734, Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.232. URL https://aclanthology.org/2021. findings-emnlp. 232 .</p>
<p>Panupong Pasupat, Yuan Zhang, and Kelvin Guu. Controllable semantic parsing via retrieval augmentation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7683-7698, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / 2021$.emnlp-main.607. URL https://aclanthology.org/2021.emnlp-main. 607 .</p>
<p>Maxim Rabinovich, Mitchell Stern, and Dan Klein. Abstract syntax networks for code generation and semantic parsing. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1139-1149, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: $10.18653 / \mathrm{v} 1 / \mathrm{P} 17-1105$. URL https://aclanthology.org/P17-1105.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. ArXiv preprint, abs/1910.10683, 2019. URL https://arxiv.org/abs/1910.10683.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67, 2020.</p>
<p>Stephen E Robertson and K Sparck Jones. Relevance weighting of search terms. Journal of the American Society for Information science, 27(3):129-146, 1976.</p>
<p>Pedro Rodriguez and Jordan Boyd-Graber. Evaluation paradigms in question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9630-9642, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021. emnlp-main.758. URL https://aclanthology.org/2021.emnlp-main.758.</p>
<p>Tobias Roehm, Rebecca Tiarks, Rainer Koschke, and Walid Maalej. How do professional developers comprehend software? In 2012 34th International Conference on Software Engineering (ICSE), pages 255-265. IEEE, 2012.</p>
<p>Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I. Wang. Natural language to code translation with execution, 2022. URL https://arxiv.org/abs/2204.11454.</p>
<p>Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large language models of code. In ICML 2022 Workshop on Knowledge Retrieval and Language Models.</p>
<p>Yue Wang, Weishi Wang, Shafiq Joty, and Steven C.H. Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8696-8708, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.685. URL https://aclanthology.org/2021.emnlp-main.685.</p>
<p>Zhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for open-domain code generation. arXiv preprint arXiv:2212.10481, 2022.</p>
<p>Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. In International Conference on Learning Representations, 2021.</p>
<p>Frank F. Xu, Zhengbao Jiang, Pengcheng Yin, Bogdan Vasilescu, and Graham Neubig. Incorporating external knowledge through pre-training for natural language to code generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6045-6052, Online, 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.538. URL https://aclanthology.org/ 2020.acl-main.538.</p>
<p>Frank F Xu, Uri Alon, Graham Neubig, and Vincent J Hellendoorn. A systematic evaluation of large language models of code. ArXiv preprint, abs/2202.13169, 2022. URL https://arxiv.org/abs/2202. 13169 .</p>
<p>Pengcheng Yin and Graham Neubig. A syntactic neural model for general-purpose code generation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 440-450, Vancouver, Canada, 2017. Association for Computational Linguistics. doi: 10. 18653/v1/P17-1041. URL https://aclanthology.org/P17-1041.</p>
<p>Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. Learning to mine aligned code and natural language pairs from stack overflow. In 2018 IEEE/ACM 15th international conference on mining software repositories (MSR), pages 476-486. IEEE, 2018.</p>
<p>Victor Zhong, Tim Rocktäschel, and Edward Grefenstette. Rtfm: Generalising to novel environment dynamics via reading. ArXiv preprint, abs/1910.08210, 2019. URL https://arxiv.org/abs/1910.08210.</p>
<h1>A tldr: A Newly Curated Shell Scripting Benchmark</h1>
<p>NL $\rightarrow$ Bash pairs For each command (e.g., cat), users contribute examples of pairs of NL descriptions and bash code (mainly one-liners), including various flags and arguments, which cover the common usages of that command. An example is shown in Figure 2.</p>
<p>We crawl NL-code pairs from the markdown files ${ }^{5}$ in the linux and common folders. We discard Bash commands whose manual is unavailable (discussed below). The detailed statistics are shown in Table 6. On average, each command has 4.84 NL $\rightarrow$ Bash pairs and there is a total of 9187 NL-code pairs. To test the generalizability of a model, we construct the training, development and the test set with completely different commands.</p>
<p>Table 6: The statistics of the tldr shell scripting benchmark</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;"># Commands</th>
<th style="text-align: center;">NL $\rightarrow$ Bash pairs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">train</td>
<td style="text-align: center;">1315</td>
<td style="text-align: center;">6414</td>
</tr>
<tr>
<td style="text-align: left;">dev</td>
<td style="text-align: center;">376</td>
<td style="text-align: center;">1845</td>
</tr>
<tr>
<td style="text-align: left;">test</td>
<td style="text-align: center;">188</td>
<td style="text-align: center;">928</td>
</tr>
<tr>
<td style="text-align: left;">total</td>
<td style="text-align: center;">1879</td>
<td style="text-align: center;">9187</td>
</tr>
</tbody>
</table>
<p>Documentation pool $\mathcal{D}$ We take the bash manual of the 1897 bash commands in tldr to construct a documentation pool. We search each command name at manned. org ${ }^{6}$, a website which archives Unix manual pages (the same as the Unix 'man <command> command), and then extract the text contents from the returned manual page. We further break each manual into multiple paragraphs by line breaks so that each paragraph delicately describes a single concept such as a command functionality or a flag usage. We make this decision due to the large volume of content each manual has, which is too long to fit the length limitation of a neural model, and too noisy and distracts the model with irrelevant information. This results in 400 k individual entries in the pool in total.</p>
<p>Oracle manual $\mathcal{D}_{i}^{<em>} \quad$ We find the ground truth documentation for each $(n, c)$ pair through command name and flag matching heuristics. For instance, given a code snippet toilet 'input_text' -f 'font_filename', we constrain our search to the documentation from toilet manual page and select documentation that starts with -f flag as an oracle paragraph. Along with the first paragraph that commonly summarizes a command, these paragraphs forms $\mathcal{D}_{n}^{</em>}$.</p>
<p>Evaluation metrics We use four evaluation metrics to measure the quality of the generated code: (a) command name accuracy (CMD Acc) - measures whether the command name (e.g., cat) is predicted correctly; (b) token-level F1 - converts the reference code and the generated code to bag of words and measures the token-level precision, recall, and F1 overlap; (c) exact match (EM) - measures the exact match between the reference and the generation; and (d) character-level BLEU (charBLEU; Lin et al., 2018; Shi et al., 2022). For token level F1, exact match, and charBLEU, we disregard all user-specific variables in the references and the system outputs. For example, "mycli -u [user] -h [host] [database]" is converted into "mycli -u \$1 -h \$2 \$3". This is mainly because the variables are not instantiated in tldr and the style of the placeholder varies among contributors. For example, some contributors might write [user] as [username] or [your_name]. Therefore, measuring the surface form of user-specific variable names is less meaningful.</p>
<h2>B Re-SPLITTING CoNALA</h2>
<p>NL $\rightarrow$ Python pairs We adapt the popular CoNaLa benchmark and re-split the dataset to test the generalization scenario. This re-split makes every example in the development and the test set have at least one Python function (e.g., plt.plot) that was not seen in the training data. There are 2135, 201, and 543 examples in the training, development and test sets, respectively. We follow the original work Yin et al. (2018) to evaluate the system outputs with BLEU-4. Since we focus on the generalization setting, we additionally report unseen function accuracy, which measures the percentage of correctly predicted held-out functions that do not appear in the training set.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Human-annotated unit tests Following Chen et al. (2021) and Austin et al. (2021), we conduct executionbased evaluation on CoNaLa to measure the functional correctness of the generated code. We randomly selected 100 examples from the test set and manually annotated unit test for each example. For example, we wrote tests such as assert gen_code ("abcds", 2) == 4 and assert gen_code ("abde", 2) == -1 to verify whether the function gen_code could perform "find the index of sub string 's' in string 'str' starting from index 2". Each example was annotated by a single annotator. The annotation was done by two authors of the paper who program with Python daily. On average, we annotate 2.03 unit tests for each example.</p>
<p>Documentation pool $\mathcal{D}$ Our documentation pool contains 35763 manuals. These functions are from all Python libraries that are available on DevDocs . These libraries contains the Python built-in library, and popular libraries like numpy and pandas. The documentation on DevDocs are curated and further transformed and indexed to allow for quick searching of APIs. We then extract each API signature and the corresponding documentation in every library, remove any content in the documentation that is not text, and segment the documentation into multiple paragraphs based on the <p> HTML tags. The documentation pool then contains pairs of the API signature and a single paragraph in the corresponding documentation. Although the documentation pool is not comprehensive to cover all Python libraries and functions, we find it has a high coverage rate on the CoNaLa dataset. This choice reflects the flexibility of our approach upon the characteristics of a target scenario.</p>
<p>Oracle manual $\mathcal{D}<em i="i">{i}^{<em>}$ To find the oracle documents for a given NL intent $\mathcal{D}_{i}^{</em>}$ from the original ${n, c}$ example, we first index the function names with absolute path (e.g., plot is indexed with matplotlib.pyplot.plot) with Elasticsearch. Then we query the search engine with clean version of $c$ where variable name are removed. The top-5 functions after de-duplication are treated as oracle manuals $\mathcal{D}</em>$.}^{*</p>
<p>Natural language and code associations during pretraining Despite our efforts, it is possible that some of the held-out functions in the test set were seen to associate with NL contexts (e.g., comments) during the pretraining of a retriever and a generator. Since the generators were initialized from the same checkpoint in both the baselines and the DocPrompting models, such a possible association is expected to equally help both models. In the retriever, such a possible association did not cause the retriever to see the exact NL intents together with the corresponding documentation, and thus the matching between NL $\leftrightarrow$ doc was not leaked. However, it is possible that there had been semantically similar intents seen along with the code snippets of the held-out functions. Nevertheless, such co-occurrence is "indirect" and "unsupervised".</p>
<h1>C Dense Retriever Training</h1>
<p>We finetune the model for 10 epochs with batch size of 512 and learning rate of $1 e-5$. Since CodeT5 does not use [CLS] token, we alternatively take the average of the hidden state of the last layer as the text representation. For CoNaLa, we also use the first 100k "mined" examples provided as part of CoNaLa as the supervised corpus. For CoNaLa, we only apply a single search step because each code snippet commonly contains more than one function. We also observed that using the first sentence that normally summarizes the usage of a function achieve the best retrieval performance than other alternatives such as using the first paragraph, or simply truncating to the maximum token length. The training takes up to 15 hours on a single A6000 GPU.</p>
<h2>D Generator Training</h2>
<p>We train our single-source generators for 20 epochs with learning rate $4 e-5$. We train our FiD-based generators for 10000 steps. The doc length is set to 200, any further content will be truncated. We follow (Izacard and Grave, 2021) to set learning rate to $5 e-5$ with 2000 steps warmup and linear learning rate decay. The batch size is set to 8 . The best model is selected based on the token-level F1 score on the development set for tldr and BLEU score for CoNaLa. The training takes 8 hours on a single A6000 GPU.</p>
<h2>E CODEX PROMPTS</h2>
<p>For the baseline, we prompt Codex with three NL-code pairs and append the test query to the end. An example on tldr is shown on top of Table 7. On the bottom, we list the prompt with DocPrompting where documentation is provided along too. In the oracle command name setting, we prepend the command name before each NL</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: The recall@ $k(\%)$ and the corresponding BLEU score by using these top- $k$ docs on CoNaLa dataset (using CodeT5).
intent for the baseline prompt. For DocPrompting prompt, we replace the potential docs with the retrieved docs from the oracle manual.</p>
<h1>F ADDITIONAL ANALYSIS</h1>
<p>Parameter efficiency As shown in Table 1, under a given parameter budget, we find that DocPrompting mostly benefits from parallel encoding (FiD). For example, the parallel encoding T5+DocPrompting (220M parameters) significantly outperforms the 125M parameters joint encoding Neo-125M+DocPrompting. Only scaling up Neo+DocPrompting to 1.3B parameters manages to match the 220M parameter T5+DocPrompting. A possible explanation is that although the base Neo-1.3B (without DocPrompting) generally performs better than the base T5 (without DocPrompting), parallel encoding allows to utilize the retrieved documents better, since documents are encoded independently on the encoder side.</p>
<p>The impact of the number of documents Figure 5 shows the recall@ $k$ and the BLEU score compared to $k$, the number of retrieved documents. Increasing $k$ consistently yields a higher recall; however, as more irrelevant documents are retrieved, the generator cannot effectively distinguish them from the relevant ones and the overall performance remain similar. For example, CodeT5 achieves the highest BLEU score using $5 \leq k \leq 10$. In contrast, when the generator is provided with the oracle docs only, its BLEU score reaches 49.04 (Table 3). This suggests that both precision and recall of docs are important, and the benefit of using larger values of $k$ in open domain QA (Izacard and Grave, 2021) does not necessarily hold in code generation.</p>
<p>Full $n$-gram overlap Table 8 shows that using documentation significantly increases the $n$-gram overlap recall between the input and the output, in tldr and CoNaLa. Since we used BM25 to retrieve docs in tldr, the NL++Retrieved docs overlap is high by construction. In CoNaLa, the NL++Retrieved docs unigram overlap is high as well, but since we used a dense retriever, the general n-gram overlap does not have to be high for DocPrompting to work well.</p>
<p>Retrieval latency Although retrieving docs results in additional test-time computation, the increase in latency is not prohibitive. First, encoding the input for the retrieval step "costs" a single forward pass through the retriever's encoder, which is significantly less expensive than generation (which requires multiple time steps of the decoder). All the documentation in the retrieval pool can be encoded in advance, and finding the top- $k$ results can be performed quickly using libraries such as FAISS Johnson et al. (2019) on the GPU or ScaNN Guo et al. (2020) on CPU. The cost of this top- $k$ search is sub-linear in the size of the document pool. Second, the additional input to the generator results in an increased memory consumption, but only a small increase in latency since the tokens of a given input can be encoded in parallel. If this difference is crucial in practical settings, we can decrease the number of retrieved documents. Figure 5 shows that retrieving as few as five docs may be sufficient in many cases.</p>
<h2>G Full Pass@ $k$ Plots</h2>
<p>In the main execution-based evaluation, pass@ $k$ results in Section 5.2 and Figure 3, we took the best temperature for every model and value of $k$. Here, we show all the pass@ $k$ plots with different temperatures in Figure 6.</p>
<div class="codehilite"><pre><span></span><code><span class="err">#</span><span class="w"> </span><span class="nx">get</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">label</span><span class="w"> </span><span class="nx">of</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">fat32</span><span class="w"> </span><span class="nx">partition</span>
<span class="nx">fatlabel</span><span class="w"> </span><span class="o">/</span><span class="nx">dev</span><span class="o">/</span><span class="nx">sdal</span>
<span class="err">#</span><span class="w"> </span><span class="nx">END</span>
<span class="err">#</span><span class="w"> </span><span class="nx">display</span><span class="w"> </span><span class="nx">information</span><span class="w"> </span><span class="nx">without</span><span class="w"> </span><span class="nx">including</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">login</span><span class="p">,</span><span class="w"> </span><span class="nx">jcpu</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="nx">pcpu</span><span class="w"> </span><span class="nx">columns</span>
<span class="nx">w</span><span class="w"> </span><span class="o">--</span><span class="nx">short</span>
<span class="err">#</span><span class="w"> </span><span class="nx">END</span>
<span class="err">#</span><span class="w"> </span><span class="nx">sort</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">csv</span><span class="w"> </span><span class="nx">file</span><span class="w"> </span><span class="nx">by</span><span class="w"> </span><span class="nx">column</span><span class="w"> </span><span class="mi">9</span>
<span class="nx">csvsort</span><span class="w"> </span><span class="o">-</span><span class="nx">c</span><span class="w"> </span><span class="mi">9</span><span class="w"> </span><span class="nx">data</span><span class="p">.</span><span class="nx">csv</span>
<span class="err">#</span><span class="w"> </span><span class="nx">END</span>
<span class="err">#</span><span class="w"> </span><span class="nx">search</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="kn">package</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="nx">your</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">sources</span>
</code></pre></div>

<p>Potential document 0: fatlabel will display or change the volume label or volume ID on the MS- DOS filesystem located on DEVICE ...
# get the label of a fat32 partition
fatlabel /dev/sdal
# END
Potential document 0: w displays information about the users currently on the machine, and their processes. The header shows, in this order ...</p>
<p>Potential document 1: -s, -short Use the short format. Don't print the login time, JCPU or PCPU times.
# display information without including the login, jcpu and pcpu columns
w --short
# END
Potential document 0: Sort CSV files. Like the Unix "sort" command, but for tabular data
Potential document 1: usage: csvsort [-h] [-d DELIMITER] [-t] [-q QUOTECHAR] [-u 0,1,2,3] [-b] [-p ESCAPECHAR] ...</p>
<p>Potential document 2: optional arguments: -h, -hel show this help message and exit -n, -names Display column names and indices from the input CSV and exit. -c COLUMNS ...</p>
<p>Potential document 3: csvsort -c 9 examples/realdata/FY09_EDU_Recipients_by_State.csv
Potential document 4: csvcut -c 1,9 examples/realdata/FY09_EDU_Recipients_by_State.csv — csvsort -r -c 2 - head -n 5
# sort a csv file by column 9
csvsort -c 9 data.csv
# END
Potential document 1: ...
Potential document 2: ...
# search for a package in your current sources</p>
<p>Table 7: Top: baseline Codex prompt with three NL-code pairs and a test intent. Bottom: DocPrompting prompt for Codex. In each in-context learning example, the oracle docs, the NL intent and the corresponding bash command are provided. We use up to five oracle docs for these examples. For a test example, the top-5 paragraphs from the retriever are represented with the NL intent. The documents' contents were omitted ("...") to save space.</p>
<p>Table 8: $n$-gram overlap between different contents (%). Using documentation significantly increases the $n$-gram overlap recall between the input and the output, in tldr and CoNaLa.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">tldr</th>
<th style="text-align: center;">1</th>
<th style="text-align: center;">2</th>
<th style="text-align: center;">3</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NL↔Code</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">(NL+retrieved docs)↔Code</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: left;">NL↔Retrieved docs</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">3</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: left;">CoNaLa</th>
<th style="text-align: left;">1</th>
<th style="text-align: left;">2</th>
<th style="text-align: left;">3</th>
<th style="text-align: left;">4</th>
<th style="text-align: left;">5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NL↔Code</td>
<td style="text-align: left;">30</td>
<td style="text-align: left;">14</td>
<td style="text-align: left;">11</td>
<td style="text-align: left;">9</td>
<td style="text-align: left;">7</td>
</tr>
<tr>
<td style="text-align: left;">(NL+retrieved docs)↔Code</td>
<td style="text-align: left;">91</td>
<td style="text-align: left;">52</td>
<td style="text-align: left;">28</td>
<td style="text-align: left;">16</td>
<td style="text-align: left;">11</td>
</tr>
<tr>
<td style="text-align: left;">NL↔Retrieved docs</td>
<td style="text-align: left;">72</td>
<td style="text-align: left;">14</td>
<td style="text-align: left;">3</td>
<td style="text-align: left;">1</td>
<td style="text-align: left;">1</td>
</tr>
</tbody>
</table>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Pass@ $k$ on 100 examples on the test set with different temperatures.</p>
<p>Table 9: Results on tldr and CoNaLa with code-davinci-002.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">tldr</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Model</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">CMD Acc (\%)</td>
<td style="text-align: center;">EM (\%)</td>
<td style="text-align: center;">Token F1</td>
<td style="text-align: center;">charBLEU</td>
</tr>
<tr>
<td style="text-align: center;">Codex</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">39.01</td>
<td style="text-align: center;">14.55</td>
<td style="text-align: center;">44.89</td>
<td style="text-align: center;">33.93</td>
</tr>
<tr>
<td style="text-align: center;">3-shots</td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">36.10</td>
<td style="text-align: center;">13.97</td>
<td style="text-align: center;">42.55</td>
<td style="text-align: center;">32.93</td>
</tr>
<tr>
<td style="text-align: center;">With the oracle command name</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">20.22</td>
<td style="text-align: center;">59.22</td>
<td style="text-align: center;">38.14</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">+DocPrompting</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">33.15</td>
<td style="text-align: center;">68.59</td>
<td style="text-align: center;">44.76</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">CoNaLa</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">BLEU</td>
<td style="text-align: center;">Recall</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">-</td>
<td style="text-align: center;">48.39</td>
<td style="text-align: center;">43.35</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ DocPrompting</td>
<td style="text-align: center;">47.21</td>
<td style="text-align: center;">44.70</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">+ DocPrompting oracle docs</td>
<td style="text-align: center;">54.67</td>
<td style="text-align: center;">59.68</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<h1>H EXPERIMENTS WITH code-davinci-002</h1>
<p>The results with code-davinci-002 under few-shot learning setting is shown in Table 9. In the non-oracle settings, Codex+DocPrompting did not improve over the base Codex; one explanation might be that the datasets are leaked into the training corpus of the Codex. For example, CoNaLa was extracted from StackOverflow, which is included in the large CommonCrawl corpus ${ }^{8}$ that was used to train GPT-3, and possibly Codex. Therefore, Codex might have memorized the target code, and thus did not need the additional documentation. Although the data leakage issue might have happened in code-davinci-001 as well, we suspect that this issue has worsened in the stronger 002 version. Regardless, we believe that the large capacity of Codex requires an equally strong retriever to improve over the base model. With an oracle retriever, DocPrompting yields significant improvement on both datasets. Thus, the non-oracle results could be further improved using a stronger non-oracle retriever.</p>
<h2>I EXAMPLES</h2>
<h2>I. 1 TLDR</h2>
<p>Examples on tldr are in Table 10. In the top three cases, the baseline T5 could not generate the correct bash command while T5+DocPrompting retrieves the correct bash documentation and use the correct arguments. In the bottom two cases, T5 also predicts the bash command correctly. However, in the puppet case, it misses concrete usages of executing the code. In the elixir case, it halluciate the argument flags --example and --arguments which elixir does not support at all. On the other hand, T5+DocPrompting does ground its generation to the retrieved docs and therefore it generates more accurate code.</p>
<h2>I. 2 CoNaLa</h2>
<p>More examples on CoNaLa are shown in Table 11.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 10: Examples of predictions from tldr, of the base T5 compared to T5+DocPrompting. In tldr, the bash commands in the test set are completely different from those in the training set.</p>
<div class="codehilite"><pre><span></span><code>play a midi file in a specific key ( \(0=\mathrm{c}\) major/a minor, \(1=\mathrm{f}\) major/d minor, \(+1=\mathrm{g}\) major/e minor, etc.)
Ground truth: timidity --force-keysig=&#39;-flats|+sharps&#39; &#39;path/to/file.mid&#39;
T5: midiplay &#39;key&#39;
T5+DocPrompting: timidity --force-keysig &#39;f|f|f&#39; &#39;path/to/file.midi&#39;
</code></pre></div>

<p>clean up all temporary tex files in the current directory
Ground truth: latexmk -c
T5: tex clean
T5+DocPrompting: latexmk -c
specify the hash algorithm for the index file (experimental)
Ground truth: git show-index --object-format='sha1|sha256' 'path/to/file'
T5: hash --experimental 'path/to/index.hashes'
T5+DocPrompting: git show-index --object-format= 'path/to/hash-algorithm'
'path/to/experimental'
execute puppet code
Ground truth: puppet apply --execute 'code'
T5: puppet
T5+DocPrompting: puppet apply --execute 'path/to/file.puppet'
evaluate elixir code by passing it as an argument
Ground truth: elixir -e 'code'
T5: elixir --example --arguments 'path/to/file.elixir'
T5+DocPrompting: elixir -e 'path/to/file.elixir'</p>
<p>Table 11: Examples of predictions from CoNaLa, of the base CodeT5 compared to CodeT5+DocPrompting. Unseen functions are underscored.</p>
<div class="codehilite"><pre><span></span><code><span class="nx">set</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">current</span><span class="w"> </span><span class="nx">working</span><span class="w"> </span><span class="nx">directory</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">c</span><span class="p">:</span><span class="err">\</span><span class="nx">Users</span><span class="err">\</span><span class="nx">uname</span><span class="err">\</span><span class="nx">desktop</span><span class="err">\</span><span class="nx">python</span><span class="err">&#39;</span>
<span class="nx">Ground</span><span class="w"> </span><span class="nx">truth</span><span class="p">:</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">chdir</span><span class="p">(</span><span class="err">&#39;</span><span class="nx">c</span><span class="p">:</span><span class="err">\</span><span class="nx">Users</span><span class="err">\</span><span class="nx">uname</span><span class="err">\</span><span class="nx">desktop</span><span class="err">\</span><span class="nx">python</span><span class="err">&#39;</span><span class="p">)</span>
<span class="nx">CodeT5</span><span class="p">:</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">system</span><span class="p">(</span><span class="err">&#39;</span><span class="nx">c</span><span class="p">:</span><span class="err">\</span><span class="nx">Users</span><span class="err">\</span><span class="nx">uname</span><span class="err">\</span><span class="nx">desktop</span><span class="err">\</span><span class="nx">python</span><span class="err">&#39;</span><span class="p">)</span>
<span class="nx">CodeT5</span><span class="o">+</span><span class="nx">DocPrompting</span><span class="p">:</span><span class="w"> </span><span class="nx">os</span><span class="p">.</span><span class="nx">chdir</span><span class="p">(</span><span class="err">&#39;</span><span class="nx">c</span><span class="p">:</span><span class="err">\</span><span class="nx">Users</span><span class="err">\</span><span class="nx">uname</span><span class="err">\</span><span class="nx">desktop</span><span class="err">\</span><span class="nx">python</span><span class="err">&#39;</span><span class="p">)</span>
<span class="nx">convert</span><span class="w"> </span><span class="nx">dataframe</span><span class="w"> </span><span class="err">&#39;</span><span class="nx">df</span><span class="err">&#39;</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">integer</span><span class="o">-</span><span class="k">type</span><span class="w"> </span><span class="nx">sparse</span><span class="w"> </span><span class="nx">object</span>
<span class="nx">Ground</span><span class="w"> </span><span class="nx">truth</span><span class="p">:</span><span class="w"> </span><span class="nx">df</span><span class="p">.</span><span class="nx">to</span><span class="p">.</span><span class="nx">sparse</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nx">CodeT5</span><span class="p">:</span><span class="w"> </span><span class="nx">np</span><span class="p">.</span><span class="nx">isinstance</span><span class="p">(</span><span class="nx">df</span><span class="p">,</span><span class="w"> </span><span class="nx">np</span><span class="p">.</span><span class="nx">integer</span><span class="p">)</span>
<span class="nx">CodeT5</span><span class="o">+</span><span class="nx">DocPrompting</span><span class="p">:</span><span class="w"> </span><span class="nx">df</span><span class="p">.</span><span class="nx">to</span><span class="p">.</span><span class="nx">sparse</span><span class="p">(</span><span class="sc">&#39;i&#39;</span><span class="p">)</span>
</code></pre></div>

<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{8}$ https://commoncrawl.org/the-data/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
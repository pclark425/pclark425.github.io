<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3572 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3572</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3572</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-79.html">extraction-schema-79</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <p><strong>Paper ID:</strong> paper-e391d266b0d43475567f59efeaeabc884a48abd0</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/e391d266b0d43475567f59efeaeabc884a48abd0" target="_blank">ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work proposes ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions and incorporating the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions.</p>
                <p><strong>Paper Abstract:</strong> Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in real-world applications. In this work, we propose ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions. To further enhance the model's robustness and interpretability, we incorporate the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions. Our experimental results demonstrate that ReLM improves the performance of state-of-the-art GNN-based methods across various chemical reaction datasets, especially in out-of-distribution settings. Codes are available at https://github.com/syr-cn/ReLM.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3572.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3572.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large transformer-based pre-trained language model accessed via the OpenAI API; used in this paper as an off-the-shelf LM to perform few-shot, prompt-based multiple-choice chemical reaction product selection and to report confidence scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large autoregressive transformer language model (GPT family). In this paper GPT-3.5 is used via the OpenAI API without further fine-tuning; exact model size and training corpus not specified in the paper (standard GPT-family pretraining referenced).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering / in-context learning: the LM receives a prompt consisting of reactants (SMILES + IUPAC), reaction conditions, a set of K candidate products generated/ranked by a GNN, and a few in‑context examples (including confidence-score annotations). The LM selects/ranks the correct product among the candidates and reports a confidence score (Confidence Score Strategy, CSS).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical reaction product prediction (selecting the correct product from GNN-provided top-K candidates); evaluated on Open Reaction Database (ORD) subsets including Imidazo, NiCOlit, Rexgen-30k, Rexgen-40k.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Top-line accuracy (Acc) on test sets; hit@K upper bound (determined by GNN candidate retrieval); # input tokens and average inference time per query; Spearman rank correlation (rho) between LM fine-grained confidence ranking and GNN candidate ranking; high-vs-low confidence subset accuracies and associated statistical significance (p-values); ablations for inclusion of reaction type/condition.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Using GPT-3.5 within ReLM improved reaction prediction accuracy relative to GNN-only baselines in several OOD datasets. Example: on Imidazo (K=3) ReLM (MolR + GPT-3.5) achieved 0.865 vs MolR alone 0.513. Improvements were dataset- and backbone-dependent and generally below the theoretical hit@K upper bound. GPT-3.5 results were computed on random subsets (n=500) due to API cost limitations. GPT-3.5 + CSS improved accuracy relative to no strategy; high-confidence outputs were significantly more accurate than low-confidence outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>Compared to GNN-only baselines (MolR, LocalRetro), ReLM with GPT-3.5 substantially increased accuracy on many OOD datasets but remained bounded by GNN hit@K. Compared different prompting strategies (MES, JSON, CSS): CSS achieved similar or better accuracy with far lower token/time cost than MES/JSON. ReLM also compared to Vicuna-based ReLM (see separate entry) and to other prompt baselines (zero-shot, CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>GPT-3.5 was not fine-tuned on task data and was used only with prompting; evaluation was restricted to random subsets (n=500) due to cost, limiting statistical power. ReLM's gains when using GPT-3.5 are constrained by the GNN candidate retrieval (hit@K upper bound). LMs sometimes output products not present in the candidate pool (failure mode). The LM's black-box nature and difficulty handling explicit molecular graph structure are noted general limitations; CSS design can be sensitive if in-context examples are not properly constructed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3572.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3572.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate or design novel chemicals for specific applications, including details of the model, the application, the generation method, evaluation metrics, results, and any reported limitations or challenges.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (Chiang et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source chat-style instruction-tuned model derived from base LLaMA checkpoints; used here as an on-premise LM to perform few-shot, prompt-based selection among GNN-proposed product candidates and to provide confidence scores.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source instruction-tuned chatbot model (derived from LLaMA family). In this work Vicuna is run locally (HuggingFace release) with no additional task-specific fine-tuning; model size/version not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Prompt engineering / in-context learning: same pipeline as GPT-3.5 — Vicuna is given reactants (SMILES + IUPAC), reaction conditions, top-K products from a GNN backbone, and N in‑context examples (including confidence annotations). The LM performs multiple-choice selection and can output per-candidate fine-grained confidence scores (fine-grained CSS variant).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Chemical reaction product prediction (select candidate product selection from GNN top-K) on multiple ORD-derived datasets (Imidazo, NiCOlit, Rexgen-30k, Rexgen-40k), with particular focus on out-of-distribution generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Accuracy (Acc) on test datasets; hit@K upper bound; # input tokens and average inference time per query; Spearman rank correlation (rho, rho+, rho-) between LM fine-grained confidence ranking and GNN ranking; high-vs-low confidence subset accuracies; ablation tests for presence/absence of reaction type/condition; sensitivity to K and in-context randomness.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Vicuna within ReLM produced substantial accuracy gains over GNN-only baselines in many OOD tasks. Example: Imidazo (K=3) ReLM (MolR + Vicuna) achieved 0.914 vs MolR alone 0.513. On Rexgen-30k / Rexgen-40k the Vicuna-based ReLM improved accuracy modestly (e.g., from 0.471 to ~0.498). CSS prompting with Vicuna yielded better accuracy than no strategy and matched or exceeded other prompting strategies while being computationally cheaper (lower tokens and inference time). Fine-grained confidence scores from Vicuna showed positive Spearman correlations (rho ≈ 0.34 across datasets) with GNN rankings and provided interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines</strong></td>
                            <td>ReLM (Vicuna) outperformed GNN-only baselines (MolR, LocalRetro) on OOD datasets; compared to other prompting strategies (MES, JSON, zero-/few-shot/CoT), CSS with Vicuna gave competitive or better accuracy with much lower inference time than MES/JSON. The paper reports upper bounds determined by GNN hit@K and shows ReLM typically improves over baseline but does not surpass the hit@K ceiling.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Performance limited by candidate retrieval (hit@K) of backbone GNNs — if ground-truth product is not among top-K, LM cannot recover it. Vicuna (and LMs generally) struggle with explicit molecular graph structure; occasionally the LM picks a product not in the candidate set. Fine-grained CSS increased interpretability but did not always improve accuracy beyond the simpler CSS. Some sensitivity to K (very large K degrades performance) and to in-context randomness; results can vary by dataset. No task-specific LM fine-tuning was performed, and some datasets lacked reaction-type or condition fields (reducing LM benefit).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chemcrow: Augmenting large-language models with chemistry tools <em>(Rating: 2)</em></li>
                <li>Chemformer: a pre-trained transformer for computational chemistry <em>(Rating: 2)</em></li>
                <li>ChemBERTa: Large-scale self-supervised pretraining for molecular property prediction <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3572",
    "paper_id": "paper-e391d266b0d43475567f59efeaeabc884a48abd0",
    "extraction_schema_id": "extraction-schema-79",
    "extracted_data": [
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "A large transformer-based pre-trained language model accessed via the OpenAI API; used in this paper as an off-the-shelf LM to perform few-shot, prompt-based multiple-choice chemical reaction product selection and to report confidence scores.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-3.5",
            "model_description": "Large autoregressive transformer language model (GPT family). In this paper GPT-3.5 is used via the OpenAI API without further fine-tuning; exact model size and training corpus not specified in the paper (standard GPT-family pretraining referenced).",
            "generation_method": "Prompt engineering / in-context learning: the LM receives a prompt consisting of reactants (SMILES + IUPAC), reaction conditions, a set of K candidate products generated/ranked by a GNN, and a few in‑context examples (including confidence-score annotations). The LM selects/ranks the correct product among the candidates and reports a confidence score (Confidence Score Strategy, CSS).",
            "application_domain": "Chemical reaction product prediction (selecting the correct product from GNN-provided top-K candidates); evaluated on Open Reaction Database (ORD) subsets including Imidazo, NiCOlit, Rexgen-30k, Rexgen-40k.",
            "evaluation_metrics": "Top-line accuracy (Acc) on test sets; hit@K upper bound (determined by GNN candidate retrieval); # input tokens and average inference time per query; Spearman rank correlation (rho) between LM fine-grained confidence ranking and GNN candidate ranking; high-vs-low confidence subset accuracies and associated statistical significance (p-values); ablations for inclusion of reaction type/condition.",
            "results_summary": "Using GPT-3.5 within ReLM improved reaction prediction accuracy relative to GNN-only baselines in several OOD datasets. Example: on Imidazo (K=3) ReLM (MolR + GPT-3.5) achieved 0.865 vs MolR alone 0.513. Improvements were dataset- and backbone-dependent and generally below the theoretical hit@K upper bound. GPT-3.5 results were computed on random subsets (n=500) due to API cost limitations. GPT-3.5 + CSS improved accuracy relative to no strategy; high-confidence outputs were significantly more accurate than low-confidence outputs.",
            "comparison_to_baselines": "Compared to GNN-only baselines (MolR, LocalRetro), ReLM with GPT-3.5 substantially increased accuracy on many OOD datasets but remained bounded by GNN hit@K. Compared different prompting strategies (MES, JSON, CSS): CSS achieved similar or better accuracy with far lower token/time cost than MES/JSON. ReLM also compared to Vicuna-based ReLM (see separate entry) and to other prompt baselines (zero-shot, CoT).",
            "limitations_challenges": "GPT-3.5 was not fine-tuned on task data and was used only with prompting; evaluation was restricted to random subsets (n=500) due to cost, limiting statistical power. ReLM's gains when using GPT-3.5 are constrained by the GNN candidate retrieval (hit@K upper bound). LMs sometimes output products not present in the candidate pool (failure mode). The LM's black-box nature and difficulty handling explicit molecular graph structure are noted general limitations; CSS design can be sensitive if in-context examples are not properly constructed.",
            "uuid": "e3572.0",
            "source_info": {
                "paper_title": "ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Vicuna",
            "name_full": "Vicuna (Chiang et al., 2023)",
            "brief_description": "An open-source chat-style instruction-tuned model derived from base LLaMA checkpoints; used here as an on-premise LM to perform few-shot, prompt-based selection among GNN-proposed product candidates and to provide confidence scores.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vicuna",
            "model_description": "Open-source instruction-tuned chatbot model (derived from LLaMA family). In this work Vicuna is run locally (HuggingFace release) with no additional task-specific fine-tuning; model size/version not specified in the paper.",
            "generation_method": "Prompt engineering / in-context learning: same pipeline as GPT-3.5 — Vicuna is given reactants (SMILES + IUPAC), reaction conditions, top-K products from a GNN backbone, and N in‑context examples (including confidence annotations). The LM performs multiple-choice selection and can output per-candidate fine-grained confidence scores (fine-grained CSS variant).",
            "application_domain": "Chemical reaction product prediction (select candidate product selection from GNN top-K) on multiple ORD-derived datasets (Imidazo, NiCOlit, Rexgen-30k, Rexgen-40k), with particular focus on out-of-distribution generalization.",
            "evaluation_metrics": "Accuracy (Acc) on test datasets; hit@K upper bound; # input tokens and average inference time per query; Spearman rank correlation (rho, rho+, rho-) between LM fine-grained confidence ranking and GNN ranking; high-vs-low confidence subset accuracies; ablation tests for presence/absence of reaction type/condition; sensitivity to K and in-context randomness.",
            "results_summary": "Vicuna within ReLM produced substantial accuracy gains over GNN-only baselines in many OOD tasks. Example: Imidazo (K=3) ReLM (MolR + Vicuna) achieved 0.914 vs MolR alone 0.513. On Rexgen-30k / Rexgen-40k the Vicuna-based ReLM improved accuracy modestly (e.g., from 0.471 to ~0.498). CSS prompting with Vicuna yielded better accuracy than no strategy and matched or exceeded other prompting strategies while being computationally cheaper (lower tokens and inference time). Fine-grained confidence scores from Vicuna showed positive Spearman correlations (rho ≈ 0.34 across datasets) with GNN rankings and provided interpretability.",
            "comparison_to_baselines": "ReLM (Vicuna) outperformed GNN-only baselines (MolR, LocalRetro) on OOD datasets; compared to other prompting strategies (MES, JSON, zero-/few-shot/CoT), CSS with Vicuna gave competitive or better accuracy with much lower inference time than MES/JSON. The paper reports upper bounds determined by GNN hit@K and shows ReLM typically improves over baseline but does not surpass the hit@K ceiling.",
            "limitations_challenges": "Performance limited by candidate retrieval (hit@K) of backbone GNNs — if ground-truth product is not among top-K, LM cannot recover it. Vicuna (and LMs generally) struggle with explicit molecular graph structure; occasionally the LM picks a product not in the candidate set. Fine-grained CSS increased interpretability but did not always improve accuracy beyond the simpler CSS. Some sensitivity to K (very large K degrades performance) and to in-context randomness; results can vary by dataset. No task-specific LM fine-tuning was performed, and some datasets lacked reaction-type or condition fields (reducing LM benefit).",
            "uuid": "e3572.1",
            "source_info": {
                "paper_title": "ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chemcrow: Augmenting large-language models with chemistry tools",
            "rating": 2
        },
        {
            "paper_title": "Chemformer: a pre-trained transformer for computational chemistry",
            "rating": 2
        },
        {
            "paper_title": "ChemBERTa: Large-scale self-supervised pretraining for molecular property prediction",
            "rating": 1
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        }
    ],
    "cost": 0.011327499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction</h1>
<p>Yaorui Shi ${ }^{\dagger}$ An Zhang ${ }^{\S <em>}$ Enzhi Zhang ${ }^{\boldsymbol{</em>}}$ Zhiyuan Liu ${ }^{\S}$ Xiang Wang ${ }^{\ddagger \dagger}$<br>${ }^{\dagger}$ University of Science and Technology of China<br>${ }^{\S}$ National University of Singapore, ${ }^{\text {H }}$ Hokkaido University<br>yaoruishi@gmail.com, anzhang@u.nus.edu, enzhi.zhang.n6@elms.hokudai.ac.jp, acharkq@gmail.com, xiangwang1223@gmail.com</p>
<h4>Abstract</h4>
<p>Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in realworld applications. In this work, we propose ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions. To further enhance the model's robustness and interpretability, we incorporate the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions. Our experimental results demonstrate that ReLM improves the performance of state-of-the-art GNN-based methods across various chemical reaction datasets, especially in out-of-distribution settings. Codes are available at https://github.com/syr-cn/ReLM.</p>
<h2>1 Introduction</h2>
<p>Pre-trained language models (LMs) possess a vast reserve of knowledge, coupled with impressive capabilities for logical inference (Brown et al., 2020; OpenAI, 2023; Taylor et al., 2022; Chowdhery et al., 2022; Touvron et al., 2023; Chiang et al., 2023). These advantages render LMs useful for question-answering tasks, such as scientific inquiry and chemical inference (Bran et al., 2023; Shao et al., 2023). However, due to LMs' black-box natures, distinguishing whether the answers stem from their inherent knowledge or are arbitrarily generated poses a significant challenge. Furthermore, recent studies indicate that LMs struggle</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>with the graph structures of molecules in chemistryrelated tasks (Bran et al., 2023).</p>
<p>Recently, graph neural networks (GNNs) have been widely used to address chemical reaction problems due to their ability to handle complex graph structures inherent in chemical compounds (He et al., 2022; Somnath et al., 2021; Sacha et al., 2021). However, GNN-based methods often suffer from limited and biased training data, resulting in poor performance in real-world applications that involve diverse reaction mechanisms. This is exemplified by the unsatisfactory performance of LocalRetro (Chen and Jung, 2021) on Imidazo and NiCOlit (see results in Table 1), especially when encountering new reaction types absent in the training set. In addition, it is difficult for GNNs to effectively leverage the textual information (e.g., reaction conditions) that could be derived from reaction descriptions. Identical reactants gas can yield different products depending on the catalyst used. An illustrative example is provided in Appendix C.1.</p>
<p>A crucial question arises: can we develop a chemical reaction framework that synergistically integrates the advantages of both GNNs and LMs? In this work, we propose ReLM, a novel framework designed to conduct chemical reaction prediction by utilizing both pre-trained LMs and GNNs. ReLM enhances the prediction accuracy in out-ofdistribution (OOD) scenarios by utilizing graph structure understanding of GNNs and the natural language understanding capabilities of LMs. More specifically, ReLM employs GNNs to generate several high-probability candidate products. These products, along with appropriate in-context examples and descriptions of the reaction conditions, are then fed into the LMs to facilitate accurate chemical reaction prediction (as depicted in Figure 1). To further improve the robustness and interpretability of ReLM, we propose a prompting technique called the confidence score strategy (CSS). Harnessing</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The overall framework of ReLM. The ReLM encompasses three key inputs for the language models: similar reactions (grey box), the target reaction along with its conditions (yellow box), and candidate products generated by GNNs (green box). Specifically, we select in-context examples with various confidence scores from the training samples that are nearest to the target reaction query. Additionally, we choose the K-candidate products with the highest likelihood as predicted by the GNN.</p>
<p>The intrinsic self-criticism capacity of LMs, CSS boosts the model's performance without involving significant computational costs (see Table 5). Extensive experiments on real-world Open Reaction Database (ORD) <em>Kearnes et al. (2021)</em> demonstrate that ReLM achieves significant improvement over state-of-the-art GNN-based methods.</p>
<h2>2 Background</h2>
<p><strong>Chemical Reaction Analysis.</strong> Recent studies utilize language models (LMs) for molecule analysis, relying solely on Simplified Molecular Input Line Entry System (SMILES) as input for molecular representation learning <em>Irwin et al. (2022); Chithrananda et al. (2020); Fabian et al. (2020)</em>. However, SMILES only provides one-dimensional linearizations of molecular structures, meaning that molecules with identical SMILES may exhibit entirely different properties <em>Wang et al. (2022)</em>. In contrast, GNN-based approaches take the structure of molecular graphs into consideration and have shown remarkable performance in in-distribution chemical reaction prediction tasks <em>Somnath et al. (2021); Dai et al. (2019)</em>.</p>
<p>A chemical reaction between reactants set $$R = {r_1, r_2, \ldots}$$, and products set $$P = {p_1, p_2, \ldots}$$, can be defined as:</p>
<p>$$r_1, r_2, \ldots \rightarrow p_1, p_2, \ldots \tag{1}$$</p>
<p>Following the evaluation protocol established by Wang et al., we can consider reaction prediction as a task of ranking products within a predetermined product corpus $$C = {P_1, P_2, \ldots}$$. This evaluation protocol ensures that $$P_i \in C$$ holds for all reactions $$R_i \rightarrow P_i$$ in the tests.</p>
<p>Given a query with reactants $$R$$, the model is required to search across the product corpus to identify the most probable product. The likelihood between reactants and products is estimated using $$L_2$$ distance between their corresponding molecular embeddings:</p>
<p>$$D(R, P') = |\sum_{r \in R} G(r|\theta) - \sum_{p \in P'} G(p|\theta)|_2 \tag{2}$$</p>
<p>where $$R$$ is the given set of reactant molecules in a reaction, $$P' \in C$$ is a product set from the corpus, and $$G(|\theta|)$$ stands for the GNN model with parameter $$\theta$$.</p>
<p><strong>Prompting Strategies.</strong> Instead of directly applying in-context learning to real-world tasks, some studies use prompting strategies for better performance <em>Shao et al. (2023); Bran et al. (2023)</em>. A commonly used strategy is instructing the language model to provide responses in a formalized format (<em>e.g.</em>, JSON or YAML). By using appropriate prompts, the thought process of an LM can be elicited using structured key-value pairs. Yang et al. develop the Multi-Query Ensemble Strategy (MES), which enhances the robustness of LMs. This strategy involves iteratively querying an LM with diverse in-context examples, conducting the inference process multiple times, and ultimately using a majority vote to determine the outcome.</p>
<h2>3 Methodology</h2>
<p>In this section, we propose ReLM, a framework combining graph neural networks (GNNs) and language models (LMs) for chemical reaction analysis. ReLM employs GNNs to generate answer candidates and in-context examples, then utilizes LMs for analysis in the form of multiple-choice</p>
<p>questions. We also propose the confidence score strategy, a generic prompting strategy to improve the reliability of LMs.</p>
<h3>3.1 Context Generation</h3>
<p>Answer Candidates. The prompt for language models can be formulated with input reactants $R$, reaction condition $c$, and a set of answer candidates $\hat{\mathbf{P}}$ generated by GNN encoder.</p>
<p>The process of answer candidates generation can be succinctly described as data retrieval directed by a pre-trained GNN model.</p>
<p>Leveraging insights from the field of chemistry, chemical equations encapsulate the conservation of matter, charge, and various chemical properties. This implies a level of correspondence in the latent representations of the molecules on both sides of the equation. Therefore, for a specific set of reactants $R$, we employ the distance measurement function defined in Equation (2) to filter out those product sets $P_{j} \in \mathcal{C}$ that exhibit the highest similarity to $R$ in the latent space.</p>
<p>Formally, the answer candidates are generated by selecting the top- $K$ products with the highest similarity from the candidate pool $\mathcal{C}$ :</p>
<p>$$
\hat{\mathbf{P}}<em j="j">{R}=\underset{P</em>\right)
$$} \in \mathcal{C}}{\operatorname{TopK}}-D\left(R, P_{j</p>
<p>where $\hat{\mathbf{P}}_{R}$ is the set of answer candidates for reactants $R$, and $D$ is the GNN-based similarity measurement defined in Equation (2).</p>
<p>In-context Examples. Besides the reaction information, the choice of in-context examples is also crucial for LM's few-shot learning performance. To acquire in-context examples that imply a reaction mechanism similar to the given reaction sample, we use the answer-aware example selection strategy proposed by Shao et al. (2023). Specifically, for a given test sample, we choose top- $N$ nearest neighbors from the training set based on the similarity of their reactants in the latent space, and use the $N$ training samples as in-context examples:</p>
<p>$$
\mathcal{T}=\underset{i \in{1,2, \ldots, M}}{\arg \operatorname{TopN}} \frac{h_{R}^{T} h_{R_{i}^{\prime}}}{\left|h_{R}\right|<em R__i="R_{i">{2}\left|h</em>
$$}^{\prime}}\right|_{2}</p>
<p>where $M$ is the size of the training set, $h_{R}=$ $\sum_{r \in R} G(r \mid \theta)$ is the sum of all reactant representations, and $\mathcal{T}$ denotes the index set of the selected training samples. With the ground truth products $a_{i}$ of each training sample, the in-context examples are defined as:</p>
<p>$$
\mathcal{E}=\left{\left(R_{i}^{\prime}, c_{i}^{\prime}, a_{i}^{\prime}, \hat{\mathbf{P}}<em i="i">{R</em>\right}
$$}^{\prime}}\right) \mid i \in \mathcal{T</p>
<p>With reaction information $\left{R, c, \hat{\mathbf{P}}_{R}\right}$ and incontext examples $\mathcal{E}$, the product prediction problem can be formulated as a single select multiple choice question. We input this question into the pre-trained language model, and the answer generated by the model is regarded as the output of our approach:</p>
<p>$$
\hat{P}=\underset{P \in \hat{\mathbf{P}}<em _mathrm_LM="\mathrm{LM">{R}}{\operatorname{argmax}} p</em>})
$$}}(P \mid{R, c, \mathcal{E</p>
<p>here $p_{\mathrm{LM}}(A \mid \mathcal{P})$ is the probability distribution of answer $A$ in a language model given prompt $\mathcal{P}$ as input. To feed molecules into language models, we use both IUPAC names and SMILES string to represent molecules. In Appendix C.2, we present a detailed, step-by-step example of the context generation process.</p>
<h3>3.2 Confidence Score Strategy</h3>
<p>Besides prompting language models for chemical reaction prediction, we propose a universal prompting strategy named the confidence score strategy, which can be seamlessly transferred to any promptbased language model application.</p>
<p>During inference, we ask the language model to report its confidence score (an integer number between 1 and 9) based on its understanding and familiarity with the given multi-choice question. To help the language model better understand how this score works, we also introduce the confidence score in the context prompt. For each in-context example, a random integer in ${8,9}$ is chosen as the confidence score, which is then combined with the ground truth answer to form the prompt.</p>
<p>Nevertheless, the above random generation behavior can easily lead to a misunderstanding of the meaning of confidence scores by the language models, resulting in a degradation where the model only generates higher scores. Hence, we deliberately distort the answer of an in-context example to an incorrect one and assign it to a lower confidence score (e.g., a random integer in ${1,2}$ ). Thus, Equation 5 can be rewritten as:</p>
<p>$$
\widetilde{\mathcal{E}}=\left{\left(R_{i}^{\prime}, c_{i}^{\prime}, \tilde{a}<em i="i">{i}^{\prime}, \tilde{s}</em>}^{\prime}, \hat{\mathbf{P}<em i="i">{R</em>\right}
$$}^{\prime}}\right) \mid i \in \mathcal{T</p>
<p>where $\tilde{a}<em i="i">{i}{ }^{\prime}$ is the perturbed answer and $\tilde{s}</em>$ is the corresponding confidence score. By asking for the confidence score, we let the language model}{ }^{\prime</p>
<p>implicitly self-critique its answers during inference. In Appendix D, we verify that the confidence score strategy can be comprehended by language models.</p>
<h3>3.3 Fine-grained Confidence Score Strategy</h3>
<p>To achieve a more detailed analysis of confidence and enhance the language model's fine-grained interpretability, we also attempt to generate confidence scores for each answer candidate instead of a singular overall score. Subsequently, these candidates are re-ranked based on their respective confidence scores, selecting the one with the highest confidence as the definitive answer. We refer to this methodology as the fine-grained confidence score strategy. Please see more experimental results and analysis in Section 4.4 and Appendix B.4</p>
<h2>4 Experimental Results</h2>
<p>We aim to answer the following research questions:
RQ1: Can ReLM improve the reaction prediction capability of graph neural networks on both out-of-distribution and in-distribution data?</p>
<p>RQ2: Does the confidence strategy enhance the accuracy of ReLM?</p>
<p>RQ3: How does confidence score strategy influence the inference process of language models?</p>
<h3>4.1 Experiment Setup</h3>
<p>Baselines. Two popular GNN chemical reaction analysis methods, MolR (Wang et al., 2022) and LocalRetro (Chen and Jung, 2021), are selected as GNN backbones. For language models, we test GPT-3.5 (Brown et al., 2020) and Vicuna (Chiang et al., 2023). See more details in Appendix B.1.</p>
<p>Datasets. We utilize the USPTO dataset to train the GNN backbones. For evaluation, four datasets from the Open Reaction Database (ORD) (Kearnes et al., 2021) are utilized as the testbed for our experiments. The ORD contains diverse real-world reaction records from open-source projects and chemical literature. Notably, its utility for GNN-based chemical reaction predictions remains largely underexplored.</p>
<h3>4.2 The OOD Capability of ReLM (RQ1)</h3>
<p>Motivation. To evaluate the model's generalization ability, a comparison is made between our method and GNN baselines on the ORD dataset. Natural language descriptions of reactions, sourced from the ORD database, are utilized to enhance the quality of the prompts.</p>
<p>Results. In Table 1 and Table 5 (see Appendix B.3), we present the average accuracy of ReLM on the ORD dataset. ReLM improves the performance of GNN backbones across all test datasets. As ReLM utilizes the top- $K$ candidates provided by the GNN, the theoretical upper bound of our approach is constrained by the hit@ $K$ accuracy of the GNN backbones. These upper bounds are also delineated in tables. To assess the stability of ReLM, we examine its accuracy under varying $K$-values and degrees of in-context randomness. Detailed results can be found in Appendices B.6 and B.7.</p>
<p>Apart from the evaluation on out-of-distribution datasets, we also run experiments under independent and identically distributed (i.i.d.) conditions. Refer to Appendix B. 5 for more results.</p>
<h3>4.3 Effectiveness of Confidence Score Strategy (RQ2)</h3>
<p>Motivation. The Confidence Score Strategy proposed in this paper is a generalizable prompting method. To verify the effectiveness of this strategy, we conduct comparative experiments against other prompting strategies specifically tailored for multiple-choice questions. The baseline strategies include the Multiple-Ensemble Strategy (MES)(Yang et al., 2022), the JSON Strategy, and a control group that does not employ any strategy. MES necessitates multiple runs of the inference by the language model ( 10 times in our experiments). A majority vote is conducted based on these results, and the most frequently occurring answer is selected as the final outcome. By JSON strategy, we refer to the strategy that asks the language models to present their understanding of reaction precursors, reaction mechanisms, and its inferring process. Under this strategy, the language model is required to output all the above information along with its answer in a machine-readable format.</p>
<p>Results. In Table 2, we compare the overall accuracy (Acc), number of input tokens (#Token), and average inference time (Time/s) for each prompting strategy. Clearly, both the MES and JSON strategies enhance the model's performance, albeit with non-negligible time costs. Conversely, our proposed Confidence Score Strategy (CSS) prominently improves the accuracy without imposing a noticeable computational burden on the language model. See Appendix B. 8 for comparison with more prompting strategies.</p>
<p>Table 1: Accuracy on out-of-distribution settings.</p>
<table>
<thead>
<tr>
<th></th>
<th>Imidazo</th>
<th></th>
<th>NiCOlit</th>
<th></th>
<th>Rexgen-30k</th>
<th></th>
<th>Rexgen-40k</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>K=3</td>
<td>K=4</td>
<td>K=3</td>
<td>K=4</td>
<td>K=3</td>
<td>K=4</td>
<td>K=3</td>
<td>K=4</td>
</tr>
<tr>
<td>MolR (Wang et al., 2022)</td>
<td>0.513</td>
<td>0.513</td>
<td>0.437</td>
<td>0.437</td>
<td>0.437</td>
<td>0.471</td>
<td>0.418</td>
<td>0.448</td>
</tr>
<tr>
<td>ReLM (MolR + Vicuna)</td>
<td>0.914 ^{+76 (45)}</td>
<td>0.878 ^{+71 (075)}</td>
<td>0.510 ^{+16 (685)}</td>
<td>0.520 ^{+19 (505)}</td>
<td>0.498 ^{+5 (375)}</td>
<td>0.499 ^{+5 (645)}</td>
<td>0.473 ^{+5 (335)}</td>
<td>0.473 ^{+5 (315)}</td>
</tr>
<tr>
<td>ReLM (MolR + GPT-3.5)</td>
<td>0.865 ^{+68 (525)}</td>
<td>0.815 ^{+58 (685)}</td>
<td>0.443 ^{+1 (375)}</td>
<td>0.478 ^{+6 (375)}</td>
<td>0.486 ^{+2 (225)}</td>
<td>0.458 ^{+2 (625)}</td>
<td>0.450 ^{+0 (207)}</td>
<td>0.467 ^{+3 (655)}</td>
</tr>
<tr>
<td>Upper Bound</td>
<td>0.945</td>
<td>0.979</td>
<td>0.640</td>
<td>0.679</td>
<td>0.584</td>
<td>0.611</td>
<td>0.562</td>
<td>0.586</td>
</tr>
<tr>
<td>LocalRetro (Chen and Jung, 2021)</td>
<td>0.023</td>
<td>0.023</td>
<td>0.086</td>
<td>0.086</td>
<td>0.279</td>
<td>0.279</td>
<td>0.245</td>
<td>0.245</td>
</tr>
<tr>
<td>ReLM (LocalRetro + Vicuna)</td>
<td>0.023 ^{+0 (885)}</td>
<td>0.037 ^{+02 (335)}</td>
<td>0.173 ^{+86 (075)}</td>
<td>0.184 ^{+112 (155)}</td>
<td>0.325 ^{+16 (175)}</td>
<td>0.329 ^{+17 (805)}</td>
<td>0.295 ^{+22 (285)}</td>
<td>0.298 ^{+31 (285)}</td>
</tr>
<tr>
<td>ReLM (LocalRetro + GPT-3.5)</td>
<td>0.031 ^{+02 (525)}</td>
<td>0.037 ^{+01 (335)}</td>
<td>0.224 ^{+117 (715)}</td>
<td>0.254 ^{+192 (225)}</td>
<td>0.348 ^{+24 (645)}</td>
<td>0.364 ^{+30 (375)}</td>
<td>0.308 ^{+25 (445)}</td>
<td>0.316 ^{+28 (505)}</td>
</tr>
<tr>
<td>Upper Bound</td>
<td>0.033</td>
<td>0.046</td>
<td>0.323</td>
<td>0.340</td>
<td>0.409</td>
<td>0.440</td>
<td>0.374</td>
<td>0.406</td>
</tr>
</tbody>
</table>
<p>Table 2: Comparison of different prompting strategies within ReLM.</p>
<table>
<thead>
<tr>
<th></th>
<th>Rexgen-30k</th>
<th></th>
<th></th>
<th>Rexgen-40k</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Acc</td>
<td>#Token</td>
<td>Time/s</td>
<td>Acc</td>
<td>#Token</td>
<td>Time/s</td>
</tr>
<tr>
<td>MolR + Vicuna w/o strategy</td>
<td>0.472</td>
<td>940.6</td>
<td>2.19</td>
<td>0.449</td>
<td>942.3</td>
<td>2.19</td>
</tr>
<tr>
<td>MolR + Vicuna JSON</td>
<td>0.456</td>
<td>1087.4</td>
<td>27.0</td>
<td>0.422</td>
<td>1091.6</td>
<td>27.5</td>
</tr>
<tr>
<td>MolR + Vicuna MES</td>
<td>0.490</td>
<td>9402.0</td>
<td>22.2</td>
<td>0.463</td>
<td>9412.3</td>
<td>22.4</td>
</tr>
<tr>
<td>MolR + Vicuna CSS</td>
<td>0.497</td>
<td>991.6</td>
<td>1.5</td>
<td>0.473</td>
<td>993.3</td>
<td>1.5</td>
</tr>
<tr>
<td>MolR + GPT-3.5 w/o strategy</td>
<td>0.464</td>
<td>956.6</td>
<td>2.09</td>
<td>0.420</td>
<td>966.4</td>
<td>1.63</td>
</tr>
<tr>
<td>MolR + GPT-3.5 JSON</td>
<td>0.456</td>
<td>1087.4</td>
<td>27.0</td>
<td>0.422</td>
<td>1091.6</td>
<td>27.5</td>
</tr>
<tr>
<td>MolR + GPT-3.5 MES</td>
<td>0.476</td>
<td>9564.1</td>
<td>18.9</td>
<td>0.426</td>
<td>9661.6</td>
<td>18.4</td>
</tr>
<tr>
<td>MolR + GPT-3.5 CSS</td>
<td>0.486</td>
<td>1007.6</td>
<td>1.6</td>
<td>0.450</td>
<td>1017.4</td>
<td>2.0</td>
</tr>
</tbody>
</table>
<p>Table 3: Rank Correlation between ReLM’s fine-grained confidence score and MolR’s candidates ranking.</p>
<table>
<thead>
<tr>
<th></th>
<th>Imidazo</th>
<th>NiCOlit</th>
<th>rexgen-30k</th>
<th>rexgen-40k</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\rho$</td>
<td>0.363</td>
<td>0.359</td>
<td>0.346</td>
<td>0.347</td>
</tr>
<tr>
<td>$\rho^{+}$</td>
<td>0.469</td>
<td>0.429</td>
<td>0.388</td>
<td>0.397</td>
</tr>
<tr>
<td>$\rho^{-}$</td>
<td>0.243</td>
<td>0.227</td>
<td>0.199</td>
<td>0.174</td>
</tr>
</tbody>
</table>
<p>It is noteworthy that ReLM achieves competitive or even superior results regardless of the prompting strategy employed for the language model. These results suggest that our reaction prediction method maintains a high level of robustness irrespective of the prompting techniques used.</p>
<h3>4.4 Interpretability of Confidence Scores (RQ3)</h3>
<p>Motivations. With more detailed confidence information, we aim to interpretably analyze the differences between language models and graph neural networks during decision-making processes, and investigate the reasons behind the performance enhancement brought by LMs. We employ Spearman’s rank correlation coefficient as the metric to ascertain how the ranking produced by the LM correlates with the original rankings from the GNN model.</p>
<p>Results. For the evaluation of the fine-grained confidence score strategy mentioned in Section 3.3, we use MolR as the GNN backbone, Vicuna as the language model, and $K=4$. In Table 3, the symbol $\rho$ denotes the rank correlation derived from ReLM and MolR across all test samples. On the other hand, $\rho^{+}$and $\rho^{-}$represent this correlation when MolR’s predictions are correct and incorrect respectively.</p>
<p>This correlation sheds light on the consistency between these two models in their decision-making processes. The overall rankings of the ReLM exhibit a positive correlation with the rankings of the MolR, implying that the ReLM concurs with most of the MolR’s judgments.</p>
<p>Moreover, the ReLM concurs with the MolR when it’s correct, and makes contradictory choices when it errs. This suggests that the ReLM is able to make informed judgments, diverging from the MolR when it believes the MolR is wrong. Additional results under the fine-grained confidence score strategy are in Appendix B.4. For further statistical analysis pertaining to confidence scores, refer to Appendix D.</p>
<h2>5 Conclusion</h2>
<p>Despite the great success of molecular structure understanding, today’s GNN-based reaction prediction methods are still far from being able to do real-world reaction analysis. In this work, we proposed ReLM, an in-context prompting method that utilizes both language models and graph neural networks for chemical reaction prediction. Extensive experiments demonstrate the remarkable improvement of ReLM on various datasets indeed comes from the reasoning ability and self-criticism of LMs.</p>
<h2>Limitations</h2>
<p>The limitations of ReLM are in two aspects, which will be addressed in future work. Firstly, ReLM focuses solely on chemical reaction prediction, neglecting other essential tasks in chemical reaction analysis such as retrosynthesis planning and yield prediction. Secondly, the performance gains achieved by ReLM are hindered by the drawbacks of backbone GNNs. The accuracy improvement is restricted by the hit@ $K$ metric of GNN models, leading to only marginal advancements on specific datasets, as demonstrated in Table 1. We believe</p>
<p>our ReLM sheds light on chemical reaction prediction tasks and will inspire more work in this research line.</p>
<h2>Acknowledgements</h2>
<p>This research is supported by the National Natural Science Foundation of China (9227010114), the University Synergy Innovation Program of Anhui Province (GXXT-2022-040), and the NExT Research Center.</p>
<h2>References</h2>
<p>Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. 2023. Chemcrow: Augmenting large-language models with chemistry tools. arXiv preprint arXiv:2304.05376.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In NeurIPS.</p>
<p>Shuan Chen and Yousung Jung. 2021. Deep retrosynthetic reaction prediction using local reactivity and global attention. JACS Au, 1(10):1612-1620.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. 2020. Chemberta: Large-scale selfsupervised pretraining for molecular property prediction.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Connor W Coley, Wengong Jin, Luke Rogers, Timothy F Jamison, Tommi S Jaakkola, William H Green, Regina Barzilay, and Klavs F Jensen. 2019. A graph-convolutional neural network model for the prediction of chemical reactivity. Chemical science, 10(2):370-377.</p>
<p>Hanjun Dai, Chengtao Li, Connor W. Coley, Bo Dai, and Le Song. 2019. Retrosynthesis prediction with conditional graph logic network. In NeurIPS.</p>
<p>Jian Du, Shanghang Zhang, Guanhang Wu, José M. F. Moura, and Soummya Kar. 2017. Topology adaptive graph convolutional networks. CoRR, abs/1710.10370.</p>
<p>Benedek Fabian, Thomas Edlich, Héléna Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, and Mohamed Ahmed. 2020. Molecular representation learning with language models and domain-relevant auxiliary tasks.</p>
<p>Jiyan He, Keyu Tian, Shengjie Luo, Yaosen Min, Shuxin Zheng, Yu Shi, Di He, Haiguang Liu, Nenghai Yu, Liwei Wang, Ji Wu, and Tie-Yan Liu. 2022. Masked molecule modeling: A new paradigm of molecular representation learning for chemistry understanding.</p>
<p>Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. 2022. Chemformer: a pre-trained transformer for computational chemistry. Machine Learning: Science and Technology, 3(1):015022.</p>
<p>Steven M Kearnes, Michael R Maser, Michael Wleklinski, Anton Kast, Abigail G Doyle, Spencer D Dreher, Joel M Hawkins, Klavs F Jensen, and Connor W Coley. 2021. The open reaction database. Journal of the American Chemical Society, 143(45):18820-18826.</p>
<p>Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han, Jane He, Siqian He, Benjamin A Shoemaker, et al. 2016. Pubchem substance and compound databases. Nucleic acids research, 44(D1):D1202-D1213.</p>
<p>Thomas N. Kipf and Max Welling. 2017. Semisupervised classification with graph convolutional networks. In 5th International Conference on Learning Representations.</p>
<p>Daniel Mark Lowe. 2012. Extraction of chemical structures and reactions from the literature. Ph.D. thesis, University of Cambridge.</p>
<p>OpenAI. 2023. Gpt-4 technical report.
Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. 2021. Stout: Smiles to iupac names using neural machine translation. Journal of Cheminformatics, 13(1):1-14.</p>
<p>Mikolaj Sacha, Mikolaj Blaz, Piotr Byrski, Pawel Dabrowski-Tumanski, Mikolaj Chrominski, Rafal Loska, Pawel Wlodarczyk-Pruszynski, and Stanislaw Jastrzebski. 2021. Molecule edit graph attention network: Modeling chemical reactions as sequences of graph edits. J. Chem. Inf. Model., 61(7):3273-3284.</p>
<p>Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, and Teodoro Laino. 2021. Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. Science Advances, 7(15):eabe4166.</p>
<p>Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. 2023. Prompting large language models with answer heuristics for knowledge-based visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1497414983.</p>
<p>Vignesh Ram Somnath, Charlotte Bunne, Connor W. Coley, Andreas Krause, and Regina Barzilay. 2021. Learning graph models for retrosynthesis prediction. In NeurIPS.</p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.</p>
<p>Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, and Martin D. Burke. 2022. Chemical-reaction-aware molecule representation learning. In $I C L R$.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface's transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771.</p>
<p>Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022. An empirical study of gpt-3 for few-shot knowledgebased vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 30813089 .</p>
<p>Shuangjia Zheng, Jiahua Rao, Zhongyue Zhang, Jun Xu , and Yuedong Yang. 2019. Predicting retrosynthetic reactions using self-corrected transformer neural networks. Journal of chemical information and modeling, 60(1):47-55.</p>
<p>A Datasets</p>
<p>The USPTO dataset used in Section 4 contains approximately 479k reaction samples, which are collected by <em>Lowe (2012)</em> and cleaned by <em>Zheng et al. (2019)</em>.</p>
<p>We conduct our evaluation on four datasets extracted from Open Reaction Database <em>Kearnes et al. (2021)</em>. Considering there’s an overlap between the two Rexgen datasets and USPTO, we exclude all reaction records in Rexgen-30k and Rexgen-40k that resulted in products already present in the training set. This operation ensures that the target products of test reactions have never been encountered during the training phase, imposing higher demands on the model’s generalization ability.</p>
<p>The datasets, Imidazo and NiCOlit, encompass a wealth of textual descriptions of chemical reactions, which can be leveraged by language models for inferential purposes (refer to Section 4.2). The contents of the datasets utilized in this study are summarized in Table 4. Additionally, we provide a representation of some examples pertaining to reaction type and reaction condition information contained within these datasets, as depicted in Figure 2.</p>
<h2>Appendix B Experiments</h2>
<h3>B.1 Experiments Settings</h3>
<p>GNN models. For MolR <em>Wang et al. (2022)</em>, we use the pre-trained checkpoints provided by its authors on their GitHub repository. The checkpoint model is trained on the USPTO dataset for 20 epochs with a batch size of 4096 and a learning rate of $1\text{\times}{10}^{-4}$. Specifically, we use a 2-layer TAG <em>Du et al. (2017)</em> as the GNN encoder, as it has demonstrated superior performance in experiments of MolR. For LocalRetro <em>Chen and Jung (2021)</em>, we train the model on the USPTO reaction prediction dataset. Following the experimental settings of <em>Chen and Jung, we use a 6-layer GCN (Kipf and Welling, 2017)</em> as the backbone GNN and train it for 50 epochs with a batch size of 16. The training process employs an Adam optimizer with a learning rate of $1\text{\times}{10}^{-4}$ and weight decay of $1\text{\times}{10}^{-6}$. During the Evaluation phase, we use 256 as batch size for both MolR and LocalRetro.</p>
<p>Language models. Training is not performed for the language model backbones due to the high training cost of LMs. We interact with GPT-3.5 through the OpenAI API, while for Vicuna, we employ the model released by its authors on HuggingFace <em>Wolf et al. (2019)</em>. It’s worth noticing that due to the substantial financial costs associated with accessing GPT-3.5, we only use random subsets of size 500 of the test datasets when using GPT-3.5 as the backbone language model.</p>
<p>Hyperparameters. There are two important hyperparameters in our approach, the number of in-context examples $N$ (see Equation 4) and the number of answer candidates $K$ (see Equation 3). For the number of answer candidates, we try $K=$ 3, 4, and 5. For the number of in-context examples, we use a constant number $N=3$ throughout our experiments.</p>
<h3>B.2 Implementation Details</h3>
<p>For the speed test demonstrated in Table 2, we test all the involved methods on a single NVIDIA RTX A500 graphic card. For token count computation, we use tiktoken, a fast open-source byte pair encoding tokenizer that can be used with OpenAI’s models. For fair comparisons, we also use this tokenizer to count the token usage of Vicuna <em>Chiang et al. (2023)</em> in Section 4.3, even though it’s not developed by OpenAI.</p>
<p>The training of LocalRetro requires atom-wise matching between reactants and products, but the training dataset used by us does not contain such data. We thus preprocess the training dataset with Rxnmapper <em>Schwaller et al. (2021)</em> to meet this requirement.</p>
<p>Besides, as shown in Figure 1, we use IUPAC names along with SMILES to represent molecules in prompt design. However, the datasets only contain the SMILES expression of molecules. We leverage the database of PubChem <em>Kim et al. (2016)</em> and open-source tool STOUT <em>Rajan et al. (2021)</em> to perform the conversion from SMILES strings to IUPAC names.</p>
<h3>B.3 Ablation Study of Reaction Conditions.</h3>
<p>Table 5 displays the results of ablation studies on descriptive reaction information. Incorporating reaction conditions (e.g., reaction temperature and catalysts) and reaction type information in the prompts leads ReLM to achieve higher performance gains than compared to only reactant information.</p>
<p>Table 4: Statistics of the Four Datasets in the Open Reaction Database</p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Size</th>
<th>Description</th>
<th>Reaction SMARTS</th>
<th>Reaction Type</th>
<th>Reaction Condition</th>
</tr>
</thead>
<tbody>
<tr>
<td>Imidazo</td>
<td>384</td>
<td>Three-component reaction approach towards diverse imidazopyridines reactions.</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
</tr>
<tr>
<td>NiCOlit</td>
<td>1762</td>
<td>Nickel-catalyzed cross-couplings reactions.</td>
<td>✔</td>
<td>✔</td>
<td>✗</td>
</tr>
<tr>
<td>Rexgen-30k</td>
<td>7700</td>
<td>Test data used by Rexgen <em>Coley et al. (2019)</em>.</td>
<td>✔</td>
<td>✗</td>
<td>✗</td>
</tr>
<tr>
<td>Rexgen-40k</td>
<td>10235</td>
<td>Validation data used by Rexgen <em>Coley et al. (2019)</em>.</td>
<td>✔</td>
<td>✗</td>
<td>✗</td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Examples of reaction type and condition in Imidazo and NiColit.</p>
<p>Table 5: Ablation study of textual reaction information.</p>
<table>
<thead>
<tr>
<th></th>
<th>Imidazo</th>
<th>NiCOlit</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GPT-3.5</td>
<td>Vicuna</td>
</tr>
<tr>
<td>MolR <em>Wang et al. (2022)</em></td>
<td>0.513</td>
<td>0.513</td>
</tr>
<tr>
<td>ReLM (MolR)</td>
<td>0.812</td>
<td>0.765</td>
</tr>
<tr>
<td>+ reaction type</td>
<td>0.822</td>
<td>0.903</td>
</tr>
<tr>
<td>+ reaction type and condition</td>
<td>0.864</td>
<td>0.914</td>
</tr>
</tbody>
</table>
<p>Table 6: Accuracies with multiple confidence strategy.</p>
<table>
<thead>
<tr>
<th></th>
<th>Imidazo</th>
<th>NiCOlit</th>
<th>rexgen-30k</th>
<th>rexgen-40k</th>
</tr>
</thead>
<tbody>
<tr>
<td>MolR</td>
<td>0.513</td>
<td>0.437</td>
<td>0.471</td>
<td>0.449</td>
</tr>
<tr>
<td>ReLLM</td>
<td>0.870</td>
<td>0.507</td>
<td>0.449</td>
<td>0.421</td>
</tr>
</tbody>
</table>
<h3>B.4 Accuracy of fine-grained Confidence Score Strategy.</h3>
<p>In this section, we test the performance of the fine-grained confidence score strategy (introduced in Section 3.3). Table 6 displays the accuracies of both the ReLM and GNN models when using the multiple-CSS strategy. Although ReLM’s performance does not surpass that of the original CSS strategy, it offers a greater degree of interpretability.</p>
<h3>B.5 Evaluation on In-distribution Dataset</h3>
<p>The experiments in Section 4.2 demonstrate the powerful out-of-distribution performance of ReLM. We also conducted experiments under the independently and identically distributed (i.i.d.) setting on the test set of USPTO-479k. The results are shown in Table 7. This part of the experiment was conducted on the test set of USPTO-479k, using Table 7: Evaluation on the test set of USPTO.</p>
<table>
<thead>
<tr>
<th></th>
<th>MolR</th>
<th>LocalRetro</th>
</tr>
</thead>
<tbody>
<tr>
<td>MolR</td>
<td>0.882</td>
<td>0.5663</td>
</tr>
<tr>
<td>ReLM (MolR+Vicuna)</td>
<td>0.871</td>
<td>0.6273</td>
</tr>
<tr>
<td>Upper Bound</td>
<td>0.9527</td>
<td>0.7583</td>
</tr>
</tbody>
</table>
<p>Vicuna as the language model and MolR as the GNN backbone, with $K=4$. Due to the extensive size of the USPTO dataset, we do not conduct experiments under all experimental settings.</p>
<p>In Table 7, it can be observed that GNN methods have solely achieved considerable performance in in-distribution scenarios, thus the performance enhancements provided by ReLM are limited in this circumstance. Furthermore, the lack of reaction type and condition information in the USPTO dataset also presents inference difficulties for the language model. Despite these challenges, ReLM still exhibited a certain capacity to select the correct answers from the options, without exhibiting significant performance deterioration.</p>
<h3>B.6 Effect of Different $K$-Values</h3>
<p>Given that the accuracy upper bound of ReLM is determined by the number of answer candidates, it is necessary to evaluate the performance of the model under different $K$ levels. In this section, we conduct experiments across a wider range of $K$ values. The results are presented in Figure 3 using MolR as the GNN backbone and Vicuna as the language model. It can be observed in the table that ReLM’s performance remains relatively stable</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Accuracy under different <em>K</em> values</p>
<p>Table 8: Accuracy over fixed and randomized in-context confidence scores.</p>
<table>
<thead>
<tr>
<th></th>
<th>Imidazo</th>
<th>NiCOlit</th>
<th>rexgen-30k</th>
<th>rexgen-40k</th>
</tr>
</thead>
<tbody>
<tr>
<td>GNN only (MolR)</td>
<td>51.30%</td>
<td>43.70%</td>
<td>47.13%</td>
<td>44.89%</td>
</tr>
<tr>
<td>Fixed: {1} and {9}</td>
<td>91.93%</td>
<td>53.94%</td>
<td>48.65%</td>
<td>45.62%</td>
</tr>
<tr>
<td>Randomized: {1,2} and {8,9}</td>
<td>87.76%</td>
<td>52.32%</td>
<td>49.88%</td>
<td>47.27%</td>
</tr>
<tr>
<td>Randomized: {1,2,3} and {7,8,9}</td>
<td>91.41%</td>
<td>53.78%</td>
<td>48.51%</td>
<td>45.52%</td>
</tr>
</tbody>
</table>
<p>as <em>K</em> increases across a wide range of values (e.g. <em>K</em> = 2 ~ 7), and at some point, the performance gets better as <em>K</em> increases. However, with very large <em>K</em> values (e.g. <em>K</em> ≥ 10), the accuracy of the model exhibited a noticeable decrease.</p>
<p>This drop in accuracy with extremely large <em>K</em> values is likely because the candidates become overly saturated with implausible options, making it more difficult for the language model to discern the correct answer. Since there could be at most one ground truth candidate, increasing <em>K</em> means adding more distracting and unlikely candidates. While the model is robust to these distractors up to a point, at some threshold of implausible options the task does become more challenging.</p>
<h3>B.7 Influence of In-context Randomness</h3>
<p>In Section 3.1 we introduce the generation process of in-context examples. For each in-context example, we choose a random integer within {8, 9} (or {1, 2}) as its confidence score. This intuitive random selection aims to faithfully mimic human behavior when answering the multiple-choice chemistry question. In this section, we demonstrate the efficacy of this strategy through comparative experiments.</p>
<p>In Table 8 we show the performance of ReLM under different levels of in-context randomness. The experiments are carried out using <em>K</em> = 4, Vicuna as the language model, and MolR as the GNN.</p>
<p>Table 9: ReLM with other strategies.</p>
<table>
<thead>
<tr>
<th></th>
<th>Imidazo</th>
<th>NiCOlit</th>
<th>rexgen-30k</th>
<th>rexgen-40k</th>
</tr>
</thead>
<tbody>
<tr>
<td>MolR</td>
<td>0.513</td>
<td>0.437</td>
<td>0.471</td>
<td>0.449</td>
</tr>
<tr>
<td>Zero-shot</td>
<td>0.870</td>
<td>0.446</td>
<td>0.301</td>
<td>0.286</td>
</tr>
<tr>
<td>Few-shot CoT</td>
<td>0.781</td>
<td>0.305</td>
<td>0.267</td>
<td>0.244</td>
</tr>
<tr>
<td>Zero-shot CoT</td>
<td>0.844</td>
<td>0.415</td>
<td>0.318</td>
<td>0.302</td>
</tr>
<tr>
<td>CSS</td>
<td>0.878</td>
<td>0.523</td>
<td>0.499</td>
<td>0.473</td>
</tr>
<tr>
<td>Upper Bound</td>
<td>0.979</td>
<td>0.679</td>
<td>0.611</td>
<td>0.587</td>
</tr>
</tbody>
</table>
<p>In the table, we can observe that using varying confidence scores causes slight fluctuations in experimental outcomes, though the degree varies by dataset. This further demonstrates that the effectiveness of our ReLM lies not in the details of prompt design. Instead, it stems from our main idea of amalgamation of the molecular modeling ability inherent to GNNs with the vast reaction knowledge of the language model.</p>
<h3>B.8 Comparison with More Prompting Strategies</h3>
<p>In addition to the prompting strategies in Section 4.3, we also included Zero-shot, Few-shot CoT, and Zero-Shot CoT as baseline prompting methods. Table B.8 shows the experimental results of these methods.</p>
<p>We observe that all prompt designs offer some benefits. However, our original confidence score approach still outperforms these baselines. Specifically, as shown in the case studies in Section C.3, the language model's CoT analysis may not provide additional insightful information regarding the reaction mechanism. At this stage, incorporating additional analytical steps may introduce more molecular structures, exacerbating the language model's comprehension difficulties. Further step-by-step elucidation of the reaction process likely necessitates incorporating more domain knowledge of chemical reactions.</p>
<h3>C Case Studies</h3>
<h4>C.1 Importance of Reaction Condition</h4>
<p>Previous GNN-based reaction analysis models could only process molecular graphs and were unable to utilize the abundant information contained within chemical reaction conditions. However, many chemical reaction conditions exert profound influences on the reaction mechanisms, determining the direction of synthesis. In this section, we illustrate the importance of reaction conditions to reaction outcomes through an example from the Imidazo dataset.</p>
<p>In Figure 4, we report the answers from GNN and ChatGPT to two chemical reaction problems involving the same reactants. The only difference between these two problems is that problem 2 does not include reaction conditions and types. Question 1 is the target reaction given the correct catalyst, while Question 2 omits the important catalyst. For GNN, it incorrectly predicts 'B' as the answer for both questions. In contrast, the language model accurately identifies option 'C' for Question 1, which aligns with the ground truth. However, for Question 2, the language model predict a product that was not present in the candidate pool.</p>
<p>In reality, this organic reaction involves three components - an aldehyde, an amine, and an isocyanide. Without an added catalyst, the combination of these three reactants undergoes a Passerini reaction, which is a type of multi-component condensation. Nevertheless, in the presence of the trifluoroacetic acid catalyst, the reaction proceeds via an imidazopyridine formation mechanism. This leads to the major product being 'C', 3-[3-(cyclohexylamino)-6-methylimidazo[1,2-a]pyridin-2-yl]benzene-1,2-diol.</p>
<p>These case studies clearly demonstrate that identical reactants yield different products under different reaction conditions, further reinforcing our driving hypothesis: the incorporation of reaction type and condition boosts the predictive accuracy of ReLM.</p>
<h3>C.2 Illustration of Inferring Process</h3>
<p>In Section 4.2, we mentioned that the accuracy upper limit of ReLM is determined by the hit@ $K$ metric of the backbone GNN model. In this section, we furnish an illustrative example to elucidate our proposed methodology more comprehensively. Figure 5 provides a step-by-step demonstration of our method when $K=4$. The in-context examples have been omitted for brevity.</p>
<h3>C.3 Examples of Prompting Strategies</h3>
<p>In Section 4.3 we compare the confidence score strategy with other prompting strategies. In Figure 6, we use an example from the NiCOlit dataset to further demonstrate the difference between the involved prompting strategies.</p>
<h2>D Statistical Analysis</h2>
<p>In this paper, we posit that language models possess the capability to comprehend confidence scores and
assign corresponding scores based on their familiarity with the questions. In this section, we elucidate this claim by presenting the distribution of these scores across a large number of samples, that confidence scores can indicate ReLM's degree of comprehension of the queries.</p>
<p>We illustrate the distribution of confidence scores using the Rexgen-40 dataset, employing Vicuna + MolR as backbones. The results are presented in Figure 7. Confidence scores of ReLM exhibit significantly different distributions between correct and incorrect answers. For incorrect ReLM choices, the proportion of high confidence scores $(7 \sim 9)$ decreased from $82.3 \%$ to $51.2 \%$, while that of low confidence scores $(1 \sim 3)$ increased from $4.4 \%$ to $22.7 \%$.</p>
<p>Additionally, we designed experiments to compare the accuracy of the large model at different confidence levels. We divide the datasets into two parts based on the model's output confidence and calculate the accuracy separately. The results are shown in Table 10. The "High Conf." column represents the model's accuracy on the subset with higher confidence, and the "Low Conf." column represents the accuracy on the part with lower confidence. For all control groups, ReLM's accuracy on high-confidence samples is significantly higher than those with lower confidence ( $p$-value $« 0.05$ ).</p>
<p>Table 10: Accuracy under high/low confidence levels. The "High Conf." column represents the accuracy of the model on the subset with higher confidence, and the "Low Conf." column represents the lower ones. ReLM's accuracy on high-confidence samples is significantly higher than those with lower confidence ( $p$-value $=0.05$ ).</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\begin{gathered} \text { Rexgen-30k } \ \mathrm{k}=3 \end{gathered}$</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\begin{gathered} \text { Rexgen-40k } \ \mathrm{k}=3 \end{gathered}$</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">MolR</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.471</td>
<td style="text-align: center;">0.448</td>
<td style="text-align: center;">0.448</td>
</tr>
<tr>
<td style="text-align: center;">ReLM (MolR + Vicuna)</td>
<td style="text-align: center;">High Conf.</td>
<td style="text-align: center;">$0.500 \pm 0.235$</td>
<td style="text-align: center;">$0.502 \pm 0.234$</td>
<td style="text-align: center;">$0.477 \pm 0.229$</td>
<td style="text-align: center;">$0.478 \pm 0.229$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Low Conf.</td>
<td style="text-align: center;">$0.361 \pm 0.300$</td>
<td style="text-align: center;">$0.329 \pm 0.305$</td>
<td style="text-align: center;">$0.330 \pm 0.291$</td>
<td style="text-align: center;">$0.288 \pm 0.293$</td>
</tr>
<tr>
<td style="text-align: center;">ReLM (MolR + GPT-3.5)</td>
<td style="text-align: center;">High Conf.</td>
<td style="text-align: center;">$0.525 \pm 0.201$</td>
<td style="text-align: center;">$0.525 \pm 0.257$</td>
<td style="text-align: center;">$0.499 \pm 0.160$</td>
<td style="text-align: center;">$0.510 \pm 0.198$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Low Conf.</td>
<td style="text-align: center;">$0.438 \pm 0.268$</td>
<td style="text-align: center;">$0.333 \pm 0.313$</td>
<td style="text-align: center;">$0.400 \pm 0.245$</td>
<td style="text-align: center;">$0.440 \pm 0.254$</td>
</tr>
<tr>
<td style="text-align: center;">LocalRetro</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.279</td>
<td style="text-align: center;">0.245</td>
<td style="text-align: center;">0.245</td>
</tr>
<tr>
<td style="text-align: center;">ReLM (LocalRetro + Vicuna)</td>
<td style="text-align: center;">High Conf.</td>
<td style="text-align: center;">$0.330 \pm 0.163$</td>
<td style="text-align: center;">$0.332 \pm 0.190$</td>
<td style="text-align: center;">$0.299 \pm 0.151$</td>
<td style="text-align: center;">$0.302 \pm 0.178$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Low Conf.</td>
<td style="text-align: center;">$0.235 \pm 0.203$</td>
<td style="text-align: center;">$0.213 \pm 0.220$</td>
<td style="text-align: center;">$0.238 \pm 0.181$</td>
<td style="text-align: center;">$0.190 \pm 0.203$</td>
</tr>
<tr>
<td style="text-align: center;">ReLM (LocalRetro + GPT-3.5)</td>
<td style="text-align: center;">High Conf.</td>
<td style="text-align: center;">$0.368 \pm 0.130$</td>
<td style="text-align: center;">$0.405 \pm 0.132$</td>
<td style="text-align: center;">$0.321 \pm 0.117$</td>
<td style="text-align: center;">$0.337 \pm 0.150$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Low Conf.</td>
<td style="text-align: center;">$0.248 \pm 0.203$</td>
<td style="text-align: center;">$0.270 \pm 0.219$</td>
<td style="text-align: center;">$0.200 \pm 0.181$</td>
<td style="text-align: center;">$0.262 \pm 0.193$</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The answers from GNN and ChatGPT to two similar chemical reaction problems. The only difference between these two questions is the existence of reaction conditions. The GNN incorrectly predicts ' B ' as the answer for both questions. In contrast, the language model accurately identifies ' C ' as the answer for the first problem, but fails to predict the second question. These case studies demonstrate that identical reactants yield different products under different reaction conditions.</p>
<p>Step 1: A reaction is sampled from the test set. Its associated reactants, reaction conditions, and other relevant information are provided as input.</p>
<ul>
<li>reactants 1: 2,3-dihydroxybenzaldehyde</li>
<li>reactants 2: 5-methylpyridin-2-amine</li>
<li>reactants 3: isocyanocyclohexane</li>
<li>catalyst 1: 2,2,2-trifluoroacetic acid</li>
<li>Temperature: $22( \pm 1)^{\circ} \mathrm{C}$</li>
<li>reaction type: a multicomponent reaction that leads to the synthesis of imidazopyridines.</li>
</ul>
<p>Step 2: The GNN generates a ranked list of the top-4 product candidates (i.e., the four choices $A, B, C$, and D in step 4) for the above set of reactants.</p>
<p>Step 3: Proper in-context examples are chosen from the training set using the GNN model.</p>
<p>Step 4: Generate prompts based on Top- $K$ possible answer candidates from the GNN model.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Step 5: The LM delivers its selection accompanied by an associated confidence score.
Ground Truth: C. 3-[3-(cyclohexylamino)-6-methylimidazo[1,2-a]pyridin-2-yl]benzene-1,2-diol
GNN: A. methyl 4-isocyanocyclohexane-1-carboxylate
Answer: C
Confidence Score: 7
Figure 5: The step-by-step illustration of the inferring process. The test case is from the Imidazo dataset.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Illustration of different prompting strategies. The in-context examples in the few-shot CoT and Confidence Score strategies have been omitted for brevity.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Distribution of Confidence Scores on Rexgen-40 dataset.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Corresponding author
${ }^{\dagger}$ Xiang Wang is also affiliated with Institute of Artificial Intelligence, Institute of Dataspace, Hefei Comprehensive National Science Center.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
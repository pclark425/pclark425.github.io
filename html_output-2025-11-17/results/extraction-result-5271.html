<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5271 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5271</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5271</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-dc7307f6e49f0ad2432be4eea92ae7f854c3cebb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/dc7307f6e49f0ad2432be4eea92ae7f854c3cebb" target="_blank">Hierarchical Generation of Molecular Graphs using Structural Motifs</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Machine Learning</p>
                <p><strong>Paper TL;DR:</strong> A new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks and significantly outperforms previous state-of-the-art baselines is proposed.</p>
                <p><strong>Paper Abstract:</strong> Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5271",
    "paper_id": "paper-dc7307f6e49f0ad2432be4eea92ae7f854c3cebb",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005765999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Hierarchical Generation of Molecular Graphs using Structural Motifs</h1>
<p>Wengong Jin ${ }^{1}$ Regina Barzilay ${ }^{1}$ Tommi Jaakkola ${ }^{1}$</p>
<h4>Abstract</h4>
<p>Graph generation techniques are increasingly being adopted for drug discovery. Previous graph generation approaches have utilized relatively small molecular building blocks such as atoms or simple cycles, limiting their effectiveness to smaller molecules. Indeed, as we demonstrate, their performance degrades significantly for larger molecules. In this paper, we propose a new hierarchical graph encoder-decoder that employs significantly larger and more flexible graph motifs as basic building blocks. Our encoder produces a multi-resolution representation for each molecule in a fine-to-coarse fashion, from atoms to connected motifs. Each level integrates the encoding of constituents below with the graph at that level. Our autoregressive coarse-to-fine decoder adds one motif at a time, interleaving the decision of selecting a new motif with the process of resolving its attachments to the emerging molecule. We evaluate our model on multiple molecule generation tasks, including polymers, and show that our model significantly outperforms previous state-of-the-art baselines.</p>
<h2>1. Introduction</h2>
<p>Deep learning models for molecule property prediction and molecule generation are improving at a fast pace. Work to date has adopted primarily two types of building blocks for representing and building molecules: atom-by-atom strategies (Li et al., 2018; You et al., 2018a; Liu et al., 2018), or substructure based (either rings or bonds) (Jin et al., 2018; 2019). While these methods have been successful for small molecules, their performance degrades significantly for larger molecules such as polymers (see Figure 1). The failure is likely due to many generation steps required to realize larger molecules and the associated challenges with gradients across the iterative steps.</p>
<p>Large molecules such as polymers exhibit clear hierarchi-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>cal structure, being built from repeated structural motifs. We hypothesize that explicitly incorporating such motifs as building blocks in the generation process can significantly improve reconstruction and generation accuracy, as already illustrated in Figure 1. While different substructures as building blocks were considered in previous work (Jin et al., 2018), their approach could not scale to larger motifs. Indeed, their decoding process required each substructure neighborhood to be assembled in one go, making it combinatorially challenging to handle large components with many possible attachment points.</p>
<p>In this paper, we propose a motif-based hierarchical encoderdecoder for graph generation. The motifs themselves are extracted separately at the outset from frequently occurring substructures, regardless of size. During generation, molecules are built step by step by attaching motifs, large or small, to the emerging molecule. The decoder operates hierarchically, in a coarse-to-fine manner, and makes three key consecutive predictions in each pass: new motif selection, which part of it attaches, and the points of contact with the current molecule. These decisions are highly coupled and naturally modeled auto-regressively. Moreover, each decision is directly guided by the information explicated in the associated layer of the mirroring hierarchical encoder. The feed-forward fine-to-coarse encoding performs iterative graph convolutions at each level, conditioned on the results from layer below.</p>
<p>The proposed model is evaluated on various tasks ranging from polymer generative modeling to graph translation for molecule property optimization. Our baselines include state-of-the-art graph generation methods (You et al., 2018a; Liu et al., 2018; Jin et al., 2019). On polymer generation, our model achieved state-of-the art results under various metrics, outperforming the best baselines with $20 \%$ absolute improvement in reconstruction accuracy. On graph translation tasks, our model outperformed all the baselines, yielding $3.3 \%$ and $8.1 \%$ improvement on QED and DRD2 optimization tasks. During decoding, our model runs 6.3 times faster than previous substructure-based methods (Jin et al., 2019). We further conduct ablation studies to validate the advantage of using larger motifs and model architecture.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. Left: Illustration of structural motifs in polymers. Right: Reconstruction accuracy for polymers with various sizes (number of atoms). Notably, the atom-based generative model CG-VAE (Liu et al., 2018) fails to reconstruct molecules over 80 atoms. In contrast, the proposed model maintains high accuracy for large molecules by utilizing motifs as building blocks for generation (red curve).</p>
<h2>2. Background and Motivation</h2>
<p>Molecules are represented as graphs $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ with atoms $\mathcal{V}$ as nodes and bonds $\mathcal{E}$ as edges. Graphs are challenging objects to generate, especially for larger molecules such as polymers. For the polymer dataset used in our experiment, there are thousands of molecules with more than 80 atoms. To illustrate the challenge, we tested two state-of-the-art variational autoencoders (Liu et al., 2018; Jin et al., 2018) on this dataset and found these models often fail to reconstruct molecules from their latent embedding (see Figure 1).</p>
<p>The reason of this failure is that these methods generate molecules based on small building blocks. In terms of autoregressive models, previous work on molecular graph generation can be roughly divided in two categories: ${ }^{1}$</p>
<ul>
<li>Atom-based methods (Li et al., 2018; You et al., 2018a; Liu et al., 2018) generate molecules atom by atom.</li>
<li>Substructure-based methods (Jin et al., 2018; 2019) generates molecules based on small substructures restricted to rings and bonds (often no more than six atoms).</li>
</ul>
<p>As the building blocks are typically small, it requires many decoding steps for current models to reconstruct polymers. Therefore they are prone to make errors when generating large molecules. On the other hand, many of these molecules consist of structural motifs beyond simple substructures. The number of decoding steps can be significantly reduced if graphs are generated motif by motif. As shown in Figure 1, our motif-based method achieves a much higher reconstruction accuracy.</p>
<p>Motivation for New Architecture Current substructurebased method (Jin et al., 2018) requires a combinatorial</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>enumeration to assemble substructures whose time complexity is exponential to substructure size. Their enumeration algorithm assumes the substructures to be of certain types (single cycles or bonds). In practice, their method often fails when handling rings with more than 10 atoms (e.g., memory error). Unlike substructures, motifs are typically much larger and can have flexible structures (see Figure 1). As a result, this method cannot be directly extended to utilize motifs in practice.</p>
<p>To this end, we propose a hierarchical encoder-decoder for graph generation. Our decoder allows arbitrary types of motifs and can assemble them efficiently without combinatorial explosion. Our encoder learns a hierarchical representation that allows the decoding process to depend on both coarsegrained motif and fine-grained atom connectivity.</p>
<h3>2.1. Motif Extraction</h3>
<p>We define a motif $\mathcal{S}<em i="i">{i}=\left(\mathcal{V}</em>}, \mathcal{E<em i="i">{i}\right)$ as a subgraph of molecule $\mathcal{G}$ induced by atoms in $\mathcal{V}</em>}$ and bonds in $\mathcal{E<em 1="1">{i}$. Given a molecule, we extract its motifs $\mathcal{S}</em>}, \cdots, \mathcal{S<em i="i">{n}$ such that their union covers the entire molecular graph: $\mathcal{V}=\bigcup</em>} \mathcal{V<em i="i">{i}$ and $\mathcal{E}=\bigcup</em>$ into disconnected fragments by breaking all the bridge bonds that will not violate chemical validity (illustrations in the appendix).} \mathcal{E}_{i}$. To extract motifs, we decompose a molecule $\mathcal{G</p>
<ol>
<li>Find all the bridge bonds $(u, v) \in \mathcal{E}$, where both $u$ and $v$ have degree $\Delta_{u}, \Delta_{v} \geq 2$ and either $u$ or $v$ is part of a ring. Detach all the bridge bonds from its neighbors.</li>
<li>Now the graph $\mathcal{G}$ becomes a set of disconnected subgraphs $\mathcal{G}<em N="N">{1}, \cdots, \mathcal{G}</em>$ if its occurrence in the training set is more than $\Delta=100$.}$. Select $\mathcal{G}_{i}$ as motif in $\mathcal{G</li>
<li>If $\mathcal{G}_{i}$ is not selected as motif, further decompose it into rings and bonds and select them as motif in $\mathcal{G}$.</li>
</ol>
<p>We apply the above procedure to all the molecules in the training set and construct a vocabulary of motifs $V_{\mathcal{S}}$. In the following section, we will describe how we encode and decode molecules using the extracted motifs.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Hierarchical graph encoder. Dashed arrows connect each atom to the motifs it belongs. In the attachment layer, each node $\mathcal{A}<em i="i">{i}$ is a particular attachment configuration of motif $\mathcal{S}</em>$. The atoms in the intersection between each motif and its neighbors are highlighted in faded block.</p>
<h2>3. Hierarchical Graph Generation</h2>
<p>Our approach extends the variational autoencoder (Kingma \&amp; Welling, 2013) to molecular graphs by introducing a hierarchical decoder and a matching encoder. In our framework, the probability of a graph $\mathcal{G}$ is modeled as a joint distribution over structural motifs $\mathcal{S}<em n="n">{1}, \cdots, \mathcal{S}</em>}$ constituting $\mathcal{G}$, together with their attachments $\mathcal{A<em n="n">{1}, \cdots, \mathcal{A}</em>}$. Each attachment $\mathcal{A<em j="j">{i}=\left{v</em>} \mid v_{j} \in \bigcup_{k} \mathcal{S<em k="k">{i} \cap \mathcal{S}</em>)$ :}\right}$ indicates the intersecting atoms between $\mathcal{S}_{i}$ and its neighbor motifs. To capture complex dependencies involved in the joint distribution of motifs and their attachments, we propose an auto-regressive factorization of $P(\mathcal{G</p>
<p>$$
P(\mathcal{G})=\int_{\boldsymbol{z}} P(\boldsymbol{z}) \prod_{k} P\left(\mathcal{S}<em k="k">{k}, \mathcal{A}</em>} \mid \mathcal{S<em _k="&lt;k">{&lt;k}, \mathcal{A}</em>
$$}, \boldsymbol{z}\right) d \boldsymbol{z</p>
<p>As illustrated in Figure 3, in each generation step, our decoder adds a new motif $\mathcal{S}<em k="k">{k}$ (motif prediction) and its attachment configuration $\mathcal{A}</em>$ (attachment prediction). Then it decides how the new motif should be attached to the current graph (graph prediction).</p>
<p>To support the above hierarchical generation, we need to design a matching encoder representing molecules at multiple resolutions in order to provide necessary information for each decoding step. Therefore, we propose to represent a molecule $\mathcal{G}$ by a hierarchical graph $\mathcal{H}_{\mathcal{G}}$ with three layers (see Figure 2):
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Hierarchical graph decoder. In each step, the decoder first runs hierarchical message passing to compute motif, attachment and atom vectors. Then it performs motif and attachment prediction for the next motif node. Finally, it decides how the new motif should be attached to the current graph via graph prediction.</p>
<ol>
<li>Motif layer: This layer represents how the motifs are coarsely connected in the graph. This layer provides essential information for the motif prediction in the decoding process. Specifically, this layer contains $n$ nodes $\mathcal{S}<em n="n">{1}, \cdots, \mathcal{S}</em>}$ and $m$ edges $\left{\left(\mathcal{S<em j="j">{i}, \mathcal{S}</em>}\right) \mid \mathcal{S<em j="j">{i} \cap \mathcal{S}</em>} \neq \emptyset\right}$ for all intersecting motifs $\mathcal{S<em j="j">{i}, \mathcal{S}</em>$. This layer is tree-structured due to our way of constructing motifs.</li>
<li>Attachment layer: This layer encodes the connectivity between motifs at a fine-grained level. Each node $\mathcal{A}<em i="i">{i}=\left(\mathcal{S}</em>},\left{v_{j}\right}\right)$ in this layer represents a particular attachment configuration of motif $\mathcal{S<em j="j">{i}$, where $\left{v</em>}\right}$ are atoms in the intersection between $\mathcal{S<em i="i">{i}$ and one of its neighbor motifs (see Figure 2). This layer provides crucial information for the attachment prediction step during decoding, which helps reducing the space of candidate attachments between $\mathcal{S}</em>}$ and its neighbor motifs. Just like the motif vocabulary $V_{\mathcal{S}}$, all the attachment configurations of $\mathcal{S<em _mathcal_A="\mathcal{A">{i}$ form a motif-specific vocabulary $V</em>$}}\left(\mathcal{S}_{i}\right)$, which is computed from the training set. ${ }^{2</li>
<li>Atom layer: The atom layer is the molecular graph $\mathcal{G}$ representing how its atoms are connected. Each atom node $v$ is associated with a label $a_{v}$ indicating its atom type and charge. Each edge $(u, v)$ in the atom layer is</li>
</ol>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>labeled with $b_{u v}$ indicating its bond type. This layer provides necessary information for the graph prediction step during decoding.</p>
<p>We further introduce edges that connect the atoms and motifs between different layers in order to propagate information in between. In particular, we draw a directed edge from atom $v$ in the atom layer to node $\mathcal{A}<em i="i">{i}$ in the attachment layer if $v \in \mathcal{S}</em>}$. We also draw edges from $\mathcal{A<em i="i">{i}$ to $\mathcal{S}</em>}$ in the motif layer. This gives us the hierarchical graph $\mathcal{H<em i="i">{\mathcal{G}}$ for molecule $\mathcal{G}$, which will be encoded by a hierarchical message passing network (MPN). During encoding, each node $\mathcal{S}</em>}$ is represented as a one-hot encoding in the motif vocabulary $V_{\mathcal{S}}$. Likewise, each node $\mathcal{A<em _mathcal_A="\mathcal{A">{i}$ is represented as a one-hot encoding in the attachment vocabulary $V</em>\right)$.}}\left(\mathcal{S}_{i</p>
<h3>3.1. Hierarchical Graph Encoder</h3>
<p>Our encoder contains three MPNs that encode each of the three layers in the hierarchical graph. For simplicity, we denote the MPN encoding process as $\mathrm{MPN}_{\psi}(\cdot)$ with parameter $\psi$, and denote $\operatorname{MLP}(\boldsymbol{x}, \boldsymbol{y})$ as a multi-layer neural network whose input is the concatenation of $\boldsymbol{x}$ and $\boldsymbol{y}$. The details of MPN architecture is listed in the appendix.</p>
<p>Atom Layer MPN We first encode the atom layer of $\mathcal{H}<em _mathcal_G="\mathcal{G">{\mathcal{G}}$ (denoted as $\mathcal{H}</em>$ for each atom $v$ :}}^{g}$ ). The inputs to this MPN are the embedding vectors $\left{\boldsymbol{e}\left(a_{u}\right)\right},\left{\boldsymbol{e}\left(b_{u v}\right)\right}$ of all the atoms and bonds in $\mathcal{G}$. During encoding, the network propagates the message vectors between different atoms for $T$ iterations and then outputs the atom representation $\boldsymbol{h}_{v</p>
<p>$$
\boldsymbol{c}<em v="v">{\mathcal{G}}^{g}=\left{\boldsymbol{h}</em>}\right}=\operatorname{MPN<em 1="1">{\psi</em>}}\left(\mathcal{H<em u="u">{\mathcal{G}}^{g},\left{\boldsymbol{e}\left(a</em>\right)\right}\right)
$$}\right)\right},\left{\boldsymbol{e}\left(b_{u v</p>
<p>Attachment Layer MPN The input feature of each node $\mathcal{A}<em _mathcal_G="\mathcal{G">{i}$ in the attachment layer $\mathcal{H}</em>}}^{a}$ is an concatenation of the embedding $\boldsymbol{e}\left(\mathcal{A<em v="v">{i}\right)$ and the sum of its atom vectors $\left{\boldsymbol{h}</em>\right}$} \mid v \in\right.$ $\left.\mathcal{S}_{i</p>
<p>$$
\boldsymbol{f}<em i="i">{\mathcal{A}</em>}}=\operatorname{MLP}\left(\boldsymbol{e}\left(\mathcal{A<em _in="\in" _mathcal_S="\mathcal{S" v="v">{i}\right), \sum</em><em v="v">{i}} \boldsymbol{h}</em>\right)
$$</p>
<p>The input feature for each edge $\left(\mathcal{A}<em j="j">{i}, \mathcal{A}</em>}\right)$ in this layer is an embedding vector $\boldsymbol{e}\left(d_{i j}\right)$, where $d_{i j}$ describes the relative ordering between node $\mathcal{A<em j="j">{i}$ and $\mathcal{A}</em>}$ during decoding. Specifically, we set $d_{i j}=k$ if node $\mathcal{A<em j="j">{i}$ is the $k$-th child of node $\mathcal{A}</em>}$ and $d_{i j}=0$ if $\mathcal{A<em _mathcal_G="\mathcal{G">{i}$ is the parent. We then run $T$ iterations of message passing over $\mathcal{H}</em>$ to compute the motif representations:}}^{a</p>
<p>$$
\boldsymbol{c}<em _mathcal_A="\mathcal{A">{\mathcal{G}}^{a}=\left{\boldsymbol{h}</em><em _psi__2="\psi_{2">{i}}\right}=\operatorname{MPN}</em>}}\left(\mathcal{H<em _mathcal_A="\mathcal{A">{\mathcal{G}}^{a},\left{\boldsymbol{f}</em><em i="i" j="j">{i}}\right},\left{\boldsymbol{e}\left(d</em>\right)\right}\right)
$$</p>
<p>Motif Layer MPN Similarly, the input feature of node $\mathcal{S}<em i="i">{i}$ in this layer is computed as the concatenation of embedding $\boldsymbol{e}\left(\mathcal{S}</em>}\right)$ and the node vector $\boldsymbol{h<em i="i">{\mathcal{A}</em>$ to obtain the motif representations:}}$ from the previous layer. Finally, we run message passing over the motif layer $\mathcal{H}_{\mathcal{G}}^{a</p>
<p>$$
\boldsymbol{f}<em i="i">{\mathcal{S}</em>}}=\operatorname{MLP}\left(\boldsymbol{e}\left(\mathcal{S<em _mathcal_A="\mathcal{A">{i}\right), \boldsymbol{h}</em>\right)
$$}_{i}</p>
<p>$$
\boldsymbol{c}<em _mathcal_S="\mathcal{S">{\mathcal{G}}^{a}=\left{\boldsymbol{h}</em><em _psi__3="\psi_{3">{i}}\right}=\operatorname{MPN}</em>}}\left(\mathcal{H<em _mathcal_S="\mathcal{S">{\mathcal{G}}^{a},\left{\boldsymbol{f}</em><em i="i" j="j">{i}}\right},\left{\boldsymbol{e}\left(d</em>\right)\right}\right)
$$</p>
<p>Finally, we represent a molecule $\mathcal{G}$ by a latent vector $\boldsymbol{z}<em _mathcal_S="\mathcal{S">{\mathcal{G}}$ sampled through reparameterization trick with mean $\boldsymbol{\mu}\left(\boldsymbol{h}</em><em _mathcal_S="\mathcal{S">{1}}\right)$ and log variance $\boldsymbol{\Sigma}\left(\boldsymbol{h}</em>\right)$ :}_{1}</p>
<p>$$
\boldsymbol{z}<em _mathcal_S="\mathcal{S">{\mathcal{G}}=\boldsymbol{\mu}\left(\boldsymbol{h}</em><em _mathcal_S="\mathcal{S">{1}}\right)+\exp \left(\boldsymbol{\Sigma}\left(\boldsymbol{h}</em>)
$$}_{1}}\right)\right) \cdot \epsilon ; \quad \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I</p>
<p>where $\mathcal{S}_{1}$ is the root motif (i.e., the first motif to be generated during reconstruction).</p>
<h3>3.2. Hierarchical Graph Decoder</h3>
<p>As illustrated in Figure 3, our graph decoder generates a molecule $\mathcal{G}$ by incrementally expanding its hierarchical graph. In $t^{\text {th }}$ generation step, we first use the same hierarchical MPN architecture to encode all the motifs and atoms in $\mathcal{H}<em _mathcal_S="\mathcal{S">{\mathcal{G}}^{(t)}$, the (partial) hierarchical graph generated till step $t$. This gives us motif vectors $\boldsymbol{h}</em><em v__j="v_{j">{k}}$ and atom vectors $\boldsymbol{h}</em>$ for the existing motifs and atoms.}</p>
<p>During decoding, the model maintains a set of frontier nodes $\mathcal{F}$ where each node $\mathcal{S}<em k="k">{k} \in \mathcal{F}$ is a motif that still has neighbors to be generated. $\mathcal{F}$ is implemented as a stack because motifs are generated in their depth-first order. Suppose $\mathcal{S}</em>$ :}$ is at the top of stack $\mathcal{F}$ in step $t$, the model makes the following predictions conditioned on latent representation $\boldsymbol{z}_{\mathcal{G}</p>
<ol>
<li>Motif Prediction: The model predicts the next motif $\mathcal{S}<em k="k">{t}$ to be attached to $\mathcal{S}</em>$ :}$. This is cast as a classification task over the motif vocabulary $V_{\mathcal{S}</li>
</ol>
<p>$$
\boldsymbol{p}<em t="t">{\mathcal{S}</em>}}=\operatorname{softmax}\left(\operatorname{MLP}\left(\boldsymbol{h<em k="k">{\mathcal{S}</em>\right)\right)
$$}}, \boldsymbol{z}_{\mathcal{G}</p>
<ol>
<li>Attachment Prediction: Now the model needs to predict the attachment configuration $\mathcal{A}<em t="t">{t}$ of motif $\mathcal{S}</em>}$ (i.e., what atoms $v_{j} \in \mathcal{S<em t="t">{t}$ belong to the intersection of $\mathcal{S}</em>\right)$ :}$ and its neighbor motifs). This is also cast as a classification task over the attachment vocabulary $V_{\mathcal{A}}\left(\mathcal{S}_{t</li>
</ol>
<p>$$
\boldsymbol{p}<em t="t">{\mathcal{A}</em>}}=\operatorname{softmax}\left(\operatorname{MLP}\left(\boldsymbol{h<em k="k">{\mathcal{S}</em>\right)\right)
$$}}, \boldsymbol{z}_{\mathcal{G}</p>
<p>This prediction step is crucial because it significantly reduces the space of possible attachments between $\mathcal{S}<em t="t">{t}$ and its neighbor motifs.
3. Graph Prediction: Finally, the model must decide how $\mathcal{S}</em>}$ should be attached to $\mathcal{S<em t="t">{k}$. The attachment between $\mathcal{S}</em>}$ and $\mathcal{S<em k="k" t="t">{k}$ is defined as atom pairs $\mathcal{M}</em>}=\left{\left(u_{j}, v_{j}\right) \mid u_{j} \in\right.$ $\left.\mathcal{A<em j="j">{k}, v</em>} \in \mathcal{A<em j="j">{t}\right}$ where atom $u</em>}$ and $v_{j}$ are attached together. The probability of a candidate attachment $M$ is computed based on the atom vectors $\boldsymbol{h<em j="j">{u</em>}}$ and $\boldsymbol{h<em j="j">{v</em>$ :}</p>
<p>$$
\begin{aligned}
&amp; \boldsymbol{p}<em M="M">{M}=\operatorname{softmax}\left(\boldsymbol{h}</em>} \cdot \boldsymbol{z<em M="M">{\mathcal{G}}\right) \
&amp; \boldsymbol{h}</em>}=\sum_{j} \operatorname{MLP}\left(\boldsymbol{h<em j="j">{u</em>}}, \boldsymbol{h<em j="j">{v</em>\right)
\end{aligned}
$$}</p>
<p>The number of possible attachments are limited because the number of attaching atoms between two motifs is small and the attaching points must be consecutive. ${ }^{3}$</p>
<p>The above three predictions together give an autoregressive factorization of the distribution over the next motif and its attachment. Each of the three decoding steps depends on the outcome of previous step, and predicted attachments will in turn affect the prediction of subsequent motifs.</p>
<p>Training During training, we apply teacher forcing to the above generation process, where the generation order is determined by a depth-first traversal over the ground truth molecule. Given a training set of molecules, we seek to minimize the negative ELBO:</p>
<p>$$
-\mathbb{E}<em _mathrm_KL="\mathrm{KL">{\boldsymbol{z} \sim Q}[\log P(\mathcal{G} \mid \boldsymbol{z})]+\lambda</em>)]
$$}} \mathcal{D}_{\mathrm{KL}}[Q(\boldsymbol{z} \mid \mathcal{G}) | P(\boldsymbol{z</p>
<h3>3.3. Extension to Graph-to-Graph Translation</h3>
<p>The proposed architecture can be naturally extended to graph-to-graph translation (Jin et al., 2019) for molecular optimization, which seeks to modify compounds in order to improve their biochemical properties. Given a corpus of molecular pairs ${(X, Y)}$, where $Y$ is a structural analog of $X$ with better chemical properties, the model is trained to translate an input molecular graph into its better form. In this case, we seek to learn a translation model $P(Y \mid X)$ parameterized by our encoder-decoder architecture. We also introduce attention layers into our model, which is crucial for translation performance (Bahdanau et al., 2014).</p>
<p>Training In graph translation, a compound $X$ can be associated with multiple outputs $Y$ since there are many ways to modify $X$ to improve its properties. In order to generate diverse outputs, we follow previous work (Zhu et al., 2017; Jin et al., 2019) and incorporate latent variables $\boldsymbol{z}$ to the translation model:</p>
<p>$$
P(Y \mid X)=\int_{\boldsymbol{z}} P(Y \mid X, \boldsymbol{z}) P(\boldsymbol{z}) d \boldsymbol{z}
$$</p>
<p>where the latent vector $\boldsymbol{z}$ indicates the intended mode of translation, sampled from a prior $P(\boldsymbol{z})$ during testing.</p>
<p>The model is trained as a conditional variational autoencoder. Given a training example $(X, Y)$, we sample $\boldsymbol{z}$ from the approximate posterior $Q(\boldsymbol{z} \mid X, Y)=\mathcal{N}\left(\boldsymbol{\mu}<em X_="X," Y="Y">{X, Y}, \boldsymbol{\sigma}</em>}\right)$. To compute $Q(\boldsymbol{z} \mid X, Y)$, we first encode $X$ and $Y$ into their representations $\boldsymbol{c<em Y="Y">{X}$ and $\boldsymbol{c}</em>$ that summarizes the structural changes from molecule $X$ to $Y$ at both atom and motif level:}$ and then compute difference vector $\boldsymbol{\delta}_{X, Y</p>
<p>$$
\boldsymbol{\delta}<em Y="Y">{X, Y}^{s}=\sum \boldsymbol{c}</em>}^{s}-\sum \boldsymbol{c<em X_="X," Y="Y">{X}^{s} \quad \boldsymbol{\delta}</em>}^{g}=\sum \boldsymbol{c<em X="X">{Y}^{g}-\sum \boldsymbol{c}</em>
$$}^{g</p>
<p>Finally, we compute $\left[\boldsymbol{\mu}<em X_="X," Y="Y">{X, Y}, \boldsymbol{\sigma}</em>}\right]=\operatorname{MLP}\left(\boldsymbol{\delta<em X_="X," Y="Y">{X, Y}^{g}, \boldsymbol{\delta}</em>$ using reparameterization trick. The latent}^{G}\right)$ and sample $\boldsymbol{z</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>code $\boldsymbol{z}$ is passed to the decoder along with the input representation $\boldsymbol{c}_{X}$ to reconstruct output $Y$. The training objective is to minimize negative ELBO similar to Eq.(12).</p>
<p>Attention For graph translation, the input molecule $X$ is embedded by our hierarchical encoder into a set of vectors $\boldsymbol{c}<em X="X">{X}=\boldsymbol{c}</em>}^{s} \cup \boldsymbol{c<em X="X">{X}^{g} \cup \boldsymbol{c}</em>$, representing the molecule at multiple resolutions. These vectors are fed into the decoder through attention mechanisms (Luong et al., 2015). Specifically, we modify the motif prediction (Eq. 8) into}^{g</p>
<p>$$
\begin{aligned}
\boldsymbol{p}<em 1="1">{\mathcal{S}</em>}} &amp; =\operatorname{softmax}\left(\operatorname{MLP}\left(\boldsymbol{h<em k="k">{\mathcal{S}</em>}}, \boldsymbol{\alpha<em k="k">{k}^{s}, \boldsymbol{z}\right)\right) \
\boldsymbol{\alpha}</em>}^{s} &amp; =\operatorname{attention}\left(\boldsymbol{h<em k="k">{\mathcal{S}</em>\right)
\end{aligned}
$$}}, \boldsymbol{c}_{X}^{s</p>
<p>where $\operatorname{attention}\left(\boldsymbol{h}<em X="X">{*}, \boldsymbol{c}</em>}^{s}\right)$ is a bilinear attention over vectors $\boldsymbol{c<em _mathcal_S="\mathcal{S">{X}^{s}$ with query vector $\boldsymbol{h}</em><em X="X">{k}}$. The attachment prediction (Eq. 9) is modified similarly with its attention over $\boldsymbol{c}</em>$. The graph prediction (Eq. 10) is modified into}^{g</p>
<p>$$
\begin{aligned}
\boldsymbol{p}<em M="M">{M} &amp; =\operatorname{softmax}\left(\boldsymbol{h}</em>} \cdot \operatorname{attention}\left(\boldsymbol{h<em X="X">{M}, \boldsymbol{c}</em>\right)\right) \
\boldsymbol{h}}^{g<em j="j">{M} &amp; =\sum</em>} \operatorname{MLP}\left(\boldsymbol{h<em j="j">{u</em>}}, \boldsymbol{h<em j="j">{v</em>\right)
\end{aligned}
$$}}, \boldsymbol{z</p>
<h2>4. Experiments</h2>
<p>We evaluate our method on two application tasks. The first task is polymer generative modeling. This experiment validates our argument in section 2 that our model is advantageous when the molecules have large sizes. The second task is graph-to-graph translation for small molecules. Here we show the proposed architecture also brings benefits to small molecules compared to previous state-of-the-art graph generation methods.</p>
<h3>4.1. Polymer Generative Modeling</h3>
<p>Dataset Our method is evaluated on the polymer dataset from St. John et al. (2019), which contains 86 K polymers in total (after removing duplicates). The dataset is divided into $76 \mathrm{~K}, 5 \mathrm{~K}$ and 5 K for training, validation and testing. Using our motif extraction, we collected 436 different motifs (examples shown in Figure 4). On average, each motif has 5.24 different attachment configurations. The distribution of motif size and their frequencies are reported in Figure 5.</p>
<p>Evaluation Metrics Our evaluation effort measures various aspects of molecule generation proposed in Kusner et al. (2017); Polykovskiy et al. (2018). Besides basic metrics like chemical validity and diversity, we compare distributional statistics between generated and real compounds. A good generative model should generate molecules which present similar aggregate statistics to real compounds. Our metrics include (with details shown in the appendix):</p>
<ul>
<li>Reconstruction accuracy: We measure how often the model can completely reconstruct a given molecule from its latent embedding $\boldsymbol{z}$. The reconstruction accuracy is computed over 5 K compounds in the test set.</li>
</ul>
<p>Table 1. Results on polymer generative modeling. The first row reports the oracle performance using real data as generated samples. The last row (small motif) is a variant of our model where we restrict the motif vocabulary to contain only single rings and bonds (similar to JT-VAE). "Recon." means reconstruction accuracy; "Div." means diversity; SNN means nearest neighbor similarity; "Frag / Scaf" means fragment and scaffold similarity. Except property statistics, all metrics are the higher the better.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">Reconstruction / Sample Quality ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Property Statistics ( $\downarrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Structural Statistics ( $\uparrow$ )</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: center;">Recon.</td>
<td style="text-align: center;">Valid</td>
<td style="text-align: center;">Unique</td>
<td style="text-align: center;">Div.</td>
<td style="text-align: center;">$\log \mathbf{P}$</td>
<td style="text-align: center;">SA</td>
<td style="text-align: center;">QED</td>
<td style="text-align: center;">MW</td>
<td style="text-align: center;">SNN</td>
<td style="text-align: center;">Frag.</td>
<td style="text-align: center;">Scaf.</td>
</tr>
<tr>
<td style="text-align: left;">Real data</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">$100 \%$</td>
<td style="text-align: center;">0.823</td>
<td style="text-align: center;">0.094</td>
<td style="text-align: center;">$6.7 \mathrm{e}-5$</td>
<td style="text-align: center;">$1.7 \mathrm{e}-5$</td>
<td style="text-align: center;">82.3</td>
<td style="text-align: center;">0.706</td>
<td style="text-align: center;">0.995</td>
<td style="text-align: center;">0.462</td>
</tr>
<tr>
<td style="text-align: left;">SMILES</td>
<td style="text-align: center;">$21.5 \%$</td>
<td style="text-align: center;">$93.1 \%$</td>
<td style="text-align: center;">$\mathbf{9 7 . 3 \%}$</td>
<td style="text-align: center;">0.821</td>
<td style="text-align: center;">1.471</td>
<td style="text-align: center;">0.011</td>
<td style="text-align: center;">$\mathbf{5 . 4 e - 4}$</td>
<td style="text-align: center;">4963</td>
<td style="text-align: center;">0.704</td>
<td style="text-align: center;">0.981</td>
<td style="text-align: center;">0.385</td>
</tr>
<tr>
<td style="text-align: left;">CG-VAE</td>
<td style="text-align: center;">$42.4 \%$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$96.2 \%$</td>
<td style="text-align: center;">$\mathbf{0 . 8 7 9}$</td>
<td style="text-align: center;">3.958</td>
<td style="text-align: center;">2.600</td>
<td style="text-align: center;">0.0030</td>
<td style="text-align: center;">3944</td>
<td style="text-align: center;">0.204</td>
<td style="text-align: center;">0.372</td>
<td style="text-align: center;">0.001</td>
</tr>
<tr>
<td style="text-align: left;">JT-VAE</td>
<td style="text-align: center;">$58.5 \%$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$94.1 \%$</td>
<td style="text-align: center;">0.864</td>
<td style="text-align: center;">2.645</td>
<td style="text-align: center;">0.157</td>
<td style="text-align: center;">0.0075</td>
<td style="text-align: center;">10867</td>
<td style="text-align: center;">0.522</td>
<td style="text-align: center;">0.925</td>
<td style="text-align: center;">0.297</td>
</tr>
<tr>
<td style="text-align: left;">HierVAE</td>
<td style="text-align: center;">$\mathbf{7 9 . 9 \%}$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$97.0 \%$</td>
<td style="text-align: center;">0.817</td>
<td style="text-align: center;">$\mathbf{0 . 5 2 5}$</td>
<td style="text-align: center;">$\mathbf{0 . 0 0 7}$</td>
<td style="text-align: center;">$5.7 \mathrm{e}-4$</td>
<td style="text-align: center;">$\mathbf{1 9 2 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 7 0 8}$</td>
<td style="text-align: center;">$\mathbf{0 . 9 8 4}$</td>
<td style="text-align: center;">$\mathbf{0 . 3 9 0}$</td>
</tr>
<tr>
<td style="text-align: left;">- Small motif</td>
<td style="text-align: center;">$71.0 \%$</td>
<td style="text-align: center;">$\mathbf{1 0 0 \%}$</td>
<td style="text-align: center;">$97.2 \%$</td>
<td style="text-align: center;">0.835</td>
<td style="text-align: center;">0.872</td>
<td style="text-align: center;">0.042</td>
<td style="text-align: center;">0.0019</td>
<td style="text-align: center;">5320</td>
<td style="text-align: center;">0.575</td>
<td style="text-align: center;">0.949</td>
<td style="text-align: center;">0.191</td>
</tr>
</tbody>
</table>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Examples of motif structures utilized by our model. These motifs consist of multiple rings and bonds, which are substantially more complex than previous methods (Jin et al., 2018).</p>
<ul>
<li>Validity: Percentage of chemically valid compounds.</li>
<li>Uniqueness: Percentage of unique compounds.</li>
<li>Diversity: We compute the pairwise molecular distance among generated compounds. The molecular distance $\operatorname{dist}(X, Y)$ is defined as the Tanimoto distance over Morgan fingerprints (Rogers \&amp; Hahn, 2010) of two molecules.</li>
<li>Property statistics: We compare property statistics between generated molecules and real data. Our properties include partition coefficient (logP), synthetic accessibility (SA), drug-likeness (QED) and molecular weight (MW). To quantitatively evaluate the distance between two distributions, we compute Frechet distance between property distributions of molecules in the generated and test sets (Polykovskiy et al., 2018).</li>
<li>Structural statistics: We also compute structural statistics between generated molecules and real data. Nearest neighbor similarity (SNN) is the average similarity of generated molecules to the nearest molecule from the test set. Fragment similarity (Frag) and scaffold similarity (Scaf) are cosine distances between vectors of fragment or scaffold frequencies of the generated and the test set.</li>
</ul>
<p>Baselines We compare our method against three state-
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5. Left: Histogram of motif frequencies with respect to their sizes (i.e., number of atoms). Right: Training speed comparison between our method and baselines (on the same hardware).
of-the-art variational autoencoders for molecular graphs. SMILES VAE (GÃ³mez-Bombarelli et al., 2018) is a sequence to sequence VAE that generates molecules based on their SMILES strings (Weininger, 1988). CG-VAE (Liu et al., 2018) is a graph-based VAE generating molecules atom by atom. JT-VAE (Jin et al., 2018) is also a graphbased VAE generating molecules based on simple substructures restricted to rings and bonds. Finally, we report the oracle performance of distributional statistics by using real molecules in the training set as our generated samples.</p>
<h3>4.1.1. RESULTS</h3>
<p>The performance of different methods are summarized in Table 1, Our method (HierVAE) significantly outperforms all previous methods in terms of reconstruction accuracy ( $79.9 \%$ vs $58.5 \%$ ). This validates the advantage of utilizing large structural motifs, which reduces the number of generation steps. In terms of distributional statistics, our method achieves state-of-the-art results on $\log \mathrm{P}(0.525$ vs 1.471$)$, molecular weight Frechet distance (1928 vs 4863) and all the structural similarity metrics. Since our model requires fewer generation steps, our training speed is much faster than other graph-based methods (see Figure 5).</p>
<p>Ablation Study To validate the importance of utilizing large structural motifs, we further experiment a variant of our model (small motif), which keeps the same architecture but replaces the large structural motifs with basic substructures such as rings and bonds (with less than ten atoms). As shown in Table 1, its performance is significantly worse than our full model even though it builds on the same hierarchical architecture.</p>
<h3>4.2. Graph-to-Graph Translation</h3>
<p>We follow the experimental design by Jin et al. (2019) and evaluate our model on their graph-to-graph translation tasks. Following their setup, we require the molecular similarity between $X$ and output $Y$ to be above certain threshold $\operatorname{sim}(X, Y) \geq \delta$ at test time. This is to prevent the model from ignoring input $X$ and translating it into arbitrary compound. Here the molecular similarity is defined as $\operatorname{sim}(X, Y)=1-\operatorname{dist}(X, Y)$.</p>
<p>Dataset The dataset consists of four property optimization tasks. In each task, we train and evaluate our model on their provided training and test sets.</p>
<ul>
<li>LogP: The penalized logP score (Kusner et al., 2017) measures the solubility and synthetic accessibility of a compound. In this task, the model needs to translate input $X$ into output $Y$ such that $\log \mathrm{P}(Y)&gt;\log \mathrm{P}(X)$. We experiment with two similarity thresholds $\delta={0.4,0.6}$.</li>
<li>QED: The QED score (Bickerton et al., 2012) quantifies a compound's drug-likeness. In this task, the model is required to translate molecules with QED scores from the lower range $[0.7,0.8]$ into the higher range $[0.9,1.0]$. The similarity constraint is $\operatorname{sim}(X, Y) \geq 0.4$.</li>
<li>DRD2: This task involves the optimization of a compound's biological activity against dopamine type 2 receptor (DRD2). The model needs to translate inactive compounds ( $p&lt;0.05$ ) into active compounds ( $p \geq 0.5$ ), where the bioactivity is assessed by a property prediction model from Olivecrona et al. (2017). The similarity constraint is $\operatorname{sim}(X, Y) \geq 0.4$.</li>
</ul>
<p>Evaluation Metrics Our evaluation metrics include translation accuracy and diversity. Each test molecule $X_{i}$ is translated $K=20$ times with different latent codes sampled from the prior distribution. On the logP optimization, we select compound $Y_{i}$ as the final translation of $X_{i}$ that gives the highest property improvement and satisfies $\operatorname{sim}\left(X_{i}, Y_{i}\right) \geq \delta$. We then report the average property improvement $\frac{1}{\mathcal{D}} \sum_{i} \log \mathrm{P}\left(Y_{i}\right)-\log \mathrm{P}\left(X_{i}\right)$ over test set $\mathcal{D}$. For other tasks, we report the translation success rate. A compound is successfully translated if one of its $K$ translation candidates satisfies all the similarity and property constraints of the task. To measure the diversity, for each molecule we compute the average pairwise Tanimoto distance between
all its successfully translated compounds.
Baselines We compare our method against the baselines including GCPN (You et al., 2018a), MMPA (Dalke et al., 2018) and translation based methods Seq2Seq and JTNN (Jin et al., 2019). Seq2Seq is a sequence-to-sequence model that generates molecules by their SMILES strings. JTNN is a graph-to-graph architecture that generates molecules structure by structure, but its decoder is not fully autoregressive.</p>
<p>To make a direct comparison possible between our method and atom-based generation, we further developed an atombased translation model (AtomG2G) as baseline. It makes three predictions in each generation step. First, it predicts whether the decoding process has completed (no more new atoms). If not, it creates a new atom $a_{t}$ and predicts its atom type. Lastly, it predicts the bond type between $a_{t}$ and other atoms autoregressively to fully capture edge dependencies (You et al., 2018b). The encoder of AtomG2G encodes only the atom-layer graph and the decoder attention only sees the atom vectors $\boldsymbol{e}_{X}^{\mathcal{G}}$. All translation models are trained under the same variational objective. Details of baseline architectures are in the appendix.</p>
<h3>4.2.1. RESULTS</h3>
<p>As shown in Table 2, our model (HierG2G) achieves the new state-of-the-art on the four translation tasks. In particular, our model significantly outperforms JTNN in both translation accuracy (e.g., $76.9 \%$ versus $59.9 \%$ on the QED task) and output diversity (e.g., 0.564 versus 0.480 on the $\log \mathrm{P}$ task). While both methods generate molecules by structures, our decoder is autoregressive which can learn more expressive mappings. In addition, our model runs 6.3 times faster than JTNN during decoding. Our model also outperforms AtomG2G on three datasets, with over 10\% improvement on the DRD2 task. This shows the advantage of our hierarchical model.</p>
<p>Ablation Study To understand the importance of different architecture choices, we report ablation studies over the QED and DRD2 tasks in Table 3. We first replace our hierarchical decoder with the atom-based decoder of AtomG2G to see how much the motif-based decoding benefits us. We keep the same hierarchical encoder but modified the input of the decoder attention to include both atom and motif vectors. Using this setup, the model performance decreases by $0.8 \%$ and $10.9 \%$ on the two tasks. We suspect the DRD2 task benefits more from motif-based decoding because biological target binding often depends on the presence of specific functional groups.</p>
<p>Our second experiment reduces the number of hierarchies in our encoder and decoder MPN, while keeping the same hierarchical decoding process. When the top motif layer is removed, the translation accuracy drops slightly by $0.8 \%$</p>
<p>Table 2: Results on graph translation tasks from <em>Jin et al. (2019)</em>. We report average improvement for continuous properties (logP), and success rate for binary properties (e.g., DRD2).</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>$\log$ (sim $\geq 0.6)$</th>
<th></th>
<th>$\log$ (sim $\geq 0.4)$</th>
<th></th>
<th>Drug likeness</th>
<th></th>
<th>DRD2</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Improvement</td>
<td>Diversity</td>
<td>Improvement</td>
<td>Diversity</td>
<td>Success</td>
<td>Diversity</td>
<td>Success</td>
<td>Diversity</td>
</tr>
<tr>
<td>JT-VAE</td>
<td>$0.28 \pm 0.79$</td>
<td>-</td>
<td>$1.03 \pm 1.39$</td>
<td>-</td>
<td>8.8%</td>
<td>-</td>
<td>3.4%</td>
<td>-</td>
</tr>
<tr>
<td>CG-VAE</td>
<td>$0.25 \pm 0.74$</td>
<td>-</td>
<td>$0.61 \pm 1.09$</td>
<td>-</td>
<td>4.8%</td>
<td>-</td>
<td>2.3%</td>
<td>-</td>
</tr>
<tr>
<td>GCPN</td>
<td>$0.79 \pm 0.63$</td>
<td>-</td>
<td>$2.49 \pm 1.30$</td>
<td>-</td>
<td>9.4%</td>
<td>0.216</td>
<td>4.4%</td>
<td>0.152</td>
</tr>
<tr>
<td>MMPA</td>
<td>$1.65 \pm 1.44$</td>
<td>0.329</td>
<td>$3.29 \pm 1.12$</td>
<td>0.496</td>
<td>32.9%</td>
<td>0.236</td>
<td>46.4%</td>
<td>0.275</td>
</tr>
<tr>
<td>Seq2Seq</td>
<td>$2.33 \pm 1.17$</td>
<td>0.331</td>
<td>$3.37 \pm 1.75$</td>
<td>0.471</td>
<td>58.5%</td>
<td>0.331</td>
<td>75.9%</td>
<td>0.176</td>
</tr>
<tr>
<td>JTNN</td>
<td>$2.33 \pm 1.24$</td>
<td>0.333</td>
<td>$3.55 \pm 1.67$</td>
<td>0.480</td>
<td>59.9%</td>
<td>0.373</td>
<td>77.8%</td>
<td>0.156</td>
</tr>
<tr>
<td>AtomG2G</td>
<td>$2.41 \pm 1.19$</td>
<td>0.379</td>
<td>$\mathbf{3 . 9 8} \pm \mathbf{1 . 5 4}$</td>
<td>0.563</td>
<td>73.6%</td>
<td>0.421</td>
<td>75.8%</td>
<td>0.128</td>
</tr>
<tr>
<td>HierG2G</td>
<td>$\mathbf{2 . 4 9} \pm \mathbf{1 . 0 9}$</td>
<td>$\mathbf{0 . 3 8 1}$</td>
<td>$\mathbf{3 . 9 8} \pm \mathbf{1 . 4 6}$</td>
<td>$\mathbf{0 . 5 6 4}$</td>
<td>$\mathbf{7 6 . 9 \%}$</td>
<td>$\mathbf{0 . 4 7 7}$</td>
<td>$\mathbf{8 5 . 9 \%}$</td>
<td>0.192</td>
</tr>
</tbody>
</table>
<p>Table 3: Ablation study: the importance of hierarchical graph encoding, LSTM MPN architecture and structure-based decoding.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: center;">QED</th>
<th style="text-align: center;">DRD2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">HierG2G</td>
<td style="text-align: center;">$\mathbf{7 6 . 9 \%}$</td>
<td style="text-align: center;">$\mathbf{8 5 . 9 \%}$</td>
</tr>
<tr>
<td style="text-align: left;">$\cdot$ atom-based decoder</td>
<td style="text-align: center;">76.1\%</td>
<td style="text-align: center;">75.0\%</td>
</tr>
<tr>
<td style="text-align: left;">$\cdot$ two-layer encoder</td>
<td style="text-align: center;">75.8\%</td>
<td style="text-align: center;">83.5\%</td>
</tr>
<tr>
<td style="text-align: left;">$\cdot$ one-layer encoder</td>
<td style="text-align: center;">67.8\%</td>
<td style="text-align: center;">74.1\%</td>
</tr>
</tbody>
</table>
<p>and 2.4\%. When we further remove the attachment layer (one-layer encoder), the performance degrades significantly on both datasets. This is because all the motif information is lost and the model needs to infer what motifs are and how motif layers are constructed for each molecule. This shows the importance of the hierarchical representation.</p>
<h2>5 Related Work</h2>
<p>Graph Generation Previous work have adopted various approaches for generating molecular graphs. <em>GÃ³mezBombarelli et al. (2018); Segler et al. (2017); Kusner et al. (2017); Dai et al. (2018); Guimaraes et al. (2017); Olivecrona et al. (2017); Popova et al. (2018); Kang &amp; Cho (2018)</em> generated molecules based on their SMILES strings ( [Weininger, 1988]). <em>Simonovsky &amp; Komodakis (2018); De Cao &amp; Kipf (2018); Ma et al. (2018)</em> developed generative models which output the adjacency matrices and node labels of the graphs at once. <em>You et al. (2018b); Li et al. (2018); Samanta et al. (2018); Liu et al. (2018); Zhou et al. (2018)</em> proposed generative models which decode molecules sequentially node by node. <em>Seff et al. (2019)</em> developed a edit-based model which generates molecules based on insertions and deletions.</p>
<p>Our model is closely related to <em>Liao et al. (2019)</em> which generate graphs one block of nodes and edges at a time. While their encoder operates on original graphs, our encoder operates on multiple hierarchies and learns multi-resolution
representations of input graphs. Our work is also closely related to <em>Jin et al. (2018, 2019)</em> that generate molecules based on substructures. Their decoder first generates a junction tree with substructures as nodes, and then predicts how the substructures should be attached to each other. Their substructure attachment process involves combinatorial enumeration and therefore their model cannot scale to substructures more complex than simple rings and bonds. In contrast, our model allows the motif to have flexible structures.</p>
<p>Graph Encoders Graph neural networks have been extensively studied for graph encoding ( [Scarselli et al., 2009; Bruna et al., 2013; Li et al., 2015; Niepert et al., 2016; Kipf &amp; Welling, 2017; Hamilton et al., 2017; Lei et al., 2017; Velickovic et al., 2017; Xu et al., 2018]). Our method is related to graph encoders for molecules ( [Duvenaud et al., 2015; Kearnes et al., 2016; Dai et al., 2016; Gilmer et al., 2017; SchÃ¼tt et al., 2017]). Different to these approaches, our method represents molecules as hierarchical graphs spanning from atom-level to motif-level graphs.</p>
<p>Our work is most closely related to ( [Defferrard et al., 2016; Ying et al., 2018; Gao &amp; Ji, 2019]) that learn to represent graphs in a hierarchical manner. In particular, <em>Defferrard et al. (2016)</em> utilized graph coarsening algorithms to construct multiple layers of graph hierarchy and <em>Ying et al. (2018); Gao &amp; Ji (2019)</em> proposed to learn the graph hierarchy jointly with the encoding process. Despite some differences, all of these methods learns the hierarchy for regression or classification tasks. In contrast, our hierarchy is constructed for efficient graph generation.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we developed a hierarchical encoder-decoder architecture generating molecular graphs using structural motifs as building blocks. The experimental results show our model outperforms prior atom and substructure based methods in both small molecule and polymer domains.</p>
<h2>References</h2>
<p>Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.</p>
<p>Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S., and Hopkins, A. L. Quantifying the chemical beauty of drugs. Nature chemistry, 4(2):90, 2012.</p>
<p>Bruna, J., Zaremba, W., Szlam, A., and LeCun, Y. Spectral networks and locally connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.</p>
<p>Dai, H., Dai, B., and Song, L. Discriminative embeddings of latent variable models for structured data. In International Conference on Machine Learning, pp. 2702-2711, 2016.</p>
<p>Dai, H., Tian, Y., Dai, B., Skiena, S., and Song, L. Syntaxdirected variational autoencoder for structured data. arXiv preprint arXiv:1802.08786, 2018.</p>
<p>Dalke, A., Hert, J., and Kramer, C. mmpdb: An open-source matched molecular pair platform for large multiproperty data sets. Journal of chemical information and modeling, 2018.</p>
<p>De Cao, N. and Kipf, T. Molgan: An implicit generative model for small molecular graphs. arXiv preprint arXiv:1805.11973, 2018.</p>
<p>Defferrard, M., Bresson, X., and Vandergheynst, P. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems, pp. 3844-3852, 2016.</p>
<p>Duvenaud, D. K., Maclaurin, D., Iparraguirre, J., Bombarell, R., Hirzel, T., Aspuru-Guzik, A., and Adams, R. P. Convolutional networks on graphs for learning molecular fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.</p>
<p>Gao, H. and Ji, S. Graph u-net. International Conference on Machine Learning, 2019.</p>
<p>Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. Neural message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.</p>
<p>GÃ³mez-Bombarelli, R., Wei, J. N., Duvenaud, D., HernÃ¡ndez-Lobato, J. M., SÃ¡nchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A. Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Science, 2018. doi: 10.1021/ acscentsci.7b00572.</p>
<p>Guimaraes, G. L., Sanchez-Lengeling, B., Farias, P. L. C., and Aspuru-Guzik, A. Objective-reinforced generative adversarial networks (organ) for sequence generation models. arXiv preprint arXiv:1705.10843, 2017.</p>
<p>Hamilton, W. L., Ying, R., and Leskovec, J. Inductive representation learning on large graphs. arXiv preprint arXiv:1706.02216, 2017.</p>
<p>Jin, W., Barzilay, R., and Jaakkola, T. Junction tree variational autoencoder for molecular graph generation. International Conference on Machine Learning, 2018.</p>
<p>Jin, W., Yang, K., Barzilay, R., and Jaakkola, T. Learning multimodal graph-to-graph translation for molecular optimization. International Conference on Learning Representations, 2019.</p>
<p>Kang, S. and Cho, K. Conditional molecular design with deep generative models. Journal of chemical information and modeling, 59(1):43-52, 2018.</p>
<p>Kearnes, S., McCloskey, K., Berndl, M., Pande, V., and Riley, P. Molecular graph convolutions: moving beyond fingerprints. Journal of computer-aided molecular design, 30(8):595-608, 2016.</p>
<p>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.</p>
<p>Kipf, T. N. and Welling, M. Semi-supervised classification with graph convolutional networks. International Conference on Learning Representations, 2017.</p>
<p>Kusner, M. J., Paige, B., and HernÃ¡ndez-Lobato, J. M. Grammar variational autoencoder. arXiv preprint arXiv:1703.01925, 2017.</p>
<p>Lei, T., Jin, W., Barzilay, R., and Jaakkola, T. Deriving neural architectures from sequence and graph kernels. International Conference on Machine Learning, 2017.</p>
<p>Li, Y., Tarlow, D., Brockschmidt, M., and Zemel, R. Gated graph sequence neural networks. arXiv preprint arXiv:1511.05493, 2015.</p>
<p>Li, Y., Vinyals, O., Dyer, C., Pascanu, R., and Battaglia, P. Learning deep generative models of graphs. arXiv preprint arXiv:1803.03324, 2018.</p>
<p>Liao, R., Li, Y., Song, Y., Wang, S., Hamilton, W., Duvenaud, D. K., Urtasun, R., and Zemel, R. Efficient graph generation with graph recurrent attention networks. In Advances in Neural Information Processing Systems, pp. 4257-4267, 2019.</p>
<p>Liu, Q., Allamanis, M., Brockschmidt, M., and Gaunt, A. L. Constrained graph variational autoencoders for molecule design. Neural Information Processing Systems, 2018.</p>
<p>Luong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.</p>
<p>Ma, T., Chen, J., and Xiao, C. Constrained generation of semantically valid graphs via regularizing variational autoencoders. In Advances in Neural Information Processing Systems, pp. 7113-7124, 2018.</p>
<p>Niepert, M., Ahmed, M., and Kutzkov, K. Learning convolutional neural networks for graphs. In International Conference on Machine Learning, pp. 2014-2023, 2016.</p>
<p>Olivecrona, M., Blaschke, T., Engkvist, O., and Chen, H. Molecular de-novo design through deep reinforcement learning. Journal of cheminformatics, 9(1):48, 2017.</p>
<p>Polykovskiy, D., Zhebrak, A., Sanchez-Lengeling, B., Golovanov, S., Tatanov, O., Belyaev, S., Kurbanov, R., Artamonov, A., Aladinskiy, V., Veselov, M., Kadurin, A., Nikolenko, S., Aspuru-Guzik, A., and Zhavoronkov, A. Molecular Sets (MOSES): A Benchmarking Platform for Molecular Generation Models. arXiv preprint arXiv:1811.12823, 2018.</p>
<p>Popova, M., Isayev, O., and Tropsha, A. Deep reinforcement learning for de novo drug design. Science advances, 4(7): eaap7885, 2018.</p>
<p>Rogers, D. and Hahn, M. Extended-connectivity fingerprints. Journal of chemical information and modeling, 50 (5):742-754, 2010.</p>
<p>Samanta, B., De, A., Jana, G., Chattaraj, P. K., Ganguly, N., and Gomez-Rodriguez, M. Nevae: A deep generative model for molecular graphs. arXiv preprint arXiv:1802.05283, 2018.</p>
<p>Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2009.</p>
<p>SchÃ¼tt, K., Kindermans, P.-J., Felix, H. E. S., Chmiela, S., Tkatchenko, A., and MÃ¼ller, K.-R. Schnet: A continuousfilter convolutional neural network for modeling quantum interactions. In Advances in Neural Information Processing Systems, pp. 992-1002, 2017.</p>
<p>Seff, A., Zhou, W., Damani, F., Doyle, A., and Adams, R. P. Discrete object generation with reversible inductive construction. In Advances in Neural Information Processing Systems, pp. 10353-10363, 2019.</p>
<p>Segler, M. H., Kogej, T., Tyrchan, C., and Waller, M. P. Generating focussed molecule libraries for drug discovery with recurrent neural networks. arXiv preprint arXiv:1701.01329, 2017.</p>
<p>Simonovsky, M. and Komodakis, N. Graphvae: Towards generation of small graphs using variational autoencoders. arXiv preprint arXiv:1802.03480, 2018.</p>
<p>St. John, P. C., Phillips, C., Kemper, T. W., Wilson, A. N., Guan, Y., Crowley, M. F., Nimlos, M. R., and Larsen, R. E. Message-passing neural networks for high-throughput polymer screening. The Journal of chemical physics, 150 (23):234111, 2019.</p>
<p>Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.</p>
<p>Weininger, D. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. Journal of chemical information and computer sciences, 28(1):31-36, 1988.</p>
<p>Xu, K., Hu, W., Leskovec, J., and Jegelka, S. How powerful are graph neural networks? arXiv preprint arXiv:1810.00826, 2018.</p>
<p>Ying, Z., You, J., Morris, C., Ren, X., Hamilton, W., and Leskovec, J. Hierarchical graph representation learning with differentiable pooling. In Advances in Neural Information Processing Systems, pp. 4800-4810, 2018.</p>
<p>You, J., Liu, B., Ying, R., Pande, V., and Leskovec, J. Graph convolutional policy network for goal-directed molecular graph generation. arXiv preprint arXiv:1806.02473, 2018a.</p>
<p>You, J., Ying, R., Ren, X., Hamilton, W. L., and Leskovec, J. Graphrnn: A deep generative model for graphs. arXiv preprint arXiv:1802.08773, 2018b.</p>
<p>Zhou, Z., Kearnes, S., Li, L., Zare, R. N., and Riley, P. Optimization of molecules via deep reinforcement learning. arXiv preprint arXiv:1810.08678, 2018.</p>
<p>Zhu, J.-Y., Zhang, R., Pathak, D., Darrell, T., Efros, A. A., Wang, O., and Shechtman, E. Toward multimodal imageto-image translation. In Advances in Neural Information Processing Systems, pp. 465-476, 2017.</p>
<h1>A. Motif Construction</h1>
<p>To extract motifs, we decompose a molecule $\mathcal{G}=(\mathcal{V}, \mathcal{E})$ into disconnected fragments by breaking all the bridge bonds that will not violate chemical validity. Our motif extraction consists of three steps (see Figure 6):</p>
<ol>
<li>Find all the bridge bonds $(u, v) \in \mathcal{E}$, where both $u$ and $v$ have degree $\Delta_{u}, \Delta_{v} \geq 2$ and either $u$ or $v$ is part of a ring.</li>
<li>Detach all the bridge bonds from its neighbors. Now the graph $\mathcal{G}$ becomes a set of disconnected subgraphs $\mathcal{G}<em N="N">{1}, \cdots, \mathcal{G}</em>$.</li>
<li>Select $\mathcal{G}<em i="i">{i}$ as motif in $\mathcal{G}$ if its occurrence in the training set is more than $\Delta=100$. If $\mathcal{G}</em>$.
}$ is not selected as motif, further decompose it into rings and bonds and put them into the motif vocabulary $V_{\mathcal{S}<img alt="img-5.jpeg" src="img-5.jpeg" /></li>
</ol>
<p>Figure 6. Illustration of motif extraction procedure.</p>
<h2>B. Network Architecture</h2>
<p>MPN Architecture Our message passing network $\operatorname{MPN}<em u="u">{\psi}\left(\mathcal{H},\left{\boldsymbol{x}</em>}\right},\left{\boldsymbol{x<em v="v">{u v}\right}\right)$ is a slight modification from the MPN architecture used in Dai et al. (2016); Jin et al. (2019). Let $N(v)$ be the neighbors of node $v, \boldsymbol{x}</em>}$ the node feature of $v$ and $\boldsymbol{x<em u="u" v="v">{u v}$ be the feature of edge $(u, v)$. During encoding, each edge $(u, v)$ is associated with two messages $\boldsymbol{\nu}</em>}$ and $\boldsymbol{\nu<em _psi="\psi">{v u}$, representing the message from $u$ to $v$ and vice versa. The messages are updated by an LSTM cell with parameters $\psi=\left{\boldsymbol{W}</em>}^{z}, \boldsymbol{W<em _psi="\psi">{\psi}^{o}, \boldsymbol{W}</em>\right}$ defined as follows:}^{r}, \boldsymbol{W}_{\psi</p>
<h2>Algorithm 1 LSTM Message Passing</h2>
<div class="codehilite"><pre><span></span><code>function \(\operatorname{LSTM}_{\psi}\left(\boldsymbol{x}_{u}, \boldsymbol{x}_{u v},\left\{\boldsymbol{\nu}_{w u}^{(t)}, \boldsymbol{c}_{w u}^{(t)}\right\}_{w \in N(u) \backslash v}\right)\)
    \(i_{u v}=\sigma\left(\boldsymbol{W}_{\psi}^{z}\left[\boldsymbol{x}_{u}, \boldsymbol{x}_{u v}, \sum_{w} \boldsymbol{\nu}_{w u}^{(t)}\right]+\boldsymbol{b}^{z}\right) \quad \boldsymbol{o}_{u v}=\sigma\left(\boldsymbol{W}_{\psi}^{o}\left[\boldsymbol{x}_{u}, \boldsymbol{x}_{u v}, \sum_{w} \boldsymbol{\nu}_{w u}^{(t)}\right]+\boldsymbol{b}^{o}\right)\)
    \(\boldsymbol{f}_{w u}=\sigma\left(\boldsymbol{W}_{\psi}^{r}\left[\boldsymbol{x}_{u}, \boldsymbol{x}_{u v}, \boldsymbol{\nu}_{w u}^{(t)}\right]+\boldsymbol{b}^{r}\right) \quad \quad \quad \quad \boldsymbol{c}_{u v}^{(t+1)}=\tanh \left(\boldsymbol{W}_{\psi}\left[\boldsymbol{x}_{u}, \boldsymbol{x}_{u v}, \sum_{w} \boldsymbol{\nu}_{w u}^{(t)}\right]+\boldsymbol{b}\right)\)
    \(\boldsymbol{c}_{u v}^{(t+1)}=\boldsymbol{i}_{u v} \odot \tilde{\boldsymbol{c}}_{u v}^{(t+1)}+\sum_{w} \boldsymbol{f}_{w u} \odot \boldsymbol{c}_{w u}^{(t)} \quad \boldsymbol{\nu}_{u v}^{(t+1)}=\boldsymbol{o}_{u v} \odot \tanh \left(\boldsymbol{c}_{u v}^{(t+1)}\right)\)
    Return \(\boldsymbol{\nu}_{u v}^{(t+1)}, \boldsymbol{c}_{u v}^{(t+1)}\)
end function
</code></pre></div>

<p>function $\operatorname{MPN}<em v="v">{\psi}\left(\mathcal{H},\left{\boldsymbol{x}</em>}\right},\left{\boldsymbol{x<em u="u" v="v">{u v}\right}\right)$
Initialize messages: $\boldsymbol{\nu}</em>}^{0}=\mathbf{0}, \boldsymbol{c<em u="u" v="v">{u v}^{0}=\mathbf{0}$
for $t=0$ to $T-1$ do
Compute messages $\boldsymbol{\nu}</em>}^{(t+1)}, \boldsymbol{c<em _psi="\psi">{u v}^{(t+1)}=\operatorname{LSTM}</em>}\left(\boldsymbol{x<em u="u" v="v">{u}, \boldsymbol{x}</em>},\left{\boldsymbol{\nu<em u="u" w="w">{w u}^{(t)}, \boldsymbol{c}</em>\right}}^{(t)<em v="v">{w \in N(u) \backslash v}\right)$ for all edges $(u, v) \in \mathcal{H}$.
end for
Return node representations $\boldsymbol{h}</em>}=\operatorname{MLP}\left(\boldsymbol{x<em N_v_="N(v)" _in="\in" u="u">{v}, \sum</em>\right)$
end function} \boldsymbol{\nu}_{u v}^{(T)</p>
<p>AtomG2G Architecture AtomG2G is an atom-based translation method that is directly comparable to HierG2G. Here molecules are represented solely as molecular graphs rather than a hierarchical graph with motifs. The encoder of AtomG2G uses the same LSTM MPN to encode molecular graph. This gives us a set of atom vectors $\boldsymbol{c}_{X}^{G}$ representing molecule $X$ only at the atom level. The decoder of AtomG2G is illustrated in Figure 7. Following You et al. (2018b); Liu et al. (2018), the model generates molecule $\mathcal{G}$ atom by atom following their breadth-first order. During generation, it maintains a FIFO queue</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7. Illustration of AtomG2G decoding process. Atoms marked with red circles are frontier nodes in the queue $\mathcal{Q}$. In each step, the model picks the first node $v_{t}$ from $\mathcal{Q}$ and predict whether there will be new atoms attached to $v_{t}$. If so, it predicts the atom type of new node $u_{t}$ (atom prediction). Then the model predicts the bond type between $u_{t}$ and other nodes in $\mathcal{Q}$ sequentially for $|\mathcal{Q}|$ steps (bond prediction, $|\mathcal{Q}|=2$ ). Finally, it adds the new atom to the queue $\mathcal{Q}$.
$\mathcal{Q}$ that contains the frontier nodes in the graph (i.e., nodes who still have neighbors to be generated). Let $v_{t}$ be the first node in $\mathcal{Q}$ and $\mathcal{G}<em t="t">{t}$ be the current graph at step $t$. In each step, the model makes three predictions to expand the graph $\mathcal{G}</em>$ :</p>
<ol>
<li>It predicts whether there will be new atoms attached to $v_{t}$. If not, the model discards $v$ and move on to the next node in $\mathcal{Q}$. The generation stops if $\mathcal{Q}$ is empty.</li>
<li>Otherwise, it creates a new atom $u_{t}$ and predicts its atom type.</li>
<li>Lastly, it predicts the bond type between $u_{t}$ and other frontier nodes in $\mathcal{Q}$ autoregressively to fully capture edge dependencies. Since nodes are generated in breadth-first order, there will be no edges between $u_{t}$ and nodes outside of $\mathcal{Q}$.
To make those predictions, we use the same LSTM MPN to encode the current graph $\mathcal{G}<em v__t="v_{t">{t}$. Let $\boldsymbol{h}</em>}}$ be the atom representation of $v_{t}$. We represent $\mathcal{G<em _mathcal_G="\mathcal{G">{t}$ as the sum of all its atom vectors $\boldsymbol{h}</em><em _in="\in" _mathcal_G="\mathcal{G" v="v">{t}}=\sum</em><em v="v">{t}} \boldsymbol{h}</em>$ as:}$. In the first step, we model the probability of expanding a new node from $v_{t</li>
</ol>
<p>$$
\boldsymbol{p}<em v__t="v_{t">{t}=\sigma\left(\operatorname{MLP}\left(\boldsymbol{h}</em>}}, \boldsymbol{h<em t="t">{\mathcal{G}</em>}}, \boldsymbol{\alpha<em t="t">{t}^{d}\right)\right) \quad \boldsymbol{\alpha}</em>}^{d}=\operatorname{attention<em v__t="v_{t">{d}\left(\left[\boldsymbol{h}</em>}}, \boldsymbol{h<em t="t">{\mathcal{G}</em>\right)
$$}}\right], \boldsymbol{c}_{X}^{\mathcal{G}</p>
<p>In the second step, the atom type of the new node $u_{t}$ is predicted using another MLP:</p>
<p>$$
\boldsymbol{q}<em v__t="v_{t">{t}=\operatorname{softmax}\left(\operatorname{MLP}\left(\boldsymbol{h}</em>}}, \boldsymbol{h<em t="t">{\mathcal{G}</em>}}, \boldsymbol{\alpha<em t="t">{t}^{s}\right)\right) \quad \boldsymbol{\alpha}</em>}^{s}=\operatorname{attention<em v__t="v_{t">{s}\left(\left[\boldsymbol{h}</em>}}, \boldsymbol{h<em t="t">{\mathcal{G}</em>\right)
$$}}\right], \boldsymbol{c}_{X}^{\mathcal{G}</p>
<p>In the last step, we predict the bonds between $u_{t}$ and nodes in $\mathcal{Q}=a_{1}, \cdots, a_{n}$ sequentially starting with $a_{1}=v_{t}$. Specifically, for each atom pair $\left(u_{t}, a_{k}\right)$, we predict their bond type (single, double, triple or none) as the following:</p>
<p>$$
\boldsymbol{b}<em t="t">{u</em>}, a_{k}}=\operatorname{softmax}\left(\operatorname{MLP}\left(\boldsymbol{h<em t="t">{\mathcal{G}</em>}}, \boldsymbol{h<em t="t">{u</em>}}^{k}, \boldsymbol{h<em k="k">{a</em>}}, \boldsymbol{\alpha<em t="t">{t}^{b}\right)\right) \quad \boldsymbol{\alpha}</em>}^{b}=\operatorname{attention<em _mathcal_G="\mathcal{G">{b}\left(\left[\boldsymbol{h}</em><em u__t="u_{t">{t}}, \boldsymbol{h}</em>}}^{k}, \boldsymbol{h<em k="k">{a</em>\right)
$$}}\right], \boldsymbol{c}_{X}^{\mathcal{G}</p>
<p>where $\boldsymbol{h}<em k="k">{a</em>}}$ is the atom representation of node $a_{k}$ and $\boldsymbol{h<em t="t">{u</em>}}^{k}$ is the representation of node $u_{t}$ at the $k^{\text {th }}$ bond prediction. Let $N_{k}\left(u_{t}\right)$ be node $u_{t}$ 's current neighbor predicted in the first $k$ steps. $\boldsymbol{h<em t="t">{u</em>$ bond prediction:}}^{k}$ is computed as follows to reflect its local graph structure after $k^{\text {th }</p>
<p>$$
\boldsymbol{h}<em t="t">{u</em>}}^{k}=\operatorname{MLP}\left(\boldsymbol{x<em t="t">{u</em>}}, \sum_{w \in N_{k}\left(u_{t}\right)} \boldsymbol{\nu<em t="t">{w, u</em>}}\right) \quad \boldsymbol{\nu<em t="t">{w, u</em>}}=\operatorname{MLP}\left(\boldsymbol{h<em u__t="u_{t" w_="w,">{w}, \boldsymbol{x}</em>\right)
$$}</p>
<p>where $\boldsymbol{x}<em t="t">{u</em>}}$ is the atom feature of $u_{t}$ (i.e., predicted atom type) and $\boldsymbol{x<em t="t">{w, u</em>}}$ is the bond feature between $w$ and $u_{t}$ (i.e., predicted bond type). Intuitively, this can be viewed as running one-step message passing at each bond prediction step (i.e., passing the message $\boldsymbol{\nu<em t="t">{w, u</em>}}$ from $w$ to $u_{t}$ ). AtomG2G is trained under the same variational objective as HierG2G, with the latent code $\boldsymbol{z}$ sampled from the posterior $Q(\boldsymbol{z} \mid X, Y)=\mathcal{N}\left(\boldsymbol{\mu<em X_="X," Y="Y">{X, Y}, \boldsymbol{\sigma}</em>}\right)$ and $\left[\boldsymbol{\mu<em X_="X," Y="Y">{X, Y}, \boldsymbol{\sigma}</em>}\right]=\operatorname{MLP}\left(\sum \boldsymbol{c<em X="X">{Y}^{\mathcal{G}}-\sum \boldsymbol{c}</em>\right)$.}^{\mathcal{G}</p>
<h1>C. Experimental Details</h1>
<h2>C.1. Polymer Generation</h2>
<p>Data The polymer dataset (St. John et al., 2019) is downloaded from https://cscdata.nrel.gov/#/datasets/ ad5d2c9a-af0a-4d72-b943-1e433d5750d6. The dataset and motif vocabulary sizes are listed in Table 4.</p>
<p>Hyperparameters For HierVAE, we set the hidden layer dimension to be 400 and latent code dimension $|\boldsymbol{z}|=20$. For CG-VAE and JT-VAE, we used their official implementations for our experiments with their suggested hyperparameters. We set the KL regularization weight $\lambda_{\mathrm{KL}}=0.1$ for all models. Each model has around 5 M parameters.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: center;">Polymer</th>
<th style="text-align: center;">$\log \mathrm{P}(\delta=0.6)$</th>
<th style="text-align: center;">$\log \mathrm{P}(\delta=0.4)$</th>
<th style="text-align: center;">QED</th>
<th style="text-align: center;">DRD2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Training set size</td>
<td style="text-align: center;">76 K</td>
<td style="text-align: center;">75 K</td>
<td style="text-align: center;">99 K</td>
<td style="text-align: center;">88 K</td>
<td style="text-align: center;">34 K</td>
</tr>
<tr>
<td style="text-align: left;">Test set size</td>
<td style="text-align: center;">5000</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">800</td>
<td style="text-align: center;">1000</td>
</tr>
<tr>
<td style="text-align: left;">Motif vocabulary size $</td>
<td style="text-align: center;">\mathcal{S}</td>
<td style="text-align: center;">$</td>
<td style="text-align: center;">436</td>
<td style="text-align: center;">478</td>
<td style="text-align: center;">462</td>
</tr>
<tr>
<td style="text-align: left;">Attachment vocabulary size (avg.) $\left</td>
<td style="text-align: center;">\mathcal{A}\left(\mathcal{S}_{t}\right)\right</td>
<td style="text-align: center;">$</td>
<td style="text-align: center;">5.24</td>
<td style="text-align: center;">3.68</td>
<td style="text-align: center;">3.50</td>
</tr>
</tbody>
</table>
<p>Table 4. Training set size and motif vocabulary size for each dataset.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8. Ablation studies in graph translation tasks. Left: Atom-based decoder; Middle: Two-layer encoder; Right: One-layer encoder.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9. Ablation study of HierVAE (coined as small motif), where motifs are restricted to single rings and bonds.</p>
<p>Metrics and Samples Our metrics are computed using the implementation from Polykovskiy et al. (2018) (https: //github.com/molecularsets/moses). Samples from our models are shown in Figure 10.</p>
<p>Ablation Study Our small motif baseline builds on the same hierarchical architecture as our model, but it generates molecules based on small motifs restricted to single rings or bonds (see Figure 9).</p>
<h1>C.2. Graph-to-Graph Translation</h1>
<p>Data The graph translation datasets are directly downloaded from the link provided in Jin et al. (2019). The dataset and motif vocabulary size for each dataset is listed in Table 4.</p>
<p>Hyperparameters For HierG2G, we set the hidden layer dimension to be 270 and the embedding layer dimension 200. We set the latent code dimension $|\boldsymbol{z}|=8$ and KL regularization weight $\lambda_{\mathrm{KL}}=0.3$. We run $T=20$ iterations of message passing in each layer of the encoder. For AtomG2G, we set the hidden layer and embedding layer dimension to be 400 so that both models have roughly the same number of parameters. We also set $\lambda_{\mathrm{KL}}=0.3$ and number of message passing iterations to be $T=20$. We train both models with Adam optimizer with default parameters.</p>
<p>Ablation Study Our ablation studies are illustrated in Figure 8. In our first experiment, we changed our decoder to the atom-based decoder of AtomG2G. As the encoder is still hierarchical, we modified the input of the decoder attention to include both atom and motif vectors. We set the hidden layer and embedding layer dimension to be 300 to match the original model size. Our next two experiments reduces the number of hierarchies in both our encoder and decoder MPN. In the two-layer model, molecules are represented by $\boldsymbol{c}<em X="X">{X}=\boldsymbol{c}</em>}^{S} \cup \boldsymbol{c<em _mathcal_A="\mathcal{A">{X}^{A}$. We make motif predictions based on hidden vector $\boldsymbol{h}</em><em _mathcal_S="\mathcal{S">{h}}$ instead of $\boldsymbol{h}</em><em X="X">{h}}$ because the motif layer is removed. In the one-layer model, molecules are represented by $\boldsymbol{c}</em>}=\boldsymbol{c<em _in="\in" _mathcal_S="\mathcal{S" v="v">{X}^{S}$ and we make motif predictions based on atom vectors $\sum</em><em v="v">{h}} \boldsymbol{h}</em>$. The hidden layer dimension is adjusted accordingly to match the original model size.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10. Sampled polymers from HierVAE.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ In our experiments, the number of possible attachments are usually less than 20 for polymers and small molecules.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
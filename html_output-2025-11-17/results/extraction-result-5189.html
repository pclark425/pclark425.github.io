<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5189 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5189</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5189</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-110.html">extraction-schema-110</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <p><strong>Paper ID:</strong> paper-268230542</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.00827v1.pdf" target="_blank">Self-Refinement of Language Models from External Proxy Metrics Feedback</a></p>
                <p><strong>Paper Abstract:</strong> It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised fine-tuned model on human annotated data.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5189.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5189.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ProMiSe</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Proxy Metric-based Self-Refinement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An iterative, principle-guided self-refinement algorithm that uses external proxy metrics (e.g., ROUGE variants and a factuality reward model) to decide when and how an LLM should refine its own responses via in-context, principle-specific prompts, best-of-N sampling, rejection sampling, and metric-thresholding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>framework (applied to multiple LMs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Algorithm applied to open-source instruction-tuned LMs (FLAN-T5-XXL, 11B; Llama-2-13B-Chat, 13B) using best-of-N sampling, in-context exemplars for principle-specific refinement prompts, and rejection sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ProMiSe (Proxy Metric-based Self-Refinement)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Generate an initial set of N responses (best-of-N), pick the best by proxy metrics, check sufficiency thresholds per metric; if insufficient, iteratively refine the chosen response per principle (specificity, faithfulness, relevance) using principle-specific in-context exemplars and prompts, produce multiple refinement candidates, select by proxy metrics (rejection sampling), update only if weighted metric improvements exceed a threshold λ; repeat until thresholds are met or a max iteration bound.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Content-grounded question answering; multi-turn dialogue generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Produce an answer grounded in a provided document and conversation history (single-turn QA on MultiDoc2Dial and QuAC), and generate multi-turn synthetic dialogues (agent and user turns) for fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Varies by model and proxy-metric set; paper reports consistent improvements from initial (post-sampling) to final (post-refinement) responses across automatic metrics and GPT-4 judging. Example (FLAN-T5-XXL, 3-shot, ROUGE-only): ROUGE-L 21.50 -> 21.84; BERT-Recall 27.27 -> 28.96; BERT K-Precision 38.08 -> 41.41; Recall 31.19 -> 33.96; K-Precision 75.58 -> 78.78.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>The comparable initial (post-sampling/rejection) scores above: e.g., FLAN-T5-XXL 3-shot ROUGE-only initial: ROUGE-L 21.50; BERT-Recall 27.27; BERT K-Precision 38.08; Recall 31.19; K-Precision 75.58.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: automatic metrics (ROUGE-L, BERTScore Recall, BERT K-Precision, Recall, K-Precision) show consistent gains from initial to final responses across settings; example numeric gains given above for FLAN-T5-XXL. Multi-turn synthetic-dialogue based fine-tuning of Llama-2-13B-Chat produced sizable gains (see separate result entry). Qualitative/robustness: GPT-4-as-judge prefers final answers over initial ones across conditions on MultiDoc2Dial (win rates reported >50% for final answers); analysis shows improvements on proxy metrics correlate with improvements on downstream evaluation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Reported limitations include: (1) using WeCheck alone can be inconsistent (it measures entailment and may prefer longer/entailed but incorrect responses), (2) final responses are slightly longer on average and may lower ROUGE-L versus gold (longer outputs can hurt some lexical metrics), (3) refinement can harm high-quality initial responses if stopping mechanisms are not well-designed (cited prior work and discussed as a general risk), (4) algorithm requires careful threshold calibration and user-defined weights/thresholds, (5) evaluated only in English and is subject to the generative-model biases/limitations of the base LMs (toxicity, stereotypes), and (6) number of refinement iterations and stopping criteria are user-configured and not guaranteed to be optimal.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5189.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5189.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>FLAN-T5-XXL (ProMiSe experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>FLAN-T5-XXL (11B, instruction-tuned T5 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 11-billion-parameter instruction-tuned encoder-decoder Transformer (FLAN-T5-XXL) used in this paper as the base LM to both generate initial answers and perform in-context, principle-guided self-refinement via ProMiSe.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>FLAN-T5-XXL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Instruction-tuned T5 family model with ~11B parameters (FLAN-T5-XXL); used for few-shot generation and refinement prompts in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ProMiSe (iterative generate-then-refine with proxy metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>The same FLAN-T5-XXL model generates initial candidate responses (best-of-N) and, when necessary, is prompted with principle-specific in-context exemplars to produce refined candidates; selection and sufficiency decisions are made by external proxy metrics (ROUGE variants and/or WeCheck).</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Single-turn content-grounded question answering (MultiDoc2Dial, QuAC)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Answer a user's question grounded in a provided document (and prior dialog context).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Example (3-shot, ROUGE-only proxy set): FINAL ROUGE-L = 21.84; BERT-Recall = 28.96; BERT K-Precision = 41.41; Recall = 33.96; K-Precision = 78.78 (these are 'final' scores after ProMiSe refinement).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Example (3-shot, ROUGE-only): INITIAL ROUGE-L = 21.50; BERT-Recall = 27.27; BERT K-Precision = 38.08; Recall = 31.19; K-Precision = 75.58 (these are post-sampling but pre-refinement scores used as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Direct numeric comparisons in Table 6 show modest but consistent improvements from INITIAL to FINAL across multiple automatic metrics when using ProMiSe with ROUGE (and ROUGE+WeCheck) thresholds; GPT-4-as-judge also prefers final answers for MultiDoc2Dial. The paper also reports that using ROUGE+WeCheck together gives the best and most consistent gains.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>FLAN-T5-XXL in zero-shot sometimes outperforms 3-shot initial generation (noted in paper), indicating sensitivity to prompt design; refinement improvements depend on in-context exemplars and metric choice; potential to increase length (and occasionally worsen some lexical metrics) and requires threshold tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5189.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5189.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using self-reflection, self-critique, or iterative generate-then-reflect methods to improve answer quality, including details of the methods, tasks, performance with and without reflection, and any evidence of answer quality improvement or limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Llama-2-13B-Chat (ProMiSe experiments & fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Llama-2-13B-Chat (13B parameter conversational variant of Llama-2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion-parameter conversational Llama-2 model used both zero-/few-shot with ProMiSe for single-turn QA and fine-tuned (via QLoRA) on synthetic multi-turn dialogues generated with and without ProMiSe to measure downstream benefits of refinement-enabled synthetic data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Llama-2-13B-Chat</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>13B-parameter instruction/conversationally-tuned Llama-2 model (chat variant); the paper uses it in few-shot zero-shot generation and also fine-tunes it with QLoRA (4-bit quantized LoRA) on synthetic dialogues.</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_name</strong></td>
                            <td>ProMiSe (generate-then-refine using proxy metrics) and synthetic-data fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>reflection_method_description</strong></td>
                            <td>Llama-2-13B-Chat is used to generate initial and refined responses with the same in-context refinement prompts under ProMiSe (best-of-N + metric-thresholding + per-principle refinement). Additionally, ProMiSe-generated multi-turn synthetic dialogues (initial vs final versions) are used as data to QLoRA fine-tune Llama-2-13B-Chat; the fine-tuned model's performance is compared when trained on data with versus without ProMiSe refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>num_iterations</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Single-turn content-grounded QA (MultiDoc2Dial, QuAC) and fine-tuned multi-turn dialogue response generation</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Single-turn: produce grounded answers; Multi-turn/fine-tuning: generate synthetic dialogues (1-3 agent responses) and fine-tune the model on those dialogues for downstream QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_reflection</strong></td>
                            <td>Single-turn example (3-shot, ROUGE+WeCheck proxy set): FINAL ROUGE-L = 21.11; BERT-Recall = 30.95; BERT K-Precision = 40.05; Recall = 38.62; K-Precision = 76.89 (from Table 6). Multi-turn fine-tuning: fine-tuning on ProMiSe-refined synthetic dialogues yields sizable relative gains: +6 to +6.75% for BERT-Recall and Recall, and +7.5 to +8% for BERT K-Precision and K-Precision (reported ranges across datasets/conditions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_reflection</strong></td>
                            <td>Single-turn example (3-shot, ROUGE+WeCheck): INITIAL ROUGE-L = 21.48; BERT-Recall = 30.29; BERT K-Precision = 39.15; Recall = 36.79; K-Precision = 76.34. Multi-turn fine-tuning baseline: models fine-tuned on synthetic data generated without ProMiSe (INITIAL) show lower metric scores; combining human annotated data with ProMiSe synthetic data outperforms training on human annotated data only (specific numeric deltas reported in Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_improvement</strong></td>
                            <td>Quantitative: direct comparison of INITIAL vs FINAL generations (Table 6) and of QLoRA fine-tuning on synthetic datasets generated WITH vs WITHOUT ProMiSe (Table 2) show measurable metric improvements; multi-turn fine-tuning results: reported +6–6.75% (BERT-Recall, Recall) and +7.5–8% (BERT K-Precision, K-Precision). LLM-as-judge (GPT-4) prefers final (refined) responses on MultiDoc2Dial across all tested metric-sets.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>WeCheck-only thresholding sometimes yields less consistent improvement (on QuAC WeCheck-only was an outlier where final was not preferred); refined outputs can be longer and may hurt some gold-aligned lexical metrics; success depends on metric selection, threshold calibration, and prompt exemplars; ProMiSe's effectiveness varies by dataset and metric configuration; algorithm and synthetic data inherit Llama-2’s biases and English-only evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Self-Refinement of Language Models from External Proxy Metrics Feedback', 'publication_date_yy_mm': '2024-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Self-refine: Iterative refinement with self-feedback <em>(Rating: 2)</em></li>
                <li>Enable language models to implicitly learn self-improvement from data <em>(Rating: 2)</em></li>
                <li>Generating sequences by learning to self-correct <em>(Rating: 2)</em></li>
                <li>Selfee: Iterative self-revising LLM empowered by self-feedback generation <em>(Rating: 1)</em></li>
                <li>Shepherd: A critic for language model generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5189",
    "paper_id": "paper-268230542",
    "extraction_schema_id": "extraction-schema-110",
    "extracted_data": [
        {
            "name_short": "ProMiSe",
            "name_full": "Proxy Metric-based Self-Refinement",
            "brief_description": "An iterative, principle-guided self-refinement algorithm that uses external proxy metrics (e.g., ROUGE variants and a factuality reward model) to decide when and how an LLM should refine its own responses via in-context, principle-specific prompts, best-of-N sampling, rejection sampling, and metric-thresholding.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "framework (applied to multiple LMs)",
            "model_description": "Algorithm applied to open-source instruction-tuned LMs (FLAN-T5-XXL, 11B; Llama-2-13B-Chat, 13B) using best-of-N sampling, in-context exemplars for principle-specific refinement prompts, and rejection sampling.",
            "reflection_method_name": "ProMiSe (Proxy Metric-based Self-Refinement)",
            "reflection_method_description": "Generate an initial set of N responses (best-of-N), pick the best by proxy metrics, check sufficiency thresholds per metric; if insufficient, iteratively refine the chosen response per principle (specificity, faithfulness, relevance) using principle-specific in-context exemplars and prompts, produce multiple refinement candidates, select by proxy metrics (rejection sampling), update only if weighted metric improvements exceed a threshold λ; repeat until thresholds are met or a max iteration bound.",
            "num_iterations": null,
            "task_name": "Content-grounded question answering; multi-turn dialogue generation",
            "task_description": "Produce an answer grounded in a provided document and conversation history (single-turn QA on MultiDoc2Dial and QuAC), and generate multi-turn synthetic dialogues (agent and user turns) for fine-tuning.",
            "performance_with_reflection": "Varies by model and proxy-metric set; paper reports consistent improvements from initial (post-sampling) to final (post-refinement) responses across automatic metrics and GPT-4 judging. Example (FLAN-T5-XXL, 3-shot, ROUGE-only): ROUGE-L 21.50 -&gt; 21.84; BERT-Recall 27.27 -&gt; 28.96; BERT K-Precision 38.08 -&gt; 41.41; Recall 31.19 -&gt; 33.96; K-Precision 75.58 -&gt; 78.78.",
            "performance_without_reflection": "The comparable initial (post-sampling/rejection) scores above: e.g., FLAN-T5-XXL 3-shot ROUGE-only initial: ROUGE-L 21.50; BERT-Recall 27.27; BERT K-Precision 38.08; Recall 31.19; K-Precision 75.58.",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: automatic metrics (ROUGE-L, BERTScore Recall, BERT K-Precision, Recall, K-Precision) show consistent gains from initial to final responses across settings; example numeric gains given above for FLAN-T5-XXL. Multi-turn synthetic-dialogue based fine-tuning of Llama-2-13B-Chat produced sizable gains (see separate result entry). Qualitative/robustness: GPT-4-as-judge prefers final answers over initial ones across conditions on MultiDoc2Dial (win rates reported &gt;50% for final answers); analysis shows improvements on proxy metrics correlate with improvements on downstream evaluation metrics.",
            "limitations_or_failure_cases": "Reported limitations include: (1) using WeCheck alone can be inconsistent (it measures entailment and may prefer longer/entailed but incorrect responses), (2) final responses are slightly longer on average and may lower ROUGE-L versus gold (longer outputs can hurt some lexical metrics), (3) refinement can harm high-quality initial responses if stopping mechanisms are not well-designed (cited prior work and discussed as a general risk), (4) algorithm requires careful threshold calibration and user-defined weights/thresholds, (5) evaluated only in English and is subject to the generative-model biases/limitations of the base LMs (toxicity, stereotypes), and (6) number of refinement iterations and stopping criteria are user-configured and not guaranteed to be optimal.",
            "uuid": "e5189.0",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "FLAN-T5-XXL (ProMiSe experiments)",
            "name_full": "FLAN-T5-XXL (11B, instruction-tuned T5 variant)",
            "brief_description": "An 11-billion-parameter instruction-tuned encoder-decoder Transformer (FLAN-T5-XXL) used in this paper as the base LM to both generate initial answers and perform in-context, principle-guided self-refinement via ProMiSe.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "FLAN-T5-XXL",
            "model_description": "Instruction-tuned T5 family model with ~11B parameters (FLAN-T5-XXL); used for few-shot generation and refinement prompts in this work.",
            "reflection_method_name": "ProMiSe (iterative generate-then-refine with proxy metrics)",
            "reflection_method_description": "The same FLAN-T5-XXL model generates initial candidate responses (best-of-N) and, when necessary, is prompted with principle-specific in-context exemplars to produce refined candidates; selection and sufficiency decisions are made by external proxy metrics (ROUGE variants and/or WeCheck).",
            "num_iterations": null,
            "task_name": "Single-turn content-grounded question answering (MultiDoc2Dial, QuAC)",
            "task_description": "Answer a user's question grounded in a provided document (and prior dialog context).",
            "performance_with_reflection": "Example (3-shot, ROUGE-only proxy set): FINAL ROUGE-L = 21.84; BERT-Recall = 28.96; BERT K-Precision = 41.41; Recall = 33.96; K-Precision = 78.78 (these are 'final' scores after ProMiSe refinement).",
            "performance_without_reflection": "Example (3-shot, ROUGE-only): INITIAL ROUGE-L = 21.50; BERT-Recall = 27.27; BERT K-Precision = 38.08; Recall = 31.19; K-Precision = 75.58 (these are post-sampling but pre-refinement scores used as baseline).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Direct numeric comparisons in Table 6 show modest but consistent improvements from INITIAL to FINAL across multiple automatic metrics when using ProMiSe with ROUGE (and ROUGE+WeCheck) thresholds; GPT-4-as-judge also prefers final answers for MultiDoc2Dial. The paper also reports that using ROUGE+WeCheck together gives the best and most consistent gains.",
            "limitations_or_failure_cases": "FLAN-T5-XXL in zero-shot sometimes outperforms 3-shot initial generation (noted in paper), indicating sensitivity to prompt design; refinement improvements depend on in-context exemplars and metric choice; potential to increase length (and occasionally worsen some lexical metrics) and requires threshold tuning.",
            "uuid": "e5189.1",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        },
        {
            "name_short": "Llama-2-13B-Chat (ProMiSe experiments & fine-tuning)",
            "name_full": "Llama-2-13B-Chat (13B parameter conversational variant of Llama-2)",
            "brief_description": "A 13-billion-parameter conversational Llama-2 model used both zero-/few-shot with ProMiSe for single-turn QA and fine-tuned (via QLoRA) on synthetic multi-turn dialogues generated with and without ProMiSe to measure downstream benefits of refinement-enabled synthetic data.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Llama-2-13B-Chat",
            "model_description": "13B-parameter instruction/conversationally-tuned Llama-2 model (chat variant); the paper uses it in few-shot zero-shot generation and also fine-tunes it with QLoRA (4-bit quantized LoRA) on synthetic dialogues.",
            "reflection_method_name": "ProMiSe (generate-then-refine using proxy metrics) and synthetic-data fine-tuning",
            "reflection_method_description": "Llama-2-13B-Chat is used to generate initial and refined responses with the same in-context refinement prompts under ProMiSe (best-of-N + metric-thresholding + per-principle refinement). Additionally, ProMiSe-generated multi-turn synthetic dialogues (initial vs final versions) are used as data to QLoRA fine-tune Llama-2-13B-Chat; the fine-tuned model's performance is compared when trained on data with versus without ProMiSe refinement.",
            "num_iterations": null,
            "task_name": "Single-turn content-grounded QA (MultiDoc2Dial, QuAC) and fine-tuned multi-turn dialogue response generation",
            "task_description": "Single-turn: produce grounded answers; Multi-turn/fine-tuning: generate synthetic dialogues (1-3 agent responses) and fine-tune the model on those dialogues for downstream QA performance.",
            "performance_with_reflection": "Single-turn example (3-shot, ROUGE+WeCheck proxy set): FINAL ROUGE-L = 21.11; BERT-Recall = 30.95; BERT K-Precision = 40.05; Recall = 38.62; K-Precision = 76.89 (from Table 6). Multi-turn fine-tuning: fine-tuning on ProMiSe-refined synthetic dialogues yields sizable relative gains: +6 to +6.75% for BERT-Recall and Recall, and +7.5 to +8% for BERT K-Precision and K-Precision (reported ranges across datasets/conditions).",
            "performance_without_reflection": "Single-turn example (3-shot, ROUGE+WeCheck): INITIAL ROUGE-L = 21.48; BERT-Recall = 30.29; BERT K-Precision = 39.15; Recall = 36.79; K-Precision = 76.34. Multi-turn fine-tuning baseline: models fine-tuned on synthetic data generated without ProMiSe (INITIAL) show lower metric scores; combining human annotated data with ProMiSe synthetic data outperforms training on human annotated data only (specific numeric deltas reported in Table 2).",
            "has_performance_comparison": true,
            "evidence_of_improvement": "Quantitative: direct comparison of INITIAL vs FINAL generations (Table 6) and of QLoRA fine-tuning on synthetic datasets generated WITH vs WITHOUT ProMiSe (Table 2) show measurable metric improvements; multi-turn fine-tuning results: reported +6–6.75% (BERT-Recall, Recall) and +7.5–8% (BERT K-Precision, K-Precision). LLM-as-judge (GPT-4) prefers final (refined) responses on MultiDoc2Dial across all tested metric-sets.",
            "limitations_or_failure_cases": "WeCheck-only thresholding sometimes yields less consistent improvement (on QuAC WeCheck-only was an outlier where final was not preferred); refined outputs can be longer and may hurt some gold-aligned lexical metrics; success depends on metric selection, threshold calibration, and prompt exemplars; ProMiSe's effectiveness varies by dataset and metric configuration; algorithm and synthetic data inherit Llama-2’s biases and English-only evaluation.",
            "uuid": "e5189.2",
            "source_info": {
                "paper_title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
                "publication_date_yy_mm": "2024-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Self-refine: Iterative refinement with self-feedback",
            "rating": 2,
            "sanitized_title": "selfrefine_iterative_refinement_with_selffeedback"
        },
        {
            "paper_title": "Enable language models to implicitly learn self-improvement from data",
            "rating": 2,
            "sanitized_title": "enable_language_models_to_implicitly_learn_selfimprovement_from_data"
        },
        {
            "paper_title": "Generating sequences by learning to self-correct",
            "rating": 2,
            "sanitized_title": "generating_sequences_by_learning_to_selfcorrect"
        },
        {
            "paper_title": "Selfee: Iterative self-revising LLM empowered by self-feedback generation",
            "rating": 1,
            "sanitized_title": "selfee_iterative_selfrevising_llm_empowered_by_selffeedback_generation"
        },
        {
            "paper_title": "Shepherd: A critic for language model generation",
            "rating": 1,
            "sanitized_title": "shepherd_a_critic_for_language_model_generation"
        }
    ],
    "cost": 0.013419,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Self-Refinement of Language Models from External Proxy Metrics Feedback
27 Feb 2024</p>
<p>Keshav Ramji keshavr@seas.upenn.edu 
University of Pennsylvania</p>
<p>Young-Suk Lee 
IBM Research AI</p>
<p>Ramón Fernandez Astudillo 
IBM Research AI</p>
<p>Md Arafat Sultan 
IBM Research AI</p>
<p>Tahira Naseem tnaseem@us.ibm.com 
IBM Research AI</p>
<p>Asim Munawar 
IBM Research AI</p>
<p>Radu Florian raduf@us.ibm.com 
IBM Research AI</p>
<p>Salim Roukos roukos@us.ibm.com 
IBM Research AI</p>
<p>Baolin Peng 
Michel Galley 
Pengcheng He 
Hao Cheng 
Yujia Xie 
Yu Hu 
Qiuyuan Huang 
Lars Liden 
Zhou Yu 
Weizhu Chen 
Jianfeng 2023 Gao 
Check 
William Saunders 
Catherine Yeh 
Jeff Wu 
Steven Bills 
Jérémy Scheurer 
Jon Ander Campos 
Tomasz Korbak 
Jun Shern Chan 
Angelica Chen 
Kyunghyun Cho 
Ethan Perez 
Training 
Noah Shinn 
Federico Cassano 
Edward Berman 
Ash- Win Gopinath 
Karthik Narasimhan 
Shunyu Yao 
Hugo Touvron 
Louis Martin 
Kevin Stone 
Peter Al- Bert 
Amjad Almahairi 
Yasmine Babaei 
Nikolay Bashlykov 
Soumya Batra 
Prajjwal Bhargava 
Shruti Bhosale 
Dan Bikel 
Lukas Blecher 
Cristian Canton Ferrer 
Moya Chen 
Guillem Cucurull 
David Esiobu 
Jude Fernandes 
Jeremy Fu 
Wenyin Fu 
Brian Fuller 
Cynthia Gao 
Vedanuj Goswami 
Naman Goyal 
An- Thony Hartshorn 
Saghar Hosseini 
Rui Hou 
Hakan Inan 
Marcin Kardas 
Viktor Kerkez 
Madian Khabsa 
Isabel Kloumann 
PunitArtem Korenev 
Singh Koura 
Marie-Anne Lachaux 
Thibaut Lavril 
Jenya Lee 
Di- Ana Liskovich 
Yinghai Lu 
Yuning Mao 
Xavier Mar- Tinet 
Todor Mihaylov 
Pushkar Mishra 
Igor Moly- Bog 
Yixin Nie 
Andrew Poulton 
Jeremy Reizen- Stein 
Rashi Rungta 
Kalyan Saladi </p>
<p>IBM Research AI</p>
<p>Alan Schelten
Ruan Silva</p>
<p>Eric Michael Smith
Ranjan Subrama-nian
Ross Tay-lor, Adina WilliamsXiaoqing Ellen Tan, Binh Tang</p>
<p>Jian Xiang Kuan
Puxin XuZheng Yan</p>
<p>Iliyan Zarov
Angela Fan, Melanie Kambadur, Sharan NarangYuchen Zhang</p>
<p>Aurelien Ro-driguez
Sergey EdunovRobert Stojnic</p>
<p>Thomas Scialom
2023Llama</p>
<p>Self-Refinement of Language Models from External Proxy Metrics Feedback
27 Feb 202477FE523A13309882B25F8AB13C91721FarXiv:2403.00827v1[cs.CL]
It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response.In documentgrounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document.In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response.ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time.We apply ProMiSe to open source language models FLAN-T5-XXL and LLAMA-2-13B-CHAT, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality.We further show that finetuning LLAMA-2-13B-CHAT on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised finetuned model on human annotated data.</p>
<p>Introduction</p>
<p>The state-of-the-art large language models (LLMs) have demonstrated to be effective in generating new synthetic data, useful in improving zero-shot task generalization through fine-tuning without requiring vast amounts of human annotations.Various approaches have been proposed to show the ability of models to evaluate and critique responses (Saunders et al., 2022;Scheurer et al., 2023;Shinn et al., 2023;Ye et al., 2023), as well as their potential to refine: given feedback, correct their outputs (Welleck et al., 2022;Peng et al., 2023;Madaan et al., 2023;Huang et al., 2023;Wang et al., 2023b).These explorations have studied various feedback mechanisms (human-in-the-loop, reward models to capture human preferences, model-generated feedback) and forms (pairwise comparisons, scalar scores, natural language descriptions), as well as refinement techniques (separate supervised refiners, domain-specific refinement).</p>
<p>Of particular note are recent works exploring the self-refinement phenomenon (Madaan et al., 2023;Wang et al., 2023b;Shinn et al., 2023), leveraging the same LLM to perform critique and/or refinement on top of generating responses.The observations of these works unveil shortcomings: smaller instruction-tuned models fail to replicate the results of systems such as GPT-3.5 and GPT-4 in refinement, and in the absence of well-designed stopping mechanisms, self-refinement applied to high-quality responses can make the results worse (Huang et al., 2023).When humans correct themselves, they do it often with one or more objectives in mind, i.e. principles.Such principles may include faithfulness, specificity, safety (i.e.nontoxic), relevance to a question posed, etc. and may vary across tasks -we seek to imbue these aspects into conversational agents, to ensure they are reflected in the agent's responses.</p>
<p>To this effect, we introduce an iterative, principle-guided approach to self-refinement in relatively smaller language models where refinement has previously proven unsuccessful.Our algorithm, termed Proxy Metric-based Self-Refinement (ProMiSe), combines proxy metric thresholding for different principles with independent principlespecific few-shot refinement and best-of-N rejection sampling.This allows for the deliberate selection of task-appropriate metrics with calibrated sufficiency thresholds, and specific prompts better designed to match the instruction-following capabilities of smaller models.In this manner, we perform multi-aspect self-refinement via iterative Figure 1: A high-level overview of our proposed self-refinement algorithm for content-grounded question answering, with both initial response generation and iterative refinement performed with the same Large Language Model M.</p>
<p>single-aspect improvement queries, as opposed to simultaneous refinement on many dimensions.</p>
<p>We apply this method to content-grounded question answering, demonstrating consistent improvements on a diverse set of evaluation metrics for single-turn response generation.We then extend ProMiSe to multi-turn dialogue data generation to generate user queries in addition to agent responses.We fine-tune LLAMA-2-13B-CHAT on the synthetic data, yielding significant improvement over the zero-shot baseline and supervised models solely fined-tuned on human annotations.</p>
<p>Crucially, this approach is built on open-source models and does not rely on propietary models with black-box API access; we note, however, that the proposed algorithm can be directly applied to closed-source models as well.Furthermore, it can be extended to other tasks, provided that proxy metrics can be defined and a few in-context exemplars can be created for the relevant principles.Our key contributions are:</p>
<p>• We introduce a novel domain-agnostic algorithm, ProMiSe, to perform multi-aspect self-refinement on desirable principles for a response through in-context learning, using proxy metrics as external quality feedback.</p>
<p>• ProMiSe is applied to both content-grounded single-turn question answering and multi-turn dialogue generation.Extensive evaluations on MultiDoc2Dial and QuAC datasets with 5 automatic evaluation metrics (RougeL, Bert-Recall, Bert-K-Precision, Recall, K-Precision) as well as LLM-as-judge with GPT-4, demonstrate its effectivenss both in few-shot and fine-tuning settings.We will release both the software and the synthetic dialogue data.</p>
<p>• We analyze the relationship between the change in proxy metric scores and the downstream evaluation metrics, revealing an unsupervised correlation and reinforcing the efficacy of our method.</p>
<p>Algorithm</p>
<p>Given an input, e.g. a document and conversation history, ProMiSe executes three main steps: (i) Generate an initial response, (ii) Obtain external feedback via proxy metrics, and (iii) Refine the response with respect to each principle, if the response is deemed inadequate by the feedback mechanism.The last two steps are run iteratively until the response meets a quality threshold.We present a detailed description of these steps below.</p>
<p>Initial Response Generation</p>
<p>For an input instance, we perform Best-of-N sampling to yield a set of responses, Y 0 , from Lan-
1: Y0 = {yn} N n=1 with yn ∼ pM(y | x, i) 2: y 0 = arg max y∈Y 0 |T | t=1 1(arg max y ′ ∈Y 0 {mt(y ′ , x)} = y) 3: if |T | t=1 1(mt(y 0 , x) ≥ τt) = |T | then
4: return y 0 5: else 6: for iteration j = 0, 1, . . .J: do 7:
Yj+1 = {yp} |P| p=1 with yp ∼ pM(y | y j , x, rp)
8:
y j+1 = arg max y∈Y j+1 |T | t=1 1(arg max y ′ ∈Y j+1 {mt(y ′ , x)} = y) 9: if |T | t=1 1(mt(y j+1 , x) ≥ τp) = |T | then 10: return y j+1 11: else if |T | t=1 wt • 1(mt(y j+1 , x) ≥ mt(y j , x)) then 12: y j+1 = y j 13:
end if 14:</p>
<p>end for 15: return y J 16: end if guage Model M, given the input and an initial generation prompt.The initial generation prompt consists of an instruction and optional in-context demonstrations.The instruction explicitly suggests that a response be generated which reflects desirable principles, the set of which is contained in P. We determine the quality of the sampled responses based on a set of proxy metrics determined a priori, designated as T .We note that the selected metrics should be designed by the user to improve alignment by reflecting the principle set for the response, P, with respect to the current task.As such, each metric m t is also predicated on the inputs provided which may be used as means to assess candidate responses -a text passage or document, conversation history, etc. (thus lending itself to the contentgrounded setting).Each responses in Y 0 is scored with each metric m t in T , and the response with the highest scores on the greatest number of metrics is chosen as the best initial response, y 0 .</p>
<p>Next, we determine the global sufficiency of y 0 as an acceptable response, by comparing the proxy scores element-wise against a threshold τ , consisting of scalar values τ 1 , τ 2 , . . ., τ |T | .τ i is the minimum value such that a response is deemed sufficient, for each metric i in T .If the scores of y 0 exceeds their respective thresholds, for all T components, we return it as the final response.If not (i.e.y 0 fails to clear the threshold on at least one metric), we proceed to the refinement module.</p>
<p>Response Refinement</p>
<p>Our approach to response refinement is predicated on in-context exemplars of principle-specific refinement for the given task.The refinement prompt also contains the previous best response, denoted y jin the first iteration, this is equal to y 0 .Aligning responses with multiple principles (i.e.where P &gt; 1) induces a multi-objective problem; rather than explicitly optimizing across the set simultaneously, we propose deliberate refinement with respect to one principle at a time, selecting an optimal candidate at each iteration based on the proxy metrics.For each iteration in the self-refinement phase, we loop through the set of principles P and generate a set of new responses, with the goal of each resulting response reflecting improvement on its respective principle.In each such query to Language Model M, we introduce a principle-specific refinement prompt, consisting of in-context demonstrations of refinement and an instruction to improve the current best response, both with respect to the current principle.Examples of such prompts are contained in Appendix C.</p>
<p>Determining Improvement We perform rejection sampling, this time on the set of refinement candidates, scoring with each metric in T and selecting the response, y j+1 , with the highest scores on the majority of metrics.The scores of y j+1 , the best refinement candidate, are then compared against the threshold τ .If the scores of y j+1 exceed the threshold on all |T | metrics, then we stop refinement and accept it as the final response.Otherwise, we now compare against the scores of the previous best response, y j .The user assigns weights w = [w 1 , w 2 , . . ., w |T | ] for the respective metrics in T ; these importances should likely be informed by the principles in P which each metric corresponds to, and the user's design goals.Then, given the scores for y j+1 and y j , we compute:
|T | t=1 w t • 1(m t (y j+1 , x) ≥ m t (y j , x))
For each metric in T , the indicator takes on a value of 1 if the new response is an improvement on the previous best response, with respect to that metric, or 0 otherwise, and is weighted by the elements in w.If this sum fails to exceed a user-defined threshold of λ, we do not update the best refinement response for this iteration (i.e.set y j+1 = y j ); else, we proceed to the next refinement iteration, until termination.</p>
<p>Evidence: Question Answering</p>
<p>We apply ProMiSe to content-grounded question answering: given a document and a conversation history, which may consist of a single user utterance (a question posed to the conversational agent) or a multi-turn dialogue between the user and the agent, we seek for the LLM to produce a response to the most recent user query.</p>
<p>Set of Principles</p>
<p>We first identify an appropriate set of principles for the task, which define key characteristics of a good agent response.They are as follows:</p>
<ol>
<li>
<p>Specificity.If an initial response is too vague, this would likely lead to more user interactions asking the agent to make its response more specific.</p>
</li>
<li>
<p>Faithfulness.We suggest that accurate, factual responses are those grounded in the document, and thus should have high (semantic and lexical) overlap with the document.</p>
</li>
<li>
<p>Relevance and Consistency.The conversational agent response should be relevant to the most recent user query, and by induction to the entire conversation history.</p>
</li>
</ol>
<p>In-Context Demonstration Selection</p>
<p>We explore our algorithm through the generation of both a single agent response and an entire multiturn dialogue.We include the algorithm for multiturn dialog generation in Appendix A.</p>
<p>Response and Query Generation.For the generation of an initial response consistent with the content-grounded QA setting, we extract 3 instances from the MultiDoc2Dial (Feng et al., 2021) training data as in-context exemplars; the prompt template is included in Appendix C.This includes the document, conversation history, and the gold response provided by the annotators.The in-context exemplars for query generation work similarly, with 3 demonstrations consisting of different conversation lengths (in number of utterances), but where the last utterance is the final user query.</p>
<p>Principle Refinement.To perform in-context refinement on a particular principle, we similarly take 3 in-context demonstrations from the training dataset, but seek to contrast between a better and worse response, with respect to the principle.</p>
<p>To accomplish this, we manually annotate a worse response for each instance relative to the gold response.In the prompt, we model this as 3 separate utterances: the worse agent response, a user turn probing the agent to improve its response to update along the principle, and another agent utterance containing the better response (i.e. the gold response).To more explicitly suggest the presence of a response quality difference, we include the tags "not {principle}" and "more {principle}", for the two agent turns, respectively, where {principle} is either 'specific', 'relevant', or 'accurate'.</p>
<p>External Proxy Metrics</p>
<p>To capture the aforementioned principles, we define relevant proxy metrics.The proxy metrics should be reflective of response quality improvement along our chosen dimensions and should not directly optimize the final evaluation metrics.ROUGE Metrics.We select three ROUGE metrics intended to correspond to each of the three principles.ROUGE-1 recall between the response and the document mostly represents specificity as more specific answers contain more details from the document.Next, we use ROUGE-L between the response and the document -this primarily addresses faithfulness, as a greater score would suggest a more extractive answer, which is clearly preferable to hallucinated facts.Finally, we compute ROUGE-L between the response and the conversation history to capture consistency between the user query and the response and relevance of the response to the query history.</p>
<p>WeCheck: Factual Consistency Checker.Given a candidate response and the grounding document, WeCheck (Wu et al., 2022) addresses the faithfulness principle.</p>
<p>Our experiments evaluate each model in three thresholding settings: solely using the three aforementioned ROUGE metrics, solely using the WeCheck model, and using a combination of both.If we use only WeCheck, rejection sampling is Table 1: Experimental Results on the MultiDoc2Dial (MD2D) and QuAC test sets, containing 10,204 and 1,000 instances, respectively.Experiments are reported with the Flan-T5-XXL (11B) and Llama-2-13B-Chat models, using 3 Rouge (ROU) measures, the WeCheck reward model (RM), and both in tandem for thresholding."Initial" refers to scoring generations after rejection sampling, while "Final" includes both "sufficient" initial responses and post-refinement responses.In proxy metrics, Rouge-L includes computing between the candidate response with both the grounding document and the given user query, and Rouge-1 is with respect to the document.Highest scores are boldfaced for each model.We decode with sampling method by setting temperature=0.7,top-k=50 and top-p=1.</p>
<p>performed to yield the highest scoring response according to WeCheck and we determine whether a refined response constitutes an improvement solely using the WeCheck scores.If using both ROUGE and WeCheck, a sufficient response must clear the threshold on all four metrics.During refinement, we yield a reward indicator with each category (Rouge and reward model, i.e.WeCheck) which is 1 if deemed to have improved during the present iteration and 0 otherwise, and compute a weighted sum using a user-defined weight vector w.If this sum is greater than 0.5, we update the best response to be the new one, else retain the previous best.</p>
<p>Experimental Results and Discussion</p>
<p>We use two widely-used open-source language models to evaluate our algorithm for contentgrounded question answering, FLAN-T5-XXL Recall, BERTScore K-Precision (K-Prec.hereafter), (Zhang et al., 2020), Recall, and K-Precision.ROUGE-L, BERTScore Recall and Recall measure the agreement between the candidate response and the provided gold response.BERTScore K-Prec.and K-Prec.measure the agreement between the candidate response and the grounding document.We chose Recall and K-Prec.metrics due to their strong correlation with human assessments of instruction-following models in content-grounded QA tasks, (Adlakah et al., 2023).</p>
<p>Single-Turn QA Results</p>
<p>Table 1 presents the results across the three possible metric sets (ROUGE metrics, the WeCheck reward model, and both) as defined in Section 3.3.It can be observed that designating T to be the combination of the three ROUGE metrics and the WeCheck reward model yields improved results across the majority of the metrics.We find that using the WeCheck reward model as the sole sufficiency metric yields less consistent improvement across the set of evaluation metrics, yet boosts performance when applied in tandem with ROUGE metrics.</p>
<p>To identify the appropriate sufficiency threshold for the proxy metrics, we perform a rigorous study of various settings, included in Appendix B. Experiments containing ROUGE metrics maintain a sufficiency threshold of 0.02 for responsedocument Rouge-1 Recall, 0.05 for responsedocument Rouge-L and 0.05 for response-query Rouge-L.Results involving the WeCheck reward model use a threshold of 0.5 between the response and document.In Table 3, we compare the average length (word count) of the initial and final responses, for the Mul-tiDoc2Dial and QuAC datasets.It can be observed that the length of final responses is marginally greater than the average initial response length.This suggests that our performance improvements exhibited in Table 1 are unlikely to be solely a re-sult of longer responses (e.g.reproducing large sections of the document).Simultaneously, our model producing longer responses relative to the gold response likely explains slight declines in Rouge-L scores with both models and both datasets; in particular, Llama-2's responses are much longer than Flan-T5's and the gold response.</p>
<p>Analysis with Proxy Metrics</p>
<p>We explore the relationship between the improvement in the final evaluation metrics and the direction of change on the proxy metrics in ProMiSe.That is, is improvement on the proxy sufficiency metrics during the execution of the algorithm correlated with the downstream evaluation metric improvement from initial to final response?</p>
<p>Count ROU-L Diff.BERT-R Diff.The relationship between the proxy metric scores and the Rouge-L and BERTScore-Recall evaluation metrics is shown in Table 4.We find that the chosen proxy metrics appear to serve as an unsupervised link to the final evaluation metrics.The number of samples that improve for each proxy metric change are roughly similar, a trend noticeable across settings.Furthermore, a greater degree of improvement on proxy metrics (e.g.improving on all three ROUGE metrics) generally corresponds to a larger average improvement (or less negative change) for Rouge-L and BERTScore-Recall with respect to the gold response.This highlights the value of our external metric feedback technique: by optimizing on a scoring scheme while simultaneously preserving the integrity of the downstream evaluation metrics, we can capture a similar notion of response quality and sufficiency.</p>
<p>ROU-Only</p>
<p>Multi-Turn Synthetic Dialogues</p>
<p>We generate synthetic dialogues of varying lengths from Flan-T5-XXL, containing refinement instances: the initial response, a user query to improve the response along a principle, and the refined response.The dialogues are generated from scratch, bootstrapping solely on the grounding documents in MultiDoc2Dial training data.They alternate between user and agent utterances, and consist of 1-3 agent responses (thus containing total 2, 4, or 6 turns).We sampled 10k dialogues with 2 turns, 2k with 4 turns and another 2k with 6 turns.We QLoRA fine-tune (Dettmers et al., 2023) Llama-2-13B-Chat model on these synthetic data.See Section D for details.</p>
<p>The results are shown in Table 2.We observe sizable improvements across all metrics when comparing the performance without refinement, denoted INITIAL, as opposed to with refinement, denoted FINAL.Notably, these improvements are present on both lexical and semantic similarity measures; +6-6.75% for both BERT-Recall and Recall, and +7.5-8% for BERT K-Precision and K-Precision.Furthermore, merging the synthetic data with 38k samples of human annotated data from the Multi-Doc2Dial train set yields improvements over solely training on human annotated data.These results suggest the value of response quality refinement in generating high-quality synthetic data and yielding downstream improvements on evaluation metrics.</p>
<p>LLM-as-a-Judge Evaluation</p>
<p>We also perform automated evaluation with GPT-4 as a judge, (Zheng et al., 2023), which has been shown strongly correlate to human evaluation.Given the initial and final generations, we prompt the model to impartially assess which response is better.We largely adapt the prompts used for MT-bench evaluation in Zheng et al. (2023), which we show in Appendix F. For MultiDoc2Dial, we randomly sample 2,551 of the indices of the test set responses (exactly one quarter), and only With the QuAC dataset, analyze all 1,000 test set instances, likewise evaluating where initial and final responses differ.The results are shown in Figure 2, where the numbers in percentage are the win rate of each response.</p>
<p>We find that GPT-4 deems the final response to be better than the initial response on all conditions for MultiDoc2Dial.The relative outlier is the QuAC dataset with RM-only; this is likely because WeCheck measures entailment rather than agreement.Often the correct short response is less likely to be entailed than an incorrect longer response by the grounding document.However, a higher win rate with the ROUGE + RM combination validates the complementary nature of our proxy metrics.Furthermore, the strong correlation between the automatic evaluation metrics in Table 1 with the GPT-4 evaluation results in Figure 2 evidences the efficacy of our algorithm.</p>
<p>Other Related Work</p>
<p>Various work on self-refinement may be distinguished according to the source of feedback, (Pan et al., 2023;Huang et al., 2023).Internal feed-back relies on the model's inherent knowledge and parameters to reassess its outputs.External feedback incorporates inputs from humans, other models.Our work is inspired by (Madaan et al., 2023).Unlike Madaan et al. (2023), however, who rely on very large LLMs (GPT-3.5,ChatGPT, GPT-4) as the source of internal feedback, we introduce external feedback with proxy metrics and enable self-refinement technique to work with relatively small LLMs including Flan-T5-XXL and Llama-2-13B-Chat in content-grounded setups.</p>
<p>Regarding internal feedback, (Bai et al., 2022) experiment with method for training a harmless AI assistant through self-improvement.(Wang et al., 2023a) propose Shepherd, a language model tuned to critique its own responses and suggest refinements.As for external feedback, (Paul et al., 2023) propose REFINER, a framework for finetuning LMs to explicitly generate intermediate reasoning steps while interacting with a critic model that provides automated feedback on the reasoning.(Gou et al., 2023) propose CRITIC that interacts with appropriate tools, e.g.calculator, search engine, wikipedia, etc., to evaluate certain aspects of the text and then revise the output based on the feedback obtained during the validation process.(Olausson et al., 2024) critically examines the LLM's ability to perform self-repair on problems taken from Hu-manEval and APPS and concludes that self-repair still lags behind what can be achieved with humanlevel debugging.(Gao et al., 2023) propose RARR (Retrofit Attribution using Research and Revision) that revises a generated text on the basis of the relevant evidence retrieved by re-search.</p>
<p>Conclusion</p>
<p>We present a novel algorithm, ProMiSe, for selfrefinement of language models.ProMiSe uses external multi-aspect feedback via proxy metrics capturing desirable principles for a high-quality response.ProMiSe is applied to content-grounded single-turn question answering and multi-turn dialogue generation.Extensive evaluations on Mul-tiDoc2Dial and QuAC datasets with 5 automatic evaluation metrics as well as LLM-as-a-judge with GPT-4, demonstrate its effectiveness in both fewshot learning and supervised fine-tuning setups.This approach crucially enables relatively small LMs like Flan-T5-XXL and Llama-2-13B-Chat to successfully perform self-refinement.</p>
<p>Limitations</p>
<p>Our work employs two open-source LMs: FLAN-T5-XXL and LLAMA-2-13B-CHAT.Therefore, the generated data, including the synthetic multi-turn dialogues, can be susceptible to the limitations of such LMs, particularly the biases inherent in the training data which may be harmful with hate, abuse and social stereotypes.We have tested the algorithm ProMiSe on English only although it would have been more desirable to verify the value of the algorithm in multi-lingual setups.We have conducted extensive evaluations including 5 wellknown automtic evaluation metrics and LLM-as-ajudge with GPT-4, which has been shown to correlate well with human evaluations.Nonetheless, inclusion of human evaluation would have strengthened our position further.</p>
<p>Ethics and Impact</p>
<p>Our technique can be used to guide generations towards user-specified targets; however, this could be applied to generate toxic or malicious content, by way of an adversarial principle selection.Nonetheless, we note that ProMiSe does present meaningful implications in enabling alignment to human preferences (where preferences, in this setting, refer to the user-defined principles).We will release the software for the ProMiSe algorithm, enabling others in the community to consider other principles of interest, or applications to other tasks.2022) model.Note that Rouge-L between response and document (Rouge-L-Doc) as well as between response and the user query (Rouge-L-Query) are maintained constant, while we experiment with changing the third metric between Rouge-1 F1, Rouge-1 K-Precision, and Rouge-1 Recall.We also vary the threshold for the WeCheck reward model (Wu et al., 2022), in isolation and in tandem with the best performing Rouge metric combination.</p>
<p>B Metric Selection and Threshold Calibration</p>
<p>ROUGE Metric</p>
<p>Thresholding.We explore a plethora of different thresholding settings to calibrate sufficiency and select the metric set T accordingly.Note that "Rouge-1 K-Prec.≥ 0.7", refers to using Rouge-1 K-Precision with a threshold of 0.7, and Rouge-L F1 of the response with respect to both the document and the user query with a threshold of 0.05.Then, maintaining the latter two with the same configuration, we vary the first metric, exploring the use of a Rouge-1 K-Precision measure in place of recall and experimenting with thresholds of 0.7, 0.8, and 0.9.</p>
<p>WeCheck and Combo Thresholding.We also vary the threshold for the WeCheck reward model when applied as a standalone sufficiency metric, from 0.4 to 0.8 in increments of size 0.1.Finally, using the chosen combination of the three ROUGE metrics (i.e.Rouge-1 Recall with 0.02, Rouge-L F1 with 0.05 for both response-document and response-query comparison), we vary the WeCheck threshold, yielding a very interesting set of results.Across the majority of metrics, our self-refinement is effective for all thresholding methods applied, although it manifests to different degrees depending on the set of proxy metrics.Notably, the similarity in performance across threshold levels (i.e.not exhibiting a clear trend correlating to an increase in threshold) allows our algorithm to more effectively serve as a means of user-defined risk control with respect to a target refinement rate α.As noted in Section 2, a greater threshold results in a higher standard for the initial response to meet, thus yielding a higher rate of refinement as more responses are deemed inadequate.</p>
<p>C Initial Response Generation, Query Generation, and Principle Refinement Prompts</p>
<p>C.1 Initial Generation Prompt</p>
<p>Provided is a dialog between two speakers, User and Agent.Generate a response that is coherent with the dialog history and the provided document.Desired traits for responses are: 1) Specific -The response contains specific content, and 2) Accurate -The response is correct and factual with respect to the document.</p>
<p>C.2 Query Generation Prompt for Synthetic Dialogue Generation</p>
<p>Provided is a dialog between two speakers, User and Agent.Generate a new question, posed by the user, that is coherent with the dialog history and contains specfic content.</p>
<p>document: \n\nBenefits Planner: Family Benefits \nWhen you start receiving disability benefits , certain members of your family may also qualify for benefits on your record.Benefits may be paid to your : spouse; divorced spouse ; children; disabled child ; and adult child disabled before age 22. . . .Our instruction explicitly suggests to improve on specificity, and that in the provided in-context exemplars, the latter response (Agent response 2) is a specificity improvement over the former response (Agent response 1).Notably, we demonstrate to the model that "Let's make this response more specific" is an utterance in between the worse and better responses.Each exemplar and the "more specific" (gold) response is derived from the MultiDoc2Dial train set, while the "not specific" response is developed by a human annotator, bootstrapping off the gold response.The three exemplars are separated by "###", and the above prompt omits the current instance inputs.</p>
<p>Find out more about</p>
<p>D Fine-tuning Experimental Setup</p>
<p>We QLoRA fine-tune LLAMA-2-13B-CHAT on both human annotated and synthetically generated Multi-Doc2Dial datasets.We set the learning rate to 1e-5, LoRA rank to 8 and LoRA alpha to 32.We apply 4bit quantization for both model training and inferencing.Unlike baseline model inferencing with few-shot learning for which we use sampling method, we use greedy decoding for fine-tuned models.We train the models with 4 A100 (80GB memory) GPUs up to 10 epochs.Training takes between 5 hours for 8k training samples and 24 hours for about 50k samples.We select the best checkpoint on the basis of the 5 evaluation metrics (RougeL, BERTScore Recall, BERTScore K-Prec., Recall and K-Prec.)scores on the development test data.1; this table presents a comparison between 0-shot and 3-shot performance, for each metric set.</p>
<p>E Zero-Shot vs Few-Shot Comparison</p>
<p>In Table 6, we also present a comparison based on the number of few-shot exemplars employed in initial response generation.Notably, we observe that the 0-shot performance of initial responses are, in fact, higher than the 3-shot results for the same phase.This suggests that instruction-tuned LMs such as Flan-T5-XXL are already fairly adept at dialogue response generation without in-context exemplars.Furthermore, we find that the zero-shot setting achieves higher initial scores relative to 3-shot for Flan-T5-XXL, but such improvement is less consistent for Llama-2-13B-Chat.Simultaneously, we found that the 3-shot results with refinement constitute an improvement over the 0-shot performance.This indicates that in-context exemplars are necessary to improve performance during the refinement phase, although only three demonstrations are sufficient to illustrate the notion of the target principle to the LM.That is, despite fairly coherent initial responses, there is still room for improvement, achieved when using three in-context exemplars per principle.Thus, the results reported above and in Table 1 hold 3 exemplars constant for the refinement phase.</p>
<p>F LLM-as-a-Judge Evaluation Setup</p>
<p>Recent literature (Zheng et al., 2023;Zhang et al., 2024) evidences the ability of using language models as discriminators, judging generation quality in lieu of (or as a supplement to) human evaluation feedback.The line of work has also been a strong motivator in influencing self-feedback and refinement approaches; it demonstrates the ability of powerful models to reflect human preferences and provide meaningful critiques.In pursuing such an approach, we deliberately choose to explicitly model certain properties in the instruction: for example, we seek permutation-invariance (while knowing that models are susceptible to position bias when given a set of multiple choice options and mitigating their preference for longer answers.</p>
<p>F.1 Judge Prompt Template</p>
<p>Please act as an impartial judge and evaluate the quality of the responses provided by the two AI assistants to the user question displayed below.Your evaluation should consider correctness and helpfulness.You will be given a reference document, a user conversation, assistant A's answer, and assistant B's answer.Your job is to evaluate which assistant's answer is better based on the information in the reference document and the user conversation so far.Begin your evaluation by comparing both assistants' answers with the document and the user conversation so far.Identify and correct any mistakes.Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.Do not allow the length of the responses to influence your evaluation.Do not favor certain names of the assistants.Be as objective as possible.After providing your explanation, output your final verdict by strictly following this format: "</p>
<p>Figure 2 :
2
Figure 2: GPT-4-as-a-Judge results on Flan-T5-XXL for MultiDoc2Dial (MD2D) and QuAC.With 2551 randomly sampled instances from the MultiDoc2Dial test set, we examine those for which the initial and final response differ: 495 samples for ROUGE-only, 131 samples for RM-only (WeCheck), and 504 samples for ROUGE + RM.We perform a similar analysis with all 1000 QuAC test set instances; the respective counts are: 193 samples for ROUGE-only, 65 samples for RM-only, and 224 samples for ROUGE + RM. perform evaluation on samples for which the initial and final responses differ as a result of refinement.With the QuAC dataset, analyze all 1,000 test set instances, likewise evaluating where initial and final responses differ.The results are shown in Figure 2, where the numbers in percentage are the win rate of each response.We find that GPT-4 deems the final response to be better than the initial response on all conditions for MultiDoc2Dial.The relative outlier is the QuAC dataset with RM-only; this is likely because WeCheck measures entailment rather than agreement.Often the correct short response is less likely to be entailed than an incorrect longer response by the grounding document.However, a higher win rate with the ROUGE + RM combination validates the complementary nature of our proxy metrics.Furthermore, the strong correlation between the automatic evaluation metrics in Table1with the GPT-4 evaluation results in Figure2evidences the efficacy of our algorithm.</p>
<p>Figure 3 :
3
Figure3: Above is the initial generation prompt, containing the instruction and the three in-context exemplars drawn from the train set of MultiDoc2Dial(Feng et al., 2021), omitting the current sample inputs (document and context).The exemplars demonstrate question answering given the conversation history, and are separated by "###".</p>
<p>Figure 4 :
4
Figure4: Query generation prompt (q in Appendix A's algorithm), containing an instruction and three in-context demonstrations of user queries given a document (separated by "###"), omitting the current instance inputs.</p>
<p>Figure 5 :
5
Figure5: Refinement prompt (r p ) with respect to the specificity principle, with an instruction and three in-context demonstrations.Our instruction explicitly suggests to improve on specificity, and that in the provided in-context exemplars, the latter response (Agent response 2) is a specificity improvement over the former response (Agent response 1).Notably, we demonstrate to the model that "Let's make this response more specific" is an utterance in between the worse and better responses.Each exemplar and the "more specific" (gold) response is derived from the MultiDoc2Dial train set, while the "not specific" response is developed by a human annotator, bootstrapping off the gold response.The three exemplars are separated by "###", and the above prompt omits the current instance inputs.</p>
<p>Figure 6 :
6
Figure6: LLM-as-a-Judge prompt template for automated response evaluation between initial and final generations in the content-grounded question answering setting.A document and conversation are given as an input for each sample, and we compare two possible agent responses to the most recent user query posed in the conversation.</p>
<p>Table 2 :
2
Effectiveness of the proposed refinement algorithm measured by the synthetic data qualities.We QLoRA fine-tune LLAMA-2-13B-CHAT model on the two sets of synthetic multi-turn dialogues, one generated with the refinement algorithm denoted by Synset x -FINAL, and the other generated without the refinement algorithm denoted by Synset x -INITIAL.Synset 1 includes 8k and Synset 2 , 10k samples of 2 turn dialogues.Synset 3 includes 10k samples of 2 turn, 2k samples of 4 turn, and 2k samples of 6 turn dialogues.The upper portion of the table compares the performance of the model fine-tuned on the synthetic data with the highest-scoring baseline without fine-tuning.The lower portion of the table compares the performance of the model fine-tuned on the combination of human annotated and synthetic data with the model fine-tuned on human annotated data only.
Both datasets feature conversations wherein an-swers to queries posed by the user are expected tocome from a document. 1Evaluation Metrics We use five automatic evalu-ation metrics: ROUGE-L (Lin, 2004), BERTScore
(Choi et al., 2018))and LLAMA-2-13B-CHAT(Touvron et al., 2023)Evaluation Datasets.We evaluate the technique on the test dataset of MultiDoc2Dial(Feng et al., 2021)(https://doc2dial.github.io/multidoc2dial/),content-groundeddialogues,and the validation dataset of QuAC(Choi et al., 2018)(https://quac.ai/),short form question-answering.</p>
<p>Table 3 :
3
Average word token counts for initial and final generations with ProMiSe.Statistics are computed for the three different settings of the proxy metric set, T ; RM is the WeCheck reward model.</p>
<p>Table 4 :
4
Analysis of the correlation between improvement on proxy ROUGE (ROU) and WeCheck reward model (RM) metrics with change in the final evaluation ROUGE-L and BERT-Recall with the gold response.Performed with Flan-T5-XXL on a 2,038 sample Mul-tiDoc2Dial development set.Proxy metric scores are computed between the candidate and either the provided document or context.↑ and ↓ represents improvement and decline, respectively."2 ↑" means that two of the proxy ROUGE metric set improved.The differences reported are averaged across the sample count.</p>
<p>Table 5 :
5
Threshold calibration and metric selection was performed on a development (validation) set split of the MultiDoc2Dial dataset, consisting of 2,038 samples.Experiments are reported with the Flan-T5-XXL (Chung et al.,</p>
<p>document: DIAL-IN search accounts#3_0Log On to DIAL -IN [1 ] \n\nWhat business records must I keep to document the searches I perform?\nThe business records you keep must exist prior to the search you perform and must establish the business purpose of the search.. . .To verify your browser is compatible to continue using any of the state's government websites, please visit https://encryption.ny.gov/ [6].If your browser is not currently compatible , please update it to the newest version.Find out more about Benefits For A Disabled Child.\n\nPublications \nDisability Benefits Benefits For Children What You Need To Know When You Get Social Security Disability Benefits Information for Government Employees Benefits For Children With Disabilities context: User: Oh, hi.Please, i'm looking for some info about family benefits.could you help me out?Agent: Are you currently receiving any disability benefits?User: Yeah, i started to receive it recently.Agent: Well, in that case, i can tell you that some members of your family may also qualify to get benefits on your record.Project 112 or Project SHAD \nIf you were a part of chemical and biological warfare testing through Project 112 or Project Shipboard Hazard and Defense SHAD , you may be at risk for certain illnesses.The Department of Defense s Deseret Test Center in Fort Douglas, Utah, conducted this testing, which took place aboard ships and on land in various locations from 1962 to 1974.Find out if you can get disability compensation or benefits.\n\nCan I get disability benefits from VA? \nYou may be able to get disability benefits if you meet both of the requirements listed below. . . .Get declassified Department of Defense fact sheets If you have a question about the tests , if you have any information that can help show you were part of them including whether you may have been part of them or contact the Department of Defense at 800 -497 -6261.
. . .document: \n\nBenefits Planner: Family Benefits \nWhen you start receivingdisability benefits , certain members of your family may also qualify for benefitson your record. Benefits may be paid to your : spouse; divorced spouse ; children;disabled child ; and adult child disabled before age 22.. . .###Agent: then, your deposit will be added to your new account balance. User: Can you tell me some of the organizations that are exempt from the search fees? Agent: Some of the exempted organizations are any public organization, its officers, a volunteer fire company, volunteer ambulance service, etc. These organizations are exempt from the search fee. User: What to do in case none of the users performed a search that the DMV contacted me for? Agent: You should contact the DMV immediately. User: Why would the DMV contact me about a search? Agent: The DMV may contact you to ask you about a search to make sure you comply with the Dial-In Terms of Service. ### . . . document: \n\nExposure through context: User: I wanted more information on VA benefits and project 112Agent: Were you part of chemical and biological warfare testing through Project 112or Project Shipboard Hazard and Defense SHAD?
context: User: I need to know how to pay the dial-in search account fees.Agent: The custoers must pay a deposit with the application, and it should be enough to pay for two months of searches.Was your application accepted?User: Yes, it was.</p>
<p>Benefits For A Disabled Child.\n\nPublications \nDisability Benefits Benefits For Children What You Need To Know When You Get Social Security Disability Benefits Information for Government Employees Benefits For Children If your service member is part of the Public Health Service , you ll need to fill out the Spouse Coverage Election and Certificate SGLV 8286A and have them turn it in to their unit s personnel officer.Download the Spouse Coverage Election and Certificate PDF context: User: How much will my service member pay for dependent coverage?Agent: Nothing.sWe provide dependent coverage at no cost until the child is 18 years old , or sometimes longer if the child meets one of the requirements listed below User: To continue receiving dependent coverage after age 18, what are the requirements?
. . .context: User: I have a restricted use license issued in NJ and need informationabout driving in NY.Agent: Do you meet NY requirements for obtaining a restricted license?User: Yes, I do.Agent: Great. You can receive a restricted license to drive in NY. The restrictionswill be the same as the same as the restrictions for a driver with a NY driver license.User: Where can I apply for the restricted driver license?###document: \n\nFamily Servicemembers Group Life Insurance (FSGLI) \nFamily SGLI, alsoknown as Family Servicemembers Group Life Insurance FSGLI, offers coverage for thespouse and dependent children of service members covered under full -time SGLI.With Disabilities. . .context: User: Oh, hi. Please, i'm looking for some info about family benefits.could you help me out?Agent: Are you currently receiving any disability benefits?User: Yeah, i started to receive it recently.###document: NY State Adventure License FAQs#3_0\n\n7. Is there an additional fee tohave icons added to my DMV photo document? \nThere are no additional fees if yourequest the icons be added at the time of your photo document renewal.. . .For Boating Safety Certificate and Empire Passport holders , contact Parks via theirwebsite: www.parks.ny.gov [3]. For Lifetime Sportsman, Small / Big Game, BowHunting, Trapping, Muzzle Loading, or Fishing, contact DEC via their website :www.dec.ny.gov [4].. . .</p>
<p>Specificity Principle Refinement Prompt with In-Context ExemplarsWe want to improve the previous response to make it more specific.To aid in this process, we provide examples of incremental improvement on specific, where Agent response 2 is more specific than Agent response 1.To verify your browser is compatible to continue using any of the state's government websites , please visit https://encryption.ny.gov/[6].If your browser is not currently compatible , please update it to the newest version.context: User: I need to know how to pay the dial-in search account fees.Agent: The custoers must pay a deposit with the application, and it should be enough to pay for two months of searches.Agent: Was your application accepted?User: Yes, it was.Agent: then, your deposit will be added to your new account balance.User: Can you tell me some of the organizations that are exempt from the search fees?Agent: Some of the exempted organizations are any public organization, its officers, a volunteer fire company, volunteer ambulance service, etc.These organizations are exempt from the search fee.
. . . document: \n\nBenefits Planner: Family Benefits \nWhen you start receiving C.4 document: DIAL-IN search accounts#3_0Log On to DIAL -IN [1 ] \n\nWhat business disability benefits , certain members of your family may also qualify for benefitsrecords must I keep to document the searches I perform? \nThe business records you on your record. Benefits may be paid to your : spouse; divorced spouse ; children;keep must exist prior to the search you perform and must establish the business disabled child ; and adult child disabled before age 22.purpose of the search.context: User: Oh, hi. Please, i'm looking for some info about family benefits. . . . could you help me out?Agent: Are you currently receiving any disability benefits?User: Yeah, i started to receive it recently.Agent response 1 (not specific): Then, some others may qualify for benefits.Let's make this response more specific.Agent response 2 (more specific): Well, in that case, i can tell you that somemembers of your family may also qualify to get benefits on your record.###document: \n\nExposure through Project 112 or Project SHAD \nIf you were a part ofchemical and biological warfare testing through Project 112 or Project ShipboardHazard and Defense SHAD , you may be at risk for certain illnesses.User: What to do in case none of the users performed a search that the DMV contacted me for? . . .Agent: You should contact the DMV immediately. Get declassified Department of Defense fact sheets If you have a question about theUser: Why would the DMV contact me about a search? tests , if you have any information that can help show you were part of them includingAgent response 1 (not specific): The DMV may contact you about a search to ensure whether you may have been part of them or contact the Department of Defense at C.3 Principle Refinement Prompt Template compliance. 800 -497 -6261.document: {document} Let's make this response more specific. context: User: I wanted more information on VA benefits and project 112Agent response 1 (not specific): Were you part of Project 112 or Project SHAD?context: {context} Agent response 2 (more specific): The DMV may contact you to ask you about a search toAgent response 1 (not {principle}): {less_principle_response} make sure you comply with the Dial-In Terms of Service. Let's make this response more specific.Let's make this response more {principle}. ### Agent response 2 (more specific): Were you part of chemical and biological warfareAgent response 2 (more {principle}): {more_principle_response} . . . testing through Project 112 or Project Shipboard Hazard and Defense SHAD?</p>
<p>Table 6 :
6
Experimental Results on MultiDoc2Dial test set, containing 10,204 instances.Experiments are reported with the Flan-T5-XXL and Llama-2-13B-Chat models, using 3 Rouge (ROU) measures, the WeCheck reward model (abbreviated as RM), and both in tandem for sufficiency thresholding.Highest scores are boldfaced for each model.The zero-shot results are the same as those included in Table
INITIAL EXEMPLARS + METRICSSTAGEROUGE-L BERT-RECALL BERT K-PREC. RECALL K-PREC.FLAN-T5-XXL (11B) RESULTS3-SHOT / ONLY ROU-L + ROU-1INITIAL21.5027.2738.0831.1975.58FINAL21.8428.9641.4133.9678.780-SHOT / ONLY ROU-L + ROU-1INITIAL21.5528.1140.4232.3476.77FINAL21.7229.2942.7434.1479.293-SHOT / ONLY RMINITIAL22.6728.8042.8333.0080.42FINAL22.6528.9143.5533.3181.140-SHOT / ONLY RMINITIAL22.3328.9144.5833.6181.29FINAL22.4329.1745.6034.2082.253-SHOT / ROU + RMINITIAL22.6128.7542.6032.7580.36FINAL22.7129.8944.8634.7681.980-SHOT / ROU + RMINITIAL22.3028.9444.5533.6881.56FINAL22.3830.1046.6035.5883.13LLAMA-2-13B-CHAT RESULTS3-SHOT / ONLY ROU-L + ROU-1INITIAL20.6329.3534.3236.4971.03FINAL20.2330.2236.0738.8272.740-SHOT / ONLY ROU-L + ROU-1INITIAL19.3128.9234.4438.4570.33FINAL18.9529.6736.0440.4371.763-SHOT / ONLY RMINITIAL21.5030.3539.2036.8976.29FINAL21.2530.1139.5136.8576.530-SHOT / ONLY RMINITIAL19.9729.6534.3338.0770.08FINAL19.9529.8940.6838.5976.733-SHOT / ROU + RMINITIAL21.4830.2939.1536.7976.34FINAL21.1130.9540.0538.6276.890-SHOT / ROU + RMINITIAL20.3630.1740.6838.5976.84FINAL20.0630.6441.4640.0077.43
We use a sub-document split on MultiDoc2Dial, to remove the information retrieval (IR) component such that we only have the most relevant document as opposed to the entire set of candidate documents. We use the validation dataset of QuAC as the test data since the testset is not publicly available.
for iteration j = 0, 1, . . .J: doWe include a complete version of Algorithm 1 adapted for synthetic dialogue generation, leveraged in our fine-tuning experiments.At first, we sample a new user query from large language model M, bootstrapping only off of the document.As the total number of turns (utterances) to be modeled is user-defined, we append each utterance to the end of the conversation history.For example, given the last user query, we append an agent response to it, which is either an initial (no refinement necessary) or final (post-refinement) response.If refinement did occur, then we first append the previous best agent response (y j ), then a user turn u of "User: Please make this response more {principle}", and then the improved and sufficient agent response y j+1 .Once an agent response has been procured and appended for the current turn, we continue back to line 1 and generate a new user query, this time conditioning on the conversation history as well; this repeats until the user-specified max turn limit is reached.
Initial Final MultiDoc2Dial (Avg. Gold: 15.55) Flan-T5-XXL ROU-Only. 32.06 35.88</p>
<p>Llama-2-13B-Chat ROU-Only. 39.30 44.40</p>
<p>Llama-2-13B-Chat RM-Only. 38.45 39.51</p>
<p>Llama-2-13B-Chat ROUGE + RM 38. 5659</p>
<p>. Quac (avg, Gold: 12.57</p>
<p>Llama-2-13B-Chat ROU-Only 29. 3341</p>
<p>Llama-2-13B-Chat RM-Only. 27.73 27.39</p>
<p>Evaluating correctness and faithfulness of instructionfollowing models for question answering. References Vaibhav Adlakah, Parishad Behnamghader, Han Xing, Nicholas Lu, Siva Meade, Reddy, 2023</p>
<p>. Yuntao Bai, Saurav Kadavath, Sadipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron Mckinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perrez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Tristan Hume, Samuel R Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan,Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston; Nicholas Joseph, Sam McCandlish, Tom Brownand Jared Kaplan. 2022. Constitutional ai: Harmlessness from ai feedback</p>
<p>Quac : Question answering in context. Eunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen Tau Yih, Yejin Choi, Percy Liang, Luke Zettlemoyer, 2018</p>
<p>. Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei2022Scaling instruction-finetuned language models</p>
<p>Qlora: Efficient finetuning of quantized llms. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer, 2023</p>
<p>MultiDoc2Dial: Modeling dialogues grounded in multiple documents. Song Feng, Sankalp Siva, Hui Patel, Sachindra Wan, Joshi, 10.18653/v1/2021.emnlp-main.498Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican Republic. Association for Computational Linguistics2021Online and Punta Cana</p>
<p>Rarr: Researching and revising what language models say, using language models. Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi Chaganty, Yicheng Fan, Vincent Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics20231</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Large language models cannot self-correct reasoning yet. Jie Huang, Xinyun Chen, Swaroop Mishra, Steven Huaixiu, Adams Wei Zheng, Xinying Yu, Denny Song, Zhou, 2023</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. 2004</p>
<p>Self-refine: Iterative refinement with self-feedback. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Prasad Bodhisattwa, Shashank Majumder, Amir Gupta, Peter Yazdanbakhsh, Clark, 2023</p>
<p>Is self-repair a silver bullet for code generation?. Jeevana Theo X Olausson, Chenglong Priya Inala, Jianfeng Wang, Armando Gao, Solar-Lezama, Proceedings of ICLR 2024. ICLR 20242024</p>
<p>Tianlu Wang, Ping Ya, Ellen Xiaoqing, Sean O Tan, Ramakanth 'brien, Jane Pasunuru, Olga Dwivedi-Yu, Luke Golovneva, Zettlemoyer, arXiv:2308.04592Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023a. Shepherd: A critic for language model generation. arXiv preprint</p>
<p>Enable language models to implicitly learn self-improvement from data. Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu, Heng Ji, 2023b</p>
<p>Generating sequences by learning to self-correct. Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, Yejin Choi, 2022</p>
<p>Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Sujian Li, Yajuan Lv, arXiv:2212.10057Wecheck: Strong factual consistency checker via weakly supervised learning. 2022arXiv preprint</p>
<p>Selfee: Iterative self-revising llm empowered by selffeedback generation. Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Minjoon Seo, 2023Blog post</p>
<p>A comprehensive analysis of the effectiveness of large language models as automatic dialogue evaluators. Chen Zhang, Luis Fernando, D' Haro, Yiming Chen, Malu Zhang, Haizhou Li, AAAI-2024. 2024</p>
<p>Bertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, 20202020</p>
<p>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. </p>            </div>
        </div>

    </div>
</body>
</html>
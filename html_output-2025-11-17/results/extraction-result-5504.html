<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5504 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5504</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5504</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-264288848</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.669.pdf" target="_blank">CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations</a></p>
                <p><strong>Paper Abstract:</strong> Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5504.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5504.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Online Forum Simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used to simulate persona responses in an online forum context)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses GPT-4 as a text-based simulator to generate open-ended forum-style comments from imagined personas across a range of topics, then evaluates whether these simulations produce caricatures (overly individuated and/or exaggerated persona-driven outputs).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art instruction-tuned large language model from OpenAI (cited OpenAI, 2023). The paper uses GPT-4 as the generator for all simulation experiments; detailed architecture and training-data composition are not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational social science / Human-Computer Interaction / Social computing</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate open-ended online-forum comments by different demographic personas (15 personas spanning race/ethnicity, gender, age, political ideology and a neutral "person") across 30 topic pairs (general and specific) to study whether generated responses reflect nuanced topical responses or caricatured persona-driven narratives.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Two-part caricature metric: (1) Individuation measured as classifier accuracy distinguishing S_{p,t,c} from default-persona S_{_,t,c} (binary classifier accuracy on embeddings). (2) Exaggeration measured as normalized cosine similarity of contextualized embeddings to a persona-topic semantic axis (value normalized to [0,1]).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Individuation: mean accuracy > 0.5 for every persona; for some settings (online forum) certain personas (e.g., marginalized groups) had higher individuation; power-analysis reported lowest mean individuation used in power calc = 0.65 (sd up to 0.2). Exaggeration: reported as normalized similarity in [0,1]; general/uncontroversial topics (e.g., Health, Philosophy & Religion, Education, Relationships, Finance) had highest exaggeration scores. A numeric baseline used in power analysis: lowest mean exaggeration among examined simulations = 0.23 (sd up to 0.37). Exact per-simulation numeric tables are shown in figures (no single aggregated accuracy number beyond reported means and figures).</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Topic specificity (more specific topics -> lower caricature/exaggeration); context (online forum context shows higher caricature for general topics compared to interview context); persona type (political leanings, marginalized race/ethnicity groups, and nonbinary gender have higher exaggeration); model quality (authors report older/open-source models produce lower-quality, less-believable simulations); prompt phrasing and implicit defaults in model (model defaults to particular societal perspectives); memorization dynamics may influence results.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Evidence comes from (a) large-scale generation: 100 outputs per simulation (S_{p,t,c}), (b) classifier-based individuation experiments showing mean accuracies >0.5 and per-context differences (figure and confidence intervals), (c) exaggeration scores computed with persona-topic semantic axes showing higher normalized cosine similarity for general topics and certain personas (figures and a fine-grained specificity experiment on Health with five specificity levels showing negative correlation between specificity and exaggeration), (d) robustness checks (alternative embedding model all-distilroberta-v1, unsupervised V-measure clustering) giving similar overall patterns, and (e) context-swapping experiments (online topics in interview context and vice versa) that preserve trends.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Automated evaluation pipeline: generate 100 outputs per simulation; compute contextualized sentence embeddings using Sentence-BERT (all-mpnet-base-v2) and also test alternate encoders; measure individuation by training a supervised random forest classifier (scikit-learn) on embeddings with stratified 80/20 train/test split and reporting test accuracy; construct persona-topic semantic axes using Fightin' Words (weighted log-odds) to select seed words from default-topic and default-persona corpora, build axis from contextualized embeddings of seed words, compute average cosine similarity of outputs to axis and normalize relative to defaults to yield exaggeration score in [0,1]; conduct internal/external validation of axes and robustness experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Method measures susceptibility to caricature (individuation + exaggeration) but is not a comprehensive evaluation of simulation quality; possible false positives (simulations that appear non-caricature-free by other measures); only one-round responses considered (multi-turn not evaluated); implicit model defaults (e.g., Western/white/masculine) and memorized content can bias results; examples shown where persona (e.g., nonbinary) outputs ignore topical information and focus on identity-related statements (failure to be topical); older and open-source LLMs produce lower-quality unrealistic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Within-paper comparisons: contexts compared (online forum vs interview vs Twitter), topic specificity levels compared (fine-grained Health specificity experiment), embedding/differentiator choices compared (all-mpnet-base-v2 vs all-distilroberta-v1; supervised classifier vs unsupervised clustering/V-measure) and context-swapping experiments. External-model comparisons: authors note that open-source and older models perform worse at simulation (cited Dubois et al., 2023 and internal observation) but the paper's experiments focus on GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Use more specific topics to mitigate caricature; researchers should test simulations in their target context with the presented caricature metric; document CoMPosT dimensions (Context, Model, Persona, Topic) and researchers' positionality; exercise care when simulating politicized or marginalized groups; inject variation and multifaceted personas rather than single flattened persona prompts; combine this metric with other evaluations (including human evaluation) since avoiding caricature is necessary but insufficient for simulation quality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5504.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5504.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Interview / Public Opinion Simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used to simulate survey/interview responses for public opinion and social science questions)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper uses GPT-4 to simulate short open-ended interview/survey answers (converted from multiple-choice Pew questions) given a persona description, then evaluates individuation and exaggeration to detect caricature in simulated public opinion data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art instruction-tuned large language model from OpenAI (OpenAI, 2023); used as the generator for interview-style simulations in this study. Detailed architecture/training data not specified in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Public opinion research / Computational social science / Psychology</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate short descriptive identity statements and open-ended answers to converted Pew Research multiple-choice survey questions for 15 personas across 30 Pew topics (most contentious questions converted to open-ended form), to evaluate whether persona influences responses appropriately or leads to caricature.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Individuation measured as classifier accuracy (random forest on Sentence-BERT embeddings); exaggeration measured as normalized cosine similarity to persona-topic semantic axis (0-1).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Individuation: mean individuation score > 0.95 for every persona in the interview context (i.e., simulations are easily differentiable from default-persona); exaggeration: overall exaggeration scores in the interview context comparable to those for more specific online-forum topics (i.e., lower than general online-forum topics); specific numeric exaggeration values are shown in figures; power-analysis references lowest mean individuation = 0.65 and lowest mean exaggeration = 0.23 across examined simulations for planning sample size.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Topic specificity (interview topics tended to be more specific -> lower caricature); context sensitivity (interview context produces lower variability and higher individuation accuracy); prompt phrasing (explicit identity description in the interview prompt increases individuability); persona type (political and marginalized groups show higher exaggeration even in this context).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>High individuation accuracies measured by supervised classifier (figure 4) show interview context outputs are easy to distinguish from defaults (>0.95). Exaggeration comparisons (figures and appendix) show interview context exaggeration scores align with those of specific online-forum topics, supporting the topic-specificity interpretation. Context-switching experiments (Appendix F) show trends persist across contexts when topics are swapped.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Generate 100 outputs per simulation; compute Sentence-BERT embeddings; train/test random forest classifier (80/20 stratified) for individuation; construct persona-topic semantic axes (Fightin' Words seed selection, contextualized embedding averages) and compute normalized cosine similarity for exaggeration; validate axes internally/externally and run robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although individuation is high, high individuation alone does not imply non-caricature (need exaggeration check); high individuability in interview context reduces usefulness of individuation for comparing caricature across personas; method still limited to one-round responses; simulations may be influenced by model defaults and memorized content; caricature metric is only one aspect of quality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Compared to online forum context, interview context yields higher individuation and generally lower exaggeration (similar to more specific topics). Context-swapping experiments tested same topics across contexts and found trends persist; also compared embeddings and differentiators for robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When using LLMs to simulate survey/interview responses, researchers should check for caricature using this two-stage measure, prefer specific question wording where possible, document CoMPosT dimensions, and combine automated metrics with careful qualitative review and human-in-the-loop evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5504.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5504.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (Twitter / Political Simulation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (used to simulate Twitter-style political user posts)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>GPT-4 is used to simulate Twitter posts from users of differing political affiliations to probe partisan worldviews and evaluate tendencies toward caricature and stereotype in political simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>State-of-the-art instruction-tuned LLM from OpenAI used to generate short social-media style posts for persona-topic combinations; exact training details not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_subdomain</strong></td>
                            <td>Computational political science / Political communication / Computational social science</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_task</strong></td>
                            <td>Simulate single-turn Twitter posts for personas with political leanings (e.g., Republican, Democrat) on topical prompts to study whether generated content mirrors partisan opinions or degenerated caricatures/stereotypes.</td>
                        </tr>
                        <tr>
                            <td><strong>accuracy_metric</strong></td>
                            <td>Individuation via supervised classifier accuracy on contextualized embeddings; exaggeration via normalized cosine similarity to persona-topic semantic axes.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_accuracy</strong></td>
                            <td>Individuation: personas (especially political) are generally individuable from defaults; exaggeration: certain persona-topic pairs in the Twitter context show particularly high caricature susceptibility (Appendix G and Figure A4) similar to online-forum trends; specific numeric values are presented in appendix figures rather than tabulated in main text.</td>
                        </tr>
                        <tr>
                            <td><strong>factors_affecting_accuracy</strong></td>
                            <td>Topic specificity, persona politicization (political personas show high exaggeration), context norms (Twitter brevity and style differences), prompt phrasing, and possible memorization of political topics.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_factors</strong></td>
                            <td>Appendix G experiments replicate main-paper trends in the Twitter context. The paper notes some persona-topic combinations are particularly susceptible to caricature (figure and appendix analyses). Context and topic swapping experiments (Appendix F) further support these factor effects.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_method</strong></td>
                            <td>Same pipeline as other contexts: 100 samples per simulation, Sentence-BERT embeddings, classifier for individuation, persona-topic semantic axes for exaggeration, robustness checks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Twitter context shares same limitations: one-round outputs only, potential for model defaults and memorized data to influence outputs, caricature metric not exhaustive for bias detection. The brevity and stylistic norms of Twitter may amplify persona markers leading to exaggerated individuation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparisons</strong></td>
                            <td>Trends in Twitter corroborate online-forum and interview findings; some persona-topic combinations more vulnerable to caricature in Twitter than other contexts. The authors compared context effects and conducted robustness experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Carefully test persona-topic-context combinations when simulating social media users, prefer more specific prompts to reduce caricature, document the CoMPosT dimensions and researchers' positionality, and complement automatic measures with qualitative inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Out of one, many: Using language models to simulate human samples <em>(Rating: 2)</em></li>
                <li>NLPositionality: Characterizing design biases of datasets and models <em>(Rating: 2)</em></li>
                <li>Social simulacra: Creating populated prototypes for social computing systems <em>(Rating: 1)</em></li>
                <li>CommunityLM: Probing partisan worldviews from language models <em>(Rating: 2)</em></li>
                <li>Using large language models to simulate multiple humans and replicate human subject studies <em>(Rating: 2)</em></li>
                <li>Alpaca-Farm: A simulation framework for methods that learn from human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5504",
    "paper_id": "paper-264288848",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [
        {
            "name_short": "GPT-4 (Online Forum Simulation)",
            "name_full": "Generative Pre-trained Transformer 4 (used to simulate persona responses in an online forum context)",
            "brief_description": "This paper uses GPT-4 as a text-based simulator to generate open-ended forum-style comments from imagined personas across a range of topics, then evaluates whether these simulations produce caricatures (overly individuated and/or exaggerated persona-driven outputs).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art instruction-tuned large language model from OpenAI (cited OpenAI, 2023). The paper uses GPT-4 as the generator for all simulation experiments; detailed architecture and training-data composition are not specified in this paper.",
            "model_size": null,
            "scientific_subdomain": "Computational social science / Human-Computer Interaction / Social computing",
            "simulation_task": "Simulate open-ended online-forum comments by different demographic personas (15 personas spanning race/ethnicity, gender, age, political ideology and a neutral \"person\") across 30 topic pairs (general and specific) to study whether generated responses reflect nuanced topical responses or caricatured persona-driven narratives.",
            "accuracy_metric": "Two-part caricature metric: (1) Individuation measured as classifier accuracy distinguishing S_{p,t,c} from default-persona S_{_,t,c} (binary classifier accuracy on embeddings). (2) Exaggeration measured as normalized cosine similarity of contextualized embeddings to a persona-topic semantic axis (value normalized to [0,1]).",
            "reported_accuracy": "Individuation: mean accuracy &gt; 0.5 for every persona; for some settings (online forum) certain personas (e.g., marginalized groups) had higher individuation; power-analysis reported lowest mean individuation used in power calc = 0.65 (sd up to 0.2). Exaggeration: reported as normalized similarity in [0,1]; general/uncontroversial topics (e.g., Health, Philosophy & Religion, Education, Relationships, Finance) had highest exaggeration scores. A numeric baseline used in power analysis: lowest mean exaggeration among examined simulations = 0.23 (sd up to 0.37). Exact per-simulation numeric tables are shown in figures (no single aggregated accuracy number beyond reported means and figures).",
            "factors_affecting_accuracy": "Topic specificity (more specific topics -&gt; lower caricature/exaggeration); context (online forum context shows higher caricature for general topics compared to interview context); persona type (political leanings, marginalized race/ethnicity groups, and nonbinary gender have higher exaggeration); model quality (authors report older/open-source models produce lower-quality, less-believable simulations); prompt phrasing and implicit defaults in model (model defaults to particular societal perspectives); memorization dynamics may influence results.",
            "evidence_for_factors": "Evidence comes from (a) large-scale generation: 100 outputs per simulation (S_{p,t,c}), (b) classifier-based individuation experiments showing mean accuracies &gt;0.5 and per-context differences (figure and confidence intervals), (c) exaggeration scores computed with persona-topic semantic axes showing higher normalized cosine similarity for general topics and certain personas (figures and a fine-grained specificity experiment on Health with five specificity levels showing negative correlation between specificity and exaggeration), (d) robustness checks (alternative embedding model all-distilroberta-v1, unsupervised V-measure clustering) giving similar overall patterns, and (e) context-swapping experiments (online topics in interview context and vice versa) that preserve trends.",
            "evaluation_method": "Automated evaluation pipeline: generate 100 outputs per simulation; compute contextualized sentence embeddings using Sentence-BERT (all-mpnet-base-v2) and also test alternate encoders; measure individuation by training a supervised random forest classifier (scikit-learn) on embeddings with stratified 80/20 train/test split and reporting test accuracy; construct persona-topic semantic axes using Fightin' Words (weighted log-odds) to select seed words from default-topic and default-persona corpora, build axis from contextualized embeddings of seed words, compute average cosine similarity of outputs to axis and normalize relative to defaults to yield exaggeration score in [0,1]; conduct internal/external validation of axes and robustness experiments.",
            "limitations_or_failure_cases": "Method measures susceptibility to caricature (individuation + exaggeration) but is not a comprehensive evaluation of simulation quality; possible false positives (simulations that appear non-caricature-free by other measures); only one-round responses considered (multi-turn not evaluated); implicit model defaults (e.g., Western/white/masculine) and memorized content can bias results; examples shown where persona (e.g., nonbinary) outputs ignore topical information and focus on identity-related statements (failure to be topical); older and open-source LLMs produce lower-quality unrealistic outputs.",
            "comparisons": "Within-paper comparisons: contexts compared (online forum vs interview vs Twitter), topic specificity levels compared (fine-grained Health specificity experiment), embedding/differentiator choices compared (all-mpnet-base-v2 vs all-distilroberta-v1; supervised classifier vs unsupervised clustering/V-measure) and context-swapping experiments. External-model comparisons: authors note that open-source and older models perform worse at simulation (cited Dubois et al., 2023 and internal observation) but the paper's experiments focus on GPT-4.",
            "recommendations_or_best_practices": "Use more specific topics to mitigate caricature; researchers should test simulations in their target context with the presented caricature metric; document CoMPosT dimensions (Context, Model, Persona, Topic) and researchers' positionality; exercise care when simulating politicized or marginalized groups; inject variation and multifaceted personas rather than single flattened persona prompts; combine this metric with other evaluations (including human evaluation) since avoiding caricature is necessary but insufficient for simulation quality.",
            "uuid": "e5504.0",
            "source_info": {
                "paper_title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (Interview / Public Opinion Simulation)",
            "name_full": "Generative Pre-trained Transformer 4 (used to simulate survey/interview responses for public opinion and social science questions)",
            "brief_description": "The paper uses GPT-4 to simulate short open-ended interview/survey answers (converted from multiple-choice Pew questions) given a persona description, then evaluates individuation and exaggeration to detect caricature in simulated public opinion data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art instruction-tuned large language model from OpenAI (OpenAI, 2023); used as the generator for interview-style simulations in this study. Detailed architecture/training data not specified in the paper.",
            "model_size": null,
            "scientific_subdomain": "Public opinion research / Computational social science / Psychology",
            "simulation_task": "Simulate short descriptive identity statements and open-ended answers to converted Pew Research multiple-choice survey questions for 15 personas across 30 Pew topics (most contentious questions converted to open-ended form), to evaluate whether persona influences responses appropriately or leads to caricature.",
            "accuracy_metric": "Individuation measured as classifier accuracy (random forest on Sentence-BERT embeddings); exaggeration measured as normalized cosine similarity to persona-topic semantic axis (0-1).",
            "reported_accuracy": "Individuation: mean individuation score &gt; 0.95 for every persona in the interview context (i.e., simulations are easily differentiable from default-persona); exaggeration: overall exaggeration scores in the interview context comparable to those for more specific online-forum topics (i.e., lower than general online-forum topics); specific numeric exaggeration values are shown in figures; power-analysis references lowest mean individuation = 0.65 and lowest mean exaggeration = 0.23 across examined simulations for planning sample size.",
            "factors_affecting_accuracy": "Topic specificity (interview topics tended to be more specific -&gt; lower caricature); context sensitivity (interview context produces lower variability and higher individuation accuracy); prompt phrasing (explicit identity description in the interview prompt increases individuability); persona type (political and marginalized groups show higher exaggeration even in this context).",
            "evidence_for_factors": "High individuation accuracies measured by supervised classifier (figure 4) show interview context outputs are easy to distinguish from defaults (&gt;0.95). Exaggeration comparisons (figures and appendix) show interview context exaggeration scores align with those of specific online-forum topics, supporting the topic-specificity interpretation. Context-switching experiments (Appendix F) show trends persist across contexts when topics are swapped.",
            "evaluation_method": "Generate 100 outputs per simulation; compute Sentence-BERT embeddings; train/test random forest classifier (80/20 stratified) for individuation; construct persona-topic semantic axes (Fightin' Words seed selection, contextualized embedding averages) and compute normalized cosine similarity for exaggeration; validate axes internally/externally and run robustness checks.",
            "limitations_or_failure_cases": "Although individuation is high, high individuation alone does not imply non-caricature (need exaggeration check); high individuability in interview context reduces usefulness of individuation for comparing caricature across personas; method still limited to one-round responses; simulations may be influenced by model defaults and memorized content; caricature metric is only one aspect of quality.",
            "comparisons": "Compared to online forum context, interview context yields higher individuation and generally lower exaggeration (similar to more specific topics). Context-swapping experiments tested same topics across contexts and found trends persist; also compared embeddings and differentiators for robustness.",
            "recommendations_or_best_practices": "When using LLMs to simulate survey/interview responses, researchers should check for caricature using this two-stage measure, prefer specific question wording where possible, document CoMPosT dimensions, and combine automated metrics with careful qualitative review and human-in-the-loop evaluation.",
            "uuid": "e5504.1",
            "source_info": {
                "paper_title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-4 (Twitter / Political Simulation)",
            "name_full": "Generative Pre-trained Transformer 4 (used to simulate Twitter-style political user posts)",
            "brief_description": "GPT-4 is used to simulate Twitter posts from users of differing political affiliations to probe partisan worldviews and evaluate tendencies toward caricature and stereotype in political simulations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "State-of-the-art instruction-tuned LLM from OpenAI used to generate short social-media style posts for persona-topic combinations; exact training details not specified in this paper.",
            "model_size": null,
            "scientific_subdomain": "Computational political science / Political communication / Computational social science",
            "simulation_task": "Simulate single-turn Twitter posts for personas with political leanings (e.g., Republican, Democrat) on topical prompts to study whether generated content mirrors partisan opinions or degenerated caricatures/stereotypes.",
            "accuracy_metric": "Individuation via supervised classifier accuracy on contextualized embeddings; exaggeration via normalized cosine similarity to persona-topic semantic axes.",
            "reported_accuracy": "Individuation: personas (especially political) are generally individuable from defaults; exaggeration: certain persona-topic pairs in the Twitter context show particularly high caricature susceptibility (Appendix G and Figure A4) similar to online-forum trends; specific numeric values are presented in appendix figures rather than tabulated in main text.",
            "factors_affecting_accuracy": "Topic specificity, persona politicization (political personas show high exaggeration), context norms (Twitter brevity and style differences), prompt phrasing, and possible memorization of political topics.",
            "evidence_for_factors": "Appendix G experiments replicate main-paper trends in the Twitter context. The paper notes some persona-topic combinations are particularly susceptible to caricature (figure and appendix analyses). Context and topic swapping experiments (Appendix F) further support these factor effects.",
            "evaluation_method": "Same pipeline as other contexts: 100 samples per simulation, Sentence-BERT embeddings, classifier for individuation, persona-topic semantic axes for exaggeration, robustness checks.",
            "limitations_or_failure_cases": "Twitter context shares same limitations: one-round outputs only, potential for model defaults and memorized data to influence outputs, caricature metric not exhaustive for bias detection. The brevity and stylistic norms of Twitter may amplify persona markers leading to exaggerated individuation.",
            "comparisons": "Trends in Twitter corroborate online-forum and interview findings; some persona-topic combinations more vulnerable to caricature in Twitter than other contexts. The authors compared context effects and conducted robustness experiments.",
            "recommendations_or_best_practices": "Carefully test persona-topic-context combinations when simulating social media users, prefer more specific prompts to reduce caricature, document the CoMPosT dimensions and researchers' positionality, and complement automatic measures with qualitative inspection.",
            "uuid": "e5504.2",
            "source_info": {
                "paper_title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Out of one, many: Using language models to simulate human samples",
            "rating": 2,
            "sanitized_title": "out_of_one_many_using_language_models_to_simulate_human_samples"
        },
        {
            "paper_title": "NLPositionality: Characterizing design biases of datasets and models",
            "rating": 2,
            "sanitized_title": "nlpositionality_characterizing_design_biases_of_datasets_and_models"
        },
        {
            "paper_title": "Social simulacra: Creating populated prototypes for social computing systems",
            "rating": 1,
            "sanitized_title": "social_simulacra_creating_populated_prototypes_for_social_computing_systems"
        },
        {
            "paper_title": "CommunityLM: Probing partisan worldviews from language models",
            "rating": 2,
            "sanitized_title": "communitylm_probing_partisan_worldviews_from_language_models"
        },
        {
            "paper_title": "Using large language models to simulate multiple humans and replicate human subject studies",
            "rating": 2,
            "sanitized_title": "using_large_language_models_to_simulate_multiple_humans_and_replicate_human_subject_studies"
        },
        {
            "paper_title": "Alpaca-Farm: A simulation framework for methods that learn from human feedback",
            "rating": 1,
            "sanitized_title": "alpacafarm_a_simulation_framework_for_methods_that_learn_from_human_feedback"
        }
    ],
    "cost": 0.013451,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations</p>
<p>Myra Cheng 
Department of Computer Science
Stanford University</p>
<p>Tiziano Piccardi piccardi@cs.stanford.edu 
Department of Computer Science
Stanford University</p>
<p>Diyi Yang diyiy@cs.stanford.edu 
Department of Computer Science
Stanford University</p>
<p>CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations
B2C147B2803391BC305B294B249DF95D
Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys.However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations.Moreover, there is growing concern that these simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes.To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic.We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration.We evaluate the level of caricature in scenarios from existing work on LLM simulations.We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have shown promise in capturing social nuances and human behavior.For instance, researchers have reproduced results from social science experiments and public opinion surveys using LLMs (Argyle et al., 2023;Aher et al., 2023;Santurkar et al., 2023, inter alia).More broadly, interest in LLM simulation is rapidly growing, and the possibility of using LLMs to simulate human behaviors has far-reaching applications in fields like education (Markel et al., 2023), product design (Park et al., 2022(Park et al., , 2023a)), psychology (Binz and Schulz, 2023), healthcare (Weizenbaum, 1966;Bassett, 2019), skill training (Hollan et al., 1984;Jones et al., 1999), and law (Hamilton, 2023).These simulations are a sort of digital compost-any new insight into human be-</p>
<p>The CoMPosT Framework</p>
<p>Context Where and when does the simulated scenario occur?Model What LLM is used?Persona Whose opinion/action is simulated?Topic What is the simulation about?</p>
<p>Table 1: Dimensions of the CoMPosT framework.We use these dimensions to characterize LLM simulations and measure their susceptibility to caricature.</p>
<p>havior that they provide draws upon the organic material (human data) used to train LLMs.Such applications currently have little to no mechanisms for comprehensive evaluation or careful deployment.Evaluation of such simulations has been limited to either (1) replicating existing results or (2) assessing believability.Both paradigms have drawbacks: (1) Replication limits us to only reproducing already-known behavior, and does not support the validation or evaluation of any simulation behaviors beyond those highly correlated with existing results from human studies.Also, existing results are typically quantified as categorical distributions across multiple-choice answers, so there is no way to directly evaluate open-ended generations, and such results may have been "memorized" from the LLMs' training data (Lewis et al., 2021;Elangovan et al., 2021).Thus, replication does not facilitate new insight into human behavior.Furthermore, while (2) Believability is useful in certain settings, such as entertainment (Bates et al., 1994), it is susceptible to the biases and fallacies of human judgment: psychology literature shows that people are more likely to believe stereotypes about groups with which they have less personal experience (Plous, 2003;Bar-Tal et al., 2013), and beliefs are easily influenced (Blair et al., 2001;Jussim et al., 2016).Recent work has also demonstrated that human judgment is insufficient for assessing AI (Schneider et al., 2020;Peng et al., 2022;Vodrahalli et al., 2022;Veselovsky et al., 2023).A6.</p>
<p>Toward clearer documentation of this emerging line of work, we first present a descriptive framework that taxonomizes LLM simulations using four dimensions: Context, Model, Persona, and Topic (CoMPosT) (Table 1).Our framework facilitates comparison across existing work on LLM simulations (Figure 1).</p>
<p>Next, we introduce a new evaluation metric that addresses growing concerns of modal responses and essentializing narratives in LLM outputs (Santurkar et al., 2023;Cheng et al., 2023b;Shumailov et al., 2023).Our metric focuses on a simulation's susceptibility to caricature: an exaggerated narrative of the persona (the demographic that we aim to simulate) rather than a meaningful response to the topic (Figure 2).Caricatured simulations are concerning because they a) fail to capture the real nuances of human behavior, thus limiting the usefulness of simulations and b) perpetuate misleading descriptions, stereotypes, and essentializing narratives about demographic groups.We define caricature using two criteria: individuation and exaggeration (Section 3).To measure individuation, we assess whether outputs from the given simulation can be differentiated from the default response for the topic.To measure exaggeration, we use a "contextualized semantic axis" whose two poles are the defining characteristics of the persona and topic dimensions respectively.</p>
<p>We evaluate the level of caricature in scenarios from existing work on LLM simulations.We find that for GPT-4, simulations of certain demographics (political and marginalized race/ethnicity groups) and topics (general, uncontroversial) are more susceptible to caricature.Our main contributions are: (1) CoMPosT, a framework for characterizing the dimensions of LLM simulations of human behavior (Section 2), (2) a novel method that relies on the persona and topic dimensions of CoMPosT to measure simulations' susceptibility to  For the simulation of a nonbinary person's response, the generations are focused on identity-related issues, while the simulation of a person's response is topical.The former constructs a homogenous narrative that defines nonbinary people only by LGBTQ+ activism.We provide more qualitative examples in Appendix A.</p>
<p>caricatures (Section 4), and (3) experiments in different contexts (Section 5) toward an analysis of the dimensions that are most susceptible to caricature (Section 6).We conclude with actionable recommendations and considerations for those interested in LLM simulation (Section 7).1</p>
<p>CoMPosT: Taxonomizing Simulations</p>
<p>We introduce CoMPosT, a descriptive framework with four dimensions to characterize LLM simulations: Context, Model, Persona, and Topic.Inspired by existing descriptive frameworks for AI fairness (Tubella et al., 2023), our framework provides a shared language to understand and articulate similarities and differences across LLM simulations.Context, persona, and topic are specified in the prompt, while model is determined externally.We map existing work on LLM simulations using these dimensions (Figure 1 and Table A4).</p>
<p>Context</p>
<p>The output from a simulation is necessarily affected by the context of where and when the imagined situation takes place.For instance, a formal interview response varies drastically from a user's comment on Twitter or Reddit.The context includes relevant structural factors and embeds information about the norms of the situation.Each context has its own unique set of norms, which may be explicit, as in the case of online communities with written-down rules, or implicit (Chandrasekharan et al., 2018;Ziems et al., 2023).Context also includes the phrasing of the prompt itself, which affects the output-LLMs are notoriously sensitive to prompt engineering (Zamfirescu-Pereira et al., 2023).The desired granularity of outcome also arises from the phrasing of the prompt and thus is embedded in the context: a simulation scenario may be worded to ask for a choice between binaryor multiple-choice options (such as in many social science experiments and public opinion polls)</p>
<p>or for an open-ended output.Previous work on evaluating simulations has largely focused on using LLMs to reproduce scenarios in which humans are asked to choose among a fixed number of options without specifying the context, as it is more challenging to evaluate the quality of open-ended responses.We bridge this gap by offering a metric for the latter.</p>
<p>Model</p>
<p>The LLM used to produce the simulation affects the quality and other characteristics of a simulation.Differences may arise from variations in models' training data and processes, including instruction-tuning, fine-tuning, and/or value alignment efforts (Solaiman and Dennison, 2021;Ouyang et al., 2022;Bakker et al., 2022).</p>
<p>Persona</p>
<p>The persona refers to the entity whose opinions/actions the simulation aims to study and replicate.This persona may include attributes that are relatively static (e.g., race/ethnicity), slowly change over time (e.g., age), or temporary and ofthe-moment (e.g., emotional status) (Yang, 2019).It may also refer to a specific individual.</p>
<p>Topic</p>
<p>The topic of the simulation may be a particular subject of discussion, question, or other event to which a response is desired.Topics vary in specificity, from very general (such as a single word that captures a broad conversation category) to very specific (such as a specific situational question in a psychology experiment).</p>
<p>These four dimensions capture a wide range of possible simulation scenarios, many of which have not yet been well-explored.Across existing work, we find that researchers typically use a state-of-theart model and choose a particular context while varying the more salient dimensions of persona and topic.We denote the simulation scenario as S p,t,c , as it is associated with a prompt containing persona p, topic t, and context c (Table A4).Our evaluation methodology and results uses these dimensions of CoMPosT to understand how different simulations may result in caricatures.Specifically, we explore how the relationship between the dimensions of persona and topic help characterize the extent of caricature in simulations.</p>
<p>3 Background: Caricature</p>
<p>Definition of Caricature</p>
<p>Building upon Lynch (1927)'s discussion of how caricatures are misrepresentations that have some sense of truth to the subject by reflecting "salient peculiarities," Perkins (1975) define caricature as "a symbol that exaggerates measurements relative to individuating norms."In so doing, Perkins identifies two key characteristics of caricature: exaggeration and individuation.A caricature is a depiction that not only exaggerates particular features of the subject but also exaggerates in a manner that meaningfully differentiates the subject from others.The exaggeration is done in such a way that it individuates by remaining faithful to the properties that distinguish the subject from others (thus, a complete distortion is not a caricature).This inspires our definition of caricature in the LLM simulation context: given that a subject has some defining characteristics, a caricature exaggerates these characteristics in a way that amplifies the ability to identify (i.e., individuate) the subject from the caricature.In CoMPosT terms, a simulation's level of caricature is the degree to which it exaggerates the individuating characteristics that are emblematic of the persona beyond a meaningful, topical response to the scenario.</p>
<p>Note that in some cases, it may be acceptable for the persona to influence the simulation, i.e., individuation alone does not entail caricature.For example, opinions on some topics differ greatly based on demographic.A caricature occurs when the simulation both individuates and exaggerates the defining characteristics of the imagined generic responses of that persona.Previous work has documented how such imagined personas reflect stereotypes, both inside and outside the LLM context (Marsden and Haag, 2016;Cheng et al., 2023b).Thus, caricatures not only fail to capture the diversity of human behavior but also may rely on stereotypes.</p>
<p>Implications of Caricature</p>
<p>Stereotypes We rely on psychology literature that broadly defines stereotypes as generalizations about the characteristics of a social category, such as associating a social category with a particular role or trait (Heilman, 2001;Fiske et al., 2002;Cao et al., 2022;Kambhatla et al., 2022).The normative value of stereotypes is context-dependent; for instance, stereotypes can help foster a sense of authenticity (Marsden and Haag, 2016), while even seemingly-positive stereotypes can have harmful implications (Fiske et al., 2002;Czopp et al., 2015).</p>
<p>Stereotypes and caricature, while closely related, are distinct in that a caricature may be a specific depiction of a stereotype: scholars have documented caricatures of stereotypes in various domains and how they facilitate misogyny, racism, and other forms of oppression (Slavney, 1984;Brown, 2010;Gottschalk and Greenberg, 2011;Takayama, 2017;Bow, 2019).Such caricatures have historically been used in literature and media to justify slavery, imperialism, and war (Demm, 1993;Kriz, 2008).But even when caricatures do not contain stereotypes, they have concerning implications of homogenous narratives.</p>
<p>Misleading Homogeneity Caricatures foster homogenous narratives that do not reflect the full diversity of the personas they aim to simulate, which limits the utility of the simulation.This concern builds upon previous work: Grudin (2006) discuss how personas can result in systematic errors in understanding human behavior, Cheng et al. (2023b) characterize the harms of LLMs reflecting essentializing narratives about demographic groups, and Santurkar et al. (2023) show that certain instructiontuned LLMs tend to generate modal responses.Others have explored the linguistic nuances within complex social categories and the ramifications of ignoring heterogeneity within social groups (Bamman et al., 2014;Hanna et al., 2020;Cheng et al., 2023a).</p>
<p>These harms of caricature are also articulated by feminist scholars who have discussed how "women in the Two-Thirds World...are constructed as one-dimensional, oppressed caricatures without an understanding of their real experiences, agency, and struggles" (Mohanty, 1988;Aneja, 1993;Kumar et al., 2019).This literature reveals that even when the caricatures are not overtly negative, such one-dimensional depictions are still damaging and harmful.Overlooking diversity within demographic groups has been connected to real-world harms including misprediction and medical misdiagnosis (McCracken et al., 2007;Borrell et al., 2021;Read et al., 2021;Wang et al., 2022).</p>
<p>Caricature Detection Method</p>
<p>The two key aspects of caricature are individuation and exaggeration.To measure the amount of caricature in a given simulation S p,t,c , our method has three steps, each of which rely on the persona and topic dimensions of CoMPosT (Figure 3): (1) defining defaults, (2) measuring individuation, and (3) measuring exaggeration.Note that this framework is sequential, as ( 3) is only necessary if the simulations can be individuated.Otherwise, we can halt after step (2) since individuation is a necessary criterion for caricature.</p>
<p>Defining Defaults</p>
<p>A simulation S p,t,c is a caricature if it has more of the defining characteristics associated with the persona p and less of the defining characteristics associated with the topic t.We first identify these defining characteristics using the default-persona simulation S <em>,t,c (simulation that does not mention any specific persona) and the default-topic simulation S p,</em>,c (simulation that does not mention any specific topic).Note that this default does not reflect a universal default but rather a modeland context-specific default: previous work has shown that LLMs implicitly default to a particular set of perspectives (Western, white, masculine, etc.) (Santy et al., 2023).We use these defaults as a comparison point for caricature to isolate the defining characteristics of particular dimensions.</p>
<p>For the default-persona simulation, we use a prompt where in lieu of a specific persona, we use an unmarked default term like "person" or "user."Thus, the outputs reflect the topic and context rather than any particular persona.(Again, such words are not true defaults and are inextricably tied to societal norms: in English, the word "person" is often conflated with "man," a phenomenon also present in web data and language models (Bailey et al., 2022;Wolfe and Caliskan, 2022).)</p>
<p>For the default-topic simulation, we use a prompt where no topic is specified.Thus, the outputs from these prompts reflect the particular persona rather than a response to the topic.It is well-documented that this type of prompt results in outputs that reflect stereotypes, both when asked to humans and to LLMs (Kambhatla et al., 2022;Cheng et al., 2023b).Note that even if we expect the output to change based on the persona, the response should still be distinct and not defined by the same distinguishing characteristics as the default-topic simulation for a given persona.</p>
<p>Measuring Individuation</p>
<p>We operationalize the desiderata of individuation using differentiability from default: we examine whether the given simulation S p,t,c is differentiable from the default-persona simulation S <em>,t,c .If not, then S p,t,c cannot be a caricature.We use a binary classifier (specifically, a random forest classifier implemented using scikit-learn) to differentiate between outputs from the target simulation of interest S p,t,c and those from the defaulttopic simulation S </em>,t,c based on the outputs' contextualized embeddings.We compute contextualized embeddings using the pre-trained Sentence-BERT model all-mpnet-base-v2 (Reimers and Gurevych, 2019).To create the training and test datasets, we use a stratified 80/20 split on S p,t,c and S _,t,c to preserve the balance between the classes.We report the accuracy2 of the classifier on the test dataset as our measure of individuation.Note that this measure is agnostic to the particular choice of differentiator and contextualized embedding model, and we show results with other choices in Appendix B.</p>
<p>This score alone is necessary but insufficient for identifying caricature, as a caricature must also exaggerate, which we measure next.</p>
<p>Measuring Exaggeration</p>
<p>We define caricature as text having more of (and exaggerating) the defining characteristics associated with persona and less of those associated with topic.Unlike individuation, exaggeration requires a more nuanced measure than differentiation from the default-topic simulation: if an output mentions the topic frequently, it can easily be differentiated, but it may still be a caricature.(Note that it is acceptable for a simulation to have many topicrelated words.)Instead, we measure the extent to which the defining characteristics of the persona are exaggerated in the target simulation via persona-topic semantic axes.</p>
<p>Specifically, we construct contextualized semantic axes, a method introduced by Lucy et al. (2022), to capture whether S p,t,c is more similar to the defining characteristics of the persona p or the topic t.Our semantic axes have two poles, P p and P t , reflecting the persona p and the topic t.To construct the set of seed words, we use the Fightin' Words method (Monroe et al., 2008) to identify the words that statistically distinguish S p,<em>,c from S </em>,t,c .We first compute the weighted log-odds ratios of the words between S <em>,t,c vs. S p,</em>,c .To control for variance in words' frequencies, we use the following prior distribution: other texts where the personas/topics are either p/t respectively or _ (i.e., the default).Then, we take the words that are statistically significant (have z-score &gt; 1.96) as the sets of seed words W p and W t for the corresponding poles P p and P t (Table A3).</p>
<p>We represent each word w  W as the mean of the contextualized embeddings of the sentences containing that word w across S <em>,t,c and S p,</em>,c .We define the semantic axis
V p,t = 1 k k i=1 p i  1 m m j=1 t j ,(1)
where p i /t j is a word in W p /W t respectively, i.e., we represent P p /P t as the mean of the embeddings of W p /W t respectively.This subtraction-based axis allows for scaling relative to how closelyrelated the topic and persona are.</p>
<p>To evaluate exaggeration, we compute the average cosine similarity of a given simulation's contextualized embedding to this axis:
cos(S p,t,c , V p,t ) = n i=1 cos(S i p,t,c , V p,t ) n ,(2)
where S i p,t,c refers to individual outputs, i.e. i = 1, 2, ..., n for n outputs from the same simulation S p,t,c .</p>
<p>The final value we report as the measure of exaggeration is this value normalized to lie between 0 and 1 (by scaling it relative to the cosine similarity of the default-persona and default-topic simulations with the axis):
cos(S p,t,c , V p,t )  cos(S <em>,t,c , V p,t ) cos(S p,</em>,c , V p,t )  cos(S _,t,c , V p,t )
.</p>
<p>(3)  We perform internal and external validation of these persona-topic semantic axes (Appendix C).</p>
<p>Experiments</p>
<p>We use our method to evaluate simulations in various contexts that have been used in previous work to demonstrate the capabilities of LLM simulation (Park et al., 2022;Santurkar et al., 2023;Jiang et al., 2022).Our experiments are focused on the two most widely-used contexts of (1) an online forum setting and (2) a question-answering interview setting.We also evaluate the Twitter context as an additional robustness study (Appendix G).These choices are based on our survey of the literature on LLM simulations (Figure 1, Table A6): Among 15 papers in this area, we found that six use the context of a virtual community or society and four rely on an open-ended interview or survey context.The remaining five are in various question-answering contexts, so conclusions about the (2) can also provide insight into these types of simulations.We use the state-of-the-art GPT-4 model for all experiments (OpenAI, 2023); like others (Dubois et al., 2023), we find that open-source LLMs and older models are worse at simulation tasks, yielding outputs that are unrealistic and significantly lower in quality.For each simulation setting S p,t,c , we generate 100 outputs and average all results across them.See Appendix E for a power analysis of this sample size.The full details for each setting, including topic lists, persona lists, and defaulttopic/persona prompts, are in Appendix D.</p>
<p>Online Forum</p>
<p>Park et al. ( 2022) demonstrate the believability of LLM simulations of users in online forums.Following their prompting format, we use the prompt: "A [persona] posted the following comment on [topic] to an online forum:"3 We explore such simulations using 15 different personas (5 race/ethnicities, 3 genders, 3 political ideologies, 3 ages, and the neutral "person") and 30 pairs of topics.</p>
<p>For topics, we aim to cover a wide range of common topics that vary in (1) specificity (e.g., overcoming fear of driving is much more specific than cars) and (2) level of controversy (e.g., abortion is much more controversial than health).To cover both dimensions, we use topics from Wiki-How, which is a knowledge base with a wide range of topics (Koupaee and Wang, 2018), and from ProCon.org, which is a website that lists popular controversial topics and has been used in the NLP context to study stance and argumentation (Misra et al., 2016;Hosseinia et al., 2020).We use the first 15 categories from WikiHow's "popular categories" webpage and randomly sample an associated specific "how-to" for each category.For ProCon.org, we randomly sample 15 topics from ProCon.org's "debate topics" webpage.Each topic on the page is listed in both more general and more specific wording, e.g., abortion and should abortion be legal?Thus, for each sampled subject, we use both the general and specific versions as topics.</p>
<p>Interview</p>
<p>Various previous works simulate opinions of different demographics using an interview-style prompt (Argyle et al., 2023;Santurkar et al., 2023;Hmlinen et al., 2023, inter alia).We reproduce the public opinion survey simulation context from Santurkar et al. (2023), using the prompt:</p>
<p>"Below you will be asked to provide a short description of your identity and then answer some questions.Description: I am [persona].</p>
<p>Question: [topic]</p>
<p>Answer:" For topics, we randomly sample 30 questions from the Pew Research's American Trends Panel survey questions that Santurkar et al. (2023) identify as "most contentious" in their OpinionQA dataset.We convert the multiple-choice questions into open-ended ones by removing the multiplechoice answer options.For personas, we use the same 15 personas as described in Section 5.1.</p>
<p>Results and Discussion</p>
<p>We apply our caricature detection method to evaluate the simulations produced in these different contexts.We further operationalize the CoM-PosT framework by aggregating the individuation and exaggeration scores across the dimensions of topic and persona.This enables us to analyze the topics and personas that lead to the most caricatures across different contexts.For instance, to examine what personas lead to the most caricatures in a particular context, we compute the mean score for each persona across all simulations (varying in topic) for that persona and then compare these scores.We also report results from additional experiments that explore the influence of the context dimension in Appendix F.</p>
<p>Simulations of all personas can be individuated from the default-persona</p>
<p>The mean individuation score and 95% confidence interval for every persona is &gt; 0.5, i.e., each persona can be meaningfully individuated from the default-persona at a rate better than random chance.Mean individuation scores across the online forum and interview contexts are in Figure 4. We see that the woman and man personas have lower mean scores in the online forum context, while in the interview context, the mean score is &gt; 0.95 for every persona-thus, this score is not informative for comparing caricature across personas.The difference in score between contexts arises from differences in the sample distributions: compared to the online forum context, the interview context simulations have lower variability, so they are easier to individuate.</p>
<p>Exaggeration scores reveal the personas and topics most susceptible to caricature</p>
<p>Next, we examine the exaggeration scores, i.e., the similarities across the simulations to their corresponding persona-topic axes (Figure 5).Since almost all the simulations are able to be individuated, we use the exaggeration score as a proxy for caricature, i.e., a given simulation is highly susceptible to caricature if it has a high exaggeration score.</p>
<p>6.2.1 Caricature : Topic specificity </p>
<p>In the online forum context, among the topics, we find that the more general topics resulted in higher exaggeration scores, and thus higher rates of caricature, while the more specific topics had much lower rates of caricature (Figure 5).In particular, the general, uncontroversial topics have highest exaggeration scores.The top five topics with the highest mean rates of caricature are Health, Philosophy and Religion, Education and Communications, Relationships, and Finance and Business.</p>
<p>To explore this pattern further, we experiment on a fine-grained range of topic specificity: For the topic with highest exaggeration score (Health), we generate simulations for a range of related topics with 5 levels of specificity (Appendix D.1.1).We find that this pattern holds: the level of caricature decreases as the specificity of the topic increases (Figure A2).We find no correlation between topic length and caricature otherwise.</p>
<p>In the interview context, the exaggeration scores are broadly comparable to the scores for the more specific topics in the online forum context (Figure A5).After controlling for context (Appendix F), we hypothesize that this is because they are similar in specificity.We also observe this inverse relationship between topic specificity and susceptibility to caricature in the Twitter context (Appendix G).The standard error for each point is &lt; 0.02 and thus not visible on the plot.Based on a classifier between the default-persona and the target persona, many of the personas are easy to individuate (have high accuracy scores).The only personas that are slightly challenging to differentiate are the gender groups woman and man in the online forum context (blue circles).All personas have mean score &gt; 0.95 in the interview context (green stars).We measure exaggeration as normalized cosine similarity to the persona-topic axis.The more general topics (purple, larger marker) have higher rates of exaggeration, and thus caricature, than the specific topics (orange, smaller marker).The uncontroversial (WikiHow, squares) topics have higher rates of caricature than the controversial (ProCon.org,stars) topics.Personas related to political leanings, race/ethnicity, and nonbinary gender broadly have the highest rates of caricature.</p>
<p>Caricature</p>
<p>the perspectives of liberal, white, and younger populations, while the perspectives of non-binary people are poorly represented by these systems (Santy et al., 2023;Santurkar et al., 2023).It is surprising that although Asian and woman reflect marginalized groups, they have relatively low rates of caricature; this may further reflect implicit defaults in LLM outputs.Certain personas and topics have more nuanced relationships, e.g., conservative and liberal personas in the online forum context have the widest gap between the scores of the uncontroversial and controversial topics.The Twitter context also reveals that, beyond analyzing the persona and topic dimensions separately, some personatopic combinations are particularly susceptible to caricature (Figure A4).</p>
<p>Stereotypes</p>
<p>We note that our framework is not an exhaustive test for bias or failure modes, but rather a measure for one way in which simulations may fail.Thus, simulations that seem caricature-free may still contain stereotypes, as our method captures how much a simulation exaggerates the persona in a particular setting, which is not an all-encompassing catalog of stereotypes.We see this in the simulated "woman" responses in the online forum context: The defaulttopic generations contain specific stereotypes, e.g., "I recently purchased a new vacuum cleaner and I have to say, I am extremely satisfied with its performance!It has made my cleaning routine so much easier and efficient."This response reflects gender bias in that it focuses on cleaning and other domestic tasks, while simulations of other personas do not.Although the simulated "women" responses contain various other gender stereotypes/biases beyond association with domestic tasks, they have low caricature scores.</p>
<p>Recommendations</p>
<p>We conclude with several recommendations for those interested in LLM simulations.</p>
<p>Mitigating Caricature Researchers should use our method to test their simulation in their particular context and critically examine whether a simulation helps illuminate the desired phenomenon.</p>
<p>While the relationship between topic, persona, and context in causing caricature is nuanced, we generally encourage researchers and practitioners to use more specific topics to mitigate caricature.Any attempt to simulate a group-especially a politicized or marginalized group-ought to be done with particular care and attention.</p>
<p>Documenting Positionality Research on LLM simulations face the well-documented challenges of human-focused, value-laden interdisciplinary work (Marsden and Haag, 2016).For instance, researchers themselves may be subject to the outgroup homogeneity effect, i.e., the tendency to rely on stereotypes and generalizations for groups to which they do not belong (Plous, 2003).Following work on model, dataset, and system documentation (Bender and Friedman, 2018;Mitchell et al., 2019;Gebru et al., 2021;Adkins et al., 2022), we call for increased transparency and documentation for simulations, including the dimensions of CoM-PosT and less-visible aspects such as the creators' positions, motivations, and process.Drawing upon HCI work on reflexivity and positionality (Keyes et al., 2020;Liang et al., 2021;Bowman et al., 2023), we encourage researchers to report how their identity and perspectives may influence their work.</p>
<p>Understanding Difference Although some applications of LLM simulations focus on aggregates rather than individuals, it is critical to understand the landscape of individuals from which these groupings arise, and it is often necessary to use more subtle forms of aggregation.Otherwise, minority opinions and subgroup disparities may be overlooked (Herring and Paolillo, 2006;Hanna et al., 2020;Wang et al., 2022).Takayama (2017) suggests countering caricature "by providing fully contextualized, balanced, and nuanced description," and in HCI, Marsden and Prbster (2019) explore how to explicitly capture users' multidimensional identities.Drawing inspiration from these works, one future direction is injecting variation and using mul-tifaceted personas into simulations.Our goal in avoiding caricature is not to erase difference, but rather the opposite: capturing relevant differences that reflect meaningful insights rather than shallow, misleading generalizations.</p>
<p>Positionality</p>
<p>The perspectives introduced in this paper have undeniably been shaped and influenced by our positionality.Myra Cheng identifies as a Chinese-American woman.The authors are a PhD student, postdoctoral scholar, and professor respectively in the Stanford University Computer Science department, which is predominantly male and white/Asian.</p>
<p>Ethical Considerations</p>
<p>From impersonation to pornography, LLM simulations can have deeply problematic applications.We are strongly against such applications, and we also do not condone research and development that may enable such applications by bad actors without guardrails in place.Our CoMPosT framework offers a shared language to meaningfully critique such work.For instance, one might imagine coming to a consensus to avoid simulating certain topics, personas, and contexts entirely.Introducing a method to measure caricature offers a way to make known this concerning limitation.Lack of caricature based on our measure does not mean that a simulation is necessarily acceptable or high quality (see Section 10).</p>
<p>Implicit Defaults</p>
<p>The least caricatured personas are also those that others have found to be implicit defaults in LLMs.Implicit defaults in LLMs may shift depending on the prompt, context, etc., as well as including aspects of identity and social factors that may be invisible or underrepresented in existing empirical data and surveys4 .Given the increasing proliferation of generated content and a limited quantity of human-written text (Xue et al., 2023;Shumailov et al., 2023), caricatures become only more relevant with the prospect of future LLMs that are trained on generated data: what will their defaults be, and how might they further amplify caricature?</p>
<p>Limitations</p>
<p>While we fill a critical gap since there is no existing work on systematically detecting stereotypes/caricatures in simulations or evaluating simulations in this manner at all, our measure is limited in scope: it is not a comprehensive evaluation of the quality of a simulation.We quantify susceptibility to caricature, which is a particular failure case of a simulation.Our method may yield false positives (simulations that seem acceptable and caricaturefree based on our method but have other problems).</p>
<p>Avoiding caricature is a necessary but insufficient criterion for simulation quality; our metric should be used in tandem with other evaluations.As a pilot study for a recently-emerging direction of work, we hope to lay the groundwork for a more comprehensive evaluation of simulations in the future, perhaps in tandem with human evaluation.</p>
<p>As we provide a first step toward characterizing and evaluating LLM simulations, an area which currently lacks a shared language for discussion and comparison, we focus only on simulations of personas that reflect subpopulations such as social groups and on one-round response formats.However, our framework easily extends to other more complex or open-ended settings.For instance, for a multi-round simulation, one could apply our framework by using the full text of the simulation across the rounds.Depending on the length and structure of the simulation, one could also split the simulation into multiple parts and characterize each part's propensity to caricature.</p>
<p>Also, note that in the semantic axes, P p does not necessarily reflect a universal notion of the model's representation or description of that particular demographic group.It merely characterizes the words that distinguish a simulation of that persona, given the particular context c, such as an opinion from that demographic's perspective.This enables us to measure if and when a simulation is dominated by language that is a caricature of the persona, but our work is not a comprehensive evaluation of stereotypes or representations of demographic groups.</p>
<p>A Examples of Caricatures in Simulation</p>
<p>Which our paper focuses on how to discover problematic patterns of caricature at the larger scale used in LLM simulations, these patterns are clearly evident on an individual example level.For example, for the "nonbinary person" persona in the online forum setting, which has some of the highest exaggeration scores, some randomly-sampled default-topic responses are: "As a nonbinary individual, I often feel like I don't fit into the traditional boxes society has created for gender.This can be difficult when navigating spaces that are dominated by cisnormativity.However, I have found solace and empowerment in connecting with other nonbinary people and advocating for inclusivity and understanding.It is important to remember that gender is a social construct, and it is up to us to define our own identities and push for a more accepting world.""As a nonbinary individual, I often find myself in situations where people are not aware of or don't understand my gender identity.While it can be frustrating at times, I choose to take it as an opportunity to educate others and promote a more inclusive society.It's important to recognize and respect everyone's experiences and identities, even if they differ from our own."</p>
<p>These descriptions focus solely on general statements regarding gender identity and inclusion.We find the same phenomenon in simulations of a nonbinary person's perspective on other topics.For the topic "how to read guitar music," some randomlysampled simulated non-binary person's responses are:</p>
<p>"Hey everyone!I just wanted to share my experience as a nonbinary individual learning how to read guitar music.It's been a really empowering journey for me, as it allows me to express myself in a way that transcends the limits of gender.Moreover, I've found that the guitar community is incredibly accepting and supportive, which has been a huge help in overcoming some of the challenges that I've faced as a nonbinary person.Keep rocking out, friends!" "As a nonbinary guitarist, I wanted to share some advice and encouragement for others who may be navigating the world of guitar music.Remember that music is a universal language and it does not adhere to any specific gender norms or societal expectations.Allow yourself the freedom to explore different styles, techniques, and genres of guitar music that resonate with you.Embrace your unique perspective and use it to create music that reflects your authentic self.And most importantly, don't forget to have fun and enjoy the journey of learning and growing as a musician."</p>
<p>Rather than actually discussing "how to read guitar music," these outputs disproportionately focus on generic statements about gender, authenticity, and inclusion (bolded).These responses exaggerate this particular imagined aspect of a nonbinary person's perspective.Similarly, for the topic "cars and vehicles," some randomly-sampled simulated non-binary person's responses are: "I just wanted to share my experience as a nonbinary individual when it comes to cars and other vehicles.For me, it's important that my mode of transportation reflects my identity and personal aesthetics.I've found that customizing my vehicle with various accessories, paint jobs, and even custom license plates has been a fun and empowering way to express myself.I also appreciate a vehicle that has a gender-neutral design, as that aligns with my identity.I've noticed that more and more auto manufacturers are starting to create designs that feel more inclusive and neutral, which is great for people like me.Ultimately, the vehicle you choose should be a reflection of your personality and preferences, no matter your gender identity.Happy driving, everyone!""Hey everyone!As a nonbinary individual, I wanted to share my perspective on car preferences Topic Output for "Man" persona Default "I recently purchased a new smartphone and I'm really impressed with its features and performance.The camera quality is amazing, and the battery life lasts all day without any issues.The interface is easy to use, and the overall design is sleek and modern.I would highly recommend this phone to anyone in the market for a new device.""I recently purchased a new laptop and I'm absolutely loving it so far.The performance is top-notch, the design is sleek, and the battery life has been really impressive.The only issue I've come across is a minor software bug, but that was easily fixed with an update.Overall, I highly recommend this model to anyone in the market for a reliable and high-performing laptop."How to read guitar music "I recently started learning how to play the guitar and I found this guide on How to Read Guitar Music extremely helpful.The step-by-step explanation, along with images and examples, made it easy to understand and follow.I'm now more confident in reading sheet music and applying it to my guitar practice.Highly recommend this guide for any beginner guitarist looking to learn how to read music.Kudos to the author for making it so simple and easy to follow!" "I recently started learning how to read guitar music and found this article very helpful.It provided clear explanations and easy-to-follow steps that have greatly improved my understanding of how to read sheet music for the guitar.The tips and examples given were a great reference for me as a beginner, and I feel much more confident in my ability now.Thanks for sharing this informative guide!" Cars and vehicles "I recently purchased a 2020 Ford Mustang GT and I couldn't be happier with my decision.The drive is smooth, the car is powerful, and the interior is incredibly comfortable and stylish.I have always loved the look of the Ford Mustang and this new model definitely lives up to my expectations.The sound system is fantastic as well.I highly recommend this car to anyone considering a sporty, powerful, and fun vehicle.""I recently bought a 2015 Honda Civic and I'm absolutely loving it!The fuel efficiency is amazing and the ride is smooth.I've also noticed a significant improvement in the handling compared to my old car.The interior is quite spacious and comfortable too.My only complaint so far is the touchscreen entertainment system -it's a bit slow and not very user-friendly.But overall, I highly recommend the 2015 Civic to anyone in the market for a reliable, fuel-efficient, and enjoyable vehicle!"In these outputs, the bolded phrases also present unrealistic caricatures that exaggerate specific aspects of the persona rather than reflecting the full range of possibilities of what a non-binary person might say about these topics.In stark contrast, for the "man" persona, the default-topic responses are not identity-related, and responses for particular topics are much more topical and do not exaggerate any aspect of the "man" persona (Table A1).</p>
<p>B Robustness of Individuation Measure</p>
<p>The notinon of measuring individuation is broadly agnostic to the choice of differentiator and model used to embed the texts.In Figure A1, we show individuation results from (a) using the unsupervised V-measure to measure differentiation and (b) using the all-distilroberta-v1 model to compute embeddings instead of all-mpnet-base-v2 (all-distilroberta-v1 is the next highest-performing model for general-use purposes).For (a), we first use K-Means to cluster the embeddings into two clusters, and then report the v-score.We find that the scores are overall slightly lower than using a binary classifier, which makes sense since the unsupervised method is less powerful in this context, but most of the personas can still be differentiated at a rate higher than random chance.We use the supervised binary classifier in the main results since the purpose of this metric is to reflect a reader's capability to differentiate between the two categories, which is more accurately reflected by the more powerful classifier.The broad patterns remain the same as the findings reported in the paper: all the personas can be individuated in the Interview context, and in the Online Forum context, marginalized personas are the most easily individuated, while the personas of man and woman are challenging to individuate at all.</p>
<p>C Internal and External Validation of Semantic Axes</p>
<p>For internal validation, following  that does not include w.Then, we measure the cosine similarity of w's contextualized embedding to P  1 and P 2 .We found that the former is larger for all of the persona-topic semantic axes that we constructed.</p>
<p>For external validation, we manually inspect the sets of seed words and find that for each axis, it is easy to differentiate which set is associated with the corresponding persona versus topic.We find that for race/ethnicity and gender personas, the sets of top words reflect similar stereotypes as reported by Cheng et al. (2023b).See Table A2  The general prompt for simulation with persona p and topic t is: 'A(n) p posted the following comment on t to an online forum."For persona p, the default-topic prompt is: "A(n) p posted the following comment to an online forum."For topic t, the default-persona prompt is: "A person posted the following comment on t to an online forum."</p>
<p>D.1.1 Fine-Grained Specificity Experiment</p>
<p>The topics for the fine-grained specificity experiment are:</p>
<p> Specificity Level 1: health The Level 1 topic is the topic with highest rate of caricature.The Level 5 topics are from the top search results for "health" in the subreddit community AskReddit.The authors constructed the intermediate specificity levels by interpolating specificity between these.The resulting exaggeration scores are in Figure A2.We see that specificity and exaggeration are negatively correlated.</p>
<p>D.2 Interview Context</p>
<p>The full list of Pew opinion survey questions that we use as topics in the interview setting are in topics/pewtopics.txt in the supplementary material.The default-topic and default-persona prompts are in Table A5.</p>
<p>E Power Analysis</p>
<p>To justify 100 examples per simulation setting, we provide the following power analysis.Note that for individuation, a simulation that cannot be individuated would have score 0.5 (random chance).Across personas and topics, the lowest mean score was 0.65, and the highest standard deviation was 0.2.A power analysis using a t-test for two independent samples reveals that the necessary sample size is 28 given the effect size (0.65  0.5)/0.2= 0.75, alpha = 0.5, and desired power = 0.8.Similarly, for exaggeration, a simulation with no exaggeration of a persona would have score 0. We found that simulations with less-specific topics and personas of many political ideology, race, and marginalized groups result in high exaggeration scores.Among these simulations, the lowest mean score was 0.23, and the highest standard deviation was 0.37.Again using a power analysis, the necessary sample size is 41 using effect size (0.23  0)/0.37 = 0.62, alpha = 0.5, and desired power = 0.8.Thus, our choice of obtaining 100 samples per simulation is more than sufficient to achieve the desired power for both the individuation and exaggeration metrics.</p>
<p>F Influence of the Context Dimension</p>
<p>In this section, we explore the effect of the context dimension.To verify that the trends we observe are not due to the difference in context alone, we also experiment with switching the contexts and topics, i.e., 1) simulations with the online forum topics in the interview context 2) simulations with the interview topics in the online forum setting.Figure A3 reveals that the trends in caricature rates persist for the same topics in different contexts rather than being based on context alone.Interestingly, the exaggeration scores are overall slightly higher in these switched contexts than in the original contexts.This may be impacted by memorization dynamics (Elangovan et al., 2021;Tirumala et al., 2022;Carlini et al., 2023), a relationship to explore in future work (e.g., are memorized topics less susceptible to caricature?).</p>
<p>G Twitter Context</p>
<p>We additionally conduct and analyze experiments in the Twitter context introduced by Jiang et al. (2022), and we find similar trends that corroborate the results of the main paper.</p>
<p>G.1 Experimental Details</p>
<p>Figure 1 :
1
Figure 1: Mapping Existing Work Using CoMPosT.Existing work on LLM simulations can be compared using our framework.MC and O denote multiple-choice and open-response respectively.More examples are in TableA6.</p>
<p>Simulation Topic: Computers and Electronics Generated nonbinary person responses: "I'm interested in getting some recommendations for any cool devices that might particularly appeal to nonbinary individuals or help increase our visibility and representation." "As a nonbinary individual, I want to create an inclusive and comfortable gaming/streaming space for myself, as well as others in the LGBTQ+ community.""I recently upgraded my desktop PC with a new graphics card and SSD, and I'm really impressed with the performance boost I got from these upgrades.""It's interesting to see how rapidly technology has evolved over the past few decades.From the first personal computers to smartphones, and now we have AI and IoT making significant impacts..." Generated person responses:</p>
<p>Figure 2 :
2
Figure 2: Examples of GPT-4 generated responses for simulations with the topic Computers and Electronics.For the simulation of a nonbinary person's response, the generations are focused on identity-related issues, while the simulation of a person's response is topical.The former constructs a homogenous narrative that defines nonbinary people only by LGBTQ+ activism.We provide more qualitative examples in Appendix A.</p>
<p>Figure 3 :
3
Figure3: Our method to measure caricature in LLM simulations.We rely on comparing the defining characteristics of the persona and topic dimensions to measure individuation and exaggeration.</p>
<p>Figure 4 :
4
Figure4: Mean individuation scores (differentiability from default).The standard error for each point is &lt; 0.02 and thus not visible on the plot.Based on a classifier between the default-persona and the target persona, many of the personas are easy to individuate (have high accuracy scores).The only personas that are slightly challenging to differentiate are the gender groups woman and man in the online forum context (blue circles).All personas have mean score &gt; 0.95 in the interview context (green stars).</p>
<p>Figure 5 :
5
Figure5: Mean exaggeration scores  standard error in the online forum context.We measure exaggeration as normalized cosine similarity to the persona-topic axis.The more general topics (purple, larger marker) have higher rates of exaggeration, and thus caricature, than the specific topics (orange, smaller marker).The uncontroversial (WikiHow, squares) topics have higher rates of caricature than the controversial (ProCon.org,stars) topics.Personas related to political leanings, race/ethnicity, and nonbinary gender broadly have the highest rates of caricature.</p>
<p>Figure A1 :
A1
Figure A1: Top: Using the unsupervised V-measure to measure differentiation results in similar patterns as our main result in Figure 4. Bottom: Using an alternative pre-trained model to encode the outputs also results in similar patterns.</p>
<p>Table A1 :
A1
Examples of Simulated "Man" Responses
and how certain vehicles cater to different gen-der expressions. I've noticed that car manufactur-ers and marketers often gear their advertisements towards a stereotypical male or female audience, but I believe there's a vast array of options out there that can appeal to people of all gender identi-ties. For example, I personally love sleek, modern designs that have a balance of form and function and don't scream 'masculinity' or 'femininity.' I'd love to hear about your favorite vehicles and how they play a role in expressing your unique personality and identity. Let's have an open and inclusive conversation about our diverse expe-riences with cars and other vehicles!"</p>
<p>Exaggeration scores in the online forum context for topics varying in specificity.Across different personas (x-axis), exaggeration score (y-axis) is negatively correlated with the specificity of the topic (marker size).
AgePoliticalRace/EthnicityGenderSpecificityExaggeration (Similarity to Axis)0.0 0.2 0.4 0.6 0.81 2 3 4 5204080Conser-vativeLiberal ModerateAsian Black HispanicMiddle-EasternWhiteManNon-BinaryWomanFigure A2: Specificity Level 2: mental health Specificity Level 3: things to do for mental health, social media's effect on mental health Specificity Level 4: What is the best thing you did for your mental health?, Is social me-dia bad for your mental health? Specificity Level 5: During a very dark pe-riod, what was the best thing you ever didfor your mental health?, To what extent doyou think social media is bad for your mentalhealth?10868</p>
<p>Table A6 :
A6
Jiang et al. (2022)demonstrate how LLM simulations of Republican and Democrat Twitter users result in opinions about public figures and groups that correspond to the outcomes of the Ameri-Additional Examples Mapping Existing Work Using the CoMPosT Framework.Extended from Figure1.MC and O mean multiple-choice and open-response respectively.Note that all works use versions of GPT for the model dimension.
Paper Argyle et (2023) Aher et (2023) Park et al. al. al. (2023b) Santurkar et al. (2023) Binz and Schulz (2023) Hmlinen et al. (2023) Park et al. (2023a) Markel et al. (2023) Liu et al. (2023) Dubois et al. (2023)Context public survey (MC) public survey (MC) social science experi-ments (MC) psychology surveys and questionnaires (MC) public survey (MC) canonical cognitive psychology experiments (vignette-based and task-based) (MC) HCI research interview (O) interactive sandbox en-vironment inspired by The Sims, small town of twenty-five agents (O) office hours for a com-puter science class with students and a teaching assistant (O) virtual societies with so-cial norms (O) pairwise preference tasks that require human annotation (MC)Models Personas GPT-3 davinci public survey participants (po-litical ideology, church atten-dance, state, race, gender, in-come, age) GPT-3 davinci variety of ANES participants with specific backstories GPT-3 davinci social science experiment par-ticipants with varying names and genders GPT-3.5 liberals and conservatives GPT-3.5 &amp; AI21 60 U.S. demographic groups including age, gender, race, po-litical leaning, political party models GPT-3 psychology experiment partici-pants GPT-3 interviewees for HCI re-searcher GPT-3.5 turbo 25 agents of Smallville, incl. occupation and relationship with other agents GPT-3 students (descriptions include age, major, characteristics, mindset) GPT-3.5, social agents GPT-4 GPT-4 human annotators (crowdwork-ers)Topics presidential election demographic data about self Ultimatum Game, Garden Path Sentences, Milgram Shock Experiment, and Wisdom of Crowds psychology studies from the Many Labs 2 replication project Pew public opinion polls questions to evaluate decision-making, information search, deliberation, and causal reason-ing abilities experiencing video games as art interactions with other agents and reactions to changes in en-vironment computer science homework assignments social interactions and dynam-ics questions from OASST, An-thropic, Vicuna, and Koala evaluations
The code and data is available at https://github.com/myracheng/lm_caricature.
In our experiments, the classes are balanced since we generate 100 outputs for each simulation, but this measure generalizes to contexts with imbalanced classes.
Park et al. (2022) also include an HTML <span> tag to improve content quality. We find that this is only necessary for older models and not GPT-4.
Existing studies often exclude various demographics: studies on human behavior oversample from the population of American college students, who have been shown to differ from other populations in significant ways(Segall et al., 1966). Psychology and social science research subjects are disproportionately Western, educated, and from industrialized, rich, and democratic countries(Henrich et al., 2010).
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the Conference on Fairness, Accountability, and Transparency, pages 220-229.
AcknowledgmentsThank you to Tony Wang, Joon Sung Park, Omar Shaikh, Caleb Ziems, Camille Harris, and Matthias Gerstgrasser for their feedback throughout this project.This work was funded in part by an NSF Graduate Research Fellowship (Grant DGE-2146755) and Stanford Knight-Hennessy Scholars graduate fellowship to MC, as well as a Meta grant and an NSF grant IIS-2247357 to DY.A2: Persona-topic semantic axes seed words for randomly-sampled simulations with race/ethnicity and gender personas.Top persona and topic words identified by our method (Section 4.3) to construct persona-topic semantic axes for measuring exaggeration.We display the seed words corresponding to simulations of each persona alongside a randomly-sampled topic in the online forum context; this table is continued with word sets for age and political ideology personas in TableA3.Each word list is truncated to 15 words in this table, but we use the full set in constructing the axes.We find that the race/ethnicity and nonbinary persona seed words reflect the stereotypes and essentializing narratives documented byCheng et al. (2023b).Simulation Persona Pole Seed WordsTopic Pole Seed Words Persona: 20-year-old person, Topic: How to Say Happy Birthday im, yearold, forward, advice, forum, from, ive, discussions, looking, hey, everyone, been, experiences, thoughts, now year, day, filled, birthday, happy, wishing, joy, may, laughter, love, another, memories, , happiness, lots Persona: 80-year-old person, Topic: Food and Entertaining technology, am, all, world, hello, yearold, learn, how, online, learning, changes, discussions, from, has, experiences dinner, recipes, love, hosting, a, parties, out, party, food, dishes, absolutely, favorite, delicious, friends, tips Persona: 40-year-old person, Topic: whether the federal minimum wage should be increasedopinions, perspectives, and, different, find, ground, common, open, understanding, discussions, various, important, respectful, us, engage the, vote, system, popular, electoral, college, states, outdated, candidate, votes, of, direct, president, gives, winTableA3: Persona-topic semantic axes seed words for randomly-sampled simulations with age and political ideology personas.Examples continued from TableA2."An Asian person posted the following comment about health to an online forum" (Similar to(Park et al., 2022)) "Below you will be asked to provide a short description of your identity and then answer some questions.Description: In politics today, I would describe my political views as liberal.How much, if at all, do you worry about the following happening to you? Being the victim of a terrorist attack:" (Similiar to(Santurkar et al., 2023)) "A Republican user posted on Twitter: labor unions are"(Jiang et al., 2022)TableA4: Mapping dimensions in different simulations.Each prompt contains persona (bolded), context (italicized), and topic (highlighted).Default-Topic (persona p) PromptAge Below you will be asked to provide a short description of your identity and then answer some questions.Description: I am p.Comment:Political Ideology Below you will be asked to provide a short description of your identity and then answer some questions.Description: In politics today, I would describe my political views as p. Comment:Race/Ethnicity Below you will be asked to provide a short description of your identity and then answer some questions.Description: I am p. Comment: Gender Below you will be asked to provide a short description of your identity and then answer some questions.Description: I identify as p.Comment:Default-Persona (topic t) Below you will be asked to provide a short description of your identity and then answer some questions.Description: I am a person.Question: t Answer:TableA5: Prompts for default-persona and default-topic simulations in the interview context.
Prescriptive and descriptive approaches to machine-learning transparency. David Adkins, Bilal Alsallakh, Adeel Cheema, Narine Kokhlikyan, Emily Mcreynolds, Pushkar Mishra, Chavez Procope, Jeremy Sawruk, Erin Wang, Polina Zvyagina, CHI Conference on Human Factors in Computing Systems Extended Abstracts. 2022</p>
<p>Using large language models to simulate multiple humans and replicate human subject studies. Gati Aher, Rosa I Arriaga, Adam Tauman, Kalai , Proceedings of the 40th International Conference on Machine Learning. the 40th International Conference on Machine Learning2023ICML'23. JMLR.org</p>
<p>Jasmine," the sweet scent of exile. Anu Aneja, Pacific Coast Philology. 1993</p>
<p>Out of one, many: Using language models to simulate human samples. Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, David Wingate, 10.1017/pan.2023.2Political Analysis. 3132023</p>
<p>Based on billions of words on the Internet, people= men. April H Bailey, Adina Williams, Andrei Cimpian, Science Advances. 81324632022</p>
<p>Fine-tuning language models to find agreement among humans with diverse preferences. Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat Mcaleese, Amelia Glaese, John Aslanides, Matt Botvinick, Advances in Neural Information Processing Systems. 202235</p>
<p>Gender identity and lexical variation in social media. David Bamman, Jacob Eisenstein, Tyler Schnoebelen, Journal of Sociolinguistics. 1822014</p>
<p>Stereotyping and prejudice: Changing conceptions. Daniel Bar-Tal, Arie W Carl F Graumann, Wolfgang Kruglanski, Stroebe, 2013Springer Science &amp; Business Media</p>
<p>The computational therapeutic: Exploring Weizenbaum's ELIZA as a history of the present. Caroline Bassett, AI &amp; SOCIETY. 342019</p>
<p>The role of emotion in believable agents. Joseph Bates, Communications of the ACM. 3771994</p>
<p>Data statements for natural language processing: Toward mitigating system bias and enabling better science. M Emily, Batya Bender, Friedman, Transactions of the Association for Computational Linguistics. 62018</p>
<p>Using cognitive psychology to understand GPT-3. Marcel Binz, Eric Schulz, Proceedings of the National Academy of Sciences. 1206e22185231202023</p>
<p>Imagining stereotypes away: The moderation of implicit stereotypes through mental imagery. Irene V Blair, Jennifer E Ma, Alison P Lenton, Journal of personality and social psychology. 8158282001</p>
<p>Race and genetic ancestry in medicine-a time for reckoning with racism. Luisa N Borrell, Jennifer R Elhawary, Elena Fuentes-Afflick, Jonathan Witonsky, Nirav Bhakta, Alan Hb Wu, Kirsten Bibbins-Domingo, Jos R Rodrguez-Santana, Michael A Lenoir, James R Gavin, Iii , New England Journal of Medicine. 38452021</p>
<p>Racist cute: Caricature, kawaii-style, and the Asian thing. Leslie Bow, American Quarterly. 7112019</p>
<p>Using thematic analysis in healthcare HCI at CHI: A scoping review. Robert Bowman, Camille Nadal, Kellie Morrissey, Anja Thieme, Gavin Doherty, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>A stereotype, wrapped in a clich, inside a caricature: Russian foreign policy and orientalism. James Dj Brown, Politics. 3032010</p>
<p>Theory-grounded measurement of US social stereotypes in english language models. Yang Cao, Anna Sotnikova, Hal Daum, Iii , Rachel Rudinger, Linda Zou, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies2022</p>
<p>Quantifying memorization across neural language models. Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, Chiyuan Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>The internet's hidden rules: An empirical study of Reddit norm violations at micro, meso, and macro scales. Eshwar Chandrasekharan, Mattia Samory, Shagun Jhaver, Hunter Charvat, Amy Bruckman, Cliff Lampe, Jacob Eisenstein, Eric Gilbert, Proceedings of the ACM on Human-Computer Interaction. 22018CSCW</p>
<p>Social norm bias: Residual harms of fairness-aware algorithms. Myra Cheng, Maria De-Arteaga, Lester Mackey, Adam Tauman, Kalai , Data Mining and Knowledge Discovery. 2023a</p>
<p>Marked personas: Using natural language prompts to measure stereotypes in language models. Myra Cheng, Esin Durmus, Dan Jurafsky, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. the 61st Annual Meeting of the Association for Computational Linguistics2023b</p>
<p>Positive stereotypes are pervasive and powerful. Aaron C Alexander M Czopp, Sapna Kay, Cheryan, Perspectives on Psychological Science. 1042015</p>
<p>Propaganda and caricature in the first world war. Eberhard Demm, Journal of Contemporary History. 2811993</p>
<p>Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Alpaca-Farm: A simulation framework for methods that learn from human feedback. 2023</p>
<p>Memorization vs. generalization: Quantifying data leakage in NLP performance evaluation. Aparna Elangovan, Jiayuan He, Karin Verspoor, 10.18653/v1/2021.eacl-main.113Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main VolumeOnline. Association for Computational Linguistics2021</p>
<p>A model of (often mixed) stereotype content: competence and warmth respectively follow from perceived status and competition. Amy Jc Susan T Fiske, Peter Cuddy, Glick, Journal of personality and social psychology. 826878Jun Xu. 2002</p>
<p>Datasheets for datasets. Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum Iii, Kate Crawford, Communications of the ACM. 64122021</p>
<p>From Muhammad to Obama: Caricatures, cartoons, and stereotypes of Muslims. Islamophobia: The challenge of pluralism in the 21st century. Peter Gottschalk, Gabriel Greenberg, 2011</p>
<p>Why personas work: The psychological evidence. The persona lifecycle. Jonathan Grudin, 200612</p>
<p>Evaluating large language models in generating synthetic HCI research data: a case study. Perttu Hmlinen, Mikke Tavast, Anton Kunnari, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Blind judgement: Agent-based supreme court modelling with GPT. Sil Hamilton, arXiv:2301.053272023arXiv preprint</p>
<p>Towards a critical race methodology in algorithmic fairness. Alex Hanna, Emily Denton, Andrew Smart, Jamila Smith-Loud, Proceedings of the 2020 conference on Fairness, Accountability, and Transparency. the 2020 conference on Fairness, Accountability, and Transparency2020</p>
<p>Description and prescription: How gender stereotypes prevent women's ascent up the organizational ladder. Madeline E Heilman, Journal of Social Issues. 5742001</p>
<p>The weirdest people in the world?. Joseph Henrich, Steven J Heine, Ara Norenzayan, Behavioral and brain sciences. 332-32010</p>
<p>Gender and genre variation in weblogs. C Susan, John C Herring, Paolillo, Journal of Sociolinguistics. 1042006</p>
<p>Steamer: An interactive inspectable simulation-based training system. Edwin L James D Hollan, Louis Hutchins, Weitzman, AI magazine. 19845</p>
<p>Stance prediction for contemporary issues: Data and experiments. Marjan Hosseinia, Eduard Dragut, Arjun Mukherjee, 10.18653/v1/2020.socialnlp-1.5Proceedings of the Eighth International Workshop on Natural Language Processing for Social Media. the Eighth International Workshop on Natural Language Processing for Social MediaOnline. Association for Computational Linguistics2020</p>
<p>CommunityLM: Probing partisan worldviews from language models. Hang Jiang, Doug Beeferman, Brandon Roy, Deb Roy, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of Korea2022International Committee on Computational Linguistics</p>
<p>Automated intelligent pilots for combat flight simulation. M Randolph, John E Jones, Paul E Laird, Karen J Nielsen, Patrick Coulter, Frank V Kenny, Koss, AI magazine. 2011999</p>
<p>Interpretations and methods: Towards a more effectively self-correcting social psychology. Lee Jussim, T Jarret, Stephanie M Crawford, Sean T Anglin, Jose L Stevens, Duarte, Journal of Experimental Social Psychology. 662016</p>
<p>Surfacing racial stereotypes through identity portrayal. Gauri Kambhatla, Ian Stewart, Rada Mihalcea, 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>Reimagining (women's) health: HCI, gender and essentialised embodiment. Os Keyes, Burren Peil, Rua M Williams, Katta Spiel, ACM Transactions on Computer-Human Interaction (TOCHI). 2742020</p>
<p>Mahnaz Koupaee, William Yang, Wang , arXiv:1810.09305Wikihow: A large scale text summarization dataset. 2018arXiv preprint</p>
<p>Slavery, sugar, and the culture of refinement: Picturing the British West Indies. Dian Kay, Kriz, 2008Paul Mellon Centre</p>
<p>Engaging feminist solidarity for comparative research, design, and practice. Neha Kumar, Naveena Karusala, Azra Ismail, Marisol Wong-Villacres, Aditya Vishwanath, Proceedings of the ACM on Human-Computer Interaction. 32019</p>
<p>Question and answer test-train overlap in opendomain question answering datasets. Patrick Lewis, Pontus Stenetorp, Sebastian Riedel, Proceedings of the 16th Conference of the European Chapter. the 16th Conference of the European ChapterMain Volume2021</p>
<p>Embracing four tensions in humancomputer interaction research with marginalized people. Calvin A Liang, Sean A Munson, Julie A Kientz, ACM Transactions on Computer-Human Interaction (TOCHI). 2822021</p>
<p>Training socially aligned language models in simulated human society. Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, Soroush Vosoughi, arXiv:2305.169602023arXiv preprint</p>
<p>Discovering differences in the representation of people using contextualized semantic axes. Li Lucy, Divya Tadimeti, David Bamman, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>A history of caricature. Bohun Lynch, 1927Faber and Gwyer</p>
<p>GPTeach: Interactive TA training with GPT-based students. Julia M Markel, Steven G Opferman, James A Landay, Chris Piech, 10.1145/3573051.3593393Proceedings of the Tenth ACM Conference on Learning @ Scale, L@S '23. the Tenth ACM Conference on Learning @ Scale, L@S '23New York, NY, USAAssociation for Computing Machinery2023</p>
<p>Stereotypes and politics: Reflections on personas. Nicola Marsden, Maren Haag, Proceedings of the 2016 CHI conference on human factors in computing systems. the 2016 CHI conference on human factors in computing systems2016</p>
<p>Personas and identity: Looking at multiple identities to inform the construction of personas. Nicola Marsden, Monika Prbster, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. the 2019 CHI Conference on Human Factors in Computing Systems2019</p>
<p>Cancer incidence, mortality, and associated risk factors among Asian Americans of Chinese, Filipino, Vietnamese, Korean, and Japanese ethnicities. Melissa Mccracken, Miho Olsen, Moon S ChenJr, Ahmedin Jemal, Michael Thun, Vilma Cokkinides, Dennis Deapen, Elizabeth Ward, CA: a cancer journal for clinicians. 5742007</p>
<p>Measuring the similarity of sentential arguments in dialogue. Amita Misra, Brian Ecker, Marilyn Walker, 10.18653/v1/W16-3636Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 17th Annual Meeting of the Special Interest Group on Discourse and DialogueLos AngelesAssociation for Computational Linguistics2016</p>
<p>Under Western eyes: Feminist scholarship and colonial discourses. Chandra Mohanty, Feminist review. 3011988</p>
<p>Fightin'words: Lexical feature selection and evaluation for identifying the content of political conflict. Michael P Burt L Monroe, Kevin M Colaresi, Quinn, Political Analysis. 1642008</p>
<p>arXivOpenAI. 2023. GPT-4 technical report. </p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Sung Joon, Park, C Joseph, Carrie J O'brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. 2023aarXiv preprint</p>
<p>Social simulacra: Creating populated prototypes for social computing systems. Sung Joon, Lindsay Park, Carrie Popowski, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology. the 35th Annual ACM Symposium on User Interface Software and Technology2022</p>
<p>Philipp Peter S Park, Chongyang Schoenegger, Zhu, arXiv:2302.07267Artificial intelligence in psychology research. 2023barXiv preprint</p>
<p>Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, Ece Kamar, Investigations of performance and bias in human-AI teamwork in hiring. Proceedings of the AAAI Conference on Artificial Intelligence. 202236</p>
<p>A definition of caricature and caricature and recognition. David Perkins, Studies in Visual Communication. 19752</p>
<p>Understanding prejudice and discrimination. Ed Scott, Plous, 2003McGraw-Hill</p>
<p>Disaggregating heterogeneity among non-hispanic whites: evidence and implications for us racial/ethnic health disparities. Population research and policy review. Jen'nan Ghazal Read, Scott M Lynch, Jessica S West, 202140</p>
<p>Sentence-bert: Sentence embeddings using siamese bert-networks. Nils Reimers, Iryna Gurevych, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingAssociation for Computational Linguistics2019</p>
<p>NLPositionality: Characterizing design biases of datasets and models. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, 10.18653/v1/2023.acl-long.505Proceedings of the 40th International Conference on Machine Learning, ICML'23. JMLR.org. Sebastin Santy, Jenny Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. Long Papers. the 40th International Conference on Machine Learning, ICML'23. JMLR.org. Sebastin Santy, Jenny Liang, Ronan Le Bras, Katharina Reinecke, and Maarten SapToronto, CanadaAssociation for Computational Linguistics2023. 20231Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</p>
<p>Deceptive AI explanations: Creation and detection. Johannes Schneider, Christian Meske, International Conference on Agents and Artificial Intelligence. SciTePress2020and Michalis Vlachos</p>
<p>The influence of culture on visual perception. Donald Marshall H Segall, Melville Jean Thomas Campbell, Herskovits, Bobbs-Merrill Indianapolis. 3101966</p>
<p>The curse of recursion: Training on generated data makes models forget. Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, Ross Anderson, arxiv:2305.174932023arXiv preprint</p>
<p>Histrionic personality and antisocial personality: caricatures of stereotypes? Comprehensive. Phillip R Slavney, Psychiatry. 2521984</p>
<p>Process for adapting language models to society (palms) with values-targeted datasets. Irene Solaiman, Christy Dennison, Advances in Neural Information Processing Systems. 202134</p>
<p>Imagining east asian education otherwise: Neither caricature, nor scandalization. Keita Takayama, Asia Pacific Journal of Education. 3722017</p>
<p>Memorization without overfitting: Analyzing the training dynamics of large language models. Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, Armen Aghajanyan, Advances in Neural Information Processing Systems. 202235</p>
<p>ACROCPoLis: A descriptive framework for making sense of fairness. Andrea Aler Tubella, Dimitri Coelho Mollo, Adam Dahlgren Lindstrm, Hannah Devinney, Virginia Dignum, Petter Ericson, Anna Jonsson, Timotheus Kampik, Tom Lenaerts, Julian Alfredo Mendez, 2023 ACM Conference on Fairness, Accountability, and Transparency. 2023</p>
<p>Artificial artificial artificial intelligence: Crowd workers widely use large language models for text production tasks. Veniamin Veselovsky, Manoel Horta Ribeiro, Robert West, arXiv:2306.078992023arXiv preprint</p>
<p>Do humans trust advice more if it comes from AI? an analysis of human-AI interactions. Kailas Vodrahalli, Roxana Daneshjou, Tobias Gerstenberg, James Zou, Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. the 2022 AAAI/ACM Conference on AI, Ethics, and Society2022</p>
<p>Towards intersectionality in machine learning: Including more identities, handling underrepresentation, and performing evaluation. Angelina Wang, V Vikram, Olga Ramaswamy, Russakovsky, 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>ELIZA-a computer program for the study of natural language communication between man and machine. Joseph Weizenbaum, Communications of the ACM. 911966</p>
<p>Robert Wolfe, Aylin Caliskan, Markedness in visual semantic AI. 2022 ACM Conference on Fairness, Accountability, and Transparency. 2022</p>
<p>Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, Yang You, arXiv:2305.13230To repeat or not to repeat: Insights from scaling llm under token-crisis. 2023arXiv preprint</p>
<p>Computational Social Roles. Diyi Yang, 2019Pittsburgh, PA, USACarnegie Mellon UniversityPh.D. thesis</p>
<p>Why Johnny can't prompt: how non-AI experts try (and fail) to design llm prompts. Richmond Y Jd Zamfirescu-Pereira, Bjoern Wong, Qian Hartmann, Yang, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>NormBank: A knowledge bank of situational social norms. Caleb Ziems, Jane Dwivedi-Yu, Yi-Chia Wang, Alon Halevy, Diyi Yang, 10.18653/v1/2023.acl-long.429Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational Linguistics20231</p>            </div>
        </div>

    </div>
</body>
</html>
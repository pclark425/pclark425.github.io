<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5279 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5279</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5279</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-108.html">extraction-schema-108</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-c77c48fe9060aa83627fc2c7f331325de0c4fdac</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c77c48fe9060aa83627fc2c7f331325de0c4fdac" target="_blank">DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work makes an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat, a graph neural network, a large language model, and an adaptor.</p>
                <p><strong>Paper Abstract:</strong> A ChatGPT-like system for drug compounds could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structure-activity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner. The DrugChat system consists of a graph neural network (GNN), a large language model (LLM), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes the compound representation transformed by the adaptor and users' questions about this compound as inputs and generates answers. All these components are trained end-to-end. To train DrugChat, we collected instruction tuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs. The code and data is available at \url{https://github.com/UCSD-AI4H/drugchat}</p>
                <p><strong>Cost:</strong> 0.008</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5279.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5279.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to generate, design, or synthesize novel chemical compounds for specific applications, including details on the model, application, generation method, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DrugChat</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DrugChat</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prototype system that connects a pretrained graph neural network (GNN) encoding of molecular graphs to a large language model (Vicuna-13b) via a linear adaptor (soft prompt) to enable multi-turn, conversational question-answering and text generation about drug compounds from their molecular graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vicuna-13b</td>
                        </tr>
                        <tr>
                            <td><strong>model_type</strong></td>
                            <td>decoder-only transformer (autoregressive LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B (Vicuna-13b)</td>
                        </tr>
                        <tr>
                            <td><strong>training_data</strong></td>
                            <td>Instruction-tuning datasets curated from ChEMBL and PubChem: combined 10,834 drug compounds and 143,517 question-answer pairs; for each drug the dataset includes SMILES/molecular graph inputs and human-written descriptive target text (mechanism, properties, usage, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Drug compound analysis and interactive drug-discovery assistance (QA, descriptive generation, and potentially suggesting structural modifications/lead optimization in principle).</td>
                        </tr>
                        <tr>
                            <td><strong>generation_method</strong></td>
                            <td>Graph-to-text via soft prompting: a pretrained GNN (Hu et al., 2020) encodes the input molecular graph to a graph representation; a linear adaptor (learned projection) maps that representation to a soft prompt vector which is concatenated with the natural language instruction and fed to a frozen Vicuna-13b LLM; only the adaptor weights are trained (supervised fine-tuning with negative log likelihood on target answers). No procedure for directly autoregressively generating new molecular structures (SMILES/graphs) is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>output_representation</strong></td>
                            <td>Natural language textual answers / descriptions (free-form text). Inputs to the system are molecular graphs derived from SMILES, but outputs are not molecular representations (no SMILES/SELFIES/graph outputs reported).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>During training: negative log-likelihood loss between generated text and ground-truth descriptive text. For system performance: qualitative multi-turn examples on held-out compounds only (no reported quantitative metrics such as validity, novelty, synthesizability, or activity for generated molecules). The authors state plans for systematic quantitative evaluation in future work.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmarks_or_datasets</strong></td>
                            <td>ChEMBL and PubChem-derived instruction-tuning datasets (this work). No generative benchmarks (e.g., MOSES, GuacaMol) or molecular property/activity benchmarks were used for evaluating molecule generation because the system was not used to generate molecules.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>DrugChat successfully generates informative, multi-turn natural language answers about unseen drug molecular graphs (demonstrated with qualitative examples). The system enables conversational analysis and descriptive generation from graphs, but the paper does not demonstrate or validate generating novel chemical structures or performing de novo molecule design; evaluation is qualitative and quantitative validation is deferred.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_methods</strong></td>
                            <td>No quantitative comparison to other molecule-generation or design methods is provided. Related work references multimodal LLM efforts (MiniGPT-4, LLaVA) and GNN pretraining (Hu et al., 2020), but DrugChat is not compared experimentally to traditional generative chemistry models (e.g., MolGPT, VAE/GAN-based molecular generators) or policy-gradient/REINFORCE/ RL-based molecule-optimizers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Authors explicitly note language hallucination (LLM producing untrustworthy or incorrect descriptive outputs) as a primary limitation; lack of validated molecule-generation capability — the LLM outputs text not molecular structures; only the adaptor is trained while GNN and LLM are frozen which may limit expressive adaptation; limited to the coverage and quality of curated ChEMBL/PubChem textual descriptions; no systematic quantitative evaluation or experimental (wet-lab) validation of any chemical suggestions; potential to mislead human decision-makers if hallucinations occur.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Artificial intelligence in drug discovery and development. <em>(Rating: 2)</em></li>
                <li>Accurate prediction of molecular properties and drug targets using a self-supervised image representation learning framework. <em>(Rating: 2)</em></li>
                <li>Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality <em>(Rating: 1)</em></li>
                <li>Language models are few-shot learners. <em>(Rating: 1)</em></li>
                <li>Minigpt-4: Enhancing vision-language understanding with advanced large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5279",
    "paper_id": "paper-c77c48fe9060aa83627fc2c7f331325de0c4fdac",
    "extraction_schema_id": "extraction-schema-108",
    "extracted_data": [
        {
            "name_short": "DrugChat",
            "name_full": "DrugChat",
            "brief_description": "A prototype system that connects a pretrained graph neural network (GNN) encoding of molecular graphs to a large language model (Vicuna-13b) via a linear adaptor (soft prompt) to enable multi-turn, conversational question-answering and text generation about drug compounds from their molecular graphs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Vicuna-13b",
            "model_type": "decoder-only transformer (autoregressive LLM)",
            "model_size": "13B (Vicuna-13b)",
            "training_data": "Instruction-tuning datasets curated from ChEMBL and PubChem: combined 10,834 drug compounds and 143,517 question-answer pairs; for each drug the dataset includes SMILES/molecular graph inputs and human-written descriptive target text (mechanism, properties, usage, etc.).",
            "application_domain": "Drug compound analysis and interactive drug-discovery assistance (QA, descriptive generation, and potentially suggesting structural modifications/lead optimization in principle).",
            "generation_method": "Graph-to-text via soft prompting: a pretrained GNN (Hu et al., 2020) encodes the input molecular graph to a graph representation; a linear adaptor (learned projection) maps that representation to a soft prompt vector which is concatenated with the natural language instruction and fed to a frozen Vicuna-13b LLM; only the adaptor weights are trained (supervised fine-tuning with negative log likelihood on target answers). No procedure for directly autoregressively generating new molecular structures (SMILES/graphs) is reported.",
            "output_representation": "Natural language textual answers / descriptions (free-form text). Inputs to the system are molecular graphs derived from SMILES, but outputs are not molecular representations (no SMILES/SELFIES/graph outputs reported).",
            "evaluation_metrics": "During training: negative log-likelihood loss between generated text and ground-truth descriptive text. For system performance: qualitative multi-turn examples on held-out compounds only (no reported quantitative metrics such as validity, novelty, synthesizability, or activity for generated molecules). The authors state plans for systematic quantitative evaluation in future work.",
            "benchmarks_or_datasets": "ChEMBL and PubChem-derived instruction-tuning datasets (this work). No generative benchmarks (e.g., MOSES, GuacaMol) or molecular property/activity benchmarks were used for evaluating molecule generation because the system was not used to generate molecules.",
            "results_summary": "DrugChat successfully generates informative, multi-turn natural language answers about unseen drug molecular graphs (demonstrated with qualitative examples). The system enables conversational analysis and descriptive generation from graphs, but the paper does not demonstrate or validate generating novel chemical structures or performing de novo molecule design; evaluation is qualitative and quantitative validation is deferred.",
            "comparison_to_other_methods": "No quantitative comparison to other molecule-generation or design methods is provided. Related work references multimodal LLM efforts (MiniGPT-4, LLaVA) and GNN pretraining (Hu et al., 2020), but DrugChat is not compared experimentally to traditional generative chemistry models (e.g., MolGPT, VAE/GAN-based molecular generators) or policy-gradient/REINFORCE/ RL-based molecule-optimizers.",
            "limitations_or_challenges": "Authors explicitly note language hallucination (LLM producing untrustworthy or incorrect descriptive outputs) as a primary limitation; lack of validated molecule-generation capability — the LLM outputs text not molecular structures; only the adaptor is trained while GNN and LLM are frozen which may limit expressive adaptation; limited to the coverage and quality of curated ChEMBL/PubChem textual descriptions; no systematic quantitative evaluation or experimental (wet-lab) validation of any chemical suggestions; potential to mislead human decision-makers if hallucinations occur.",
            "uuid": "e5279.0",
            "source_info": {
                "paper_title": "DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Artificial intelligence in drug discovery and development.",
            "rating": 2
        },
        {
            "paper_title": "Accurate prediction of molecular properties and drug targets using a self-supervised image representation learning framework.",
            "rating": 2
        },
        {
            "paper_title": "Vicuna: An open-source chatbot impressing gpt-4 with $90 \\%$ * chatgpt quality",
            "rating": 1
        },
        {
            "paper_title": "Language models are few-shot learners.",
            "rating": 1
        },
        {
            "paper_title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models.",
            "rating": 1
        }
    ],
    "cost": 0.007861749999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs</h1>
<p>Youwei Liang<em><br>Ruiyi Zhang ${ }^{</em>}$<br>Li Zhang<br>Pengtao Xie<br>University of California San Diego, *Equal contribution.</p>
<h4>Abstract</h4>
<p>A ChatGPT-like system for drug compound analysis could be a game-changer in pharmaceutical research, accelerating drug discovery, enhancing our understanding of structureactivity relationships, guiding lead optimization, aiding drug repurposing, reducing the failure rate, and streamlining clinical trials. In this work, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner. The DrugChat system consists of a graph neural network (GNN), a large language model (LLM), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes the compound representation transformed by the adaptor and users' questions about this compound as inputs and generates answers. All these components are trained end-toend. To train DrugChat, we collected instruction tuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs. The code and data is available at https://github.com/UCSD-AI4H/drugchat</p>
<h2>1. Introduction</h2>
<p>The process of drug discovery and development is a time-intensive and costly endeavor, often taking years and billions of dollars to bring a single drug to market (Avorn, 2015). This process involves the exploration and understanding of vast chemical spaces and the intricate relationships between molecular structures and their biological activities, commonly known as structure-activity relationships (SAR) (Idakwo et al., 2020). Traditional methods (Rycker et al., 2018) often involve laborious iterative testing, with a high rate of late-stage failures. Recent advancements in computational chemistry and chemoinformatics (Zeng et al., 2022) have offered some respite, but there is still a pressing need for tools that can intuitively understand and generate meaningful insights from the complex data inherent in molecular graphs of drug compounds.</p>
<p>This technical report introduces the concept of applying ChatGPT-like capabilities to drug molecule graphs, aiming to revolutionize the way we interact with and understand these complex entities. By transforming these molecular graphs into a form amenable to AI analysis, we can enable dynamic exploration of chemical spaces, efficient prediction of compound properties, and intelligent suggestions for drug design and optimization. A ChatGPT-like AI system capable of understanding drug compound molecule graphs and answering various questions about these drugs could revolutionize pharmaceutical research in several ways:</p>
<ul>
<li>Speeding up Drug Discovery. A ChatGPT-like AI system could drastically cut down the time required for initial stages of drug discovery by providing immediate insights into a compound's potential therapeutic uses, side effects, and contraindications based on its structure.</li>
<li>Predicting Drug Interactions. A ChatGPT-like AI system could predict potential interactions between new drug candidates and existing drugs. By comparing the molecular structures of thousands of known substances, the system could identify possible conflicts or synergies, helping researchers to better anticipate how a new drug might behave in the real world.</li>
<li>Understanding Structure-Activity Relationships (SAR). SAR (Idakwo et al., 2020) is a crucial aspect of drug design. A ChatGPT-like AI system could help researchers understand the relationship between a drug's chemical structure and its biological activity. It could also help predict what modifications to the chemical structure might enhance its effectiveness or reduce unwanted side effects.</li>
<li>Guiding Lead Optimization. During the drug discovery process, 'lead' compounds (those that show promise in initial screenings) (Hughes et al., 2011) are typically optimized for better efficacy, reduced toxicity, and improved pharmacokinetics. A ChatGPT-like AI system could offer suggestions for structural modifications to enhance these parameters, guiding researchers in the right direction and saving valuable time.</li>
<li>Supporting Drug Repurposing. A ChatGPT-like AI system could also aid in drug repurposing (Pushpakom et al., 2019) efforts. By understanding the structural properties of existing drugs, it could identify candidates that may be effective against diseases they were not initially developed to treat. This could help breathe new life into existing drugs and offer more rapid routes to treatment for challenging diseases.</li>
<li>Reducing the Failure Rate. The failure rate in drug discovery is high (Hughes et al., 2011), often due to unforeseen toxicity or efficacy issues that emerge late in development. By providing more accurate predictions about a drug's properties and effects at the outset, a ChatGPT-like AI system could help reduce these costly latestage failures.</li>
<li>Streamlining Clinical Trials. A ChatGPT-like AI could help design more effective clinical trials by predicting how a drug will interact with other medications or condi-</li>
</ul>
<p>tions, enabling researchers to target their trials more effectively and recruit suitable patient cohorts.</p>
<p>Developing a ChatGPT-like system for drug molecule graphs is highly challenging. First, one of the primary challenges lies in the representation of molecular graphs. Unlike text data, which is sequential and has a well-defined structure, molecular graphs are inherently non-sequential and highly complex, with no clear start or end point. Developing a methodology to translate these graphs into a format that a GPT model can process is crucial. Second, another significant challenge is capturing and understanding the intricate structure-activity relationships (SAR) in drug compounds. These relationships are often not straightforward and can involve subtle interactions between different parts of a molecule. Third, to train such a system, large datasets of molecular structures, along with their associated biological activities, side effects, and other properties, would be required. The generation or compilation of such datasets is a non-trivial task, given the proprietary nature of much of this information and the sheer diversity of the chemical space.</p>
<p>In this technical report, we make an initial attempt towards enabling ChatGPT-like capabilities on drug molecule graphs, by developing a prototype system DrugChat. DrugChat works in a similar way as ChatGPT. Users upload a compound molecule graph and ask various questions about this compound. DrugChat will answer these questions in a multi-turn, interactive manner.</p>
<p>The DrugChat system consists of a graph neural network (GNN) (Hu et al., 2020), a large language model (LLM) (Chiang et al., 2023), and an adaptor. The GNN takes a compound molecule graph as input and learns a representation for this graph. The adaptor transforms the graph representation produced by the GNN into another representation that is acceptable to the LLM. The LLM takes the compound representation transformed by the adaptor and users' questions about this compound as inputs and generates answers. All these components are trained end-to-end. To train DrugChat, we collected instruction tuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs.</p>
<p>The major contributions of this work are as follows:</p>
<ul>
<li>We develop DrugChat, a prototype system aiming to achieve ChatGPT-like capabilities on drug molecule graphs. DrugChat allows users to interactively ask open-ended questions about drug compounds and provides informative answers.</li>
<li>We collect instruction tuning datasets which contain 10,834 drug compounds and 143,517 question-answer pairs. These datasets enable the training of ChatGPT-like models for drug compounds and are publicly available.</li>
<li>To our best knowledge, DrugChat represents the first system that bridges graph data with large language models (LLMs), which enables interactive conversations on graphs. Our system seamlessly integrates graph neural networks with LLMs and can be easily extended to analyze other graph data beyond compound molecule graphs.</li>
</ul>
<h1>2. Related Works</h1>
<p>AI-based drug properties analysis (Paul et al., 2021) is a promising approach to significantly reduce costs and time associated with the traditional drug discovery and development</p>
<p>pipeline. For example, ImageMol (Zeng et al., 2022) is an unsupervised pretraining method that processes images of chemical structures and outputs molecular properties, such as drug metabolism, brain penetration, toxicity, and molecular target profiles like beta-secretase enzyme and kinases. ImageMol was pretrained on 10 million unlabeled drug-like, bioactive molecules through predicting molecular targets of candidate compounds. It was assessed on 51 benchmark datasets and demonstrated high accuracy in identifying anti-SARS-CoV-2 molecules across 13 high-throughput experimental datasets.</p>
<p>Large language models (LLMs) (Brown et al., 2020) have demonstrated outstanding capabilities in generating inventive text, responding to reading comprehension queries, mathematical reasoning, etc. Nevertheless, the weight parameters of some of the most powerful LLMs are not publicly available, which considerably hinders academic research. Moreover, early LLMs were limited to processing text information as input, restricting their ability to understand information in other modalities. Consequently, researchers have developed an array of publicly accessible LLMs, including multi-modal variants, to address these challenges.</p>
<p>LLaMA (Touvron et al., 2023), an LLM with 65 billion parameters, was developed by Meta AI ${ }^{1}$. This model is trained on publicly accessible datasets including English CommonCrawl, C4, Github, Wikipedia, Gutenberg Project, ArXiv, and Stack Exchange. This training dataset encompasses 20 languages with the most speakers. Although it is smaller than GPT-3, LLaMA demonstrates superior performance on many benchmarks, including commonsense reasoning, question answering, reading comprehension, code generation, etc. Vicuna (Chiang et al., 2023) is an open-source chatbot trained by fine-tuning LLaMA on around 70,000 user-shared conversations with ChatGPT ${ }^{2}$. Vicuna was evaluated using GPT-4. MiniGPT-4 (Zhu et al., 2023) is a vision-language model capable of processing images and generating textual answers ${ }^{3}$. For instance, it can create stories and poems from input images and offer solutions to problems depicted in images. MiniGPT-4 incorporates a linear projection layer to align the visual information from a pretrained vision encoder with a large language model. LLaVA (Liu et al., 2023) is another multi-modal model that integrates a vision encoder with an LLM to facilitate general-purpose visual and language understanding ${ }^{4}$. Different from MiniGPT-4, LLaVA is trained on multi-turn conversation data. The projection layer and the LLM are both updated in the training stage while only the visual encoder parameters are frozen. The training data was created by converting raw image-text pairs into a suitable instruction-following format using ChatGPT/GPT-4.</p>
<h1>3. Drug Instruction Tuning Data</h1>
<p>To train DrugChat, we curate instruction tuning data for drug compounds, from two sources - ChEMBL and PubChem. Table 1 summarizes the statistics of the curated two datasets.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Table 1: Dataset statistics.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Dataset</th>
<th style="text-align: center;">Number of Drugs</th>
<th style="text-align: center;">Number of Question-Answer Pairs</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">ChEMBL</td>
<td style="text-align: center;">3,892</td>
<td style="text-align: center;">129,699</td>
</tr>
<tr>
<td style="text-align: center;">PubChem</td>
<td style="text-align: center;">6,942</td>
<td style="text-align: center;">13,818</td>
</tr>
</tbody>
</table>
<h1>3.1. ChEMBL Drug Instruction Tuning Dataset</h1>
<p>In this section, we outline the process of constructing the ChEMBL Drug Instruction Tuning dataset. We gathered data from the ChEMBL website ${ }^{5}$, which houses information for a total of $2,354,965$ chemical compounds. We downloaded the sqlite version of the data dump, which was last updated on February 28, $2023^{6}$. Out of the entire dataset, we identified 14,816 compounds that contain drug information. After applying further filtering to exclude drugs with insufficient descriptive information, we ended up with a dataset containing 3,892 drugs.</p>
<p>For each drug, we initially gathered its SMILES string, representing the molecular structure. Subsequently, we obtained various molecular features, including the complete molecular formula and its classification as an acid, base, or neutral compound. Additionally, we collected drug-specific properties, such as the mechanism of action and therapeutic applications. Based on these attributes, we manually crafted 129,699 question-answer pairs. Question-answer (QA) pairs for an exemplar drug in the ChEMBL Drug Instruction Tuning Dataset is shown in Table 2.</p>
<p>Table 2. Question-answer (QA) pairs for an exemplar drug in the ChEMBL Drug Instruction Tuning Dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">SMILES of the drug</th>
<th style="text-align: center;">$\mathrm{CC}(=\mathrm{O}) / \mathrm{N}=\mathrm{c} 1 / \mathrm{sc}(\mathrm{S}(\mathrm{N})(=\mathrm{O})=\mathrm{O}) \mathrm{nn} 1 \mathrm{C}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QA Pair 1</td>
<td style="text-align: center;">Q: Please provide a description of this drug's mechanism of action. <br> A: Carbonic anhydrase II inhibitor.</td>
</tr>
<tr>
<td style="text-align: left;">QA Pair 2</td>
<td style="text-align: center;">Q: Is this compound an acid, a base, or neutral? <br> A: Neutral.</td>
</tr>
<tr>
<td style="text-align: left;">QA Pair 3</td>
<td style="text-align: center;">Q: How many rotatable bonds does this compound have? <br> A: 1.</td>
</tr>
<tr>
<td style="text-align: left;">QA Pair 4</td>
<td style="text-align: center;">Q: Determine if this drug is administered as a racemic mixture, a single <br> stereoisomer, an achiral molecule, or has an unknown chirality. <br> A: An achiral molecule.</td>
</tr>
<tr>
<td style="text-align: left;">QA Pair 5</td>
<td style="text-align: center;">Q: Does this compound satisfy the rule-of-three criteria? <br> A: No.</td>
</tr>
<tr>
<td style="text-align: left;">QA Pair 6</td>
<td style="text-align: center;">Q: How many violations of Lipinski's Rule of Five are there for this <br> compound, using the HBA_LIPINSKI and HBD_LIPINSKI counts? <br> A: 0.</td>
</tr>
</tbody>
</table>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;">QA Pair 7</th>
<th style="text-align: center;">Q: Is it known whether this drug is administered parenterally? A: No.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">QA Pair 8</td>
<td style="text-align: center;">Q: Is this compound a small molecule polymer, such as polystyrene sulfonate? <br> A: No.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 9</td>
<td style="text-align: center;">Q: What is the calculated ALogP value for this compound? A: -1.42 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 10</td>
<td style="text-align: center;">Q: Is this molecule characterized by a small molecular structure or a protein sequence? <br> A: It has a small molecule structure.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 11</td>
<td style="text-align: center;">Q: Determine if this molecule is inorganic, meaning it contains only metal atoms and fewer than two carbon atoms. <br> A: No.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 12</td>
<td style="text-align: center;">Q: What is the highest development stage achieved for this compound across all indications? Please respond with Approved, Phase 3 Clinical Trials, Phase 2 Clinical Trials, Phase 1 Clinical Trials, Early Phase 1 Clinical Trials, or Clinical Phase Unknown. <br> A: Approved.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 13</td>
<td style="text-align: center;">Q: What is the first recorded year of approval for this drug? <br> A: 1959.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 14</td>
<td style="text-align: center;">Q: Is there a black box warning associated with this drug? <br> A: No.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 15</td>
<td style="text-align: center;">Q: Is it known if this drug is the first approved in its class, regardless of the indication or route of administration, acting on a specific target? <br> A: No.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 16</td>
<td style="text-align: center;">Q: How many hydrogen bond acceptors does this compound have? <br> A: 6 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 17</td>
<td style="text-align: center;">Q: Is this drug used for therapeutic purposes, rather than for imaging, additives, or other non-therapeutic applications? <br> A: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 18</td>
<td style="text-align: center;">Q: What is the polar surface area (PSA) value of this compound? <br> A: 107.41 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 19</td>
<td style="text-align: center;">Q: Has this approved drug been withdrawn due to toxicity reasons for all indications, populations, and doses in at least one country (not necessarily the US)? <br> A: No.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 20</td>
<td style="text-align: center;">Q: How many violations of Lipinski's Rule of Five (using HBA and HBD definitions) are there for this compound? <br> A: 0 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 21</td>
<td style="text-align: center;">Q: How many heavy (non-hydrogen) atoms does this compound have? <br> A: 14 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 22</td>
<td style="text-align: center;">Q: How many aromatic rings does this compound have? <br> A: 1 .</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">QA Pair 23</th>
<th style="text-align: center;">Q: Determine the type of availability for this drug. A: Prescription only.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">QA Pair 24</td>
<td style="text-align: center;">Q: Is it known whether this drug is taken orally? <br> A: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 25</td>
<td style="text-align: center;">Q: How many hydrogen bond acceptors are there in this compound, calculated according to Lipinski's original rules (i.e., counting N and O atoms)? <br> A: 7 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 26</td>
<td style="text-align: center;">Q: Is the drug administered in this specific form, such as a particular salt? <br> A: Yes.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 27</td>
<td style="text-align: center;">Q: Determine if this compound is a prodrug. <br> A: No.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 28</td>
<td style="text-align: center;">Q: What is the molecular weight of this compound's parent molecule? <br> A: 236.28 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 29</td>
<td style="text-align: center;">Q: How many hydrogen bond donors are there in this compound, calculated according to Lipinski's original rules (i.e., counting NH and OH groups)? <br> A: 2 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 30</td>
<td style="text-align: center;">Q: Which USAN substem can this drug or clinical candidate name be matched with? <br> A: Zolamide.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 31</td>
<td style="text-align: center;">Q: What is the definition of this compound's USAN stem? <br> A: Carbonic anhydrase inhibitors.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 32</td>
<td style="text-align: center;">Q: What is the molecular formula of this compound, including any salt that it may have? <br> A: C5H8N4O3S2.</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 33</td>
<td style="text-align: center;">Q: How many hydrogen bond donors does this compound have? <br> A: 1 .</td>
</tr>
<tr>
<td style="text-align: center;">QA Pair 34</td>
<td style="text-align: center;">Q: What is the classification of this molecule? Please respond with Small Molecule, Protein, Antibody, Oligosaccharide, Oligonucleotide, Cell, Enzyme, Gene, or Unknown. <br> A: Small molecule.</td>
</tr>
</tbody>
</table>
<h1>3.2. PubChem Drug Instruction Tuning Dataset</h1>
<p>In this section, we outline the process of constructing the PubChem Drug Instruction Tuning Dataset. The data was collected from the PubChem website ${ }^{7}$, which houses information of $66,469,244$ chemical compounds. We utilized the data version last updated on May 9, $2023^{8}$. Among these compounds, 19,319 possess drug information. After filtering out drugs lacking detailed textual descriptions, we retained 6,942 drugs.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>For each drug, we gathered its SMILES string and descriptions from databases including ChEBI, LOTUS, and YMDB. In total, we acquired 13,818 compound description summaries and curated 13,818 question-answer (QA) pairs. QA pairs for an exemplar drug in PubChem are provided in Table 3.</p>
<p>Table 3: Question-answer (QA) pairs for an exemplar drug in the PubChem Drug Instruction Tuning Dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Compound Name</th>
<th style="text-align: center;">Acetylcarnitine</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Canonical SMILES</td>
<td style="text-align: center;">$\mathrm{CC}(=\mathrm{O}) \mathrm{OC}(\mathrm{CC}(=\mathrm{O})[\mathrm{O}-]) \mathrm{C}<a href="\mathrm{C}">\mathrm{N}+</a>(\mathrm{C}) \mathrm{C}$</td>
</tr>
<tr>
<td style="text-align: center;">ChEBI Database</td>
<td style="text-align: center;">Q: Please describe this drug. <br> A: O-acetylcarnitine is an O-acylcarnitine having acetyl as the acyl substituent. <br> It has a role as a human metabolite. <br> It is functionally related to an acetic acid. <br> It is a conjugate base of an O-acetylcarnitinium.</td>
</tr>
<tr>
<td style="text-align: center;">LOTUS Database</td>
<td style="text-align: center;">Q: Please describe this drug. <br> A: Acetylcarnitine is a natural product found in Pseudo-nitzschia multistriata, Euglena gracilis, and other organisms with data available.</td>
</tr>
<tr>
<td style="text-align: center;">YMDB Database</td>
<td style="text-align: center;">Q: Please describe this drug. <br> A: L-Acetylcarnitine is a metabolite found in or produced by Saccharomyces cerevisiae.</td>
</tr>
</tbody>
</table>
<h1>4. Method</h1>
<p>An overview of DrugChat is provided in Figure 1. It takes a compound molecule graph as input and allows users to ask multi-turn questions about this compound. For each question, DrugChat generates an answer. DrugChat consists of a graph neural network (GNN), a large language model (LLM), and an adaptor between GNN and LLM. The GNN learns a representation for the compound molecule graph. The adaptor (which is a linear transformation matrix) transforms the graph representation into an LLM-compatible soft prompt vector. The LLM takes a user-question and the graph prompt vector as inputs and generates an answer. We employ a pretrained GNN from (Hu et al., 2020) and a pretrained LLM - Vicuna13b (Chiang et al., 2023). When training DrugChat, we fix the weight parameters of the GNN and LLM, and only update the adaptor's weights. Given a drug in the instruction tuning data and a question about this drug, the drug's graph is first fed into the GNN to produce a representation vector which is then fed into the adaptor to produce a prompt vector. The prompt vector and the question are fed into the LLM to generate an answer. A negative log likelihood loss between the generated answer and groundtruth answer is calculated. The adaptor is trained by minimizing this loss. Next, we introduce each component in DrugChat.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An overview of the DrugChat framework.</p>
<h1>4.1. Graph Neural Networks (GNNs)</h1>
<p>A GNN (Hu et al., 2020) utilizes the graph structure and node features to learn multiple layers of representation vectors for each node $v$ and a representation vector $h_{G}$ for the entire graph $G$. The representation of each node is updated by aggregating features from its neighboring nodes. At layer $k$, the representation $h_{v}^{k}$ of node $v$ encompasses information from nodes within the $k$-hop network neighborhood of $v$. Formally, $h_{v}^{k}$ is calculated as follows:</p>
<p>$$
h_{v}^{k}=\sigma\left(h_{v}^{k-1}, A G G\left(\left{h_{u}^{k-1}, u \in \mathcal{N}(v)\right}\right)\right)
$$</p>
<p>where $A G G$ represents a function that aggregates information from neighboring nodes. $\sigma$ is a function that combines information from the node feature of the previous layer and neighboring nodes. $\mathcal{N}(v)$ indicates all neighboring nodes for node $v$. To acquire the representation vector for the entire graph $G$, a pooling function $f$ (which is permutationinvariant, such as averaging) extracts information from all node features at the final layer $K$ :</p>
<p>$$
h_{G}=f\left(\left{h_{v}^{K}, v \in G\right}\right)
$$</p>
<h3>4.2. Large Language Models (LLMs)</h3>
<p>LLMs utilize the Transformer (Vaswani et al., 2017) decoder to model the conditional probability $p_{\theta}\left(n_{i} \mid n_{&lt;i}\right)$ for token $n_{i}$ in a language model. The Transformer decoder applies a multi-head self-attention block (Vaswani et al., 2017) to the input context tokens and uses a position-wise feedforward network to calculate the probabilities over output tokens. Given the context vector of tokens, the token generation probability is computed as follows:</p>
<p>$$
\begin{aligned}
&amp; h_{0}=N_{i-1} W_{e}+W_{p} \
&amp; h_{l}=\text { transformer_block }\left(h_{l-1}\right)(1 \leq l \leq m) \
&amp; p\left(n_{i} \mid N_{i-1}\right)=\operatorname{Softmax}\left(h_{m} W_{e}^{T}\right)
\end{aligned}
$$</p>
<p>where $N_{i-1}=\left(n_{1}, n_{2}, \ldots, n_{i-1}\right)$ denotes the context vector of tokens, $m$ refers to the number of layers, $W_{e}$ denotes the token embedding matrix, and $W_{p}$ represents the positional embedding matrix.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>2 what makes this compound unique?</p>
<p>This compound is unique because it combines both isoenzymes of AChE inhibition (donepezil and galantamine) in a single molecule, which theoretically could lead to greater synergy in AChE inhibition, resulting in better clinical outcomes.</p>
<p>3
what diseases can this compound potentially treat?</p>
<p>This compound has the potential to treat neurodegenerative disorders such as Alzheimer's disease and other dementias, as well as other conditions such as myasthenia gravis and mild cognitive impairment.
why can it treat Alzheimer's disease?</p>
<p>This compound can potentially treat Alzheimer's disease by inhibiting the breakdown of the neurotransmitter acetylcholine in the brain. Acetylcholine is involved in memory and cognition, and a deficiency in this neurotransmitter is thought to contribute to the symptoms of Alzheimer's disease. By inhibiting AChE, this compound may increase the levels of acetylcholine in the brain, which could potentially improve cognitive function in people with Alzheimer's disease.</p>
<p>2
what are the potential side effects of this compound?</p>
<p>The potential side effects of this compound are not yet known. It is important to note that all drugs have the potential to cause side effects, and it is always possible that this compound could have unexpected or unintended side effects. Further research is needed to determine the safety and efficacy of this compound in humans.</p>
<p>Figure 2: A demonstration of the DrugChat framework.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A demonstration of the DrugChat framework.</p>
<h1>4.3. Aligned Graph-Text Generation</h1>
<p>At this stage, we create a prompt for every training graph-text pair, which allows the LLM to generate descriptions from drug compound graphs. We utilize a prompt template that adheres to the conversational format of Vicuna-13b:</p>
<p>$$
\begin{aligned}
&amp; \text { Q: }&lt;\text { Graph }&gt;&lt;\text { GraphFeature }&gt;&lt;/ \text { Graph }&gt;&lt;\text { Instruction }&gt; \
&amp; \text { A: }&lt;\text { Desc }&gt;
\end{aligned}
$$</p>
<p>In this prompt, $&lt;$ GraphFeature $&gt;$ is a soft prompt that symbolizes the graph structure feature encoded by the linear projection layer. $&lt;$ Instruction $&gt;$ serves as a directive sentence, prompting the LLM to generate descriptions for the drug, such as "Describe the mechanism of this drug". During the training stage, $&lt;\operatorname{Desc}&gt;$ is populated with descriptive text from human experts to train the linear projection layer. In the testing stage, $&lt;\operatorname{Desc}&gt;$ remains empty, and the model is expected to generate descriptive text for the provided drug structure.</p>
<h2>5. Results</h2>
<p>We tested DrugChat on compound graphs that are not contained in the training data. Figure 2 and 3 show two examples. DrugChat can answer diverse multi-turn questions about compounds, such as "what makes this compound unique?", "what diseases can this compound potentially treat?". Note that these questions are not contained in the training data. We will perform a systematic quantitative evaluation by collaborating with pharmaceutical scientists.</p>
<h2>6. Conclusions, Limitations and Future Work</h2>
<p>In this report, we present the DrugChat framework, designed to answer questions and generate text descriptions for drugs from their molecular graphs. We develop the ChEMBL Drug Dataset and the PubChem Drug Dataset to train the DrugChat model. With further development and evaluation, DrugChat has the potential to enable conversational analysis of drug compounds.</p>
<p>A potential limitation of DrugChat is language hallucination. Since DrugChat incorporates an LLM module, it may occasionally produce untrustworthy answers and descriptions for drugs, which hampers its practical application in real drug discovery pipelines. If DrugChat generates seemingly convincing yet incorrect text descriptions, it could mislead human decision-makers and potentially result in adverse consequences.</p>
<p>In the future, this issue could be mitigated by utilizing higher-quality training data and implementing effective filtering strategies. Additionally, more advanced GNN encoders and LLMs will play a crucial role in addressing this challenge. As the number of users increases, human feedback can also be utilized to fine-tune the DrugChat model through reinforcement learning techniques (Ouyang et al., 2022).</p>
<h1>References</h1>
<p>Jerry Avorn. The $\$ 2.6$ billion pill-methodologic and policy considerations. N Engl J Med., 2015 .</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/ paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \%$ * chatgpt quality, March 2023. URL https://lmsys.org/blog/2023-03-30-vicuna/.</p>
<p>Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id= HJlWWJSFDH.</p>
<p>JP. Hughes, S. Rees, SB. Kalindjian, and KL. Philpott. Principles of early drug discovery. Br J Pharmacol., 2011.
G. Idakwo, S. Thangapandian, Luttrell, and J. et al. Structure-activity relationship-based chemical classification of highly imbalanced tox21 datasets. J Cheminform, 2020.</p>
<p>Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv:2304.08485, 2023.</p>
<p>Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 27730-27744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/ b1efde53be364a73914f58805a001731-Paper-Conference.pdf.
D. Paul, G. Sanap, S. Shenoy, D. Kalyane, K. Kalia, and RK. Tekade. Artificial intelligence in drug discovery and development. Drug Discov Today., 2021.</p>
<p>S. Pushpakom, F. Iorio, and P. et al. Eyers. Drug repurposing: progress, challenges and recommendations. Nat Rev Drug Discov, 2019.</p>
<p>De Rycker, B. M., Baragaña, and S.L. et al. Duce. Challenges and recent progress in drug discovery for tropical diseases. Nature, 2018.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.</p>
<p>Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.</p>
<p>Xiangxiang Zeng, Hongxin Xiang, Linhui Yu, Jianmin Wang, Kenli Li, Ruth Nussinov, and Feixiong Cheng. Accurate prediction of molecular properties and drug targets using a self-supervised image representation learning framework. Nature Machine Intelligence, 2022 .</p>
<p>Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>https://pubchem.ncbi.nlm.nih.gov/</li>
<li>https://pubchem.ncbi.nlm.nih.gov/classification/</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
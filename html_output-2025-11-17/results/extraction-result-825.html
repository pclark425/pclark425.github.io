<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-825 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-825</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-825</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-273661879</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.21741v1.pdf" target="_blank">Enhancing Financial Question Answering with a Multi-Agent Reflection Framework</a></p>
                <p><strong>Paper Abstract:</strong> While Large Language Models (LLMs) have shown impressive capabilities in numerous Natural Language Processing (NLP) tasks, they still struggle with financial question answering (QA), particularly when numerical reasoning is required. Recently, LLM-based multi-agent frameworks have demonstrated remarkable effectiveness in multi-step reasoning, which is crucial for financial QA tasks as it involves extracting relevant information from tables and text and then performing numerical reasoning on the extracted data to infer answers. In this study, we propose a multi-agent framework incorporating a critic agent that reflects on the reasoning steps and final answers for each question. Additionally, we enhance our system by adding multiple critic agents, each focusing on a specific aspect of the answer. Our results indicate that this framework significantly improves performance compared to single-agent reasoning, with an average performance increase of 15% for the LLaMA3-8B model and 5% for the LLaMA3-70B model. Furthermore, our framework performs on par with, and in some cases surpasses, larger single-agent LLMs such as LLaMA3.1-405B and GPT-4o-mini, though it falls slightly short compared to Claude-3.5 Sonnet. Overall, our framework presents an effective solution to enhance open-source LLMs for financial QA tasks, offering a cost-effective alternative to larger models like Claude-3.5 Sonnet.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e825.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e825.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Agent Reflection Framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Reflection Framework with Expert and Critic Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system-level multi-agent architecture that composes an LLM-based financial expert (chain-of-thought prompting) with one or more critic agents that reflect on and provide targeted feedback (data-extraction critic and calculation critic) to iteratively refine answers on numerical, multi-step financial QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Expert + Critic Agents (LLM core)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Multi-agent setup implemented via AutoGen: an expert/financial-analyst agent (LLM with CoT prompting) and one or two critic agents with specialized system prompts (data-extraction critic and calculations critic). A User-Proxy executor mediates messages. All agents share an LLM core (experimented with LLaMA3-8B and LLaMA3-70B).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (LLaMA3-8B, LLaMA3-70B)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>FinQA, ConvFinQA, TAT-QA (financial tabular+text numerical QA)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Improved Exact Match (EM) relative to single-agent baselines: ~+10% (LLaMA3-8B with one critic) and additional ~+5% with second critic (total ~+15% for 8B reported); LLaMA3-70B improvements smaller (~+3.8–5.2% per dataset; ~+5% average reported).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step numerical reasoning (iterative refinement / reflective multi-step QA)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>multi-agent decomposition (expert + critic(s)), reflection mechanism, chain-of-thought prompting, specialized critic roles (data-extraction critic, calculation critic), message-mediated iterative refinement (AutoGen framework).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting/in-context prompting (no fine-tuning of the LLMs for the agent interactions); temperature fixed at 0.1.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change / prompting strategy (multi-agent reflective architecture with specialized critic prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Replace or augment a single LLM prompt-driven expert with a pipeline: (1) expert produces CoT answer, (2) critic1 reviews data extraction (numbers/spans), (3) critic2 reviews calculation steps and reasoning, (4) expert re-generates answer conditioned on critic feedback. Implemented with AutoGen; critics have distinct system messages to focus critique.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>For LLaMA3-8B: adding one critic improved EM by ~10% across datasets; adding a second (split responsibilities) added another ~5% (cumulative ~15% improvement quoted). For LLaMA3-70B: critic additions yielded modest gains (3.83%, 3.89%, 5.19% on FinQA, ConvFinQA, TAT-QA respectively), reported as ~5% average improvement in abstract.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Smaller single-agent LLMs insufficiently capture domain-specific financial knowledge and produce extraction or arithmetic errors in multi-step numerical problems; multi-step reasoning and error propagation across steps amplify these weaknesses. The critic agents reduce this gap by checking extraction and calculations and guiding revision.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Financial Question Answering with a Multi-Agent Reflection Framework', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e825.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e825.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3-8B (single-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3-8B (single-agent financial expert with CoT prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An 8-billion-parameter LLaMA3 model used as the standalone financial expert agent with chain-of-thought prompting to answer hybrid tabular and textual numerical QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA3-8B (single-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open-source LLaMA3 family LLM (8B parameters) used with a specialized system prompt to act as a financial analyst; relies on chain-of-thought style outputs but not fine-tuned for the datasets in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>FinQA, ConvFinQA (subset), TAT-QA (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step numerical reasoning (financial QA)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>single LLM agent with chain-of-thought prompting; no external critics or tool interfaces.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting/in-context (no reported fine-tuning in this study).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Substantial underperformance relative to larger fine-tuned models due to limited financial knowledge, difficulty extracting numbers accurately from hybrid inputs, and arithmetic/stepwise calculation errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Financial Question Answering with a Multi-Agent Reflection Framework', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e825.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e825.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3-8B (multi-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3-8B as Expert with One or Two Critic Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of LLaMA3-8B for both the expert and critic agents in the multi-agent reflection framework; critics specialize in extraction and/or calculations and provide iterative feedback to the expert.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA3-8B (expert + critic(s))</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same LLaMA3-8B core run in multiple agent roles (financial expert and critic agents) with specialized system messages; implemented via AutoGen message passing and iterative refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>8B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>FinQA, ConvFinQA (subset), TAT-QA (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Two-agent setting: average improvement ~+10% EM across datasets over single-agent LLaMA3-8B; three-agent (two critics) setting: additional ~+5% (reported cumulative ~+15% improvement in abstract).</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step numerical reasoning (iterative reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>reflection mechanism (critic feedback loop), role-specialization of critics (data-extraction critic, calculation critic), chain-of-thought outputs from expert, AutoGen-based message orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting/in-context only (no fine-tuning reported); critics and expert are prompt-engineered via system messages.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural / prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Introduce critic agent(s) that (a) check extracted numeric spans and (b) verify arithmetic/stepwise calculations; feed those critiques back to the expert which re-generates a revised answer.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Substantial: ~+10% EM from one critic; splitting refinement across two critics provided an additional ~+5%, yielding the ~+15% improvement stated in the abstract for the 8B experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Critic agents mitigate error accumulation from poor extraction or arithmetic by explicitly checking and prompting corrections, addressing weaknesses of the smaller single-agent LLaMA3-8B.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Financial Question Answering with a Multi-Agent Reflection Framework', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e825.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e825.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3-70B (single-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3-70B (single-agent financial expert with CoT prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 70-billion-parameter LLaMA3 model used as a single expert agent with chain-of-thought prompting; shows markedly better numerical reasoning and fewer calculation/extraction errors compared to 8B.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA3-70B (single-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA3 large model (70B) used with a financial expert system prompt; not fine-tuned in this work but evaluated in single-agent and multi-agent configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>FinQA, ConvFinQA (subset), TAT-QA (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step numerical reasoning (financial QA)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>single LLM agent with chain-of-thought prompting; no critic in single-agent configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting/in-context (no fine-tuning reported for experiments here).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>While larger model size reduces extraction and arithmetic errors relative to 8B, remaining errors still allow modest gains from critic-based refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Financial Question Answering with a Multi-Agent Reflection Framework', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e825.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e825.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3-70B (multi-agent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3-70B as Expert with Critic Agent(s)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using LLaMA3-70B as the core for expert and critic agents; critic additions yield modest but consistent improvements over single-agent 70B performance on financial QA.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA3-70B (expert + critic(s))</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same 70B core instantiated as expert and critic agent(s) with specialized system messages; iterative feedback mechanism identical to 8B experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>70B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>FinQA, ConvFinQA (subset), TAT-QA (subset)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>Addition of critic agent(s) produced modest gains: +3.83% (FinQA), +3.89% (ConvFinQA), +5.19% (TAT-QA) in reported per-dataset improvements; abstract rounds to ~+5% average.</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>multi-step numerical reasoning (iterative reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>reflection mechanism, critic specialization, chain-of-thought outputs, AutoGen orchestration.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting/in-context only (no fine-tuning reported).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural / prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Same critic-based iterative refinement as for 8B; because 70B makes fewer extraction/calculation errors, scope for improvement is smaller.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Measured improvements of ~3.8–5.2% EM per dataset (FinQA, ConvFinQA, TAT-QA); overall smaller absolute gains than for 8B but consistent benefit.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Larger model capacity reduces but does not eliminate errors; critic agents still help by catching remaining incorrect steps or mis-extractions, though fewer errors limit absolute improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Financial Question Answering with a Multi-Agent Reflection Framework', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e825.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e825.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude-3.5-Sonnet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude 3.5 Sonnet (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Proprietary high-performance LLM referenced as a single-agent baseline that outperformed other single-agent models in financial QA in this study's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Claude-3.5-Sonnet (single-agent baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary large LLM (Anthropic) used as a single-agent baseline in the paper's comparative analysis (no architecture/training details provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>FinQA, ConvFinQA, TAT-QA (as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Financial Question Answering with a Multi-Agent Reflection Framework', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e825.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e825.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o-mini (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A faster/cost-effective proprietary OpenAI model used as a single-agent baseline; reported in comparisons where LLaMA3 multi-agent setups performed on par or surpassed it in some cases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-4o-mini (single-agent baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Proprietary LLM baseline; included in comparative single-agent performance table but not central to the paper's interventions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>FinQA, ConvFinQA, TAT-QA (as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Financial Question Answering with a Multi-Agent Reflection Framework', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e825.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e825.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA3.1-405B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA3.1-405B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A very large LLaMA3 family model (405B parameters) used as a single-agent baseline; reported to slightly outperform GPT-4o-mini in single-agent financial QA comparisons in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>LLaMA3.1-405B (single-agent baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large open/closed model used as baseline; included for comparative single-agent performance but not central to the experiments of this study.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>405B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>FinQA, ConvFinQA, TAT-QA (as baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Enhancing Financial Question Answering with a Multi-Agent Reflection Framework', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Reconcile: Round-table conference improves reasoning via consensus among diverse llms. <em>(Rating: 2)</em></li>
                <li>Critic: Large language models can self-correct with tool-interactive critiquing. <em>(Rating: 2)</em></li>
                <li>Improving factuality and reasoning in language models through multiagent debate <em>(Rating: 2)</em></li>
                <li>Self-Reflection in LLM Agents: Effects on Problem-Solving Performance. <em>(Rating: 2)</em></li>
                <li>React: Synergizing reasoning and acting in language models. <em>(Rating: 2)</em></li>
                <li>Toolllm: Facilitating large language models to master 16000+ real-world apis. <em>(Rating: 1)</em></li>
                <li>Husky: A Unified. (HUSKY paper referenced) <em>(Rating: 2)</em></li>
                <li>TAT-LLM: A specialized language model for discrete reasoning over tabular and textual data. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-825",
    "paper_id": "paper-273661879",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "Multi-Agent Reflection Framework",
            "name_full": "Multi-Agent Reflection Framework with Expert and Critic Agents",
            "brief_description": "A system-level multi-agent architecture that composes an LLM-based financial expert (chain-of-thought prompting) with one or more critic agents that reflect on and provide targeted feedback (data-extraction critic and calculation critic) to iteratively refine answers on numerical, multi-step financial QA.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "Expert + Critic Agents (LLM core)",
            "model_description": "Multi-agent setup implemented via AutoGen: an expert/financial-analyst agent (LLM with CoT prompting) and one or two critic agents with specialized system prompts (data-extraction critic and calculations critic). A User-Proxy executor mediates messages. All agents share an LLM core (experimented with LLaMA3-8B and LLaMA3-70B).",
            "model_size": "various (LLaMA3-8B, LLaMA3-70B)",
            "qa_task_name": "FinQA, ConvFinQA, TAT-QA (financial tabular+text numerical QA)",
            "qa_performance": "Improved Exact Match (EM) relative to single-agent baselines: ~+10% (LLaMA3-8B with one critic) and additional ~+5% with second critic (total ~+15% for 8B reported); LLaMA3-70B improvements smaller (~+3.8–5.2% per dataset; ~+5% average reported).",
            "interactive_task_name": null,
            "interactive_task_type": "multi-step numerical reasoning (iterative refinement / reflective multi-step QA)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "multi-agent decomposition (expert + critic(s)), reflection mechanism, chain-of-thought prompting, specialized critic roles (data-extraction critic, calculation critic), message-mediated iterative refinement (AutoGen framework).",
            "training_method": "prompting/in-context prompting (no fine-tuning of the LLMs for the agent interactions); temperature fixed at 0.1.",
            "intervention_type": "architectural change / prompting strategy (multi-agent reflective architecture with specialized critic prompts)",
            "intervention_description": "Replace or augment a single LLM prompt-driven expert with a pipeline: (1) expert produces CoT answer, (2) critic1 reviews data extraction (numbers/spans), (3) critic2 reviews calculation steps and reasoning, (4) expert re-generates answer conditioned on critic feedback. Implemented with AutoGen; critics have distinct system messages to focus critique.",
            "intervention_effect": "For LLaMA3-8B: adding one critic improved EM by ~10% across datasets; adding a second (split responsibilities) added another ~5% (cumulative ~15% improvement quoted). For LLaMA3-70B: critic additions yielded modest gains (3.83%, 3.89%, 5.19% on FinQA, ConvFinQA, TAT-QA respectively), reported as ~5% average improvement in abstract.",
            "hypothesized_cause_of_gap": "Smaller single-agent LLMs insufficiently capture domain-specific financial knowledge and produce extraction or arithmetic errors in multi-step numerical problems; multi-step reasoning and error propagation across steps amplify these weaknesses. The critic agents reduce this gap by checking extraction and calculations and guiding revision.",
            "uuid": "e825.0",
            "source_info": {
                "paper_title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3-8B (single-agent)",
            "name_full": "LLaMA3-8B (single-agent financial expert with CoT prompting)",
            "brief_description": "An 8-billion-parameter LLaMA3 model used as the standalone financial expert agent with chain-of-thought prompting to answer hybrid tabular and textual numerical QA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA3-8B (single-agent)",
            "model_description": "Open-source LLaMA3 family LLM (8B parameters) used with a specialized system prompt to act as a financial analyst; relies on chain-of-thought style outputs but not fine-tuned for the datasets in this work.",
            "model_size": "8B",
            "qa_task_name": "FinQA, ConvFinQA (subset), TAT-QA (subset)",
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "multi-step numerical reasoning (financial QA)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "single LLM agent with chain-of-thought prompting; no external critics or tool interfaces.",
            "training_method": "prompting/in-context (no reported fine-tuning in this study).",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Substantial underperformance relative to larger fine-tuned models due to limited financial knowledge, difficulty extracting numbers accurately from hybrid inputs, and arithmetic/stepwise calculation errors.",
            "uuid": "e825.1",
            "source_info": {
                "paper_title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3-8B (multi-agent)",
            "name_full": "LLaMA3-8B as Expert with One or Two Critic Agents",
            "brief_description": "Use of LLaMA3-8B for both the expert and critic agents in the multi-agent reflection framework; critics specialize in extraction and/or calculations and provide iterative feedback to the expert.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA3-8B (expert + critic(s))",
            "model_description": "Same LLaMA3-8B core run in multiple agent roles (financial expert and critic agents) with specialized system messages; implemented via AutoGen message passing and iterative refinement.",
            "model_size": "8B",
            "qa_task_name": "FinQA, ConvFinQA (subset), TAT-QA (subset)",
            "qa_performance": "Two-agent setting: average improvement ~+10% EM across datasets over single-agent LLaMA3-8B; three-agent (two critics) setting: additional ~+5% (reported cumulative ~+15% improvement in abstract).",
            "interactive_task_name": null,
            "interactive_task_type": "multi-step numerical reasoning (iterative reflection)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "reflection mechanism (critic feedback loop), role-specialization of critics (data-extraction critic, calculation critic), chain-of-thought outputs from expert, AutoGen-based message orchestration.",
            "training_method": "prompting/in-context only (no fine-tuning reported); critics and expert are prompt-engineered via system messages.",
            "intervention_type": "architectural / prompting strategy",
            "intervention_description": "Introduce critic agent(s) that (a) check extracted numeric spans and (b) verify arithmetic/stepwise calculations; feed those critiques back to the expert which re-generates a revised answer.",
            "intervention_effect": "Substantial: ~+10% EM from one critic; splitting refinement across two critics provided an additional ~+5%, yielding the ~+15% improvement stated in the abstract for the 8B experiments.",
            "hypothesized_cause_of_gap": "Critic agents mitigate error accumulation from poor extraction or arithmetic by explicitly checking and prompting corrections, addressing weaknesses of the smaller single-agent LLaMA3-8B.",
            "uuid": "e825.2",
            "source_info": {
                "paper_title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3-70B (single-agent)",
            "name_full": "LLaMA3-70B (single-agent financial expert with CoT prompting)",
            "brief_description": "A 70-billion-parameter LLaMA3 model used as a single expert agent with chain-of-thought prompting; shows markedly better numerical reasoning and fewer calculation/extraction errors compared to 8B.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA3-70B (single-agent)",
            "model_description": "LLaMA3 large model (70B) used with a financial expert system prompt; not fine-tuned in this work but evaluated in single-agent and multi-agent configurations.",
            "model_size": "70B",
            "qa_task_name": "FinQA, ConvFinQA (subset), TAT-QA (subset)",
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": "multi-step numerical reasoning (financial QA)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "single LLM agent with chain-of-thought prompting; no critic in single-agent configuration.",
            "training_method": "prompting/in-context (no fine-tuning reported for experiments here).",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "While larger model size reduces extraction and arithmetic errors relative to 8B, remaining errors still allow modest gains from critic-based refinement.",
            "uuid": "e825.3",
            "source_info": {
                "paper_title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3-70B (multi-agent)",
            "name_full": "LLaMA3-70B as Expert with Critic Agent(s)",
            "brief_description": "Using LLaMA3-70B as the core for expert and critic agents; critic additions yield modest but consistent improvements over single-agent 70B performance on financial QA.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "LLaMA3-70B (expert + critic(s))",
            "model_description": "Same 70B core instantiated as expert and critic agent(s) with specialized system messages; iterative feedback mechanism identical to 8B experiments.",
            "model_size": "70B",
            "qa_task_name": "FinQA, ConvFinQA (subset), TAT-QA (subset)",
            "qa_performance": "Addition of critic agent(s) produced modest gains: +3.83% (FinQA), +3.89% (ConvFinQA), +5.19% (TAT-QA) in reported per-dataset improvements; abstract rounds to ~+5% average.",
            "interactive_task_name": null,
            "interactive_task_type": "multi-step numerical reasoning (iterative reflection)",
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "reflection mechanism, critic specialization, chain-of-thought outputs, AutoGen orchestration.",
            "training_method": "prompting/in-context only (no fine-tuning reported).",
            "intervention_type": "architectural / prompting strategy",
            "intervention_description": "Same critic-based iterative refinement as for 8B; because 70B makes fewer extraction/calculation errors, scope for improvement is smaller.",
            "intervention_effect": "Measured improvements of ~3.8–5.2% EM per dataset (FinQA, ConvFinQA, TAT-QA); overall smaller absolute gains than for 8B but consistent benefit.",
            "hypothesized_cause_of_gap": "Larger model capacity reduces but does not eliminate errors; critic agents still help by catching remaining incorrect steps or mis-extractions, though fewer errors limit absolute improvement.",
            "uuid": "e825.4",
            "source_info": {
                "paper_title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Claude-3.5-Sonnet",
            "name_full": "Claude 3.5 Sonnet (Anthropic)",
            "brief_description": "Proprietary high-performance LLM referenced as a single-agent baseline that outperformed other single-agent models in financial QA in this study's comparisons.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "Claude-3.5-Sonnet (single-agent baseline)",
            "model_description": "Proprietary large LLM (Anthropic) used as a single-agent baseline in the paper's comparative analysis (no architecture/training details provided in this paper).",
            "model_size": null,
            "qa_task_name": "FinQA, ConvFinQA, TAT-QA (as baselines)",
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e825.5",
            "source_info": {
                "paper_title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o-mini",
            "name_full": "GPT-4o-mini (OpenAI)",
            "brief_description": "A faster/cost-effective proprietary OpenAI model used as a single-agent baseline; reported in comparisons where LLaMA3 multi-agent setups performed on par or surpassed it in some cases.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "GPT-4o-mini (single-agent baseline)",
            "model_description": "Proprietary LLM baseline; included in comparative single-agent performance table but not central to the paper's interventions.",
            "model_size": null,
            "qa_task_name": "FinQA, ConvFinQA, TAT-QA (as baselines)",
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e825.6",
            "source_info": {
                "paper_title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLaMA3.1-405B",
            "name_full": "LLaMA3.1-405B",
            "brief_description": "A very large LLaMA3 family model (405B parameters) used as a single-agent baseline; reported to slightly outperform GPT-4o-mini in single-agent financial QA comparisons in the paper.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_or_agent_name": "LLaMA3.1-405B (single-agent baseline)",
            "model_description": "Large open/closed model used as baseline; included for comparative single-agent performance but not central to the experiments of this study.",
            "model_size": "405B",
            "qa_task_name": "FinQA, ConvFinQA, TAT-QA (as baselines)",
            "qa_performance": null,
            "interactive_task_name": null,
            "interactive_task_type": null,
            "interactive_performance": null,
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e825.7",
            "source_info": {
                "paper_title": "Enhancing Financial Question Answering with a Multi-Agent Reflection Framework",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Reconcile: Round-table conference improves reasoning via consensus among diverse llms.",
            "rating": 2,
            "sanitized_title": "reconcile_roundtable_conference_improves_reasoning_via_consensus_among_diverse_llms"
        },
        {
            "paper_title": "Critic: Large language models can self-correct with tool-interactive critiquing.",
            "rating": 2,
            "sanitized_title": "critic_large_language_models_can_selfcorrect_with_toolinteractive_critiquing"
        },
        {
            "paper_title": "Improving factuality and reasoning in language models through multiagent debate",
            "rating": 2,
            "sanitized_title": "improving_factuality_and_reasoning_in_language_models_through_multiagent_debate"
        },
        {
            "paper_title": "Self-Reflection in LLM Agents: Effects on Problem-Solving Performance.",
            "rating": 2,
            "sanitized_title": "selfreflection_in_llm_agents_effects_on_problemsolving_performance"
        },
        {
            "paper_title": "React: Synergizing reasoning and acting in language models.",
            "rating": 2,
            "sanitized_title": "react_synergizing_reasoning_and_acting_in_language_models"
        },
        {
            "paper_title": "Toolllm: Facilitating large language models to master 16000+ real-world apis.",
            "rating": 1,
            "sanitized_title": "toolllm_facilitating_large_language_models_to_master_16000_realworld_apis"
        },
        {
            "paper_title": "Husky: A Unified. (HUSKY paper referenced)",
            "rating": 2,
            "sanitized_title": "husky_a_unified_husky_paper_referenced"
        },
        {
            "paper_title": "TAT-LLM: A specialized language model for discrete reasoning over tabular and textual data.",
            "rating": 1,
            "sanitized_title": "tatllm_a_specialized_language_model_for_discrete_reasoning_over_tabular_and_textual_data"
        }
    ],
    "cost": 0.015869249999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Enhancing Financial Question Answering with a Multi-Agent Reflection Framework
29 Oct 2024</p>
<p>Sorouralsadat Fatemi sfatem6@uic.edu 
University of Illinois at Chicago Chicago
ILUSA</p>
<p>Yuheng Hu yuhenghu@uic.edu 
University of Illinois at Chicago Chicago
ILUSA</p>
<p>Enhancing Financial Question Answering with a Multi-Agent Reflection Framework
29 Oct 2024B02214D2B1FE87389AFF76D1E2DD26B0arXiv:2410.21741v1[cs.CL]Large Language Models (LLMs)Financial Question AnsweringMulti-Agent LLMs
While Large Language Models (LLMs) have shown impressive capabilities in numerous Natural Language Processing (NLP) tasks, they still struggle with financial question answering (QA), particularly when numerical reasoning is required.Recently, LLM-based multiagent frameworks have demonstrated remarkable effectiveness in multi-step reasoning, which is crucial for financial QA tasks as it involves extracting relevant information from tables and text and then performing numerical reasoning on the extracted data to infer answers.In this study, we propose a multi-agent framework incorporating a critic agent that reflects on the reasoning steps and final answers for each question.Additionally, we enhance our system by adding multiple critic agents, each focusing on a specific aspect of the answer.Our results indicate that this framework significantly improves performance compared to single-agent reasoning, with an average performance increase of 15% for the LLaMA3-8B model and 5% for the LLaMA3-70B model.Furthermore, our framework performs on par with, and in some cases surpasses, larger singleagent LLMs such as LLaMA3.1-405Band GPT-4o-mini, though it falls slightly short compared to Claude-3.5 Sonnet.Overall, our framework presents an effective solution to enhance open-source LLMs for financial QA tasks, offering a cost-effective alternative to larger models like Claude-3.5 Sonnet.</p>
<p>Introduction</p>
<p>The analysis of financial documents, such as SEC filings, is crucial for assessing business and company performance.Analyzing these financial documents requires advanced expertise in financial knowledge, the ability to reason across both tabular and textual data sources, and the capability to perform complex numerical reasoning [27].Recent studies have explored the effectiveness of various models and approaches in comprehending financial documents through Question-Answering (QA) tasks [3,4].Financial QA datasets typically contain hybrid data, comprising both structured tabular information and unstructured textual content.This heterogeneous nature of financial documents presents unique challenges for natural language processing systems.Effectively comprehending and answering questions in such hybrid contexts requires not only an understanding of the intricate relationships between tabular data and narrative paragraphs but also the ability to perform advanced numerical reasoning.These skills are essential for tasks such as interpreting financial statements, analyzing performance metrics, and drawing insights from complex financial reports [3,4,27].The complexity of financial QA extends beyond basic data comprehension.To effectively answer each question, relevant numerical data must be accurately extracted from both textual descriptions and tabular representations.Furthermore, deriving answers often involves combining and applying various mathematical operations-including counting, sorting, comparing, and basic arithmetic (addition, subtraction, multiplication, and division) [26].This multi-step process, involving complex data interpretation and manipulation, renders financial QA tasks particularly challenging, requiring advanced numerical reasoning capabilities that go beyond traditional natural language processing techniques.</p>
<p>To address these challenges in financial QA tasks, researchers have proposed various techniques.Some studies have employed sequence tagging to extract relevant numbers from tables and pertinent spans from text, facilitating semantic inference [27].Another study proposed a hierarchical approach using multi-level graphs to model semantic relationships between quantities, dates, and text.This method aims to improve the extraction of relevant information from hybrid contexts [28].Furthermore, some researchers developed heterogeneous graphs to represent the relationships between different data types, capturing the correlation between tables and texts and aggregating them efficiently [10].While these approaches have shown promising results, their effectiveness is often constrained by the considerable complexity involved in data preprocessing and graph construction to encode numbers and texts from various data types.Moreover, the performance of these approaches is often limited by the capabilities of the underlying language models typically employed in these studies, such as encoder-only or encoder-decoder architectures.</p>
<p>Recently, Large Language Models (LLMs) have demonstrated remarkable success in various reasoning tasks, including mathematical and commonsense reasoning [25].Despite these advances, LLMs still face challenges with complex reasoning tasks [7,23,30].To address these challenges, researchers have begun developing LLM-based multi-agent frameworks capable of executing intricate multi-step decision-making and reasoning tasks [8,17,24].These frameworks have attracted considerable attention for their potential to enhance mathematical and strategic reasoning across diverse applications, such as sequential decision-making, evidence-based QA, and language reasoning [6,15,31].Notably, some of these frameworks incorporate reflective agents that employ an iterative refinement process.In this approach, the LLM agent improves its answers based on previous outputs and feedback, enabling more sophisticated reasoning for complex tasks [11,16,19,22].</p>
<p>However, to the best of our knowledge, the effectiveness of these multi-agent systems has not been explored in the context of financial QA tasks.Financial QA is an integrated task requiring arithmetic and financial knowledge, unlike other reasoning tasks that focus on a single specific ability, and thus necessitates multistep reasoning.This gap presents an opportunity to investigate how these LLM-based agents can be applied to financial QA tasks.</p>
<p>To address this research gap, we propose a multi-agent framework for financial QA tasks.Within this framework, we implement agents built upon a large language model (LLM) core and examine three settings.</p>
<p>The framework begins with an expert agent that leverages chainof-thought (CoT) prompting for data extraction from tables and text, followed by mathematical reasoning to generate answers.Building upon this foundation, the second setting adds a critic agent that analyzes the expert agent's responses to enhance reasoning in future attempts.To further improve the refinement process, we divide the task into two sub-tasks, assigning them to separate critic agents.Their reflection feedback is then passed to the first agent for re-evaluation of the initial response.To demonstrate the advantages of our multi-agent system, we conducted experiments with various agent configurations using different LLMs, including LLaMA3-8b and LLaMA3-70b, on three popular finance domain benchmarks for hybrid tabular and textual QA: FinQA, ConFinQA, and TAT-QA.Our experimental results indicate that our multi-agent framework with critic agents can enhance numerical reasoning capabilities compared to single-agent frameworks, consistently improving performance across the three datasets.Dividing refinement tasks between agents also improves performance, though by a smaller margin.Significantly, this approach achieves results comparable to state-of-the-art fine-tuned LLMs without the costs associated with dataset preparation and fine-tuning.Furthermore, it performs on par with proprietary larger models such as GPT-4o-mini and Claude-3.5-Sonnet,while circumventing their higher inference costs.</p>
<p>Related Work</p>
<p>Research on tabular and textual QA in the finance domain has primarily focused on decomposing tasks into multiple steps, generating intermediate results that guide the final answer [10,26,28].Earlier studies, constrained by the limitations of encoder-only or encoder-decoder models in QA and reasoning tasks, developed novel methods to overcome these challenges.One study proposed a semantic-parsing-based approach, substituting the prediction vocabulary with an operation set and integrating a copy mechanism into the BART model.This allowed for the retrieval of reasoningrelated numbers or text from tables and documents when decoding programs.The researchers constructed programs based on annotated derivations for training instances [26].Another study introduced a relational graph modeling method to align questions, tables, and paragraphs, framing numerical QA over hybrid table-text content as an expression tree generation task [10].A more recent study modeled semantic relationships and dependencies among questions, table cells, text paragraphs, quantities, and dates using hierarchical graphs.At the lower level, graphs were built to model value magnitude, comparison information, and semantic relations in text.The higher level captured semantic relationships and dependencies [28].While effective, these methods require extensive data preparation, such as graph or tree structures, to extract semantic relationships from hybrid financial data.</p>
<p>The emergence of LLMs has demonstrated impressive language understanding and generation capabilities [1,13,14,20].However, LLMs still struggle with multi-step and numerical reasoning tasks, encountering challenges such as hallucination and reasoning errors [23].To address these limitations, some studies have focused on further pre-training or fine-tuning smaller open-source LLMs.In the finance domain, a recent study proposed a three-step approach (Extractor, Reasoner, and Executor) to enhance LLMs' multi-step inference abilities.The researchers constructed a step-wise pipeline dataset and fine-tuned various sizes of LLaMA models (7B, 13B, and 70B) [27].While promising, this approach faces challenges related to the high computational costs and memory requirements associated with fine-tuning LLMs.More recently, inspired by the Society-of-Mind concept, multi-agent discussion frameworks such as Multi-Agent Debate (MAD) and ReConcile have emerged [2,5,6,12].These frameworks involve multiple agents powered by LLMs, engaging in discussions on given topics or tasks to improve reasoning abilities by emulating human discussion processes.This approach has shown promising results in various tasks, including problem-solving, evidence-based reasoning, and knowledge-based question-answering [18,31].</p>
<p>The efficacy of this multi-agent approach for financial QA tasks remains unexplored in current research.Our study investigates the potential of designing a multi-agent framework with critic agents built on smaller open-source LLMs to improve their numerical reasoning capabilities in financial contexts.</p>
<p>Methodology</p>
<p>Multi-agent communication enables autonomous dialogue between LLM-powered agents, each guided by specific prompts.Our research applies this paradigm to financial QA tasks, employing two</p>
<p>Financial Analyst Agent System Message</p>
<p>Critic Agent System Message You are a financial analysis agent specializing in interpreting earnings reports and financial statements.Your task is to answer specific financial questions based on the given context from financial reports.When answering questions:</p>
<p>• Carefully read and analyze the provided financial information.</p>
<p>• Extract the relevant data points needed to answer the question from the table or text provided.</p>
<p>• Perform any necessary calculations.</p>
<p>• Remember to be precise in your calculations and clear in your step-by-step explanation.Maintain a professional and objective tone in your response.</p>
<p>• Use only the information provided in the context.Do not introduce external information.</p>
<p>• Provide the answer in the unit specified in the question (million, percentage, or billion).If no unit is specified, use the most appropriate unit based on the context and question.</p>
<p>You are a reflective AI agent tasked with critically analyzing financial analyses.Your job is to review a given context, question, and the response provided by another agent.Then, you must reflect on the analysis and provide a detailed critique.Your tasks are:</p>
<p>• Carefully read the provided context, question, and response.</p>
<p>• Analyze whether the question was correctly understood and addressed.</p>
<p>• Verify if the correct numbers were extracted from tables and text in the context.Double-check these numbers against the original context.</p>
<p>• Check the accuracy of the calculations in each step provided.Recalculate each step to ensure correctness.</p>
<p>• Verify if the logic of the steps provided is sound and appropriate for answering the question.</p>
<p>• Assess if the final answer calculation is correct.Perform the calculation independently to confirm.• Question Comprehension: Does the response correctly understand the original question?</p>
<p>• Data Extraction: Are all relevant numbers accurately extracted from the provided text/tables?Focus only on these two aspects.Do not evaluate calculations or provide additional analysis.</p>
<p>You are a meticulous financial analyst and critic.Your task is to review the response provided by another agent regarding financial calculations and provide feedback on its accuracy and completeness.Pay close attention to the following aspects:</p>
<p>• Calculation Steps: Confirm that all calculation steps are correct.</p>
<p>• Calculation Accuracy: Verify the accuracy of all calculations, including intermediate and final results.</p>
<p>• Unit Consistency: Ensure the final answer's unit matches what the question requires.</p>
<p>Table 2: Critic Agent-Data Extraction System Message (Left), and Critic Agent-Calculations System Message (Right) specialized agents: a financial analyst expert and a critic.The expert interprets questions and analyzes hybrid financial data, while the critic evaluates and refines these responses.This iterative, collaborative approach enhances the accuracy of financial analysis, particularly for queries requiring advanced numerical reasoning.</p>
<p>Multi-Agent QA Framework</p>
<p>Our multi-agent QA framework experiments with three different settings:</p>
<p>Single-Agent Setting: This framework, as illustrated in Figure 1, consists of two primary components: an executor agent and a financial expert agent.The financial expert agent is built upon a large language model and is equipped with specific financial analysis capabilities through a crafted system message, effectively providing the agent with a specialized persona.This system message is shown in Table 1.Despite having two components, we categorize this as a single-agent system because it utilizes only one LLM-based agent (the financial expert) guided by the specific system prompt.</p>
<p>The process can be formalized as follows:
𝐴 = agent expert (𝑇 , 𝐷, 𝑄, 𝑃 expert )(1)
In this formulation,  represents the output generated by the agent, which includes a detailed, step-by-step response outlining the agent's thinking process, the numbers extracted from the provided data, the mathematical operations performed, and the final answer to the question.The inputs to the agent include  , which represents the input text data; , which denotes any tabular data provided; and , which is the specific question. expert refers to the system prompt.The term agent expert represents the LLM-based financial expert agent, which handles these inputs to generate the final output .</p>
<p>Two-Agent Setting: This two-agent setting, illustrated in Figure 2, enhances our framework by incorporating a critic agent, guided by a system message shown in Figure 1, to refine the analysis process.The initial response from the financial expert agent is obtained in the single-agent setting.Subsequently, the critic agent, represented by agent critic , receives the original question , input text  , tabular data , its specific prompt  critic , and the financial expert's initial answer .The critic agent then generates a reflective or critical comment , as formalized in Equation ( 2):
𝑅 = agent critic (𝑇 , 𝐷, 𝑄, 𝑃 critic , 𝐴)(2)
This comment  is then fed back to the financial expert agent, agent expert , along with its system prompt  expert .The financial expert agent uses this feedback to refine its reasoning, calculations, and analysis, producing a revised answer,  revised , as expressed in Equation ( 3):
𝐴 revised = agent expert (𝑅, 𝑃 expert )(3)
The detailed workflow of this two-agent system is illustrated in Figure 5 in Appendix A. As depicted in this figure, the fourth box demonstrates the critic agent's role in providing feedback on the steps outlined in the financial expert's initial answer.In this particular example, the critic agent identifies that the last step in the financial expert's reasoning is incorrect.Subsequently, upon receiving this feedback, the financial expert agent revises its answer, correcting the identified error and providing the accurate steps along with the final answer.This example underscores the effectiveness of the iterative refinement process, demonstrating how the interplay between the critic and financial expert agents leads to more accurate answers.</p>
<p>Three-Agent Setting: In this configuration, as shown in Figure 3, we leverage two specialized critic agents, each focusing on refining specific aspects of the response.The refinement process is divided into two sub-tasks, with each critic agent specializing in one aspect.Both critic agents have their specific system messages, as depicted in Figure 2.</p>
<p>The process is formalized as follows:
𝑅 1 = agent critic1 (𝑇 , 𝐷, 𝑄, 𝑃 critic1 , 𝐴)(4)𝑅 2 = agent critic2 (𝑇 , 𝐷, 𝑄, 𝑃 critic2 , 𝐴, 𝑅 1 )(5)𝐴 revised = agent expert (𝑅 1 , 𝑅 2 , 𝑃 expert )(6)
The first critic agent, represented by agent critic1 , is provided with the tabular data , text data  , the question , and the initial answer  from the financial expert agent.This agent is specifically prompted by  critic1 to give feedback  1 on the numbers extracted to answer the question.</p>
<p>Following this, the context, along with the feedback  1 from the first critic agent, is passed to the second critic agent, agent critic2 .This agent, guided by its prompt  critic2 , reviews the calculation steps and reasoning process used to derive the final answer, producing feedback  2 .</p>
<p>Lastly, both refinement messages  1 and  2 are passed to the financial expert agent, agent expert , which then refines its answer based on this feedback, producing  revised .This multi-layered critique process allows for a more comprehensive review of both the data extraction and the reasoning steps, potentially leading to more accurate responses.</p>
<p>Implementation Details</p>
<p>All multi-agent systems in our study are implemented using the AutoGen framework [21], an open-source system for multi-agent development.The executor agent is constructed using the User-ProxyAgent class from AutoGen, while the financial expert and critic agents are implemented using the AssistantAgent class.Our study focuses on improving the numerical reasoning capabilities of open-source LLMs through collaborative agent interactions.In line with this objective, we equip the financial expert and critic agents with LLMs and experiment with two settings.In the first setting, we use LLaMA3-8B for both the financial expert and critic agents.To investigate the effect of model size, we conduct a second set of experiments using the larger LLaMA3-70B model.For all experiments, the temperature parameter is set to 0.1 to ensure consistent outputs from the models.This experimental setup allows us to evaluate the impact of refinement and specialized critique on the quality and accuracy of financial analysis and question-answering tasks.</p>
<p>Experiments</p>
<p>To validate the effectiveness of our proposed multi-agent framework, we conduct comprehensive experiments comparing it against several state-of-the-art models.</p>
<p>Datasets and Evaluation Metrics</p>
<p>We apply the proposed framework to three popular tabular and textual QA datasets that require numerical reasoning.These datasets are:</p>
<p>• FinQA [3]: An expert-annotated dataset for financial QA, featuring tabular and textual data from financial reports.It tests numerical reasoning skills including addition, subtraction, multiplication, division, and numerical comparison.We use the test set, which comprises 1,147 questions.1 • ConvFinQA [4]: A dataset derived from FinQA, designed to simulate conversation flow.It employs a framework for decomposing and concatenating of multi-hop questions, creating a more interactive and dialogue-like structure for financial QA tasks.We employ a subset of the dataset comprising 800 questions.2 • TAT-QA [27]: A dataset built from tables and paragraphs extracted from financial reports.While the full test dataset contains 1,669 questions, we focus on the subset requiring numerical reasoning.We use 731 questions (approximately 40% of the total) that specifically demand numerical reasoning. 3he subsets of datasets utilized in this study are accessible through our Hugging Face repository. 4 For performance evaluation, we employ the Exact Match (EM) metric.To optimize computational resources, critic agents only reassessed questions incorrectly answered in the single-agent setting.The final performance score for critic agents combines the single agent's correct answers with additional correct answers from the critic agents [16].</p>
<p>Baseline Methods</p>
<p>We compare the performance of our multi-agent QA framework, which incorporates smaller LLMs (LLaMA3-8B and LLaMA3-70B), against state-of-the-art fine-tuned LLMs on tabular and textual QA datasets.Our comparison includes:</p>
<p>• TAT-LLM [29]: Fine-tuned LLaMA-13B and LLaMA-70B models on a combination of tabular QA datasets.We report the results of the best-performing model.• HUSKY [9]: An approach that pre-trains an action generator to iteratively predict high-level steps and associated tools for solving tasks across different domains.It then fine-tunes base language models with high-quality training data to integrate highly expert models.</p>
<p>Additionally, we contrast our framework with proprietary and larger LLMs, including GPT-4o-mini (one of the fastest and most cost-effective proprietary models), Claude 3.5 Sonnet, and LLaMA3.1-405B in a single-agent setting.This comparative analysis aims to demonstrate the effectiveness of our multi-agent framework, particularly highlighting the impact of the critic agent in improving multi-step reasoning.3: Performance comparison of Fine-Tuned TAT-LLM, HUSKY-LLM (results from respective papers [9,29]), and LLaMA models (LLaMA3-8B and LLaMA3-70B) in Single-Agent, Two-Agent, and Three-Agent configurations.Includes multi-agent models (with performance gains over single-agent setup shown in green), as well as single-agent performance of Claude 3.5 Sonnet, LLaMA3.1-405B, and GPT-4-mini.Our results span across the FinQA, ConvFinQA, and TAT-QA datasets.</p>
<p>Main Results</p>
<p>Table 3 and Figure 4 summarize experimental outcomes.Our analysis reveals several key findings:</p>
<p>Single-Agent Performance: The single-agent system utilizing LLaMA3-8B substantially underperformed compared to the TAT-LLM-70B model on the FinQA dataset (Note: For TAT-QA, we employed a subset of the dataset and therefore cannot report comprehensive results).This discrepancy highlights the limitations of smaller LLMs in financial QA tasks requiring numerical reasoning.Our examination of the responses indicated that the model often lacked sufficient financial knowledge to accurately interpret questions.In some instances, while it successfully extracted correct numerical data from texts and tables, it struggled to execute division operations accurately.</p>
<p>Impact of Critic Agent: As depicted in Table 3 and Figure 4, incorporating a critic agent significantly enhanced performance, yielding an average improvement of 10% across all datasets for the LLaMA3-8B model.This substantial gain underscores the efficacy of our approach in enhancing the capabilities of smaller open-source LLMs in financial QA tasks.Upon examining the critic agent's responses, we observed its ability to identify incorrect steps or miscalculations in deriving answers.However, when the initial question interpretation was inaccurate, the critic agent struggled to provide constructive feedback.The addition of a second critic agent further improved performance, resulting in an average margin increase of 5% across models.</p>
<p>Effect of Model Size: In the single-agent configuration, LLaMA3-70B significantly outperformed the 8B version, even surpassing the three-agent setup on the ConvFinQA dataset.This superiority suggests enhanced numerical reasoning capabilities in larger models.Despite this improvement, LLaMA3-70B still encountered challenges in comprehending questions and accurately extracting numerical data from tables, though these issues were less prevalent compared to LLaMA3-8B.The larger model also exhibited fewer calculation errors.</p>
<p>The introduction of a critic agent to the LLaMA3-70B model yielded more modest improvements of 3.83%, 3.89%, and 5.19% for the FinQA, ConvFinQA, and TAT-QA datasets, respectively.This smaller enhancement can be attributed to the model's inherently lower rate of calculation errors, which limits the scope for improvement through error correction.</p>
<p>Comparison with Larger LLMs: Among the single-agent performances of LLaMA3.1-405B,GPT-4o-mini, and Claude 3.5 Sonnet, the Sonnet model significantly outperformed its counterparts, showcasing impressive logical and numerical reasoning abilities in financial question comprehension and answer derivation.Notably, the LLaMA3.1-405Bmodel marginally outperformed the GPT-4omini model, possibly due to more comprehensive training data that may have included financial information.Remarkably, our framework, particularly the three-agent LLaMA3-70B setting, performed comparably to or even surpassed larger and proprietary LLMs.This outcome highlights the effectiveness of our refinement process in enhancing smaller LLMs' reasoning and calculation capabilities for financial QA tasks.</p>
<p>Performance Across Datasets: We observed superior performance on ConvFinQA, especially for LLaMA3-70B, LLaMA3.1-405B,GPT-4o-mini, and Claude 3.5 Sonnet models.This dataset presents each intermediate step as a separate question, with answers provided for multi-step problems.This format minimizes the risk of incorrect information extraction from texts or tables, leading to higher overall performance compared to the FinQA dataset.A similar trend was noted for TAT-QA, which includes simpler questions, some requiring extraction of only a single number from a table without complex calculations.</p>
<p>Conclusion: Our results demonstrate the efficacy of the proposed framework in significantly enhancing the performance of smaller LLMs.The framework's output surpassed state-of-the-art fine-tuned LLMs, illustrating how our LLM-based agent approach can rival extensively trained models without the need for substantial computational resources.Furthermore, the results were comparable to, though slightly below, powerful LLMs like Claude 3.5 Sonnet.This suggests that our method could potentially reduce reliance on costly API calls to these advanced proprietary models while maintaining competitive performance in financial QA tasks.</p>
<p>Conclusions and Future Work</p>
<p>In this study, we introduced a multi-agent framework for financial QA tasks, demonstrating its effectiveness in enhancing the numerical reasoning capabilities of smaller LLMs over tabular and textual data.Our approach, which incorporates a critic agent for refinement, significantly improved the accuracy of smaller LLMs.Notably, our method achieved performance comparable to state-of-the-art fine-tuned models without the need for extensive computational resources for training.Although it slightly underperformed compared to the most advanced models like Claude 3.5 Sonnet, our framework offers a cost-effective alternative for tackling complex financial QA tasks, potentially reducing reliance on costly API calls to advanced LLMs.In future work, we aim to develop a multi-step expert agent for extracting and performing calculation steps, allowing us to observe the impact of multiple agents in deriving answers.Additionally, we plan to experiment with an iterative (multi-turn) refinement process, similar to a debate-like discussion, to examine the performance of a debate framework as explored in previous studies [11].</p>
<p>Figure 1 :
1
Figure 1: Single-Agent Setting</p>
<p>Figure 2 :
2
Figure 2: Two-Agent Setting</p>
<p>Figure 3 :
3
Figure 3: Three-Agent Setting</p>
<p>Figure 4 :
4
Figure 4: Performance Comparison of Single-Agent, Two-Agent, and Three-Agent Settings Across FinQA, ConvFinQA, and TAT-QA Datasets Using LLaMA3-8B and LLaMA3-70B Models</p>
<p>Figure 5 :
5
Figure 5: Workflow of the Two-Agent Setting with Refinement Process</p>
<p>Table 1 :
1
System Message for Financial Analyst Agent (Left), and System Message for Critic Agent (Right)
Critic Agent-</p>
<p>Data Extraction System Message Critic Agent-Calculations System Message</p>
<p>You are a meticulous financial analyst and critic.Your task is to review the response provided by another agent regarding financial calculations and provide feedback on its accuracy and completeness.Pay close attention to the following aspects: Review the given response for:</p>
<p>The dataset was obtained from https://huggingface.co/TheFinAI
The dataset was obtained from https://huggingface.co/FinGPT
The dataset was obtained from https://huggingface.co/TheFinAI
https://huggingface.co/Sorour
A AppendixThe image demonstrates the interaction between a financial expert agent and a critic agent.The workflow begins with the financial expert agent providing an initial answer to a given financial question.The critic agent then reviews this response, offering detailed feedback on the accuracy of calculations and the interpretation of financial data.Finally, the financial expert agent uses this critique to refine its answer, correcting any identified errors and improving the overall response.This iterative process showcases how the multi-agent framework enhances the accuracy of financial QA tasks, particularly in handling complex numerical reasoning and data interpretation challenges.
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, arXiv:2303.12712Sparks of artificial general intelligence: Early experiments with gpt-4. 2023. 2023arXiv preprint</p>
<p>Justin Chih, -Yao Chen, arXiv:2309.13007Swarnadeep Saha, and Mohit Bansal. 2023. Reconcile: Round-table conference improves reasoning via consensus among diverse llms. 2023arXiv preprint</p>
<p>Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, arXiv:2109.00122Finqa: A dataset of numerical reasoning over financial data. 2021. 2021arXiv preprint</p>
<p>Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, William Yang, Wang , arXiv:2210.038492022. 2022arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021. 2021arXiv preprint</p>
<p>Improving factuality and reasoning in language models through multiagent debate. Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, Igor Mordatch, arXiv:2305.143252023. 2023arXiv preprint</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, Weizhu Chen, arXiv:2305.117382023. 2023arXiv preprint</p>
<p>Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, Zhiting Hu, arXiv:2305.14992Reasoning with language model is planning with world model. 2023. 2023arXiv preprint</p>
<p>Husky: A Unified. Joongwon Kim, Bhargavi Paranjape, Tushar Khot, Hannaneh Hajishirzi, arXiv:2406.064692024. 2024Open-Source Language Agent for Multi-Step Reasoning. arXiv preprint</p>
<p>Answering numerical reasoning questions in table-text hybrid contents with graph-based encoder and tree-based decoder. Fangyu Lei, Shizhu He, Xiang Li, Jun Zhao, Kang Liu, arXiv:2209.076922022. 2022arXiv preprint</p>
<p>Interpreting and Mitigating Hallucination in MLLMs through Multi-agent Debate. Zheng Lin, Zhenxing Niu, Zhibin Wang, Yinghui Xu, arXiv:2407.205052024. 2024arXiv preprint</p>
<p>Society of mind. Marvin Minsky, 1988Simon and Schuster</p>
<p>Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023. 2023arXiv preprint</p>
<p>Matthew Renze, Erhan Guven, arXiv:2405.06682Self-Reflection in LLM Agents: Effects on Problem-Solving Performance. 2024. 2024arXiv preprint</p>
<p>Auto-GPT: an autonomous GPT-4 experiment. Toran Bruce, Richards , 2023. 2023. 2023</p>
<p>Towards faithful and robust llm specialists for evidence-based questionanswering. Tobias Schimanski, Jingwei Ni, Mathias Kraus, Elliott Ash, Markus Leippold, arXiv:2402.082772024. 2024arXiv preprint</p>
<p>Reflexion: Language agents with verbal reinforcement learning. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, Shunyu Yao, Advances in Neural Information Processing Systems. 362024. 2024</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Azhar, arXiv:2302.13971Llama: Open and efficient foundation language models. 2023. 2023arXiv preprint</p>
<p>Autogen: Enabling next-gen llm applications via multi-agent conversation framework. Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, Chi Wang, arXiv:2308.081552023. 2023arXiv preprint</p>
<p>arXiv:2401.05799Frank Xing. 2024. Designing Heterogeneous LLM Agents for Financial Sentiment Analysis. 2024arXiv preprint</p>
<p>Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, arXiv:2306.09841Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. Jun Liu, and Erik Cambria. 2023. 2023arXiv preprint</p>
<p>Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao, arXiv:2210.03629React: Synergizing reasoning and acting in language models. 2022. 2022arXiv preprint</p>
<p>Kun Wayne Xin Zhao, Junyi Zhou, Tianyi Li, Xiaolei Tang, Yupeng Wang, Yingqian Hou, Beichen Min, Junjie Zhang, Zican Zhang, Dong, arXiv:2303.18223A survey of large language models. 2023. 2023arXiv preprint</p>
<p>Yongwei Zhou, Junwei Bao, Chaoqun Duan, Youzheng Wu, Xiaodong He, Tiejun Zhao, arXiv:2210.08249Unirpg: Unified discrete reasoning over table and text as program generation. 2022. 2022arXiv preprint</p>
<p>TAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, Tat-Seng Chua, arXiv:2105.076242021. 2021arXiv preprint</p>
<p>Soargraph: Numerical reasoning over financial table-text data via semanticoriented hierarchical graphs. Fengbin Zhu, Moxin Li, Junbin Xiao, Fuli Feng, Chao Wang, Tat Seng, Chua , Companion Proceedings of the ACM Web Conference 2023. 2023</p>
<p>Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, Tat-Seng Chua, arXiv:2401.13223Tat-llm: A specialized language model for discrete reasoning over tabular and textual data. 2024. 2024arXiv preprint</p>
<p>Xinyu Zhu, Junjie Wang, Lin Zhang, Yuxiang Zhang, Yongfeng Huang, Ruyi Gan, Jiaxing Zhang, Yujiu Yang, arXiv:2210.16257Solving math word problems via cooperative reasoning induced language models. 2022. 2022arXiv preprint</p>
<p>Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Yueting Zhuang, arXiv:2402.14320Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering. 2024. 2024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2231 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2231</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2231</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-61.html">extraction-schema-61</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <p><strong>Paper ID:</strong> paper-281410801</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2509.16058v1.pdf" target="_blank">Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers</a></p>
                <p><strong>Paper Abstract:</strong> Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition. Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources. Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks. Our initial experiments focused on embedding the ASAC module within transformer architectures. This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management. By explicitly modeling attention allocation, our approach aims to enhance system efficiency. We demonstrate ASAC's effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process. Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning. Furthermore, we have demonstrated the model's robustness and generalization capabilities across noisy and out-of-distribution datasets. In addition, we have showcased improved performance in multi-task settings. Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples. These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2231.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2231.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASAC</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Attention Schema-based Attention Control</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A transformer-integrated attention controller that inserts a VQVAE over scaled dot-product attention to compress attention scores into discrete codebook vectors (attention schemas) enabling task-aligned, discrete attention patterns and dynamic allocation of attention resources.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ASAC (Attention Schema-based Attention Control)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A module (VQVAE) placed inside transformer attention that encodes the scaled dot-product attention scores into discrete codebook indices and decodes reconstructed attention; integrates residual connection to original scores and optimizes a composite loss (task loss + reconstruction + VQ loss). Designed to abstract, manipulate and predict attention allocation to enable task-specific attention schemas and dynamic resource allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Vector-Quantized Variational Autoencoder (VQVAE) applied to attention scores producing discrete codebook indices (attention schemas) which are used to reconstruct/modify attention; residual addition to original attention.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Vision classification, multi-task learning, adversarial robustness, few-shot & transfer learning, and NLP (GLUE fine-tuning on DistilBERT)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>Examples reported in paper: Triangles (with ASAC integrated into ViT across layers) 96.71 ± 1.28 accuracy (ASAC) vs baseline 78.66 ± 0.83; Sort-of-Clevr: ASAC 67.08 ± 2.84 vs baseline 63.15 ± 7.39; MNIST: ASAC ≈97.17 ± 0.13 vs baseline 96.78 ± 0.49; CIFAR10-SVHN mixed multi-task: ASAC 86.58 ± 0.64 vs baseline 85.94 ± 1.72. In GLUE (DistilBERT+ASAC last layer): SST-2 0.9052 vs baseline 0.8986 (p=0.0001), RTE 0.6238 vs 0.5870 (p=0.0002), QNLI 0.8895 vs 0.8850 (p=0.0105); other GLUE tasks show small improvements or non-significant changes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Vision transformer baseline (no ASAC): Triangles 78.66 ± 0.83; SOC 63.15 ± 7.39; MNIST 96.78 ± 0.49; CIFAR10-SVHN 85.94 ± 1.72. DistilBERT baseline (last-layer random init) GLUE: SST-2 0.8986, RTE 0.5870, QNLI 0.8850 (see paper Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Authors state total parameter counts of baseline and ASAC are 'almost equal' across vision experiments; no FLOPs, latency, or memory measurements reported. Training used NVIDIA V100 16GB. (explicit FLOPs/time not reported)</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Same as above: parameter count nearly equal; no measured FLOPs/inference-time reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Reported that ASAC 'consistently outperforms the baseline across all datasets and varying data percentages' in learning-efficiency experiments (Figure 11) and 'accelerates learning' reaching higher accuracy earlier (e.g., Places365 faster early learning), but no exact sample-count multipliers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Transfer learning: ASAC 'consistently outperforms the baseline' when pre-training on one dataset and fine-tuning on another (Figure 9). OOD/generalization: ASAC shows higher accuracies on corrupted CIFAR datasets and marginal increase on Tiny-ImageNet-C, with 'significant improvement' on Triangles-OOD and Polygons-OOD. Adversarial: under PGD (iterative) attacks ASAC consistently outperforms baseline across datasets and epsilons; for FGSM (single-step) results are mixed (better on FashionMNIST, worse on CIFAR10 across epsilons; better at low epsilons on CIFAR100 and SVHN).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Codebook/code-usage analysis: for CIFAR10-SVHN multi-task experiment, KS test p-value = 0.0052 implying statistically different code usage between tasks (evidence of task-specific code allocation). For ODIR-5K, pairwise KS p-values mostly > 0.7 indicating code reuse across similar tasks. These analyses support interpretable, task-aligned code usage patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Multi-task experiments (Table 1) show ASAC variants outperform baseline on several datasets. Best-performing variant differs by dataset: e.g., TaskID in input worked best for ODIR-5K, Triangles and CIFAR10-SVHN; TaskID to decoder worked best for Sort-of-Clevr; TaskID in both input & decoder best for MNIST and Places365. Overall ASAC (and its task-conditioned variants) improved multi-task adaptability.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Authors note resource constraints prevented multiple seeds/hyperparameter sweeps for large datasets (Places365). No explicit experiments under limited compute or budget are reported beyond noting parameter counts are similar to baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Inserting a discrete VQVAE attention controller (ASAC) produces task-aligned, discrete attention schemas that improve accuracy, accelerate learning, and improve robustness and transfer/generalization in many but not all tested settings, while keeping parameter counts similar to the baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>ASAC's empirical improvements, task-conditional variants' benefits, codebook usage differences (KS test), and better OOD/transfer behaviour in many cases provide evidence that task-aligned, discretized attention abstractions can improve performance and generalization compared to uniform attention.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2231.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2231.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ViT baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vision Transformer (baseline, no ASAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Standard vision-transformer architecture used as the paper's uniform-representation baseline; attention blocks produce continuous attention weights learned during training without explicit discrete codebook abstraction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Vision Transformer (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based image classifier (ViT-style) without an attention-schema/VQVAE controller; uses continuous scaled dot-product multi-head attention trained end-to-end.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>uniform/fixed learned attention weights (standard transformer attention), no discrete task-aligned abstraction</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Vision classification, multi-task settings used in experiments (Triangles, Polygons, Sort-of-Clevr, CIFAR, Tiny-ImageNet, Places365, CelebA, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>Reported baseline results: Triangles 78.66 ± 0.83 accuracy; Sort-of-Clevr 63.15 ± 7.39; MNIST 96.78 ± 0.49; CIFAR10-SVHN 85.94 ± 1.72; Places365 60.46 (other metrics in Table 1/figures).</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td>Parameter count nearly equal to ASAC (authors state total parameters 'almost equal'); no FLOPs/latency reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Baseline is the comparator in learning-efficiency experiments; ASAC is reported to learn faster and achieve higher accuracy with fewer samples than this baseline (paper-level claim), but baseline-specific sample-efficiency numbers are not enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Baseline performance on transfer, OOD, and adversarial tests is reported as the comparison point; ASAC often outperforms baseline (see ASAC entry) but specific baseline-only generalization numbers are the ones reported in figures and tables.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>Baseline generally underperforms ASAC and task-conditioned ASAC variants on many multi-task datasets (see Table 1), but in some tasks differences are small.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Baseline vision transformer with uniform attention provides a reference point; adding a discrete attention-schema (ASAC) typically improves accuracy, robustness, and learning speed while keeping parameter count similar.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>neutral</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>Baseline represents the uniform-representation strategy; comparisons show ASAC often outperforms it, but the baseline itself offers no task-aligned abstraction.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2231.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2231.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ASAC-DistilBERT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DistilBERT fine-tuned with ASAC module in last layer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A small NLP setup where the ASAC VQVAE module is integrated only into the last layer of a pre-trained DistilBERT to avoid disrupting pre-trained weights; fine-tuned on GLUE tasks to test whether attention-schema benefits transfer to NLP.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DistilBERT + ASAC (last layer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Pre-trained DistilBERT with randomly initialized last layer; ASAC (VQVAE attention controller) is inserted only in the last transformer layer and fine-tuned together (other pre-trained layers kept), to test improvements on GLUE benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>DistilBERT (~66M typical) for base; ASAC module parameter count not separately reported; overall model-size reported as 'almost equal' to baseline in vision experiments but explicit parameter breakdown for DistilBERT+ASAC not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>VQVAE over attention scores in the last transformer layer producing discrete attention schemas; task abstraction localized to final layer</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Natural language understanding (GLUE benchmark tasks: STS-B, SST-2, QNLI, MRPC, QQP, RTE, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td>GLUE task examples (aggregate results across 5 seeds reported): SST-2 accuracy 0.9052 (ASAC) vs baseline 0.8986 (p=0.0001); RTE accuracy 0.6238 vs baseline 0.5870 (p=0.0002); QNLI 0.8895 vs 0.8850 (p=0.0105); MRPC and STS-B show small improvements in reported metrics; some tasks had non-significant p-values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td>DistilBERT baseline (same experiment conditions): SST-2 0.8986 ± 0.0102; RTE 0.5870 ± 0.0126; QNLI 0.8850 ± 0.0019; other GLUE numbers in Table 2.</td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>No FLOPs/latency reported; authors note difficulty of integrating VQVAE into many pre-trained layers so they limited to last layer to avoid instability; no explicit compute-cost penalty numbers provided.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Not explicitly quantified; authors report ASAC outperforms baseline in GLUE fine-tuning across seeds, implying improved fine-tuning efficiency in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>Not directly applicable beyond GLUE fine-tuning; the ASAC integration improved downstream GLUE task performance in several tasks indicating beneficial transfer/fine-tuning behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>GLUE is a suite of tasks; ASAC produced consistent improvements on several tasks but not universally statistically significant; indicates potential for task-aligned attention schemas to help across multiple language tasks when added carefully.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td>Authors explicitly limited ASAC to last layer due to practical constraints in training pre-trained models with added untrained modules; no further resource-constrained performance experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Integrating ASAC into the last layer of a pre-trained DistilBERT yielded consistent improvements on several GLUE tasks, suggesting the attention-schema mechanism can transfer to NLP when carefully integrated.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>ASAC's positive GLUE results indicate task-aligned discrete attention abstractions can improve downstream task performance in NLP fine-tuning scenarios, consistent with the principle.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2231.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2231.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems, models, or agents that use task-specific, adaptive, or dynamically allocated representations versus uniform representations, including performance comparisons, computational efficiency, and generalization results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VQVAE attention controller</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vector-Quantized Variational AutoEncoder attention controller (used in ASAC)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A VQVAE module applied to transformer scaled dot-product attention scores; encoder maps attention into latent vectors, quantized to nearest codebook embedding, decoder reconstructs attention scores; losses include reconstruction, VQ (codebook + commitment), and task loss.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VQVAE (as attention controller)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-linear-layer encoder and decoder with LeakyReLU activations, discrete learnable codebook updated via EMA; produces discrete latent codes that act as attention schemas to compress and stabilize attention patterns and enable task-conditioned behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uses_task_aligned_abstraction</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>abstraction_mechanism</strong></td>
                            <td>Vector quantization into a discrete codebook (neural discrete representations) that maps attention patterns to a finite set of learned prototypes; codebook indices serve as task-aligned abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>is_dynamic_or_adaptive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Used within vision transformers and one NLP (DistilBERT) experiment as attention controller; supports multi-task conditioning via inclusion of TaskID in inputs/decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_task_aligned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_uniform_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_direct_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_task_aligned</strong></td>
                            <td>Authors report parameter count parity with baseline; codebook adds discrete latent parameters but overall model size reported similar; no FLOPs/latency given.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_efficiency_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_results</strong></td>
                            <td>Implicit: contributes to reported faster learning and better few-shot/transfer performance in ASAC experiments, but no isolated VQVAE-only sample-efficiency numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_generalization_results</strong></td>
                            <td>As part of ASAC, VQVAE-enabled models show improved transfer and OOD robustness in many experiments (see ASAC entry).</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_results</strong></td>
                            <td>Codebook usage analysis (KS-tests) demonstrates the VQVAE codebook exhibits task-specific usage distributions in multi-task settings (e.g., CIFAR10-SVHN p=0.0052), providing interpretable discrete attention schemas.</td>
                        </tr>
                        <tr>
                            <td><strong>multi_task_performance</strong></td>
                            <td>As the core mechanism of ASAC, VQVAE supports task-conditioned variants; different placements of TaskID (input, decoder, both) impact multi-task results (Table 1), indicating the VQVAE facilitates task-specific adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_constrained_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_finding_summary</strong></td>
                            <td>Vector quantization of attention scores yields discrete attention schemas that enable task-specific code usage, improved robustness/generalization, and faster learning when integrated into transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory</strong></td>
                            <td>supports</td>
                        </tr>
                        <tr>
                            <td><strong>supports_or_challenges_theory_explanation</strong></td>
                            <td>VQVAE-mediated discretization produces evidence of task-aligned code usage and associated performance gains, supporting the idea that discrete, task-aware abstractions help allocation of representation resources.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural discrete representation learning <em>(Rating: 2)</em></li>
                <li>Attention schema in neural agents <em>(Rating: 2)</em></li>
                <li>Semi-supervised multimodal representation learning through a global workspace <em>(Rating: 1)</em></li>
                <li>Flexible parameter sharing networks <em>(Rating: 1)</em></li>
                <li>The attention schema theory: A foundation for engineering artificial consciousness <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2231",
    "paper_id": "paper-281410801",
    "extraction_schema_id": "extraction-schema-61",
    "extracted_data": [
        {
            "name_short": "ASAC",
            "name_full": "Attention Schema-based Attention Control",
            "brief_description": "A transformer-integrated attention controller that inserts a VQVAE over scaled dot-product attention to compress attention scores into discrete codebook vectors (attention schemas) enabling task-aligned, discrete attention patterns and dynamic allocation of attention resources.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ASAC (Attention Schema-based Attention Control)",
            "model_description": "A module (VQVAE) placed inside transformer attention that encodes the scaled dot-product attention scores into discrete codebook indices and decodes reconstructed attention; integrates residual connection to original scores and optimizes a composite loss (task loss + reconstruction + VQ loss). Designed to abstract, manipulate and predict attention allocation to enable task-specific attention schemas and dynamic resource allocation.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Vector-Quantized Variational Autoencoder (VQVAE) applied to attention scores producing discrete codebook indices (attention schemas) which are used to reconstruct/modify attention; residual addition to original attention.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Vision classification, multi-task learning, adversarial robustness, few-shot & transfer learning, and NLP (GLUE fine-tuning on DistilBERT)",
            "performance_task_aligned": "Examples reported in paper: Triangles (with ASAC integrated into ViT across layers) 96.71 ± 1.28 accuracy (ASAC) vs baseline 78.66 ± 0.83; Sort-of-Clevr: ASAC 67.08 ± 2.84 vs baseline 63.15 ± 7.39; MNIST: ASAC ≈97.17 ± 0.13 vs baseline 96.78 ± 0.49; CIFAR10-SVHN mixed multi-task: ASAC 86.58 ± 0.64 vs baseline 85.94 ± 1.72. In GLUE (DistilBERT+ASAC last layer): SST-2 0.9052 vs baseline 0.8986 (p=0.0001), RTE 0.6238 vs 0.5870 (p=0.0002), QNLI 0.8895 vs 0.8850 (p=0.0105); other GLUE tasks show small improvements or non-significant changes.",
            "performance_uniform_baseline": "Vision transformer baseline (no ASAC): Triangles 78.66 ± 0.83; SOC 63.15 ± 7.39; MNIST 96.78 ± 0.49; CIFAR10-SVHN 85.94 ± 1.72. DistilBERT baseline (last-layer random init) GLUE: SST-2 0.8986, RTE 0.5870, QNLI 0.8850 (see paper Table 2).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "Authors state total parameter counts of baseline and ASAC are 'almost equal' across vision experiments; no FLOPs, latency, or memory measurements reported. Training used NVIDIA V100 16GB. (explicit FLOPs/time not reported)",
            "computational_efficiency_baseline": "Same as above: parameter count nearly equal; no measured FLOPs/inference-time reported.",
            "sample_efficiency_results": "Reported that ASAC 'consistently outperforms the baseline across all datasets and varying data percentages' in learning-efficiency experiments (Figure 11) and 'accelerates learning' reaching higher accuracy earlier (e.g., Places365 faster early learning), but no exact sample-count multipliers provided.",
            "transfer_generalization_results": "Transfer learning: ASAC 'consistently outperforms the baseline' when pre-training on one dataset and fine-tuning on another (Figure 9). OOD/generalization: ASAC shows higher accuracies on corrupted CIFAR datasets and marginal increase on Tiny-ImageNet-C, with 'significant improvement' on Triangles-OOD and Polygons-OOD. Adversarial: under PGD (iterative) attacks ASAC consistently outperforms baseline across datasets and epsilons; for FGSM (single-step) results are mixed (better on FashionMNIST, worse on CIFAR10 across epsilons; better at low epsilons on CIFAR100 and SVHN).",
            "interpretability_results": "Codebook/code-usage analysis: for CIFAR10-SVHN multi-task experiment, KS test p-value = 0.0052 implying statistically different code usage between tasks (evidence of task-specific code allocation). For ODIR-5K, pairwise KS p-values mostly &gt; 0.7 indicating code reuse across similar tasks. These analyses support interpretable, task-aligned code usage patterns.",
            "multi_task_performance": "Multi-task experiments (Table 1) show ASAC variants outperform baseline on several datasets. Best-performing variant differs by dataset: e.g., TaskID in input worked best for ODIR-5K, Triangles and CIFAR10-SVHN; TaskID to decoder worked best for Sort-of-Clevr; TaskID in both input & decoder best for MNIST and Places365. Overall ASAC (and its task-conditioned variants) improved multi-task adaptability.",
            "resource_constrained_results": "Authors note resource constraints prevented multiple seeds/hyperparameter sweeps for large datasets (Places365). No explicit experiments under limited compute or budget are reported beyond noting parameter counts are similar to baseline.",
            "key_finding_summary": "Inserting a discrete VQVAE attention controller (ASAC) produces task-aligned, discrete attention schemas that improve accuracy, accelerate learning, and improve robustness and transfer/generalization in many but not all tested settings, while keeping parameter counts similar to the baseline.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "ASAC's empirical improvements, task-conditional variants' benefits, codebook usage differences (KS test), and better OOD/transfer behaviour in many cases provide evidence that task-aligned, discretized attention abstractions can improve performance and generalization compared to uniform attention.",
            "uuid": "e2231.0"
        },
        {
            "name_short": "ViT baseline",
            "name_full": "Vision Transformer (baseline, no ASAC)",
            "brief_description": "Standard vision-transformer architecture used as the paper's uniform-representation baseline; attention blocks produce continuous attention weights learned during training without explicit discrete codebook abstraction.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Vision Transformer (baseline)",
            "model_description": "Transformer-based image classifier (ViT-style) without an attention-schema/VQVAE controller; uses continuous scaled dot-product multi-head attention trained end-to-end.",
            "model_size": null,
            "uses_task_aligned_abstraction": false,
            "abstraction_mechanism": "uniform/fixed learned attention weights (standard transformer attention), no discrete task-aligned abstraction",
            "is_dynamic_or_adaptive": false,
            "task_domain": "Vision classification, multi-task settings used in experiments (Triangles, Polygons, Sort-of-Clevr, CIFAR, Tiny-ImageNet, Places365, CelebA, etc.)",
            "performance_task_aligned": null,
            "performance_uniform_baseline": "Reported baseline results: Triangles 78.66 ± 0.83 accuracy; Sort-of-Clevr 63.15 ± 7.39; MNIST 96.78 ± 0.49; CIFAR10-SVHN 85.94 ± 1.72; Places365 60.46 (other metrics in Table 1/figures).",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": null,
            "computational_efficiency_baseline": "Parameter count nearly equal to ASAC (authors state total parameters 'almost equal'); no FLOPs/latency reported.",
            "sample_efficiency_results": "Baseline is the comparator in learning-efficiency experiments; ASAC is reported to learn faster and achieve higher accuracy with fewer samples than this baseline (paper-level claim), but baseline-specific sample-efficiency numbers are not enumerated.",
            "transfer_generalization_results": "Baseline performance on transfer, OOD, and adversarial tests is reported as the comparison point; ASAC often outperforms baseline (see ASAC entry) but specific baseline-only generalization numbers are the ones reported in figures and tables.",
            "interpretability_results": null,
            "multi_task_performance": "Baseline generally underperforms ASAC and task-conditioned ASAC variants on many multi-task datasets (see Table 1), but in some tasks differences are small.",
            "resource_constrained_results": null,
            "key_finding_summary": "Baseline vision transformer with uniform attention provides a reference point; adding a discrete attention-schema (ASAC) typically improves accuracy, robustness, and learning speed while keeping parameter count similar.",
            "supports_or_challenges_theory": "neutral",
            "supports_or_challenges_theory_explanation": "Baseline represents the uniform-representation strategy; comparisons show ASAC often outperforms it, but the baseline itself offers no task-aligned abstraction.",
            "uuid": "e2231.1"
        },
        {
            "name_short": "ASAC-DistilBERT",
            "name_full": "DistilBERT fine-tuned with ASAC module in last layer",
            "brief_description": "A small NLP setup where the ASAC VQVAE module is integrated only into the last layer of a pre-trained DistilBERT to avoid disrupting pre-trained weights; fine-tuned on GLUE tasks to test whether attention-schema benefits transfer to NLP.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DistilBERT + ASAC (last layer)",
            "model_description": "Pre-trained DistilBERT with randomly initialized last layer; ASAC (VQVAE attention controller) is inserted only in the last transformer layer and fine-tuned together (other pre-trained layers kept), to test improvements on GLUE benchmarks.",
            "model_size": "DistilBERT (~66M typical) for base; ASAC module parameter count not separately reported; overall model-size reported as 'almost equal' to baseline in vision experiments but explicit parameter breakdown for DistilBERT+ASAC not provided.",
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "VQVAE over attention scores in the last transformer layer producing discrete attention schemas; task abstraction localized to final layer",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Natural language understanding (GLUE benchmark tasks: STS-B, SST-2, QNLI, MRPC, QQP, RTE, etc.)",
            "performance_task_aligned": "GLUE task examples (aggregate results across 5 seeds reported): SST-2 accuracy 0.9052 (ASAC) vs baseline 0.8986 (p=0.0001); RTE accuracy 0.6238 vs baseline 0.5870 (p=0.0002); QNLI 0.8895 vs 0.8850 (p=0.0105); MRPC and STS-B show small improvements in reported metrics; some tasks had non-significant p-values.",
            "performance_uniform_baseline": "DistilBERT baseline (same experiment conditions): SST-2 0.8986 ± 0.0102; RTE 0.5870 ± 0.0126; QNLI 0.8850 ± 0.0019; other GLUE numbers in Table 2.",
            "has_direct_comparison": true,
            "computational_efficiency_task_aligned": "No FLOPs/latency reported; authors note difficulty of integrating VQVAE into many pre-trained layers so they limited to last layer to avoid instability; no explicit compute-cost penalty numbers provided.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Not explicitly quantified; authors report ASAC outperforms baseline in GLUE fine-tuning across seeds, implying improved fine-tuning efficiency in these experiments.",
            "transfer_generalization_results": "Not directly applicable beyond GLUE fine-tuning; the ASAC integration improved downstream GLUE task performance in several tasks indicating beneficial transfer/fine-tuning behaviour.",
            "interpretability_results": null,
            "multi_task_performance": "GLUE is a suite of tasks; ASAC produced consistent improvements on several tasks but not universally statistically significant; indicates potential for task-aligned attention schemas to help across multiple language tasks when added carefully.",
            "resource_constrained_results": "Authors explicitly limited ASAC to last layer due to practical constraints in training pre-trained models with added untrained modules; no further resource-constrained performance experiments reported.",
            "key_finding_summary": "Integrating ASAC into the last layer of a pre-trained DistilBERT yielded consistent improvements on several GLUE tasks, suggesting the attention-schema mechanism can transfer to NLP when carefully integrated.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "ASAC's positive GLUE results indicate task-aligned discrete attention abstractions can improve downstream task performance in NLP fine-tuning scenarios, consistent with the principle.",
            "uuid": "e2231.2"
        },
        {
            "name_short": "VQVAE attention controller",
            "name_full": "Vector-Quantized Variational AutoEncoder attention controller (used in ASAC)",
            "brief_description": "A VQVAE module applied to transformer scaled dot-product attention scores; encoder maps attention into latent vectors, quantized to nearest codebook embedding, decoder reconstructs attention scores; losses include reconstruction, VQ (codebook + commitment), and task loss.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "VQVAE (as attention controller)",
            "model_description": "Two-linear-layer encoder and decoder with LeakyReLU activations, discrete learnable codebook updated via EMA; produces discrete latent codes that act as attention schemas to compress and stabilize attention patterns and enable task-conditioned behavior.",
            "model_size": null,
            "uses_task_aligned_abstraction": true,
            "abstraction_mechanism": "Vector quantization into a discrete codebook (neural discrete representations) that maps attention patterns to a finite set of learned prototypes; codebook indices serve as task-aligned abstractions.",
            "is_dynamic_or_adaptive": true,
            "task_domain": "Used within vision transformers and one NLP (DistilBERT) experiment as attention controller; supports multi-task conditioning via inclusion of TaskID in inputs/decoder.",
            "performance_task_aligned": null,
            "performance_uniform_baseline": null,
            "has_direct_comparison": null,
            "computational_efficiency_task_aligned": "Authors report parameter count parity with baseline; codebook adds discrete latent parameters but overall model size reported similar; no FLOPs/latency given.",
            "computational_efficiency_baseline": null,
            "sample_efficiency_results": "Implicit: contributes to reported faster learning and better few-shot/transfer performance in ASAC experiments, but no isolated VQVAE-only sample-efficiency numbers reported.",
            "transfer_generalization_results": "As part of ASAC, VQVAE-enabled models show improved transfer and OOD robustness in many experiments (see ASAC entry).",
            "interpretability_results": "Codebook usage analysis (KS-tests) demonstrates the VQVAE codebook exhibits task-specific usage distributions in multi-task settings (e.g., CIFAR10-SVHN p=0.0052), providing interpretable discrete attention schemas.",
            "multi_task_performance": "As the core mechanism of ASAC, VQVAE supports task-conditioned variants; different placements of TaskID (input, decoder, both) impact multi-task results (Table 1), indicating the VQVAE facilitates task-specific adaptation.",
            "resource_constrained_results": null,
            "key_finding_summary": "Vector quantization of attention scores yields discrete attention schemas that enable task-specific code usage, improved robustness/generalization, and faster learning when integrated into transformers.",
            "supports_or_challenges_theory": "supports",
            "supports_or_challenges_theory_explanation": "VQVAE-mediated discretization produces evidence of task-aligned code usage and associated performance gains, supporting the idea that discrete, task-aware abstractions help allocation of representation resources.",
            "uuid": "e2231.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural discrete representation learning",
            "rating": 2
        },
        {
            "paper_title": "Attention schema in neural agents",
            "rating": 2
        },
        {
            "paper_title": "Semi-supervised multimodal representation learning through a global workspace",
            "rating": 1
        },
        {
            "paper_title": "Flexible parameter sharing networks",
            "rating": 1
        },
        {
            "paper_title": "The attention schema theory: A foundation for engineering artificial consciousness",
            "rating": 1
        }
    ],
    "cost": 0.0149905,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers
19 Sep 2025</p>
<p>Krati Saxena saxenakrati18@gmail.com 
Federico Jurado Ruiz federico.juradoruiz@gmail.com 
Guido Manzi g.manzi@alientt.com 
Dianbo Liu d.liu@alientt.com 
Alex Lamb a.lamb@alientt.com 
Attention Schema-based Attention Control (ASAC): A Cognitive-Inspired Approach for Attention Management in Transformers
19 Sep 2025CEF6BED232905352D6FDF060EC4EB03DarXiv:2509.16058v1[cs.AI]
Attention mechanisms have become integral in AI, significantly enhancing model performance and scalability by drawing inspiration from human cognition.Concurrently, the Attention Schema Theory (AST) in cognitive science posits that individuals manage their attention by creating a model of the attention itself, effectively allocating cognitive resources.Inspired by AST, we introduce ASAC (Attention Schema-based Attention Control), which integrates the attention schema concept into artificial neural networks.Our initial experiments focused on embedding the ASAC module within transformer architectures.This module employs a Vector-Quantized Variational AutoEncoder (VQVAE) as both an attention abstractor and controller, facilitating precise attention management.By explicitly modeling attention allocation, our approach aims to enhance system efficiency.We demonstrate ASAC's effectiveness in both the vision and NLP domains, highlighting its ability to improve classification accuracy and expedite the learning process.Our experiments with vision transformers across various datasets illustrate that the attention controller not only boosts classification accuracy but also accelerates learning.Furthermore, we have demonstrated the model's robustness and generalization capabilities across noisy and out-of-distribution datasets.In addition, we have showcased improved performance in multi-task settings.Quick experiments reveal that the attention schema-based module enhances resilience to adversarial attacks, optimizes attention to improve learning efficiency, and facilitates effective transfer learning and learning from fewer examples.These promising results establish a connection between cognitive science and machine learning, shedding light on the efficient utilization of attention mechanisms in AI systems.</p>
<p>Introduction</p>
<p>Inspired by human attention, the attention mechanisms in machine learning have emerged as crucial elements in enhancing the effectiveness of AI models across a wide array of applications.Analogous to human attention, these mechanisms within AI systems, notably transformers (Vaswani et al., 2017), facilitate comprehensive global contextual understanding (each token attends to other tokens), concurrent information processing (computing weighted sums of input representations), and focus on diverse aspects (using multihead attention), all while adapting to specific tasks through acquired attention weights.Nevertheless, human attention is a little more complicated.The Attention Schema Theory (AST) (Graziano, 2017;2020;2022;Wilterson &amp; Graziano, 2021) provides a conceptual framework for elucidating how we internally regulate our focus, deliberately disregarding certain stimuli to concentrate on selected objectives.</p>
<p>In this work, we introduce Attention Schema-based Attention Control (ASAC), an attention abstractor and controller inspired by AST, integrated into the transformer's dot-product attention mechanism.We use Vector Quantized Variational AutoEncoders (VQVAE) as the attention controller (Van Den Oord et al.,</p>
<p>Related Works</p>
<p>Attention controller in transformers: Several deep learning models have attempted to draw inspiration from theories of consciuosness in humans, with a focus on representation learning.The Global Workspace theory (Newman &amp; Baars, 1993) proposes a system that distributes information among specialized modules to enhance cognition and awareness.</p>
<p>Deep learning models inspired by the concept of a global shared workspace include architectures that utilize a common representation for multiple input modalities (Devillers et al., 2024).These models leverage specialized systems for each modality, with latent representations encoded and decoded within a shared workspace (Goyal et al., 2021;VanRullen &amp; Kanai, 2021).(Piao et al., 2020) explores the Hybrid CNN and Adaptive DenseNet models inspired by a global shared workspace in the Flexible Parameter Sharing Networks approach.(Peis et al., 2023) presents a novel deep generative model based on non-i.i.d.variational autoencoders that capture global dependencies among observations in a fully unsupervised fashion and study the ability of the global space.The shared workspace facilitates communication among different specialized modules, akin to a bandwidth-limited communication channel in cognitive science (Franklin et al., 2007;2012).Furthermore, the proposal of a global latent workspace (GLW) (VanRullen &amp; Kanai, 2021) through unsupervised neural translation between multiple latent spaces aligns with the Global Workspace theory for creating higher-level cognition.Additionally, the LIDA cognitive architecture (Franklin et al., 2007;2012) implements the Global Workspace Theory conceptually and computationally, emphasizing the importance of modeling high-level cognitive processes inspired by biological cognition.These models, inspired by the Global Workspace Theory, represent significant steps toward creating biologically inspired cognitive architectures that can mimic the human mind's ability to process and integrate information across various domains and modalities.</p>
<p>Attention Schema Theory: In cognitive neuroscience, the Attention Schema Theory (AST) is a significant framework that proposes the brain develops a schema to manage attention efficiently (Graziano, 2017;2020;2022;Wilterson &amp; Graziano, 2021).This schema predicts and characterizes attention focus independently from directing attention.AST highlights the benefits of having an attention schema, especially in enhancing visuo-spatial attention control (Liu et al., 2023).It also links subjective awareness to attention modulation, emphasizing awareness's role in attention governance.Wilterson &amp; Graziano (2021) support the attention schema's role in improving social intelligence and agent coordination.</p>
<p>Attention schema is viewed as an internal model enabling the brain to abstractly represent, manipulate and forecast attention dynamics (Liu et al., 2023;Graziano, 2017;Graziano &amp; Webb, 2015;van den Boogaard et al., 2017).Similar to the body schema for movement control, this model aids top-down attentional control and task performance.AST suggests that lack of awareness may compromise attention control, showing the interplay between attention and consciousness.Simulations based on AST illustrate how attention control can impact cognitive functions.</p>
<p>Building on these insights, our paper introduces an architecture inspired by the Attention Schema Theory (AST), integrated within transformer models.We demonstrate how this novel integration enhances the performance and adaptability of transformers, both in vision tasks and preliminary experiments with NLP tasks.Our findings suggest that applying cognitive theories like AST to AI models can significantly improve their efficiency and robustness, opening new avenues for developing more sophisticated and cognitive-inspired AI systems.</p>
<p>Attention Schema-based Attention Control in Deep Neural Networks</p>
<p>Attention Schema and Integration of Human-Like Attention Mechanisms in AI: AST proposes that the brain creates an "attention schema" reflecting our focus and related cognitive activities.This schema supports complex cognitive tasks like reasoning and decision-making.AST explains that just as the brain creates a simplified model of the body to control movements, it also makes a model of attention to manage focus and cognitive processes.While humans naturally manage attention, replicating this in AI systems is challenging.</p>
<p>Most sophisticated AI systems employ conventional attention mechanisms (Liu et al., 2019;Yang et al., 2019;Dai et al., 2019;Sanh et al., 2019;Lewis et al., 2019) or algorithmic adjustments to enhance performance (Child et al., 2019;Kitaev et al., 2020;Beltagy et al., 2020;Wang et al., 2020), yet replicating the nuanced attention control observed in humans remains a work in progress.In this paper, we present a cognitiveinspired approach, ASAC, to efficiently control the attention in AI models.</p>
<p>Examining the Potential Roles of Attention Schema in Transformers: Attention Schema Theory (AST) posits that the brain creates a simplified model of its attention system to manage cognitive resources efficiently (Graziano &amp; Webb, 2015).This internal schema helps predict and adjust attention for various tasks.For instance, when repeatedly performing similar actions like picking up objects, the brain reuses and refines this map to distribute resources efficiently.Conversely, when faced with new challenges, such as solving a puzzle, the brain uses the attention schema to adapt the attention to meet changing demands.We explore whether the ASAC model develops a similar attention schema to allocate cognitive resources effectively.In standard transformers (Vaswani et al., 2017), the attention blocks learn weights during training that determine how inputs are processed.We hypothesize that multiple distinct attention patterns emerge, effectively clustering inputs into subsets, each processed according to its asSOCiated attention configuration.To capture this behavior, we introduce a higher-level structure that maps attention scores to a finite set of latent representations.Specifically, we apply a Vector Quantized Variational Autoencoder (VQ-VAE) to the attention scores, compressing them into discrete codes.These codes act as attention schemas, consistent with AST, and enable more robust associations between inputs and their corresponding attention patterns.This compression improves noise tolerance and stability when processing out-of-distribution data, as demonstrated in our experiments.</p>
<p>AST suggests the brain strategically distributes cognitive resources and adapts to changing environments.Similarly, we propose that ASAC can dynamically allocate attention, enhancing resilience to noise and unfamiliar data.Instead of fixed attention patterns, these models should flexibly adapt to diverse inputs.We demonstrate this through experiments on noisy and out-of-distribution datasets.The human brain transitions seamlessly between tasks due to cognitive flexibility.We propose that AST-based models can similarly allocate attention across tasks, dynamically adjusting based on requirements rather than treating jobs in isolation.Additionally, AST-based model should optimize the resource allocation for solving similar or dissimilar tasks, just as human brain does.Similar tasks should re-use the cognitive resources while dissimilar tasks may distribute the resources to adapt to the needs.We demonstrate this capability in multi-task scenarios.</p>
<p>Furthermore, preliminary studies indicate that the ASAC module enhances robustness and adaptability in various contexts.ASAC improves resilience to adversarial attacks by dynamically adjusting attention patterns to mitigate the impact of misleading inputs.Additionally, similar to how the brain efficiently learns new skills with minimal repetition, ASAC enhances learning efficiency, enabling models to achieve better performance with fewer training epochs.In transfer learning scenarios, akin to how humans apply knowledge from one domain to another, ASAC facilitates the effective transfer of knowledge, promoting quicker adaptation to new tasks.Moreover, reflecting the brain's ability to learn effectively from limited information, ASAC excels in scenarios with limited data, focusing on the most relevant information to learn from fewer examples.These capabilities underscore the comprehensive adaptive nature of ASAC, making it an invaluable tool for advancing AI model performance across a wide range of challenges.</p>
<p>Proposed Method</p>
<p>In this section, we present the proposed method and ways we have integrated the ASAC module in different scenarios to test its performance.</p>
<p>Attention Controller</p>
<p>The AST would inspire a framework that facilitates abstraction, manipulation, and prediction.Abstraction involves creating a simplified attention schema to guide attention on vital information, improving model performance.Manipulation entails fine-tuning attention to emphasize important details, and ensuring relevant data is prioritized.Prediction involves the model's forecasts about future attention allocation based on processed data, allowing it to foresee trends and enhance predictive accuracy.This triad of capabilities is central to our model's design, enabling it to efficiently process and anticipate information.</p>
<p>We incorporate an attention controller into the transformers architecture (Dosovitskiy et al., 2020), using Vector Quantized Variational Auto-encoder (VQVAE) (Van Den Oord et al., 2017) to embody the concepts of abstraction, manipulation, and prediction.VQVAE reflects Attention Schema Theory by abstracting data into a discrete space, thereby creating an attention schema.It manipulates attention through vector quantization, learning from data to focus on specific elements, and its decoder predicts by reconstructing inputs from these discrete latent representations.</p>
<p>We show the overall architecture in Figure 1.We plug the attention controller (VQVAE) into the attention mechanism of the transformer, where the attention scores are calculated.The input involves the scaled dot product of the query and keys, which is then fed into the encoder.Both the encoder and the decoder consist of two linear layers separated by a LeakyReLU activation function.A discrete embedding vector matrix, referred to as a codebook, is randomly initialized at the start.The distance between the codebook vectors and the encoding output is calculated, and the closest vectors to the encoder input are indexed from the codebook, and exponential moving average for these vectors is calculated to update the codebook while learning.The chosen vectors from the codebook serve as the input to the decoder.Throughout the training phase, adjustments are made to the codebook to reduce the reconstruction error between the input and output.The output is the reconstructed scaled dot product, via which we compute the final attention.We also add residual connections between the input and the output attentions scores to facilitate stable training.For this, we simply add the reconstructed scaled dot product to the original input scaled dot product.</p>
<p>We employ a composite loss function for the model, comprising three distinct types of losses.The reconstruction loss L recon , is the mean squared error loss between the reconstructed attention and the original attention.The reconstruction loss is defined by the following equation:
L recon = 1 N N i=1 A (i) original − A (i) reconstructed 2 (1)
where,
A (i) original and A (i)
reconstructed are the original and reconstructed attention matrix, N is the total number of scores calculated in one forward-pass.The VQVAE attention controller has a vector quantization loss (Dosovitskiy et al., 2020) specific to the VQVAE architecture.It ensures that the latent representations (the encodings) match the closest vector in a learnable discrete codebook.The VQ loss can be broken down into two components: the codebook loss and the commitment loss, which help maintain a balance between the flexibility of the codebook and the stability of the encodings.The VQ loss is defined as:
L V Q = ∥sg[z e (x)] − e∥ 2 2 + β∥z e (x) − sg[e]∥ 2 2 (2)
where, the first term is the codebook loss and the second term is the commitment loss (Dosovitskiy et al., 2020).The codebook loss ensures that the chosen codebook vectors (the embeddings (e)) are close to the encoder outputs (z e (x))).(sg) represents the stop-gradient operator, meaning no gradient flows through whatever it's applied on.This loss moves the embedding vectors (e) towards the encoder output (z e (x))).</p>
<p>The commitment loss encourages the encoder outputs to commit to a codebook vector, preventing them from drifting too far during training, (β) is a hyper-parameter that balances the importance of the commitment loss relative to the codebook loss.For experiments in this paper, β is set to 1.</p>
<p>The task loss, L task , as the name suggests is the overall loss function specific to the task being solved by the model.It can be binary cross-entropy loss or multi-class cross-entropy based on the task.The cross-entropy loss for multi-class classification is given by:
L task = − M c=1 y i,c log(p i,c )(3)
where, (M ) is the number of classes, (y i,c ) is a binary indicator of whether class (c) is the correct classification for observation (i), and (p i,c ) is the predicted probability that observation (i) is of class (c).Similarly, if the task is binary classification, then the loss is given as:
L task = − <a href="4">y i log(p i ) + (1 − y i ) log(1 − p i )</a>
where, for the (i th ) sample, (y i ) is the true label (0 or 1), and (p i ) is the predicted probability of the observation being in class 1.</p>
<p>The overall loss is calculated as:
L final = L task + λ (L recon + L V Q ) (5)</p>
<p>Attention Control on Multi-task Scenario</p>
<p>The Attention Schema Theory (AST) suggests that cognitive flexibility is essential for smooth transitions between tasks within the human brain.Similarly, a model based on AST should be capable of dynamically transitioning between tasks by adjusting the allocation of attention according to the requirements of each task.A core part of our study involves evaluating our model in scenarios involving binary or multiple tasks, which can range from simple binary or multi-class classification to more complex multi-class multi-label classification.</p>
<p>To perform this experiment, we incorporate unique task identifiers (Task IDs) within the dataset to differentiate between various tasks.Although the dataset remains constant, the corresponding labels may vary depending on the task identifiers.During the classification trials involving an attention controller, the original input images are transformed into a sequence of distinct patches.These patches are then conveRTEd into patch embeddings, with positional embeddings computed to form the final input sequence fed into the model.In a multi-task setting, we augment this input sequence with a task embedding before processing it through the model.We tried the following variations for task information processing:</p>
<p>• Including the Task ID information in the input sequence: This is like priming the attention schema with the task to be solved from the beginning.</p>
<p>• Including the Task ID information to the VQVAE decoder: This concept is aligned with the notion that the attention schema is adaptable based on the task requirements during the interpretative phase.</p>
<p>• Including the Task ID information to both the input sequence and the decoder: This represents holistic integration where attention schema is continuously informed and adjusted based on the task information from initial processing to final decision-making.</p>
<p>Attention Control on Noisy and Out-of-Distribution Datasets</p>
<p>The attention schema enables prioritizing and focusing on the most relevant information.We hypothesize that the AST-based model should also be able to generate this selective attention on data features to attenuate irrelevant or misleading signals.Consequently, the attention schema should enhance the generalization power of the model.To validate this proposition, we assess our model using datasets containing noise and data points outside the distribution while being trained on the original, non-noisy dataset.</p>
<p>Assessing Versatility and Adaptability of the Attention Controller</p>
<p>The attention schema flexibly adapts to diverse inputs, and due to the discretization of information using a codebook, it enhances resilience to subtle changes in the inputs, such as adversarial attacks.ASAC should also be adept at learning faster with fewer samples by optimizing attention and focusing on relevant information.Additionally, it should facilitate the transfer of knowledge from one task to another by adapting attention patterns.We conduct preliminary investigation on versatility and adaptability of ASAC.</p>
<p>Attention Control for Optimizing Resource Distribution</p>
<p>AST posits that the brain creates a simplified model of its attention system to manage cognitive resources efficiently (Graziano &amp; Webb, 2015).This internal schema helps predict and adjust attention for various tasks.For instance, when repeatedly performing similar actions like picking up objects, the brain reuses and refines this map to distribute resources efficiently.Conversely, when faced with new challenges, such as multi-tasking, the brain adapts the schema to meet changing demands.We explore whether the ASAC model develops a similar attention schema to allocate cognitive resources effectively.</p>
<p>Experiments on Vision Datasets</p>
<p>In this section, we describe the experiments we conducted and the datasets used.Each subsection contains the results for those experiments.</p>
<p>Attention Controller Performance on Classification</p>
<p>To evaluate the effectiveness of ASAC on transformers, we conducted classification experiments on various vision datasets.For the vision experiments, we used vision transformers1 without the ASAC module as a baseline.We compared this baseline to a model with the ASAC module integrated into all layers.Both models were trained from scratch on the datasets, with hyper-parameters and training conditions detailed in the Appendix.</p>
<p>Datasets</p>
<p>The   We explain the rest of the datasets below:</p>
<p>• Triangles: We adopt this dataset from (Goyal et al., 2021;Ahmad &amp; Omohundro, 2022), containing images of three white dot clusters on a black background.The task is to predict if the clusters form an equilateral Triangle.</p>
<p>• Polygons: This variation of the Triangles dataset includes images with three or more clusters.The task is to identify regular Polygons with equal side lengths, similar to equilateral Triangles but with added complexities like background noise and alternating hues.We train the model on images with 3, 4, or 8 vertices and 5% noise, and test on images with 5, 6, or 7 vertices and 25% noise.Images may also be negative, with switched foreground and background colors.We show the samples from Triangles and Polygons data in Figure 2.</p>
<p>• Sort-of-Clevr: Sort-of-Clevr (SOC)6 (Goyal et al., 2021;Santoro et al., 2017) is a dataset for testing reasoning abilities, similar to CLEVR (Johnson et al., 2017).It includes images of 2D objects and questions about their properties and relationships.The dataset has two tasks: non-relational reasoning [SOC (norel)], focusing on a single object's properties like shape and location, and relational reasoning [SOC (rel)], involving relationships between objects like proximity or counting similar shapes.</p>
<p>More information on datasets is available in Appendix.Figure 4 (first three plots) displays the results for the multi-label classification experiment on the CelebA dataset, demonstrating that ASAC achieves superior performance compared to the baseline.The last plot of Figure 4 shows the results for the Places365 dataset.Here, ASAC exhibits faster learning and reaches higher accuracy quicker than the baseline.However, after several epochs, we observe over-fitting in both ASAC and the baseline models.Although we show results for 30 epochs for consistency, early stopping would have halted training within 10 epochs for Places365 to prevent over-fitting.Places365 is a large dataset requiring significant computational resources, and due to cost constraints, we could not run multiple seeds or test various hyper-parameter settings.</p>
<p>Results</p>
<p>Multi-task Handling</p>
<p>In these experiments, we assess the efficacy of the attention controller in comprehending various tasks and generating predictions according to the task at hand.</p>
<p>Datasets</p>
<p>We asses the efficacy of the attention controller in comprehending various tasks using the following datasets:</p>
<p>• Sort-of-Clevr: We use the same dataset as before, posing relational and non-relational tasks separately.</p>
<p>• Triangles: Using the same images as before, we create labels for two tasks: identifying equilateral Triangles and determining if most cluster points are in the upper half.We show a sample in Figure 5. • MNIST: MNIST7 is classic ML dataset for which we create labels for two tasks: distinguishing odd from even numbers and identifying prime versus composite numbers.</p>
<p>• Ocular Disease Intelligent Recognition: ODIR-5K8 (Prabhakaran et al., 2024) is real-world ophthalmic disease dataset with 8 labels.We pose it as a multi-task problem, with each task being a binary classification of whether an image represents a specific disease.</p>
<p>• Places365: Using the same images, we create three tasks: single-label classification, and multi-label classification into level-1 and level-2 hierarchy labels (Zhou et al., 2017a;b).</p>
<p>• CIFAR10-SVHN: We mix the CIFAR10 and SVHN dataset and the task is to label the images of CIFAR10 or SVHN based on the task ID input.</p>
<p>More information on the datasets is available in Appendix.</p>
<p>Results</p>
<p>Table 1 illustrates that the AST-based attention controller exhibits superior capability in adjusting the attention schema within a multi-task environment when compared to the model lacking the controller.In these experiments, we run three different seeds and present aggregate results.We see that the best-performing model among different datasets is a different variant of the model.In datasets like ODIR-5K, Triangles and CIFAR10-SVHN, the taskID in input performs the best, which means task-specific information (TaskID) provided at the input level helps the model to quickly adapt and focus on the relevant features for each task.We observe that task information in the decoder helps in Sort-of-Clevr.By sending the TaskID to the decoder, the model can leverage the task-specific information at a later stage, allowing the encoder to focus on extracting general features without being biased by the task.For MNIST, we see that the best performing model is when the task information is present in the input as well as the decoder, meaning that the model is able to make better decisions when the model is continuously informed and adjusted based on the task information.Since Places365 is a huge dataset and the tasks vary significantly (single-label and multi-label classification in different tasks), we assumed that task information in both the input and the decoder will help solve this task as the model can retrieve features based on the task and also conditioned to predict the final output based on the task information.We see ASAC performing better than the baseline in this setting for Places365.</p>
<p>Generalization Power</p>
<p>In these experiments, we train the model with non-noisy original datasets and test on noisy and out-ofdistribution (OOD) datasets to check the generalization power of the attention controller.</p>
<p>Datasets</p>
<p>We test the generalization power of our model two noisy datasets: CIFAR-10C9 and Tiny Imagenet-C10 .Additionally, we test ASAC on the following OOD datasets:</p>
<p>• Triangles-OOD: We create test images with different sizes of clusters.We also vary the shape of clusters to circles, squares, or Triangles and the shape can be empty or filled.We train using the original (filled, circle, constant size clusters) Triangles dataset.</p>
<p>• Polygons-OOD: The train set contains 3,4,8 vertices with colored noise on 5% of the background.The test set contains 5, 6, and 7 vertices with colored noise on 25% of the background area.</p>
<p>Results</p>
<p>The results for generalization experiments are shown in Figure 6.There is a marginal increase in performance with attention controller on corrupted Tiny Imagenet dataset.Since this is a challenging dataset, the overall accuracy obtained by both the baseline and VQVAE models is relatively low.The ASAC model achieves higher accuracies in corrupted CIFAR datasets.Furthermore, we also see significant improvement in the OOD experiments with Triangles and Polygons data.</p>
<p>Versatility and Adaptability</p>
<p>We conduct preliminary investigations to evaluate the versatility and adaptability of the model with the attention controller.Our experiments focus on four tasks: adversarial attacks, transfer learning, few-shot learning, and learning efficiency.</p>
<p>For the adversarial attacks experiments, we train the model for 20 epochs on the original dataset and test it on perturbed images.These attacked images, generated using the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2014) and Projected Gradient Descent Method (PGDM) (Ayas et al., 2022), are slightly altered to cause the model to misclassify them.FGSM calculates the gradient of the model's loss with respect to the image and adds a scaled version of this gradient to the original image, where the scale factor, ϵ, controls the perturbation magnitude.PGDM iteratively applies small perturbations, recalculating the gradient at each step to maximize the model's error.</p>
<p>For transfer learning experiment, we pre-train the model on one dataset for 10 epochs and fine-tune on another (related) dataset for 5 epochs.For the few-shot experiments, we train the model for 10 epochs on one dataset and then use different percentages of another data to do few-shot training.For testing the learning efficiency, we train the model using different percentages of data for 25 epochs and test on the original test set of the same dataset.We run the experiments for three different seeds and show the aggregated results.For these experiments, we used CIFAR10, CIFAR100, FashionMNIST, SVHN dataset, described in subsubsection 5.1.1.</p>
<p>Results</p>
<p>We show the results for FGSM and PGDM adversarial attacks in Figure 7 and Figure 8.In the FGSM adversarial attacks, the performance varies.Specifically, for CIFAR10, ASAC consistently underperforms compared to the baseline across all ϵ values.Conversely, ASAC demonstrates superior performance for FashionMNIST across all ϵ values.For CIFAR100 and SVHN, ASAC exhibits better performance at smaller ϵ values; however, as ϵ values increase, indicating stronger attacks, the baseline begins to outperform ASAC.In contrast, the results for PGDM attacks show that ASAC consistently surpasses the baseline across all datasets and ϵ values.</p>
<p>The mixed results for FGSM attacks can be attributed to the nature of the datasets and the varying complexity of adversarial examples.For CIFAR10, the simpler architecture of the baseline might be more resilient to minor perturbations, leading to better performance at all ϵ levels.For FashionMNIST, the inherent robustness of ASAC's attention mechanisms likely provides it with an advantage over the baseline, enabling it to better handle adversarial examples.In the cases of CIFAR100 and SVHN, ASAC's superior performance at lower ϵ values could be due to its ability to capture complex features effectively.However, as the attack strength increases, the baseline's more straightforward architecture might become advantageous by avoiding overfitting to perturbed data, thus outperforming ASAC in higher ϵ scenarios.Regarding PGDM attacks, ASAC's consistent outperformance across all datasets and ϵ values highlights its robust design.The combination of improved feature extraction and attention mechanisms likely equips ASAC with enhanced capabilities to mitigate the effects of stronger, iterative adversarial attacks compared to the baseline.</p>
<p>The results of our transfer learning experiments are illustrated in Figure 9.We observe that ASAC consistently outperforms the baseline when trained on one dataset and fine-tuned on another.This superior performance can be attributed to ASAC's ability to effectively utilize attention patterns during the transfer of knowledge.Its design allows it to selectively focus on crucial aspects of the pre-training data, which enhances its generalization capability.Consequently, ASAC not only learns robust representations from the initial dataset but also successfully adapts and fine-tunes these representations to excel on the target dataset.</p>
<p>We show the results of few-shot learning in Figure 10.We observe that ASAC demonstrates enhanced accuracies when pre-training is performed on CIFAR100 and fine-tuning on CIFAR10, as well as in another experiment where pre-training is conducted on SVHN followed by few-shot fine-tuning on CIFAR10.These improvements can be attributed to the complementary nature of the datasets and the effective transfer of learned features, allowing ASAC to leverage its attention mechanisms efficiently.</p>
<p>For the former combination, the shared object-centric nature of CIFAR100 and CIFAR10 allows ASAC to apply its learned representations effectively, resulting in improved performance.</p>
<p>In the case of pre-training on SVHN and fine-tuning on CIFAR10, the larger number of training samples in SVHN may contribute to the development of more robust initial features.However, this scenario may not be as straightforward, and the effective transfer could also be attributed to the general adaptability of ASAC's Conversely, for the scenarios where pre-training is done on CIFAR100 or CIFAR10 and few-shot fine-tuning on SVHN, we observe mixed results across different few-shot percentages of the data.This variability can be explained by the differences in data distribution and feature complexity between these datasets.While CIFAR datasets focus more on varied objects, SVHN is centered around digit recognition in different contexts, leading to potential mismatches in learned features during transfer learning.Consequently, the effectiveness of knowledge transfer fluctuates, resulting in inconsistent performance for different percentages of the fewshot data.</p>
<p>Our findings on learning efficiency experiments, depicted in Figure 11, reveal that ASAC consistently outperforms the baseline across all datasets and varying data percentages used for training the models.ASAC excels in rapid learning with fewer samples by optimizing attention and effectively focusing on relevant information.This efficiency allows ASAC to achieve higher accuracies compared to the baseline, starting from the very initial epochs.</p>
<p>Allocation of Cognitive Resources</p>
<p>In the VQVAE architecture, the codebook is crucial as it represents part of the attention schema, enabling efficient allocation of computational resources.We test this by analyzing the ODIR-5K and CIFAR10-SVHN experiments in multi-task scenarios, whose results are presented in subsubsection 5.2.2.Specifically, we calculate and analyze the usage of distinct codes from the codebook for different tasks to demonstrate effective resource allocation.</p>
<p>Results</p>
<p>We assess codebook usage by counting the frequency of specific codes for each task and calculating the Kolmogorov-Smirnov (KS) test p-value.For the CIFAR10-SVHN dataset, ASAC produces a p-value of 0.0052, indicating significant differences in code usage between tasks.This suggests the model uses distinct codes to label images in CIFAR10 (everyday objects) and SVHN (house numbers), given their differing image types.</p>
<p>For ODIR-5K, we calculate pairwise KS-Test p-values for 8 tasks shown in Figure 12.All pairwise p-values fall between 0.21 and 1.0, with most above 0.7.Since no value for ODIR-5K is below the significance level of 0.05, the results suggest similar codebook usage across tasks, indicating code reuse for solving different tasks but with similar images.</p>
<p>ASAC Integration to Language Models</p>
<p>We tested the ASAC integration with a small NLP model using simple classification tasks.Training an NLP model from scratch is impractical due to the vast datasets and significant computational resources required.Thus, we use pre-trained DistilBERT model (Sanh et al., 2019) for NLP experiments.However, integrating the VQVAE module into all layers of such a pre-trained model presents challenges.Ideally, one would train only the new VQVAE parameters while keeping the pre-trained weights unchanged.However, adding untrained parameters to a pre-trained network disrupts the synergy between components, as the pre-trained weights are already optimized to work together, whereas the new VQVAE parameters have not undergone this collective training process.This misalignment causes the model to struggle to adapt, leading to poor performance.To prevent disrupting the pre-trained weights, we incorporate the ASAC module solely into the last layer of the model, with its parameters initialized randomly.The baseline model retains pre-trained weights for all layers except the last one, which is initialized randomly.Similarly, the tested model maintains pre-trained weights for all layers but includes the ASAC module in the last layer.We then fine-tune both the baseline DistilBERT model and the ASAC-integrated DistilBERT.We test the model on GLUE11 benchmark datasets.</p>
<p>Results</p>
<p>We show the results with the GLUE dataset in Table 2.We fine-tune the DistilBERT model for five different seeds and present the aggregate result.We observe that for all the experiments, the ASAC model outperforms the baseline.We also report the p-value to indicate the statistical significance of the differences between the VQVAE performance and the baseline.A lower p-value (typically less than 0.05) suggests that the differences are statistically significant and unlikely to be due to random variation, thereby indicating that the VQVAE integration has a meaningful impact on the model's performance.However, in some instances, the p-value exceeds 0.05, indicating that the results may not be statistically significant.</p>
<p>Discussion</p>
<p>In this paper, we delve into the integration of cognitive science principles into artificial intelligence (AI), specifically through the lens of Attention Schema Theory.We present Attention Schema-based Attention Controller, for more efficient control of attention particularly in transformers.ASAC is built on the premise  that human attention is an active process involving internal models that manage cognitive resources.According to the AST, individuals create simplified representations of their attention systems, enabling them to prioritize and allocate cognitive resources effectively.This concept has been translated into ASAC, which employs a Vector Quantized Variational AutoEncoder as both an attention abstractor and controller within transformer architectures.</p>
<p>Our experimental results across various datasets in vision and NLP domains highlight ASAC's effectiveness in improving classification accuracy and accelerating learning.For instance, when embedded within vision transformers, ASAC consistently outperformed baseline models across multiple datasets.These findings demonstrate that ASAC not only boosts classification accuracy but also expedites the learning process, showcasing its potential to enhance overall AI model performance.</p>
<p>One standout feature of ASAC is its robustness and generalization capabilities, particularly when exposed to noisy and out-of-distribution datasets.ASAC's ability to maintain performance under such challenging conditions underscores its adaptability and resilience-traits essential for real-world applications where data can often be unpredictable and varied.</p>
<p>Moreover, ASAC has shown promising results in multi-task settings, effectively managing attention across different tasks.This capability aligns with the human brain's cognitive flexibility, allowing seamless transitions between tasks and efficient resource allocation.Experimental results indicate that ASAC can dynamically adjust its attention allocation based on each task's requirements, a significant advancement over traditional attention mechanisms that often operate with fixed patterns.</p>
<p>Overall, with some preliminary experiments, we also demonstrate that the ASAC architecture is versatile and adaptable in many scenarios with some mixed results here and there.ASAC shows enhanced resilience to adversarial attacks.ASAC also excels in scenarios involving efficient learning and knowledge transfer.</p>
<p>The model demonstrates improved performance in few-shot learning tasks, effectively learning from limited examples.</p>
<p>The integration of ASAC into language models, specifically the DistilBERT model, further demonstrates its potential for enhancing NLP tasks.Despite the challenges associated with incorporating the VQVAE module into pre-trained models, the results indicate that ASAC can improve performance on benchmark datasets like GLUE.This integration highlights ASAC's potential to enhance not only vision models but also language models, thereby broadening its applicability across different AI domains.</p>
<p>In future, we will address the current limitations of this study by effectively incorporating the attention controller into larger and pre-trained models like large language models (LLMs).Additionally, we aim to explore and develop better architectures that more closely mimic human cognition.These efforts will not only enhance the performance and applicability of ASAC but also push the boundaries of how cognitive science principles can be integrated into advanced AI systems, ensuring they perform robustly and efficiently across various domains and tasks.</p>
<p>In conclusion, the development of ASAC represents a significant step forward in integrating cognitive science principles into AI.The implications of ASAC extend beyond mere performance improvements; they also shed light on the potential for creating AI systems that more closely mimic human cognitive processes.As AI continues to evolve, approaches like ASAC will be instrumental in developing models that can adapt, learn efficiently, and perform robustly in diverse and unpredictable environments.</p>
<p>APPENDIX A Pseudo Code and Details of Design of Attention Controller</p>
<p>We present the pseudo codes for the implementation of ASAC in Algorithm 1.The VQVAE function mentioned in Algorithm 1 is further described in Algorithm 2.</p>
<p>Algorithm 1 ASAC plugged into multi-head attention
Input: scaled dot product: Z = QK T √ d k
, where Q are Queries and K are Keys in Multi-Head Attention</p>
<p>Output: Reconstructed scaled dot product: Ẑ
1: Ẑ ← V QV AE(Z</p>
<p>B Datasets</p>
<p>In this section, we describe all the datasets used in the paper.A summary of the vision datasets is available in Table 3.It features images containing 2D objects of various shapes and colors and questions about the objects' properties and their relationships to each other.The dataset has two tasks: relational and non-relational reasoning.Non-relational reasoning focuses on questions about a single object's properties.The properties include the object's shape and its horizontal and vertical location in the image.These questions do not require the model to consider relationships between multiple things.Alternatively, relational reasoning involves questions about the relationships between objects.These include identifying the object closest or furthest from another item or counting the number of objects sharing the same shape as another object.These questions necessitate the agent to consider the relations between different objects.Therefore, the Sort-of-Clevr dataset provides a comprehensive tool for evaluating an AI agent's ability to reason about both individual objects and their relationships within a given context.A sample from the dataset is shown in Figure 20.We use the datatset in the multi-task experiments.In that experiment, the two tasks are relational and non-relational experiments within the Sort-of-Clevr dataset.We also use the same images for multi-task experiments with the following three tasks: first, classification into original labels, second, classification into level 1 hierarchy and third is the classification into level 2 hierarchy.The first task is single-label classification as opposed to the second and the third task which are multi-label classification.The level 1 hierarchy contains three labels: indoor, outdoor (natural), and outdoor (man-made).The level 2 hierarchy has 16 labels like shopping and dining, workplace, cultural, water, forest, transportation, sport fields, commercial buildings, etc.</p>
<p>MNIST:</p>
<p>The MNIST database is a large database of handwritten digits.Samples from the dataset are shown in Figure 24.For the two-task scenario, we alter the labels of the data based on the tasks, the first one is to detect odd and even numbers and the second one is to detect prime and composite numbers.Just as we demonstrate in Triangles multi-task scenario above, here too, the task ID is 0 and 1 and the labels for the first task are 0 corresponding to even numbers and 1 for odd numbers.For task ID 1, the labels 0 and 1 correspond to prime and composite numbers, respectively.</p>
<p>GLUE: GLUE, the General Language Understanding Evaluation benchmark12 is a collection of resources for training, evaluating, and analyzing natural language understanding systems.Specifically, we use the following datasets in the benchmark:  • STSB: The Semantic Textual Similarity Benchmark is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data.Each pair is humanannotated with a similarity score from 1 to 5.</p>
<p>• SST2: The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment.The task is to predict the sentiment of a given sentence.It uses the two-way (positive/negative) class split, with only sentence-level labels.</p>
<p>• QNLI: The Stanford Question Answering Dataset is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator).The authors of the benchmark convert the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence.The task is to determine whether the context sentence contains the answer to the question.This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue.</p>
<p>• MRPC: The Microsoft Research Paraphrase Corpus is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.</p>
<p>• QQP: The Quora Question Pairs2 dataset is a collection of question pairs from the community question-answering website Quora.The task is to determine whether a pair of questions are semantically equivalent.We show summary of GLUE benchmark in Table 4.In order to conduct the multi-task experiments, we include unique task identifiers in the dataset.These task identifiers (Task IDs) serve to distinguish between different types of tasks.Although the dataset remains constant, the corresponding labels may vary depending on the task identifiers.During the classification trials involving an attention controller, the original input images undergo a transformation into a sequence of distinct patches.Subsequently, these patches are converted into patch embeddings, and positional embeddings are computed, ultimately forming the final input sequence that is fed into the model.In the context of a multi-task setting, we augment the input sequence with a task embedding before passing it to the model for processing.As shown in Figure 25, the (right) yellow part is the standard way of processing the input when the input only contains input data and labels (X, y).If the input data are images, then X are images which are converted to a sequence of patches.Patch embeddings and their positional encodings are calculated and fed to the transformers attention module.In the multi-task scenarios, the dataset contains an extra information of task ID: (id, X, y).The task ID is used to calculate the Task ID embeddings (left, blue part</p>
<p>C Details of the Experiments</p>
<p>Figure 1 :
1
Figure 1: Proposed architecture of ASAC.The VQVAE module is inseRTEd in the attention mechanism and reconstructs the scaled dot product between queries and keys.</p>
<p>vision classification tasks include: 1) binary classification on Triangles and Polygons, 2) multi-class classification on FashionMNIST, SVHN, TypefaceMNIST, CIFAR10, CIFAR100, Tiny Imagenet, and Places365, 3) multi-label classification on CelebA, and 4) VQA on the Sort-of-Clevr dataset.Out of these, CIFAR10, CIFAR100 2 , FashionMNIST 3 , TypefaceMNIST 4 , SVHN 5 , Tiny Imagenet (Deng et al., 2009), Places365-Standard (Zhou et al., 2017a;b) and CelebA (Liu et al., 2015) are standard classification datasets.</p>
<p>(a) Samples from Triangle dataset.(b) Samples from Polygon dataset.</p>
<p>Figure 2 :
2
Figure 2: Samples from different datasets.</p>
<p>Figure 3 :
3
Figure 3: Classification results on vision datasets.Blue and orange represents ASAC and baseline, respectively.</p>
<p>Figure 4 :
4
Figure 4: Classification results on CelebA and Places365.</p>
<p>Figure 5 :
5
Figure 5: Multi-task scenario with Triangles data.Left side shows task 1: detecting equilateral Triangles and right side shows task 2: detecting majority upper points.First entry in the data points are the task ID.</p>
<p>Figure 6 :
6
Figure 6: Generalization results on noisy and OOD datasets.First and second row are the results of corrupted TinyImagenet and CIFAR10, respectively.Last row shows the results for OOD experiments for Triangles and Polygons.Blue and orange color represnts ASAC and baseline, respectively.</p>
<p>Figure 7 :
7
Figure 7: Adversarial attacks results for FGSM attacks on different datasets.</p>
<p>Figure 8 :
8
Figure 8: Adversarial attacks results for PGDM attacks on different datasets.</p>
<p>Figure 9 :
9
Figure 9: Results of transfer learning experiments.PT and FT stands for datasets used for pre-training and fine-tuning.</p>
<p>Figure 10 :
10
Figure 10: Results of few-shot experiments.PT and FS stands for datasets used for pre-training and few-shot fine-tuning.</p>
<p>Figure 11 :
11
Figure 11: Results of learning efficiency experiments.Each row represents results for training the model with different percentages of data.</p>
<p>Figure 12 :
12
Figure 12: KS test p-value for ODIR-5K.</p>
<p>) 2 :
2
return Reconstructed scaled dot product Ẑ Algorithm 2 VQVAE as an Attention Controller Input: scaled dot product: Z Output: Reconstructed scaled dot product: Ẑ 1: Z e ← Encoder(Z) 2: Z e = EncoderLayers(Z) 3: // EncoderLayers contain two linear layers separated by ReLU activation 4: Z v ← VQ_update(Z e ) 5: Compute distances:d ij = ∥Z e [:, i] − Z prev v [:, j]∥ 2 6: Find nearest indices: j i = arg min j d ij 7: Update codebook: Z v,new = βZ prev v + (1 − β)Z e [:, j i ] 8: return Z v,new 9: Z d ← Decoder(Z v ) 10: Z d = DecoderLayers(Z v ) 11: // DecoderLayers contain two linear layers separated by ReLU activation 12: return Reconstructed scaled dot product Ẑ</p>
<p>Triangles:</p>
<p>This dataset contains images of three white dot clusters forming a Triangle on a black background, as shown in Figure2(a).The task is to predict whether the Triangle is equilateral, making it a binary classification task.The images are 64 x 64 pixels, and the model must determine if the centroids of the white clusters are equidistant.The training data comprises 5 million pictures, and the test data contains 2 million images.We divide the image into 4 x 4 patches, each serving as a position in the input sequence for the architecture.The model must control its attention to ascertain the distance between the centroids.For multi-task scenario, we use the same images but create different labels for different tasks.We create two tasks with Triangles dataset: one, to detect whether the clusters form an equilateral Triangle, and second, whether majority of the clusters are in the upper half of the image.Samples from the data are shown in Figure5.For Out-of-Distribution experiments, we generate images with different shapes, and sizes and fill content for the Triangle vertices.As shown in Figure13, the shape can be a circle, Triangle, or square.The sizes can vary and the shape can be filled or empty.</p>
<p>Figure 13 :
13
Figure 13: Samples from Triangles-OOD data.</p>
<p>Figure 14 :
14
Figure 14: Samples from Polygons-OOD data.Polygons:The Regular Polygons dataset comprises images of Polygons created from dot clusters to predict whether a Polygon is regular, meaning all its sides are of equal length.The task is akin to identifying</p>
<p>Figure 15 :
15
Figure 15: Samples from CIFAR10 (left) and CIFAR-100 (right).</p>
<p>Figure 16 :
16
Figure 16: Samples from Corrupted CIFAR data.</p>
<p>Figure 17 :
17
Figure 17: Samples from SVHN data.</p>
<p>Figure 18 :
18
Figure 18: Samples from FashionMNIST data.</p>
<p>Figure 19 :
19
Figure 19: Samples from Tiny Imagenet data.</p>
<p>Figure 20 :
20
Figure 20: Samples from Sort-of-Clevr data.</p>
<p>Figure 21 :
21
Figure 21: Samples from ODIR-5K data.</p>
<p>Figure 22 :
22
Figure 22: Samples from Places365 data.</p>
<p>Figure 23 :
23
Figure 23: Samples from CelebA data.</p>
<p>Figure 24 :
24
Figure 24: Samples from MNIST data.</p>
<p>•</p>
<p>RTE: The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges.Examples are constructed based on news and Wikipedia text.The authors of the benchmark convert all datasets to a two-class split, where for three-class datasets they collapse neutral and contradiction into not entailment, for consistency.</p>
<p>C. 1
1
Figure 25: Task ID integration in the input</p>
<p>Table 1 :
1
Results of the multi-task experiments.Due to cost constraints for computational resources, we run Places365 with only one variant where the taskID is added to the VQVAE input and decoder.Bold values represent the best performance.
Model →BaselineASACTaskID in →inputinputdecoderinput&amp;decoderMetrics ↓ Dataset ↓SOC63.15 ± 7.39 67.08 ± 2.8473.23 ± 6.4 71.09 ± 4.35MNIST96.78 ± 0.497.17 ± 0.1397.04 ± 0.26 97.35 ± 0.32AccuracyCIFAR10-SVHN 85.94 ± 1.72 86.58 ± 0.64 84.25 ± 0.28 86.56 ± 0.63Triangles78.66 ± 0.83 96.71 ± 1.28 96.01 ± 1.39 94.39 ± 3.2ODIR-5K83.44 ± 0.01 83.46 ± 0.01 75.60 ± 5.28 79.32 ± 5.24Precision Places36560.46--61.03Recall54.04--54.56F156.25--56.82</p>
<p>Table 2 :
2
Results with GLUE dataset
GLUE Dataset MetricBaselineASACp-valueSTSBPearson SpearmanR 0.8622 ± 0.0027 0.8625 ± 0.0010 0.8327 0.8644 ± 0.0029 0.8651 ± 0.0012 0.5795SST2Accuracy0.8986 ± 0.0102 0.9052 ± 0.0025 0.0001QNLIAccuracy0.8850 ± 0.0019 0.8895 ± 0.0027 0.0105MRPCAccuracy F10.8884 ± 0.0011 0.8915 ± 0.0026 0.0311 0.8367 ± 0.0021 0.8431 ± 0.0073 0.022QQPAccuracy F10.9028 ± 0.0008 0.9033 ± 0.001 0.87 ± 0.0013 0.8705 ± 0.0016 0.05 0.057RTEAccuracy0.5870 ± 0.0126 0.6238 ± 0.0131 0.0002</p>
<p>Table 3 :
3
Statistics of Dataset
DatasetChannels Image size Train samples Test samples Classes Samples per classFashionMNIST128x286000010000107000CIFAR-10332x325000010000106000CIFAR-100332x325000010000100600SVHN332x32732572603210variableTriangles164x645000000100000022500000Polygons164x645000000100000022500000Tiny Imagenet364x6410000010000200500Places3653256x2561803460328500365variableCelebA3218x1781627703982940variableSort-of-Clevr375x755000010000105000MNIST128x286000010000106000ODIR-5K3variable28007008variableCIFAR10-SVHN 332x321232573603210variable</p>
<p>Table 4 :
4
GLUE dataset summary
Dataset MetricsTrain Validation Labels DistributionSTSBPearson, spearmanR 5.75k 1.5k0 to 5 imbalancedSST2Accuracy67.3k 8720,1imbalancedQNLIAccuracy105k5.46k0,1balancedMRPCAccuracy, F13.67k 4080,1imbalancedQQPAccuracy, F1364k40.4k0,1imbalancedRTEAccuracy2.49k 2770,1balanced
https://huggingface.co/docs/transformers/main/en/model_doc/vit
https://www.cs.toronto.edu/~kriz/cifar.html
https://docs.pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html
https://www.kaggle.com/datasets/nimishmagre/tMNIST-typeface-MNIST
http://ufldl.stanford.edu/housenumbers/
https://github.com/RishikMani/Sort-of-Clevr
https://huggingface.co/datasets/ylecun/MNIST
https://www.kaggle.com/datasets/andrewmvd/ocular-disease-recognition-odir5k
https://github.com/hendrycks/robustness
https://github.com/hendrycks/robustness
https://huggingface.co/datasets/nyu-mll/glue
https://gluebenchmark.com/
25) which are concatenated to the input embeddings.Figure25also show the variations that we mention in the main content of how the task information is passed into the model.D Hyperparameters for different experimentsWe ran all the experiments on NVIDIA V100 16GB GPU from Google Cloud Platform and all the codes were written in PyTorch.The following hyperparameters are common in all the experiments: patch_size = 4, λ = 0.01, train_batch_size = 64, eval_batch_size = 64, learning_rate = 0.0001, weight_decay = 0.01, hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1, threshold_ema_dead_code = 2, commitment_cost = 1.0.The other hyperparameters are shown in Table5.For noisy and OOD experiments, the hyperparameters for CIFAR-C, Tiny Imagenet-C, Triangles-OOD, Polygons-OOD experiments remain the same as classification experiment hyperparameters of that of their non-corrupted datasets shown in the Table5. Hyperparameters for DistilBERT experiments are available in Table6.Other parameters apart from VQVAE-specific parameters remain the same in DistilBERT model.We performed grid search on several values of hyperparameters and provide the hyperparameter with best results presented in the main content.We show the hyperparameters for multi-task experiments in Table7.The following hyperparameters are common in all the multi-task experiments: patch_size = 4, λ = 0.01, train_batch_size = 64, eval_batch_size = 64, learning_rate = 0.0001, weight_decay = 0.01, hidden_dropout_prob = 0.1, attention_probs_dropout_prob = 0.1, threshold_ema_dead_code = 2, commitment_cost = 1.0.In all the vision experiments, the total number of parameters in the baseline and ASAC remains almost equal.
Equilateral triangles: A challenge for connectionist vision. Subutai Ahmad, Stephen Omohundro, 12th Annual Conference. CSS Pod. Psychology Press2022</p>
<p>Projected gradient descent adversarial attack and its defense on a fault diagnosis system. Mustafa Sinasi Ayas, Selen Ayas, Seddik M Djouadi, 2022 45th international conference on telecommunications and signal processing. IEEE2022</p>
<p>Longformer: The long-document transformer. Iz Beltagy, Matthew E Peters, Arman Cohan, arXiv:2004.051502020arXiv preprint</p>
<p>Generating long sequences with sparse transformers. Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever, arXiv:1904.105092019arXiv preprint</p>
<p>Transformer-xl: Attentive language models beyond a fixed-length context. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Ruslan Quoc V Le, Salakhutdinov, arXiv:1901.028602019arXiv preprint</p>
<p>Benjamin Devillers, Léopold Maytié, and Rufin VanRullen. Semi-supervised multimodal representation learning through a global workspace. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. Ieee2009. 202436Imagenet: A large-scale hierarchical image database</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, arXiv:2010.119292020arXiv preprint</p>
<p>Lida: A computational model of global workspace theory and developmental learning. Stan Franklin, Uma Ramamurthy, K D' Sidney, Lee Mello, Aregahegn Mccauley, Rodrigo L Negatu, Vivek Silva, Datla, 2007</p>
<p>Global workspace theory, its lida model and the underlying neuroscience. Stan Franklin, Steve Strain, Javier Snaider, Ryan Mccall, Usef Faghihi, Biologically Inspired Cognitive Architectures. 12012</p>
<p>Explaining and harnessing adversarial examples. Ian J Goodfellow, Jonathon Shlens, Christian Szegedy, arXiv:1412.65722014arXiv preprint</p>
<p>Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, arXiv:2103.01197Michael Mozer, and Yoshua Bengio. Coordination among neural modules through a shared global workspace. 2021arXiv preprint</p>
<p>The attention schema theory: A foundation for engineering artificial consciousness. S A Michael, Graziano, Frontiers in Robotics and AI. 4602017</p>
<p>Consciousness and the attention schema: Why it has to be right. S A Michael, Graziano, Cognitive Neuropsychology. 373-42020</p>
<p>A conceptual framework for consciousness. S A Michael, Graziano, Proceedings of the National Academy of Sciences. 11918e21169331192022</p>
<p>The attention schema theory: a mechanistic account of subjective awareness. S A Michael, Taylor W Graziano, Webb, Frontiers in psychology. 65002015</p>
<p>Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, Ross Girshick, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Reformer: The efficient transformer. Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya, arXiv:2001.044512020arXiv preprint</p>
<p>Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, Luke Zettlemoyer, arXiv:1910.134612019BartarXiv preprint</p>
<p>Dianbo Liu, Samuele Bolotta, He Zhu, Yoshua Bengio, Guillaume Dumas, arXiv:2305.17375Attention schema in neural agents. 2023arXiv preprint</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. 2019arXiv preprint</p>
<p>Deep learning face attributes in the wild. Ziwei Liu, Ping Luo, Xiaogang Wang, Xiaoou Tang, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision2015</p>
<p>A neural attentional model for access to consciousness: A global workspace perspective. Bernard J Jb Newman, Baars, Concepts in Neuroscience. 421993</p>
<p>Unsupervised learning of global factors in deep generative models. Ignacio Peis, Pablo M Olmos, Antonio Artes-Rodriguez, Pattern Recognition. 1341091302023</p>
<p>Flexible parameter sharing networks. Chengkai Piao, Jinmao Wei, Yapeng Zhu, Hengpeng Xu, CCF International Conference on Natural Language Processing and Chinese Computing. Springer2020</p>
<p>Enhance eye disease detection using learnable probabilistic discrete latents in machine learning architectures. Anirudh Prabhakaran, Yekun Xiao, Ching-Yu Cheng, Dianbo Liu, arXiv:2402.168652024arXiv preprint</p>
<p>Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter. Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, arXiv:1910.011082019arXiv preprint</p>
<p>A simple neural network module for relational reasoning. Advances in neural information processing systems. Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap, 201730</p>
<p>A neurologically inspired network model for graziano's attention schema theory for consciousness. Erik Van Den Boogaard, Jan Treur, Maxim Turpijn, International Work-Conference on the Interplay Between Natural and Artificial Computation. Springer2017</p>
<p>Advances in neural information processing systems. Aaron Van Den, Oriol Oord, Vinyals, 201730Neural discrete representation learning</p>
<p>Deep learning and the global workspace theory. Rufin Vanrullen, Ryota Kanai, Trends in Neurosciences. 4492021</p>
<p>Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, 201730Attention is all you need</p>
<p>Linformer: Self-attention with linear complexity. Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, Hao Ma, arXiv:2006.047682020arXiv preprint</p>
<p>The attention schema theory in a neural network agent: Controlling visuospatial attention using a descriptive model of attention. Andrew I Wilterson, Michael, Graziano, Proceedings of the National Academy of Sciences. 11833e21024211182021</p>
<p>Xlnet: Generalized autoregressive pretraining for language understanding. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, Quoc V Le, Advances in neural information processing systems. 322019</p>
<p>Places: A 10 million image database for scene recognition. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, IEEE transactions on pattern analysis and machine intelligence. 2017a40</p>
<p>Scene hierarchy for places365-standard dataset. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, 2017b</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3100 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3100</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3100</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-264590648</p>
                <p><strong>Paper Title:</strong> <a href="https://aclanthology.org/2023.emnlp-main.225.pdf" target="_blank">Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities. Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem. A significant disadvantage is that foundational LLMs are typically not available for fine-tuning, making adaptation computationally prohibitive. We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM. We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps. These subproblems are answered by a solver. We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as black-box) and guide it through subproblems, thereby rendering our method solver-agnostic. Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4. Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique. Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3100.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3100.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaSLaM (GPT-3.5 solver)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaSLaM: Decomposition And Solution LAnguage Models (with text-davinci-003 solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular system that separates a small finetuned decomposer LM (LLaMA-13B) from a black-box solver LM (text-davinci-003 / GPT-3.5); the decomposer is trained (supervised + PPO) to generate subproblems conditioned on the original question and the solver's initial CoT output, iteratively guiding the solver to a correct final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's 175B autoregressive instruction-tuned model used as a black-box solver in experiments; prompted with chain-of-thought and appended subproblem contexts generated by the decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT) reasoning executed by the solver', 'explicit subproblem decomposition generated by a separate decomposer', 'feedback-guided iterative prompting (decomposer conditions on solver initial CoT and answer)', 'policy-gradient (PPO) optimized decomposition policy']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The solver uses CoT-style prompting to produce an initial answer and reasoning steps; the decomposer (LLaMA-13B) is first supervised to generate subproblems from gold CoTs, then policy-gradient (PPO) finetuned using rewards that incorporate entity coverage, subproblem-answer consistency, operation-order matching, CoT proximity, and final-answer correctness. Generated subproblems are fed back to the solver iteratively, and the original question is appended for final answer generation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse: the system explicitly separates two distinct reasoning styles — decomposition (a small RL-trained policy model producing subproblems) and solution generation (a large pretrained CoT solver). Diversity arises via architectural modularity and differing training objectives (supervised LM fine-tuning + PPO for decomposer vs. standard CoT prompting for solver).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, AQuA, JEEBench (Mathematics and Physics)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks of multi-step mathematical and scientific reasoning: MATH (various math domains), AQuA (middle-school math multiple-choice), and JEEBench (hard math/physics exam-style questions with MCQ, numerical, and multi-answer tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported highlights: AQuA exact-match accuracy: CoT (baseline) = 41.6% -> DaSLaM (GPT-3.5 solver) = 54.5% (zero-shot). MATH aggregated: DaSLaM+GPT-3.5 = 30.23% (reported aggregate) vs ChatGPT (26.4%); per-domain MATH improvements (absolute over CoT): Pre-Algebra +11.7, Intermediate Algebra +8.4, Number Theory +7.7, Geometry +7.2, Probability & Combinatorics +5.0, Algebra +5.8, Calculus +4.2. JEEBench: GPT-3.5 base ~10.4 (Physics) and ~10.7 (Math) -> DaSLaM: 22.42 (Physics) and 22.07 (Math) (absolute increases >10 points; relative >100% in some categories). The paper also reports DaSLaM sometimes outperforming few-shot GPT-4 on AQuA by ~3.9 points in the reported comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Extensive comparisons/ablations against CoT, Least-to-most (L2M), Progressive Hint Prompting (PHP), Demonstrate-Search-Predict (DSP), and three ablations: DaSLaM-NF (decomposer without solver feedback), Finetuned-solver (finetuned LLaMA solver), and GPT-3.5-decomposer (prompted GPT-3.5 used as decomposer). Key contrasts: (1) DaSLaM (finetuned LLaMA decomposer + feedback) consistently outperforms CoT and other prompting baselines across datasets; (2) DaSLaM-NF improves over CoT but is substantially worse than full DaSLaM, demonstrating the importance of solver-feedback conditioning; (3) a finetuned small decomposer (LLaMA-13B) outperforms using GPT-3.5 as a prompted decomposer despite the latter being much larger; (4) finetuning the decomposer is more effective than finetuning the solver.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A specialized, relatively small decomposer LM fine-tuned (supervised + PPO) to generate solver-conditioned subproblems can greatly improve performance of a large black-box solver that uses CoT prompting; modular heterogeneity (different models specialized for decomposition vs solution) yields substantial gains and solver-agnostic generalization to different solver scales; solver-feedback during decomposer training is crucial; small finetuned decomposers can outperform prompting much larger models to act as decomposers.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>DaSLaM-NF (no feedback) gives smaller gains than DaSLaM; cases where the decomposer generates subproblems that are not answerable or not helpful (failure modes discussed); certain domains (Probability & Combinatorics, Calculus) show smaller absolute gains than algebraic domains; some individual failure cases exist where DaSLaM could not trace back or correct the solver errors because it does not generate subproblems based on answers to previous subproblems (no explicit backward-tracing mechanism).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3100.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3100.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (CoT) - GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting (applied to text-davinci-003 / GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that asks the model to produce intermediate reasoning steps (rationales) before producing a final answer; used as the baseline solver behavior in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI 175B autoregressive instruction-tuned model; used with CoT prompts to elicit reasoning steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (CoT)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Prompting the solver to produce a sequence of intermediate reasoning steps (rationales) and then the final answer in a single pass of LM inference.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single-style: relies on the solver to both decompose and solve within the same CoT trace; no explicit modular decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, AQuA, JEEBench (used as baseline measurements)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>See DaSLaM entry; CoT is used as the primary baseline across the same datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported baseline numbers include AQuA CoT = 41.6% exact match; MATH per-category baselines shown in Table 1 (e.g., Algebra 27.6%, Pre-Algebra 16.4% etc.). JEEBench baseline scores near-random (e.g., GPT-3.5 without DaSLaM scoring ~10.4–10.7 on some JEEBench categories).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT is compared to modular decomposition (DaSLaM) and other prompting strategies; CoT alone underperforms DaSLaM consistently. L2M and PHP sometimes do not improve over CoT and can even degrade performance in some domains.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT alone often misses critical decomposition steps or hallucinates irrelevant steps; when decomposition and solution are conflated in one model, larger models are needed for both abilities to emerge, motivating modular approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>CoT sometimes produces incorrect assumptions that propagate (error multiplication along chain), and in many experiments is significantly outperformed by DaSLaM-augmented solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3100.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3100.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-most (L2M) - GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-most prompting (applied to GPT-3.5 solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting method that asks the LM to generate simpler, related subproblems from a complex problem and solve them sequentially (a prompting-only decomposition approach); used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as the solver for L2M prompting baseline; L2M asks the same solver to produce and solve subproblems via prompting rather than using a separate decomposer LM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['prompted decomposition via Least-to-most', 'sequential in-context subproblem solving']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>L2M prompts the LM to break the problem into a sequence of subproblems (least difficult first) and then conditions subsequent solutions on prior subproblem answers; all done via prompt engineering without separate finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar-style prompting: decomposition and solving are performed by the same LM via prompting (not architecturally separated), resulting in similar reasoning style across steps.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, AQuA, JEEBench</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks for multi-step reasoning used to evaluate L2M as a prompting baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Paper reports that L2M often fails to improve over CoT on several MATH domains (e.g., showed drops on Probability & Combinatorics, Number Theory, and Algebra in some cases) and 'generally fails' when used with smaller LLaMA solvers; specific numeric degradations are reported in Table 1/2 but are dataset- and domain-dependent.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to DaSLaM and CoT; L2M underperforms DaSLaM and in multiple domains shows no gain or negative change versus CoT. With smaller solvers (LLaMA 13B/33B) L2M fails to provide improvement while DaSLaM still helps.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompting the solver itself to decompose (L2M) lacks the specialized expertise of a finetuned decomposer and is less robust across solver scales; errors introduced early can persist when the same model both decomposes and solves.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>L2M showed drops in accuracy in several MATH categories compared to CoT and often failed entirely for smaller solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3100.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3100.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Progressive Hint Prompting (PHP) - GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Progressive Hint Prompting (applied to GPT-3.5 solver)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential prompting method that first asks the LM for a base answer and then uses that answer as an iterative hint to guide it progressively to the final answer; used as a baseline in the paper (1-shot variant).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Progressive-hint prompting improves reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as a solver baseline with PHP prompting; original PHP uses multi-shot prompting but the paper evaluates a 1-shot variant for parity with other zero-shot methods.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['progressive-hint iterative prompting']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The model is asked to provide an initial (often flawed) answer, and subsequent prompts present hints or corrections derived from that initial response, progressively steering the model toward a refined final answer.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar-style iterative prompting: method still uses the same LM for hint generation and consumption; not architecturally modular.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, AQuA, JEEBench</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks used to measure iterative hinting baseline performance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Paper reports that PHP in the 1-shot configuration generally fails to improve much over CoT in several tasks and is often inferior to DaSLaM; numeric specifics vary by dataset (reported in tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to DaSLaM and other baselines; PHP's improvements are small or absent relative to CoT and much smaller than DaSLaM's gains. The paper notes that original PHP uses 8-shot prompting, but to be fair to other zero-shot methods they used a 1-shot variant.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Iterative hint prompting (when constrained to 1-shot) does not match the performance gains achieved by a specialized finetuned decomposer that conditions on solver feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>1-shot PHP often gives meager or no improvement over CoT and is outperformed by DaSLaM across datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3100.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3100.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Demonstrate-Search-Predict (DSP) - GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Demonstrate-Search-Predict composing retrieval and language models (used as baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented pipeline (Demonstrate-Search-Predict) combining retrieval with language model reasoning; used as one of the baselines for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Used as the language-model component in DSP baseline comparisons; DSP composes retrieval and LM steps for knowledge-intensive tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['retrieval-augmented reasoning (Demonstrate-Search-Predict)', 'composed pipeline of evidence demonstration, search, and prediction']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DSP integrates retrieval-search steps with LM-based demonstration and prediction in a pipeline; in this paper DSP is used as a baseline though the datasets are primarily reasoning/math rather than knowledge retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse in principle (retrieval + LM), but in this paper DSP is only a baseline and not the focus of modular decomposition experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, AQuA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Knowledge- or reasoning-intensive benchmarks; DSP is compared as one baseline among prompting strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported in the paper's baseline tables; DSP generally underperforms DaSLaM on the reasoning benchmarks used (exact numeric values reported in paper tables).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>DSP is outperformed by DaSLaM on the evaluated reasoning tasks; the paper emphasizes modular decomposition (DaSLaM) over retrieval augmentation for these specific math/logic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>For these multi-step reasoning benchmarks, modular decomposition (DaSLaM) gives larger gains than DSP baseline; DSP is not the competitive approach here.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>DSP did not match DaSLaM's improvements on math reasoning tasks in the authors' experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3100.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3100.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-13B decomposer (finetuned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 13B finetuned as a solver-agnostic decomposer (supervised + PPO)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13-billion-parameter LLaMA model finetuned in two stages (supervised LM fine-tuning on subproblems, then PPO policy optimization) to act as a decomposer that generates solver-conditioned subproblems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B (finetuned decomposer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA 13B base model adapted via LoRA supervised fine-tuning using subproblem examples (generated by text-davinci-003) and then PPO policy-gradient finetuning to optimize decomposer behavior against a black-box solver; used as the primary decomposer in DaSLaM.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['supervised decomposition (generate subproblems from gold CoTs)', 'reinforcement learning (PPO) guided decomposition conditioned on solver outputs']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Stage 1: supervised LM fine-tuning to learn mapping (question, solver initial CoT/answer) -> subproblems. Stage 2: PPO optimizing a composite reward (entity coverage, consistency, operation-order match, CoT proximity, final correctness) treating the solver as a black-box environment.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse at the system level: the decomposer's generation policy is a distinct method (RL-optimized subproblem generation) versus the solver's CoT style; architectural separation enforces differing roles and reasoning styles.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, AQuA, GSM8K, StrategyQA (training sources) and MATH/AQuA/JEEBench (evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Decomposer trained on samples from MATH, AQuA, GSM8K, and StrategyQA (7k supervised examples), then PPO trained using more data; evaluated on MATH/AQuA/JEEBench tasks by pairing with various solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Using this finetuned LLaMA-13B decomposer with GPT-3.5 solver yields the large performance gains reported under 'DaSLaM' (see main DaSLaM entry). Ablation: replacing this decomposer with prompted GPT-3.5 (no finetuning) reduces performance despite GPT-3.5 being larger.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared against prompted-large-model decomposers (GPT-3.5 as decomposer) and against not using solver-feedback (DaSLaM-NF). The finetuned LLaMA-13B decomposer + feedback outperforms these alternatives consistently.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Finetuning a small model specifically for decomposition (including RL to account for solver behavior) is more effective than prompting much larger LMs to act as decomposers; the finetuned decomposer generalizes across solver sizes.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>The decomposer can still generate unhelpful subproblems in some failure cases; token-budget and RL exploration costs are noted limitations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3100.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3100.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5 as decomposer (prompted)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Using OpenAI text-davinci-003 (GPT-3.5) prompted as a decomposer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation where the larger GPT-3.5 model is prompted (one-shot examples) to generate subproblems instead of using a finetuned small decomposer; evaluated to test whether prompting a big model can replace a finetuned small model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-003 (GPT-3.5) used as prompted decomposer</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-engineered GPT-3.5 (one-shot) used to produce subproblems for the solver; no finetuning of GPT-3.5 was performed for this role in the ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>175B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['prompted decomposition (one-shot example)', 'solver CoT for answers']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>A prompted approach where GPT-3.5 is given an example of decomposition and then asked to produce subproblems for new questions; produced subproblems are fed to the solver similarly to DaSLaM but without decomposer finetuning or PPO.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar-style prompting approach: decomposition and solution use the same LM style (prompted outputs), lacking the RL-optimized modular specialization of DaSLaM.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, AQuA, JEEBench (used as an ablation baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks where prompted GPT-3.5 decomposer was compared to the finetuned LLaMA-13B decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>The paper reports that a finetuned LLaMA-13B decomposer outperforms the prompted GPT-3.5 decomposer, despite GPT-3.5 being much larger; numeric table entries show DaSLaM (with finetuned LLaMA decomposer) > GPT-3.5-decomposer variants across tasks (exact numeric table entries in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct ablation: finetuned small decomposer > prompted large decomposer. The prompted GPT-3.5 decomposer (no finetuning) yields smaller improvements than DaSLaM with LLaMA-13B decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompting a large pretrained LM to decompose does not match the performance of a small LM that was specialized (supervised + PPO) for decomposition; demonstrates value of task-specific finetuning even on smaller models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Prompted GPT-3.5 as decomposer is a weaker choice than the finetuned small decomposer across the evaluated benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3100.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3100.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DaSLaM with LLaMA solvers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DaSLaM (LLaMA-13B decomposer) paired with LLaMA-13B / LLaMA-33B solvers</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Experiments showing that the finetuned decomposer generalizes to smaller solvers (LLaMA-13B and LLaMA-33B) and yields substantial improvements even when solver capacity is limited.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-13B and LLaMA-33B (used as solvers)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Open LLaMA family models: 13B and 33B parameter variants used as solvers. The 13B solver was instruction-finetuned using Alpaca; 33B solver used an instruction-finetuned HuggingFace checkpoint. Both were evaluated with CoT vs. DaSLaM augmentation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B and 33B</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['chain-of-thought (solver)', 'external decomposer-generated subproblems (DaSLaM)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>The solver (LLaMA 13B/33B) follows CoT prompting for reasoning; the decomposer supplies subproblems and feedback context generated by a PPO-trained LLaMA-13B decomposer as in DaSLaM.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse at system level: separate decomposer policy and solver CoT, leading to improved performance relative to single-model prompting approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>MATH, AQuA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Benchmarks to assess cross-solver generalization of the decomposer module.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Qualitative and tabulated results show DaSLaM nearly doubles base CoT performance across multiple MATH tasks for LLaMA solvers; L2M and PHP generally fail with these smaller solvers while DaSLaM yields consistent improvements. Exact per-task numbers are provided in Table 2 of the paper (garbled in places in the text), but the paper emphasizes consistent, often large relative gains for DaSLaM across solver scales.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>DaSLaM vs CoT/L2M/PHP on LLaMA solvers: L2M/PHP often fail to improve; DaSLaM (with or without feedback) produces substantial improvements, and feedback importance scales with solver size (bigger solvers benefit more from feedback-conditioned decomposer).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>DaSLaM generalizes to smaller solvers and consistently improves performance across solver scales, demonstrating solver-agnostic behavior of the finetuned decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Some baseline methods (L2M/PHP) collapse when solver capacity is reduced; DaSLaM still has failure cases when the decomposer generates subproblems that are not helpful or answerable by the solver.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Progressive-hint prompting improves reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Decomposed prompting: A modular approach for solving complex tasks <em>(Rating: 2)</em></li>
                <li>Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp <em>(Rating: 1)</em></li>
                <li>Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3100",
    "paper_id": "paper-264590648",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "DaSLaM (GPT-3.5 solver)",
            "name_full": "DaSLaM: Decomposition And Solution LAnguage Models (with text-davinci-003 solver)",
            "brief_description": "A modular system that separates a small finetuned decomposer LM (LLaMA-13B) from a black-box solver LM (text-davinci-003 / GPT-3.5); the decomposer is trained (supervised + PPO) to generate subproblems conditioned on the original question and the solver's initial CoT output, iteratively guiding the solver to a correct final answer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5)",
            "model_description": "OpenAI's 175B autoregressive instruction-tuned model used as a black-box solver in experiments; prompted with chain-of-thought and appended subproblem contexts generated by the decomposer.",
            "model_size": "175B",
            "reasoning_methods": [
                "chain-of-thought (CoT) reasoning executed by the solver",
                "explicit subproblem decomposition generated by a separate decomposer",
                "feedback-guided iterative prompting (decomposer conditions on solver initial CoT and answer)",
                "policy-gradient (PPO) optimized decomposition policy"
            ],
            "reasoning_methods_description": "The solver uses CoT-style prompting to produce an initial answer and reasoning steps; the decomposer (LLaMA-13B) is first supervised to generate subproblems from gold CoTs, then policy-gradient (PPO) finetuned using rewards that incorporate entity coverage, subproblem-answer consistency, operation-order matching, CoT proximity, and final-answer correctness. Generated subproblems are fed back to the solver iteratively, and the original question is appended for final answer generation.",
            "diversity_of_methods": "diverse: the system explicitly separates two distinct reasoning styles — decomposition (a small RL-trained policy model producing subproblems) and solution generation (a large pretrained CoT solver). Diversity arises via architectural modularity and differing training objectives (supervised LM fine-tuning + PPO for decomposer vs. standard CoT prompting for solver).",
            "reasoning_task_name": "MATH, AQuA, JEEBench (Mathematics and Physics)",
            "reasoning_task_description": "Benchmarks of multi-step mathematical and scientific reasoning: MATH (various math domains), AQuA (middle-school math multiple-choice), and JEEBench (hard math/physics exam-style questions with MCQ, numerical, and multi-answer tasks).",
            "performance_by_method": "Reported highlights: AQuA exact-match accuracy: CoT (baseline) = 41.6% -&gt; DaSLaM (GPT-3.5 solver) = 54.5% (zero-shot). MATH aggregated: DaSLaM+GPT-3.5 = 30.23% (reported aggregate) vs ChatGPT (26.4%); per-domain MATH improvements (absolute over CoT): Pre-Algebra +11.7, Intermediate Algebra +8.4, Number Theory +7.7, Geometry +7.2, Probability & Combinatorics +5.0, Algebra +5.8, Calculus +4.2. JEEBench: GPT-3.5 base ~10.4 (Physics) and ~10.7 (Math) -&gt; DaSLaM: 22.42 (Physics) and 22.07 (Math) (absolute increases &gt;10 points; relative &gt;100% in some categories). The paper also reports DaSLaM sometimes outperforming few-shot GPT-4 on AQuA by ~3.9 points in the reported comparison.",
            "comparison_of_methods": "Extensive comparisons/ablations against CoT, Least-to-most (L2M), Progressive Hint Prompting (PHP), Demonstrate-Search-Predict (DSP), and three ablations: DaSLaM-NF (decomposer without solver feedback), Finetuned-solver (finetuned LLaMA solver), and GPT-3.5-decomposer (prompted GPT-3.5 used as decomposer). Key contrasts: (1) DaSLaM (finetuned LLaMA decomposer + feedback) consistently outperforms CoT and other prompting baselines across datasets; (2) DaSLaM-NF improves over CoT but is substantially worse than full DaSLaM, demonstrating the importance of solver-feedback conditioning; (3) a finetuned small decomposer (LLaMA-13B) outperforms using GPT-3.5 as a prompted decomposer despite the latter being much larger; (4) finetuning the decomposer is more effective than finetuning the solver.",
            "key_findings": "A specialized, relatively small decomposer LM fine-tuned (supervised + PPO) to generate solver-conditioned subproblems can greatly improve performance of a large black-box solver that uses CoT prompting; modular heterogeneity (different models specialized for decomposition vs solution) yields substantial gains and solver-agnostic generalization to different solver scales; solver-feedback during decomposer training is crucial; small finetuned decomposers can outperform prompting much larger models to act as decomposers.",
            "counter_examples_or_negative_results": "DaSLaM-NF (no feedback) gives smaller gains than DaSLaM; cases where the decomposer generates subproblems that are not answerable or not helpful (failure modes discussed); certain domains (Probability & Combinatorics, Calculus) show smaller absolute gains than algebraic domains; some individual failure cases exist where DaSLaM could not trace back or correct the solver errors because it does not generate subproblems based on answers to previous subproblems (no explicit backward-tracing mechanism).",
            "uuid": "e3100.0",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Chain-of-Thought (CoT) - GPT-3.5",
            "name_full": "Chain-of-Thought prompting (applied to text-davinci-003 / GPT-3.5)",
            "brief_description": "A prompting method that asks the model to produce intermediate reasoning steps (rationales) before producing a final answer; used as the baseline solver behavior in this paper.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5)",
            "model_description": "OpenAI 175B autoregressive instruction-tuned model; used with CoT prompts to elicit reasoning steps.",
            "model_size": "175B",
            "reasoning_methods": [
                "chain-of-thought (CoT)"
            ],
            "reasoning_methods_description": "Prompting the solver to produce a sequence of intermediate reasoning steps (rationales) and then the final answer in a single pass of LM inference.",
            "diversity_of_methods": "single-style: relies on the solver to both decompose and solve within the same CoT trace; no explicit modular decomposition.",
            "reasoning_task_name": "MATH, AQuA, JEEBench (used as baseline measurements)",
            "reasoning_task_description": "See DaSLaM entry; CoT is used as the primary baseline across the same datasets.",
            "performance_by_method": "Reported baseline numbers include AQuA CoT = 41.6% exact match; MATH per-category baselines shown in Table 1 (e.g., Algebra 27.6%, Pre-Algebra 16.4% etc.). JEEBench baseline scores near-random (e.g., GPT-3.5 without DaSLaM scoring ~10.4–10.7 on some JEEBench categories).",
            "comparison_of_methods": "CoT is compared to modular decomposition (DaSLaM) and other prompting strategies; CoT alone underperforms DaSLaM consistently. L2M and PHP sometimes do not improve over CoT and can even degrade performance in some domains.",
            "key_findings": "CoT alone often misses critical decomposition steps or hallucinates irrelevant steps; when decomposition and solution are conflated in one model, larger models are needed for both abilities to emerge, motivating modular approaches.",
            "counter_examples_or_negative_results": "CoT sometimes produces incorrect assumptions that propagate (error multiplication along chain), and in many experiments is significantly outperformed by DaSLaM-augmented solvers.",
            "uuid": "e3100.1",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Least-to-most (L2M) - GPT-3.5",
            "name_full": "Least-to-most prompting (applied to GPT-3.5 solver)",
            "brief_description": "A prompting method that asks the LM to generate simpler, related subproblems from a complex problem and solve them sequentially (a prompting-only decomposition approach); used as a baseline.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5)",
            "model_description": "Used as the solver for L2M prompting baseline; L2M asks the same solver to produce and solve subproblems via prompting rather than using a separate decomposer LM.",
            "model_size": "175B",
            "reasoning_methods": [
                "prompted decomposition via Least-to-most",
                "sequential in-context subproblem solving"
            ],
            "reasoning_methods_description": "L2M prompts the LM to break the problem into a sequence of subproblems (least difficult first) and then conditions subsequent solutions on prior subproblem answers; all done via prompt engineering without separate finetuning.",
            "diversity_of_methods": "similar-style prompting: decomposition and solving are performed by the same LM via prompting (not architecturally separated), resulting in similar reasoning style across steps.",
            "reasoning_task_name": "MATH, AQuA, JEEBench",
            "reasoning_task_description": "Benchmarks for multi-step reasoning used to evaluate L2M as a prompting baseline.",
            "performance_by_method": "Paper reports that L2M often fails to improve over CoT on several MATH domains (e.g., showed drops on Probability & Combinatorics, Number Theory, and Algebra in some cases) and 'generally fails' when used with smaller LLaMA solvers; specific numeric degradations are reported in Table 1/2 but are dataset- and domain-dependent.",
            "comparison_of_methods": "Compared to DaSLaM and CoT; L2M underperforms DaSLaM and in multiple domains shows no gain or negative change versus CoT. With smaller solvers (LLaMA 13B/33B) L2M fails to provide improvement while DaSLaM still helps.",
            "key_findings": "Prompting the solver itself to decompose (L2M) lacks the specialized expertise of a finetuned decomposer and is less robust across solver scales; errors introduced early can persist when the same model both decomposes and solves.",
            "counter_examples_or_negative_results": "L2M showed drops in accuracy in several MATH categories compared to CoT and often failed entirely for smaller solvers.",
            "uuid": "e3100.2",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Progressive Hint Prompting (PHP) - GPT-3.5",
            "name_full": "Progressive Hint Prompting (applied to GPT-3.5 solver)",
            "brief_description": "A sequential prompting method that first asks the LM for a base answer and then uses that answer as an iterative hint to guide it progressively to the final answer; used as a baseline in the paper (1-shot variant).",
            "citation_title": "Progressive-hint prompting improves reasoning in large language models",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5)",
            "model_description": "Used as a solver baseline with PHP prompting; original PHP uses multi-shot prompting but the paper evaluates a 1-shot variant for parity with other zero-shot methods.",
            "model_size": "175B",
            "reasoning_methods": [
                "progressive-hint iterative prompting"
            ],
            "reasoning_methods_description": "The model is asked to provide an initial (often flawed) answer, and subsequent prompts present hints or corrections derived from that initial response, progressively steering the model toward a refined final answer.",
            "diversity_of_methods": "similar-style iterative prompting: method still uses the same LM for hint generation and consumption; not architecturally modular.",
            "reasoning_task_name": "MATH, AQuA, JEEBench",
            "reasoning_task_description": "Benchmarks used to measure iterative hinting baseline performance.",
            "performance_by_method": "Paper reports that PHP in the 1-shot configuration generally fails to improve much over CoT in several tasks and is often inferior to DaSLaM; numeric specifics vary by dataset (reported in tables).",
            "comparison_of_methods": "Compared to DaSLaM and other baselines; PHP's improvements are small or absent relative to CoT and much smaller than DaSLaM's gains. The paper notes that original PHP uses 8-shot prompting, but to be fair to other zero-shot methods they used a 1-shot variant.",
            "key_findings": "Iterative hint prompting (when constrained to 1-shot) does not match the performance gains achieved by a specialized finetuned decomposer that conditions on solver feedback.",
            "counter_examples_or_negative_results": "1-shot PHP often gives meager or no improvement over CoT and is outperformed by DaSLaM across datasets.",
            "uuid": "e3100.3",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Demonstrate-Search-Predict (DSP) - GPT-3.5",
            "name_full": "Demonstrate-Search-Predict composing retrieval and language models (used as baseline)",
            "brief_description": "A retrieval-augmented pipeline (Demonstrate-Search-Predict) combining retrieval with language model reasoning; used as one of the baselines for comparison.",
            "citation_title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5)",
            "model_description": "Used as the language-model component in DSP baseline comparisons; DSP composes retrieval and LM steps for knowledge-intensive tasks.",
            "model_size": "175B",
            "reasoning_methods": [
                "retrieval-augmented reasoning (Demonstrate-Search-Predict)",
                "composed pipeline of evidence demonstration, search, and prediction"
            ],
            "reasoning_methods_description": "DSP integrates retrieval-search steps with LM-based demonstration and prediction in a pipeline; in this paper DSP is used as a baseline though the datasets are primarily reasoning/math rather than knowledge retrieval.",
            "diversity_of_methods": "diverse in principle (retrieval + LM), but in this paper DSP is only a baseline and not the focus of modular decomposition experiments.",
            "reasoning_task_name": "MATH, AQuA",
            "reasoning_task_description": "Knowledge- or reasoning-intensive benchmarks; DSP is compared as one baseline among prompting strategies.",
            "performance_by_method": "Reported in the paper's baseline tables; DSP generally underperforms DaSLaM on the reasoning benchmarks used (exact numeric values reported in paper tables).",
            "comparison_of_methods": "DSP is outperformed by DaSLaM on the evaluated reasoning tasks; the paper emphasizes modular decomposition (DaSLaM) over retrieval augmentation for these specific math/logic tasks.",
            "key_findings": "For these multi-step reasoning benchmarks, modular decomposition (DaSLaM) gives larger gains than DSP baseline; DSP is not the competitive approach here.",
            "counter_examples_or_negative_results": "DSP did not match DaSLaM's improvements on math reasoning tasks in the authors' experiments.",
            "uuid": "e3100.4",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLaMA-13B decomposer (finetuned)",
            "name_full": "LLaMA 13B finetuned as a solver-agnostic decomposer (supervised + PPO)",
            "brief_description": "A 13-billion-parameter LLaMA model finetuned in two stages (supervised LM fine-tuning on subproblems, then PPO policy optimization) to act as a decomposer that generates solver-conditioned subproblems.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B (finetuned decomposer)",
            "model_description": "LLaMA 13B base model adapted via LoRA supervised fine-tuning using subproblem examples (generated by text-davinci-003) and then PPO policy-gradient finetuning to optimize decomposer behavior against a black-box solver; used as the primary decomposer in DaSLaM.",
            "model_size": "13B",
            "reasoning_methods": [
                "supervised decomposition (generate subproblems from gold CoTs)",
                "reinforcement learning (PPO) guided decomposition conditioned on solver outputs"
            ],
            "reasoning_methods_description": "Stage 1: supervised LM fine-tuning to learn mapping (question, solver initial CoT/answer) -&gt; subproblems. Stage 2: PPO optimizing a composite reward (entity coverage, consistency, operation-order match, CoT proximity, final correctness) treating the solver as a black-box environment.",
            "diversity_of_methods": "diverse at the system level: the decomposer's generation policy is a distinct method (RL-optimized subproblem generation) versus the solver's CoT style; architectural separation enforces differing roles and reasoning styles.",
            "reasoning_task_name": "MATH, AQuA, GSM8K, StrategyQA (training sources) and MATH/AQuA/JEEBench (evaluation)",
            "reasoning_task_description": "Decomposer trained on samples from MATH, AQuA, GSM8K, and StrategyQA (7k supervised examples), then PPO trained using more data; evaluated on MATH/AQuA/JEEBench tasks by pairing with various solvers.",
            "performance_by_method": "Using this finetuned LLaMA-13B decomposer with GPT-3.5 solver yields the large performance gains reported under 'DaSLaM' (see main DaSLaM entry). Ablation: replacing this decomposer with prompted GPT-3.5 (no finetuning) reduces performance despite GPT-3.5 being larger.",
            "comparison_of_methods": "Compared against prompted-large-model decomposers (GPT-3.5 as decomposer) and against not using solver-feedback (DaSLaM-NF). The finetuned LLaMA-13B decomposer + feedback outperforms these alternatives consistently.",
            "key_findings": "Finetuning a small model specifically for decomposition (including RL to account for solver behavior) is more effective than prompting much larger LMs to act as decomposers; the finetuned decomposer generalizes across solver sizes.",
            "counter_examples_or_negative_results": "The decomposer can still generate unhelpful subproblems in some failure cases; token-budget and RL exploration costs are noted limitations.",
            "uuid": "e3100.5",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "GPT-3.5 as decomposer (prompted)",
            "name_full": "Using OpenAI text-davinci-003 (GPT-3.5) prompted as a decomposer",
            "brief_description": "An ablation where the larger GPT-3.5 model is prompted (one-shot examples) to generate subproblems instead of using a finetuned small decomposer; evaluated to test whether prompting a big model can replace a finetuned small model.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "text-davinci-003 (GPT-3.5) used as prompted decomposer",
            "model_description": "Prompt-engineered GPT-3.5 (one-shot) used to produce subproblems for the solver; no finetuning of GPT-3.5 was performed for this role in the ablation.",
            "model_size": "175B",
            "reasoning_methods": [
                "prompted decomposition (one-shot example)",
                "solver CoT for answers"
            ],
            "reasoning_methods_description": "A prompted approach where GPT-3.5 is given an example of decomposition and then asked to produce subproblems for new questions; produced subproblems are fed to the solver similarly to DaSLaM but without decomposer finetuning or PPO.",
            "diversity_of_methods": "similar-style prompting approach: decomposition and solution use the same LM style (prompted outputs), lacking the RL-optimized modular specialization of DaSLaM.",
            "reasoning_task_name": "MATH, AQuA, JEEBench (used as an ablation baseline)",
            "reasoning_task_description": "Benchmarks where prompted GPT-3.5 decomposer was compared to the finetuned LLaMA-13B decomposer.",
            "performance_by_method": "The paper reports that a finetuned LLaMA-13B decomposer outperforms the prompted GPT-3.5 decomposer, despite GPT-3.5 being much larger; numeric table entries show DaSLaM (with finetuned LLaMA decomposer) &gt; GPT-3.5-decomposer variants across tasks (exact numeric table entries in paper).",
            "comparison_of_methods": "Direct ablation: finetuned small decomposer &gt; prompted large decomposer. The prompted GPT-3.5 decomposer (no finetuning) yields smaller improvements than DaSLaM with LLaMA-13B decomposer.",
            "key_findings": "Prompting a large pretrained LM to decompose does not match the performance of a small LM that was specialized (supervised + PPO) for decomposition; demonstrates value of task-specific finetuning even on smaller models.",
            "counter_examples_or_negative_results": "Prompted GPT-3.5 as decomposer is a weaker choice than the finetuned small decomposer across the evaluated benchmarks.",
            "uuid": "e3100.6",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "DaSLaM with LLaMA solvers",
            "name_full": "DaSLaM (LLaMA-13B decomposer) paired with LLaMA-13B / LLaMA-33B solvers",
            "brief_description": "Experiments showing that the finetuned decomposer generalizes to smaller solvers (LLaMA-13B and LLaMA-33B) and yields substantial improvements even when solver capacity is limited.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LLaMA-13B and LLaMA-33B (used as solvers)",
            "model_description": "Open LLaMA family models: 13B and 33B parameter variants used as solvers. The 13B solver was instruction-finetuned using Alpaca; 33B solver used an instruction-finetuned HuggingFace checkpoint. Both were evaluated with CoT vs. DaSLaM augmentation.",
            "model_size": "13B and 33B",
            "reasoning_methods": [
                "chain-of-thought (solver)",
                "external decomposer-generated subproblems (DaSLaM)"
            ],
            "reasoning_methods_description": "The solver (LLaMA 13B/33B) follows CoT prompting for reasoning; the decomposer supplies subproblems and feedback context generated by a PPO-trained LLaMA-13B decomposer as in DaSLaM.",
            "diversity_of_methods": "diverse at system level: separate decomposer policy and solver CoT, leading to improved performance relative to single-model prompting approaches.",
            "reasoning_task_name": "MATH, AQuA",
            "reasoning_task_description": "Benchmarks to assess cross-solver generalization of the decomposer module.",
            "performance_by_method": "Qualitative and tabulated results show DaSLaM nearly doubles base CoT performance across multiple MATH tasks for LLaMA solvers; L2M and PHP generally fail with these smaller solvers while DaSLaM yields consistent improvements. Exact per-task numbers are provided in Table 2 of the paper (garbled in places in the text), but the paper emphasizes consistent, often large relative gains for DaSLaM across solver scales.",
            "comparison_of_methods": "DaSLaM vs CoT/L2M/PHP on LLaMA solvers: L2M/PHP often fail to improve; DaSLaM (with or without feedback) produces substantial improvements, and feedback importance scales with solver size (bigger solvers benefit more from feedback-conditioned decomposer).",
            "key_findings": "DaSLaM generalizes to smaller solvers and consistently improves performance across solver scales, demonstrating solver-agnostic behavior of the finetuned decomposer.",
            "counter_examples_or_negative_results": "Some baseline methods (L2M/PHP) collapse when solver capacity is reduced; DaSLaM still has failure cases when the decomposer generates subproblems that are not helpful or answerable by the solver.",
            "uuid": "e3100.7",
            "source_info": {
                "paper_title": "Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models",
            "rating": 2,
            "sanitized_title": "leasttomost_prompting_enables_complex_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Progressive-hint prompting improves reasoning in large language models",
            "rating": 1,
            "sanitized_title": "progressivehint_prompting_improves_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Decomposed prompting: A modular approach for solving complex tasks",
            "rating": 2,
            "sanitized_title": "decomposed_prompting_a_modular_approach_for_solving_complex_tasks"
        },
        {
            "paper_title": "Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp",
            "rating": 1,
            "sanitized_title": "demonstratesearchpredict_composing_retrieval_and_language_models_for_knowledgeintensive_nlp"
        },
        {
            "paper_title": "Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions",
            "rating": 2,
            "sanitized_title": "distilling_multistep_reasoning_capabilities_of_large_language_models_into_smaller_models_via_semantic_decompositions"
        }
    ],
    "cost": 0.0208175,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning</p>
<p>Gurusha Juneja 
Iit Delhi 
India Subhabrata Dutta 
Sunny Manchhanda 
Tanmoy Chakraborty </p>
<p>IIT Delhi
India</p>
<p>Soumen Chakrabarti IIT Bombay
India</p>
<p>DYSL-AI
India</p>
<p>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning
2D9D0B1B135200C1C577934B01BED884
Large Language Models (LLMs) prompted to generate chain-of-thought (CoT) exhibit impressive reasoning capabilities.Recent attempts at prompt decomposition toward solving complex, multi-step reasoning problems depend on the ability of the LLM to simultaneously decompose and solve the problem.A significant disadvantage is that foundational LLMs are typically not available for finetuning, making adaptation computationally prohibitive.We believe (and demonstrate) that problem decomposition and solution generation are distinct capabilites, better addressed in separate modules, than by one monolithic LLM.We introduce DaSLaM, which uses a decomposition generator to decompose complex problems into subproblems that require fewer reasoning steps.These subproblems are answered by a solver.We use a relatively small (13B parameters) LM as the decomposition generator, which we train using policy gradient optimization to interact with a solver LM (regarded as blackbox) and guide it through subproblems, thereby rendering our method solver-agnostic.Evaluation on multiple different reasoning datasets reveal that with our method, a 175 billion parameter LM (text-davinci-003) can produce competitive or even better performance, compared to its orders-of-magnitude larger successor, GPT-4.Additionally, we show that DaSLaM is not limited by the solver's capabilities as a function of scale; e.g., solver LMs with diverse sizes give significant performance improvement with our solver-agnostic decomposition technique.Exhaustive ablation studies evince the superiority of our modular finetuning technique over exorbitantly large decomposer LLMs, based on prompting alone.</p>
<p>Introduction</p>
<p>In recent years, an astounding variety of text and NLP tasks have been accomplished by language models (LMs) (Devlin et al., 2019) -in essence, fitting continuous feature vectors to tokens and modeling smooth conditional distributions over thousands of token positions with multi-task objectives.The next generation of large LMs (LLMs) such as T5, GPT4 and Bard (Raffel et al., 2020;OpenAI, 2023) developed protean capabilities, extending to mathematical and logical ability, based on prompting and in-context learning.Chain-ofthought (CoT) prompting has been a key enabler (Wei et al., 2022;Feng et al., 2023).LLMs can solve middle-school word problems and equations reasonably well.It has also acquired the ability to invoke specialized external tools such as Wolfram Alpha (Wolfram, 2023;Schick et al., 2023).</p>
<p>Recent advances in LLMs have arisen largely from cleverly-engineered, ever-growing training data, rather than any significant change in network structure, which remains relatively regular, but with rapidly increasing network size and number of parameters.One outcome of such giant monolithic LLMs is unsustainable levels of hardware and energy (Dhar, 2020) to train them.Meanwhile, neurologists and brain scientists have known, via fMRI scans, inter alia, that cerebral functions are specialized and spatially localized (Fedorenko and Varley, 2016;Mahowald et al., 2023).</p>
<p>Many recent complex reasoning challenges thrown at LLMs have a two-level character -the input task needs to be decomposed into subtasks, then the subtasks need to be solved, and finally, subtask solutions have to be consolidated and combined to solve the input task.Existing approaches use the same LLM to both decompose and solve the task, sometimes in tangled and uninterpretable ways.Because the sharing of an LLM across these functions cannot be closely controlled, very large models are needed for this double ability (decompose and solve) to emerge.</p>
<p>Staying entirely inside the LLM regime, and avoiding the possibility of specialized tools, we Figure 1: Working example of DaSLaM on a mathematical reasoning question from the JEEBench dataset (Arora et al., 2023).In this example, the solver LM is text-davinci-003.In step 1 , the solver is prompted to answer the question (blue textbox) and it fails to answer correctly (red textbox).A problem decomposing LM generates subproblems (violet textboxes) conditioned on the original question and the initial response of the solver in step 2 .In step 3 , the solver answers these subproblems iteratively and appends to the prompt.Finally, the original problem is appended to the prompt in step 4 , and the solver answers it correctly (green textbox).ask a simple question -is it possible to offload the ability of problem decomposition to a dedicated, relatively smaller scale model, which is specialized and can act in synergy with any solver model of choice?To incorporate flexibility and better generalization, an immediate requirement of such a setup would be to enable a model-agnostic communication between the decomposer and the solver.</p>
<p>Our contributions.To study this research question, we develop DaSLaM (Decomposition And Solution LAnguage Models), in which we separate the decomposer from the solver, as shown in Figure 1.The solver LM can be conventionally trained or fine-tuned.In the illustration, when it answers a question incorrectly, the decomposer LM takes over to produce sub-questions.The partial solutions are appended and resubmitted to the solver LM, which solves the question correctly.The decomposer LM regards the solver as a black box, and uses reinforcement learning (RL) to become a specialized expert at decomposition, informed by the solver's mistakes.</p>
<p>Extensive experiments with three reasoning datasets (MATH, AQuA, and JEEBench) show that the proposed specialization improves the perfor-mance of OpenAI GPT-3 text-davinci-003 to outperform GPT-3.5 and even begins to compete with GPT-4, outperforming other similar methods.</p>
<p>DaSLaM boosts text-davinci-003 from an exact match accuracy of 41.6 to 54.5 in zero-shot regime, which is 3.9 points higher than few-shot GPT-4.Similarly, on Physics problems from JEEBench dataset, DaSLaM-augmented text-davinci-003 scores only 0.58 points short of GPT-4 while outperforming GPT-3.5.The decomposer LM in DaSLaM reduces decomposition errors, and generalizes well across diverse small-scale LMs.It is also more robust in the face of difficult datasets, where the solver gives near-random performance.</p>
<p>These results support our founding hypothesis that heterogeneous functional specialization improves model efficiency and robustness of LLMs.A crucial findings from our experiments is that finetuning the decomposer is much more powerful choice than finetuning the solver.Moreover, a finetuned decomposer is largely superior compared to an orders of magnitude larger LLM prompted to act as a decomposer.Given the prohibitive cost of finetuning LLMs like GPT 3, 3.5, or 4, we hope this method would provide us a promising direction to-wards future development of task-expert models.1</p>
<p>Related Work</p>
<p>Eliciting superior reasoning abilities in LM through specially designed prompts has found its popularity through CoT prompting (Wei et al., 2022) -asking the LM to explain the reasoning steps improves overall performance.Decomposing a complex reasoning requirement into multiple, simple steps results in superior reasoning capabilities across modalities other than free-form natural language text as well, e.g., reasoning over tabular data (Ye et al., 2023), visual question answering (Lu et al., 2022), etc.These methods generally solicit a single run of LM inference with no intermediate prompting interactions.Consequently, the LM often misses key reasoning steps or hallucinates irrelevant ones.</p>
<p>On the other hand, a prototypical prompter, sequentially interacting with the LM, has shown impressive performance.Progressive Hint Prompting (Zheng et al., 2023) uses such a setting; first, the LM is asked to provide a base answer.The prompt then uses the answer as a hint to the LM that progressively guides it to the final answer.Zhou et al. (2023) followed a similar direction by breaking down the problem itself.Their method, Least-tomost prompting, asks the LM to generate simpler, related problems from a complex problem.The final solution to the original question is generated by the LM conditioned upon the solution of the subproblems.A major bottleneck then becomes the solver's ability to identify the critical subproblems.Decomposing a complex task and then solving each task via multiple LLMs with their own in-context examples have been attempted as well (Dua et al., 2022;Khot et al., 2023).Recently, Shridhar et al. (2022) explored subquestion generation from complex questions as means of distilling reasoning abilities from larger LMs to smaller ones.</p>
<p>Our proposed method, DaSLaM makes a departure from these mentioned approaches in three particular features: (i) we seek to separate out the decomposer from the solver to get rid of the solver's limitations affecting decomposition, (ii) the decomposer acts as a plug-and-play module that can generalize to any solver, and (iii) the decomposition actuates with complete knowledge of the solver's actions.</p>
<p>When answering Q requires multistep reasoning, one can conceptualize S as a sequence of smaller steps
{S ′ 1 , S ′ 2 , • • • , S ′ n } such that the LM iteratively answers a sequence of subproblems {Q ′ 1 , • • • , Q ′ n }
to finally reach the desired answer to the original question.Eq. 1 can then be rewritten as,
Â = arg max A ′ n ∈A i p θ (A ′ i |S ′ i , Q ′ i ) arg max S ′ i p θ (S ′ i |Q ′ i ) (2)
where A ′ i is the answer to the subproblem Q ′ i .In the usual regime of CoT, the subproblems Q ′ i are implicit; the LM discovers them on its own and generates the reasoning steps and the answers.Due to the repeated multiplication in Eq. 2, any error in the initial stages quickly propagates along the chain of steps.</p>
<p>In DaSLaM, we seek to alleviate this problem by offloading the task of inferring the subproblems {Q ′ i } to a decomposer LM ϕ.For a more guided problem decomposition, DaSLaM uses the answer and the steps generated by the solver LM, θ in the naive CoT regime as described in Eq. 1 to generate a set of subproblems { Qi } i=1,...,n as follows:
Qi = arg max Q ′ i p ϕ (Q ′ i |{ Qj : j ∈ [1, i − 1]}, Q, Â0, S0),(3)
for i ∈ [1, n], where Â0 and S 0 are the initial answer and reasoning steps, respectively generated by θ.The solver LM θ then solves the subproblem set one-by-one similar to Eq. 2. However, instead of seeking to generate the final answer as a response to the last subproblem Qn , we append the original question at the end and let θ answer it directly given the context generated by the subproblems, their answers, and the corresponding CoTs.The four stages of workflow with DaSLaM, as described The first stage is straightforward with the decomposer LM being finetuned using language modeling objective.In the first stage, we seek to optimize the following objective:
min ϕ [− log(p ϕ (Q ′ gold |Q, S))]
(5)</p>
<p>This step is somewhat similar to instruction tuning, where an LM is asked to generate some text conditioned on a context, instead of following the usual sentence completion-style behavior of the LM.Intuitively, the role of this stage is to align the LM to the task of the decomposer.</p>
<p>Decomposition via policy network.The previous stage inculcates the ability to decompose a problem within ϕ.However, it is still blind to the actual errors made by the solver LM.In the next stage, we seek to make the decomposer LM work in synergy with any solver LM of choice.This solver agnosticism restrains ϕ to observe the internal representations computed by θ while solving the problem.To handle this imposed blackbox characteristics of the solver, we resort to policy gradient optimization of ϕ assuming θ to be part of the environment.We formalize the setup as follows.</p>
<p>Elements of the environment: The solver LM θ constitutes the core component of the environment.Given a question Q, the solver-generated CoT S and the answer A as sequences of tokens define the observation of the policy.</p>
<p>State and action space:</p>
<p>We define the state space as S. The initial state, s 0 ∈ S, is defined by the original question Q and the initial response from the solver, S 0 , A 0 , that are provided to the decomposer LM ϕ as input.A single timestep is defined on generation of a single token, i.e., the action a t at time t.Given the autoregressive nature of LM, we define the state at t-th timestep as s t = (s t−1 , {a t−1 }), i.e., the token generated at t − 1 appended to the tokens generated till t − 1. Trivially, the action space is the same as V, the vocabulary of ϕ.</p>
<p>Policy:</p>
<p>The decomposer LM is conceptualized as a policy network π ϕ : S − → V, i.e., it generates a token given the inputs and the token generated hitherto, till the end of episode T .Inspired by the recent success in Reinforcement Learning from Human Feedback (RLHF) with autoregressive LMs, we choose the Proximal Policy Optimization (PPO) algorithm to train the policy π ϕ (a|s).In a typical PPO setup, we define the advantage function as follows:
Ĝt = T −t+1 i=0 (γλ) i [rt+i + γV (st+i+1) − V (st+i)] (6)
where r t is the reward at step t, V (s t ) : S − → R is the value function determining the reward associated to state s t , and γ, λ are hyperparameters.We use the policy model augmented with a randomly initialized feedforward layer as the value function.A crucial component in on-policy learning regime is the reward function r t .While the end goal of the learning is to have the solver answering correctly, the decomposer LM should receive some incremental signal aligned to its generation as well for it to stably converge to the optimal policy.With this goal in mind, we construct the reward function as,
r t = R 1 + R 2 + R 3 + R 4 + R 5
(7) where R 1 to R 5 are defined as follows (see Appendix B for a detailed treatment on the reward computation method).Here cos-sim represents cosine similarity.
I(x) = 1 if x is true is the indicator function. Entity coverage: R 1 = |E Q ′ | |E Q |
, where E Q ′ and E Q are the sets of distinct entities in the generated subproblems and the original question, respectively.Consistency of answers to subproblems:
R2 = i I(ei = êi) + cos-sim(Q ′ i , Ai) (8)
where êi is the entity whose value has been asked in the subproblem Q ′ i , and e i is the entity answered.This reward penalizes the decomposer LM for generating questions whose answers are not consistent.Order of operations: R 3 = l m , where l is the number of operations matched in order between S and S gold , and m is the total number of operations in S gold .</p>
<p>CoT proximity: To ensure that the distance of reasoning produced by the model after prompting S to the gold reasoning S gold is less than the distance of reasoning produced without prompt S 0 to the gold reasoning steps S gold , we design a reward based on the cosine similarity of each step of S gold .We break S and S 0 at new-line token to form reasoning steps.At each step j, we compute c 1j = cos-sim(S j , S j gold ) and c 2j = cos-sim(S j 0 , S j gold ).The reward is
R4 = m j=0 I(c1j &gt; c2j)c1j + I(c2j &gt; c1j)(−1 − c2j),(9)
Correctness of final answer: R 5 =I( Â=A gold ).Now, we can define the PPO objective as follows:
max ϕ Et π ϕ (at|st) π ref (at|st) Ĝt − βEt <a href="10">Kt</a>
where π ref is the reference model that is initialized with supervised finetuned ϕ.
K t = KL[π ref (•|s t ), π ϕ (•|s t )]
is the KL-divergence between the reference model and the policy model.The resulting decomposer LM ϕ optimized using the above mentioned three stages of finetuning can then be used with DaSLaM.</p>
<p>Experiments</p>
<p>Training data curation.The training process of DaSLaM consists of two stages as mentioned previ-ously.In the first stage, we require the subproblems along with the reasoning steps for a given problem.We use samples from four existing datasets -MATH (Hendrycks et al., 2021), AQuA (Ling et al., 2017), GSM8K (Cobbe et al., 2021), and Strate-gyQA (Geva et al., 2021).Each question in these four datasets contains a question Q gold , a step-bystep illustration of the reasoning process S gold , and the final answer A gold .We sample 7, 000 examples from the training splits of these datasets and employ OpenAI's text-davinci-003 model to generate the corresponding subquestions.We provide the model with one-shot example illustrating how to decompose a question into subquestions based on the reasoning.In the second stage of training, we utilize the remaining training data from MATH and AQuA datasets to conduct the policy optimization since this step does not require any supervised examples of subproblems.</p>
<p>LMs used.We use LLaMA 13 billion (Touvron et al., 2023) as the decomposer LM.For the solver LM, we primarily use text-davinci-003 (henceforth, we denote it as GPT-3.5 for brevity).We also experiment with the LLaMA 13 bilion and LLaMA 33 billion models as solvers to test the model-agnostic generalizability of DaSLaM.</p>
<p>Baselines.We compare DaSLaM with four existing methods of prompting: Chain-of-thought prompting (CoT) (Wei et al., 2022), Least-to-most prompting (L2M) (Zhou et al., 2023), Progressive Hint Prompting (PHP) (Zheng et al., 2023), and, Demonstrate-Search-Predict (DSP) (Khattab et al., 2022a).The original setting of PHP requires an 8-shot prompting; however, since all other methods including DaSLaM predict in the zero-shot setting, we use PHP in 1-shot for a fairer comparison.Additionally, we experiment with three ablation variants: DaSLaM-NF does not take the solver feedback into account while generating the subproblems; Finetuned is the solver LM (LLaMA 13B in this case, we could not finetune 33B variant due to computational constraints) finetuned without any decomposer; GPT-3.5 decomposer does away with the finetuned LLaMA 13B decomposer and uses pretrained GPT-3.5 as the prompted decomposer.</p>
<p>Test datasets.For evaluation purposes, we use three datasets -MATH (Hendrycks et al., 2021), AQuA (Ling et al., 2017), and JEEBench (Arora et al., 2023).For the first two datasets, only the test splits are used during evaluation since their</p>
<p>Experimental Results</p>
<p>The tasks used to evaluate the performance of DaSLaM contain questions that can be answered either of the three types -numerical, single correct answer MCQ, and multiple correct answer MCQ.DaSLaM is better than pure prompting We start with DaSLaM augmented with GPT-3.5 as the solver LM on MATH and AQuA datasets (see Table 1).The improvement achieved with DaSLaM prompting compared to standard CoT is staggering across all types of problems in the MATH dataset: +11.7 on Pre-Algebra, +8.4 on Intermediate Algebra, +7.7 on Number Theory, +7.2 on Geometry, +5.0 on Probability and Combinatorics, +5.8 on Algebra, and +4.2 on Calculus.The absolute improvement is even larger on the AQuA dataset, i.e., +12.9 over CoT.It is noticeable that the effects of DaSLaM are stronger across tasks containing algebraic reasoning (AQuA, Pre-and Intermediate-Algebra, etc.) compared to Probability and Combinatorics or Calculus, which require more implicit knowledge.The performance gain achieved via DaSLaM is significantly better compared to methods like L2M or PHP.The latter methods often fail to improve over standard CoT (e.g., on Probability and combinatorics, Number Theory, and Algebra problems, L2M shows a drop in accuracy).Even when improving over CoT, their improvement is meager compared to DaSLaM.This trend entails our earlier argument in support of offloading the problem decomposition task to a specialized LM; methods that prompt the solver LM to decompose the problem lack the expertise achieved via dedicated finetuning in DaSLaM.</p>
<p>Finetuned decomposer is essential.Despite being orders of magnitude smaller, a finetuned LLaMA 13B model delivers better performance compared to GPT-3.5 as a decomposer (DaSLaM vs. GPT-3.5generator in Table 1 and 2).This further justifies our choice of separately finetuning the decomposer and the added flexibility that it offers.In fact, finetuning the decomposer is far effective compared to finetuning the solver (DaSLaM vs Finetuned solver in Table 2).</p>
<p>Feedback from the solver is important.In the preceding paragraph, we attributed the superiority of DaSLaM over other methods to the usage of a specialized LM for problem decomposition.However, manipulating the problem decomposition upon feedback from the solver is also an important factor here.None of the existing methods does so, and therefore, remains blind towards what reasoning (and possible errors) is followed by the solver model.This is further manifested when we compare DaSLaM with itself without the feedback module, DaSLaM-NF.While DaSLaM-NF is able to improve upon basic CoT and other prompting methods, it falls short of a decomposer LM that has access to the initial response of the solver.</p>
<p>DaSLaM generalizes to smaller solvers.An important aspect of a prompting method is its ability to work with LMs of different scales.Despite being finetuned with GPT-3.5 responses only, DaSLaM is able to improve upon the base performance of smaller scale LLaMA models as well (see Table 2).L2M prompting generally fails with both LLaMA 13 billion and 33 billion variants.On the other hand, DaSLaM, with or without feedback, almost doubles the performance of the base CoT across multiple tasks of the MATH dataset.It shows substantial improvement on AQuA as well.The importance of feedback from the solver LM usually manifests strongly in proportion to the scale of the solver.</p>
<p>DaSLaM generalizes to harder problems.Since the decomposer LM ϕ is trained using a subset of the training data of MATH and AQuA, we opt for a harder (in terms of benchmark performance of different LMs) reasoning evaluation on the JEEBench dataset.Table 3 summarizes the performance of the baselines and DaSLaM with GPT-3.5 as the solver LM on Mathematics and Physics questions of the JEEBench dataset.We notice that the superiority of DaSLaM manifests even more profoundly on this task compared to the former ones.Both PHP and L2M prompting absolutely fail to improve upon basic CoT prompting, often with a sharp fall in performance (e.g., Physics MCQ questions).On the other hand, DaSLaM boosts the LMs performance, very often over 100% relative improvement (all three types of problems in Physics and numerical problems in Mathematics).Aggregated across question types, DaSLaM boosts the performance of GPT-3.5 to 22.420 in Physics and 22.07 in Mathematics.It is noteworthy that the same LM in its base setting performs near-random, i.e., 10.4 and 10.7 in Physics and Mathematics, respectively, whereas a random selection baseline gives scores of 9.6 and 10.3, respectively (Arora et al., 2023).Furthermore, GPT-3.5 with DaSLaM outperforms a better optimized candidate of the GPT series, GPT-3.5 on both these subjects (note that Arora et al. (2023) reported 18.9 and 15.7 scores with GPT-3.5 on Physics and Mathematics, respectively).Comparison with GPT-4.The colossal compute used by GPT-4 makes the comparison with any of its predecessors like GPT-3.5 quite unfair.However, it is tempting to observe that DaSLaM boosts the performance of GPT-3.5 often to the level of GPT-4.For example, on arithmetic problems of the AQuA dataset, DaSLaM surprisingly outperforms both zero-shot and few-shot  respectively, compared to 54.5 with GPT-3.5 and DaSLaM).On MATH dataset, DaSLaM augmented GPT-3.5 scores an aggregate of 30.23, which is better than ChatGPT (26.4) and close to .On JEEBench Mathematics problems, GPT-Figure 2: An example case study on a problem from the MATH dataset.GPT-3.5 is used as the solver LM with three different methods of prompting -standard CoT, Least-to-most, and DaSLaM.Only DaSLaM is able to guide the model to the correct answer.</p>
<p>4 comes up with an aggregate score of 23.1, which is pretty close to our 22.42.In Physics and Math MCQ questions, DaSLaM with GPT-3.5 outperforms GPT-4.These results definitely do not claim any assumed superiority of DaSLaM-boosted GPT-3.5 over GPT-4 since there are multiple other cases that state otherwise.Instead, we seek to demonstrate how much leftover potential these LMs possess that can be unleashed via our proposed method of feedback-guided automatic problem decomposition.</p>
<p>Case Study</p>
<p>To this point, we have compared the numbers produced by DaSLaM-boosted models across different datasets.While they provide an overall assessment, deeper analyses are needed to comprehend the actual reasoning steps adopted by these different methods.Figure 2 shows the reasoning steps generated by GPT-3.5 given an example problem from the MATH dataset with three different prompting methods -vanilla CoT, L2M, and DaSLaM.Note that DaSLaM uses the CoT output to decompose the problem.</p>
<p>Both CoT and L2M end up with the model an-swering incorrectly.With CoT, the solver wrongly assumes that the given equation must have two real roots though it should not have any real roots.Also, it mistakes the value of a 2 as a.The effect is prominent in the subproblems generated by DaSLaM as it asks to find the value of a explicitly.Furthermore, the solver LM specifically announces that y ≤ 0 to answer the first subproblem generated by DaSLaM.This helps to correct the reasoning about the sign of the discriminant.</p>
<p>With L2M, the confusion around the value of a and a 2 persists, as the solver LM substitutes a in the given equation by 49 (which is the value of a 2 ) twice in the answering process.Although it substituted the correct value of a once while answering the second question, it is not explicitly declared like in DaSLaM.We observe multiple similar failure cases with L2M.It is quite likely that prompting the model to generate the final answer after each subproblem accumulates the erroneous reasoning steps that the model falls prey to.</p>
<p>With DaSLaM, the reasoning steps followed by the solver remains robust throughout.It reaches the final answer much earlier (third and fourth subproblems).In the final answer, the solver simply reiterates the steps that it earlier generated to answer the subproblems.This is a common behavior that we observed across multiple problems from multiple datasets.In Appendix D (see Figures 3  and 4), we provide similar case studies on LLaMA 13B and 33B with different prompting methods.With reduced solver capacity, the difference between Least-to-most and CoT generated reasoning steps further diminishes with both leading to incorrect answers; DaSLaM, on the other hand, still guides the solver through correct steps.</p>
<p>An interesting observation can be made by comparing how the solver behaves with CoT vs. with DaSLaM.With DaSLaM, we do not provide any new knowledge to the solver.Yet, the same model can rectify its errors made in CoT response.This may point to the intuition that current LLMs are actually underutilized, and one can unfold even more impressive performance with cleverly composed guidance.</p>
<p>We further provide case analyses with when DaSLaM fails to guide the model (GPT 3.5 in this case) to successful final answers, in Appendix E. While we do not find any obvious pattern of errors, one can see that the decomposer generates questions that are not readily answerable within that context.DaSLaM does not use any method to trace back the error or generate subproblems based on the answers to the previous subproblems.This might raise such issues where the subproblems generated are not actually helping the solver in the right order.</p>
<p>Conclusion</p>
<p>We challenged the design of ever-larger monolithic LLMs as homogeneous network structures, where diverse aspects of problem decomposition and solution are stored in a tangled and opaque manner.The formidable general-purpose problem-solving capabilities of LLMs are exceedingly resource-hungry, dependent on immense data engineering.Inspired by brain science, we took a first step toward heterogeneity -let two different LLMs evolve independently and adapt to their roles of decomposing and solving complex reasoning problems.Through extensive experiments on several benchmarks, we showed that such a heterogeneous network can match or exceed some of the largest contemporary LLMs, at a much smaller parameter count.</p>
<p>Limitations</p>
<p>A potential limitation of DaSLaM, as with many system that uses an LLM-as-a-service API charging per token exchange, is the increased token usage because of the RL exploration.Asserting a token budget on the decomposer LM is left as an avenue for future exploration.Ideally, the decomposer LM should seamlessly invoke solvers of many forms, such as retrievers (Khattab et al., 2022b) or mathematical calculators (Schick et al., 2023;Wolfram, 2023).Future work may extend DaSLaM to such tools.DaSLaM is limited to purely text-based subproblem decomposition; it is not possible at present to incorporate reasoning through other modalities (e.g., visual inputs for geometric reasoning) into DaSLaM in its current form.</p>
<p>Small Language Models Fine-tuned to Coordinate Larger Language Models improve Complex Reasoning (Appendix)</p>
<p>A Supervised Fine-tuning Dataset</p>
<p>The training data for supervised fine-tuning stage was generated using text-davinci-003.Since Jake owns 3 of the boots, the subset from which the 4 boots should be chosen are the 12 boots not owned by Jake from the universe of 15.The first boot can be one of the 12 from the 15 with probability 12 15 .The second boot can be one of the 11 from the 14 remaining with probability 11 14 .The third boot can be one of the 10 from the 13 remaining with probability 10</p>
<p>13 .The fourth boot can be one of the 9 from the 12 remaining with probability 9 12 .The total probability will be 12 15 • 11 14 • 10 13 • 9 12 .On cancellation, this comes to 33 91 Q ′ gold 1.How many boots did Jake own? 2. How many boots were on the field?3. How many boots did Peter take? 4. What is the probability of choosing one of the 12 boots not owned by Jake from the universe of 15? 5. What is the probability of choosing the second, third and fourth boots not owned by Jake? 6.What is the total probability?</p>
<p>B Reward Calculation</p>
<p>During policy gradient optimization of the decomposer LM ϕ, rewards were provided in an incremental fashion so that the generated sub-questions align well with the initial question, the answer generated by the solver LM θ and the sub-question solving capabilities of θ.For this, the reward was designed as a combination of five terms.The definitions of these terms are given in Section 4.Here we provide an example of the reward calculation.</p>
<p>Entity coverage Reward (R 1 ).Here, we find distinct nouns and numbers in the question and the sub-questions using the nltk library.The reward is calculated using
R 1 = |E Q ′ | |E Q |
, where E Q ′ and E Q are the sets of distinct nouns and numbers in the sub-questions and questions, respectively.An example of the same is shown below.</p>
<p>Qgold:</p>
<p>Each good worker can paint my new house alone in 12 hours.Each bad worker can paint my house alone in 36 hours.I need my house painted in 3 hours.If I can only find 3 good workers, how many bad workers must I also find in order to have my house painted on time?
Q ′ gold :
1. How many good workers are needed to paint the house in 3 hours?2. How many bad workers are needed to paint the house in 3 hours?3. What is the total number of workers needed to paint the house in 3 hours?
|E Q ′ | = 4 + 1 = 5 (11) |E Q ′ | = 6 + 3 = 9 (12) R1 = |E Q ′ | |EQ| = 5 9 = 0.55(13)
Consistency of answers to subproblems reward (R 2 ).To ensure that the sub-question is such that the solver LLM ϕ does not produce random answers, we compare the entity in the solution to the entity asked in the subquestion along with the cosine similarity of the embeddings of sub-question and sub-answers.To find the embeddings, we use the pre-trained model in the spacy library.R 2 = i (I(e i = êi ) + cosine-similarity(Q ′ i , A i )), where I(x) = 1 if x is true, êi is entity whose value has been asked in the subproblem Q ′ i , and e i is the entity answered.An example for the reward calculation is given below.</p>
<p>Q gold : Three friends Alan, Roger and Peter attempt to answer a question on an exam.Alan randomly guesses the answer, giving him a 1 5 probability of guessing correctly.Roger cheats by looking at the paper of the student in front of him, giving him a 2 3 probability of answering correctly.And Peter dutifully performs the calculations, then marks the answer, giving him a 5 6 probability of a correct answer.What is the probability that the question is answered correctly, but not via cheating?
Sgold : Prob(Alan) = 1 5 Prob(Roger) without cheating = 2 3 − 1 = 1 3 Prob(Peter) = 5 6 Total Probability = 1 5 • 1 3 • 5 6 = 118
Smodel : Alan has a 1 5 chance of getting the answer correct.Roger has a 2 3 chance of getting the answer correct.Peter has a 5 6 chance of getting the answer correct.The probability that the question is answered correctly is 1 5 + 2 3 + 5 6 = 13 12 .The probability that the question is answered correctly, but
not via cheating is1 − ( 1 5 + 2 3 + 5 6 ) = 1 − 13 12 = −112
Agold:
1 18 Amodel: −1 12 R5 = 0</p>
<p>C Hyperparameter Selection</p>
<p>We performed hyperparameter tuning on a subset of 250 examples from the training dataset.Based on the improvement in accuracy and compute resources, we converged at the following values.</p>
<p>For the supervised fine-tuning stage, we used LoRA r = 16, LoRA α = 32, LoRA droput = 0.05.For the RLMF stage, we finetuned the last 3 layers of LoRA adapters, using a batch size of 8, gradient accumulation steps=8, init kl coef=0.01,target=4.For inference, we used the following generation parameters: temperature = 0.95, top p=0.18,pad token id = 0, do sample = False, number of beams = 1, maxi length = 2048.</p>
<p>All the models were implemented using Huggingface with PyTorch, and loaded in int_8 precision.For solver-LLaMA 13B model, we instruction finetuned the LLaMA13B model using the Alpaca dataset by stanford (Taori et al., 2023).For solver-LLaMA 33 billion model, we used an instruction-finetuned version of LLAMA-33 billion from Huggingface.To implement the PPO algorithm, we used the TRL library.We used a single NVIDIA A100 (80GB) GPU for all training and inference purposes.</p>
<p>D Case Study on LLAMA Models</p>
<p>Similar to Figure 2, we performed example-wise case study on the answering process of the LLaMA solvers (13 billion and 33 billion models) with different prompting methods (see Figures 3 and 4).The example problem was selected from the MATH dataset.As we can see, with relatively small scale solvers like LLaMA, Least-to-most fails to add any improvement over basic CoT prompting.DaSLaM is able to guide the model to the correct answer in both cases.</p>
<p>E Failure cases with DaSLaM</p>
<p>E. 1
1
Case I Question A train running at a speed of 100 miles/hour, takes 10 hours to reach its destination.After covering quarter of the distance, it starts raining and the train has to be slowed to speed of 75 miles/hour.What is the total journey duration?A)10 B)11.5 C)12.5 D)13.5 E)15Initial Answer The train covers the first quarter of the distance at a speed of 100 miles/hour.So, the time taken to cover the first quarter of the distance = (1/4) x 10 hours = 2.5 hours The train covers the remaining three quarters of the distance at a speed of 75 miles/hour.So, the time taken to cover the remaining three quarters of the distance = (3/4) x 10 hours = 7.5 hours Therefore, the total journey duration = 2.5 + 7.5 = 10 hours Sub problem 1 What is the total distance to be covered?SubAnswer 1 Let the total distance to be covered be d.Time taken to cover the first quarter of the distance = (1/4)d/100 = 10/400 hours Time taken to cover the remaining distance = (3/4)d/75 = 300/400 hours Total time taken = 10/400 + 300/400 = 10/4 hours = 2.5 hours Since the train takes 10 hours to reach its destination, Total distance to be covered = 10 x 100 = 1000 miles Sub problem 2 What is the speed of the train?Sub Answer 2 After covering quarter of the distance, the train has to be slowed to speed of 75 miles/hour.Let the remaining distance be x.Therefore, x = d -(d/4) = 1000 -(1000/4) = 750 miles.The time taken to cover the remaining distance x = 750 miles at a speed of 75 miles/hour = (750/75) hours = 10 hours.Therefore, the speed of the train = x/time taken = 750/10 = 75 miles/hour.Sub problem 3 What is the time taken to cover the distance?Sub Answer 3 Time taken to cover the first quarter of the distance = (d/4)/100 hours = d/400 hours Time taken to cover the remaining distance = (3d/4)/75 hours = d/300 hours Total time taken = d/400 + d/300 = (5d + 4d)/1200 hours = 9d/1200 hours.Putting d=1000, Final answer: The time taken to cover the distance is 9 x 1000/1200 hours.Sub problem 4 What is the total duration of the journey?Sub Answer 4 Time taken to cover the first quarter of the distance = (d/4)/100 hours = d/400 hours Time taken to cover the remaining three quarters of the distance = (3d/4)/75 hours = 4d/300 hours Total time taken = d/400 + 4d/300 = (5d + 4d)/(400 + 300) = 9d/700 hours Total duration of the journey = 9d/700 hours = 9 hours and (d/700) hours = 9 hours and (100/700) hours = 9 hours and 14.29 minutes Final Answer Step 1: The train covers the first quarter of the journey at a speed of 100 miles/hour.Step 2: The time taken to cover the first quarter of the journey is given by: Time = Distance/Speed Time = (1/4) x 100/100 Time = 1/4 hour Step 3: The train covers the remaining three quarters of the journey at a speed of 75 miles/hour.Step 4: The time taken to cover the remaining three quarters of the journey is given by: Time = Distance/Speed Time = (3/4) x 75/100 Time = 3/4 hour Step 5: The total duration of the journey is the sum of the time taken to cover the first quarter and the remaining three quarters of the journey.Total Time = 1/4 + 3/4 Total Time = 4/4 Total Time = 1 hour Therefore, the total duration of the journey is 1 hour.</p>
<p>Table 1 :
1
Performance comparison on MATH and AQuA datasets using GPT-3.5 as the solver LM.See Section 5 for abbreviations.trainingsplits are used while finetuning the decomposer.The MATH dataset contains mathematical problems on multiple different domains.We report the results on each of them separately and use the following abbreviations -ALG, I-ALG, and P-ALG for Algebra, Intermediate Algebra, and Pre-
DatasetMethod CoT L2M PHP DSP GPT3.5 Decomposer DaSLaM-NF DaSLaMPnC16.4 16.010.2 16.216.020.021.4NT14.4 11.09.820.314.218.426.1ALG27.6 22.424.0 15.332.131.633.4I-ALG 16.4 16.810.0 17.018.420.824.8Calc.14.0 14.58 14.28 18.812.015.118.2P-ALG 32.3 28.026.5 28.035.538.044.0Geom. 14.2 12.514.05.222.019.0421.4AQuA 41.6 44.744.4 44.045.453.254.5
Algebra, respectively; Calc for Calculus, Geom for Geometry, PnC for Probability and Combinatorics, NT for Number theory.From the JEEBench dataset, we use the problems in Physics (Phy) and Mathematics (Math).Each of these two subjects has three types of problems -single-answer multiple-choice questions (MCQ), numerical problems (Num), and multi-answer multiple-choice questions (Multi).For all these datasets, we use exact match criteria to evaluate the correctness of the model-inferred answers.Details of training and inference hyperparameters and compute resource usage are provided in Appendix C.</p>
<p>Table 3 :
3
Performance comparison on the JEE benchmark dataset with GPT-3.5 as the solver LM. 0<em> signifies that the model was not able to answer any problem in the task correctly.
Method CoT L2M Finetuned GPT3.5 Decomposer 2.05 PnC 2.05 1.66 2.8 DaSLaM-NF 2.93 DaSLaM 4.0 CoT 2.4 L2M 2.38 4.16 NT LLaMA 13 billion Dataset ALG iALG Geom 4.0 3.12 2.4 3.2 3.2 3.33 2.8 2.0 3.6 3.57 3.2 4.1 5.0 4.68 2.8 2.08 4.8 4.68 3.2 4.0 5.6 4.70 3.4 4.3 LLaMA 33 billion 4.16 4.54 3.7 4.0 4.2 6.0 4.25 DaSLaM-NF 3.2 5.83 5.6 5.6 5.1 DaSLaM 4.0 7.36 9.09 6.02 5.3Cal 2.08 3.33 4.54 Palg AQuA 5.0 17.7 16.6 3.05 6.04 19.4 4.0 6.66 20.4 3.9 6.2 21.6 4.1 8.33 22.0 4.0 5.2 20.0 5.71 5.55 21.6 5.71 5.2 22.5 6.03 8.44 26.8Table 2: Performance on MATH and AQuA with LLaMA 13 billion and LLaMA 33 billion as solvers. PHP is not reported as one-shot PHP generated randomly with both LLaMA variants. DaSLaM provides consistent improvement across all the tasks while other baseline methods mostly fail.Method CoT PHP L2M DaSLaM-NF DaSLaM GPT-4Dataset Phy MCQ Math MCQ Phy Multi Math Multi Phy Num Math Num Phy Int Math Int 33.33 21.9 6.25 12.0 3.03 1.69 12.5 20.0 22.22 17.07 6.25 7.59 3.03 1.69 0</em> 4.0 22.22 21.9 6.25 12.5 3.03 3.38 10.0 20.0 20.8 31.7 7.5 10.12 3.03 3.38 12.5 16.0 55.55 36.5 18.75 16.0 6.06 10.16 22.5 24.0 55.55 34.14 27.5 21.5 15.15 11.8 25.0 20.0</p>
<p>Each data point in the dataset consisted of a tuple ⟨Q gold , S gold , Q ′ gold ⟩, where Q gold represents the reasoning question, S gold represents the gold reasoning steps, and Q ′ gold represent the sub-questions generated by text-davinci-003 in a one-shot setting.An example of a single data point is given below.
Q goldJohn borrowed 3 soccer boots from Jake, and forgot them on the field, if peter came across a total of 15 boots on the field Jake's boots inclusive, and he took 4 boots at random, what is the probability that Jake's boots were not amongst the 4 taken?. (A) 12 91 , (B) 3 15 , (C) 12 15 (D) 33 91 , (E) 3 91S gold
The codebase is given at: https://github.com/ LCS2-IIITD/DaSLaM
AcknowledgmentsThe authors acknowledge the financial support of DYSL-AI.Q ′1 : How many good workers are needed to paint the house in 3 hours?A1: We can calculate the amount of work done in 3 hours by a good worker by dividing 12 hours by 3 hours.We get 4 as the answer.We know that a bad worker can paint the house in 36 hours.We can calculate the amount of work done in 3 hours by a bad worker by dividing 36 hours by 3 hours.We get 12 as the answer.We need to find out how many good workers are needed to paint the house in 3 hours.We can calculate the number of good workers needed by dividing 12 by 4. Answer: 3 good workers can paint the house in 3 hours.cos-sim(Q ′ i , Ai) = 0.70457435 I(ei = êi) = 1 R2 = 1 + 0.70457435 = 1.70457435Order of operations reward (R 3 ).To evaluate the correctness of the mathematical solution produced by the model, we compare the order of operations.R 3 = l m , where l is the number of operations matched in order between S and S gold , and m is the total number of operations in S gold .Below is an example of R 3 calculation.S gold1. : We first consider the possible neighbors of the chief's mother, one must be the chief, the other is one of the remaining 5 natives.That native can, in turn, have one of 4 neighbors in addition to the chief's mother 2. Continuing around, there are 5! = 120 possible arrangements 3. The chief can be on his mother's left or right, for a total of 2 • 120 = 240 arrangements 4. Alternatively, we know that the seating arrangement is the same if it is rotated, once the chief's spot is chosen, the rotations are eliminated 5.There are 2 ways to seat his wife and his mother, and then there are 5 distinct positions for the remaining 5 natives, which makes for 2 • 5! = 240 possible arrangements S0:1. We can see that the chief must be seated between his mother and his wife.2. This means that there are 3 possible positions for the chief, and 5 possible positions for the other 6 people.3. Therefore, there are 3! • 5! = 1800 different seating arrangements.S:1.The chief must sit between his mother and his wife.2. There are 8 people in the tribe.3. So, the chief's mother and wife can be arranged in 2! ways.4. The remaining 5 people can be arranged in 5! ways. 5. Therefore, the total number of possible seating arrangements after removing rotation = 5! = 240 Cosine similarity calculation per step:1. c1j = 0.47673503, c2j = 0.44773823 2. c1j = 0.45063934, c2j = 0.47917843 3. c1j = 0.5173945, c2j = 0.20383504 4. c1j = 0.46866685, c2j = 0 5. c1j = 0.47825924, c2j = 0 Hence, R4 = 0.47673503 + (−1 − 0.47917843) + 0.5173945 + 0.46866685 + 0.47825924 = 0.46187719Correctness of final answer(R 5 ), R 5 = I( Â = A gold ).This reward checks if the final answer matches the gold answer.A negative example of the same is given below.As n ranges over the positive integers, what is the maximum possible value that the greatest common divisor of 80n + 49 and 18n + 11 can take?What is the greatest common divisor of 8n + 5 and 2n + 1?The greatest common divisor of 8n + 5 and 2n + 1 is the greatest common divisor of 8n + 5 and 8n + 5 − 2n − 1 = 6n + 4.The greatest common divisor of 8n + 5 and 2n + 1 is the greatest common divisor of 8n + 5 and 6n + 4The greatest common divisor of 8n + 5 and 2n + 1 is the greatest common divisor of 8n + 5 and 4n + 3The greatest common divisor of 8n + 5 and 2n + 1 is the greatest common divisor of 8n + 5 and 2n + 2 − 2n − 1 = 1.Hence, The greatest common divisor of 8n + 5 and 2n + 1 is 1 What is the greatest common divisor of 18n + 11 and 8n + 5?The greatest common divisor of 18n + 11 and 8n + 5 is the greatest common divisor of 8n + 5 and 18n + 11 − 8n − 5 = 10n + 6 = greatest common divisor of 8n + 5 and 10n + 6DaSLaMLeast to Most COT Figure3: A case study on LLAMA-13B.The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.What is the largest of the three consecutive integers?What is the product of three consecutive integers?The product of three consecutive integers is 120.What is the mean of the three integers?The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.So, 120/mean = 24 Mean = 120/24 = 5 What is the largest of the three consecutive integers?The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.So, 120/mean = 24 Mean = 120/24 = 5 So, the three consecutive integers are 4, 5, 6.The largest of the three consecutive integers is 6The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.The mean of the three integers is the sum of the three integers divided by 3. The sum of the three integers is 120.Therefore, the mean of the three integers is 40.The product of the three integers is 120, so the product of the three integers divided by the mean of the three integers is 3.This means that the largest of the three consecutive integers is 40.The product of three consecutive integers is 120.That product divided by the mean of the three integers is 24.The mean of the three integers is the sum of the three integers divided by 3. The sum of the three integers is 120.Therefore, the mean of the three integers is 40.The largest of the three consecutive integers is 40 + 24 = 64.DaSLaMComparing the coefficients of x on both sides, m = 2 sec θ Comparing the constant terms on both sides, c = 1 Therefore, the equation of the line x 2 − 2x sec θ + 1 = 0 is given by y = 2 sec θx + 1 Hence, the answer is y = 2 sec θx + 1.What is the equation of the line x 2 + 2x tan θ − 1 = 0? Given, − π We need to find the value of α 1 + β 2 Let us consider the equation x 2 − 2x sec θ + 1 = 0 α 1 and β 1 are the roots of the equation.Therefore, α 1 + β 1 = −2 sec θ Let us consider the equation x 2 + 2x tan θ − 1 = 0 α 2 and β 2 are the roots of the equation.Therefore,Hence, the answer is (A) 2(sec θ − tan θ).Hence, the answer is (A) 2(sec θ − tan θ).
Have llms advanced enough? a challenging problem solving benchmark for large language models. Daman Arora, Himanshu Gaurav Singh, Mausam , 2023</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>The carbon impact of artificial intelligence. Payal Dhar, Nature Machine Intelligence. 282020</p>
<p>Successive Prompting for Decomposing Complex Questions. Dheeru Dua, Shivanshu Gupta, Sameer Singh, Matt Gardner, Empirical Methods in Natural Language Processing (EMNLP). 2022</p>
<p>Language and thought are not the same thing: evidence from neuroimaging and neurological patients. Evelina Fedorenko, Rosemary A Varley, 2016Annals of the New York Academy of Sciences1369</p>
<p>Towards revealing the mystery behind chain of thought: a theoretical perspective. Guhao Feng, Bohang Zhang, Yuntian Gu, Haotian Ye, Di He, Liwei Wang, 2023</p>
<p>Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 92021</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, 2021NeurIPS</p>
<p>LoRA: Low-rank adaptation of large language models. J Edward, Phillip Hu, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, International Conference on Learning Representations. 2022</p>
<p>Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive nlp. Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.140242022aarXiv preprint</p>
<p>Omar Khattab, Keshav Santhanam, Lisa Xiang, David Li, Percy Hall, Christopher Liang, Matei Potts, Zaharia, arXiv:2212.14024Demonstrate-searchpredict: Composing retrieval and language models for knowledge-intensive NLP. 2022barXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Program induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, 10.18653/v1/P17-1015Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics20171</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 202235</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, Dissociating language and thought in large language models: a cognitive perspective. 2023</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Gpt-4 technical report. 2023. 202021OpenAI</p>
<p>Toolformer: Language models can teach themselves to use tools. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, Thomas Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>Distilling multi-step reasoning capabilities of large language models into smaller models via semantic decompositions. Kumar Shridhar, Alessandro Stolfo, Mrinmaya Sachan, arXiv:2212.001932022arXiv preprint</p>
<p>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori B Hashimoto, Stanford alpaca: An instruction-following llama model. 2023</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Faisal Hambro, Aurelien Azhar, Armand Rodriguez, Edouard Joulin, Guillaume Grave, Lample, Llama: Open and efficient foundation language models. 2023</p>
<p>Chain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed H Chi, Denny Quoc V Le, Zhou, Advances in Neural Information Processing Systems. 2022</p>
<p>Stephen Wolfram, ChatGPT gets its "Wolfram superpowers"! (blog). 2023</p>
<p>Large language models are versatile decomposers: Decompose evidence and questions for table-based reasoning. Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li, arXiv:2301.138082023arXiv preprint</p>
<p>Progressive-hint prompting improves reasoning in large language models. Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, Yu Li, 2023</p>
<p>Least-to-most prompting enables complex reasoning in large language models. Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Ed H Quoc V Le, Chi, The Eleventh International Conference on Learning Representations. 2023</p>            </div>
        </div>

    </div>
</body>
</html>
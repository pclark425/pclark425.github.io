<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5177 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5177</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5177</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-109.html">extraction-schema-109</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <p><strong>Paper ID:</strong> paper-258822829</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.11597v1.pdf" target="_blank">Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments</a></p>
                <p><strong>Paper Abstract:</strong> Trustworthiness of artificially intelligent agents is vital for the acceptance of human-machine teaming in industrial manufacturing environments. Predictable behaviours and explainable (and understandable) rationale allow humans collaborating with (and building) these agents to understand their motivations and therefore validate decisions that are made. To that aim, we make use of G\"ardenfors's cognitively inspired Conceptual Space framework to represent the agent's knowledge using concepts as convex regions in a space spanned by inherently comprehensible quality dimensions. A simple typicality quantification model is built on top of it to determine fuzzy category membership and classify instances interpretably. We apply it on a use case from the manufacturing domain, using objects' physical properties obtained from cobots' onboard sensors and utilisation properties from crowdsourced commonsense knowledge available at public knowledge bases. Such flexible knowledge representation based on property decomposition allows for data-efficient representation learning of typically highly specialist or specific manufacturing artefacts. In such a setting, traditional data-driven (e.g., computer vision-based) classification approaches would struggle due to training data scarcity. This allows for comprehensibility of an AI agent's acquired knowledge by the human collaborator thus contributing to trustworthiness. We situate our approach within an existing explainability framework specifying explanation desiderata. We provide arguments for our system's applicability and appropriateness for different roles of human agents collaborating with the AI system throughout its design, validation, and operation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5177.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5177.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Gärdenfors's Conceptual Spaces framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A cognitively motivated geometric framework that represents concepts as convex regions in a multidimensional space spanned by interpretable quality domains (e.g., colour, size, utilisation), allowing continuous instance vectors, similarity metrics and concept algebraic operations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Conceptual spaces: The geometry of thought</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Conceptual Spaces</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts are represented functionally as regions (often convex) within a metric, multidimensional space whose axes (quality dimensions/domains) correspond to cognitively meaningful properties; instances are points (vectors) in this space and categorization proceeds by proximity/typicality relative to concept regions or prototypes.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>prototype/region-based geometric (continuous, interpretable, feature-based)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Multidimensional continuous representation; interpretable quality domains; prototypes and convex concept regions; natural similarity metric; supports concept combination and algebra; bridges subsymbolic (sensory vectors) and symbolic (languageable concepts).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Behavioral cognitive-semantic phenomena captured include prototypicality and family-resemblance effects (Rosch et al.), schematicity, similarity-based categorization, and successful applications in modelling categorization and concept combination; the framework has been used in computational implementations and cited as compatible with observed human conceptual behaviour in many cognitive studies.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Requires selection and delimitation of appropriate quality domains for a given domain; real-world property correlations and interdependencies complicate simple orthogonal-space assumptions; scalability and formal guarantees for complex high-dimensional/structured concepts are debated; empirical mapping from sensory data to stable quality dimensions is nontrivial.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Categorization/classification, concept combination, similarity judgements, semantic modelling, explainable knowledge representation (this paper's manufacturing use case), and computational cognitive architectures.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Differs from symbolic models by retaining continuous structure and from exemplar/distributed accounts by positing interpretable dimensions and region/prototype representations; touted as a middle-ground (neuro-symbolic) that preserves interpretability unlike opaque distributed neural representations.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Form concepts by defining prototypes/regions in quality-domain space; compute instance typicality via distance/similarity to prototypes or by evaluating membership across weighted dimensions; combine concepts via geometric operations; allow explanations via explicit dimensions and prototype comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to choose and learn a domain-appropriate set of quality dimensions; how to handle correlated or non-orthogonal properties; how to scale to highly structured relational concepts and events; normative learning algorithms for regions and weights; how exactly this maps functionally onto cognitive processes of concept acquisition.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5177.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5177.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prototype theory (Roschian prototype view of categories)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A theory positing that categories are organized around central best examples (prototypes) rather than via necessary and sufficient features, producing graded membership and typicality effects in categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Family resemblances: Studies in the internal structure of categories</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Prototype theory</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is functionally organized around prototypical exemplars; category membership is graded and determined by similarity to prototypes rather than strict rule-based membership.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>prototype/central tendency (continuous, similarity-based)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Graded membership/typicality; family resemblance structure; emphasis on central tendency rather than necessary features; supports similarity-based generalization and flexible category boundaries.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Robust behavioral findings: typicality gradients, faster verification for prototypical items, graded category membership in adults and children (Rosch et al., Rosch & Mervis), used widely in experimental categorization studies.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Fails to account for certain rule-based category phenomena, relational or theory-driven inferences, some cases where exemplar or rule models better predict data; cannot by itself explain why some features are central and others peripheral (requires additional mechanisms such as causal/essentialist constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Basic-level categorization, concept learning from limited data, similarity judgements, explaining graded category membership in psychology and computational models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Contrasted with exemplar models (which represent many stored instances) and rule/theory-based models (which posit structured causal knowledge); prototype theory is simpler and explains graded typicality but is weaker for causal and relational inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Form prototypes (centroids) from experienced instances; compute similarity/typicality to prototypes to determine category membership and guide generalization; weights over dimensions determine the contribution of each property.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How prototypes are formed and updated with sparse data; how to integrate causal/functional knowledge that affects feature centrality; how to represent complex structured or relational concepts beyond similarity to prototypes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5177.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5177.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>µw-model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>µw-model (membership-weight typicality model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple interpretable typicality classification model that represents an instance as a vector of per-dimension typicality (µ) values weighted (w) by dimension importance, producing a representativeness score for concepts via a weighted norm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>µw-model (typicality / membership-weight model)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Represents instances by per-dimension membership functions µ (typicality relative to concept prototypes) and per-dimension weights w (importance for the concept), combining them (via a weighted Euclidean norm) to yield an overall typicality/representativeness score used for interpretable categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>prototype-based weighted-feature vector (interpretable, fuzzy-membership-based)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Per-dimension fuzzy membership (µ); explicit per-dimension weights (w) reflecting feature centrality; transparent computation (weighted norm) giving decomposable explanations (which dimensions drove classification).</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Designed to incorporate empirically motivated hypotheses (e.g., inverse relation between dimension weight and cross-instance variability) and prototype effects; used in the paper's manufacturing simulation to classify objects using physical and utilisation properties and produce interpretable outputs validated with domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Simplifying assumptions (independent orthogonal dimensions, frequentistic µ estimation) may break down with correlated properties or noisy detectors; continuous weighting and robust automatic learning of w and µ in complex domains require further empirical validation; existing evidence is preliminary (proof-of-concept simulation and qualitative expert feedback).</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Interpretable object classification in data-scarce manufacturing settings, where decomposition into interpretable properties (physical and utilisation) and explainable typicality scores are desired.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Unlike opaque deep-network classifiers, µw-model is inherently interpretable and directly decomposable into per-feature contributions; compared to exemplar models it is more compact (prototype-based); compared to formal fuzzy-set models it operationalizes µ and w for practical classification.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Learn µ per dimension frequentistically from labelled instances (prototype most common value); learn w via empirical hypothesis linking lower within-category variability to higher weight; compute per-concept representativeness vector and overall typicality via weighted norm; choose category with maximal typicality and provide per-dimension explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How to automatically learn or adapt quality domains and correlated dimensions; sensitivity to choice of prototype and to noisy property detectors; extension to richer event or relational concepts; need for rigorous quantitative evaluation in real-world settings beyond simulation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5177.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5177.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Fuzzy membership</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Fuzzy set / membership function approach</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of fuzzy-set style membership functions µ to represent graded membership or typicality of instances with respect to a quality dimension for a concept, enabling soft, interpretable categorization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Fuzzy sets</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Fuzzy membership (membership-function approach)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Rather than binary inclusion, an instance's degree of belonging to a concept along a given quality dimension is represented by a membership value µ in [0,1], permitting graded typicality and partial category membership.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>fuzzy graded-membership per-dimension (continuous scalar values)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Graded membership; smooth interpolation across property values; compatible with prototype views by centring membership around prototypical values; decomposable across interpretable dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Fuzzy representations align with observed graded typicality effects and human judgments that are not strictly binary; fuzzy constructs have been fruitfully used in cognitive and computational models to capture vagueness in natural categories.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Selecting specific membership function shapes and parameters can be ad hoc; pure fuzzy models do not by themselves explain why some features matter more (need weights or causal constraints); mapping from sensory measurements to robust µ values requires reliable detectors.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Interpretable classification, graded categorization, systems requiring soft decision thresholds and explainable per-feature contributions (as in the manufacturing use case).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Provides a graded alternative to crisp symbolic membership and complements prototype models by operationalizing degree of resemblance; unlike probabilistic Bayesian degrees, fuzzy µ are not direct beliefs but degrees of fuzzy membership—both formalisms can sometimes be related but differ conceptually.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Compute per-dimension µ based on distance from prototypical value or frequency distributions; combine µ values (optionally weighted) to yield overall category typicality; use thresholds to flag disputable classifications.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Principled methods to learn membership functions from sparse noisy data; integration with causal/theory-based constraints; relationship and conversion between fuzzy µ and probabilistic beliefs in inductive learning.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5177.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5177.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Property decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Property-decomposition / feature-based representation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Decomposing concepts into interpretable properties (quality dimensions) such as colour, size, texture, and utilisation (affordances), and representing instances as vectors of these property values for classification and explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Property decomposition (feature-based conceptual representation)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Concepts are functionally represented as structured sets of property/value dimensions; instances are mapped to these dimensions and categorization proceeds by comparing these property vectors to concept prototypes or rules, enabling transparent, decomposable explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>interpretable feature-vector (structured property decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Explicit, human-interpretable dimensions; supports mixing of sensor-derived physical properties and knowledge-derived utilisation properties; amenable to sparse-data learning and frugal AI; provides per-feature explanations and importance weights.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Behavioral and developmental evidence shows humans attend to different properties for generalization (e.g., children use causal/functional features in naming tasks); in this paper, property decomposition enabled data-efficient categorization in simulation and comprehension by domain experts.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Choice and granularity of properties may be domain-specific and require engineering; correlations between properties (non-independence) complicate simple vector assumptions; learning which properties matter (weights) may need causal inference beyond frequency statistics.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Object recognition under data scarcity, explainable AI, human-machine teaming, affordance and utilisation inference, developmental naming/generalization tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>More interpretable than opaque distributed representations; less rigid than symbolic rule lists because properties can be continuous and weighted; bridges between raw sensory inputs and symbolic concepts (as with Conceptual Spaces).</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Detect physical properties from sensors; extract utilisation properties from knowledge bases; standardize/normalize property values into a vector; compute per-dimension membership and combine via weighted norms to produce category typicality and explain which properties drove classification.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Automating selection and extraction of high-quality utilisation properties; handling interdependent/correlated properties; integrating learned latent features from deep models while preserving interpretability; empirical methods to learn domain-appropriate weights robustly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5177.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5177.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Theory-based Bayesian models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Theory-based Bayesian models of inductive learning and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of models that explain rapid human concept learning by positing structured hypothesis spaces (theories) and using Bayesian inference to select and generalize hypotheses from sparse data guided by inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Theory-based Bayesian models of inductive learning and reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Theory-based Bayesian models (theory-theory)</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual knowledge is represented functionally as structured hypotheses or causal theories; learning and inference proceed by Bayesian updating over these hypotheses, allowing strong generalizations from few examples via background inductive constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>theory/structured-hypothesis space (probabilistic, causal, hierarchical)</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Structured causal hypotheses; explicit inductive priors; hierarchical abstraction; data-efficiency and compositional generalization; explains counterfactual and causal inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Accounts for human ability to learn rich concepts from minimal data, matches human inductive generalisations in numerous experiments (Tenenbaum et al., Lake et al.), and provides normative computational accounts of concept learning phenomena.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Specifying appropriate hypothesis spaces and priors is hard and sometimes hand-engineered; computational intractability for large-scale real-world domains; psychological realism of full Bayesian computation is debated (approximate algorithms needed).</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Few-shot concept learning, causal reasoning, inductive generalization, modeling child concept acquisition, structured concept learning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Stronger than pure similarity/prototype models at explaining theory-like inferences and generalizations from sparse data; contrasts with exemplar/prototype accounts by positing rich internal causal structure rather than only similarity metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Represent candidate causal/theoretical hypotheses; observe data and compute posterior probabilities over hypotheses via Bayes' rule; use most probable hypotheses to guide categorization and prediction; structure guides which features are relevant.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How hypotheses/priors are acquired developmentally; tractable algorithmic implementations for embodied agents operating in rich real environments; integration with continuous perceptual representations like Conceptual Spaces.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5177.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5177.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Causal status / feature centrality</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Causal status as determinant of feature centrality (causal-feature models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The empirical and theoretical claim that features' importance for category membership is determined by their causal role in the kind's underlying structure, so causal properties become central and more heavily weighted in conceptual representation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Causal status effect in children's categorization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Causal status / feature centrality</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptual representations functionally weight features according to their causal relevance to the category; features judged to be causally central have higher representational weight and determine generalization and categorization more than superficial correlated features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>feature-weighted, causally-structured feature representation</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Feature importance determined by causal role; explains why some properties (utilisation/affordances) dominate artifact categorization; yields asymmetries in generalization and feature centrality across natural kinds and artifacts.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Experimental studies (e.g., Ahn et al.) show that children and adults prioritize causally-relevant features in categorization and that causal explanations change feature centrality; paper uses this empirical hypothesis to set inverse relation between weight and within-category variability.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Identifying and inferring causal roles from observations is nontrivial; causal attributions can be context-dependent; integrating causal learning with purely similarity-based representations is an open computational problem.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Feature selection for categorization, learning artifact affordances, developmental studies of concept acquisition, interpretable AI systems needing meaningful feature weights.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Augments prototype/feature-based models by providing a principled reason for weight assignment; contrasts with purely statistical weighting (e.g., by frequency/variance) by positing causal explanatory priority.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Infer or assume causal relationships linking surface properties to underlying functions; increase representation weights for causally central features; use weighted membership/typicality calculations to drive categorization and explanation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>Mechanisms for reliably discovering causal roles in embodied agents; interplay with statistical correlations (features that correlate but are not causal); scaling causal inference in complex real-world domains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5177.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5177.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of theories, models, or empirical findings about the representational format of conceptual knowledge in brains at a functional (not neural) level.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Typicality / prototypical effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Typicality effects in categorization (graded membership phenomena)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical class of findings that category membership and cognitive processing (naming, verification, generalization) vary continuously with an item's typicality/proximity to a category prototype rather than being strictly binary.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cognition and Categorization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_name</strong></td>
                            <td>Typicality / prototypicality effects</td>
                        </tr>
                        <tr>
                            <td><strong>theory_or_model_description</strong></td>
                            <td>Conceptually, category membership is graded: instances closer to a concept's prototype are judged more typical and are processed faster and generalized from more readily; typicality modulates categorization decisions and explanations.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_format_type</strong></td>
                            <td>graded similarity/prototype-based representation</td>
                        </tr>
                        <tr>
                            <td><strong>key_properties</strong></td>
                            <td>Graded membership scores; affects response latencies and generalization patterns; supports fuzzy/continuous membership modelling and weighted-dimension influences.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_support</strong></td>
                            <td>Extensive behavioral evidence (Rosch, Mervis, and others) showing typicality gradients, faster recognition and naming for prototypical items, prototypicality effects in learning and judgement tasks; used as a basis for the µw-model's typicality computations.</td>
                        </tr>
                        <tr>
                            <td><strong>empirical_challenges</strong></td>
                            <td>Some categorization tasks reveal rule-like or theory-based judgments that typicality alone cannot explain; atypical category members can still be endorsed given causal/theoretical considerations.</td>
                        </tr>
                        <tr>
                            <td><strong>applied_domains_or_tasks</strong></td>
                            <td>Perceptual categorization, language (naming), concept formation from few examples, explainable AI outputs that show graded confidence/typicality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models</strong></td>
                            <td>Operationalized in prototype and fuzzy-set theories and contrasted with exemplar models which explain some graded effects via stored instances; theory-based models may override typicality when causal knowledge applies.</td>
                        </tr>
                        <tr>
                            <td><strong>functional_mechanisms</strong></td>
                            <td>Compute distance/similarity between instance and prototype across weighted dimensions to derive a typicality score used for categorization, explanation, and confidence estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_open_questions</strong></td>
                            <td>How typicality interacts with causal/theoretical knowledge in guiding judgments; mechanisms for shifting weights when contexts emphasize different dimensions; deriving typicality from sparse or noisy perceptual data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Conceptual spaces: The geometry of thought <em>(Rating: 2)</em></li>
                <li>Family resemblances: Studies in the internal structure of categories <em>(Rating: 2)</em></li>
                <li>Theory-based Bayesian models of inductive learning and reasoning <em>(Rating: 2)</em></li>
                <li>Causal status effect in children's categorization <em>(Rating: 2)</em></li>
                <li>Feature centrality and conceptual coherence <em>(Rating: 2)</em></li>
                <li>Organizing conceptual knowledge in humans with a gridlike code <em>(Rating: 1)</em></li>
                <li>The parallel distributed processing approach to semantic cognition <em>(Rating: 1)</em></li>
                <li>Family resemblances: Studies in the internal structure of categories <em>(Rating: 1)</em></li>
                <li>Towards the cognitive plausibility of conceptual space models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5177",
    "paper_id": "paper-258822829",
    "extraction_schema_id": "extraction-schema-109",
    "extracted_data": [
        {
            "name_short": "Conceptual Spaces",
            "name_full": "Gärdenfors's Conceptual Spaces framework",
            "brief_description": "A cognitively motivated geometric framework that represents concepts as convex regions in a multidimensional space spanned by interpretable quality domains (e.g., colour, size, utilisation), allowing continuous instance vectors, similarity metrics and concept algebraic operations.",
            "citation_title": "Conceptual spaces: The geometry of thought",
            "mention_or_use": "use",
            "theory_or_model_name": "Conceptual Spaces",
            "theory_or_model_description": "Concepts are represented functionally as regions (often convex) within a metric, multidimensional space whose axes (quality dimensions/domains) correspond to cognitively meaningful properties; instances are points (vectors) in this space and categorization proceeds by proximity/typicality relative to concept regions or prototypes.",
            "representation_format_type": "prototype/region-based geometric (continuous, interpretable, feature-based)",
            "key_properties": "Multidimensional continuous representation; interpretable quality domains; prototypes and convex concept regions; natural similarity metric; supports concept combination and algebra; bridges subsymbolic (sensory vectors) and symbolic (languageable concepts).",
            "empirical_support": "Behavioral cognitive-semantic phenomena captured include prototypicality and family-resemblance effects (Rosch et al.), schematicity, similarity-based categorization, and successful applications in modelling categorization and concept combination; the framework has been used in computational implementations and cited as compatible with observed human conceptual behaviour in many cognitive studies.",
            "empirical_challenges": "Requires selection and delimitation of appropriate quality domains for a given domain; real-world property correlations and interdependencies complicate simple orthogonal-space assumptions; scalability and formal guarantees for complex high-dimensional/structured concepts are debated; empirical mapping from sensory data to stable quality dimensions is nontrivial.",
            "applied_domains_or_tasks": "Categorization/classification, concept combination, similarity judgements, semantic modelling, explainable knowledge representation (this paper's manufacturing use case), and computational cognitive architectures.",
            "comparison_to_other_models": "Differs from symbolic models by retaining continuous structure and from exemplar/distributed accounts by positing interpretable dimensions and region/prototype representations; touted as a middle-ground (neuro-symbolic) that preserves interpretability unlike opaque distributed neural representations.",
            "functional_mechanisms": "Form concepts by defining prototypes/regions in quality-domain space; compute instance typicality via distance/similarity to prototypes or by evaluating membership across weighted dimensions; combine concepts via geometric operations; allow explanations via explicit dimensions and prototype comparisons.",
            "limitations_or_open_questions": "How to choose and learn a domain-appropriate set of quality dimensions; how to handle correlated or non-orthogonal properties; how to scale to highly structured relational concepts and events; normative learning algorithms for regions and weights; how exactly this maps functionally onto cognitive processes of concept acquisition.",
            "uuid": "e5177.0"
        },
        {
            "name_short": "Prototype theory",
            "name_full": "Prototype theory (Roschian prototype view of categories)",
            "brief_description": "A theory positing that categories are organized around central best examples (prototypes) rather than via necessary and sufficient features, producing graded membership and typicality effects in categorization.",
            "citation_title": "Family resemblances: Studies in the internal structure of categories",
            "mention_or_use": "use",
            "theory_or_model_name": "Prototype theory",
            "theory_or_model_description": "Conceptual knowledge is functionally organized around prototypical exemplars; category membership is graded and determined by similarity to prototypes rather than strict rule-based membership.",
            "representation_format_type": "prototype/central tendency (continuous, similarity-based)",
            "key_properties": "Graded membership/typicality; family resemblance structure; emphasis on central tendency rather than necessary features; supports similarity-based generalization and flexible category boundaries.",
            "empirical_support": "Robust behavioral findings: typicality gradients, faster verification for prototypical items, graded category membership in adults and children (Rosch et al., Rosch & Mervis), used widely in experimental categorization studies.",
            "empirical_challenges": "Fails to account for certain rule-based category phenomena, relational or theory-driven inferences, some cases where exemplar or rule models better predict data; cannot by itself explain why some features are central and others peripheral (requires additional mechanisms such as causal/essentialist constraints).",
            "applied_domains_or_tasks": "Basic-level categorization, concept learning from limited data, similarity judgements, explaining graded category membership in psychology and computational models.",
            "comparison_to_other_models": "Contrasted with exemplar models (which represent many stored instances) and rule/theory-based models (which posit structured causal knowledge); prototype theory is simpler and explains graded typicality but is weaker for causal and relational inferences.",
            "functional_mechanisms": "Form prototypes (centroids) from experienced instances; compute similarity/typicality to prototypes to determine category membership and guide generalization; weights over dimensions determine the contribution of each property.",
            "limitations_or_open_questions": "How prototypes are formed and updated with sparse data; how to integrate causal/functional knowledge that affects feature centrality; how to represent complex structured or relational concepts beyond similarity to prototypes.",
            "uuid": "e5177.1"
        },
        {
            "name_short": "µw-model",
            "name_full": "µw-model (membership-weight typicality model)",
            "brief_description": "A simple interpretable typicality classification model that represents an instance as a vector of per-dimension typicality (µ) values weighted (w) by dimension importance, producing a representativeness score for concepts via a weighted norm.",
            "citation_title": "",
            "mention_or_use": "use",
            "theory_or_model_name": "µw-model (typicality / membership-weight model)",
            "theory_or_model_description": "Represents instances by per-dimension membership functions µ (typicality relative to concept prototypes) and per-dimension weights w (importance for the concept), combining them (via a weighted Euclidean norm) to yield an overall typicality/representativeness score used for interpretable categorization.",
            "representation_format_type": "prototype-based weighted-feature vector (interpretable, fuzzy-membership-based)",
            "key_properties": "Per-dimension fuzzy membership (µ); explicit per-dimension weights (w) reflecting feature centrality; transparent computation (weighted norm) giving decomposable explanations (which dimensions drove classification).",
            "empirical_support": "Designed to incorporate empirically motivated hypotheses (e.g., inverse relation between dimension weight and cross-instance variability) and prototype effects; used in the paper's manufacturing simulation to classify objects using physical and utilisation properties and produce interpretable outputs validated with domain experts.",
            "empirical_challenges": "Simplifying assumptions (independent orthogonal dimensions, frequentistic µ estimation) may break down with correlated properties or noisy detectors; continuous weighting and robust automatic learning of w and µ in complex domains require further empirical validation; existing evidence is preliminary (proof-of-concept simulation and qualitative expert feedback).",
            "applied_domains_or_tasks": "Interpretable object classification in data-scarce manufacturing settings, where decomposition into interpretable properties (physical and utilisation) and explainable typicality scores are desired.",
            "comparison_to_other_models": "Unlike opaque deep-network classifiers, µw-model is inherently interpretable and directly decomposable into per-feature contributions; compared to exemplar models it is more compact (prototype-based); compared to formal fuzzy-set models it operationalizes µ and w for practical classification.",
            "functional_mechanisms": "Learn µ per dimension frequentistically from labelled instances (prototype most common value); learn w via empirical hypothesis linking lower within-category variability to higher weight; compute per-concept representativeness vector and overall typicality via weighted norm; choose category with maximal typicality and provide per-dimension explanations.",
            "limitations_or_open_questions": "How to automatically learn or adapt quality domains and correlated dimensions; sensitivity to choice of prototype and to noisy property detectors; extension to richer event or relational concepts; need for rigorous quantitative evaluation in real-world settings beyond simulation.",
            "uuid": "e5177.2"
        },
        {
            "name_short": "Fuzzy membership",
            "name_full": "Fuzzy set / membership function approach",
            "brief_description": "Use of fuzzy-set style membership functions µ to represent graded membership or typicality of instances with respect to a quality dimension for a concept, enabling soft, interpretable categorization.",
            "citation_title": "Fuzzy sets",
            "mention_or_use": "use",
            "theory_or_model_name": "Fuzzy membership (membership-function approach)",
            "theory_or_model_description": "Rather than binary inclusion, an instance's degree of belonging to a concept along a given quality dimension is represented by a membership value µ in [0,1], permitting graded typicality and partial category membership.",
            "representation_format_type": "fuzzy graded-membership per-dimension (continuous scalar values)",
            "key_properties": "Graded membership; smooth interpolation across property values; compatible with prototype views by centring membership around prototypical values; decomposable across interpretable dimensions.",
            "empirical_support": "Fuzzy representations align with observed graded typicality effects and human judgments that are not strictly binary; fuzzy constructs have been fruitfully used in cognitive and computational models to capture vagueness in natural categories.",
            "empirical_challenges": "Selecting specific membership function shapes and parameters can be ad hoc; pure fuzzy models do not by themselves explain why some features matter more (need weights or causal constraints); mapping from sensory measurements to robust µ values requires reliable detectors.",
            "applied_domains_or_tasks": "Interpretable classification, graded categorization, systems requiring soft decision thresholds and explainable per-feature contributions (as in the manufacturing use case).",
            "comparison_to_other_models": "Provides a graded alternative to crisp symbolic membership and complements prototype models by operationalizing degree of resemblance; unlike probabilistic Bayesian degrees, fuzzy µ are not direct beliefs but degrees of fuzzy membership—both formalisms can sometimes be related but differ conceptually.",
            "functional_mechanisms": "Compute per-dimension µ based on distance from prototypical value or frequency distributions; combine µ values (optionally weighted) to yield overall category typicality; use thresholds to flag disputable classifications.",
            "limitations_or_open_questions": "Principled methods to learn membership functions from sparse noisy data; integration with causal/theory-based constraints; relationship and conversion between fuzzy µ and probabilistic beliefs in inductive learning.",
            "uuid": "e5177.3"
        },
        {
            "name_short": "Property decomposition",
            "name_full": "Property-decomposition / feature-based representation",
            "brief_description": "Decomposing concepts into interpretable properties (quality dimensions) such as colour, size, texture, and utilisation (affordances), and representing instances as vectors of these property values for classification and explanation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "theory_or_model_name": "Property decomposition (feature-based conceptual representation)",
            "theory_or_model_description": "Concepts are functionally represented as structured sets of property/value dimensions; instances are mapped to these dimensions and categorization proceeds by comparing these property vectors to concept prototypes or rules, enabling transparent, decomposable explanations.",
            "representation_format_type": "interpretable feature-vector (structured property decomposition)",
            "key_properties": "Explicit, human-interpretable dimensions; supports mixing of sensor-derived physical properties and knowledge-derived utilisation properties; amenable to sparse-data learning and frugal AI; provides per-feature explanations and importance weights.",
            "empirical_support": "Behavioral and developmental evidence shows humans attend to different properties for generalization (e.g., children use causal/functional features in naming tasks); in this paper, property decomposition enabled data-efficient categorization in simulation and comprehension by domain experts.",
            "empirical_challenges": "Choice and granularity of properties may be domain-specific and require engineering; correlations between properties (non-independence) complicate simple vector assumptions; learning which properties matter (weights) may need causal inference beyond frequency statistics.",
            "applied_domains_or_tasks": "Object recognition under data scarcity, explainable AI, human-machine teaming, affordance and utilisation inference, developmental naming/generalization tasks.",
            "comparison_to_other_models": "More interpretable than opaque distributed representations; less rigid than symbolic rule lists because properties can be continuous and weighted; bridges between raw sensory inputs and symbolic concepts (as with Conceptual Spaces).",
            "functional_mechanisms": "Detect physical properties from sensors; extract utilisation properties from knowledge bases; standardize/normalize property values into a vector; compute per-dimension membership and combine via weighted norms to produce category typicality and explain which properties drove classification.",
            "limitations_or_open_questions": "Automating selection and extraction of high-quality utilisation properties; handling interdependent/correlated properties; integrating learned latent features from deep models while preserving interpretability; empirical methods to learn domain-appropriate weights robustly.",
            "uuid": "e5177.4"
        },
        {
            "name_short": "Theory-based Bayesian models",
            "name_full": "Theory-based Bayesian models of inductive learning and reasoning",
            "brief_description": "A family of models that explain rapid human concept learning by positing structured hypothesis spaces (theories) and using Bayesian inference to select and generalize hypotheses from sparse data guided by inductive biases.",
            "citation_title": "Theory-based Bayesian models of inductive learning and reasoning",
            "mention_or_use": "mention",
            "theory_or_model_name": "Theory-based Bayesian models (theory-theory)",
            "theory_or_model_description": "Conceptual knowledge is represented functionally as structured hypotheses or causal theories; learning and inference proceed by Bayesian updating over these hypotheses, allowing strong generalizations from few examples via background inductive constraints.",
            "representation_format_type": "theory/structured-hypothesis space (probabilistic, causal, hierarchical)",
            "key_properties": "Structured causal hypotheses; explicit inductive priors; hierarchical abstraction; data-efficiency and compositional generalization; explains counterfactual and causal inferences.",
            "empirical_support": "Accounts for human ability to learn rich concepts from minimal data, matches human inductive generalisations in numerous experiments (Tenenbaum et al., Lake et al.), and provides normative computational accounts of concept learning phenomena.",
            "empirical_challenges": "Specifying appropriate hypothesis spaces and priors is hard and sometimes hand-engineered; computational intractability for large-scale real-world domains; psychological realism of full Bayesian computation is debated (approximate algorithms needed).",
            "applied_domains_or_tasks": "Few-shot concept learning, causal reasoning, inductive generalization, modeling child concept acquisition, structured concept learning tasks.",
            "comparison_to_other_models": "Stronger than pure similarity/prototype models at explaining theory-like inferences and generalizations from sparse data; contrasts with exemplar/prototype accounts by positing rich internal causal structure rather than only similarity metrics.",
            "functional_mechanisms": "Represent candidate causal/theoretical hypotheses; observe data and compute posterior probabilities over hypotheses via Bayes' rule; use most probable hypotheses to guide categorization and prediction; structure guides which features are relevant.",
            "limitations_or_open_questions": "How hypotheses/priors are acquired developmentally; tractable algorithmic implementations for embodied agents operating in rich real environments; integration with continuous perceptual representations like Conceptual Spaces.",
            "uuid": "e5177.5"
        },
        {
            "name_short": "Causal status / feature centrality",
            "name_full": "Causal status as determinant of feature centrality (causal-feature models)",
            "brief_description": "The empirical and theoretical claim that features' importance for category membership is determined by their causal role in the kind's underlying structure, so causal properties become central and more heavily weighted in conceptual representation.",
            "citation_title": "Causal status effect in children's categorization",
            "mention_or_use": "use",
            "theory_or_model_name": "Causal status / feature centrality",
            "theory_or_model_description": "Conceptual representations functionally weight features according to their causal relevance to the category; features judged to be causally central have higher representational weight and determine generalization and categorization more than superficial correlated features.",
            "representation_format_type": "feature-weighted, causally-structured feature representation",
            "key_properties": "Feature importance determined by causal role; explains why some properties (utilisation/affordances) dominate artifact categorization; yields asymmetries in generalization and feature centrality across natural kinds and artifacts.",
            "empirical_support": "Experimental studies (e.g., Ahn et al.) show that children and adults prioritize causally-relevant features in categorization and that causal explanations change feature centrality; paper uses this empirical hypothesis to set inverse relation between weight and within-category variability.",
            "empirical_challenges": "Identifying and inferring causal roles from observations is nontrivial; causal attributions can be context-dependent; integrating causal learning with purely similarity-based representations is an open computational problem.",
            "applied_domains_or_tasks": "Feature selection for categorization, learning artifact affordances, developmental studies of concept acquisition, interpretable AI systems needing meaningful feature weights.",
            "comparison_to_other_models": "Augments prototype/feature-based models by providing a principled reason for weight assignment; contrasts with purely statistical weighting (e.g., by frequency/variance) by positing causal explanatory priority.",
            "functional_mechanisms": "Infer or assume causal relationships linking surface properties to underlying functions; increase representation weights for causally central features; use weighted membership/typicality calculations to drive categorization and explanation.",
            "limitations_or_open_questions": "Mechanisms for reliably discovering causal roles in embodied agents; interplay with statistical correlations (features that correlate but are not causal); scaling causal inference in complex real-world domains.",
            "uuid": "e5177.6"
        },
        {
            "name_short": "Typicality / prototypical effects",
            "name_full": "Typicality effects in categorization (graded membership phenomena)",
            "brief_description": "Empirical class of findings that category membership and cognitive processing (naming, verification, generalization) vary continuously with an item's typicality/proximity to a category prototype rather than being strictly binary.",
            "citation_title": "Cognition and Categorization",
            "mention_or_use": "use",
            "theory_or_model_name": "Typicality / prototypicality effects",
            "theory_or_model_description": "Conceptually, category membership is graded: instances closer to a concept's prototype are judged more typical and are processed faster and generalized from more readily; typicality modulates categorization decisions and explanations.",
            "representation_format_type": "graded similarity/prototype-based representation",
            "key_properties": "Graded membership scores; affects response latencies and generalization patterns; supports fuzzy/continuous membership modelling and weighted-dimension influences.",
            "empirical_support": "Extensive behavioral evidence (Rosch, Mervis, and others) showing typicality gradients, faster recognition and naming for prototypical items, prototypicality effects in learning and judgement tasks; used as a basis for the µw-model's typicality computations.",
            "empirical_challenges": "Some categorization tasks reveal rule-like or theory-based judgments that typicality alone cannot explain; atypical category members can still be endorsed given causal/theoretical considerations.",
            "applied_domains_or_tasks": "Perceptual categorization, language (naming), concept formation from few examples, explainable AI outputs that show graded confidence/typicality.",
            "comparison_to_other_models": "Operationalized in prototype and fuzzy-set theories and contrasted with exemplar models which explain some graded effects via stored instances; theory-based models may override typicality when causal knowledge applies.",
            "functional_mechanisms": "Compute distance/similarity between instance and prototype across weighted dimensions to derive a typicality score used for categorization, explanation, and confidence estimates.",
            "limitations_or_open_questions": "How typicality interacts with causal/theoretical knowledge in guiding judgments; mechanisms for shifting weights when contexts emphasize different dimensions; deriving typicality from sparse or noisy perceptual data.",
            "uuid": "e5177.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Conceptual spaces: The geometry of thought",
            "rating": 2,
            "sanitized_title": "conceptual_spaces_the_geometry_of_thought"
        },
        {
            "paper_title": "Family resemblances: Studies in the internal structure of categories",
            "rating": 2,
            "sanitized_title": "family_resemblances_studies_in_the_internal_structure_of_categories"
        },
        {
            "paper_title": "Theory-based Bayesian models of inductive learning and reasoning",
            "rating": 2,
            "sanitized_title": "theorybased_bayesian_models_of_inductive_learning_and_reasoning"
        },
        {
            "paper_title": "Causal status effect in children's categorization",
            "rating": 2,
            "sanitized_title": "causal_status_effect_in_childrens_categorization"
        },
        {
            "paper_title": "Feature centrality and conceptual coherence",
            "rating": 2,
            "sanitized_title": "feature_centrality_and_conceptual_coherence"
        },
        {
            "paper_title": "Organizing conceptual knowledge in humans with a gridlike code",
            "rating": 1,
            "sanitized_title": "organizing_conceptual_knowledge_in_humans_with_a_gridlike_code"
        },
        {
            "paper_title": "The parallel distributed processing approach to semantic cognition",
            "rating": 1,
            "sanitized_title": "the_parallel_distributed_processing_approach_to_semantic_cognition"
        },
        {
            "paper_title": "Family resemblances: Studies in the internal structure of categories",
            "rating": 1,
            "sanitized_title": "family_resemblances_studies_in_the_internal_structure_of_categories"
        },
        {
            "paper_title": "Towards the cognitive plausibility of conceptual space models",
            "rating": 2,
            "sanitized_title": "towards_the_cognitive_plausibility_of_conceptual_space_models"
        }
    ],
    "cost": 0.01704325,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments</p>
<p>Vedran Galetić vedran.galetic@airbus.com 
Airbus Central R&amp;T AI Research Team Filton
Airbus Central R&amp;T AI Research Team Filton
United Kingdom, United Kingdom</p>
<p>Alistair Nottle alistair.nottle@airbus.com 
Airbus Central R&amp;T AI Research Team Filton
Airbus Central R&amp;T AI Research Team Filton
United Kingdom, United Kingdom</p>
<p>Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments
Knowledge Representation and ReasoningConceptual SpacesEx- plainable AITrustworthy AIComprehensibilityInterpretabilityHuman-Machine TeamingEmbodied Intelligent AgentsFrugal AI
Trustworthiness of artificially intelligent agents is vital for the acceptance of human-machine teaming in industrial manufacturing environments. Predictable behaviours and explainable (and understandable) rationale allow humans collaborating with (and building) these agents to understand their motivations and therefore validate decisions that are made. To that aim, we make use of Gärdenfors's cognitively inspired Conceptual Space framework to represent the agent's knowledge using concepts as convex regions in a space spanned by inherently comprehensible quality dimensions. A simple typicality quantification model is built on top of it to determine fuzzy category membership and classify instances interpretably. We apply it on a use case from the manufacturing domain, using objects' physical properties obtained from cobots' onboard sensors and utilisation properties from crowdsourced commonsense knowledge available at public knowledge bases. Such flexible knowledge representation based on property decomposition allows for data-efficient representation learning of typically highly specialist or specific manufacturing artefacts. In such a setting, traditional data-driven (e.g., computer vision-based) classification approaches would struggle due to training data scarcity. This allows for comprehensibility of an AI agent's acquired knowledge by the human collaborator thus contributing to trustworthiness. We situate our approach within an existing explainability framework specifying explanation desiderata. We provide arguments for our system's applicability and appropriateness for different roles of human agents collaborating with the AI system throughout its design, validation, and operation.</p>
<p>INTRODUCTION</p>
<p>Use of Artificial Intelligence (AI) is increasing across industry, often providing performance approaching, or even surpassing, that of humans on cognitive tasks for specific applications (e.g., [32,51]). This in turn leads to an increase in the use of embodied AI agents cooperating with humans and becoming embedded on critical systems. There is an increasing need to build trust within these teams. For instance, within the frame of aerospace manufacturing and operations, AI can assist workers in practical tasks such as: optimising scheduling of resources; automated visual inspections; and natural language interactions for receiving commands and reporting events; as well reducing user's cognitive load and supporting automated decision making in fleet management.</p>
<p>Whilst performance gains can be clear and measurable, ensuring acceptance of these technologies, and thereby realising those gains, is difficult. Explainability is one of key elements in building appropriate trust (i.e., not complacency), which is necessary for their acceptance.</p>
<p>Current high-performing AI systems, often based on deep neural network models (i.e., Deep Learning (DL)), tend to be 'black boxes', shrouding their internal knowledge and decision making processes, which in turn may not be readily accessible or interpretable to human users and subjects interacting with, or developing, them. Explainable AI (XAI) is widely seen as one of the cornerstones of AI trustworthiness. Indeed, the European Commission's High Level Expert Group on Artificial Intelligence (HLEGAI) specifies 'explainability' (or explicability) as one of their four ethical principles of fundamental trustworthiness of AI, with the others being: respect for human autonomy, prevention of harm, and fairness [28] (and XAI can play a part in ensuring those other three principles). Furthermore, the Group specifies seven key requirements that are to be addressed throughout an AI product's life-cycle, from which 'transparency', 'accountability', and 'human agency and oversight' are ones clearly related to explainability. These guidelines are upheld by the European Union Aviation Safety Agency (EASA), focusing on challenges around certification that black-box AI models impose, echoing explainability as one of the three main components of trustworthy AI [14], and fully recognising human-centricity in its AI Roadmap [13].</p>
<p>While the interest in XAI research has been tremendous both in academia and industry (e.g., [4,12,24,41]), the outputs have primarily been focussed on post-hoc explainability methods and techniques, i.e., providing explanations of an already developed high-performing yet 'explanatorily opaque' machine learning systems. Instead, we attempt inherent (or intrinsic) explainability by incorporating interpretability requirements early in the system development cycle and focusing on building inherently interpretable 1 and understandable AI systems. In this paper we demonstrate this through a more inherently explainable knowledge representation of the AI agent.</p>
<p>We model the agent's knowledge using a complementary combination of abstracted information from the agent's sensors and openly available commonsense general knowledge sources, subject to simple heuristic engineering. By representing both classes of knowledge on the same representation framework, typical for hybrid AI approaches, allows for knowledge modelling flexibility across application domains. Also, it addresses challenges of (high-performant) computer vision-based approaches of object classification pertaining to data scarcity for rare and specialist objects (not uncommon in aerospace manufacturing) and account of its rationale and outputs in a human-understandable actionable way.</p>
<p>INTERPRETABLE KNOWLEDGE REPRESENTATION</p>
<p>One means of increasing trustworthiness, and consequent certifiability 2 , of an AI agent teaming with humans in operational environments is through inherently interpretable knowledge representation modelling. This allows for the agent's comprehensibility, i.e., the ability to represent its acquired knowledge in humanunderstandable terms [4]. Symbolic representations are a classical way to represent knowledge due to symbols' intrinsic meaning. Although symbols are amenable for computational approaches involving logical calculus, one challenge of this level of representation is modelling the intelligent agent's concept acquisition 3 . Artificially intelligent agents, especially those that are embedded in complex environments and perform higher cognitive tasks, are predominantly based on deep neural networks, excelling at learning from statistical regularities of sensory inputs. Human mind employs prominent qualities of both of these representations.</p>
<p>Therefore, another challenge is mapping between the continuous space of knowledge representation engendered by learnt network models and the symbolic representations. Systematic neurosymbolic mapping, or an intermediary knowledge representation interfacing with both levels, would alleviate one of the major challenges of neural representations, namely, their opacity and consequent incomprehensibility of learnt knowledge, standing in the way of the AI's trustworthiness and certifiability.</p>
<p>Conceptual Knowledge Representation</p>
<p>The semantic theory of Conceptual Spaces [20,21] is an apt formalism in bridging the neuro-symbolic gap in knowledge modelling, proved useful in various application domains [57]. Concepts have inherent meaning, often denoted by corresponding symbols (those 1 As a note, we do not equate inherent interpretability with algorithmic or model transparency characteristic [41]. We adopt a more general definition of interpretability as 'the ability to explain or to present in understandable terms to a human' [12]. 2 By Certifiability we mean the ability for a system to be certified for use in a regulated environment. 3 It is, however, fair to acknowledge some promising steps forward in modelling interpretable concept acquisition and representation on the neural implementation level, e.g., see [9]. concepts that are languageable), whilst retaining their continuous nature arising from neural sensing of environmental input. The framework represents concepts as convex regions in a geometric space spanned by quality domains. A concept (e.g., apple) is described by pertaining ranges across quality domains (e.g., colour, size, taste) and encompasses instances (e.g., this green and sour apple sitting on my desk) represented as vectors characterised by specific properties (e.g., green, sour).</p>
<p>The Conceptual Spaces framework adopts basic tenets of cognitive semantics (inherited in turn from cognitive psychology), thus assuming prototypical effects in categorisation [46][47][48] and schematicity [37]. Conceptual algebraic operations and space mappings can be used to describe metaphoric and metonymic communication operations, while modelling concept combinations and quantifying similarity 4 arise naturally from the very structure of the geometric space.</p>
<p>Moreover, it has been empirically demonstrated that neural populations exhibit geometric (hexadirectional) organisation of conceptual knowledge in the mind, demonstrated on spatial tasks in rodents [25] as well as humans on spatial and, crucially, abstract non-spatial tasks [10]. It is suggested that cognitive maps of concepts may utilise place cells for concept indexing within a geometric space spanned by the hippocampal-entorhinal grid cell system. That makes Conceptual Spaces a valuable candidate for a neuroscientifically (along with cognitively) motivated knowledge representation framework [7].</p>
<p>Crowdsourced Knowledge Base</p>
<p>Humans are able to rapidly construct hyper-hypotheses based on remarkably little data [53,54] and utilise them as background knowledge and constraints in various tasks such as ad-hoc classification. Capturing the structure and content of commonsense knowledge is a challenging task in modelling an artificially intelligent agent. As a remedy, manually-crafted and crowdsourced knowledge bases can be used as a proxy. They are typically knowledge graphs such as ontologies (e.g., [38]) or hierarchically organised synsets [44].</p>
<p>ConceptNet [50] is an openly-available knowledge graph constructed through orchestrated input from both structured manuallycrafted sources (e.g., Wiktionary, OpenCyc [38]) and crowdsourcing campaigns. Despite heterogeneous sources and a vast number of concepts (8 million nodes) and relations between them (21 million edges), the knowledge base is remarkably tidy and manageable with only 36 consolidated relation types. Some of these relations such as UsedFor, MadeOf, and PartOf are of particular relevance for our use case in the aerospace manufacturing domain.</p>
<p>As we will see in the upcoming sections, utilisation properties that are acquired from this knowledge base shall be instrumental in modelling concepts as utilisation properties tend to be more relevant for classifying manufacturing objects than their surface physical properties. </p>
<p>Typicality Model</p>
<p>Whilst some very developed and faithful Conceptual Space implementations exist [5], we draw inspiration from the Conceptual Space framework and property decomposition-based representation of concepts that it employs, and propose utilising a simple model for interpretable fuzzy [56] classification. The model, called µw-model [17], relies on the prototype theory [46] and empirical theories on causal status [1,2] and concept centrality [49]. The model's name originates from two parameters that are used to describe an instance's typicality status in the frame of a concept, namely: the membership function µ quantifying typicality for one quality dimension (see Fig. 1); and the weight w per quality dimension. It is thereby possible to ascertain and explain that, for example, although an object observed in the manufacturing environment has surface similarities to the prototype of a previously learnt class, the facts that it is used for a different activity (see § 2.2) and that the utilisation property is much weightier than surface properties in the manufacturing environment jointly steer its classification to another class (e.g., Fig. 2).</p>
<p>These parameters are learnt in different ways, yet both are based on the agent's property detection capabilities (e.g., visual sensor) accompanied by tutor-provided labels of encountered instances. Based on instances' labels, and quality dimension value distributions, the agent is able to quantify µ frequentistically (e.g., assuming the prototype has the most occurring value), while w is quantified by using an empirically confirmed [19] hypothesis, based on concept centrality, on inverse relationship between the dimension's weight on one side and its variability of instances' property values across the category on the other (e.g., colour tends to be more important for natural kinds [48] such as apple than for artefacts such as car as the latter can usually be in an arbitrary colour).</p>
<p>Each instance is represented as a vector as shown in Eq. 1:
− → ( ) = ∑︁ √︄ ( ) ( ) ( ) · − → (1)
where is a quality dimension (e.g., width), − → are basis vectors spanning the space, is a quality domain (e.g., size), is an instance (e.g., this apple), ( ) is the weight of the quality dimension for the concept (e.g., apple), and ( ) is the typicality measure of the instance for the concept with respect to the quality dimension (e.g., the representativeness of 'this zebra-striped ball' for the concept zebra with respect to the quality dimension texture; which, as a note, would be high for this dimension, but extremely low for virtually any other dimension).</p>
<p>Typicality of the instance c with respect to concept C is the second norm of vector in Eq. 1, thus is calculated as in Eq. 2:
( ) = ∥ − → ( )∥ = √︄ ∑︁ ( ) ( ) 2 ( )(2)
while it is sensible to treat the instance as one pertaining to that concept for which its typicality is the highest, as in Eq. 3.
( ) = max ( )(3)
As the system is able to flag any disputable classifications (e.g., those for which the difference between the maximal and the second largest typicality from Eq. 2 is not above a threshold) in an interpretable way, it is possible to investigate the case using intrinsically interpretable explanation of the rationale as, e.g., a developer debugging the system or an operator on the shop floor.</p>
<p>To examine the feasibility of our model in a 'real-world' scenario, we applied the described knowledge modelling approach to an aerospace manufacturing process. There is the potential to make improvements to existing capability as well as develop new ones though the use of collaborative robots, or 'cobots'. These 'cobots' will improve quality and free up skilled workers to focus on their specialities. Therefore, explainability of decisions from robotic assistants is vital to building acceptance and trust and enabling effective collaboration, allowing these benefits to be realised.</p>
<p>In order to build within cobots an awareness of their environment they need to be able to rapidly and efficiently recognise objects which they may not have seen previously as well as with clear and understandable explanations for this recognition. By modelling the artificial agent's knowledge interpretably, using the Conceptual Space framework [20], we reduce the need for post-hoc explanation.</p>
<p>As an example, a cobot may classify an object as a drill because it because it is of similar size, shape, colour, and material composition, as examples used during the training process. Capturing utilisation properties, which can indicate for instance that the item is used for drilling, allows us to produce classification which is by its very nature interpretable. For example, the operator may obtain a natural language output such as 'I believe this is a drill as it looks similar to other drills I've seen in the past, and it is used for drilling', accompanied by a visualisation with associated typicality measures. Moreover, if an object bears surface similarities to previously seen instances of a drill, but is perceived to be used for riveting would instead be classified as a riveter. This reflects that utilisation properties of industrial artefacts are more heavily weighted in a classification than the surface properties (Fig. 2).</p>
<p>Setup</p>
<p>Industrial environments can be extremely sensitive to disruption, particularly with just-in-time manufacturing processes. Therefore, current industrial setups do not readily allow for the deployment of experimental robotics on the factory floor. To combat this we have instead used a simulated environment of a cobot in a factory setting to provide training data, as well as to get feedback and validation of the proposed methodologies. We have used the Webots Open Source Robotics Simulator [11] to create, in the first instance, a simple environment with a controllable e-puck robot [15] equipped with a simple sensor package, such as a standard vision sensor (i.e., a camera) as well as a time-of-flight sensor.</p>
<p>The simulation environment is populated with a variety of objects, using simplistic idealised examples such as 'Green Ball' or 'Red Cube', to allow for easier recognition, but still 'relatable' and inherently interpretable objects (see Fig. 4). As we enhanced the AI system, we introduced additional objects to represent real-world, ecologically relevant, objects such as a hammer or screwdriver.</p>
<p>To further augment the properties available, we attach custom properties to the objects within the simulator (e.g., stripy texture). This assists in the determining of ground truth values for colour, texture, utilisation properties, etc. These values can be extracted from the simulated environment programmatically along with the labels of the objects to provide a training set from which to learn a conceptual representation using inherently explainable and interpretable terms.</p>
<p>A conceptual space is learned by Voronoi tessellation [23] around prototypes. In the simpler simulation environment, the objects are considered to already be idealised prototypes in order to make it easier to carry out proof-of-concept space construction. With the migration to a more complex simulation environment, prototypes are calculated as centroids of the labelled instances used during training.</p>
<p>The robot can then be manipulated by a user to move through the simulation, and the field-of-view of the camera is determined. Any objects coming within this field of vision will then trigger the Webots software to forward the image to our API, alongside the ground truth properties relating to that object.</p>
<p>In a real-world implementation, capturing of images and detection of the objects therein would be carried out by state-of-the-art object / region of interest detection algorithms. As these are not the focus of our research, we have instead extracted ground truth data and imagery from the simulation environment itself. This modular approach allows for algorithms (such as property detectors) to be inserted as needed, and users can select the most appropriate algorithm for the use case being exploited.</p>
<p>This geometric representation level can thus be treated as an intermediary "middle ground" between the symbolic and neural levels by, on the one hand, abstracting sensory inputs onto appropriate points in a conceptual space, whilst, on the other hand, grounding symbols onto concept prototypes (as suggested in, e.g., [52]).</p>
<p>Property Extraction for Interpretable Classification</p>
<p>Separate property detectors are applied to extract properties of the observed object. These take two broad categories of detectors: physical property detectors and utilisation property detectors. Basic physical properties can be inferred from the sensors on the robotic platform. Examples of properties we have experimented with include: texture (using Concept Activation Vectors [30] to determine distinctive textural properties of an object, e.g., stripey, smooth, etc.); colour (represented within the HSB colour space, using simple computer vision approaches to determine the dominant colour of the object); size (determined by a depth-aware camera); current explorations include shape parametrisation (e.g., following [6]). A simple data flow diagram (Fig. 3) shows the high-level implementation we have used. Utilisation properties require additional transformations to derive. Sources for these could include task recognition through computer vision techniques (e.g., [16] to determine which tasks are being carried out in the observed video stream (e.g., 'hammering' or 'drilling'); or newer work around the Conceptual Space framework proposing event representation using force and result vectors [22,43]. Currently, utilisation properties are extracted in the training phase using crowdsourced knowledge bases.</p>
<p>General Knowledge Extraction.</p>
<p>We use ConceptNet's (see § 2.2) API 5 to acquire crowdsourced values for the UsedFor property of various manufacturing artefacts (e.g., drill). Each response (e.g.,</p>
<p>Simulation Environment</p>
<p>Virtual Sensors Property Extractors Concept Algebra Interpretable Classification Figure 3: Data flow in the manufacturing use case. Currently, simulated environment is used in the proof-of-concept scenario. Virtual, visual-based sensors are used to capture objects in the environment. Captured images are fed into the property extractor modules (e.g., for colour, texture, etc.), each dedicated to extracting the value corresponding to the pertaining quality dimension (e.g., hue, stripy, etc.). It is worth mentioning that some quality domains, whose extraction is more challenging due to limitations of the simulated environment (e.g., size), are quantified using the simulated environment specification directly, which is a temporary fix in line with the fact the focus of our work is not as much on the property detectors and computer vision as on concept building and class inference proof of concept in the "back end". An instance described by its (standardised) property values is represented as a vector and its representativeness measures for candidate classes quantified using the µw-model.</p>
<p>'drilling holes in things') is accompanied by the weight, reflecting the source's reliability. Due to the nature of free-form submissions, messiness of the corresponding response cannot be avoided (e.g., 'drilling holes in things' and 'drill a hole in something' are separate entries). To fix that, in the first iteration we group all responses with similar semantics, such as the examples above, and run the softmax function across all weights of semantically different items to acquire the quantity that we use to ground the membership function 6 for this utilisation property. Concretely, we get
( _ _ _ _ ) = 0.9999997
which is a reasonable quantity.</p>
<p>Since the described procedure would be quite cumbersome and time-consuming to run for each utilisation property, a more thorough approach to automating utilisation knowledge extraction from the public knowledge bases was developed. It consults ConceptNet and WordNet programmatically, via their respective APIs, alongside the NLTK 7 package, does not require any manual intervention, and proceeds in the following steps:</p>
<p>(1) Find the appropriate WordNet synset (i.e., a meaning that can be represented by multiple synonymous lexemes) for the given object label (e.g., 'drill'). The condition is that it denotes an artefact noun. In case there are multiple such synsets, as in the case of 'hammer' (handheld tool and a gun part), heuristically choose one whose encompassed lexemes have the highest occurrence frequency in the corpus; (2) Get all synonym lexeme lemmas for that synset using Word-Net; (3) For each lexeme, extract edges from ConceptNet originating from WordNet (higher credibility than crowdsourced data) that have the lexeme as the start node and 'UsedFor' as the relation. End nodes denote utilisation properties; (a) If no such edge is found (as, e.g., for forklift), then consult ConceptNet's crowdsourced knowledge. Of all edges that have the start node and relation as specified, eliminate those whose weight is 1.0 or less (these can be considered unreliable or noise). In each end node of the remaining edges, find a verb using part-of-speech tagger, lemmatise it (e.g., change 'carrying' to 'carry'), and add it to the list of utilisations; (b) If that does not yield a meaningful utilisation property either (as it does not, e.g., for riveter), use a heuristic inspired by low inflection of English language: Stem the artifact name ('riveter' to 'rivet') and check whether it can be used as a verb ('hammer', 'drill' are both nouns and verbs). If it can, add it to the utilisation list; (4) For the extracted verb synsets, extract only those whose meaning falls under WordNet categories 'contact', 'change' or 'motion' (more can be added where necessary). Synsets characterised as, e.g., 'verb.cognitive' would be excluded; (5) Use the retrieved synsets to infer the value for each selected utilisation quality dimension (currently, 'drill', 'hammer', 'lift', 'rivet'). Do this by comparing these synsets to physical utilisation synsets for each quality dimension, again retrieved from knowledge bases. Where there is a non-empty intersection of synsets, set the value of the corresponding quality dimension to one; where there is not, set it to zero 8 . * * * The majority of the described pipeline allows for the implementation of state-of-the-art property detectors. Similarly, the simulation environment could be substituted with real-world cobots with realworld sensors maintaining the same interfaces throughout.</p>
<p>Quality dimensions' membership values, along with weights, make up the representativeness vector (Eq. 1), from which we measure the instance's typicality across concepts (Eq. 2) and determine for which one its representativeness is the highest (Eq. 3, visualised in Fig. 4). The representativeness score and associated property weights and typicality measures are directly human-comprehensible and constitute our interpretable classification approach, as an (at this point simplified) alternative to traditional computer visionbased object recognition approaches.</p>
<p>Situation in the XAI Ecosystem</p>
<p>Our knowledge representation-based inherent explainability approach may be situated in a wider XAI framework against roles of system users and previously proposed explanation characteristics. This way it may be compared against other XAI techniques and Figure 4: Simulated industrial use case environment. We start with the simple case with simulated idealised objects (upper left), such as 'Red Sphere', 'Wooden Cube', etc. The robot encounters these objects and extracts ground-truth property values available from the simulator (colour, shape, composition), used as the basis for learning the conceptual space. Having validated the data pipeline and knowledge representation modelling, we moved towards the more ecologically valid manufacturing simulated environment (upper middle), where quality dimensions include physical properties like size, texture, composition, and utilisation properties. Generally, every instance's quality dimension values are used as input for membership quantification per various concepts of interest. The spider charts (lower) show example membership values per quality dimensions across four artefact concepts for two example observations. Together with the quality domain weights (currently arbitrated, in the future quantified via empirical hypotheses, see § 2.3), these membership values make up the vector representation of the instance, as per Eq. 1. The bar chart (upper right) represents the typicality of the two example instances across the four observed concepts, calculated via Eq. 2.</p>
<p>represent a candidate for the right application context and provided requirements.</p>
<p>Considering development of explainable AI systems teaming with humans in manufacturing environments, the following roles [55] of agents interacting with the AI can be envisioned for such applications:</p>
<p>• Creator -Developer of the AI system using property decomposition explainability for sanity check and debugging, as well as guidance in hyperparameter selection (e.g., threshold for disputable classification, see § 2.3); • Operator and Executor (possibly the same agent) -Manufacturing worker provided with an understandable explanation of the classifier's rationale (see Fig. 4), ready to act upon the output (e.g., take and use the intended fetched tool) or flag the system's incorrect behaviour; • Decision-Subject -In a more advanced use case, a human agent teaming with an embodied AI agent on a physical task (e.g., processing a large component), able to understand the agent's rationale of a tool choice. Obviously, trustworthiness of the AI system is of essence for the Decision-Subject role; • Examiner -Prior to industrialisation and deployment, any AI to be used in applications involving human safety requirements needs to be certified, for which thorough sanity checks of the AI's learnt knowledge and operation rationale are crucial.</p>
<p>It should be noted that the expectations from users will likely evolve as interactions increase. For instance, when first working together, it is likely an operator will require more frequent and detailed explanations as they begin to build trust. As this trust develops, it is likely that either less detailed or frequent explanations will be required, and could even cause user frustration (if a system consistently explains a 'simple' action undertaken many times per shift). This longitudinal approach is a matter of future work as discussed in § 4.</p>
<p>Our explainability approach can be described across various axes of the explanation characterisation framework, such as one by [26, Table 1]. Although a rigorous empirical validation of the technique in operation is pending, some characteristics may already be ascertained, e.g.:</p>
<p>• Interpretability -High-level interpretability of classification output endowed with visualisation artefacts involving interpretable quality domains and understandable diagrams. As opposed to post-hoc explainability, employed knowledge modelling via decomposition into interpretable quality dimensions is an integral part of the system's design; • Local/global explainability -In the interpretable classification use case ( § 3.2) it is possible to explore the membership function and weights per quality dimensions and thus validate the categorisation rationale, which adds to global explainability; each instance classification is accompanied by local explainability visual (or textual) artefacts (e.g., Fig. 4); • Feature importance quantifiability and visualisation -Learnt knowledge includes empirically validated dimension weights, which provides a cognitively-motivated importance quantification; • Explanation by example -The representational space is partitioned around the prototypes (see § 3.2), which may in turn be used to elucidate a seemingly unlikely categorisation outcome; • Interactivity -A simple visual tool has been developed that allows for interactive probing of the classifier by manipulating quality dimension weights and membership functions of the model, as well as property values of an instance; • Counterfactual reasoning -Using the interactive tool (see above) it is possible to understand what the categorisation outcome would have been had the parameters or properties been different.</p>
<p>The authors [26] treat the 'generalisability' characteristic as versatility across types of applicable black-box models and as such pertains solely to post-hoc explainability techniques. However, when discussing versatility of utilisation, it is worth noting that the underlying Conceptual Space framework and the proposed typicality model and its parameter learning allow for flexibility of application domain, with an (admittedly strong) assumption of selecting an appropriate set of quality domains and dimensions and their apt modelling. Once these are selected for a given domain, the pertaining parameters are learnt from the combination of exploring in the world (i.e., from observed distributions of quality dimension values) and expert guidance (i.e., providing the labels as input to the concept space construction process).</p>
<p>Qualitative Validation</p>
<p>Empirical validation of interpretability and explainability are still emerging topics in the field of XAI. To try and obtain validation of the work undertaken, we have consulted with subject matter experts and potential users to assess and understand the quality of the of the proposed system.</p>
<p>As discussed in § 3.3, we identified a number of different user roles, e.g., Creator-Developer, Operator, etc., and presented them with relevant explanations and demonstrations of the system. User feedback was then taken into account to implement improvements. For instance, a number of different visualisations were evaluated for displaying 'typicality' of objects to users (e.g., spider / radar charts, bar charts, doughnut charts, polar charts, etc.), to determine which ones conveyed the most intelligible and useful information.</p>
<p>On top of that, semantics of the interpretable classification output was validated with domain experts, for example, the two-level information presentation. This presentation includes visualisation of the overall calculated representativeness of an instance across categories, which can in turn be 'zoomed-into' to explore typicality across utilised quality dimensions, as in the bar chart and spider charts from Fig. 4, respectively.</p>
<p>Future work will include a rigorous methodological framework for more objective evaluation (on top of subjective, reporting-based approaches) of explainability and interpretability metrics, building on previous domain-relevant and domain-independent work such as [26] and [58].</p>
<p>CONCLUSIONS AND FUTURE WORK</p>
<p>We pursue a knowledge modelling approach towards inherent (vs. post-hoc) explainability of (embodied) AI agents teaming with humans in industrial environments. Heterogeneous knowledge involves physical properties acquirable by equipped sensors and utilisation properties obtained from publicly available crowdsourced and expert-manufactured resources of general knowledge. The properties are consulted by a simple classification model drawing inspiration from Gärdenfors's Conceptual Space framework. The model is defined by the membership function in the context of fuzzy set theory (quantified frequentistically from experience with environment); and the weight of the property (based on empirically confirmed cognitive semantic hypotheses).</p>
<p>Highly specific industrial environments, such as the aerospace manufacturing one, involve rare and specialist objects, for which there typically do not exist image datasets of sufficient size to train data-hungry high-performant (convolutional) neural networks for object classification. Therefore, our move from computer vision object recognition to property decomposition-based interpretable classification is relevant for such applications characterised by data scarcity, thus necessitating frugal AI approaches. Furthermore, decomposing concepts into interpretable property components facilitates model validation and debugging by a developer, scrutinous examination by a certifier, behaviour understandability for a teaming operator, and rationale interpretability for a decision-subject.</p>
<p>Whilst the described use case is a rather limited and illustratory one, it does open a few avenues for future research in knowledge representation and inference modelling of (embodied) AI agents teaming with humans in manufacturing and other industrial and operational environments.</p>
<p>Humans are remarkably successful in learning concepts from scarce data [27], the mechanics and phenomenology of which is of high interest to cognitive science, cognitive neuroscience, computational neuroscience, and artificial intelligence [34]. One of the challenges is modelling acquisition of core concepts and intuitive theories (e.g., in physics and psychology) as well as generic causal structures [36], all supporting commonsense reasoning. Some promising generative models are based on Bayesian reasoning in the context of hierarchies and structures of hypotheses and associated inductive constraints [54]. An artificial agent able to acquire and manipulate core concepts arguably makes its behaviour more predictable, its rationales more interpretable, and it itself more trustworthy.</p>
<p>In the simplified examples used we represent quality dimensions via orthogonal basis vectors spanning the conceptual space. However, in reality, property correlations are an important component of concept description [45], stemming from generic, causally originating, core concept structures, and empirically demonstrated by humans' effortless acquisition of systematic correlations [8,29,31,42], especially for natural kinds (e.g., many times the colour of fruit predicts its taste and ripeness). It is a matter of further exploration to address property interdependencies, possibly taking into account psychological essentialism, according to which perceptible properties are surface manifestation of entities' true nature [18,33].</p>
<p>Moreover, for artefacts, which can theoretically take arbitrary values of surface properties, it makes sense to focus on the objects' purported utilisation capability (affordances) stemming from their physical properties, like shape or composition (e.g., a large handheld-sized object with a hard flat metal head is likely to be used to hit nails). Attention-driven visual classification and visualisation techniques may be promising approaches for utilisation inference.</p>
<p>Whilst the current use case deals only with physical objects, newer work around the Conceptual Space framework proposes event representation using force and result vectors [22,43]. This work justifies further exploration as it may prove particularly useful on the shop floor environments. At the same time, one shall need to pay attention to interpretability of pertaining quality domains in this respect.</p>
<p>More recent work in the Conceptual Space theory is focussed on modelling events via force and result vectors [22,43] using the same underlying representational structure as in the described case for (physical) objects. This research is particularly relevant for industrial domains involving human-AI teaming, e.g., for gauging typicality of task execution. A recognised challenge is interpretability of force patterns constituting quality dimensions, which the event-based extension of the Conceptual Space framework is based on.</p>
<p>Cognitive modelling of operators using cognitive architectures [3,35] is a useful building block for modelling AI cognitive assistants aware of task representations, capable of anomalous behaviour detection, and cognitive load estimation as an input to autonomy engagement. A subsymbolic yet interpretable knowledge representation framework would arguably make a promising step towards modelling the declarative module ((e.g., [39,40]), which is yet to be demonstrated in industrial environments involving human-AI teaming.</p>
<p>Reported parallels between Conceptual Spaces' structural principles and empirical findings on conceptual knowledge neural implementation (see §2.1) invite researchers to explore the appropriateness of such an immanently multidimensional representation for mapping onto what appears to be two-dimensional hexadirectional grid-based encoding system on the neural substrate level [7]. Whilst running human studies (e.g., functional magnetic resonance imaging (fMRI) experiments) for hypothesis testing around candidate mapping mechanisms would not be feasible in industrial environments where the authors are affiliated, they shall make every effort to liaise with academic partners who may be better positioned to execute such studies, on top of continually following advances in the area.</p>
<p>Explainability capabilities of an AI system should be able to gauge the human user's expectations stemming from their experience with the task at hand and the teaming AI agent. Hence a longitudinal approach in context-aware trust and interpretability measurement, possibly involving user's overt or implicit feedback, and consequent XAI system's adaptability should be considered in the future.</p>
<p>Finally, it is important to recognise that the path towards a trustworthy AI is multi-faceted and explainability represents one pillar, albeit arguably the one most human user-focussed. The other pillars that are essential for responsible and ethical AI design are robustness and learning assurance, and fairness and nondiscrimination [13,14,28]). Obviously, AI trustworthiness should be addressed in an interdisciplinary manner, one difficult without a strong collaboration of academia and industry.</p>
<p>Figure 1 :
1Illustration of the membership functions for a continuous quality dimension across three example concepts (prototypical instances are assumed to bear prototypical value of the property) and of a nominal quality dimension for the concept.</p>
<p>Figure 2 :
2The artificial agent wants to classify a new object (upper right image) and extracts its physical and utilisation properties. The table (below) illustrates the and values for the 'Drill' concept. Although the new object is physically similar to the learnt 'Drill' prototype (upper left image shows a typical instance of 'Drill'), it will not be classified as one due to incompatibility of the utilisation property bearing the highest weight.
Conceptual Spaces also allow for discerning between similarity (e.g., car and van) and relatedness (e.g., car and driver), which is one of the challenges of distributional semantics approach.
http://api.conceptnet.io (accessed on 3 October 2022)
Somewhat counter-intuitively, what is called 'weight' in the ConceptNet system is semantically closer to the membership function of the w-model than the weight . See § 2.3 for details. 7 https://www.nltk.org/ (accessed on 4 October 2022)
Obviously, the proposed heuristics only allow for binary values of the UsedFor property. While the current approach served our purpose well, it is a matter of future work to infer continuous values.</p>
<p>Causal status effect in children's categorization. Susan A Woo-Kyoung Ahn, Jennifer A Gelman, Jill Amsterlaw, Charles W Hohenstein, Kalish, Cognition. 76Woo-kyoung Ahn, Susan A Gelman, Jennifer A Amsterlaw, Jill Hohenstein, and Charles W Kalish. 2000. Causal status effect in children's categorization. Cognition 76, 2 (2000), B35-B43.</p>
<p>Causal status as a determinant of feature centrality. Nancy S Woo-Kyoung Ahn, Mary E Kim, Martin J Lassaline, Dennis, Cognitive Psychology. 41Woo-kyoung Ahn, Nancy S Kim, Mary E Lassaline, and Martin J Dennis. 2000. Causal status as a determinant of feature centrality. Cognitive Psychology 41, 4 (2000), 361-416.</p>
<p>An integrated theory of the mind. Daniel John R Anderson, Bothell, D Michael, Scott Byrne, Christian Douglass, Yulin Lebiere, Qin, Psychological review. 1111036John R Anderson, Daniel Bothell, Michael D Byrne, Scott Douglass, Christian Lebiere, and Yulin Qin. 2004. An integrated theory of the mind. Psychological review 111, 4 (2004), 1036.</p>
<p>Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. Information fusion. 58Alejandro Barredo Arrieta, Natalia Díaz-Rodríguez, Javier Del Ser, Adrien Ben- netot, Siham Tabik, Alberto Barbado, Salvador García, Sergio Gil-López, Daniel Molina, Richard Benjamins, et al. 2020. Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI. In- formation fusion 58 (2020), 82-115.</p>
<p>A thorough formalization of conceptual spaces. Lucas Bechberger, Kai-Uwe Kühnberger, Joint German/Austrian Conference on Artificial Intelligence (Künstliche Intelligenz. SpringerLucas Bechberger and Kai-Uwe Kühnberger. 2017. A thorough formalization of conceptual spaces. In Joint German/Austrian Conference on Artificial Intelligence (Künstliche Intelligenz). Springer, Springer, 58-71.</p>
<p>Representing Complex Shapes with Conceptual Spaces. Lucas Bechberger, Margit Scheibel, Second International Workshop'Concepts in Action: Representation, Learning, and Application. CARLALucas Bechberger and Margit Scheibel. 2020. Representing Complex Shapes with Conceptual Spaces. In Second International Workshop'Concepts in Action: Representation, Learning, and Application'(CARLA 2020).</p>
<p>Navigating cognition: Spatial codes for human thinking. L S Jacob, Peter Bellmund, Gärdenfors, I Edvard, Christian F Moser, Doeller, Science. 3626766Jacob LS Bellmund, Peter Gärdenfors, Edvard I Moser, and Christian F Doeller. 2018. Navigating cognition: Spatial codes for human thinking. Science 362, 6415 (2018), eaat6766.</p>
<p>Unsupervised concept learning and value systematicitiy: A complex whole aids learning the parts. Dorrit Billman, James Knutson, Journal of Experimental Psychology: Learning, Memory, and Cognition. 22458Dorrit Billman and James Knutson. 1996. Unsupervised concept learning and value systematicitiy: A complex whole aids learning the parts. Journal of Experi- mental Psychology: Learning, Memory, and Cognition 22, 2 (1996), 458.</p>
<p>Explainable neural networks that simulate reasoning. J Paul, Milo M Blazek, Lin, Nature Computational Science. 1Paul J Blazek and Milo M Lin. 2021. Explainable neural networks that simulate reasoning. Nature Computational Science 1, 9 (2021), 607-618.</p>
<p>Organizing conceptual knowledge in humans with a gridlike code. Alexandra O Constantinescu, Jill X O&apos;reilly, Timothy Ej Behrens, Science. 352Alexandra O Constantinescu, Jill X O'Reilly, and Timothy EJ Behrens. 2016. Organizing conceptual knowledge in humans with a gridlike code. Science 352, 6292 (2016), 1464-1468.</p>
<p>Webots: robot simulator. Cyberbotics, Cyberbotics. 2022. Webots: robot simulator. https://cyberbotics.com/. Accessed: 2022-02-18.</p>
<p>Towards a rigorous science of interpretable machine learning. Finale Doshi, - Velez, Been Kim, arXiv:1702.08608arXiv preprintFinale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of inter- pretable machine learning. arXiv preprint arXiv:1702.08608 (2017).</p>
<p>EASA Artificial Intelligence Roadmap 1.0 A human-centric approach to AI in aviation. Easa, EASA. 2020. EASA Artificial Intelligence Roadmap 1.0 A human-centric approach to AI in aviation. EASA. https://www.easa.europa.eu/document-library/general- publications/easa-artificial-intelligence-roadmap-10</p>
<p>EASA Concept Paper: First usable guidance for Level 1 machine learning applications. Easa, EASA. 2021. EASA Concept Paper: First usable guidance for Level 1 machine learning applications. EASA. https://www.easa.europa.eu/sites/default/files/dfu/ easa_concept_paper_first_usable_guidance_for_level_1_machine_learning_ applications_-_proposed_issue_01_1.pdf</p>
<p>. Epfl, EPFL. 2018. e-puck education robot. http://www.e-puck.org/. Accessed: 2022- 02-18.</p>
<p>Slowfast networks for video recognition. Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, Kaiming He, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. 2019. Slow- fast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision. 6202-6211.</p>
<p>An Aggressive Robin in the Backyard: Formal Quantification of Prototypicality Level within the Frame of the Prototype Semantic Theory of Cognitive Linguistics. Suvremena lingvistika (Contemporary Linguistics). Vedran Galetić, 3771Vedran Galetić. 2011. An Aggressive Robin in the Backyard: Formal Quantification of Prototypicality Level within the Frame of the Prototype Semantic Theory of Cognitive Linguistics. Suvremena lingvistika (Contemporary Linguistics) 37, 71 (2011).</p>
<p>Towards the cognitive plausibility of conceptual space models. Vedran Galetić, 41Suvremena lingvistika (Contemporary LinguisticsVedran Galetić. 2015. Towards the cognitive plausibility of conceptual space models. Suvremena lingvistika (Contemporary Linguistics) 41, 80 (2015), 71-85.</p>
<p>Formalisation and quantification of a cognitively motivated concecptual space model based on the prototype theory. Vedran Galetić, Ph.D. Dissertation. University of ZagrebVedran Galetić. 2016. Formalisation and quantification of a cognitively moti- vated concecptual space model based on the prototype theory. Ph.D. Dissertation. University of Zagreb.</p>
<p>Conceptual spaces: The geometry of thought. Peter Gardenfors, MIT pressPeter Gardenfors. 2004. Conceptual spaces: The geometry of thought. MIT press.</p>
<p>The geometry of meaning: Semantics based on conceptual spaces. Peter Gardenfors, MIT pressPeter Gardenfors. 2014. The geometry of meaning: Semantics based on conceptual spaces. MIT press.</p>
<p>An Epigenetic Approach to Semantic Categories. Peter Gärdenfors, IEEE Transactions on Cognitive and Developmental Systems. 12Peter Gärdenfors. 2018. An Epigenetic Approach to Semantic Categories. IEEE Transactions on Cognitive and Developmental Systems 12, 2 (2018), 139-147.</p>
<p>Reasoning about categories in conceptual spaces. Peter Gärdenfors, Mary-Anne Williams, IJCAI. Citeseer. Peter Gärdenfors and Mary-Anne Williams. 2001. Reasoning about categories in conceptual spaces. In IJCAI. Citeseer, 385-392.</p>
<p>DARPA's explainable artificial intelligence (XAI) program. David Gunning, David Aha, AI magazine. 40David Gunning and David Aha. 2019. DARPA's explainable artificial intelligence (XAI) program. AI magazine 40, 2 (2019), 44-58.</p>
<p>Microstructure of a spatial map in the entorhinal cortex. Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, Edvard I Moser, Nature. 436Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I Moser. 2005. Microstructure of a spatial map in the entorhinal cortex. Nature 436, 7052 (2005), 801-806.</p>
<p>A systematic method to understand requirements for explainable AI (XAI) systems. Mark Hall, Daniel Harborne, Richard Tomsett, Vedran Galetic, Santiago Quintana-Amate, Alistair Nottle, Alun Preece, Proceedings of the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2019). the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2019)Macau, China11Mark Hall, Daniel Harborne, Richard Tomsett, Vedran Galetic, Santiago Quintana- Amate, Alistair Nottle, and Alun Preece. 2019. A systematic method to understand requirements for explainable AI (XAI) systems. In Proceedings of the IJCAI Work- shop on eXplainable Artificial Intelligence (XAI 2019), Macau, China, Vol. 11.</p>
<p>. Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, Matthew Botvinick, Neuroscience-inspired artificial intelligence. Neuron. 95Demis Hassabis, Dharshan Kumaran, Christopher Summerfield, and Matthew Botvinick. 2017. Neuroscience-inspired artificial intelligence. Neuron 95, 2 (2017), 245-258.</p>
<p>Ethics guidelines for Trustworthy AI. Hlgeai, HLGEAI. 2019. Ethics guidelines for Trustworthy AI. European Commission. https: //digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai</p>
<p>How children know the relevant properties for generalizing object names. S Susan, Linda B Jones, Smith, Developmental Science. 5Susan S Jones and Linda B Smith. 2002. How children know the relevant properties for generalizing object names. Developmental Science 5, 2 (2002), 219-232.</p>
<p>Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, PMLRInternational conference on machine learning. Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda Viegas, et al. 2018. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International conference on machine learning. PMLR, 2668-2677.</p>
<p>What's behind different kinds of kinds: effects of statistical density on learning and representation of categories. Heidi Kloos, M Vladimir, Sloutsky, Journal of Experimental Psychology: General. 13752Heidi Kloos and Vladimir M Sloutsky. 2008. What's behind different kinds of kinds: effects of statistical density on learning and representation of categories. Journal of Experimental Psychology: General 137, 1 (2008), 52.</p>
<p>The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Matthieu Komorowski, A Leo, Omar Celi, Badawi, C Anthony, Gordon, Nature medicine. 24Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. 2018. The artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care. Nature medicine 24, 11 (2018), 1716-1720.</p>
<p>Inductive inference and its natural ground: An essay in naturalistic epistemology. Hilary Kornblith, Mit PressHilary Kornblith. 1995. Inductive inference and its natural ground: An essay in naturalistic epistemology. Mit Press.</p>
<p>Cognitive computational neuroscience. Nikolaus Kriegeskorte, Pamela K Douglas, Nature neuroscience. 21Nikolaus Kriegeskorte and Pamela K Douglas. 2018. Cognitive computational neuroscience. Nature neuroscience 21, 9 (2018), 1148-1160.</p>
<p>A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. E John, Christian Laird, Paul S Lebiere, Rosenbloom, 38Ai MagazineJohn E Laird, Christian Lebiere, and Paul S Rosenbloom. 2017. A standard model of the mind: Toward a common computational framework across artificial intelligence, cognitive science, neuroscience, and robotics. Ai Magazine 38, 4 (2017), 13-26.</p>
<p>Building machines that learn and think like people. Brenden M Lake, Joshua B Tomer D Ullman, Samuel J Tenenbaum, Gershman, Behavioral and brain sciences. 40Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and think like people. Behavioral and brain sciences 40 (2017).</p>
<p>Women, fire, and dangerous things: What categories reveal about the mind. George Lakoff, University of Chicago pressGeorge Lakoff. 2008. Women, fire, and dangerous things: What categories reveal about the mind. University of Chicago press.</p>
<p>Building large knowledge-based systems; representation and inference in the Cyc project. B Douglas, Lenat, V Ramanathan, Guha, Addison-Wesley Longman Publishing Co., IncDouglas B Lenat and Ramanathan V Guha. 1989. Building large knowledge-based systems; representation and inference in the Cyc project. Addison-Wesley Longman Publishing Co., Inc.</p>
<p>Conceptual spaces for cognitive architectures: A lingua franca for different levels of representation. Antonio Lieto, Antonio Chella, Marcello Frixione, Biologically inspired cognitive architectures. 19Antonio Lieto, Antonio Chella, and Marcello Frixione. 2017. Conceptual spaces for cognitive architectures: A lingua franca for different levels of representation. Biologically inspired cognitive architectures 19 (2017), 1-9.</p>
<p>Dual PECCS: a cognitive system for conceptual representation and categorization. Antonio Lieto, Daniele P Radicioni, Valentina Rho, Journal of Experimental &amp; Theoretical Artificial Intelligence. 29Antonio Lieto, Daniele P Radicioni, and Valentina Rho. 2017. Dual PECCS: a cognitive system for conceptual representation and categorization. Journal of Experimental &amp; Theoretical Artificial Intelligence 29, 2 (2017), 433-452.</p>
<p>The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Zachary C Lipton, Queue. 16Zachary C Lipton. 2018. The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue 16, 3 (2018), 31-57.</p>
<p>The parallel distributed processing approach to semantic cognition. L James, Timothy T Mcclelland, Rogers, Nature reviews neuroscience. 4James L McClelland and Timothy T Rogers. 2003. The parallel distributed pro- cessing approach to semantic cognition. Nature reviews neuroscience 4, 4 (2003), 310-322.</p>
<p>Construals of meaning: The role of attention in robotic language production. Anne-Laure Mealier, Grégoire Pointeau, Peter Gärdenfors, Peter Ford Dominey, Interaction Studies. 17Anne-Laure Mealier, Grégoire Pointeau, Peter Gärdenfors, and Peter Ford Dominey. 2016. Construals of meaning: The role of attention in robotic lan- guage production. Interaction Studies 17, 1 (2016), 41-69.</p>
<p>WordNet: a lexical database for English. A George, Miller, Commun. ACM. 38George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995), 39-41.</p>
<p>Formalizing conceptual spaces. Martin Raubal, Formal ontology in information systems, proceedings of the third international conference (FOIS 2004). 114Martin Raubal. 2004. Formalizing conceptual spaces. In Formal ontology in information systems, proceedings of the third international conference (FOIS 2004), Vol. 114. 153-164.</p>
<p>Cognition and Categorization. L. Erlbaum Associates, Chapter Principles of categorization. Eleanor Rosch and Barbara B. LloydEleanor Rosch and Barbara B. Lloyd (Eds.). 1978. Cognition and Categorization. L. Erlbaum Associates, Chapter Principles of categorization, 27-48.</p>
<p>Family resemblances: Studies in the internal structure of categories. Eleanor Rosch, Carolyn B Mervis, Cognitive psychology. 7Eleanor Rosch and Carolyn B Mervis. 1975. Family resemblances: Studies in the internal structure of categories. Cognitive psychology 7, 4 (1975), 573-605.</p>
<p>Basic objects in natural categories. Eleanor Rosch, Carolyn B Mervis, Wayne D Gray, M David, Penny Johnson, Boyes-Braem, Cognitive psychology. 8Eleanor Rosch, Carolyn B Mervis, Wayne D Gray, David M Johnson, and Penny Boyes-Braem. 1976. Basic objects in natural categories. Cognitive psychology 8, 3 (1976), 382-439.</p>
<p>Feature centrality and conceptual coherence. A Steven, Sloman, C Bradley, Woo-Kyoung Love, Ahn, Cognitive Science. 22Steven A Sloman, Bradley C Love, and Woo-Kyoung Ahn. 1998. Feature centrality and conceptual coherence. Cognitive Science 22, 2 (1998), 189-228.</p>
<p>Conceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Thirty-first AAAI conference on artificial intelligence. Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of general knowledge. In Thirty-first AAAI conference on artificial intelligence.</p>
<p>Going deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. 2015. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1-9.</p>
<p>Gramatika i predočavanje. Elżbieta Tabakowska, Filozofski Fakultet Sveučilišta u Zagrebu-FF Pressuvod u kognitivnu lingvistikuElżbieta Tabakowska. 2005. Gramatika i predočavanje: uvod u kognitivnu lingvis- tiku. Filozofski Fakultet Sveučilišta u Zagrebu-FF Press.</p>
<p>Theory-based Bayesian models of inductive learning and reasoning. B Joshua, Tenenbaum, L Thomas, Charles Griffiths, Kemp, Trends in cognitive sciences. 107Joshua B Tenenbaum, Thomas L Griffiths, and Charles Kemp. 2006. Theory-based Bayesian models of inductive learning and reasoning. Trends in cognitive sciences 10, 7 (2006), 309-318.</p>
<p>How to grow a mind: Statistics, structure, and abstraction. B Joshua, Charles Tenenbaum, Kemp, L Thomas, Noah D Griffiths, Goodman, science. 331Joshua B Tenenbaum, Charles Kemp, Thomas L Griffiths, and Noah D Goodman. 2011. How to grow a mind: Statistics, structure, and abstraction. science 331, 6022 (2011), 1279-1285.</p>
<p>Interpretable to whom? A role-based model for analyzing interpretable machine learning systems. Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, Supriyo Chakraborty, arXiv:1806.07552arXiv preprintRichard Tomsett, Dave Braines, Dan Harborne, Alun Preece, and Supriyo Chakraborty. 2018. Interpretable to whom? A role-based model for analyzing interpretable machine learning systems. arXiv preprint arXiv:1806.07552 (2018).</p>
<p>Fuzzy sets. In Fuzzy sets, fuzzy logic, and fuzzy systems: selected papers by Lotfi A Zadeh. A Lotfi, Zadeh, World ScientificLotfi A Zadeh. 1996. Fuzzy sets. In Fuzzy sets, fuzzy logic, and fuzzy systems: selected papers by Lotfi A Zadeh. World Scientific, 394-432.</p>
<p>Applications of conceptual spaces. Frank Zenker, Peter Gärdenfors, 25Frank Zenker and Peter Gärdenfors. 2015. Applications of conceptual spaces. Cited on 25 (2015).</p>
<p>Towards an Integrated Evaluation Framework for XAI: An Experimental Study. Qiyuan Zhang, Mark Hall, Mark Johansen, Vedran Galetic, Jacques Grange, Santiago Quintana-Amate, Alistair Nottle, Dylan M Jones, Phillip L Morgan, 10.1016/j.procs.2022.09.450Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 26th International Conference KES2022. 207Qiyuan Zhang, Mark Hall, Mark Johansen, Vedran Galetic, Jacques Grange, Santiago Quintana-Amate, Alistair Nottle, Dylan M Jones, and Phillip L Morgan. 2022. Towards an Integrated Evaluation Framework for XAI: An Experimental Study. Procedia Computer Science 207 (2022), 3884-3893. https://doi.org/10.1016/ j.procs.2022.09.450 Knowledge-Based and Intelligent Information &amp; Engineering Systems: Proceedings of the 26th International Conference KES2022.</p>            </div>
        </div>

    </div>
</body>
</html>
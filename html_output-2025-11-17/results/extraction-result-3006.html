<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3006 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3006</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3006</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-73.html">extraction-schema-73</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <p><strong>Paper ID:</strong> paper-263622222</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2310.02989v1.pdf" target="_blank">xVal: A Continuous Number Encoding for Large Language Models</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3006.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3006.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>XVAL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>XVAL: A continuous number encoding for large language models</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A number encoding that represents any real number with a single [NUM] token whose learned embedding vector is multiplied by the scalar numeric value, producing an embedding continuous in the numeric input and enabling an MSE-trained scalar number head at output for end-to-end continuity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 variant (experiment model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer architecture based on GPT-2 used in experiments (6 transformer blocks, 6 heads, head width 128, embedding width 768 ≈ 43.5M parameters); absolute positional encodings; layernorms and residuals as described in paper; trained with MLM or AR depending on task.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>multi-digit multiplication (3-, 4-, 5-digit), multi-operand arithmetic expressions (binary trees with 2–4 operands using +, -, *), and other numerical inference tasks used for comparison (temperature forecasting, planetary parameter inference).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Multiplicative continuous encoding: a single learned [NUM] embedding vector is scaled by the real numeric value, combined with positional encodings and layer-norm, producing token embeddings that vary continuously with numeric magnitude; a separate scalar number head (MSE loss) produces continuous numeric outputs for [NUM] tokens, avoiding discrete argmax over textual tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Empirical improvements in interpolation and out-of-distribution (OOD) generalization across several tasks: high R^2 on arithmetic (Table 2 and Table 3), superior OOD interpolation in planetary timestep and semi-major axis prediction (Figures 5), best temperature-forecasting MSE/runtime tradeoff (Table 4). Figure 2 shows projected embedding after layer-norm follows analytic scaling; OOD prediction curves are smooth (unlike text-based encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Limited dynamic range due to layer-norm normalization (numbers normalized into roughly [−5,5] in experiments). XVAL performed poorly on predicting planetary mass in one task (suspected multimodal target distribution), producing worse MSE than some text-based encodings for that target. Very large/small numbers can saturate or be negligible respectively. These limitations are discussed and empirically observed.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural and training design: (1) multiplicative [NUM] embedding + scalar number head trained with MSE; (2) preprocessing normalization of numeric range to [−5,5]; (3) layer-norm placement/choices; (4) ablations on masking probability, weight decay, biases in number-head and trunk.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Made the model end-to-end continuous in numeric inputs and improved interpolation/OOD performance; number-head (MSE) allowed continuous numeric output instead of discrete classification; layer-norm preserves direction of [NUM] vector while rescaling magnitude (explained analytically and validated empirically). Ablations (Table 9) show turning off some layer-norms or altering hyperparameters degrades performance; number-head bias toggle had little effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Arithmetic: Table 2 reports adjusted R^2 for multi-digit multiplication: XVAL R^2 = 0.9986 (3-digit), 0.9975 (4-digit), 0.9958 (5-digit). Multi-operand binary-tree evaluation (Table 3): XVAL R^2 = 0.99998 (2 operands), 0.99994 (3 operands), 0.99998 (4 operands). Temperature forecasting (Table 4): MSE = 1.75 (Equal Samples run) with runtime 19h on 4 H100s (comparable/better than alternatives). OOD interpolation plots (Figure 5) show continuous predictions between training values. Note: exact planetary-task numeric table entries for some metrics are reported in paper tables/figures.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>1) Dynamic-range saturation: very large or very small numbers become problematic due to layer-norm rescaling; 2) Poor fit to multimodal output distributions (e.g., planetary mass), where categorical/multi-modal predictions from token-based models may be advantageous; 3) occasional prediction of non-numeric tokens (rare, decreases with training); 4) reliance on preprocessing normalization of numeric range.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Paper cites prior work showing even very large LLMs struggle on multi-digit multiplication (Dziri et al. reference: GPT-4 zero-shot accuracies 59% for 3-digit, 4% for 4-digit, 0% for 5-digit as reported by Dziri et al.), highlighting that symbolic calculators/algorithms remain more reliable for exact arithmetic; XVAL improves LM interpolation but is not claimed to match symbolic arithmetic algorithms.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3006.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3006.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Number head (MSE scalar)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scalar number head trained with mean-squared error</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A dedicated regression head that outputs a scalar numeric value for each [NUM] token and is trained with MSE loss, used in tandem with the token classification head to recover numeric values at output continuously.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 variant (experiment model) with dual heads</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same GPT-2-based transformer trunk; token (vocabulary) head plus a separate MLP number-head (one hidden layer width equal to embedding dim) that outputs scalar numeric values for [NUM] tokens; number-head sometimes includes bias while trunk does not.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used for all numeric prediction tasks where numbers are masked: arithmetic expression evaluation, temperature forecasting (masked future timesteps), planetary parameter inference.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Regresses numeric values directly rather than predicting them via token classification; combined with XVAL it yields continuous mapping from input numerical values to output numbers; avoids argmax induced discontinuities of classification-based numeric outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Improved interpolation/OOD performance when models use the number-head with XVAL compared to text-based classification outputs; ablations show promoting both heads to MLPs (including number-head) helps separate processing of token and numeric predictions and aids performance.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Number-head predicts a single scalar and struggles when the target distribution is multimodal or highly uncertain (planetary mass case). Paper proposes future extension to mixture-of-Gaussians outputs to capture multimodality.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural addition (new output head) and loss change (MSE instead of cross-entropy for numbers); optional bias toggles on number-head.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Enabled continuous numeric output and better interpolation; reduced discrete argmax discontinuity for numbers; slight architectural choices (bias on/off) had limited impact per ablation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Improvements shown indirectly via higher R^2 and better OOD interpolation in XVAL experiments (see XVAL entry). Percentage of unparsable non-numeric token outputs reported as <0.01% for most encodings; where present percentages shown in Table 5.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Single-scalar output cannot capture multimodal target distributions leading to worse MSE when targets are multimodal; occasional non-numeric token predictions happen if token head outputs non-[NUM] token (rare).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Direct regression differs from symbolic algorithms that produce exact discrete outputs; better matched to continuous scientific prediction tasks than exact symbolic arithmetic.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3006.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3006.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Text-based numeric encodings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>P10, P1000, B1999, FP15 and other tokenized number encodings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Tokenization schemes that encode numbers as sequences of textual tokens (digits, signs, exponents, prototype numerals, or learned BPE tokens); vary in tokens-per-number and vocabulary size trade-offs and are trained with standard cross-entropy token classification.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 variant (experiment model) using alternative numeric encodings</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same transformer trunk; numbers represented as multiple textual tokens per chosen encoding (e.g., P10: ±, digit, E±d; P1000: ±, ddd, E±d; B1999: learned BPE; FP15: single-token float prototypes). Encoding differences change input length, vocabulary size, and embedding learned for numeric tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Used on same arithmetic tasks: multi-digit multiplication, multi-operand expressions, temperature forecasting, planetary parameter inference.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Arithmetic performed via learned associations over discrete token embeddings and token-level sequence modelling; embeddings for numeric tokens can form continuous-looking manifolds but the discrete output argmax and cross-entropy training do not enforce numeric-distance-aware losses.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>PCA visualizations (Figure 9) show structure in learned embeddings (rotary/curve-like). Logit traces (Figure 6) for P1000 exhibit discontinuous, jagged predictions OOD, indicating that token logits do not vary smoothly with numeric value. Text-based models exploit spurious correlations (e.g., token-length correlated with numeric magnitude) as shown in Appendix B.3 and Figure 8.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Despite forming continuous-looking embeddings in some cases, the argmax stage and cross-entropy loss can produce discontinuous numeric outputs OOD. Empirically, text-based encodings often fail to interpolate to unseen numeric values (Figure 5).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Different encoding choices (P10, P1000, B1999, FP15) and preprocessing (significant figures, dropping leading zeros); use of MLM vs AR for non-causal JSON data; tokenizers with different vocabularies (including learned BPE).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Encoding affects trade-offs: FP15 (single-token per number) gives best in-distribution performance but poor interpolation and high embedding cost; P10 (vocabulary-sparse) gives best interpolation but poor in-distribution performance and long sequences; BPE variable-length encodings lead to spurious correlations where encoding length is exploited. These effects are shown in Tables 2–4 and Figures 3–5.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Arithmetic (Table 2 & 3): e.g., P1000 R^2 on multiplication: 0.9989 (3-digit), 0.6071 (4-digit), 0.9439 (5-digit) [note: table contains multiple encodings—see paper for full matrix]. FP15 had strong in-distribution numbers in many tasks but worse OOD. Temperature forecasting (Table 4): FP15 Equal Samples MSE=2.14 runtime 19h; P10 and P1000 had larger runtime/tokenization costs. Exact values for each encoding per task are in the paper tables.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>1) Discontinuous OOD predictions due to argmax over logits and discrete embeddings; 2) Exploitation of spurious correlations, e.g., encoding length correlating with numeric ranges leading to systematic mispredictions (Appendix B.3, Figure 8); 3) Large token sequences for some encodings (P10, P1000) hamper modeling of long-range interactions and increase compute cost; 4) Mislearning joint distributions (e.g., temperature dataset confusion due to common digit/exponent co-occurrence).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>These encodings are closer to symbolic tokenization but tend to produce brittle behavior compared to exact symbolic algorithms; do not produce continuous numeric outputs unless additional mechanisms are used.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3006.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3006.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Learned numeric embeddings / internal representations</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Learned embedding geometries and logits behavior for numeric tokens</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Observed internal representations include continuous-looking manifolds (curves, rotary structures) for digit/mantissa/exponent token embeddings and discontinuous/highly non-smooth logits for text-based encodings, while XVAL produces embeddings whose projection onto the [NUM] direction scales approximately with numeric magnitude after layer-norm.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 variant (experiment model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Internal analyses performed on trained transformer models using different numeric encodings; visualizations include PCA of mantissa token embeddings (Figure 9), projection of layer-normed [NUM] embedding (Figure 2), and logit-trace plots (Figure 6).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Analyses performed in context of arithmetic and numeric inference tasks (multiplication, planetary/temperature prediction).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>LMs form structured internal embedding geometries for numeric tokens (e.g., curves and rotary-like patterns) that can encode relative numeric information via inner products; however, discrete decoding and training objectives (cross-entropy) cause final outputs to be discontinuous even if embeddings appear smooth.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Figure 9 shows 2D PCA projections demonstrating curve/rotary structures in learned mantissa embeddings; Figure 2 empirically validates analytic relation of layer-normed XVAL encoding; Figure 6 shows jagged/highly variable logits for P1000 indicating lack of smooth numeric interpolation at output.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Smooth-looking embeddings do not guarantee smooth numeric outputs due to argmax and cross-entropy training; discontinuities in logits (Figure 6) demonstrate that internal smoothness can be destroyed before output decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Visualization/probing (PCA, projection), logit-trace analysis; ablations on layer-norm placement and MLP layer-norm toggles to see effect on numeric embedding normalization.</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Revealed that layer-norm near input helps normalize numeric embeddings and preserve XVAL direction; turning off some layer-norms harms performance (Table 9). Probing confirmed that discrete decoding is a major source of OOD discontinuity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Qualitative/internal metrics: PCA plots, projected embedding-vs-value curves (Figure 2), logit traces showing non-smoothness (Figure 6). These visual pieces of evidence accompany task performance metrics in other entries.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Apparent continuity of embeddings can be misleading: models still produce discontinuous outputs; embeddings spread more for datasets where numeric sensitivity is low (planet data) vs. arithmetic tasks where embeddings form tighter curves.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>Internal learned geometries are not equivalent to algorithmic symbolic representations; they can encode numeric relations but do not provide exact arithmetic algorithms akin to symbolic computation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3006.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3006.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Architectural & training interventions (ablations)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Layer-norm placement, masking probability, weight decay, biases, and preprocessing interventions</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Ablations and interventions explored include placement of layer-norm, masking probability in MLM, weight decay, inclusion of biases in transformer modules and number-head, and numeric preprocessing (normalization to limited range); these affect numeric performance and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 variant (experiment model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same transformer backbone; ablation experiments used shorter runs (100k iterations) across several hyperparameter settings and learning rates and reported validation-loss changes (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Evaluated on temperature forecasting validation loss primarily, with implications for other numeric tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>These interventions modulate how numeric information is normalized, represented, and learned (e.g., layer-norm prior to first MLP normalizes multiplicative numeric embedding; mask rate changes task difficulty and generalization; weight decay influences regularization/bias-variance).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Table 9: Turning off certain layer-norms significantly degraded performance; increasing masking probability to 30% sometimes improved on this dataset; increasing weight decay led to largest improvement (dataset-dependent); bias inclusion in transformer modules improved performance at cost of variability; number-head bias toggle had little effect.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>Some effects are dataset dependent and do not generalize uniformly (e.g., optimal masking probability differs across datasets); changing min-LR/LR ratio had limited effect in short runs.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Architectural ablations and hyperparameter sweeps: layer-norm toggles, masking probability (10%, 20%, 30%), weight decay variations (0.0001, 0.1, 1), inclusion/exclusion of biases, learning-rate schedules and warmups, numeric preprocessing (normalization to range).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Changing these settings had measurable effects on validation loss and numeric performance: removing first-layer norm had small effect for some datasets but removing MLP layer-norm worsened performance significantly; larger weight decay improved performance in this corpus; higher masking sometimes helped; number-head bias off had negligible effect.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Ablation validation losses reported in Table 9 (e.g., baseline Normal config val loss ≈ (6.8 ± 0.2) × 10^−3; MLP Layer Norm = False increased loss to (9.0 ± 0.1) × 10^−3; Weight decay = 1 reduced loss to (5.3 ± 0.3) × 10^−3).</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Some beneficial changes were dataset-dependent; turning off normalization layers can destabilize numeric encoding; hyperparameter choices can enable or disable spurious-correlation exploitation by token-length.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>These interventions are engineering strategies rather than cognitive comparisons; they shape how LMs approximate numeric functions but do not alter the fundamental difference between learned numeric regression and exact symbolic computation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3006.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3006.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic, including mechanisms, internal representations, interventions, and performance on arithmetic tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Observed failure modes & suggested fixes</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Failure modes (argmax discontinuity, spurious correlations, dynamic-range limits) and proposed mitigations (mixture outputs, Fourier features on log)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Identified failure modes include discontinuous OOD outputs caused by argmax and discrete encodings, spurious exploitation of encoding-length correlations, limited dynamic range for XVAL; proposed fixes include number-head mixtures (mixture of Gaussians) and Fourier features on log(number) to extend dynamic range.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-2 variant (experiment model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Empirical observations across multiple experiments; suggestions for future architectural/training changes rather than implemented ones (except some ablations).</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_task_type</strong></td>
                            <td>Relevant for all arithmetic/numeric inference tasks in the paper (multiplication, expression evaluation, forecasting, planetary inference).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_mechanism</strong></td>
                            <td>Failure mechanisms are diagnostic: (1) discrete decoding (argmax over vocabulary) introduces discontinuities even when embeddings are smooth; (2) tokenization-dependent spurious signals (encoding length) can be exploited; (3) XVAL's multiplicative encoding combined with layer-norm limits dynamic range.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_for_mechanism</strong></td>
                            <td>Logit traces (Figure 6) show jagged, non-smooth logits for P1000 OOD; Appendix B.3 and Figure 8 show models exploiting encoding-length cues; XVAL embedding projection and theory show limited dynamic range and empirical saturation (Section 2, Figure 2).</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_against_mechanism</strong></td>
                            <td>No direct counterevidence provided; proposed mitigations are prospective. Some text-based encodings still achieve strong in-distribution performance suggesting argmax-discontinuity is primarily an OOD/interpolation issue.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>Proposed: generalize number-head to predict mixtures (e.g., mixture of Gaussians) to capture multimodality; extend XVAL dynamic range by encoding log(number) with Fourier features (continuous analogue of floating point).</td>
                        </tr>
                        <tr>
                            <td><strong>effect_of_intervention</strong></td>
                            <td>Not implemented in paper; authors hypothesize mixture outputs would improve multimodal targets (planet mass) and Fourier-log features would increase dynamic range while preserving continuity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Failure modes quantified indirectly: percentage of unparsable predictions reported in Table 5 (rare, <0.01% typically); discontinuities and OOD errors visualized in figures; paper documents relative differences in R^2 and MSE across encodings to illustrate impact.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_modes</strong></td>
                            <td>Discrete-output discontinuities, spurious length-based shortcuts, saturation of XVAL for extreme magnitudes, inability of scalar-regression to represent multimodality.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_humans_or_symbolic</strong></td>
                            <td>These failure modes highlight that LMs (with current encodings) do not implement robust algorithmic arithmetic like symbolic calculators and can be brittle in ways humans or symbolic systems are not.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Linear algebra with transformers <em>(Rating: 2)</em></li>
                <li>Learning numeral embeddings <em>(Rating: 2)</em></li>
                <li>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets <em>(Rating: 2)</em></li>
                <li>Faith and fate: Limits of transformers on compositionality <em>(Rating: 1)</em></li>
                <li>Methods for numeracy-preserving word embeddings <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3006",
    "paper_id": "paper-263622222",
    "extraction_schema_id": "extraction-schema-73",
    "extracted_data": [
        {
            "name_short": "XVAL",
            "name_full": "XVAL: A continuous number encoding for large language models",
            "brief_description": "A number encoding that represents any real number with a single [NUM] token whose learned embedding vector is multiplied by the scalar numeric value, producing an embedding continuous in the numeric input and enabling an MSE-trained scalar number head at output for end-to-end continuity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 variant (experiment model)",
            "model_description": "Transformer architecture based on GPT-2 used in experiments (6 transformer blocks, 6 heads, head width 128, embedding width 768 ≈ 43.5M parameters); absolute positional encodings; layernorms and residuals as described in paper; trained with MLM or AR depending on task.",
            "arithmetic_task_type": "multi-digit multiplication (3-, 4-, 5-digit), multi-operand arithmetic expressions (binary trees with 2–4 operands using +, -, *), and other numerical inference tasks used for comparison (temperature forecasting, planetary parameter inference).",
            "reported_mechanism": "Multiplicative continuous encoding: a single learned [NUM] embedding vector is scaled by the real numeric value, combined with positional encodings and layer-norm, producing token embeddings that vary continuously with numeric magnitude; a separate scalar number head (MSE loss) produces continuous numeric outputs for [NUM] tokens, avoiding discrete argmax over textual tokens.",
            "evidence_for_mechanism": "Empirical improvements in interpolation and out-of-distribution (OOD) generalization across several tasks: high R^2 on arithmetic (Table 2 and Table 3), superior OOD interpolation in planetary timestep and semi-major axis prediction (Figures 5), best temperature-forecasting MSE/runtime tradeoff (Table 4). Figure 2 shows projected embedding after layer-norm follows analytic scaling; OOD prediction curves are smooth (unlike text-based encodings).",
            "evidence_against_mechanism": "Limited dynamic range due to layer-norm normalization (numbers normalized into roughly [−5,5] in experiments). XVAL performed poorly on predicting planetary mass in one task (suspected multimodal target distribution), producing worse MSE than some text-based encodings for that target. Very large/small numbers can saturate or be negligible respectively. These limitations are discussed and empirically observed.",
            "intervention_type": "Architectural and training design: (1) multiplicative [NUM] embedding + scalar number head trained with MSE; (2) preprocessing normalization of numeric range to [−5,5]; (3) layer-norm placement/choices; (4) ablations on masking probability, weight decay, biases in number-head and trunk.",
            "effect_of_intervention": "Made the model end-to-end continuous in numeric inputs and improved interpolation/OOD performance; number-head (MSE) allowed continuous numeric output instead of discrete classification; layer-norm preserves direction of [NUM] vector while rescaling magnitude (explained analytically and validated empirically). Ablations (Table 9) show turning off some layer-norms or altering hyperparameters degrades performance; number-head bias toggle had little effect.",
            "performance_metrics": "Arithmetic: Table 2 reports adjusted R^2 for multi-digit multiplication: XVAL R^2 = 0.9986 (3-digit), 0.9975 (4-digit), 0.9958 (5-digit). Multi-operand binary-tree evaluation (Table 3): XVAL R^2 = 0.99998 (2 operands), 0.99994 (3 operands), 0.99998 (4 operands). Temperature forecasting (Table 4): MSE = 1.75 (Equal Samples run) with runtime 19h on 4 H100s (comparable/better than alternatives). OOD interpolation plots (Figure 5) show continuous predictions between training values. Note: exact planetary-task numeric table entries for some metrics are reported in paper tables/figures.",
            "notable_failure_modes": "1) Dynamic-range saturation: very large or very small numbers become problematic due to layer-norm rescaling; 2) Poor fit to multimodal output distributions (e.g., planetary mass), where categorical/multi-modal predictions from token-based models may be advantageous; 3) occasional prediction of non-numeric tokens (rare, decreases with training); 4) reliance on preprocessing normalization of numeric range.",
            "comparison_to_humans_or_symbolic": "Paper cites prior work showing even very large LLMs struggle on multi-digit multiplication (Dziri et al. reference: GPT-4 zero-shot accuracies 59% for 3-digit, 4% for 4-digit, 0% for 5-digit as reported by Dziri et al.), highlighting that symbolic calculators/algorithms remain more reliable for exact arithmetic; XVAL improves LM interpolation but is not claimed to match symbolic arithmetic algorithms.",
            "uuid": "e3006.0"
        },
        {
            "name_short": "Number head (MSE scalar)",
            "name_full": "Scalar number head trained with mean-squared error",
            "brief_description": "A dedicated regression head that outputs a scalar numeric value for each [NUM] token and is trained with MSE loss, used in tandem with the token classification head to recover numeric values at output continuously.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-2 variant (experiment model) with dual heads",
            "model_description": "Same GPT-2-based transformer trunk; token (vocabulary) head plus a separate MLP number-head (one hidden layer width equal to embedding dim) that outputs scalar numeric values for [NUM] tokens; number-head sometimes includes bias while trunk does not.",
            "arithmetic_task_type": "Used for all numeric prediction tasks where numbers are masked: arithmetic expression evaluation, temperature forecasting (masked future timesteps), planetary parameter inference.",
            "reported_mechanism": "Regresses numeric values directly rather than predicting them via token classification; combined with XVAL it yields continuous mapping from input numerical values to output numbers; avoids argmax induced discontinuities of classification-based numeric outputs.",
            "evidence_for_mechanism": "Improved interpolation/OOD performance when models use the number-head with XVAL compared to text-based classification outputs; ablations show promoting both heads to MLPs (including number-head) helps separate processing of token and numeric predictions and aids performance.",
            "evidence_against_mechanism": "Number-head predicts a single scalar and struggles when the target distribution is multimodal or highly uncertain (planetary mass case). Paper proposes future extension to mixture-of-Gaussians outputs to capture multimodality.",
            "intervention_type": "Architectural addition (new output head) and loss change (MSE instead of cross-entropy for numbers); optional bias toggles on number-head.",
            "effect_of_intervention": "Enabled continuous numeric output and better interpolation; reduced discrete argmax discontinuity for numbers; slight architectural choices (bias on/off) had limited impact per ablation.",
            "performance_metrics": "Improvements shown indirectly via higher R^2 and better OOD interpolation in XVAL experiments (see XVAL entry). Percentage of unparsable non-numeric token outputs reported as &lt;0.01% for most encodings; where present percentages shown in Table 5.",
            "notable_failure_modes": "Single-scalar output cannot capture multimodal target distributions leading to worse MSE when targets are multimodal; occasional non-numeric token predictions happen if token head outputs non-[NUM] token (rare).",
            "comparison_to_humans_or_symbolic": "Direct regression differs from symbolic algorithms that produce exact discrete outputs; better matched to continuous scientific prediction tasks than exact symbolic arithmetic.",
            "uuid": "e3006.1"
        },
        {
            "name_short": "Text-based numeric encodings",
            "name_full": "P10, P1000, B1999, FP15 and other tokenized number encodings",
            "brief_description": "Tokenization schemes that encode numbers as sequences of textual tokens (digits, signs, exponents, prototype numerals, or learned BPE tokens); vary in tokens-per-number and vocabulary size trade-offs and are trained with standard cross-entropy token classification.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 variant (experiment model) using alternative numeric encodings",
            "model_description": "Same transformer trunk; numbers represented as multiple textual tokens per chosen encoding (e.g., P10: ±, digit, E±d; P1000: ±, ddd, E±d; B1999: learned BPE; FP15: single-token float prototypes). Encoding differences change input length, vocabulary size, and embedding learned for numeric tokens.",
            "arithmetic_task_type": "Used on same arithmetic tasks: multi-digit multiplication, multi-operand expressions, temperature forecasting, planetary parameter inference.",
            "reported_mechanism": "Arithmetic performed via learned associations over discrete token embeddings and token-level sequence modelling; embeddings for numeric tokens can form continuous-looking manifolds but the discrete output argmax and cross-entropy training do not enforce numeric-distance-aware losses.",
            "evidence_for_mechanism": "PCA visualizations (Figure 9) show structure in learned embeddings (rotary/curve-like). Logit traces (Figure 6) for P1000 exhibit discontinuous, jagged predictions OOD, indicating that token logits do not vary smoothly with numeric value. Text-based models exploit spurious correlations (e.g., token-length correlated with numeric magnitude) as shown in Appendix B.3 and Figure 8.",
            "evidence_against_mechanism": "Despite forming continuous-looking embeddings in some cases, the argmax stage and cross-entropy loss can produce discontinuous numeric outputs OOD. Empirically, text-based encodings often fail to interpolate to unseen numeric values (Figure 5).",
            "intervention_type": "Different encoding choices (P10, P1000, B1999, FP15) and preprocessing (significant figures, dropping leading zeros); use of MLM vs AR for non-causal JSON data; tokenizers with different vocabularies (including learned BPE).",
            "effect_of_intervention": "Encoding affects trade-offs: FP15 (single-token per number) gives best in-distribution performance but poor interpolation and high embedding cost; P10 (vocabulary-sparse) gives best interpolation but poor in-distribution performance and long sequences; BPE variable-length encodings lead to spurious correlations where encoding length is exploited. These effects are shown in Tables 2–4 and Figures 3–5.",
            "performance_metrics": "Arithmetic (Table 2 & 3): e.g., P1000 R^2 on multiplication: 0.9989 (3-digit), 0.6071 (4-digit), 0.9439 (5-digit) [note: table contains multiple encodings—see paper for full matrix]. FP15 had strong in-distribution numbers in many tasks but worse OOD. Temperature forecasting (Table 4): FP15 Equal Samples MSE=2.14 runtime 19h; P10 and P1000 had larger runtime/tokenization costs. Exact values for each encoding per task are in the paper tables.",
            "notable_failure_modes": "1) Discontinuous OOD predictions due to argmax over logits and discrete embeddings; 2) Exploitation of spurious correlations, e.g., encoding length correlating with numeric ranges leading to systematic mispredictions (Appendix B.3, Figure 8); 3) Large token sequences for some encodings (P10, P1000) hamper modeling of long-range interactions and increase compute cost; 4) Mislearning joint distributions (e.g., temperature dataset confusion due to common digit/exponent co-occurrence).",
            "comparison_to_humans_or_symbolic": "These encodings are closer to symbolic tokenization but tend to produce brittle behavior compared to exact symbolic algorithms; do not produce continuous numeric outputs unless additional mechanisms are used.",
            "uuid": "e3006.2"
        },
        {
            "name_short": "Learned numeric embeddings / internal representations",
            "name_full": "Learned embedding geometries and logits behavior for numeric tokens",
            "brief_description": "Observed internal representations include continuous-looking manifolds (curves, rotary structures) for digit/mantissa/exponent token embeddings and discontinuous/highly non-smooth logits for text-based encodings, while XVAL produces embeddings whose projection onto the [NUM] direction scales approximately with numeric magnitude after layer-norm.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 variant (experiment model)",
            "model_description": "Internal analyses performed on trained transformer models using different numeric encodings; visualizations include PCA of mantissa token embeddings (Figure 9), projection of layer-normed [NUM] embedding (Figure 2), and logit-trace plots (Figure 6).",
            "arithmetic_task_type": "Analyses performed in context of arithmetic and numeric inference tasks (multiplication, planetary/temperature prediction).",
            "reported_mechanism": "LMs form structured internal embedding geometries for numeric tokens (e.g., curves and rotary-like patterns) that can encode relative numeric information via inner products; however, discrete decoding and training objectives (cross-entropy) cause final outputs to be discontinuous even if embeddings appear smooth.",
            "evidence_for_mechanism": "Figure 9 shows 2D PCA projections demonstrating curve/rotary structures in learned mantissa embeddings; Figure 2 empirically validates analytic relation of layer-normed XVAL encoding; Figure 6 shows jagged/highly variable logits for P1000 indicating lack of smooth numeric interpolation at output.",
            "evidence_against_mechanism": "Smooth-looking embeddings do not guarantee smooth numeric outputs due to argmax and cross-entropy training; discontinuities in logits (Figure 6) demonstrate that internal smoothness can be destroyed before output decoding.",
            "intervention_type": "Visualization/probing (PCA, projection), logit-trace analysis; ablations on layer-norm placement and MLP layer-norm toggles to see effect on numeric embedding normalization.",
            "effect_of_intervention": "Revealed that layer-norm near input helps normalize numeric embeddings and preserve XVAL direction; turning off some layer-norms harms performance (Table 9). Probing confirmed that discrete decoding is a major source of OOD discontinuity.",
            "performance_metrics": "Qualitative/internal metrics: PCA plots, projected embedding-vs-value curves (Figure 2), logit traces showing non-smoothness (Figure 6). These visual pieces of evidence accompany task performance metrics in other entries.",
            "notable_failure_modes": "Apparent continuity of embeddings can be misleading: models still produce discontinuous outputs; embeddings spread more for datasets where numeric sensitivity is low (planet data) vs. arithmetic tasks where embeddings form tighter curves.",
            "comparison_to_humans_or_symbolic": "Internal learned geometries are not equivalent to algorithmic symbolic representations; they can encode numeric relations but do not provide exact arithmetic algorithms akin to symbolic computation.",
            "uuid": "e3006.3"
        },
        {
            "name_short": "Architectural & training interventions (ablations)",
            "name_full": "Layer-norm placement, masking probability, weight decay, biases, and preprocessing interventions",
            "brief_description": "Ablations and interventions explored include placement of layer-norm, masking probability in MLM, weight decay, inclusion of biases in transformer modules and number-head, and numeric preprocessing (normalization to limited range); these affect numeric performance and stability.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 variant (experiment model)",
            "model_description": "Same transformer backbone; ablation experiments used shorter runs (100k iterations) across several hyperparameter settings and learning rates and reported validation-loss changes (Table 9).",
            "arithmetic_task_type": "Evaluated on temperature forecasting validation loss primarily, with implications for other numeric tasks.",
            "reported_mechanism": "These interventions modulate how numeric information is normalized, represented, and learned (e.g., layer-norm prior to first MLP normalizes multiplicative numeric embedding; mask rate changes task difficulty and generalization; weight decay influences regularization/bias-variance).",
            "evidence_for_mechanism": "Table 9: Turning off certain layer-norms significantly degraded performance; increasing masking probability to 30% sometimes improved on this dataset; increasing weight decay led to largest improvement (dataset-dependent); bias inclusion in transformer modules improved performance at cost of variability; number-head bias toggle had little effect.",
            "evidence_against_mechanism": "Some effects are dataset dependent and do not generalize uniformly (e.g., optimal masking probability differs across datasets); changing min-LR/LR ratio had limited effect in short runs.",
            "intervention_type": "Architectural ablations and hyperparameter sweeps: layer-norm toggles, masking probability (10%, 20%, 30%), weight decay variations (0.0001, 0.1, 1), inclusion/exclusion of biases, learning-rate schedules and warmups, numeric preprocessing (normalization to range).",
            "effect_of_intervention": "Changing these settings had measurable effects on validation loss and numeric performance: removing first-layer norm had small effect for some datasets but removing MLP layer-norm worsened performance significantly; larger weight decay improved performance in this corpus; higher masking sometimes helped; number-head bias off had negligible effect.",
            "performance_metrics": "Ablation validation losses reported in Table 9 (e.g., baseline Normal config val loss ≈ (6.8 ± 0.2) × 10^−3; MLP Layer Norm = False increased loss to (9.0 ± 0.1) × 10^−3; Weight decay = 1 reduced loss to (5.3 ± 0.3) × 10^−3).",
            "notable_failure_modes": "Some beneficial changes were dataset-dependent; turning off normalization layers can destabilize numeric encoding; hyperparameter choices can enable or disable spurious-correlation exploitation by token-length.",
            "comparison_to_humans_or_symbolic": "These interventions are engineering strategies rather than cognitive comparisons; they shape how LMs approximate numeric functions but do not alter the fundamental difference between learned numeric regression and exact symbolic computation.",
            "uuid": "e3006.4"
        },
        {
            "name_short": "Observed failure modes & suggested fixes",
            "name_full": "Failure modes (argmax discontinuity, spurious correlations, dynamic-range limits) and proposed mitigations (mixture outputs, Fourier features on log)",
            "brief_description": "Identified failure modes include discontinuous OOD outputs caused by argmax and discrete encodings, spurious exploitation of encoding-length correlations, limited dynamic range for XVAL; proposed fixes include number-head mixtures (mixture of Gaussians) and Fourier features on log(number) to extend dynamic range.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "GPT-2 variant (experiment model)",
            "model_description": "Empirical observations across multiple experiments; suggestions for future architectural/training changes rather than implemented ones (except some ablations).",
            "arithmetic_task_type": "Relevant for all arithmetic/numeric inference tasks in the paper (multiplication, expression evaluation, forecasting, planetary inference).",
            "reported_mechanism": "Failure mechanisms are diagnostic: (1) discrete decoding (argmax over vocabulary) introduces discontinuities even when embeddings are smooth; (2) tokenization-dependent spurious signals (encoding length) can be exploited; (3) XVAL's multiplicative encoding combined with layer-norm limits dynamic range.",
            "evidence_for_mechanism": "Logit traces (Figure 6) show jagged, non-smooth logits for P1000 OOD; Appendix B.3 and Figure 8 show models exploiting encoding-length cues; XVAL embedding projection and theory show limited dynamic range and empirical saturation (Section 2, Figure 2).",
            "evidence_against_mechanism": "No direct counterevidence provided; proposed mitigations are prospective. Some text-based encodings still achieve strong in-distribution performance suggesting argmax-discontinuity is primarily an OOD/interpolation issue.",
            "intervention_type": "Proposed: generalize number-head to predict mixtures (e.g., mixture of Gaussians) to capture multimodality; extend XVAL dynamic range by encoding log(number) with Fourier features (continuous analogue of floating point).",
            "effect_of_intervention": "Not implemented in paper; authors hypothesize mixture outputs would improve multimodal targets (planet mass) and Fourier-log features would increase dynamic range while preserving continuity.",
            "performance_metrics": "Failure modes quantified indirectly: percentage of unparsable predictions reported in Table 5 (rare, &lt;0.01% typically); discontinuities and OOD errors visualized in figures; paper documents relative differences in R^2 and MSE across encodings to illustrate impact.",
            "notable_failure_modes": "Discrete-output discontinuities, spurious length-based shortcuts, saturation of XVAL for extreme magnitudes, inability of scalar-regression to represent multimodality.",
            "comparison_to_humans_or_symbolic": "These failure modes highlight that LMs (with current encodings) do not implement robust algorithmic arithmetic like symbolic calculators and can be brittle in ways humans or symbolic systems are not.",
            "uuid": "e3006.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Linear algebra with transformers",
            "rating": 2,
            "sanitized_title": "linear_algebra_with_transformers"
        },
        {
            "paper_title": "Learning numeral embeddings",
            "rating": 2,
            "sanitized_title": "learning_numeral_embeddings"
        },
        {
            "paper_title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
            "rating": 2,
            "sanitized_title": "grokking_generalization_beyond_overfitting_on_small_algorithmic_datasets"
        },
        {
            "paper_title": "Faith and fate: Limits of transformers on compositionality",
            "rating": 1,
            "sanitized_title": "faith_and_fate_limits_of_transformers_on_compositionality"
        },
        {
            "paper_title": "Methods for numeracy-preserving word embeddings",
            "rating": 2,
            "sanitized_title": "methods_for_numeracypreserving_word_embeddings"
        }
    ],
    "cost": 0.01603975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>XVAL: A CONTINUOUS NUMBER ENCODING FOR LARGE LANGUAGE MODELS
4 Oct 2023</p>
<p>Siavash Golkar siavash.golkar@gmail.com 
Flatiron Institute</p>
<p>Mariel Pettee 
Flatiron Institute</p>
<p>Lawrence Berkeley National Laboratory</p>
<p>Michael Eickenberg 
Flatiron Institute</p>
<p>Alberto Bietti 
Flatiron Institute</p>
<p>Miles Cranmer 
University of Cambridge</p>
<p>Geraud Krawezik 
Flatiron Institute</p>
<p>Francois Lanusse 
Flatiron Institute</p>
<p>Université Paris-Saclay
Université Paris Cité
CEA
CNRS</p>
<p>Michael Mccabe 
Flatiron Institute</p>
<p>University of Colorado</p>
<p>Ruben Ohana 
Flatiron Institute</p>
<p>Liam Parker 
Flatiron Institute</p>
<p>Bruno Régaldo-Saint Blancard 
Flatiron Institute</p>
<p>Tiberiu Tesileanu 
Flatiron Institute</p>
<p>Kyunghyun Cho 
Flatiron Institute</p>
<p>New York University
7 Prescient DesignGenentech</p>
<p>CIFAR Fellow</p>
<p>Princeton University</p>
<p>Shirley Ho 
New York University
7 Prescient DesignGenentech</p>
<p>The Polymathic AI Collaboration</p>
<p>XVAL: A CONTINUOUS NUMBER ENCODING FOR LARGE LANGUAGE MODELS
4 Oct 2023457657BD464745941484772005478F37arXiv:2310.02989v1[stat.ML]
Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers.We propose XVAL, a numerical encoding scheme that represents any real number using just a single token.XVAL represents a given real number by scaling a dedicated embedding vector by the number value.Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string.This leads to an inductive bias that is generally more suitable for applications in scientific domains.We empirically evaluate our proposal on a number of synthetic and real-world datasets.Compared with existing number encoding schemes, we find that XVAL is more token-efficient and demonstrates improved generalization.</p>
<p>INTRODUCTION</p>
<p>Even as Large Language Models (LLMs) exhibit sophisticated behavior in the generation and analysis of textual data, the scientific community has seen little success in applying these models to datasets consisting mostly of numerical values.LLMs have historically struggled to solve simple arithmetic problems such as multi-digit multiplication (Dziri et al., 2023) and have a tendency to "confabulate" answers (OpenAI, 2023;Frieder et al., 2023).Standard LLM tokenization schemes do not inherently capture the precise quantitative properties that distinguish numerical data from other natural language inputs (Testolin, 2023;Choi, 2021).</p>
<p>Recent work has explored several potential improvements for encoding numerical information as inputs to language models (see Thawani et al. (2021) for a review).For instance, numbers can be encoded digit-by-digit, in scientific notation format, or in base-10 format.(Jiang et al., 2020) maps numbers onto a finite set of "prototype numerals", while (Sundararaman et al., 2020) enforces constraints such that the cosine distances between the embeddings of numbers reflects their actual mathematical distance.Transformers that use such encodings have been shown to successfully solve various mathematical problems, such as linear algebra problems including matrix multiplication (Charton, 2022).Despite these improvements, many challenges remain unresolved.Language models are known to exploit shortcuts and spurious correlations in the data (Tu et al., 2020;Liu et al., 2022;Dziri et al., 2023) and still struggle with interpolation and out-of-distribution generalization in mathematical problems and in scientific domains (Grosse et al., 2023;Anil et al., 2022).Functions appearing in such domains are often continuous or smooth, with certain exceptions such as points of criticality.Similarly, transformer architectures applied to vision and audio domains (e.g., Dosovitskiy et al., 2020;Garg et al., 2022) typically treat numbers continuously without tokenization (see however Copet et al., 2023;Chen et al., 2020b), but these models typically require highly structured inputs, and cannot be applied to datasets with arbitrary sequences of text and numbers.On the other hand, when encoding numbers as text, LLMs are inherently discontinuous in both the encoding and decoding stages.While discrete models can (and do) learn to approximate continuous functions (d'Ascoli et al., 2022), this can be more challenging and less sample efficient compared to models that have continuity built-in by construction, as in many non-parametric regression models (Wasserman, 2006).In order to overcome this inherent challenge, it is necessary to impose the appropriate inductive bias based on our knowledge of the continuous nature of numbers.</p>
<p>We introduce XVAL, an inherently continuous method of encoding numerical values in Large Language Models.By encoding the magnitude of numerical values multiplicatively and orienting them in a learnable direction within the embedding space, XVAL substantially changes how numbers are processed and interpreted by transformer architectures.This leads to an encoding scheme with a single vocabulary element that also encodes every number as a single token.XVAL is therefore both token-efficient and has minimal vocabulary footprint.</p>
<p>Coupled with a modified number-inference paradigm, XVAL allows a transformer model to be continuous (or smooth given smooth non-linearities) when considered as a map between the numbers of the input string and those of the output.We expect that this leads to a better inductive bias when the functions being approximated are continuous or smooth.We evaluate XVAL on a number of synthetic and real-world scientific datasets and compare with existing number encoding schemes.We demonstrate that XVAL is both more token-efficient and exhibits better interpolation properties.</p>
<p>OUR CONTRIBUTIONS</p>
<p>• We introduce XVAL, a novel approach for encoding numerical values in Large Language models.Compared to existing number encoding schemes, XVAL is both token-efficient (every number is encoded as a single token) and has a minimal vocabulary footprint (a single number token).</p>
<p>• We introduce a modified number inference scheme that, when used in conjunction with XVAL, renders transformer models continuous as a function of the numerical values appearing in the text.</p>
<p>• We evaluate XVAL and a number of existing number encoding schemes on several synthetic and real world datasets.We demonstrate that XVAL consistently provides better interpolation properties and is more compute-efficient than prior work.</p>
<p>METHODS</p>
<p>In this section, we describe the details of the XVAL number encoding as well as the number inference paradigm of our model.</p>
<p>XVAL: A CONTINUOUS NUMBER ENCODING</p>
<p>Instead of using different tokens for different digits or composite numbers, XVAL embeds numerical values directly along a specific learnable direction of the embedding space.A diagram of this procedure can be seen in Fig. 1.Specifically, given a string input x comprising both numbers and text, we first parse x to extract all the numerical values and collect them in a separate list x num .We then construct a new string x text by replacing all numbers in x with a designated token [NUM] that acts as a placeholder for numerical values.We tokenize and embed x text , arriving at h text .We then Figure 1: A simplified example illustrating the XVAL number encoding and the modified number inference paradigm.On the left, XVAL is contrasted with the P1000 text-based numerical encoding scheme.On the right, we illustrate how numbers are addressed within the decoder.multiply the embedding of each appearance of the [NUM] token with its associated numerical value in x num .This process can be done efficiently by defining a new list h num by scattering x num to have the same length as the tokenized x text and inserting a 1 for any token other than [NUM].The final embedding of the sample is h emb = h num × h text , which is then fed to the transformer trunk.This encoding process can be performed both for masked language modeling (MLM) and autoregressive (AR) generation.During training, in cases where MLM is used, we simultaneously mask both h text and h num , i.e., if the token being masked is a [NUM] token, we replace the corresponding number in h num with 1. Implicit normalization via layer-norm.In our implementation, the multiplicative embedding of XVAL is followed by the addition of a positional encoding vector and then a layernorm in the first transformer block.The effect of the layernorm is to normalize the embedding of each token on a persample basis.When the added positional embeddings are not collinear to the embedding of the [NUM] token, the scalar value is effectively passed through a non-linear rescaling function.Indeed, denoting u ∈ R d as the embedding of [NUM], p ∈ R d as the positional embedding, and x ∈ R as the scalar to be encoded, and assuming for simplicity u • p = 0 with ∥u∥ = ∥p∥ = 1, we have
u • xu + p ∥xu + p∥ = x √ 1 + x 2 ,
such that the value x is still encoded in the same direction u.</p>
<p>Figure 2 shows that such a property approximately holds empirically up to a constant after training, and we found these curves to be near-identical for any positional embedding.</p>
<p>This normalization property implies that the dynamic range of XVAL is more limited than those of other text-based encoding schemes.In the experiments of this paper, we normalize numbers in the text corpus such that they fall within the range [−5, 5] as a preprocessing step before training.</p>
<p>NUMERICAL VALUE INFERENCE</p>
<p>XVAL defines an embedding that is continuous in the numerical values of the input.However, if we use a multi-class classification task as our output and training algorithm, the model as a whole</p>
<p>will not be end-to-end continuous when considering the map from the input numbers to the output numbers.For this reason, we treat numbers separately at the output layer.This process is illustrated in the right-hand portion of Fig. 1.</p>
<p>As is standard practice in transformer-based language models, we define a token head that outputs a probability distribution of the tokens of the vocabulary.However, since our formalism replaces numbers with the [NUM] token, this head does not carry any information about the number value.</p>
<p>We therefore introduce a new number head with a scalar output, trained via mean squared error (MSE) loss, to recover the numerical value associated with each instance of the [NUM] token.For any input, we first look at the output of the token head.If the generated token is the [NUM] token, we then look at the number head to fill in the value for this token.As shown in Section 3, since the transformer is now end-to-end continuous when inferring numerical values, it performs better when interpolating to previously unseen values.</p>
<p>EXPERIMENTS</p>
<p>In this section, we evaluate the performance of XVAL and highlight its strengths and weaknesses compared to existing numerical encoding algorithms.In particular, we look at three datasets: a synthetic dataset of arithmetic operations, a dataset of global temperature data, and a dataset of planetary orbit simulations.</p>
<p>For our transformer models, we use an architecture based on GPT-2 (Radford et al., 2019).Details of our specific architecture are included in Appendix A).We explore the effects of various architectural design choices in Appendix B.4.
3 918 B1999 {±ddd, E±d} [-602, E-1] 2 1816 FP15 {±ddd E±d} [-602 E-1] 1 28800 XVAL {[NUM]} [NUM] 1 1
Comparison with other number encodings.We compare the performance of XVAL with four other number encodings, following the notation of Charton (2022).In these encodings, numbers are first processed into the format ±ddd E±d.The encodings are then determined by which parts of this format are encoded as single or multiple tokens.These range from encodings with limited vocabulary size but high number of tokens per number, leading to longer encoded sequence lengths (e.g., P10), to those with very large vocabulary footprints but only one token per number, leading to shorter encoded sequence lengths (e.g., FP15).XVAL provides a minimal vocabulary footprint and uses just a single token per number, leading to the shortest sequence lengths.A summary of these encodings and an example can be seen in Table 1.</p>
<p>Number encodings that do not lead to a fixed number of tokens for all numbers (e.g., learned Byte Pair Encoding (Gage, 1994) used in GPT-2 (Radford et al., 2019)) can lead to erratic behaviors where the transformer learns spurious correlations that exist between the length of the encoded numbers in the dataset.An example of this type of behavior is shown in Appendix B.3.</p>
<p>LEARNING ARITHMETIC</p>
<p>Simple arithmetic problems have acted as a test bed for probing the mathematical reasoning abilities of language models (Dziri et al., 2023).In this section, we investigate the effect of the number encoding scheme on the ability of language models to perform multi-digit multiplications as well as multi-operand mathematical operations.Multi-digit multiplication is a notably challenging task for even the largest LLMs (Borji, 2023).Dziri et al. (2023) show that GPT-4 achieves only 59%</p>
<p>zero-shot accuracy on three-digit multiplication problems, while its accuracy for four-and five-digit multiplication drops to 4% and 0%, respectively.</p>
<p>Table 2 reports the R 2 scores for multi-digit multiplication problems on several language models designed to handle numerical values.All number encodings generally perform well on this task.However, we find that some encoding schemes (P10 and FP15) show a tendency to yield a small percentage of highly erroneous predictions in some contexts, thereby reducing the R 2 score, while XVAL does not produce such outliers.</p>
<p>For a more challenging arithmetic task, we designed a dataset of multi-operand mathematical operations.We used random binary trees combining a fixed number of operands (2, 3, or 4) using the binary operators of addition, subtraction, and multiplication.build a dataset in which each sample is an arithmetic statement such as ((1.32 * 32.1) + (1.42-8.20))= 35.592.We then processed the samples according to the processing requirements of each number-encoding scheme.</p>
<p>The task is evaluation of the expression on the left-hand side of the equation, implemented as a mask completion, where the right-hand-side number is masked.Table 3 shows the adjusted R 2 scores results on this task.XVAL performs remarkably well on this task.Arithmetic experiments alone are not sufficient for fully evaluating the mathematical abilities of language models.The samples in these datasets are often short sequences and the underlying data manifold is low-dimensional.These problems therefore do not push the boundary of what is computationally possible with LLMs.</p>
<p>In the remainder of this section, we consider experiments in more complex settings and much longer sequences.The goal of the next two subsections is not to construct state-of-the-art models in their respective domains, but rather to compare the performance of language models with different number encoding schemes in more complicated real-world scenarios.</p>
<p>TEMPERATURE FORECASTING</p>
<p>As an example of real-world scientific analysis, we look at the task of temperature forecasting.In this experiment, we construct a dataset as a subset of the ERA5 global climate dataset (Hersbach et al., 2020).For simplicity, we only focus on the surface temperature data (T2m field in ERA5).We split the dataset into individual samples, where each sample includes 2-4 days of surface temperature data (normalized to have unit variance) as well as the latitude and longitude from 60-90 randomly selected reporting stations.We also include the time of the first included timestep.We encode the coordinates by using the sine of the latitude and the sine and cosine of the longitude such that we preserve the periodicity.Similarly, we encode the time of year and time of day using the sine and cosine of the position along the 24 hour and 365 day cycles.We include all this information in a JSON format as follows1 :</p>
<p>{'description':{'coords': [[1,-.32,.95The coords, start, and data correspond to the reporting station coordinates, the time of the first sample, and the normalized temperature data, each reported separately per station and then concatenated in the data list.In this way, the model needs to parse both the textual aspects of the sample (e.g., where the commas appear to separate different parts of the data) as well as the numerical values.Furthermore, as is often the case with JSON-formatted data, the data does not have a causal format.We therefore train the language models using an MLM approach instead of the more common AR approach.We evaluate the performance of the different numerical encodings on the task of predicting the next temperature timestep for all reporting stations simultaneously in a held out test set.We do so by masking the tokens (and numbers, if applicable) of all the data associated with the final timestep.Because the temperature data is provided separately per station, the masks are scattered throughout the input data and are not all simply at the end of the sample.Table 4 shows the results of this experiment.XVAL provides the best performance while taking considerably less compute time.</p>
<p>This task exemplifies one of the shortcomings of text-based encoding schemes: they can take advantage of spurious correlations in the data.In this case, P10, P1000 and B1999 have a tendency to predict normalized temperature ±0.1, which manifest as extended protrusions in Fig. 3.This is due to the over-abundance of this number in the dataset compared to other numbers, as seen in Fig 4.</p>
<p>While individually, 100 and E-3 are the most common numbers and exponents in the dataset, when combined, 100E-2 is much more frequent than 100E-3.This explains why FP15, which encodes the digits and exponents as one token, does not get confused in this case.It also implies that the model has failed to learn the correct joint distribution of the numbers.In these cases, because of the tokenization scheme, the length of the tokenized samples are very long, averaging around 8000 and 5000 tokens respectively for P1000 and P10 (compared to 1800 tokens for FP15 and XVAL).</p>
<p>The poor performance in these models might therefore be due to the the challenges of modelling long-range interactions (Qin et al., 2023).</p>
<p>For more details on the performance of the different encodings, as well as comparison with some non-transformer baselines, see Appendix B.1.In Appendix B.3 we look at the performance of a BPE tokenizer on this task and demonstrate how LLMs can exploit the tokenized length of the number.</p>
<p>PREDICTING PLANETARY ORBITS</p>
<p>We then compare the performance of the various number encoding schemes on a simulated dataset of planetary orbits.We construct a dataset consisting of planetary motion simulations generated by the REBOUND N-body codebase (Rein, H. &amp; Liu, S.-F., 2012) and integrated using IAS15 (Rein &amp; Spiegel, 2015).The dataset consists of 1.25 million samples, split into 80%, 10%, 10% for training, validation, and test.Each sample consists of simulation parameters (mass and orbit properties of each planet and the simulation timestep size) as well as a sequence of (x, y) positions for each planet, organized in a JSON format.We pretrain the models using MLM and evaluate the models on the task of inferring the simulation parameters, specifically the simulation timestep ∆t, and the semi-major axis, eccentricity and mass of the first planet (a 1 , e 1 , m 1 ) by masking the appropriate locations.The quantities ∆t and a 1 in the training corpus take values that are either discrete or are sampled from intervals with gaps.This property makes these quantities a good testing ground for interpolation generalization.</p>
<p>Table 5: Performance of the different encodings on the planetary motion inference problem.Here, OoD implies evaluation on samples where the quantity was not seen in the training corpus.The percentages in brackets denote the fraction of the predictions that could not be parsed as numbers.</p>
<p>When not specified, this fraction was less than 0.01%.( †) The poor performance here is because of a number of outliers that are being mis-classified.The results of this test are presented in Table 5.In the numerical encoding schemes other than XVAL, we see an overall inverse relationship between performance in-and out-of-distribution.For example, P10-the encoding with the fewest vocabulary elements-provides the worst in-distribution performance but is best on out of distribution tasks.This is an example of the bias/variance trade-off applied to the number of vocabulary elements.In comparison, we see that XVAL provides the best out-of-distribution performance while staying competitive in-distribution (with one exception).The out-of-distribution performance of these encoding methods can be seen in Fig. 5.Here we see that the text-based encodings, with the exception of P10, simply do not predict any number that they did not explicitly see for this parameter in the training corpus.As expected from a function that is continuous by construction, XVAL continuously interpolates between the values seen in the training set and offers much better performance.</p>
<p>Method</p>
<p>Figure 5 shows that the predictions coming from the text-based encodings can be discontinuous when evaluated out-of-distribution.This discontinuity has two potential sources: the discontinuous nature of the number embeddings and the argmax that is taken over the logits during inference.Since the encodings of the number tokens in text-based encodings have been shown to form continuouslooking structures (see Sec.  2022)), it is possible that the discontinuiuty is only a side effect of the argmax and that the logits themselves vary more smoothly.Figure 6 shows an example of the logits of the P1000 encoding when predicting the step-size outof-distribution.Here, the color lines denote the highest-value logits, with the other logits carrying negligible weight.The dashed gray lines denote the values of the step-size seen in the training set.We see that these lines are smooth in neither small or larger scales.We expect that this is a combination of the text-based number encodings' discrete embedding schemes together with the cross-entropy training paradigm that does not incorporate number distances into the loss.It is evident that embedding the magnitude of numbers directly, as in XVAL, leads to a different inductive bias than treating numbers as tokenized text.This can be clearly seen in the varying performance of these language models in different tasks.</p>
<p>When predicting the next timestep in the temperature dataset, XVAL provides by far the best results.On the other hand, in the mass prediction on the planetary task, it fails to learn the correct relationship, along with vocabulary-sparse P10.</p>
<p>Where XVAL excels is in out-of-distribution performance, while the text-based encoding schemes fail to interpolate properly.The best interpolation for the text-based encodings is given by the vocabulary-sparse P10, which performs poorly on the in-distribution tasks.However, it often performs poorly when evaluated on in-distribution tasks.The the extra encoding length of P10 also makes it prohibitively expensive to deploy as can be seen in Table 4. On the other hand, FP15 provides the best in-distribution performance but it has poor interpolation properties and expensive embedding cost.Overall, XVAL provides the best mix of in-distribution and out-of-distribution performance.Moreover, it is the most computationally efficient of the encoding schemes we considered.</p>
<p>Failure modes.There are a number of ways that number inference via a large language model can fail.The language model can predict a non-numeric token in the place of the number, leading to an invalid prediction.These are denoted in the percentages in brackets in Table 5, shown only when the percentage exceeded 0.01%.This failure mode is uncommon and becomes less frequent the more the model is trained.Another failure mode is when the model exploits spurious correlations.</p>
<p>For example, the model can learn the distribution of the digits, as discussed in the example of temperature dataset, or the length of the encoding (see Appendix B.3).</p>
<p>A model can also fail to learn the correct distribution.In the planetary orbits example, learning the mass of the planet is the most challenging task -all encodings struggle with this.In this task, XVAL performs uncharacteristically poorly.We suspect that this is due to the high uncertainty in estimating the mass and that a multi-modal distribution such as the categorical distribution learned by traditional LLMs would perform better.This can be seen in Fig. 7, where the predictions of P10 and XVAL are shown.While both of these models perform poorly when considering the MSE of the prediction, the multi-modal prediction of P10 would be a better starting point for capturing an uncertain distribution.We therefore suspect that generalizing the number-head such that instead of predicting a scalar for each number, it fits a mixture of Gaussians, would improve this performance.We leave explorations in this direction for future investigation.In this work, we introduced XVAL, a continuous number encoding that makes transformer-based models endto-end continuous when considered as a function mapping the numerical values of the input to those of the output.We demonstrated that even though XVAL is more token-efficient and has a minimal vocabulary footprint, it excels in numerical tasks and leads to superior performance, especially when evaluated on out-ofdistribution samples.Because of the fundamentally different treatment of numbers across these cases, XVAL and text-based encodings lead to different inductive biases, making the choice of the best encoding method on a given dataset highly dependent on the problem under consideration.</p>
<p>DISCUSSION</p>
<p>Future directions.As we have seen, using the XVAL encoding scheme renders the LLM not just continuous, but also differentiable as a function of the numbers it predicts.This enables the LLM loss to incorporate not just an MSE loss, but other statistical learning schemes.For example, we can add a Gaussian Mixture Model or any other differentiable loss and train the LLM to optimize this objective.This holds the promise to improve the experiments in which XVAL underperformed in this paper.</p>
<p>A shortcoming of XVAL is that, because it embeds number values directly in the embedding space, its dynamic range is limited compared to text-based encodings.Very large numbers saturate the normalization, as discussed in Sec. 2, and very small numbers are negligible from the model's perspective.There are methods that allow high dynamic ranges that maintain continuity (or smoothness).</p>
<p>One such example is to use Fourier features on the logarithm of the number.This can be considered as a continuous analog of floating point precision encoding and would drastically improve the dynamic range of the XVAL encoding.</p>
<p>XVAL, combined with our proposed number-inference paradigm, makes LLMs generally more suitable for applications in scientific domains.LLMs have become increasingly integrated in many scientific workflows today, enabling researchers to parse scientific language in sophisticated ways.However, their usefulness for analyzing data-heavy corpuses is currently limited.Crafting LLMs that have a better understanding of numerics has the potential to greatly increase their usefulness in scientific analysis and discovery.</p>
<p>A ARCHITECTURE DETAILS</p>
<p>In our experiments, all the language models, regardless of encoding, adopt the main features of GPT-2 (Radford et al., 2019).That is, we use absolute position encoding and the transformer blocks have layer norms prior to the attention module and the MLP (i.e., after each residual connection).We also set the width of the MLP hidden layer equal to 4 times the width of the embedding.We deviate from GPT-2 in that we initialize all the weights of the transformer blocks with a normal distribution with standard deviation given by (2 × fan-in × num-layers) −1/2 .The dependence of the standard deviation on the number of transformer blocks is to counteract the effect of having a series of residual connections.We also do not use any biases in the trunk of the transformer.As is standard, after the transformer blocks, we have a token-head, comprised of a single linear layer, which maps the latent embedding of each token into a distribution over the vocabulary.As in GPT-2, we tie this weight to that of the embedding matrix which maps the tokens of the input to the embedding space.</p>
<p>For the LLMs using XVAL encoding and an MSE number-head in addition to the token head, we promote both heads (number and token) to be MLPs with one hidden layer of width equal to the embedding dimension.This was to allow the two different prediction types (the number and the distribution over the vocabulary) to be processed separately before the final prediction.In particular, we explore the possibility of having biases for the number-head and not in the token-head in Sec.The samples are individually preprocessed such that the temperature range across all samples has mean zero and standard deviation equal to 1.We also include the lattitude longitude information.</p>
<p>To respect the periodicity of this information, we provide the sine of the lattitude and the sine and cosine of the longitude.Furthermore, we specify the starting time for each sample as the day of year and time of day.Again to respect the periodicity of these quantities, we provide the sine and cosine of the phase of these quantities.</p>
<p>Architecture design hyperparameters.For all experiments done with this dataset, we use transformers with 6 transformer blocks, each with 6 heads and each head having width 128, resulting in a embedding width of 768 (43.5M parameters).</p>
<p>Training hyperparameters.For the equal samples training runs, we train each model for 500k iterations with batch size equal to 64 samples.For the equal tokens runs, we increase the number of iterations proportionately such that the total number of tokens seen is equal.This implies: 500k samples for P10, 820k for P1000, 1.2M for B1999, and 2.3M for FP15 and XVAL.Since there is non-numeric data in the samples, the ratio of the length of the equal tokens is slightly different angle in the (x, y) plane equal to zero for 30% of the samples and uniform θ ∈ [−π/6, π/6] for the remainder.These choices are made such that when generating the large number of samples required for training, we do not come across instabilities or collisions.Finally, we use an integration step-size sampled uniformly from {0.2, 0.3, 0.5, 0.8}.</p>
<p>We generate 1.25 million examples in this way and split it into 1 million train, 125 thousand validation and test set samples.We normalize the masses such that they take value between 1 and 5 and the eccentricities such that they are between 0 and 2. We then construct a JSON format sample including all of this information.A generic sample is given in this example.Training hyperparameters.We train each model for 500k iterations with batch size equal to 64 samples.The hyperparameters in this task are given in Table 8.In many JSON formatted datasets, the data does not follow a causal pattern, i.e. earlier entries might depend logically on latter entries.This is also the case for our JSON formatted samples.Because of this we used Masked Language Modeling (MLM) for pretraining our models.In the context of MLM, number encodings that lead to encoding lengths that vary based on the number can prove troublesome both during training and during testing.During train time, the length of the encoding acts as a cue to help the model figure what the number is.This is an example of spurious correlations that LLMs are known to exploit (Tu et al., 2020;Liu et al., 2022;Dziri et al., 2023).Similarly at test time, the length of the mask can bias the model toward predicting one number or another.</p>
<p>As a demonstration of this feature, we first preprocessed the Temperature Forecast dataset such that every number has only two significant figures and drop leading zeros for efficiency (e.g.0.12 → .12).2We then used a tokenizer that included single and double digits as well as ±, the decimal point and exponents ranging from (E-8 to E+2).In this dataset, Positive and negative floats with magnitude between 0.1 and 1 (e.g..23 and -.34) would have encoding lengths equal to 2 and 3 and Positive and negative floats with magnitude between 0.01 and 0.1 (e.g.-.034 = 3.4E-2) would have encoding lengths 4 and 5.There are exceptions however.For example in this scheme 0.030=3E-2 has encoding length 2.</p>
<p>The results of this experiment can be seen in Fig. 8.We see that even though the model's overall performance is not great, it can tell with very high accuracy the numbers sign, whether or not it has absolute value greater/less than 1, or greater/less than 0.1.This is due to the fact that the model is exploiting the correlation of the numbers with the length of the encoding.We verify this by highlighting in orange the cases where in the range between 0.01 and 0.1, the number has encoding length 2, that is it does not follow the general trend mentioned above.We see that the model believes that these numbers are greater than 0.1 (which as we saw generally had encoding length 2).</p>
<p>Figure 8: LLMs can exploit spurious correlations in the data.In this case, the model has learned the correlation between the number signs/values with the length of the encoding.Highlighted in orange are numbers between 0 and 0.1 that do not have the encoding length equal to 2..</p>
<p>B.4 ARCHITECTURAL EXPLORATIONS</p>
<p>There are a number of engineering choices that we made regarding the architecture and hyperparameters of the transformer models trained with XVAL and the number head.Here, we explore the effect of these on the Temperature Forecast task.Because of the large exploration space and the high amount of compute required, we do the ablation tests on a shorter run, 100k iterations compared to 500k iterations of the main text.For this exploration, we first run all of the configurations with 4 different learning rates (2.5E-5, 5E-5, 1E-4, 2E-4).We then choose the best performing learning rate for each configuration and then run each configuration two more times with this learning rate.The result of this exploration is given in Table 9.</p>
<p>Table 9: Ablation tests for the various design choices.Here Normal refers to min-LR/lr=0.1,Weight decay = 0.1 and MLM probability = 0.2, and the opposite dichotomy for the other choices.• Turning off the layer norm prior to the MLPs of all transformer blocks (MLP Layer Norm = False) This change had a significant negative impact on the performance of the model.</p>
<p>• Changing the masking probability to 10% or 30% (default is 20%).Decreasing (resp.increasing) this probability lead to performance deterioration (resp.improvement) in this experiment.However, this seems to be dependent on the dataset as in other instances 30% seems to be too high for effective learning.</p>
<p>• Changing the weight decay to 0.0001 or 1 (default is 0.1).Increasing this value lead to the largest improvement.However, similar to the masking probability, this seems to be dataset dependent.The effect of increased weight decay can also depend on the length of the run.</p>
<p>• Including a bias in the modules of the transformer block (they are absent by default).Including this bias improved performance at the cost of increased variability.</p>
<p>• Turning off the bias in the number head (present by default).This change did not affect the performance significantly.</p>
<p>B.5 LEARNED EMBEDDINGS FOR TEXT-BASED NUMBER ENCODINGS</p>
<p>Figure 9 shows the structure of number embeddings learned on different datasets for different encodings.For P10 the models learn rotary structure which is reminiscent of other works such as grokking (Power et al., 2022), and allows recovering relative numbers from inner products.It is also interesting to see how different datasets can lead to different learned encoding structures, for instance the arithmetic tasks seem to induce a more precise curve structure, while the planet data leads to more spread out embeddings, perhaps because the task is less sensitive to small perturbations of the numbers.</p>
<p>Figure 2 :
2
Figure 2: Value of the embedding of the number x after layer-norm, projected onto the direction of the [NUM] embedding vector.</p>
<p>Figure 3 :
3
Figure 3: Performance of the encoding schemes predicting the temperature of the next timestep.</p>
<p>Figure 4 :
4
Figure 4: A failure mode of text based encoding scheme (left).Because of the distribution of the numbers in the training set (center and right), numbers that are close to ±1 (denoted by the black arrows) get misclassified as 100E-3, i.e. 0.1, the combination of the most common digit and the most common exponent in the dataset.</p>
<p>Figure 5 :
5
Figure 5: Out of distribution generalization properties of the different number encoding schemes.Left: Inferring ∆t, which takes discrete values in the training set.Right: Inferring a 1 which is either 1 or &gt; 1.16 in the training set.Because of the generation procedure, taking a 1 → 1.16 here does not result in an in-train-distribution sample.</p>
<p>Figure5shows that the predictions coming from the text-based encodings can be discontinuous when evaluated out-of-distribution.This discontinuity has two potential sources: the discontinuous nature of the number embeddings and the argmax that is taken over the logits during inference.Since the encodings of the number tokens in text-based encodings have been shown to form continuouslooking structures (seeSec.B.5 and Power et al. (2022);d'Ascoli et al. (2022)), it is possible that the discontinuiuty is only a side effect of the argmax and that the logits themselves vary more smoothly.Figure6shows an example of the logits of the P1000 encoding when predicting the step-size outof-distribution.Here, the color lines denote the highest-value logits, with the other logits carrying negligible weight.The dashed gray lines denote the values of the step-size seen in the training set.We see that these lines are smooth in neither small or larger scales.We expect that this is a combination of the text-based number encodings' discrete embedding schemes together with the cross-entropy training paradigm that does not incorporate number distances into the loss.</p>
<p>Figure 6 :
6
Figure6: example of the logits of model trained with P1000 encoding evaluated on the step-size prediction task.</p>
<p>Figure 7 :
7
Figure 7: Different failure modes of XVAL and text-based encodings.XVAL and P10 perform equally poorly on this task, but their predictions look different.</p>
<p>Figure 9 :
9
Figure 9: Two-dimensional PCA projection of the learned embeddings for mantissa tokens.(left) P10 encoding trained on the planet dataset; (center) P1000 encoding trained on the planet dataset; (right) P1000 encoding trained on the arithmetic dataset.Brighter colors denote higher number values.</p>
<p>Table 1 :
1
Comparison of XVAL with four other number encodings.XVAL is more token-efficient and has a minimal vocabulary footprint.Vocabulary size differs from Charton (2022) because we only consider exponents from 1E-8 to 1E+8.
EncodingTokens−6.02 × 10 1Tokens per number Vocabulary SizeP10{±, d, E±d}[-, 6, 0, 2, E-1]528P1000{±, ddd, E±d}[-, 602, E-1]</p>
<p>Table 2 :
2
Adjusted R 2 scores calculated between predictions and true values for the different encodings on various arithmetic datasets.(Higher is better; R 2 = 1 is the theoretical maximum.)
Encoding 3-digit Multiplication 4-digit Multiplication 5-digit MultiplicationP100.99890.60710.9439P10000.99970.97830.9991B19990.99980.99840.9997FP150.71190.99590.9980XVAL0.99860.99750.9958</p>
<p>Table 3 :
3
Arithmetic evaluation task of random binary trees combining different numbers of operands with addition, subtraction, and multiplication.R 2 measured between true expression value and transformer prediction.
Encoding 2 operands 3 operands 4 operandsP100.9980.9960.992P10000.9910.9900.991FP150.9930.9810.935XVAL0.999980.999940.99998</p>
<p>Table 4 :
4
Performance (in MSE) and runtime of the different encodings on predicting the temperature for the next time step."Equal Samples" columns refer to all models being trained for 500k iterations.Training was performed on 4 Nvidia H100 GPUs using Pytorch Distributed Data Parallelism.
Equal SamplesEqual TokensEqual RuntimeMethod Loss Runtime Loss Runtime Loss RuntimeP10732d 22h732d 22h732d 22hP1000202d 2h233d 10h212d 22hB19992020h192d 23h192d 22hFP152.1419h1.763d 12h1.852d 22hXVAL1.759h1.621d 15h1.512d 22h</p>
<p>B.4.For all of our training runs, we use a cosine learning-rate schedule with warm-up.The scheduler is adjusted such that it reaches the minimum learning rate at the end of the training run.
B FURTHER EXPERIMENTAL DETAILSB.1 TEMPERATURE FORECASTINGB.1.1 EXPERIMENT DETAILSDataset details. The ERA5 dataset (Hersbach et al., 2020) is a high-resolution, state-of-the-artglobal atmospheric reanalysis product provided by the European Centre for Medium-Range WeatherForecasts (ECMWF). It is the fifth generation of ECMWF atmospheric reanalyses and represents thelatest advancement in the ERA (ECMWF Re-Analysis) project. The dataset covers the period from1979 to near-real-time and is updated regularly.In our experiment, we take only the surface temperature of the dataset (field T2m) sampled at 8hour intervals. For each sample, we randomly choose 60-90 of the ∼1-million spatial grid pointsof the dataset, and include 8-16 temperature time points at 8-hour intervals (corresponding to 2-4days), starting from a random time. We generate 1.25 million examples in this way and split it into1 million train, 125 thousand validation and test set samples.{'description':{'coords':[[1,-.32,.95] ... [.96,.61,.79]],'start':[0,1,-.026,-1]}, 'data':[[-2.6,-2.6 ... -3.2,-3.1,-3]]}</p>
<p>Architecture design hyperparameters.Similar to the Temperature Forecasting dataset, for all experiments, we use transformers with 6 transformer blocks, each with 6 heads and each head having width 128, resulting in a embedding width of 768 (43.5M parameters).
{'description':{'planet0':{'m':2.38, 'a':2.96, 'e':1.73},'planet1':{'m':1.35, 'a':2.96, 'e':1.73}, ... , 'stepsize':0.2},'data':[[[2.60,-0.75],[0.81, 0.42]],[[2.63,-0.63],[0.70,0.60]]...]}</p>
<p>Table 8 :
8
Training hyperparameters for the different encodings on the Temperature Forecast dataset.
Encoding Learning Rate Minimum LR Warmup Max Context LengthP1010 −410 −520002707B199910 −410 −520001251P100010 −410 −520001736FP1510 −410 −52000767XVAL2 × 10 −52 × 10 −62000767B.3 ERRATIC BEHAVIOR OF NUMBER ENCODINGS OF UNFIXED LENGTH
For demonstration purposes, we show a few digits per number, but for both scientific datasets, all numbers are floating point numbers. For the text-based encodings, this text string is then processed according to the procedure described above.
In the experiments of the Sec.
, the numbers have three significant figures. Therefore the results of this section are not directly comparable to those of the main text.
ACKNOWLEDGMENTSThe computations in this work were, in part, run at facilities supported by the Scientific Computing Core at the Flatiron Institute, a division of the Simons Foundation.M.P. is supported by the Department of Energy, Office of Science under contract number DE-AC02-05CH11231.from the ratio of the length of each encoding scheme's tokenization length for numbers.The other hyperparameters in this task are given in Table6.The MLPs acting on single stations have 3 hidden layers of width 256.The MLP looking at 60 stations simultaneously is larger to validate that the poorer performance is not because of limited network size.We tried width from 256-8192 and up to 5 layers and the results remain similar.The results of these tests can be seen in Table7.We see that for good performance, it is important for the model to have access to both the time of year as well as the coordinate of the reporting station.However, providing the information for multiple reporting stations at once makes the performance worse.This implies that for the transformer model to be able to predict the temperature with MSE less than 1.7, it needs to properly parse all this information that is scattered across the different parts of the input string.XVAL was the only model to achieve MSE below that of the MLP model (Table4) meaning that it has likely learned to leverage the temperature of other reporting stations as well.B.2 PLANETARY MOTIONDataset details.In this dataset we use the REBOUND N-body simulation codebase(Rein, H. &amp; Liu, S.-F., 2012)and IAS15 integrator(Rein &amp; Spiegel, 2015)to generate a number of planetary systems (with a central mass m ⊙ ≡ 1) and follow their orbits for a number of time points.Each planetary property is drawn from a uniform prior: the number of planets n ∈ [2, 4], mass m/m ⊙ ∈ [10 −5 , 5 • 10 −5 ], semimajor axis equally spaced for the planets between 1 and a f ∈ [1.5, 3] (i.e. if 3 planets and a f = 1.8 then a 1 = 1, a 2 = 1.4 and a 3 = 1.8), eccentricity e ∈ [0, 0.1], and starting Configuration Best Validation Loss Learning Rate Normal (6.8 ± 0.2) × 10 −3 0.0002 min-LR/LR = 0.01 (7.0 ± 0.1) × 10 −3 0.0002 First Layer Norm = False (6.8 ± 0.5) × 10 −3 0.0002 MLP Layer Norm = False (9.0 ± 0.1) × 10 −3 0.0001 MLM probability = 0.1 (8.2 ± 0.6) × 10 −3 0.0002 MLM probability = 0.3 (6.4 ± 0.4) × 10 −3 0.0002 Weight decay = 0.0001 (8.2 ± 0.6) × 10 −3 0.0002 Weight decay = 1(5.3 ± 0.3) × 10 −3 0.0002 Trunk bias = True (6.2 ± 0.4) × 10 −3 0.0002 Num-head bias = False (6.9 ± 0.1) × 10 −3 0.0002We summarize the various configurations that we run this experiments in and their effects as follows:• Ratio of the final learning rate of the cosine scheduler to the initial learning rate (min-LR/LR).We found decreasing this ratio from 0.1 to 0.01 does not affect performance in this experiment.But we found that it does increase stability in longer runs.• Turning off the layer norm prior to the MLP of the first transformer block (First Layer Norm = False).This change does not affect average performance.This is not surprising since the effect of the layer norm at this stage is simply to normalize the numbers and the numbers in this dataset are in the regime where the normalization discussed in Sec.2.1 is linear.
Exploring Length Generalization in Large Language Models. Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, Behnam Neyshabur, 2022</p>
<p>A Categorical Archive of ChatGPT Failures, 2023. Franc ¸ois Charton. Linear algebra with transformers. Ali Borji, 2022</p>
<p>Question directed graph attention network for numerical reasoning over text. Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xiaochuan, Yuyu Zhang, Le Song, Taifeng Wang, Yuan Qi, Wei Chu, 2020a</p>
<p>Generative Pretraining From Pixels. Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, Ilya Sutskever, Proceedings of the 37th International Conference on Machine Learning. Hal Daumé, Iii , Aarti Singh, the 37th International Conference on Machine LearningPMLRJul 2020b119</p>
<p>7 revealing ways ais fail: Neural networks can be disastrously brittle, forgetful, and surprisingly bad at math. Q Charles, Choi, 10.1109/MSPEC.2021.9563958IEEE Spectrum. 58102021</p>
<p>. Jade Copet, Simple and Controllable Music GenerationFelix Kreuk, Simple and Controllable Music GenerationItai Gat, Simple and Controllable Music GenerationTal Remez, Simple and Controllable Music GenerationDavid Kant, Simple and Controllable Music GenerationGabriel Synnaeve, Simple and Controllable Music GenerationYossi Adi, Simple and Controllable Music GenerationAlexandre Défossez, Simple and Controllable Music Generation2023</p>
<p>Pierre-Alexandre Stéphane D'ascoli, Kamienny, Guillaume Lample, and Franc ¸ois Charton. Deep Symbolic Regression for Recurrent Sequences. 2022</p>
<p>An image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, ICLR. 2020</p>
<p>Faith and fate: Limits of transformers on compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jiang, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, 2023</p>
<p>Mathematical capabilities of chatgpt, 2023. Philip Gage. A new algorithm for data compression. Simon Frieder, Luca Pinchetti, Alexis Chevalier, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz, Philipp Christian Petersen, Julius Berner, C Users J. 0898-9788122feb 1994</p>
<p>What can transformers learn in-context? a case study of simple function classes. Shivam Garg, Dimitris Tsipras, Percy S Liang, Gregory Valiant, Advances in Neural Information Processing Systems. 2022</p>
<p>Studying Large Language Model Generalization with Influence Functions. Roger Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamilė Lukošiūtė, Karina Nguyen, Nicholas Joseph, Sam Mccandlish, Jared Kaplan, Samuel R Bowman, 2023</p>
<p>Gabor Radnoti, Patricia de Rosnay, Iryna Rozum, Freja Vamborg, Sebastien Villaume, and Jean-Noël Thépaut. The era5 global reanalysis. Hans Hersbach, Bill Bell, Paul Berrisford, Shoji Hirahara, András Horányi, Joaquín Muñoz-Sabater, Julien Nicolas, Carole Peubey, Raluca Radu, Dinand Schepers, Adrian Simmons, Cornel Soci, Saleh Abdalla, Xavier Abellan, Gianpaolo Balsamo, Peter Bechtold, Gionata Biavati, Jean Bidlot, Massimo Bonavita, Giovanna De Chiara, Per Dahlgren, Dick Dee, Michail Diamantakis, Rossana Dragani, Johannes Flemming, Richard Forbes, Manuel Fuentes, Alan Geer, Leo Haimberger, Sean Healy, Robin J Hogan, Elías Hólm, Marta Janisková, Sarah Keeley, Patrick Laloyaux, Philippe Lopez, Cristina Lupu, 10.1002/qj.3803Quarterly Journal of the Royal Meteorological Society. 1467302020</p>
<p>Learning numeral embeddings. Chengyue Jiang, Zhonglin Nian, Kaihao Guo, Shanbo Chu, Yinggong Zhao, Libin Shen, Kewei Tu, 2020</p>
<p>Health systemscale language models are all-purpose prediction engines. Yao Lavender, Xujin Jiang, Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Antony Howard, Ilya Riina, Paawan Laufer, Punjabi, Nature. 2023</p>
<p>Solving quantitative reasoning problems with language models. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag. Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra2022</p>
<p>Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, Cyril Zhang, arXiv:2210.10749Transformers learn shortcuts to automata. 2022arXiv preprint</p>
<p>. OpenAI. Gpt-4 technical report. 2023</p>
<p>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra, 2022</p>
<p>The NLP Task Effectiveness of Long-Range Transformers. Guanghui Qin, Yukun Feng, Benjamin Van Durme, 2023</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>IAS15: a fast, adaptive, high-order integrator for gravitational dynamics, accurate to machine precision over a billion orbits. Hanno Rein, David S Spiegel, 10.1093/mnras/stu2164Monthly Notices of the Royal Astronomical Society. 4462January 2015</p>
<p>Rebound: an open-source multi-purpose n-body code for collisional dynamics. H Rein, S.-F Liu, 10.1051/0004-6361/201118085A&amp;A. 537A1282012</p>
<p>Numeracy for language models: Evaluating and improving their ability to predict numbers. Georgios Spithourakis, Sebastian Riedel, 10.18653/v1/p18-1196Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 56th Annual Meeting of the Association for Computational Linguistics20181Association for Computational Linguistics</p>
<p>Methods for numeracy-preserving word embeddings. Dhanasekar Sundararaman, Shijing Si, Vivek Subramanian, Guoyin Wang, Devamanyu Hazarika, Lawrence Carin, 10.18653/v1/2020.emnlp-main.384Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsNovember 2020</p>
<p>Can neural networks do arithmetic? a survey on the elementary numerical skills of state-of-the-art deep learning models. Alberto Testolin, 2023</p>
<p>Representing numbers in nlp: a survey and a vision. Avijit Thawani, Jay Pujara, Pedro A Szekely, Filip Ilievski, 2021</p>
<p>An Empirical Study on Robustness to Spurious Correlations using Pre-trained Language Models. Lifu Tu, Garima Lalwani, CoRR, abs/2007.06778Spandana Gella, and He He. 2020</p>
<p>Solving math word problems with process-and outcome-based feedback. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, Irina Higgins, 2022</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 2023</p>
<p>Do NLP models know numbers? probing numeracy in embeddings. Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, Matt Gardner, 10.18653/v1/D19-1534Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsNovember 2019</p>
<p>All of nonparametric statistics. Larry Wasserman, 2006Springer Science &amp; Business Media</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-958 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-958</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-958</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-23.html">extraction-schema-23</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods that use probabilistic symbolic world models (such as PPDDL, PDDL, or belief states) for planning in text-based environments, especially those that integrate uncertainty from large language models.</div>
                <p><strong>Paper ID:</strong> paper-36679c13bb3ab945746b75e0abb37767eacf5394</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/36679c13bb3ab945746b75e0abb37767eacf5394" target="_blank">Value-Function Approximations for Partially Observable Markov Decision Processes</a></p>
                <p><strong>Paper Venue:</strong> Journal of Artificial Intelligence Research</p>
                <p><strong>Paper TL;DR:</strong> This work surveys various approximation methods, analyzes their properties and relations and provides some new insights into their differences, and presents a number of new approximation methods and novel refinements of existing techniques.</p>
                <p><strong>Paper Abstract:</strong> Partially observable Markov decision processes (POMDPs) provide an elegant mathematical framework for modeling complex decision and planning problems in stochastic domains in which states of the system are observable only indirectly, via a set of imperfect or noisy observations. The modeling advantage of POMDPs, however, comes at a price -- exact methods for solving them are computationally very expensive and thus applicable in practice only to very simple problems. We focus on efficient approximation (heuristic) methods that attempt to alleviate the computational problem and trade off accuracy for speed. We have two objectives here. First, we survey various approximation methods, analyze their properties and relations and provide some new insights into their differences. Second, we present a number of new approximation methods and novel refinements of existing techniques. The theoretical results are supported by experiments on a problem from the agent navigation domain.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-958",
    "paper_id": "paper-36679c13bb3ab945746b75e0abb37767eacf5394",
    "extraction_schema_id": "extraction-schema-23",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005735249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Value-Function Approximations for Partially Observable Markov Decision Processes</h1>
<p>Milos Hauskrecht<br>MILOS@CS.BROWN.EDU<br>Computer Science Department, Brown University<br>Box 1910, Brown University, Providence, RI 02912, USA</p>
<h4>Abstract</h4>
<p>Partially observable Markov decision processes (POMDPs) provide an elegant mathematical framework for modeling complex decision and planning problems in stochastic domains in which states of the system are observable only indirectly, via a set of imperfect or noisy observations. The modeling advantage of POMDPs, however, comes at a price exact methods for solving them are computationally very expensive and thus applicable in practice only to very simple problems. We focus on efficient approximation (heuristic) methods that attempt to alleviate the computational problem and trade off accuracy for speed. We have two objectives here. First, we survey various approximation methods, analyze their properties and relations and provide some new insights into their differences. Second, we present a number of new approximation methods and novel refinements of existing techniques. The theoretical results are supported by experiments on a problem from the agent navigation domain.</p>
<h2>1. Introduction</h2>
<p>Making decisions in dynamic environments requires careful evaluation of the cost and benefits not only of the immediate action but also of choices we may have in the future. This evaluation becomes harder when the effects of actions are stochastic, so that we must pursue and evaluate many possible outcomes in parallel. Typically, the problem becomes more complex the further we look into the future. The situation becomes even worse when the outcomes we can observe are imperfect or unreliable indicators of the underlying process and special actions are needed to obtain more reliable information. Unfortunately, many real-world decision problems fall into this category.</p>
<p>Consider, for example, a problem of patient management. The patient comes to the hospital with an initial set of complaints. Only rarely do these allow the physician (decisionmaker) to diagnose the underlying disease with certainty, so that a number of disease options generally remain open after the initial evaluation. The physician has multiple choices in managing the patient. He/she can choose to do nothing (wait and see), order additional tests and learn more about the patient state and disease, or proceed to a more radical treatment (e.g. surgery). Making the right decision is not an easy task. The disease the patient suffers can progress over time and may become worse if the window of opportunity for a particular effective treatment is missed. On the other hand, selection of the wrong treatment may make the patient's condition worse, or may prevent applying the correct treatment later. The result of the treatment is typically non-deterministic and more outcomes are possible. In addition, both treatment and investigative choices come with different costs. Thus, in</p>
<p>a course of patient management, the decision-maker must carefully evaluate the costs and benefits of both current and future choices, as well as their interaction and ordering. Other decision problems with similar characteristics - complex temporal cost-benefit tradeoffs, stochasticity, and partial observability of the underlying controlled process - include robot navigation, target tracking, machine mantainance and replacement, and the like.</p>
<p>Sequential decision problems can be modeled as Markov decision processes (MDPs) (Bellman, 1957; Howard, 1960; Puterman, 1994; Boutilier, Dean, \&amp; Hanks, 1999) and their extensions. The model of choice for problems similar to patient management is the partially observable Markov decision process (POMDP) (Drake, 1962; Astrom, 1965; Sondik, 1971; Lovejoy, 1991b). The POMDP represents two sources of uncertainty: stochasticity of the underlying controlled process (e.g. disease dynamics in the patient management problem), and imperfect observability of its states via a set of noisy observations (e.g. symptoms, findings, results of tests). In addition, it lets us model in a uniform way both control and information-gathering (investigative) actions, as well as their effects and cost-benefit tradeoffs. Partial observability and the ability to model and reason with information-gathering actions are the main features that distinguish the POMDP from the widely known fully observable Markov decision process (Bellman, 1957; Howard, 1960).</p>
<p>Although useful from the modeling perspective, POMDPs have the disadvantage of being hard to solve (Papadimitriou \&amp; Tsitsiklis, 1987; Littman, 1996; Mundhenk, Goldsmith, Lusena, \&amp; Allender, 1997; Madani, Hanks, \&amp; Condon, 1999), and optimal or $\epsilon$-optimal solutions can be obtained in practice only for problems of low complexity. A challenging goal in this research area is to exploit additional structural properties of the domain and/or suitable approximations (heuristics) that can be used to obtain good solutions more efficiently.</p>
<p>We focus here on heuristic approximation methods, in particular approximations based on value functions. Important research issues in this area are the design of new and efficient algorithms, as well as a better understanding of the existing techniques and their relations, advantages and disadvantages. In this paper we address both of these issues. First, we survey various value-function approximations, analyze their properties and relations and provide some insights into their differences. Second, we present a number of new methods and novel refinements of existing techniques. The theoretical results and findings are also supported empirically on a problem from the agent navigation domain.</p>
<h1>2. Partially Observable Markov Decision Processes</h1>
<p>A partially observable Markov decision process (POMDP) describes a stochastic control process with partially observable (hidden) states. Formally, it corresponds to a tuple $(S, A, \Theta, T, O, R)$ where $S$ is a set of states, $A$ is a set of actions, $\Theta$ is a set of observations, $T: S \times A \times S \rightarrow[0,1]$ is a set of transition probabilities that describe the dynamic behavior of the modeled environment, $O: S \times A \times \Theta \rightarrow[0,1]$ is a set of observation probabilities that describe the relationships among observations, states and actions, and $R: S \times A \times S \rightarrow \mathbb{R}$ denotes a reward model that assigns rewards to state transitions and models payoffs associated with such transitions. In some instances the definition of a POMDP also includes an a priori probability distribution over the set of initial states $S$.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Part of the influence diagram describing a POMDP model. Rectangles correspond to decision nodes (actions), circles to random variables (states) and diamonds to reward nodes. Links represent the dependencies among the components. $s_{t}, a_{t}, o_{t}$ and $r_{t}$ denote state, action, observation and reward at time $t$. Note that an action at time $t$ depends only on past observations and actions, not on states.</p>
<h1>2.1 Objective Function</h1>
<p>Given a POMDP, the goal is to construct a control policy that maximizes an objective (value) function. The objective function combines partial (stepwise) rewards over multiple steps using various kinds of decision models. Typically, the models are cumulative and based on expectations. Two models are frequently used in practice:</p>
<ul>
<li>a finite-horizon model in which we maximize $E\left(\sum_{t=0}^{T} r_{t}\right)$, where $r_{t}$ is a reward obtained at time $t$.</li>
<li>an infinite-horizon discounted model in which we maximize $E\left(\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right)$, where $0&lt;$ $\gamma&lt;1$ is a discount factor.</li>
</ul>
<p>Note that POMDPs and cumulative decision models provide a rich language for modeling various control objectives. For example, one can easily model goal-achievement tasks (a specific goal must be reached) by giving a large reward for a transition to that state and zero or smaller rewards for other transitions.</p>
<p>In this paper we focus primarily on discounted infinite-horizon model. However, the results can be easily applied also to the finite-horizon case.</p>
<h3>2.2 Information State</h3>
<p>In a POMDP the process states are hidden and we cannot observe them while making a decision about the next action. Thus, our action choices are based only on the information available to us or on quantities derived from that information. This is illustrated in the influence diagram in Figure 1, where the action at time $t$ depends only on previous observations and actions, not on states. Quantities summarizing all information are called information states. Complete information states represent a trivial case.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Influence diagram for a POMDP with information states and corresponding information-state MDP. Information states ( $I_{t}$ and $I_{t+1}$ ) are represented by double-circled nodes. An action choice (rectangle) depends only on the current information state.</p>
<p>Definition 1 (Complete information state). The complete information state at time $t$ (denoted $I_{t}^{C}$ ) consists of:</p>
<ul>
<li>a prior belief $b_{0}$ on states in $S$ at time 0 ;</li>
<li>a complete history of actions and observations $\left{o_{0}, a_{0}, o_{1}, a_{1}, \cdots, o_{t-1}, a_{t-1}, o_{t}\right}$ starting from time $t=0$.</li>
</ul>
<p>A sequence of information states defines a controlled Markov process that we call an information-state Markov decision process or information-state MDP. The policy for the information-state MDP is defined in terms of a control function $\mu: \mathcal{I} \rightarrow A$ mapping information state space to actions. The new information state $\left(I_{t}\right)$ is a deterministic function of the previous state $\left(I_{t-1}\right)$, the last action $\left(a_{t-1}\right)$ and the new observation $\left(o_{t}\right)$ :</p>
<p>$$
I_{t}=\tau\left(I_{t-1}, o_{t}, a_{t-1}\right)
$$</p>
<p>$\tau: \mathcal{I} \times \Theta \times A \rightarrow \mathcal{I}$ is the update function mapping the information state space, observations and actions back to the information space. ${ }^{1}$ It is easy to see that one can always convert the original POMDP into the information-state MDP by using complete information states. The relation between the components of the two models and a sketch of a reduction of a POMDP to an information-state MDP, are shown in Figure 2.</p>
<h1>2.3 Bellman Equations for POMDPs</h1>
<p>An information-state MDP for the infinite-horizon discounted case is like a fully-observable MDP and satisfies the standard fixed-point (Bellman) equation:</p>
<p>$$
V^{<em>}(I)=\max <em I_prime="I^{\prime">{a \in A}\left{\rho(I, a)+\gamma \sum</em> \mid I, a\right) V^{}} P\left(I^{\prime</em>}\left(I^{\prime}\right)\right}
$$</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Here, $V^{*}(I)$ denotes the optimal value function maximizing $E\left(\sum_{t=0}^{\infty} \gamma^{t} r_{t}\right)$ for state $I . \rho(I, a)$ is the expected one-step reward and equals</p>
<p>$$
\rho(I, a)=\sum_{s \in S} \rho(s, a) P(s \mid I)=\sum_{s \in S} \sum_{s^{\prime} \in S} R\left(s, a, s^{\prime}\right) P\left(s^{\prime} \mid s, a\right) P(s \mid I)
$$</p>
<p>$\rho(s, a)$ denotes an expected one-step reward for state $s$ and action $a$.
Since the next information state $I^{\prime}=\tau(I, o, a)$ is a deterministic function of the previous information state $I$, action $a$, and the observation $o$, the Equation 1 can be rewritten more compactly by summing over all possible observations $\Theta$ :</p>
<p>$$
V^{<em>}(I)=\max <em S="S" _in="\in" s="s">{a \in A}\left{\sum</em> P(o \mid I, a) V^{} \rho(s, a) P(s \mid I)+\gamma \sum_{o \in \Theta</em>}(\tau(I, o, a))\right}
$$</p>
<p>The optimal policy (control function) $\mu^{*}: \mathcal{I} \rightarrow A$ selects the value-maximizing action</p>
<p>$$
\mu^{<em>}(I)=\arg \max <em S="S" _in="\in" s="s">{a \in A}\left{\sum</em> P(o \mid I, a) V^{} \rho(s, a) P(s \mid I)+\gamma \sum_{o \in \Theta</em>}(\tau(I, o, a))\right}
$$</p>
<p>The value and control functions can be also expressed in terms of action-value functions ( $Q$-functions)</p>
<p>$$
\begin{aligned}
&amp; V^{<em>}(I)=\max _{a \in A} Q^{</em>}(I, a) \quad \mu^{<em>}(I)=\arg \max _{a \in A} Q^{</em>}(I, a) \
&amp; Q^{<em>}(I, a)=\sum_{s \in S} \rho(s, a) P(s \mid I)+\gamma \sum_{o \in \Theta} P(o \mid I, a) V^{</em>}(\tau(I, o, a))
\end{aligned}
$$</p>
<p>A $Q$-function corresponds to the expected reward for chosing a fixed action (a) in the first step and acting optimally afterwards.</p>
<h1>2.3.1 SUFFICIENT StatisticS</h1>
<p>To derive Equations $1-3$ we implicitly used complete information states. However, as remarked earlier, the information available to the decision-maker can be also summarized by other quantities. We call them sufficient information states. Such states must preserve the necessary information content and also the Markov property of the information-state decision process.</p>
<p>Definition 2 (Sufficient information state process). Let $\mathcal{I}$ be an information state space and $\tau: \mathcal{I} \times A \times \Theta \rightarrow \mathcal{I}$ be an update function defining an information process $I_{t}=$ $\tau\left(I_{t-1}, a_{t-1}, o_{t}\right)$. The process is sufficient with regard to the optimal control when, for any time step $t$, it satisfies</p>
<p>$$
\begin{aligned}
P\left(s_{t} \mid I_{t}\right) &amp; =P\left(s_{t} \mid I_{t}^{C}\right) \
P\left(o_{t} \mid I_{t-1}, a_{t-1}\right) &amp; =P\left(o_{t} \mid I_{t-1}^{C}, a_{t-1}\right)
\end{aligned}
$$</p>
<p>where $I_{t}^{C}$ and $I_{t-1}^{C}$ are complete information states.
It is easy to see that Equations $1-3$ for complete information states must hold also for sufficient information states. The key benefit of sufficient statistics is that they are often</p>
<p>easier to manipulate and store, since unlike complete histories, they may not expand with time. For example, in the standard POMDP model it is sufficient to work with belief states that assign probabilities to every possible process state (Astrom, 1965). ${ }^{2}$ In this case the Bellman equation reduces to:</p>
<p>$$
V(b)=\max <em S="S" _in="\in" s="s">{a \in A}\left{\sum</em> P(o \mid s, a) b(s) V(\tau(b, o, a))\right}
$$} \rho(s, a) b(s)+\gamma \sum_{o \in \Theta} \sum_{s \in S</p>
<p>where the next-step belief state $b^{\prime}$ is</p>
<p>$$
b^{\prime}(s)=\tau(b, o, a)(s)=\beta P(o \mid s, a) \sum_{s^{\prime} \in S} P\left(s \mid a, s^{\prime}\right) b\left(s^{\prime}\right)
$$</p>
<p>$\beta=1 / P(o \mid b, a)$ is a normalizing constant. This defines a belief-state $M D P$ which is a special case of a continuous-state MDP. Belief-state MDPs are also the primary focus of our investigation in this paper.</p>
<h1>2.3.2 Value-Function Mappings and their Properties</h1>
<p>The Bellman equation 2 for the belief-state MDP can be also rewritten in the value-function mapping form. Let $\mathcal{V}$ be a space of real-valued bounded functions $V: \mathcal{I} \rightarrow \mathbb{R}$ defined on the belief information space $\mathcal{I}$, and let $h: \mathcal{I} \times A \times B \rightarrow \mathbb{R}$ be defined as</p>
<p>$$
h(b, a, V)=\sum_{s \in S} \rho(s, a) b(s)+\gamma \sum_{o \in \Theta} \sum_{s \in S} P(o \mid s, a) b(s) V(\tau(b, o, a))
$$</p>
<p>Now by defining the value function mapping $H: \mathcal{V} \rightarrow \mathcal{V}$ as $(H V)(b)=\max _{a \in A} h(b, a, V)$, the Bellman equation 2 for all information states can be written as $V^{<em>}=H V^{</em>}$. It is well known that $H$ (for MDPs) is an isotone mapping and that it is a contraction under the supremum norm (see (Heyman \&amp; Sobel, 1984; Puterman, 1994)).</p>
<p>Definition 3 The mapping $H$ is isotone, if $V, U \in \mathcal{V}$ and $V \leq U$ implies $H V \leq H U$.
Definition 4 Let $|\cdot|$ be a supremum norm. The mapping $H$ is a contraction under the supremum norm, if for all $V, U \in \mathcal{V},|H V-H U| \leq \beta|V-U|$ holds for some $0 \leq \beta&lt;1$.</p>
<h3>2.4 Value Iteration</h3>
<p>The optimal value function (Equation 2) or its approximation can be computed using $d y$ namic programming techniques. The simplest approach is the value iteration (Bellman, 1957) shown in Figure 3. In this case, the optimal value function $V^{*}$ can be determined in the limit by performing a sequence of value-iteration steps $V_{i}=H V_{i-1}$, where $V_{i}$ is the $i$ th approximation of the value function ( $i$ th value function). ${ }^{3}$ The sequence of estimates</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Value iteration $(P O M D P, \epsilon)$
initialize $V$ for all $b \in \mathcal{I}$;
repeat
$V^{\prime} \leftarrow V ;$
update $V \leftarrow H V^{\prime}$ for all $b \in \mathcal{I}$;
until $\sup _{b}\left|V(b)-V^{\prime}(b)\right| \leq \epsilon$
return V;</p>
<p>Figure 3: Value iteration procedure.
converges to the unique fixed-point solution which is the direct consequence of Banach's theorem for contraction mappings (see, for example, Puterman (1994)).</p>
<p>In practice, we stop the iteration well before it reaches the limit solution. The stopping criterion we use in our algorithm (Figure 3) examines the maximum difference between value functions obtained in two consecutive steps - the so-called Bellman error (Puterman, 1994; Littman, 1996). The algorithm stops when this quantity falls below the threshold $\epsilon$. The accuracy of the approximate solution ( $i$ th value function) with regard to $V^{*}$ can be expressed in terms of the Bellman error $\epsilon$.</p>
<p>Theorem 1 Let $\epsilon=\sup <em i="i">{b}\left|V</em>-V^{}(b)-V_{i-1}(b)\right|=\left|V_{i}-V_{i-1}\right|$ be the magnitude of the Bellman error. Then $\left|V_{i<em>}\right| \leq \frac{\gamma \varepsilon}{1-\gamma}$ and $\left|V_{i-1}-V^{</em>}\right| \leq \frac{\varepsilon}{1-\gamma}$ hold.</p>
<p>Then, to obtain the approximation of $V^{*}$ with precision $\delta$ the Bellman error should fall below $\frac{\delta(1-\gamma)}{\gamma}$.</p>
<h1>2.4.1 Piecewise Linear and Convex Approximations of the Value Function</h1>
<p>The major difficulty in applying the value iteration (or dynamic programming) to beliefstate MDPs is that the belief space is infinite and we need to compute an update $V_{i}=H V_{i-1}$ for all of it. This poses the following threats: the value function for the $i$ th step may not be representable by finite means and/or computable in a finite number of steps.</p>
<p>To address this problem Sondik (Sondik, 1971; Smallwood \&amp; Sondik, 1973) showed that one can guarantee the computability of the $i$ th value function as well as its finite description for a belief-state MDP by considering only piecewise linear and convex representations of value function estimates (see Figure 4). In particular, Sondik showed that for a piecewise linear and convex representation of $V_{i-1}, V_{i}=H V_{i-1}$ is computable and remains piecewise linear and convex.</p>
<p>Theorem 2 (Piecewise linear and convex functions). Let $V_{0}$ be an initial value function that is piecewise linear and convex. Then the ith value function obtained after a finite number of update steps for a belief-state MDP is also finite, piecewise linear and convex, and is equal to:</p>
<p>$$
V_{i}(b)=\max <em i="i">{\alpha</em>(s)
$$} \in \Gamma_{i}} \sum_{s \in S} b(s) \alpha_{i</p>
<p>where $b$ and $\alpha_{i}$ are vectors of size $|S|$ and $\Gamma_{i}$ is a finite set of vectors (linear functions) $\alpha_{i}$.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: A piecewise linear and convex function for a POMDP with two process states $\left{s_{1}, s_{2}\right}$. Note that $b\left(s_{1}\right)=1-b\left(s_{2}\right)$ holds for any belief state.</p>
<p>The key part of the proof is that we can express the update for the $i$ th value function in terms of linear functions $\Gamma_{i-1}$ defining $V_{i-1}$ :</p>
<p>$$
V_{i}(b)=\max <em S="S" _in="\in" s="s">{a \in A}\left{\sum</em> \max } \rho(s, a) b(s)+\gamma \sum_{o \in \Theta<em i-1="i-1">{\alpha</em>\right)\right}
$$} \in \Gamma_{i-1}} \sum_{s^{\prime} \in S}\left[\sum_{s \in S} P\left(s^{\prime}, o \mid s, a\right) b(s)\right] \alpha_{i-1}\left(s^{\prime</p>
<p>This leads to a piecewise linear and convex value function $V_{i}$ that can be represented by a finite set of linear functions $\alpha_{i}$, one linear function for every combination of actions and permutations of $\alpha_{i-1}$ vectors of size $|\Theta|$. Let $W=\left(a,\left{o_{1}, \alpha_{i-1}^{j_{1}}\right},\left{o_{2}, \alpha_{i-1}^{j_{2}}\right}, \cdots\left{o_{|\Theta|}, \alpha_{i-1}^{j_{|\Theta|}}\right}\right)$ be such a combination. Then the linear function corresponding to it is defined as</p>
<p>$$
\alpha_{i}^{W}(s)=\rho(s, a)+\gamma \sum_{o \in \Theta} \sum_{s^{\prime} \in S} P\left(s^{\prime}, o \mid s, a\right) \alpha_{i-1}^{j_{o}}\left(s^{\prime}\right)
$$</p>
<p>Theorem 2 is the basis of the dynamic programming algorithm for finding the optimal solution for the finite-horizon models and the value-iteration algorithm for finding nearoptimal approximations of $V^{<em>}$ for the discounted, infinite-horizon model. Note, however, that this result does not imply piecewise linearity of the optimal (fixed-point) solution $V^{</em>}$.</p>
<h1>2.4.2 Algorithms for Computing Value-Function Updates</h1>
<p>The key part of the value-iteration algorithm is the computation of value-function updates $V_{i}=H V_{i-1}$. Assume an $i$ th value function $V_{i}$ that is represented by a finite number of linear segments ( $\alpha$ vectors). The total number of all its possible linear functions is $|A|\left|\Gamma_{i-1}\right|^{|\Theta|}$ (one for every combination of actions and permutations of $\alpha_{i-1}$ vectors of size $|\Theta|$ ) and they can be enumerated in $O\left(|A||S|^{2}\left|\Gamma_{i-1}\right|^{|\Theta|}\right)$ time. However, the complete set of linear functions is rarely needed: some of the linear functions are dominated by others and their omission does not change the resulting piecewise linear and convex function. This is illustrated in Figure 5.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Redundant linear function. The function does not dominate in any of the regions of the belief space and can be excluded.</p>
<p>A linear function that can be eliminated without changing the resulting value function solution is called redundant. Conversely, a linear function that singlehandedly achieves the optimal value for at least one point of the belief space is called useful. ${ }^{4}$</p>
<p>For the sake of computational efficiency it is important to make the size of the linear function set as small as possible (keep only useful linear functions) over value-iteration steps. There are two main approaches for computing useful linear functions. The first approach is based on a generate-and-test paradigm and is due to Sondik (1971) and Monahan (1982). The idea here is to enumerate all possible linear functions first, then test the usefulness of linear functions in the set and prune all redundant vectors. Recent extensions of the method interleave the generate and test stages and do early pruning on a set of partially constructed linear functions (Zhang \&amp; Liu, 1997a; Cassandra, Littman, \&amp; Zhang, 1997; Zhang \&amp; Lee, 1998).</p>
<p>The second approach builds on Sondik's idea of computing a useful linear function for a single belief state (Sondik, 1971; Smallwood \&amp; Sondik, 1973), which can be done efficiently. The key problem here is to locate all belief points that seed useful linear functions and different methods address this problem differently. Methods that implement this idea are Sondik's one- and two-pass algorithms (Sondik, 1971), Cheng's methods (Cheng, 1988), and the Witness algorithm (Kaelbling, Littman, \&amp; Cassandra, 1999; Littman, 1996; Cassandra, 1998).</p>
<h1>2.4.3 Limitations and Complexity</h1>
<p>The major difficulty in solving a belief-state MDP is that the complexity of a piecewise linear and convex function can grow extremely fast with the number of update steps. More specifically, the size of a linear function set defining the function can grow exponentially (in the number of observations) during a single update step. Then, assuming that the initial value function is linear, the number of linear functions defining the $i$ th value function is $O\left(|A|^{|\Theta|^{i-1}}\right)$.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The potential growth of the size of the linear function set is not the only bad news. As remarked earlier, a piecewise linear convex value function is usually less complex than the worst case because many linear functions can be pruned away during updates. However, it turned out that the task of identifying all useful linear functions is computationally intractable as well (Littman, 1996). This means that one faces not only the potential super-exponential growth of the number of useful linear functions, but also inefficiencies related to the identification of such vectors. This is a significant drawback that makes the exact methods applicable only to relatively simple problems.</p>
<p>The above analysis suggests that solving a POMDP problem is an intrinsically hard task. Indeed, finding the optimal solution for the finite-horizon problem is PSPACE-hard (Papadimitriou \&amp; Tsitsiklis, 1987). Finding the optimal solution for the discounted infinitehorizon criterion is even harder. The corresponding decision problem has been shown to be undecidable (Madani et al., 1999), and thus the optimal solution may not be computable.</p>
<h1>2.4.4 Structural Refinements of the Basic Algorithm</h1>
<p>The standard POMDP model uses a flat state space and full transition and reward matrices. However, in practice, problems often exhibit more structure and can be represented more compactly, for example, using graphical models (Pearl, 1988; Lauritzen, 1996), most often dynamic belief networks (Dean \&amp; Kanazawa, 1989; Kjaerulff, 1992) or dynamic influence diagrams (Howard \&amp; Matheson, 1984; Tatman \&amp; Schachter, 1990). ${ }^{5}$ There are many ways to take advantage of the problem structure to modify and improve exact algorithms. For example, a refinement of the basic Monahan algorithm to compact transition and reward models has been studied by Boutilier and Poole (1996). A hybrid framework that combines MDP-POMDP problem-solving techniques to take advantage of perfectly and partially observable components of the model and the subsequent value function decomposition was proposed by Hauskrecht (1997, 1998, 2000). A similar approach with perfect information about a region (subset of states) containing the actual underlying state was discussed by Zhang and Liu (1997b, 1997a). Finally, Casta√±on (1997) and Yost (1998) explore techniques for solving large POMDPs that consist of a set of smaller, resource-coupled but otherwise independent POMDPs.</p>
<h3>2.5 Extracting Control Strategy</h3>
<p>Value iteration allow us to compute an $i$ th approximation of the value function $V_{i}$. However, our ulimate goal is to find the optimal control strategy $\mu^{*}: \mathcal{I} \rightarrow A$ or its close approximation. Thus our focus here is on the problem of extraction of control strategies from the results of value iteration.</p>
<h3>2.5.1 Lookahead Design</h3>
<p>The simplest way to define the control function $\mu: \mathcal{I} \rightarrow A$ from the value function $V_{i}$ is via greedy one-step lookahead:</p>
<p>$$
\mu(b)=\arg \max <em S="S" _in="\in" s="s">{a \in A}\left{\sum</em>(\tau(b, o, a))\right}
$$} \rho(s, a) b(s)+\gamma \sum_{o \in \Theta} P(o \mid b, a) V_{i</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Direct control design. Every linear function defining $V_{i}$ is associated with an action. The action is selected if its linear function (or Q-function) is maximal.</p>
<p>As $V_{i}$ represents only the $i$ th approximation of the optimal value function, the question arises how good the resulting controller really is. ${ }^{6}$ The following theorem (Puterman, 1994; Williams \&amp; Baird, 1994; Littman, 1996) relates the accuracy of the (lookahead) controller and the Bellman error.</p>
<p>Theorem 3 Let $\epsilon=\left|V_{i}-V_{i-1}\right|$ be the magnitude of the Bellman error. Let $V_{i}^{L A}$ be the expected reward for the lookahead controller designed for $V_{i}$. Then $\left|V_{i}^{L A}-V^{*}\right| \leq \frac{2 \epsilon \gamma}{1-\gamma}$.</p>
<p>The bound can be used to construct the value-iteration routine that yields a lookahead strategy with a minimum required precision. The result can be also extended to the $k$ step lookahead design in a straightforward way; with $k$ steps, the error bound becomes $\left|V_{i}^{L A(k)}-V^{*}\right| \leq \frac{2 \epsilon \gamma^{k}}{(1-\gamma)}$.</p>
<h1>2.5.2 Direct Design</h1>
<p>To extract the control action via lookahead essentially requires computing one full update. Obviously, this can lead to unwanted delays in reaction times. In general, we can speed up the response by remembering and using additional information. In particular, every linear function defining $V_{i}$ is associated with the choice of action (see Equation 7). The action is a byproduct of methods for computing linear functions and no extra computation is required to find it. Then the action corresponding to the best linear function can be selected directly for any belief state. The idea is illustrated in Figure 6.</p>
<p>The bound on the accuracy of the direct controller for the infinite-horizon case can be once again derived in terms of the magnitude of the Bellman error.</p>
<p>Theorem 4 Let $\epsilon=\left|V_{i}-V_{i-1}\right|$ be the magnitude of the Bellman error. Let $V_{i}^{D R}$ be an expected reward for the direct controller designed for $V_{i}$. Then $\left|V_{i}^{D R}-V^{*}\right| \leq \frac{2 \epsilon}{1-\gamma}$.</p>
<p>The direct action choice is closely related to the notion of action-value function (or Q-function). Analogously to Equation 4, the $i$ th Q-function satisfies</p>
<p>$$
V_{i}(b)=\max <em i="i">{a \in A} Q</em>(b, a)
$$</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: A policy graph (finite-state machine) obtained after two value iteration steps. Nodes correspond to linear functions (or states of the finite-state machine) and links to dependencies between linear functions (transitions between states). Every linear function (node) is associated with an action. To ensure that the policy can be also applied to the infinite-horizon problem, we add a cycle to the last state (dashed line).</p>
<p>$$
Q_{i}(b, a)=R(b, a)+\gamma \sum_{o \in \Theta} P(o \mid b, a) V_{i-1}(\tau(b, a, o))
$$</p>
<p>From this perspective, the direct strategy selects the action with the best (maximum) Qfunction for a given belief state. ${ }^{7}$</p>
<h1>2.5.3 Finite-State Machine Design</h1>
<p>A more complex refinement of the above technique is to remember, for every linear function in $V_{i}$, not only the action choice but also the choice of a linear function for the previous step and to do this for all observations (see Equation 7). As the same idea can be applied recursively to the linear functions for all previous steps, we can obtain a relatively complex dependency structure relating linear functions in $V_{i}, V_{i-1}, \cdots V_{0}$, observations and actions that itself represents a control strategy (Kaelbling et al., 1999).</p>
<p>To see this, we model the structure in graphical terms (Figure 7). Here different nodes represent linear functions, actions associated with nodes correspond to optimizing actions, links emanating from nodes correspond to different observations, and successor nodes correspond to linear functions paired with observations. Such graphs are also called policy graphs (Kaelbling et al., 1999; Littman, 1996; Cassandra, 1998). One interpretation of the dependency structure is that it represents a collection of finite-state machines (FSMs) with many possible initial states that implement a POMDP controller: nodes correspond to states of the controller, actions to controls (outputs), and links to transitions conditioned on inputs</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>(observations). The start state of the FSM controller is chosen greedily by selecting the linear function (controller state) optimizing the value of an initial belief state.</p>
<p>The advantage of the finite-state machine representation of the strategy is that for the first $i$ steps it works with observations directly; belief-state updates are not needed. This contrasts with the other two policy models (lookahead and direct models), which must keep track of the current belief state and update it over time in order to extract appropriate control. The drawback of the approach is that the FSM controller is limited to $i$ steps that correspond to the number of value iteration steps performed. However, in the infinitehorizon model the controller is expected to run for an infinite number of steps. One way to remedy this deficiency is to extend the FSM structure and to create cycles that let us visit controller states repeatedly. For example, adding a cycle transition to the end state of the FSM controller in Figure 7 (dashed line) ensures that the controller is also applicable to the infinite-horizon problem.</p>
<h1>2.6 Policy Iteration</h1>
<p>An alternative method for finding the solution for the discounted infinite-horizon problem is policy iteration (Howard, 1960; Sondik, 1978). Policy iteration searches the policy space and gradually improves the current control policy for one or more belief states. The method consists of two steps performed iteratively:</p>
<ul>
<li>policy evaluation: computes expected value for the current policy;</li>
<li>policy improvement: improves the current policy.</li>
</ul>
<p>As we saw in Section 2.5, there are many ways to represent a control policy for a POMDP. Here we restrict attention to a finite-state machine model in which observations correspond to inputs and actions to outputs (Platzman, 1980; Hansen, 1998b; Kaelbling et al., 1999). ${ }^{8}$</p>
<h3>2.6.1 Finite-State Machine Controller</h3>
<p>A finite-state machine (FSM) controller $C=(M, \Theta, A, \phi, \eta, \psi)$ for a POMDP is described by a set of memory states $M$ of the controller, a set of observations (inputs) $\Theta$, a set of actions (outputs) $A$, a transition function $\phi: M \times \Theta \rightarrow M$ mapping states of the FSM to next memory states given the observation, and an output function $\eta: M \rightarrow A$ mapping memory states to actions. A function $\psi: \mathcal{I}<em 0="0">{0} \rightarrow M$ selects the initial memory state given the initial information state. The initial information state corresponds either to a prior or a posterior belief state at time $t</em>$ depending on the availability of an initial observation.</p>
<h3>2.6.2 Policy Evaluation</h3>
<p>The first step of the policy iteration is policy evaluation. The most important property of the FSM model is that the value function for a specific FSM strategy can be computed efficiently in the number of controller states $M$. The key to efficient computability is the</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: An example of a four-state FSM policy. Nodes represent states, links transitions between states (conditioned on observations). Every memory state has an associated control action (output).
fact that the value function for executing an FSM strategy from some memory state $x$ is linear (Platzman, 1980). ${ }^{9}$</p>
<p>Theorem 5 Let $C$ be a finite-state machine controller with a set of memory states $M$. The value function for applying $C$ from a memory state $x \in M, V^{C}(x, b)$, is linear. Value functions for all $x \in M$ can be found by solving a system of linear equations with $|S||M|$ variables.</p>
<p>We illustrate the main idea by an example. Assume an FSM controller with four memory states $\left{x_{1}, x_{2}, x_{3}, x_{4}\right}$, as in Figure 8, and a stochastic process with two hidden states $S=$ $\left{s_{1}, s_{2}\right}$. The value of the policy for an augmented state space $S \times M$ satisfies a system of linear equations</p>
<p>$$
\begin{aligned}
V\left(x_{1}, s_{1}\right) &amp; =\rho\left(s_{1}, \eta\left(x_{1}\right)\right)+\gamma \sum_{o \in \Theta} \sum_{s \in S} P\left(o, s \mid s_{1}, \eta\left(x_{1}\right)\right) V\left(\phi\left(x_{1}, o\right), s\right) \
V\left(x_{1}, s_{2}\right) &amp; =\rho\left(s_{2}, \eta\left(x_{1}\right)\right)+\gamma \sum_{o \in \Theta} \sum_{s \in S} P\left(o, s \mid s_{2}, \eta\left(x_{1}\right)\right) V\left(\phi\left(x_{1}, o\right), s\right) \
V\left(x_{2}, s_{1}\right) &amp; =\rho\left(s_{1}, \eta\left(x_{2}\right)\right)+\gamma \sum_{o \in \Theta} \sum_{s \in S} P\left(o, s \mid s_{1}, \eta\left(x_{2}\right)\right) V\left(\phi\left(x_{2}, o\right), s\right) \
&amp; \cdots \
V\left(x_{4}, s_{2}\right) &amp; =\rho\left(s_{2}, \eta\left(x_{4}\right)\right)+\gamma \sum_{o \in \Theta} \sum_{s \in S} P\left(o, s \mid s_{2}, \eta\left(x_{4}\right)\right) V\left(\phi\left(x_{4}, o\right), s\right)
\end{aligned}
$$</p>
<p>where $\eta(x)$ is the action executed in $x$ and $\phi(x, o)$ is the state to which one transits after seeing an input (observation) $o$. Assuming we start the policy from the memory state $x_{1}$, the value of the policy is:</p>
<p>$$
V^{C}\left(x_{1}, b\right)=\sum_{s \in S} V\left(x_{1}, s\right) b(s)
$$</p>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>Thus the value function is linear and can be computed efficiently by solving a system of linear equations.</p>
<p>Since in general the FSM controller can start from any memory state, we can always choose the initial memory state greedily, maximizing the expected value of the result. In such a case the optimal choice function $\psi$ is defined as:</p>
<p>$$
\psi(b)=\arg \max _{x \in M} V^{C}(x, b)
$$</p>
<p>and the value for the FSM policy C and belief state $b$ is:</p>
<p>$$
V^{C}(b)=\max _{x \in M} V^{C}(x, b)=V^{C}(\psi(b), b)
$$</p>
<p>Note that the resulting value function for the strategy $C$ is piecewise linear and convex and represents expected rewards for following $C$. Since no strategy can perform better that the optimal strategy, $V^{C} \leq V^{*}$ must hold.</p>
<h1>2.6.3 Policy Improvement</h1>
<p>The policy-iteration method, searching the space of controllers, starts from an arbitrary initial policy and improves it gradually by refining its finite-state machine (FSM) description. In particular, one keeps modifying the structure of the controller by adding or removing controller states (memory) and transitions. Let $C$ and $C^{\prime}$ be an old and a new FSM controller. In the improvement step we must satisfy</p>
<p>$$
\begin{gathered}
V^{C^{\prime}}(b) \geq V^{C}(b) \text { for all } b \in \mathcal{I} \
\exists b \in \mathcal{I} \text { such that } V^{C^{\prime}}(b)&gt;V^{C}(b)
\end{gathered}
$$</p>
<p>To guarantee the improvement, Hansen (1998a, 1998b) proposed a policy-iteration algorithm that relies on exact value function updates to obtain a new improved policy structure. ${ }^{10}$ The basic idea of the improvement is based on the observation that one can switch back and forth between the FSM policy description and the piecewise-linear and convex representation of a value function. In particular:</p>
<ul>
<li>the value function for an FSM policy is piecewise-linear and convex and every linear function describing it corresponds to a memory state of a controller;</li>
<li>individual linear functions comprising the new value function after an update can be viewed as new memory states of an FSM policy, as described in Section 2.5.3.</li>
</ul>
<p>This allows us to improve the policy by adding new memory states corresponding to linear functions of the new value function obtained after the exact update. The technique can be refined by removing some of the linear functions (memory states) whenever they are fully dominated by one of the other linear functions.</p>
<p><sup id="fnref9:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: A two-step decision tree. Rectangles correspond to the decision nodes (moves of the decision-maker) and circles to chance nodes (moves of the environment). Black rectangles represent leaves of the tree. The reward for a specific path is associated with every leaf of the tree. Decision nodes are associated with information states obtained by following action and observation choices along the path from the root of the tree. For example, $b_{1,1}$ is a belief state obtained by performing action $a_{1}$ from the initial belief state $b$ and observing observation $o_{1}$.</p>
<h1>2.7 Forward (Decision Tree) Methods</h1>
<p>The methods discussed so far assume no prior knowledge of the initial belief state and treat all belief states as equally likely. However, if the initial state is known and fixed, methods can often be modified to take advantage of this fact. For example, for the finite-horizon problem, only a finite number of belief states can be reached from a given initial state. In this case it is very often easier to enumerate all possible histories (sequences of actions and observations) and represent the problem using stochastic decision trees (Raiffa, 1970). An example of a two-step decision tree is shown in Figure 9.</p>
<p>The algorithm for solving the stochastic decision tree basically mimics value-function updates, but is restricted only to situations that can be reached from the initial belief state. The key difficulty here is that the number of all possible trajectories grows exponentially with the horizon of interest.</p>
<h3>2.7.1 Combining Dynamic-Programming and Decision-Tree Techniques</h3>
<p>To solve a POMDP for a fixed initial belief state, we can apply two strategies: one constructs the decision tree first and then solves it, the other solves the problem in a backward fashion via dynamic programming. Unfortunately, both these techniques are inefficient, one suffering from exponential growth in the decision tree size, the other from super-exponential growth in the value function complexity. However, the two techniques can be combined in</p>
<p>a way that at least partially eliminates their disadvantages. The idea is based on the fact that the two techniques work on the solution from two different sides (one forward and the other backward) and the complexity for each of them worsens gradually. Then the solution is to compute the complete $k$ th value function using dynamic programming (value iteration) and cover the remaining steps by forward decision-tree expansion.</p>
<p>Various modifications of the above idea are possible. For example, one can often replace exact dynamic programming with two more efficient approximations providing upper and lower bounds of the value function. Then the decision tree must be expanded only when the bounds are not sufficient to determine the optimal action choice. A number of search techniques developed in the AI literature (Korf, 1985) combined with branch-and-bound pruning (Satia \&amp; Lave, 1973) can be applied to this type of problem. Several researchers have experimented with them to solve POMDPs (Washington, 1996; Hauskrecht, 1997; Hansen, 1998b). Other methods applicable to this problem are based on Monte-Carlo sampling (Kearns, Mansour, \&amp; Ng, 1999; McAllester \&amp; Singh, 1999) and real-time dynamic programming (Barto, Bradtke, \&amp; Singh, 1995; Dearden \&amp; Boutilier, 1997; Bonet \&amp; Geffner, 1998).</p>
<h1>2.7.2 Classical Planning Framework</h1>
<p>POMDP problems with fixed initial belief states and their solutions are closely related to work in classical planning and its extensions to handle stochastic and partially observable domains, particularly the work on BURIDAN and C-BURIDAN planners (Kushmerick, Hanks, \&amp; Weld, 1995; Draper, Hanks, \&amp; Weld, 1994). The objective of these planners is to maximize the probability of reaching some goal state. However, this task is similar to the discounted reward task in terms of complexity, since a discounted reward model can be converted into a goal-achievement model by introducing an absorbing state (Condon, 1992).</p>
<h2>3. Heuristic Approximations</h2>
<p>The key obstacle to wider application of the POMDP framework is the computational complexity of POMDP problems. In particular, finding the optimal solution for the finitehorizon case is PSPACE-hard (Papadimitriou \&amp; Tsitsiklis, 1987) and the discounted infinitehorizon case may not even be computable (Madani et al., 1999). One approach to such problems is to approximate the solution to some $\epsilon$-precision. Unfortunately, even this remains intractable and in general POMDPs cannot be approximated efficiently (Burago, Rougemont, \&amp; Slissenko, 1996; Lusena, Goldsmith, \&amp; Mundhenk, 1998; Madani et al., 1999). This is also the reason why only very simple problems can be solved optimally or near-optimally in practice.</p>
<p>To alleviate the complexity problem, research in the POMDP area has focused on various heuristic methods (or approximations without the error parameter) that are more efficient. ${ }^{11}$ Heuristic methods are also our focus here. Thus, when referring to approximations, we mean heuristics, unless specifically stated otherwise.</p>
<p><sup id="fnref10:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>The many approximation methods and their combinations can be divided into two often very closely related classes: value-function approximations and policy approximations.</p>
<h1>3.1 Value-Function Approximations</h1>
<p>The main idea of the value-function approximation approach is to approximate the optimal value function $V: \mathcal{I} \rightarrow \mathbb{R}$ with a function $\tilde{V}: \mathcal{I} \rightarrow \mathbb{R}$ defined over the same information space. Typically, the new function is of lower complexity (recall that the optimal or nearoptimal value function may consist of a large set of linear functions) and is easier to compute than the exact solution. Approximations can be often formulated as dynamic programming problems and can be expressed in terms of approximate value-function updates $\tilde{H}$. Thus, to understand the differences and advantages of various approximations and exact methods, it is often sufficient to analyze and compare their update rules.</p>
<h3>3.1.1 Value-Function Bounds</h3>
<p>Although heuristic approximations have no guaranteed precision, in many cases we are able to say whether they overestimate or underestimate the optimal value function. The information on bounds can be used in multiple ways. For example, upper- and lowerbounds can help in narrowing the range of the optimal value function, elimination of some of the suboptimal actions and subsequent speed-ups of exact methods. Alternatively, one can use knowledge of both value-function bounds to determine the accuracy of a controller generated based on one of the bounds (see Section 3.1.3). Also, in some instances, a lower bound alone is sufficient to guarantee the control choice that always achieves an expected reward at least as high as the one given by that bound (Section 4.7.2).</p>
<p>The bound property of different methods can be determined by examining the updates and their bound relations.</p>
<p>Definition 5 (Upper bound). Let $H$ be the exact value-function mapping and $\tilde{H}$ its approximation. We say that $\tilde{H}$ upper-bounds $H$ for some $V$ when $(\tilde{H} V)(b) \geq(H V)(b)$ holds for every $b \in \mathcal{I}$.</p>
<p>An analogous definition can be constructed for the lower bound.</p>
<h3>3.1.2 Convergence of Approximate Value Iteration</h3>
<p>Let $\tilde{H}$ be a value-function mapping representing an approximate update. Then the approximate value iteration computes the $i$ th value function as $\tilde{V}<em i-1="i-1">{i}=\tilde{H} \tilde{V}</em>$. The fixed-point solution $\widetilde{V^{<em>}}=\tilde{H} \tilde{V}^{</em>}$ or its close approximation would then represent the intended output of the approximation routine. The main problem with the iteration method is that in general it can converge to unique or multiple solutions, diverge, or oscillate, depending on $\tilde{H}$ and the initial function $\tilde{V}_{0}$. Therefore, unique convergence cannot be guaranteed for an arbitrary mapping $\tilde{H}$ and the convergence of a specific approximation method must be proved.</p>
<p>Definition 6 (Convergence of $\tilde{H}$ ). The value iteration with $\tilde{H}$ converges for a value function $V_{0}$ when $\lim <em 0="0">{n \rightarrow \infty}\left(\tilde{H}^{n} V</em>\right)$ exists.</p>
<p>Definition 7 (Unique convergence of $\tilde{H}$ ). The value iteration converges uniquely for $\mathcal{V}$ when for every $V \in \mathcal{V}, \lim <em _infty="\infty" _rightarrow="\rightarrow" n="n">{n \rightarrow \infty}\left(\tilde{H}^{n} V\right)$ exists and for all pairs $V, U \in \mathcal{V}, \lim </em> U\right)$}\left(\tilde{H}^{n} V\right)=$ $\lim _{n \rightarrow \infty}\left(\tilde{H}^{n</p>
<p>A sufficient condition for the unique convergence is to show that $\tilde{H}$ be a contraction. The contraction and the bound properties of $\tilde{H}$ can be combined, under additional conditions, to show the convergence of the iterative approximation method to the bound. To address this issue we present a theorem comparing fixed-point solutions of two value-function mappings.</p>
<p>Theorem 6 Let $H_{1}$ and $H_{2}$ be two value-function mappings defined on $\mathcal{V}<em 2="2">{1}$ and $\mathcal{V}</em>$ such that</p>
<ol>
<li>$H_{1}, H_{2}$ are contractions with fixed points $V_{1}^{<em>}, V_{2}^{</em>}$;</li>
<li>$V_{1}^{<em>} \in \mathcal{V}<em 2="2">{2}$ and $H</em>^{} V_{1</em>} \geq H_{1} V_{1}^{<em>}=V_{1}^{</em>}$;</li>
<li>$H_{2}$ is an isotone mapping.</li>
</ol>
<p>Then $V_{2}^{<em>} \geq V_{1}^{</em>}$ holds.
Note that this theorem does not require that $\mathcal{V}<em 2="2">{1}$ and $\mathcal{V}</em>}$ cover the same space of value functions. For example, $\mathcal{V<em 1="1">{2}$ can cover all possible value functions of a belief-state MDP, while $\mathcal{V}</em>$ can be restricted to a space of piecewise linear and convex value functions. This gives us some flexibility in the design of iterative approximation algorithms for computing value-function bounds. An analogous theorem also holds for the lower bound.</p>
<h1>3.1.3 Control</h1>
<p>Once the approximation of the value-function is available, it can be used to generate a control strategy. In general, control solutions correspond to options presented in Section 2.5 and include lookahead, direct (Q-function) and finite-state machine designs.</p>
<p>A drawback of control strategies based on heuristic approximations is that they have no precision guarantee. One way to find the accuracy of such strategies is to do one exact update of the value function approximation and adopt the result of Theorems 1 and 3 for the Bellman error. An alternative solution to this problem is to bound the accuracy of such controllers using the upper- and the lower-bound approximations of the optimal value function. To illustrate this approach, we present and prove (in the Appendix) the following theorem that relates the quality of bounds to the quality of a lookahead controller.</p>
<p>Theorem 7 Let $\widehat{V}<em L="L">{U}$ and $\widehat{V}</em>$ be upper and lower bounds of the optimal value function for the discounted infinite-horizon problem. Let $\epsilon=\sup <em U="U">{b}\left|\widehat{V}</em>}(b)-\widehat{V<em U="U">{L}(b)\right|=\left|\widehat{V}</em>}-\widehat{V<em U="U">{L}\right|$ be the maximum bound difference. Then the expected reward for a lookahead controller $\widehat{V}^{L A}$, constructed for either $\widehat{V}</em>$.}$ or $\widehat{V}_{L}$, satisfies $\left|\widehat{V}^{L A}-V^{*}\right| \leq \frac{\epsilon(2-\gamma)}{(1-\gamma)</p>
<h3>3.2 Policy Approximation</h3>
<p>An alternative to value-function approximation is policy approximation. As shown earlier, a strategy (controller) for a POMDP can be represented using a finite-state machine (FSM) model. The policy iteration searches the space of all possible policies (FSMs) for the optimal or near-optimal solution. This space is usually enormous, which is the bottleneck of the</p>
<p>method. Thus, instead of searching the complete policy space, we can restrict our attention only to its subspace that we believe to contain the optimal solution or a good approximation. Memoryless policies (Platzman, 1977; White \&amp; Scherer, 1994; Littman, 1994; Singh, Jaakkola, \&amp; Jordan, 1994), policies based on truncated histories (Platzman, 1977; White \&amp; Scherer, 1994; McCallum, 1995), or finite-state controllers with a fixed number of memory states (Platzman, 1980; Hauskrecht, 1997; Hansen, 1998a, 1998b) are all examples of a policy-space restriction. In the following we consider only the finite-state machine model (see Section 2.6.1), which is quite general; other models can be viewed as its special cases.</p>
<p>States of an FSM policy model represent the memory of the controller and, in general, summarize information about past activities and observations. Thus, they are best viewed as approximations of the information states, or as feature states. The transition model of the controller $(\phi)$ then approximates the update function of the information-state MDP $(\tau)$ and the output function of an FSM $(\eta)$ approximates the control function $(\mu)$ mapping information states to actions. The important property of the model, as shown Section 2.6.2, is that the value function for a fixed controller and fixed initial memory state can be obtained efficiently by solving a system of linear equations (Platzman, 1980).</p>
<p>To apply the policy approximation approach we first need to decide (1) how to restrict a space of policies and (2) how to judge the policy quality.</p>
<p>A restriction frequently used is to consider only controllers with a fixed number of states, say $k$. Other structural restrictions further narrowing the space of policies can restrict either the output function (choice of actions at different controller states), or the transitions between the current and next states. In general, any heuristic or domain-related insight may help in selecting the right biases.</p>
<p>Two different policies can yield value functions that are better in different regions of the belief space. Thus, in order to decide which policy is the best, we need to define the importance of different regions and their combinations. There are multiple solutions to this. For example, Platzman (1980) considers the worst-case measure and optimizes the worst (minimal) value for all initial belief states. Let $\mathcal{C}$ be a space of FSM controllers satisfying given restrictions. Then the quality of a policy under the worst case measure is:</p>
<p>$$
\max <em _in="\in" _mathcal_I="\mathcal{I" b="b">{C \in \mathcal{C}} \min </em> \max }<em C="C">{x \in M</em>(x, b)
$$}} V^{C</p>
<p>Another option is to consider a distribution over all initial belief states and maximize the expectation of their value function values. However, the most common objective is to choose the policy that leads to the best value for a single initial belief state $b_{0}$ :</p>
<p>$$
\max <em M__C="M_{C" _in="\in" x="x">{C \in \mathcal{C}} \max </em>\right)
$$}} V^{C}\left(x, b_{0</p>
<p>Finding the optimal policy for this case reduces to a combinatorial optimization problem. Unfortunately, for all but trivial cases, even this problem is computationally intractable. For example, the problem of finding the optimal policy for a memoryless case (only current observations are considered) is NP-hard (Littman, 1994). Thus, various heuristics are typically applied to alleviate this difficulty (Littman, 1994).</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Value-function approximation methods.</p>
<h1>3.2.1 Randomized Policies</h1>
<p>By restricting the space of policies we simplify the policy optimization problem. On the other hand, we simultaneously give up an opportunity to find the best optimal policy, replacing it with the best restricted policy. Up to this point, we have considered only deterministic policies with a fixed number of internal controller states, that is, policies with deterministic output and transition functions. However, finding the best deterministic policy is not always the best option: randomized policies, with randomized output and transition functions, usually lead to the far better performance. The application of randomized (or stochastic) policies to POMDPs was introduced by Platzman (1980). Essentially, any deterministic policy can be represented as a randomized policy with a single action and transition, so that the best randomized policy is no worse than the best deterministic policy. The difference in control performance of two policies shows up most often in cases when the number of states of the controller is relatively small compared to that in the optimal strategy.</p>
<p>The advantage of stochastic policies is that their space is larger and parameters of the policy are continuous. Therefore the problem of finding the optimal stochastic policy becomes a non-linear optimization problem and a variety of optimization methods can be applied to solve it. An example is the gradient-based approach (see Meuleau et al., 1999).</p>
<h2>4. Value-Function Approximation Methods</h2>
<p>In this section we discuss in more depth value-function approximation methods. We focus on approximations with belief information space. ${ }^{12}$ We survey known techniques, but also include a number of new methods and modifications of existing methods. Figure 10 summarizes the methods covered. We describe the methods by means of update rules they</p>
<p><sup id="fnref11:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ol>
<li>Alternative value-function approximations may work with complete histories of past actions and observations. Approximation methods used by White and Scherer (1994) are an example.</li>
</ol>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
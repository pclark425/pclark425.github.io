<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5310 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5310</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5310</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-112.html">extraction-schema-112</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <p><strong>Paper ID:</strong> paper-257636780</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.11436v2.pdf" target="_blank">Mind meets machine: Unravelling GPT-4's cognitive psychology</a></p>
                <p><strong>Paper Abstract:</strong> Cognitive psychology delves on understanding perception, attention, memory, language, problem-solving, decision-making, and reasoning. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required. In this study, we focus on the evaluation of GPT-4's performance on a set of cognitive psychology datasets such as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how GPT-4 processes and integrates cognitive psychology with contextual information, providing insight into the underlying cognitive processes that enable its ability to generate the responses. We show that GPT-4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior state-of-the-art models. Our results strengthen the already available assessments and confidence on GPT-4's cognitive psychology abilities. It has significant potential to revolutionize the field of AI, by enabling machines to bridge the gap between human and machine reasoning.</p>
                <p><strong>Cost:</strong> 0.009</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5310.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5310.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of OpenAI's GPT-4 on the CommonsenseQA benchmark measuring commonsense question answering (5-way multiple choice). The paper reports GPT-4 accuracy substantially higher than older LLMs but slightly below human performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4, a transformer-based large language model accessed in this study via ChatGPT-Plus; the paper references the OpenAI GPT-4 technical report for broader model details.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>CommonsenseQA</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>commonsense reasoning / language</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A 12,247-question multiple-choice dataset (5 choices each) designed to test commonsense knowledge using ConceptNet to generate challenging questions; created via crowdworkers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported GPT-4 accuracy ≈ 83.2% (paper also states 'around 84%').</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Reported human accuracy ≈ 89% (as cited in the CommonsenseQA paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 (≈83.2%) performs substantially better than prior language models reported in the original CommonsenseQA paper (e.g., a model at 55.9%), but remains below the cited human baseline (~89%). No statistical significance tests reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The paper notes GPT-4 outperforms earlier LLMs but does not report confidence intervals or statistical tests; demographic details of human baseline are not provided here. The paper's reported GPT-4 result is from ChatGPT-Plus runs but experimental details (prompting, few-shot vs zero-shot, seed variability) are not fully described.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5310.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5310.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / MATH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on the MATH dataset</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-4 on the MATH benchmark for contest-level mathematics problems spanning topics and difficulty levels; the paper reports per-subcategory accuracies rather than a single overall exact-match score.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 accessed via ChatGPT-Plus; transformer-based LLM with chain-of-thought capability referenced in context, exact training corpus and parameter count not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>MATH</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>mathematical problem solving / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A dataset of ~12,500 competition-style math problems across topics (prealgebra, algebra, geometry, etc.) with graded difficulty (1–5); evaluation uses exact-match on normalized answers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Paper reports GPT-4 accuracies by subtopic: prealgebra ≈ 82%, geometry ≈ 35%. (No single overall exact-match percentage reported for GPT-4 in this paper's text beyond subtopic values.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Paper cites human variability: e.g., a computer-science PhD scored ≈40%, an IMO gold medallist scored ≈90% on MATH (from the MATH dataset paper).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>GPT-4 shows strong performance on lower-difficulty / prealgebra items (≈82%), comparable to or exceeding some non-expert humans, but much lower on geometry (≈35%) compared to top-skilled humans; GPT-4 substantially outperforms older GPT-2/GPT-3 results (reported <10%). No statistical tests reported.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Reported results are uneven across subdomains (notably poor on geometry), suggesting modality-specific or spatial-reasoning limitations; the paper provides limited methodological detail (prompting, few-shot/coT use) and no error analysis or confidence intervals. References to prior literature show much lower base rates for other LLMs on MATH (3–6.9%).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5310.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5310.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / SuperGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on SuperGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-4 on the SuperGLUE benchmark, a suite of challenging NLU tasks designed to measure higher-level language understanding and reasoning; the paper reports a high aggregate score for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 accessed through ChatGPT-Plus; the paper frames GPT-4 as a state-of-the-art transformer LLM but does not specify parameter count or training corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>SuperGLUE</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>natural language understanding / reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A benchmark composed of multiple challenging NLU tasks (many low-data tasks) intended to be tougher than GLUE, measuring a range of language understanding capabilities with an aggregate scoring metric.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Reported GPT-4 aggregate score ≈ 91.2% (as reported in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>No explicit numeric human baseline is reported in this paper for SuperGLUE; the paper references the original SuperGLUE finding that baseline BERT models score ~20 points below humans but does not provide the human aggregate number here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Paper claims GPT-4 achieves high SuperGLUE accuracy (≈91.2%), substantially surpassing older baselines (e.g., BERT, which was ~20 points worse relative to humans in the original SuperGLUE study). Direct comparison to human performance is not numerically reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>The paper does not report experimental details (prompting method, calibration, aggregate vs per-task scores) or statistical tests; without the human aggregate in-text, the degree to which GPT-4 matches or exceeds human performance cannot be precisely assessed from this paper alone.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5310.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5310.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLMs being evaluated on cognitive psychology tests, including details of the tests, LLM performance, human baseline performance, and any direct comparisons or notable differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 / HANS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 evaluated on HANS (Heuristic Analysis for NLI Systems)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Evaluation of GPT-4 on HANS, a diagnostic dataset designed to detect reliance on shallow syntactic heuristics in NLI models; the paper reports a perfect score but flags methodological caveats.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>OpenAI's GPT-4 accessed via ChatGPT-Plus; treated in the paper as a state-of-the-art transformer LLM, full architectural/training details are not specified here.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_name</strong></td>
                            <td>HANS</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_type</strong></td>
                            <td>natural language inference robustness / syntactic heuristics</td>
                        </tr>
                        <tr>
                            <td><strong>cognitive_test_description</strong></td>
                            <td>A diagnostic dataset that crafts NLI examples to expose models that rely on shallow heuristics (lexical overlap, subsequence, and component heuristics) rather than genuine syntactic/semantic generalization; designed to include cases where heuristics fail.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_performance</strong></td>
                            <td>Paper reports GPT-4 accuracy = 100% on the HANS examples they used.</td>
                        </tr>
                        <tr>
                            <td><strong>human_baseline_performance</strong></td>
                            <td>Paper cites human performance on HANS in prior work as ranging from ≈76% to 97% (depending on subset/task as reported by the HANS authors).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Reported GPT-4 (100%) exceeds the cited human range (76–97%) on the subset used in this paper; however the paper explicitly cautions this result may be an artifact because the authors used HANS examples that were all non-entailment, allowing a simple heuristic (always predict non-entailment) to achieve perfect scores.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_differences_or_limitations</strong></td>
                            <td>Major caveat: the authors note their HANS run used only non-entailment examples, so the perfect score likely reflects dataset sampling and potential memorization of the non-entailment label rather than robust NLI reasoning; BERT baselines in prior work performed poorly on HANS non-entailment (<10% in some categories). The paper acknowledges ongoing experiments with mixed HANS data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gpt-4 technical report <em>(Rating: 2)</em></li>
                <li>Commonsenseqa: A question answering challenge targeting commonsense knowledge <em>(Rating: 2)</em></li>
                <li>Superglue: A stickier benchmark for general-purpose language understanding systems <em>(Rating: 2)</em></li>
                <li>Measuring mathematical problem solving with the math dataset <em>(Rating: 2)</em></li>
                <li>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference <em>(Rating: 2)</em></li>
                <li>Hellaswag: Can a machine really finish your sentence? <em>(Rating: 1)</em></li>
                <li>Winogrande: An adversarial winograd schema challenge at scale <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5310",
    "paper_id": "paper-257636780",
    "extraction_schema_id": "extraction-schema-112",
    "extracted_data": [
        {
            "name_short": "GPT-4 / CommonsenseQA",
            "name_full": "GPT-4 evaluated on CommonsenseQA",
            "brief_description": "Evaluation of OpenAI's GPT-4 on the CommonsenseQA benchmark measuring commonsense question answering (5-way multiple choice). The paper reports GPT-4 accuracy substantially higher than older LLMs but slightly below human performance.",
            "citation_title": "MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4, a transformer-based large language model accessed in this study via ChatGPT-Plus; the paper references the OpenAI GPT-4 technical report for broader model details.",
            "model_size": null,
            "cognitive_test_name": "CommonsenseQA",
            "cognitive_test_type": "commonsense reasoning / language",
            "cognitive_test_description": "A 12,247-question multiple-choice dataset (5 choices each) designed to test commonsense knowledge using ConceptNet to generate challenging questions; created via crowdworkers.",
            "llm_performance": "Reported GPT-4 accuracy ≈ 83.2% (paper also states 'around 84%').",
            "human_baseline_performance": "Reported human accuracy ≈ 89% (as cited in the CommonsenseQA paper).",
            "performance_comparison": "GPT-4 (≈83.2%) performs substantially better than prior language models reported in the original CommonsenseQA paper (e.g., a model at 55.9%), but remains below the cited human baseline (~89%). No statistical significance tests reported in this paper.",
            "notable_differences_or_limitations": "The paper notes GPT-4 outperforms earlier LLMs but does not report confidence intervals or statistical tests; demographic details of human baseline are not provided here. The paper's reported GPT-4 result is from ChatGPT-Plus runs but experimental details (prompting, few-shot vs zero-shot, seed variability) are not fully described.",
            "uuid": "e5310.0"
        },
        {
            "name_short": "GPT-4 / MATH",
            "name_full": "GPT-4 evaluated on the MATH dataset",
            "brief_description": "Evaluation of GPT-4 on the MATH benchmark for contest-level mathematics problems spanning topics and difficulty levels; the paper reports per-subcategory accuracies rather than a single overall exact-match score.",
            "citation_title": "MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 accessed via ChatGPT-Plus; transformer-based LLM with chain-of-thought capability referenced in context, exact training corpus and parameter count not specified in this paper.",
            "model_size": null,
            "cognitive_test_name": "MATH",
            "cognitive_test_type": "mathematical problem solving / reasoning",
            "cognitive_test_description": "A dataset of ~12,500 competition-style math problems across topics (prealgebra, algebra, geometry, etc.) with graded difficulty (1–5); evaluation uses exact-match on normalized answers.",
            "llm_performance": "Paper reports GPT-4 accuracies by subtopic: prealgebra ≈ 82%, geometry ≈ 35%. (No single overall exact-match percentage reported for GPT-4 in this paper's text beyond subtopic values.)",
            "human_baseline_performance": "Paper cites human variability: e.g., a computer-science PhD scored ≈40%, an IMO gold medallist scored ≈90% on MATH (from the MATH dataset paper).",
            "performance_comparison": "GPT-4 shows strong performance on lower-difficulty / prealgebra items (≈82%), comparable to or exceeding some non-expert humans, but much lower on geometry (≈35%) compared to top-skilled humans; GPT-4 substantially outperforms older GPT-2/GPT-3 results (reported &lt;10%). No statistical tests reported.",
            "notable_differences_or_limitations": "Reported results are uneven across subdomains (notably poor on geometry), suggesting modality-specific or spatial-reasoning limitations; the paper provides limited methodological detail (prompting, few-shot/coT use) and no error analysis or confidence intervals. References to prior literature show much lower base rates for other LLMs on MATH (3–6.9%).",
            "uuid": "e5310.1"
        },
        {
            "name_short": "GPT-4 / SuperGLUE",
            "name_full": "GPT-4 evaluated on SuperGLUE",
            "brief_description": "Evaluation of GPT-4 on the SuperGLUE benchmark, a suite of challenging NLU tasks designed to measure higher-level language understanding and reasoning; the paper reports a high aggregate score for GPT-4.",
            "citation_title": "MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 accessed through ChatGPT-Plus; the paper frames GPT-4 as a state-of-the-art transformer LLM but does not specify parameter count or training corpus.",
            "model_size": null,
            "cognitive_test_name": "SuperGLUE",
            "cognitive_test_type": "natural language understanding / reasoning",
            "cognitive_test_description": "A benchmark composed of multiple challenging NLU tasks (many low-data tasks) intended to be tougher than GLUE, measuring a range of language understanding capabilities with an aggregate scoring metric.",
            "llm_performance": "Reported GPT-4 aggregate score ≈ 91.2% (as reported in the paper).",
            "human_baseline_performance": "No explicit numeric human baseline is reported in this paper for SuperGLUE; the paper references the original SuperGLUE finding that baseline BERT models score ~20 points below humans but does not provide the human aggregate number here.",
            "performance_comparison": "Paper claims GPT-4 achieves high SuperGLUE accuracy (≈91.2%), substantially surpassing older baselines (e.g., BERT, which was ~20 points worse relative to humans in the original SuperGLUE study). Direct comparison to human performance is not numerically reported in this paper.",
            "notable_differences_or_limitations": "The paper does not report experimental details (prompting method, calibration, aggregate vs per-task scores) or statistical tests; without the human aggregate in-text, the degree to which GPT-4 matches or exceeds human performance cannot be precisely assessed from this paper alone.",
            "uuid": "e5310.2"
        },
        {
            "name_short": "GPT-4 / HANS",
            "name_full": "GPT-4 evaluated on HANS (Heuristic Analysis for NLI Systems)",
            "brief_description": "Evaluation of GPT-4 on HANS, a diagnostic dataset designed to detect reliance on shallow syntactic heuristics in NLI models; the paper reports a perfect score but flags methodological caveats.",
            "citation_title": "MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY",
            "mention_or_use": "use",
            "model_name": "GPT-4",
            "model_description": "OpenAI's GPT-4 accessed via ChatGPT-Plus; treated in the paper as a state-of-the-art transformer LLM, full architectural/training details are not specified here.",
            "model_size": null,
            "cognitive_test_name": "HANS",
            "cognitive_test_type": "natural language inference robustness / syntactic heuristics",
            "cognitive_test_description": "A diagnostic dataset that crafts NLI examples to expose models that rely on shallow heuristics (lexical overlap, subsequence, and component heuristics) rather than genuine syntactic/semantic generalization; designed to include cases where heuristics fail.",
            "llm_performance": "Paper reports GPT-4 accuracy = 100% on the HANS examples they used.",
            "human_baseline_performance": "Paper cites human performance on HANS in prior work as ranging from ≈76% to 97% (depending on subset/task as reported by the HANS authors).",
            "performance_comparison": "Reported GPT-4 (100%) exceeds the cited human range (76–97%) on the subset used in this paper; however the paper explicitly cautions this result may be an artifact because the authors used HANS examples that were all non-entailment, allowing a simple heuristic (always predict non-entailment) to achieve perfect scores.",
            "notable_differences_or_limitations": "Major caveat: the authors note their HANS run used only non-entailment examples, so the perfect score likely reflects dataset sampling and potential memorization of the non-entailment label rather than robust NLI reasoning; BERT baselines in prior work performed poorly on HANS non-entailment (&lt;10% in some categories). The paper acknowledges ongoing experiments with mixed HANS data.",
            "uuid": "e5310.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gpt-4 technical report",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Commonsenseqa: A question answering challenge targeting commonsense knowledge",
            "rating": 2,
            "sanitized_title": "commonsenseqa_a_question_answering_challenge_targeting_commonsense_knowledge"
        },
        {
            "paper_title": "Superglue: A stickier benchmark for general-purpose language understanding systems",
            "rating": 2,
            "sanitized_title": "superglue_a_stickier_benchmark_for_generalpurpose_language_understanding_systems"
        },
        {
            "paper_title": "Measuring mathematical problem solving with the math dataset",
            "rating": 2,
            "sanitized_title": "measuring_mathematical_problem_solving_with_the_math_dataset"
        },
        {
            "paper_title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference",
            "rating": 2,
            "sanitized_title": "right_for_the_wrong_reasons_diagnosing_syntactic_heuristics_in_natural_language_inference"
        },
        {
            "paper_title": "Hellaswag: Can a machine really finish your sentence?",
            "rating": 1,
            "sanitized_title": "hellaswag_can_a_machine_really_finish_your_sentence"
        },
        {
            "paper_title": "Winogrande: An adversarial winograd schema challenge at scale",
            "rating": 1,
            "sanitized_title": "winogrande_an_adversarial_winograd_schema_challenge_at_scale"
        }
    ],
    "cost": 0.00908725,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</p>
<p>Sifatkaur Dhingra sifatkaurd13@gmail.com 
Manmeet Singh manmeet.cat@tropmet.res.in 
Vaisakh Sb vaisakh.sb@tropmet.res.in 
Neetiraj Malviya neetirajmalviya@gmail.com 
Sukhpal Singh Gill s.s.gill@qmul.ac.uk </p>
<p>Department of Psychology
Indian Institute of Tropical Meteorology Pune
Indian Institute of Tropical Meteorology Pune
Defence Institute Of Advanced Technology Pune
Nowrosjee Wadia College Pune
India, India, India, India</p>
<p>Queen Mary University of London London
United Kingdom</p>
<p>MIND MEETS MACHINE: UNRAVELLING GPT-4'S COGNITIVE PSYCHOLOGY</p>
<p>Cognitive psychology delves on understanding perception, attention, memory, language, problemsolving, decision-making, and reasoning. Large language models (LLMs) are emerging as potent tools increasingly capable of performing human-level tasks. The recent development in the form of GPT-4 and its demonstrated success in tasks complex to humans exam and complex problems has led to an increased confidence in the LLMs to become perfect instruments of intelligence. Although GPT-4 report has shown performance on some cognitive psychology tasks, a comprehensive assessment of GPT-4, via the existing well-established datasets is required. In this study, we focus on the evaluation of GPT-4's performance on a set of cognitive psychology datasets such as CommonsenseQA, Super-GLUE, MATH and HANS. In doing so, we understand how GPT-4 processes and integrates cognitive psychology with contextual information, providing insight into the underlying cognitive processes that enable its ability to generate the responses. We show that GPT-4 exhibits a high level of accuracy in cognitive psychology tasks relative to the prior state-of-the-art models. Our results strengthen the already available assessments and confidence on GPT-4's cognitive psychology abilities. It has significant potential to revolutionize the field of AI, by enabling machines to bridge the gap between human and machine reasoning.</p>
<p>Introduction</p>
<p>Cognitive psychology aims to decipher how humans learn new things, retain knowledge, and recall it when needed. Cognitive psychologists seek to understand how the mind works by conducting studies on people's thoughts and actions and by using other experimental methods like brain imaging and computer modelling. Understanding the human mind and developing our cognitive skills to excel in a variety of areas is the ultimate objective of cognitive psychology. Language models have come a long way since the first statistical models for modelling language were introduced. With the advent of deep learning and the availability of large amounts of data, recent years have seen a rapid evolution of language models that have achieved human-like performance on many language tasks. Large Language Models (LLMs) are a type of artificial intelligence framework that have garnered significant attention in recent years due to their remarkable language processing capabilities (Harrer 2023). These models are trained on vast amounts of text data and are able to generate coherent, human-like responses to natural language queries. One of the key features of LLMs is their ability to generate novel and creative responses to text-based prompts, which has led to their increasing use in fields such as chatbots, question answering systems, and language translation. The use of self-attention has been a key factor in this success, as it allows for more efficient and accurate modeling of long-range dependencies within the input sequence, resulting in better performance compared to traditional RNN-based models. LLMs have demonstrated impressive performance on a wide range of language tasks, including language modeling, machine translation, sentiment analysis, and text classification. These capabilities have led to the increased use of LLMs in various fields, including language-based customer service, virtual assistants, and creative writing. One of the key areas measuring intelligence in humans, other species and machines is the cognitive psychology. There are several tasks that are considered to be the benchmarks for testing cognitive psychology. Some of them are text interpretation, computer vision, planning and reasoning. For cognitive psychology to work, we rely on a complex and potent social practise: the attribution and assessment of thoughts and actions [1]. The scientific psychology of cognition and behaviour, a relatively recent innovation, focuses primarily on the information-processing mechanisms and activities that characterise human cognitive and behavioural capabilities. Researchers have attempted to create systems that could use natural language to reason about their surroundings [2] or that could use a world model to get a more profound comprehension of spoken language [3]. The report introducing GPT-4 [4] has tested the HellaSwag [5] and WinoGrande [6] datasets for cognitive psychology. Although, these tests are relevant, they lack the sophistication required to understand deep heuristics of GPT-4. Hellaswag entails the task of finishing a sentence and WinoGrande involves identifying the correct noun for the pronouns in a sentence, which are quite simple. Other tasks and standardized datasets [7] which test the psychology are needed in order to perform a comprehensive assessment of cognitive psychology for GPT-4. Moreover GPT-4 needs to go through complex reasoning tasks than just predicting the last word of the sentence such as in Hellaswag, to emerge as a model capable of high-level intelligence. [8] note that SuperGLUE [9], CommonsenseQA [10], MATH [11] and HANS [12] are four such datasets that are needed to be tested for a comprehensive cognitive psychology evaluation of AI models. In this study, we evaluate the performance of GPT-4 on the SuperGLUE, CommonsenseQA, MATH and HANS datasets. This is a work in progress and we are performing continuous tests with the other datasets as suggested by [8]. Our study can be used to build up higher-order psychological tests using GPT-4.</p>
<p>Datasets and Methodology</p>
<p>In this study, four datasets have been used to test the cognitive psychology capabilities of GPT-4. The four datasets are CommonsenseQA, MATH, SuperGLUE and HANS. They are described as below:</p>
<p>CommonsenseQA</p>
<p>CommonsenseQA is a dataset composed for testing commonsense reasoning. There are 12,247 questions in the dataset, each with 5 possible answers. Workers using Amazon's Mechanical Turk were used to build the dataset. The goal of the dataset is to evaluate the commonsense knowledge using CONCEPTNET to generate difficult questions. The language model tested in the CommonsenseQA paper has an accuracy of 55.9 % whereas the authors report that human accuracy on the dataset is around 89 %.</p>
<p>MATH</p>
<p>The MATH dataset includes almost 12,500 problems from scholastic mathematics contests. Machine learning models take a mathematical problem as input and produce an answer-encoding sequence, such as f rac23. After normalisation, their answers are distinct, therefore MATH may be evaluated using exact match instead of heuristic metrics like BLEU. Problems in seven different areas of mathematics, including geometry, are categorised by complexity from 1 to 5, and diagrams can be expressed in text using the Asymptote language. This allows for a nuanced evaluation of problem-solving skills in mathematics across a wide range of rigour and content. Problems now have comprehensive, detailed, step-by-step answers. To improve learning and make model outputs more interpretable, models can be trained on these to develop their own step-by-step solutions. The MATH dataset presents a significant challenge, with accuracy rates for big language models ranging from 3.0% to 6.9%. Models attain up to 15% accuracy on the least difficulty level and can develop step-by-step answers that are coherent and on-topic even when erroneous, suggesting that they do possess some mathematical knowledge despite their low accuracies. The results of human evaluations on MATH show that it may be difficult for humans as well; a computer science PhD student who does not really like mathematics scored about 40%, while a three-time IMO gold medallist scored 90%.</p>
<p>SuperGLUE</p>
<p>SuperGLUE is an updated version of the GLUE benchmark that includes a more challenging set of language understanding tasks. Using the gap between human and machine performance as a metric, SuperGLUE improves upon the GLUE benchmark by defining a new set of difficult Natural Language Understanding (NLU) problems. About half of the tasks in the SuperGLUE benchmark have fewer than 1k instances, and all but one have fewer than 10k examples, highlighting the importance of different task formats and low-data training data problems. As compared to humans, SuperGLUE scores roughly 20 points worse when using BERT as a baseline in the original study. To get closer to human-level performance on the benchmark, the authors argue that advances in multi-task, transfer, and unsupervised/self-supervised learning approaches are essential.</p>
<p>HANS</p>
<p>The strength of neural networks lies in their ability to analyse a training set for statistical patterns and then apply those patterns to test instances that come from the same distribution. This advantage is not without its drawbacks, however, as statistical learners, such as traditional neural network designs, tend to rely on simplistic approaches that work for the vast majority of training samples rather than capturing the underlying generalisations. The loss function may not motivate the model to learn to generalise to increasingly difficult scenarios in the same way a person would if heuristics tend to produce mostly correct results. This problem has been observed in several applications of AI. Contextual heuristics mislead object-recognition neural networks in computer vision, for example; a network that can accurately identify monkeys in a normal situation may mistake a monkey carrying a guitar for a person, since guitars tend to co-occur with people but not monkeys in the training set. Visual question answering systems are prone to the same heuristics. This problem is tackled by HANS (Heuristic Analysis for NLI Systems), which uses heuristics to determine if a premise sentence entails (i.e., suggests the truth of) a hypothesis sentence. Neural Natural Language Inference (NLI) models have been demonstrated to learn shallow heuristics based on the presence of specific words, as has been the case in other fields. As not often appears in the instances of contradiction in normal NLI training sets, a model can categorise all inputs containing the word not as contradiction. HANS prioritises heuristics that are founded on elementary syntactic characteristics. Think about the entailment-focused phrase pair below:</p>
<p>Premise: The judge was paid by the actor.</p>
<p>Hypothesis: The actor paid the judge.</p>
<p>An NLI system may accurately label this example not by deducing the meanings of these lines but by assuming that the premise involves any hypothesis whose terms all occur in the premise. Importantly, if the model is employing this heuristic, it will incorrectly classify the following as entailed even when it is not.</p>
<p>Premise: The actor was paid by the judge.</p>
<p>Hypothesis: The actor paid the judge.</p>
<p>HANS is intended to detect the presence of such faulty structural heuristics. The authors focus on the lexical overlap, subsequence, and component heuristics. These heuristics are not legitimate inference procedures despite often producing correct labels. Rather than just having reduced overall accuracy, HANS is meant to ensure that models using these heuristics fail on specific subsets of the dataset. Four well-known NLI models, including BERT, are compared and contrasted using the HANS dataset. For this dataset, all models significantly underperformed the chance distribution, with accuracy just exceeding 0% in most situations.</p>
<p>Methodology</p>
<p>We test the four datasets as described above to test the cognitive psychology capabilities of GPT-4. The model is accessed using the ChatGPT-Plus offered by OpenAI. We evaluate these models as shown in the results and discussion section.</p>
<p>Results</p>
<p>We will first discuss the human and machine skill of the different models traditionally used in the datasets used to test cognitive psychology. As compared to humans, SuperGLUE scores roughly 20 points worse when using BERT as a baseline in the original study. To get closer to human-level performance on the benchmark, the authors argue that advances in multi-task, transfer, and unsupervised/self-supervised learning approaches are essential. The language model tested in the CommonsenseQA paper has an accuracy of 55.9 % whereas the authors report that human accuracy on the dataset is around 89 %. The accuracy of humans on HANS dataset ranged from 76-97 % and the authors show   </p>
<p>Conclusions</p>
<p>GPT-4, which is a state-of-the-art large language model, is a revolution in the field of psychology since it gives psychologists unprecedented resources to use in their studies and work. This sophisticated AI model offers psychologists and psychiatrists to learn more about the human mind and come up with novel treatment theories and approaches. It provides an avenue for improved efficacy of psychological therapies and allowing professionals to spend more time with clients, leading to deeper and more fruitful therapeutic bonds. The potential applications of GPT-4 can only be realized if the model is thoroughly tested on basic tests of reasoning and cognition. Cognitive psychology enables the humans to perform various activities [13] in their personal and professional lives. We show that the performance of GPT-4 greatly surpasses the language model used in the original studies from where the different datasets are sourced, thus it can make a tool of day-to-day utility for psychologists. This development can lead to cascading benefits in addressing the mental health challenges faced by today's society. Problem: $ABCD$, a rectangle with $AB = 12$ and $BC = 16$, is the base of pyramid $P$, which has a height of $24$. A plane parallel to $ABCD$ is passed through $P$, dividing $P$ into a frustum $F$ and a smaller pyramid $P'$. Let $X$ denote the center of the circumsphere of $F$, and let $T$ denote the apex of $P$. If the volume of $P$ is eight times that of $P'$, then the value of $XT$ can be expressed as $ \frac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. Compute the value of $m + n$.</p>
<p>Finally, we have $m + n = 108 + 10 = \boxed{118}$. </p>
<p>Figure 1 :
1Datasets used in the study with the different categories contained in them. that the BERT model performed below 10 % on the non-entailment category. The human performance on MATH varied from 40-90 % and GPT-2/GPT-3 showed accuracies below 10 %.</p>
<p>Figure 1
1shows that GPT-4 has an accuracy of 83.2 % on CommonSenseQA, data, we find that GPT-4 has an accuracy of around 84%, 82 % on prealgebra, 35% on geometry, 100% on HANS and 91.2 % on SuperGLUE. It is to be noted that the perfect results on HANS data might be because all the examples used are of non-entailment, as the model might be memorizing this particular heuristic. The experiments to generate GPT-4 results with mixed data from HANS are ongoing.</p>
<p>She caught the first snowflake of Sher life on her tongue, she was naively excited to be spending a Swhat in a region that actually had snow? question concept: punishing choices: SA:cloud B:december C:air D:africa E:winterwhat in a region that actually had snow? question concept: punishing choices:Problem: Find $a+b+c$ if the graph 
of the equation $y=ax^2+bx+c$ 
is a parabola with vertex $(5,3)$, 
vertical axis of symmetry, and contains 
the point $(2,0)$. Level: Level 5 Type: 
Algebra </p>
<p>Non entailment
entailmentProblem: What is the greatest integer $x$ for which $\frac79 &gt; \frac{x} {13}$? Level: Level 3 Type: Prealgebra Solution: Sentence1 Tanks were developed by Britain and France, and were first used in combat by the British during a battle. Sentence2 Tanks were developed by Britain and France, and were first used in combat by the British during a battle with German forces. Entailment or Non-EntailmentFigure 2: Examples of sample prompts and the respective responses of GPT4 on CommonsenseQA, MATH and SuperGLUE datasets$a+b+c = -
\frac{7}{3}$. </p>
<p>the greatest 
integer $x$ that 
satisfies the 
inequality is $x = 
10$. </p>
<p>E:winter </p>
<p>Dataset/Task 
Sample Prompt 
GPT4 
response </p>
<p>Commonsense 
reasoning </p>
<p>Algebra </p>
<p>Prealgebra </p>
<p>SuperGLUE </p>
<p>Geometry </p>
<p>Text and patterns: For effective chain of thought. Aman Madaan, Amir Yazdanbakhsh, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. Figure 3: Accuracy of GPT4 on cognitive psychology tasksAman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022. Figure 3: Accuracy of GPT4 on cognitive psychology tasks</p>
<p>A basis for a mathematical theory of computation. John Mccarthy, Studies in Logic and the Foundations of Mathematics. Elsevier26John McCarthy. A basis for a mathematical theory of computation. In Studies in Logic and the Foundations of Mathematics, volume 26, pages 33-70. Elsevier, 1959.</p>
<p>Understanding natural language. Terry Winograd, Cognitive psychology. 31Terry Winograd. Understanding natural language. Cognitive psychology, 3(1):1-191, 1972.</p>
<p>Gpt-4 technical report. Openai, OpenAI. Gpt-4 technical report. 2023.</p>
<p>Hellaswag: Can a machine really finish your sentence?. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, Yejin Choi, arXiv:1905.07830arXiv preprintRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.</p>
<p>Winogrande: An adversarial winograd schema challenge at scale. Keisuke Sakaguchi, Le Ronan, Chandra Bras, Yejin Bhagavatula, Choi, Communications of the ACM. 649Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99-106, 2021.</p>
<p>Systematic generalization and emergent structures in transformers trained on structured tasks. Yuxuan Li, James L Mcclelland, arXiv:2210.00400arXiv preprintYuxuan Li and James L McClelland. Systematic generalization and emergent structures in transformers trained on structured tasks. arXiv preprint arXiv:2210.00400, 2022.</p>
<p>Probing the psychology of ai models. Richard Shiffrin, Melanie Mitchell, Proceedings of the National Academy of Sciences. 120102300963120Richard Shiffrin and Melanie Mitchell. Probing the psychology of ai models. Proceedings of the National Academy of Sciences, 120(10):e2300963120, 2023.</p>
<p>Superglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in neural information processing systems. 32Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32, 2019.</p>
<p>Commonsenseqa: A question answering challenge targeting commonsense knowledge. Alon Talmor, Jonathan Herzig, Nicholas Lourie, Jonathan Berant, arXiv:1811.00937arXiv preprintAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge. arXiv preprint arXiv:1811.00937, 2018.</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.03874arXiv preprintDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021.</p>
<p>Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. Thomas Mccoy, Ellie Pavlick, Tal Linzen, arXiv:1902.01007arXiv preprintR Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. arXiv preprint arXiv:1902.01007, 2019.</p>
<p>Using large language models to simulate multiple humans. Gati Aher, I Rosa, Adam Tauman Arriaga, Kalai, arXiv:2208.10264arXiv preprintGati Aher, Rosa I Arriaga, and Adam Tauman Kalai. Using large language models to simulate multiple humans. arXiv preprint arXiv:2208.10264, 2022.</p>            </div>
        </div>

    </div>
</body>
</html>
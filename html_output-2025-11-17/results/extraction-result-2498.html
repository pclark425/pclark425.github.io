<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2498 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2498</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2498</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-235417529</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2106.06356v2.pdf" target="_blank">Nonmyopic Multifidelity Active Search</a></p>
                <p><strong>Paper Abstract:</strong> Active search is a learning paradigm where we seek to identify as many members of a rare, valuable class as possible given a labeling budget. Previous work on active search has assumed access to a faithful (and expensive) oracle reporting experimental results. However, some settings offer access to cheaper surrogates such as computational simulation that may aid in the search. We propose a model of multifidelity active search, as well as a novel, computationally efficient policy for this setting that is motivated by state-of-the-art classical policies. Our policy is nonmyopic and budget aware, allowing for a dynamic tradeoff between exploration and exploitation. We evaluate the performance of our solution on real-world datasets and demonstrate significantly better performance than natural benchmarks.</p>
                <p><strong>Cost:</strong> 0.023</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2498.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2498.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MF-ENS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Efficient Nonmyopic Search (MF-ENS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-stage, budget-aware multifidelity active search policy that nonmyopically allocates parallel low-fidelity (L) and high-fidelity (H) queries to maximize expected discoveries on the expensive oracle by approximating the Bayesian optimal policy with an efficient rollout and greedy batching heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MF-ENS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>MF-ENS is a two-stage rollout policy for multifidelity active search with one exact (H) and one or more noisy lower-fidelity (L_i) oracles. In its lookahead it: (1) assumes after the current putative query remaining H budget will be spent in a final greedy H batch, (2) allows an intermediate simultaneous exploratory L batch of size k (the number of L queries available before the next H result) whose labels are marginalized, and (3) computes the expected improvement in the final H batch induced by each candidate L label. To select the L batch it computes a per-point value v(x|D)=E_{y_L}[max(Pr(y_H=1|x,y_L,D)-p*,0)] where p* is the lowest success probability in the current greedy H batch, and greedily picks the top-k v scores. The decision for the next query (on either fidelity) is the candidate maximizing the approximate expected terminal utility (expected number of H positives) after marginalizing the exploratory L labels. The method assumes conditional independence across points after the putative query except intra-point H–L coupling to keep computation tractable, employs pruning (upper bounds and partial pruning) and lazy evaluation to speed up candidate scoring, and can be extended to multiple low fidelities by marginalizing batches per fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Automated scientific discovery tasks such as drug discovery (compound screening) and materials discovery (bulk metallic glass identification); more generally rare-target search where experiments have different fidelities and latencies.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates queries by maximizing an approximation to the expected terminal utility (number of H positives). For each putative next query it simulates (via two-stage rollout) the effect of making k intermediate L queries and then greedily batching remaining H queries; L candidates are chosen by their expected marginal improvement v(x|D) to the final H batch, and H candidates by their posterior probability of being positive. The policy is budget-aware: it computes the number of remaining H queries H=(T-i-1)/(k+1) and sizes batches accordingly, and it marginalizes over possible L label outcomes when scoring candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured by algorithmic time complexity and wall-clock time: naive worst-case time complexity O(2^k n^2 log n) (paper states O 2^k n^2 log n) and optimized implementation O(2^k n (n + m log m)) where m is number of affected points; empirical wall-clock examples: ≈30 seconds to score 1000 candidates and under one hour to cover 100k points when pruning is unsuccessful. Cost also discussed in terms of number of posterior-marginalizations over label combinations (2^k factor).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected terminal utility defined as the expected number of targets found on H (i.e., sum of posterior probabilities of selected H batch members); approximated expected utility of a batch equals the sum of top posterior probabilities (linearity of expectation). For L queries the marginal expected increase in that sum (v(x|D)) is used.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Nonmyopic two-stage rollout balances exploitation and exploration: H queries include an immediate exploitation term Pr(y_H=1|x,D) plus expected future utility; L queries have no immediate utility but are valued by their expected marginal improvement to the final H batch. The mechanism is budget-aware (explicitly accounts for remaining H budget), smoothly shifting from exploration early (selecting lower-probability L/H queries to gain information) to exploitation later (selecting high-probability H points).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity-promoting objective; diversity arises implicitly via exploration on L (values v(x|D) may prefer points that can replace low-probability members of the H batch) and neighborhood-based probabilistic model, but there is no explicit determinantal/diversity constraint or diversity-regularized acquisition.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed discrete budget on H queries (t) with parallel L queries allowed at rate k (i.e., while waiting for an H result up to k L queries may be run); total scheduling horizon T determined by t and k.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget-awareness is built into the lookahead: remaining H queries H is computed and used to size the final greedy H batch; the rollout approximation explicitly marginalizes over L queries that can be made before the next H completion. The algorithm shortens intractable full lookahead via one- or two-stage rollout heuristics and can sample marginalizations when k is large.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>No separate novelty/breakthrough score—breakthroughs are operationalized as discovering more members of the rare valuable class on H; success measured by count of targets discovered on H.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics are numbers of H targets found (counts) averaged across repeated simulations and datasets (e.g., aggregate average 237 targets found by MF-ENS in reported experiments with H budget t=300). Also computational metrics: ≈30s to score 1000 candidates; pruning increases combined pruning rate to ≈99% on average when successful.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against single-fidelity ENS (Jiang et al., 2017), H-ENS (one-stage approximation), MF-UCB (Klyuchnikov et al., 2019 style UCB), and UG heuristic (uncertainty sampling on L + greedy on H).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MF-ENS outperforms all tested baselines substantially across multiple real-world datasets (drug and materials) and simulation settings; averaged across experiments MF-ENS finds ≈237 targets whereas H-ENS finds ≈235 (p = 0.044), and comparisons to ENS, MF-UCB and UG reject null differences with p-values ≤ 4×10^-5 in many columns. Plots show MF-ENS dominates ENS by roughly a linearly increasing margin in cumulative targets over the search horizon.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Pruning strategies (upper-bound total pruning and extended partial pruning) dramatically reduce compute: combined pruning rates reach ≈99% on average when successful; successful pruning reduces decision-time from under an hour to a few seconds in experiments. Algorithmic speedups (lazy evaluation, pruning, and focused m) reduce practical runtime; theoretical improvements from baseline ENS add a 2^k factor for exhaustive marginalization (sampled when necessary).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper analyzes tradeoffs empirically and algorithmically: MF-ENS explicitly trades off immediate reward (Pr(y_H=1|x)) against expected future reward via nonmyopic lookahead; it trades computation for better allocation (two-stage marginalization costs 2^k factor) and provides sampling approximations when k is large; experiments show increased use of L (larger k) and better L fidelity (lower noise θ) improve final discovery counts, validating the benefit of cheaply allocated information. The hardness of approximation result is cited to motivate that polynomial-time policies must use heuristics, and MF-ENS is positioned as an efficient approximation that balances computation and information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Key principles: (1) Run cheap low-fidelity queries in parallel to expensive experiments and use their information to adaptively alter which expensive experiments to run; (2) Intractable full lookahead can be approximated via multi-stage rollouts where cheap oracles are used to improve the final batch of expensive queries; (3) Choose L queries by expected marginal improvement to the final H batch (v(x|D)), swapping into the greedy H batch only when Pr(y_H|x,y_L,D) exceeds the current batch minimum p*; (4) Use pruning, lazy evaluation and conditional independence assumptions to make the allocation computationally tractable; (5) Be budget-aware: explicitly account for remaining expensive-query budget to automatically balance exploration and exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nonmyopic Multifidelity Active Search', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2498.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2498.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>H-ENS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>One-stage Multifidelity ENS (H-ENS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A one-stage adaptation of ENS to multifidelity settings that assumes after the putative query all remaining H queries will be spent simultaneously and ignores intermediate L queries in the lookahead.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>H-ENS (One-stage approximation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>H-ENS applies the ENS heuristic by assuming that following the putative query, all remaining budget on the high-fidelity oracle H is spent in one greedy batch; it uses the sum of the top posterior H probabilities to approximate expected remaining utility and selects the next query accordingly. It is simpler than MF-ENS because it does not account for future L queries in its lookahead.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Same scientific discovery tasks as MF-ENS (drug/materials discovery) used as an ablation/baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Greedy-batch-based selection for H: selects the next candidate by approximating the terminal expected utility as the sum of top posterior probabilities of H for the remaining budget, ignoring intermediate L queries.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Time complexity similar to ENS: O(n (log n + m log m)) for optimized version; in practice limited by candidate scoring and pruning thresholds.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected number of H discoveries approximated as the sum of posterior probabilities of the greedy H batch.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Nonmyopic but single-stage: implicitly balances exploration and exploitation through budget-aware batch selection, yet does not exploit the opportunity to run intermediate low-fidelity queries to improve the H batch.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity promotion—greedy top-probability batch selection.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed H budget; ignores L scheduling in lookahead.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Uses the remaining H budget to size and select the final greedy H batch but does not account for planned L queries between H queries.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Number of H positives found.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Across experiments H-ENS finds on average slightly fewer targets than MF-ENS (e.g., 235 vs 237 average in aggregate experiments) but outperforms purely myopic baselines in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MF-ENS, ENS, MF-UCB, UG in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>H-ENS performs well relative to myopic baselines and single-fidelity ENS; MF-ENS offers slight but significant improvement over H-ENS (p = 0.044 reported).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Lower computational cost than exhaustive two-stage marginalization (no 2^k marginalization), same complexity as single-fidelity ENS optimizations.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>H-ENS represents the tradeoff of computational simplicity vs potential information gained by intermediate L queries; ignoring L queries in lookahead reduces computation but misses gains from cheap parallel experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>A one-stage batch approximation (H-ENS) is effective and computationally cheaper than full rollouts, but explicitly modeling intermediate L queries (as in MF-ENS) yields additional discovery gains.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nonmyopic Multifidelity Active Search', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2498.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2498.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ENS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Efficient Nonmyopic Search (ENS)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A nonmyopic active search policy for single-fidelity settings that approximates the Bayesian optimal policy by assuming all remaining queries are spent simultaneously in a final batch, yielding an efficiently computable expected utility equal to the sum of top posterior probabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Efficient Nonmyopic Active Search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>ENS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>ENS is a single-fidelity nonmyopic policy that shortens the full Bellman lookahead by assuming that after the putative query all remaining budget will be used in a single greedy batch; the expected utility of that batch is the sum of the highest posterior probabilities, making selection efficient. ENS is budget-aware and was used as a baseline and the conceptual inspiration for MF-ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active search for rare targets in domains like drug discovery and materials (single-fidelity experimental settings).</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Selects the query that maximizes the approximate expected terminal utility computed by forming the greedy batch of top-probability points after the putative query (i.e., picks points whose inclusion increases the sum-of-probabilities-most for the remaining batch).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Algorithmic time complexity given as O(n (log n + m log m)) for optimized implementations; cost measured by number of candidate score computations and batch constructions.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected number of discovered positives (sum of posterior probabilities over the final batch).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Nonmyopic budget-aware lookahead via the batch-greedy heuristic: balances exploration and exploitation by considering remaining budget when selecting queries (implicitly prefers exploration early when budget large and exploitation later).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanisms; batch selection is greedy by posterior probability.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of queries (budget on total queries).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Incorporates remaining budget into the one-stage lookahead to size the final greedy batch and compute expected utility.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Count of positive labels discovered (expected sum).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>ENS reported as a strong single-fidelity baseline; in multifidelity experiments ENS (which ignores L) is outperformed by MF-ENS and other multifidelity-aware policies.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as a baseline for multifidelity methods in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>In the multifidelity experiments ENS (single-fidelity) performs worse than MF-ENS and other methods that leverage L; MF-ENS shows linearly increasing advantage over ENS in cumulative discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Efficient compared to full dynamic programming; computationally tractable for large candidate pools using efficient data structures and pruning.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>ENS embodies the compute vs lookahead tradeoff: shorter, tractable lookahead that gives strong empirical performance but can be improved when additional fidelities are leveraged.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Under single-fidelity settings, ENS is an effective approximate allocation strategy: use a budget-aware greedy batch approximation to capture nonmyopic value.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nonmyopic Multifidelity Active Search', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2498.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2498.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MF-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Upper Confidence Bound (MF-UCB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multifidelity UCB-style policy adapted for active search where each candidate is scored by α(x,D)=π + β π(1−π), balancing exploitation (posterior mean π) and exploration (proxy variance π(1−π)), used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MF-UCB</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An upper-confidence-bound style scoring policy applied to multifidelity active search (as adopted from Klyuchnikov et al., 2019): a candidate's score combines its posterior probability π of being positive and an exploration bonus βπ(1−π). Separate β parameters can be set for L and H queries to prioritize exploration on cheap fidelities (large β on L) and exploitation on expensive fidelities (small β on H). The policy selects the highest-scoring candidate at each step.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multifidelity recommender and discovery problems; used as a baseline in simulated discovery experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Scores and selects candidates by a UCB-like acquisition α(x,D)=π + βπ(1−π); different β for fidelities controls tradeoff between exploring (on L) and exploiting (on H).</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Not explicitly defined in paper; computational cost dominated by posterior probability computation and per-candidate score evaluation; empirical runtime depends on classifier and candidate pool size.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit exploration proxy via π(1−π) (Bernoulli variance) rather than explicit expected information or mutual information.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit via UCB-like score: increasing β increases exploration pressure; paper used β=0.01 for L and β=0.001 for H as suggested in referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity promotion beyond exploration bonus term; batch or correlation effects not explicitly modeled.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget on queries; per-step greedy selection until budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Does not perform multi-step lookahead or batch-aware budget reasoning; selects per-step by UCB score.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Number of targets discovered on H (counts).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in experiments as a myopic multifidelity baseline; MF-ENS outperforms MF-UCB across datasets and settings with statistically significant margins (p-values reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MF-ENS, ENS, UG, and H-ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MF-UCB is outperformed by MF-ENS; MF-ENS recovers from early-exploration deficits and surpasses MF-UCB in cumulative discoveries by end of searches.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Simple per-candidate scoring is computationally cheap compared to nonmyopic rollouts, but lacks budget-aware nonmyopia advantages.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Represents a computationally cheap, myopic tradeoff mechanism (variance-based exploration) that can be less effective than nonmyopic budget-aware strategies when cheap fidelities can be used to inform future expensive queries.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>UCB-style scoring with tuned β can be effective for online multifidelity allocation but may underperform nonmyopic policies that marginalize future cheap-query outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nonmyopic Multifidelity Active Search', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2498.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2498.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>UG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>UG (Uncertainty + Greedy) heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple multifidelity heuristic that always queries most-uncertain points on the low-fidelity oracle (exploration) and most-likely positive points on the high-fidelity oracle (exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>UG (Uncertainty sampling on L, greedy on H)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Hybrid heuristic: when making L queries, select points with maximum predictive uncertainty (e.g., near 0.5 posterior) to explore; when making H queries, select points with highest posterior probability of being positive to exploit. This mirrors a design where cheap queries are used to narrow search regions and expensive queries confirm likely positives.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multifidelity active search in discovery tasks; used as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Simple rule-based allocation: L queries targeted at high-uncertainty points; H queries at high-probability points. No lookahead or marginalization over label outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Low computational overhead—cost dominated by uncertainty/probability evaluations per candidate.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Implicit: uncertainty sampling aims to reduce model uncertainty but no explicit expected utility calculation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Hard-coded: L = exploration (maximizes uncertainty), H = exploitation (maximizes posterior probability).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism beyond the inherent diversity of uncertainty-selected points.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget on H and L determined by k ratio.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Does not explicitly account for remaining budget in a nonmyopic manner; selection rules are per-step and myopic.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Number of H positives found.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as a baseline; MF-ENS outperforms UG in experiments, though UG can perform well early due to strong exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MF-ENS, ENS, H-ENS, MF-UCB.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>UG is outperformed by MF-ENS, particularly in long-horizon performance where nonmyopic strategies recover and exceed UG's cumulative finds.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Very low computational cost; useful as a cheap heuristic when compute is severely limited.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Illustrates the limits of hard-coded exploration/exploitation splits—no dynamic budget-aware adjustment leads to suboptimal long-horizon discovery compared to nonmyopic policies.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Simple heuristics can be effective for initial exploration but should be supplanted by budget-aware nonmyopic allocation to maximize final discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nonmyopic Multifidelity Active Search', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2498.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2498.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Co-kriging (Klyuchnikov)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Co-kriging multifidelity predictive model (as used by Klyuchnikov et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Gaussian process-based co-kriging model that jointly models outputs of multiple fidelity information sources to infer the true high-fidelity label, previously proposed for multifidelity active search.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Figuring out the User in a Few Steps: Bayesian Multifidelity Active Search with Cokriging</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Co-kriging multifidelity model</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A co-kriging (multi-output Gaussian process) predictive model that models correlations between multiple fidelity information sources (e.g., model predictions as low fidelity and user labels as high fidelity), enabling Bayesian inference of the high-fidelity quantity conditioned on cheaper evaluations. Klyuchnikov et al. used this model with sequential querying and UCB-style policies.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Multifidelity recommendation / active search problems (user preference discovery) and more general multifidelity surrogate modeling.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Combined with sequential querying policies (e.g., UCB), resource allocation is driven by predictive posterior mean and covariance from the co-kriging model to pick informative/ promising points.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>GP-based cost measured in cubic complexity w.r.t. number of observations for naive implementations (e.g., O(n^3) for exact inference); not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Typically uses posterior mean/variance and acquisition functions (e.g., UCB, expected improvement) derived from GP predictive distributions; not explicitly detailed here.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Via Bayesian acquisition functions applied to co-kriging posterior (e.g., UCB or EI) in referenced work.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No explicit diversity mechanism described here; co-kriging captures correlations which can implicitly affect diversity of chosen queries.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Sequential query budget (fixed number of queries); in Klyuchnikov et al. queries made sequentially one after another.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Allocation chosen per-step using acquisition driven by co-kriging posterior; no parallel L/H latency model as in MF-ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Number of discovered targets or recommendation accuracy depending on application.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Not evaluated in this paper's experiments; referenced as prior related multifidelity work that could be adapted to different scheduling models.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned in related work; MF-ENS compared empirically to MF-UCB rather than directly to co-kriging implementations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not directly compared in this paper; authors note MF-ENS could be adopted to sequential settings like Klyuchnikov's but that marginalization differs due to parallel/pending H labels.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>GP-based co-kriging can provide strong statistical sharing across fidelities but can be computationally expensive for large datasets without approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Co-kriging trades statistical fidelity-sharing for computational cost; sequential querying reduces parallelism benefits available in settings modeled by MF-ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Co-kriging is a strong modeling choice for multifidelity inference; allocation rules must then be chosen to exploit the model (e.g., UCB, EI), but parallel latency differences are not handled in the same way as MF-ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nonmyopic Multifidelity Active Search', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2498.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2498.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multifidelity BO acquisitions</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multifidelity Bayesian Optimization acquisition functions (EI, KG, UCB adaptations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Adaptations of standard Bayesian optimization acquisition functions—Expected Improvement (EI), Knowledge Gradient (KG), and Upper Confidence Bound (UCB)—to the multifidelity setting where cheaper approximations inform expensive evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multifidelity BO acquisition functions</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Existing multifidelity BO literature adapts acquisition functions like Expected Improvement, Knowledge Gradient, and UCB to handle multiple information sources by modeling the correlation structure and cost differences between sources and modifying acquisition to trade off information gain per unit cost or expected improvement to the expensive objective.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Hyperparameter tuning, engineering design, and other optimization tasks where cheaper simulators inform expensive experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocation is driven by acquisition functions that combine expected utility (e.g., improvement, knowledge gradient) with cost considerations, selecting queries that maximize acquisition per unit cost or expected value of information.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Often expressed in cost units (dollars, wall-clock time) or number of evaluations per fidelity; some methods optimize acquisition/cost ratios.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected improvement, knowledge gradient (value of information), or UCB-based optimistic bounds derived from posterior mean and variance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Established acquisition functions explicitly balance exploration/exploitation (e.g., EI focuses on expected improvement, KG explicitly computes value of information, UCB provides optimism-driven exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>No general explicit diversity mechanism; batch BO extensions sometimes include diversity penalties or repulsion to avoid redundant evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Usually cost-constrained (monetary/time) or fixed evaluation budgets across fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Acquisitions are often cost-aware (choose the point/fidelity with highest acquisition per unit cost) or include fidelity selection in the acquisition optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Objective improvement (function value) or meeting performance thresholds; not binary-target oriented unless specialized.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in cited multifidelity BO literature; not directly experimented with in this paper but mentioned as related work.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Referenced to situate MF-ENS; no direct experimental comparison in this paper besides noting lack of nonmyopic multifidelity BO policies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>BO adaptations can reduce expensive evaluations by leveraging cheap fidelities; tradeoffs between model fidelity, cost, and acquisition computation exist.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Multifidelity BO literature explicitly studies tradeoffs between cost and expected information gain; MF-ENS differs by focusing on cumulative discovery and parallel latency-aware scheduling.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Acquisition design should incorporate fidelity-specific cost and information value; however, many BO-derived methods are greedy and not nonmyopic across mixed-fidelity parallel schedules as in MF-ENS.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nonmyopic Multifidelity Active Search', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2498.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2498.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-fidelity MAB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-fidelity Multi-armed Bandit (Kandasamy et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A formulation of the multi-armed bandit problem where each arm can be pulled at different fidelities with different costs and accuracies, and allocation seeks to maximize cumulative reward or minimize regret given fidelity tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Multi-fidelity Multi-armed Bandit</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-fidelity MAB</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Models each arm (candidate) as having multiple fidelities; pulls at low fidelity are cheaper but less accurate. Policies (e.g., fidelity-aware UCB variants) allocate pulls across arms and fidelities using fidelity-dependent accuracy assumptions to trade off exploration cost vs information quality, with theoretical regret guarantees when fidelity accuracies are known.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Sequential allocation problems, experimental design, and optimization tasks where multiple evaluation fidelities exist.</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Choose arm and fidelity per pull to maximize expected reward under cost constraints, often by upper-confidence bound style indices adjusted for fidelity accuracy and cost.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Typically counts of arm pulls per fidelity or aggregate cost; theoretical analyses often use regret per time step or per cost unit.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Proxy via confidence/variance estimates per arm-fidelity; exploration via UCB-style bonuses.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Index-based (UCB-like) exploration bonuses adjusted by fidelity accuracy/cost; theoretical bounds guide allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Not explicitly focused on diversity across hypothesis space; focuses instead on per-arm optimization under fidelity tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time or query budget; can be cost-weighted budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Policies manage per-step fidelity selection to trade off cost and information, with regret bounds under assumptions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Cumulative reward / regret; not necessarily binary target counts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Theoretical regret guarantees and empirical performance on bandit benchmarks; referenced as related methodology.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Related to UCB-family and multifidelity allocation algorithms; cited for theoretical fidelity-aware allocation results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not empirically compared in this paper; referenced to contextualize MF-ENS relative to bandit-theoretic fidelity-aware allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>When fidelity accuracies are known, can provide provably efficient fidelity allocations; practical performance depends on modeling accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Multi-fidelity MAB explicitly analyzes cost vs accuracy tradeoffs and provides allocation rules that balance exploration and exploitation across fidelities.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Provable allocation rules exist under certain assumptions; practical multifidelity discovery problems may require additional modeling (e.g., correlation structures) beyond classical MAB formulations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Nonmyopic Multifidelity Active Search', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Efficient Nonmyopic Active Search <em>(Rating: 2)</em></li>
                <li>Efficient nonmyopic batch active search <em>(Rating: 2)</em></li>
                <li>Figuring out the User in a Few Steps: Bayesian Multifidelity Active Search with Cokriging <em>(Rating: 2)</em></li>
                <li>The Multi-fidelity Multi-armed Bandit <em>(Rating: 2)</em></li>
                <li>Multi-Information Source Optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2498",
    "paper_id": "paper-235417529",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "MF-ENS",
            "name_full": "Multifidelity Efficient Nonmyopic Search (MF-ENS)",
            "brief_description": "A two-stage, budget-aware multifidelity active search policy that nonmyopically allocates parallel low-fidelity (L) and high-fidelity (H) queries to maximize expected discoveries on the expensive oracle by approximating the Bayesian optimal policy with an efficient rollout and greedy batching heuristics.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MF-ENS",
            "system_description": "MF-ENS is a two-stage rollout policy for multifidelity active search with one exact (H) and one or more noisy lower-fidelity (L_i) oracles. In its lookahead it: (1) assumes after the current putative query remaining H budget will be spent in a final greedy H batch, (2) allows an intermediate simultaneous exploratory L batch of size k (the number of L queries available before the next H result) whose labels are marginalized, and (3) computes the expected improvement in the final H batch induced by each candidate L label. To select the L batch it computes a per-point value v(x|D)=E_{y_L}[max(Pr(y_H=1|x,y_L,D)-p*,0)] where p* is the lowest success probability in the current greedy H batch, and greedily picks the top-k v scores. The decision for the next query (on either fidelity) is the candidate maximizing the approximate expected terminal utility (expected number of H positives) after marginalizing the exploratory L labels. The method assumes conditional independence across points after the putative query except intra-point H–L coupling to keep computation tractable, employs pruning (upper bounds and partial pruning) and lazy evaluation to speed up candidate scoring, and can be extended to multiple low fidelities by marginalizing batches per fidelity.",
            "application_domain": "Automated scientific discovery tasks such as drug discovery (compound screening) and materials discovery (bulk metallic glass identification); more generally rare-target search where experiments have different fidelities and latencies.",
            "resource_allocation_strategy": "Allocates queries by maximizing an approximation to the expected terminal utility (number of H positives). For each putative next query it simulates (via two-stage rollout) the effect of making k intermediate L queries and then greedily batching remaining H queries; L candidates are chosen by their expected marginal improvement v(x|D) to the final H batch, and H candidates by their posterior probability of being positive. The policy is budget-aware: it computes the number of remaining H queries H=(T-i-1)/(k+1) and sizes batches accordingly, and it marginalizes over possible L label outcomes when scoring candidates.",
            "computational_cost_metric": "Measured by algorithmic time complexity and wall-clock time: naive worst-case time complexity O(2^k n^2 log n) (paper states O 2^k n^2 log n) and optimized implementation O(2^k n (n + m log m)) where m is number of affected points; empirical wall-clock examples: ≈30 seconds to score 1000 candidates and under one hour to cover 100k points when pruning is unsuccessful. Cost also discussed in terms of number of posterior-marginalizations over label combinations (2^k factor).",
            "information_gain_metric": "Expected terminal utility defined as the expected number of targets found on H (i.e., sum of posterior probabilities of selected H batch members); approximated expected utility of a batch equals the sum of top posterior probabilities (linearity of expectation). For L queries the marginal expected increase in that sum (v(x|D)) is used.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Nonmyopic two-stage rollout balances exploitation and exploration: H queries include an immediate exploitation term Pr(y_H=1|x,D) plus expected future utility; L queries have no immediate utility but are valued by their expected marginal improvement to the final H batch. The mechanism is budget-aware (explicitly accounts for remaining H budget), smoothly shifting from exploration early (selecting lower-probability L/H queries to gain information) to exploitation later (selecting high-probability H points).",
            "diversity_mechanism": "No explicit diversity-promoting objective; diversity arises implicitly via exploration on L (values v(x|D) may prefer points that can replace low-probability members of the H batch) and neighborhood-based probabilistic model, but there is no explicit determinantal/diversity constraint or diversity-regularized acquisition.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed discrete budget on H queries (t) with parallel L queries allowed at rate k (i.e., while waiting for an H result up to k L queries may be run); total scheduling horizon T determined by t and k.",
            "budget_constraint_handling": "Budget-awareness is built into the lookahead: remaining H queries H is computed and used to size the final greedy H batch; the rollout approximation explicitly marginalizes over L queries that can be made before the next H completion. The algorithm shortens intractable full lookahead via one- or two-stage rollout heuristics and can sample marginalizations when k is large.",
            "breakthrough_discovery_metric": "No separate novelty/breakthrough score—breakthroughs are operationalized as discovering more members of the rare valuable class on H; success measured by count of targets discovered on H.",
            "performance_metrics": "Reported metrics are numbers of H targets found (counts) averaged across repeated simulations and datasets (e.g., aggregate average 237 targets found by MF-ENS in reported experiments with H budget t=300). Also computational metrics: ≈30s to score 1000 candidates; pruning increases combined pruning rate to ≈99% on average when successful.",
            "comparison_baseline": "Compared against single-fidelity ENS (Jiang et al., 2017), H-ENS (one-stage approximation), MF-UCB (Klyuchnikov et al., 2019 style UCB), and UG heuristic (uncertainty sampling on L + greedy on H).",
            "performance_vs_baseline": "MF-ENS outperforms all tested baselines substantially across multiple real-world datasets (drug and materials) and simulation settings; averaged across experiments MF-ENS finds ≈237 targets whereas H-ENS finds ≈235 (p = 0.044), and comparisons to ENS, MF-UCB and UG reject null differences with p-values ≤ 4×10^-5 in many columns. Plots show MF-ENS dominates ENS by roughly a linearly increasing margin in cumulative targets over the search horizon.",
            "efficiency_gain": "Pruning strategies (upper-bound total pruning and extended partial pruning) dramatically reduce compute: combined pruning rates reach ≈99% on average when successful; successful pruning reduces decision-time from under an hour to a few seconds in experiments. Algorithmic speedups (lazy evaluation, pruning, and focused m) reduce practical runtime; theoretical improvements from baseline ENS add a 2^k factor for exhaustive marginalization (sampled when necessary).",
            "tradeoff_analysis": "The paper analyzes tradeoffs empirically and algorithmically: MF-ENS explicitly trades off immediate reward (Pr(y_H=1|x)) against expected future reward via nonmyopic lookahead; it trades computation for better allocation (two-stage marginalization costs 2^k factor) and provides sampling approximations when k is large; experiments show increased use of L (larger k) and better L fidelity (lower noise θ) improve final discovery counts, validating the benefit of cheaply allocated information. The hardness of approximation result is cited to motivate that polynomial-time policies must use heuristics, and MF-ENS is positioned as an efficient approximation that balances computation and information gain.",
            "optimal_allocation_findings": "Key principles: (1) Run cheap low-fidelity queries in parallel to expensive experiments and use their information to adaptively alter which expensive experiments to run; (2) Intractable full lookahead can be approximated via multi-stage rollouts where cheap oracles are used to improve the final batch of expensive queries; (3) Choose L queries by expected marginal improvement to the final H batch (v(x|D)), swapping into the greedy H batch only when Pr(y_H|x,y_L,D) exceeds the current batch minimum p*; (4) Use pruning, lazy evaluation and conditional independence assumptions to make the allocation computationally tractable; (5) Be budget-aware: explicitly account for remaining expensive-query budget to automatically balance exploration and exploitation.",
            "uuid": "e2498.0",
            "source_info": {
                "paper_title": "Nonmyopic Multifidelity Active Search",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "H-ENS",
            "name_full": "One-stage Multifidelity ENS (H-ENS)",
            "brief_description": "A one-stage adaptation of ENS to multifidelity settings that assumes after the putative query all remaining H queries will be spent simultaneously and ignores intermediate L queries in the lookahead.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "H-ENS (One-stage approximation)",
            "system_description": "H-ENS applies the ENS heuristic by assuming that following the putative query, all remaining budget on the high-fidelity oracle H is spent in one greedy batch; it uses the sum of the top posterior H probabilities to approximate expected remaining utility and selects the next query accordingly. It is simpler than MF-ENS because it does not account for future L queries in its lookahead.",
            "application_domain": "Same scientific discovery tasks as MF-ENS (drug/materials discovery) used as an ablation/baseline.",
            "resource_allocation_strategy": "Greedy-batch-based selection for H: selects the next candidate by approximating the terminal expected utility as the sum of top posterior probabilities of H for the remaining budget, ignoring intermediate L queries.",
            "computational_cost_metric": "Time complexity similar to ENS: O(n (log n + m log m)) for optimized version; in practice limited by candidate scoring and pruning thresholds.",
            "information_gain_metric": "Expected number of H discoveries approximated as the sum of posterior probabilities of the greedy H batch.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Nonmyopic but single-stage: implicitly balances exploration and exploitation through budget-aware batch selection, yet does not exploit the opportunity to run intermediate low-fidelity queries to improve the H batch.",
            "diversity_mechanism": "No explicit diversity promotion—greedy top-probability batch selection.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed H budget; ignores L scheduling in lookahead.",
            "budget_constraint_handling": "Uses the remaining H budget to size and select the final greedy H batch but does not account for planned L queries between H queries.",
            "breakthrough_discovery_metric": "Number of H positives found.",
            "performance_metrics": "Across experiments H-ENS finds on average slightly fewer targets than MF-ENS (e.g., 235 vs 237 average in aggregate experiments) but outperforms purely myopic baselines in many settings.",
            "comparison_baseline": "Compared against MF-ENS, ENS, MF-UCB, UG in experiments.",
            "performance_vs_baseline": "H-ENS performs well relative to myopic baselines and single-fidelity ENS; MF-ENS offers slight but significant improvement over H-ENS (p = 0.044 reported).",
            "efficiency_gain": "Lower computational cost than exhaustive two-stage marginalization (no 2^k marginalization), same complexity as single-fidelity ENS optimizations.",
            "tradeoff_analysis": "H-ENS represents the tradeoff of computational simplicity vs potential information gained by intermediate L queries; ignoring L queries in lookahead reduces computation but misses gains from cheap parallel experiments.",
            "optimal_allocation_findings": "A one-stage batch approximation (H-ENS) is effective and computationally cheaper than full rollouts, but explicitly modeling intermediate L queries (as in MF-ENS) yields additional discovery gains.",
            "uuid": "e2498.1",
            "source_info": {
                "paper_title": "Nonmyopic Multifidelity Active Search",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "ENS",
            "name_full": "Efficient Nonmyopic Search (ENS)",
            "brief_description": "A nonmyopic active search policy for single-fidelity settings that approximates the Bayesian optimal policy by assuming all remaining queries are spent simultaneously in a final batch, yielding an efficiently computable expected utility equal to the sum of top posterior probabilities.",
            "citation_title": "Efficient Nonmyopic Active Search",
            "mention_or_use": "use",
            "system_name": "ENS",
            "system_description": "ENS is a single-fidelity nonmyopic policy that shortens the full Bellman lookahead by assuming that after the putative query all remaining budget will be used in a single greedy batch; the expected utility of that batch is the sum of the highest posterior probabilities, making selection efficient. ENS is budget-aware and was used as a baseline and the conceptual inspiration for MF-ENS.",
            "application_domain": "Active search for rare targets in domains like drug discovery and materials (single-fidelity experimental settings).",
            "resource_allocation_strategy": "Selects the query that maximizes the approximate expected terminal utility computed by forming the greedy batch of top-probability points after the putative query (i.e., picks points whose inclusion increases the sum-of-probabilities-most for the remaining batch).",
            "computational_cost_metric": "Algorithmic time complexity given as O(n (log n + m log m)) for optimized implementations; cost measured by number of candidate score computations and batch constructions.",
            "information_gain_metric": "Expected number of discovered positives (sum of posterior probabilities over the final batch).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Nonmyopic budget-aware lookahead via the batch-greedy heuristic: balances exploration and exploitation by considering remaining budget when selecting queries (implicitly prefers exploration early when budget large and exploitation later).",
            "diversity_mechanism": "No explicit diversity mechanisms; batch selection is greedy by posterior probability.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed number of queries (budget on total queries).",
            "budget_constraint_handling": "Incorporates remaining budget into the one-stage lookahead to size the final greedy batch and compute expected utility.",
            "breakthrough_discovery_metric": "Count of positive labels discovered (expected sum).",
            "performance_metrics": "ENS reported as a strong single-fidelity baseline; in multifidelity experiments ENS (which ignores L) is outperformed by MF-ENS and other multifidelity-aware policies.",
            "comparison_baseline": "Used as a baseline for multifidelity methods in the paper.",
            "performance_vs_baseline": "In the multifidelity experiments ENS (single-fidelity) performs worse than MF-ENS and other methods that leverage L; MF-ENS shows linearly increasing advantage over ENS in cumulative discoveries.",
            "efficiency_gain": "Efficient compared to full dynamic programming; computationally tractable for large candidate pools using efficient data structures and pruning.",
            "tradeoff_analysis": "ENS embodies the compute vs lookahead tradeoff: shorter, tractable lookahead that gives strong empirical performance but can be improved when additional fidelities are leveraged.",
            "optimal_allocation_findings": "Under single-fidelity settings, ENS is an effective approximate allocation strategy: use a budget-aware greedy batch approximation to capture nonmyopic value.",
            "uuid": "e2498.2",
            "source_info": {
                "paper_title": "Nonmyopic Multifidelity Active Search",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "MF-UCB",
            "name_full": "Multifidelity Upper Confidence Bound (MF-UCB)",
            "brief_description": "A multifidelity UCB-style policy adapted for active search where each candidate is scored by α(x,D)=π + β π(1−π), balancing exploitation (posterior mean π) and exploration (proxy variance π(1−π)), used as a baseline in experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "MF-UCB",
            "system_description": "An upper-confidence-bound style scoring policy applied to multifidelity active search (as adopted from Klyuchnikov et al., 2019): a candidate's score combines its posterior probability π of being positive and an exploration bonus βπ(1−π). Separate β parameters can be set for L and H queries to prioritize exploration on cheap fidelities (large β on L) and exploitation on expensive fidelities (small β on H). The policy selects the highest-scoring candidate at each step.",
            "application_domain": "Multifidelity recommender and discovery problems; used as a baseline in simulated discovery experiments.",
            "resource_allocation_strategy": "Scores and selects candidates by a UCB-like acquisition α(x,D)=π + βπ(1−π); different β for fidelities controls tradeoff between exploring (on L) and exploiting (on H).",
            "computational_cost_metric": "Not explicitly defined in paper; computational cost dominated by posterior probability computation and per-candidate score evaluation; empirical runtime depends on classifier and candidate pool size.",
            "information_gain_metric": "Implicit exploration proxy via π(1−π) (Bernoulli variance) rather than explicit expected information or mutual information.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Explicit via UCB-like score: increasing β increases exploration pressure; paper used β=0.01 for L and β=0.001 for H as suggested in referenced work.",
            "diversity_mechanism": "No explicit diversity promotion beyond exploration bonus term; batch or correlation effects not explicitly modeled.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed budget on queries; per-step greedy selection until budget exhausted.",
            "budget_constraint_handling": "Does not perform multi-step lookahead or batch-aware budget reasoning; selects per-step by UCB score.",
            "breakthrough_discovery_metric": "Number of targets discovered on H (counts).",
            "performance_metrics": "Reported in experiments as a myopic multifidelity baseline; MF-ENS outperforms MF-UCB across datasets and settings with statistically significant margins (p-values reported).",
            "comparison_baseline": "Compared to MF-ENS, ENS, UG, and H-ENS.",
            "performance_vs_baseline": "MF-UCB is outperformed by MF-ENS; MF-ENS recovers from early-exploration deficits and surpasses MF-UCB in cumulative discoveries by end of searches.",
            "efficiency_gain": "Simple per-candidate scoring is computationally cheap compared to nonmyopic rollouts, but lacks budget-aware nonmyopia advantages.",
            "tradeoff_analysis": "Represents a computationally cheap, myopic tradeoff mechanism (variance-based exploration) that can be less effective than nonmyopic budget-aware strategies when cheap fidelities can be used to inform future expensive queries.",
            "optimal_allocation_findings": "UCB-style scoring with tuned β can be effective for online multifidelity allocation but may underperform nonmyopic policies that marginalize future cheap-query outcomes.",
            "uuid": "e2498.3",
            "source_info": {
                "paper_title": "Nonmyopic Multifidelity Active Search",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "UG",
            "name_full": "UG (Uncertainty + Greedy) heuristic",
            "brief_description": "A simple multifidelity heuristic that always queries most-uncertain points on the low-fidelity oracle (exploration) and most-likely positive points on the high-fidelity oracle (exploitation).",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "UG (Uncertainty sampling on L, greedy on H)",
            "system_description": "Hybrid heuristic: when making L queries, select points with maximum predictive uncertainty (e.g., near 0.5 posterior) to explore; when making H queries, select points with highest posterior probability of being positive to exploit. This mirrors a design where cheap queries are used to narrow search regions and expensive queries confirm likely positives.",
            "application_domain": "Multifidelity active search in discovery tasks; used as a baseline.",
            "resource_allocation_strategy": "Simple rule-based allocation: L queries targeted at high-uncertainty points; H queries at high-probability points. No lookahead or marginalization over label outcomes.",
            "computational_cost_metric": "Low computational overhead—cost dominated by uncertainty/probability evaluations per candidate.",
            "information_gain_metric": "Implicit: uncertainty sampling aims to reduce model uncertainty but no explicit expected utility calculation.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Hard-coded: L = exploration (maximizes uncertainty), H = exploitation (maximizes posterior probability).",
            "diversity_mechanism": "No explicit diversity mechanism beyond the inherent diversity of uncertainty-selected points.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed budget on H and L determined by k ratio.",
            "budget_constraint_handling": "Does not explicitly account for remaining budget in a nonmyopic manner; selection rules are per-step and myopic.",
            "breakthrough_discovery_metric": "Number of H positives found.",
            "performance_metrics": "Used as a baseline; MF-ENS outperforms UG in experiments, though UG can perform well early due to strong exploration.",
            "comparison_baseline": "Compared against MF-ENS, ENS, H-ENS, MF-UCB.",
            "performance_vs_baseline": "UG is outperformed by MF-ENS, particularly in long-horizon performance where nonmyopic strategies recover and exceed UG's cumulative finds.",
            "efficiency_gain": "Very low computational cost; useful as a cheap heuristic when compute is severely limited.",
            "tradeoff_analysis": "Illustrates the limits of hard-coded exploration/exploitation splits—no dynamic budget-aware adjustment leads to suboptimal long-horizon discovery compared to nonmyopic policies.",
            "optimal_allocation_findings": "Simple heuristics can be effective for initial exploration but should be supplanted by budget-aware nonmyopic allocation to maximize final discoveries.",
            "uuid": "e2498.4",
            "source_info": {
                "paper_title": "Nonmyopic Multifidelity Active Search",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Co-kriging (Klyuchnikov)",
            "name_full": "Co-kriging multifidelity predictive model (as used by Klyuchnikov et al.)",
            "brief_description": "A Gaussian process-based co-kriging model that jointly models outputs of multiple fidelity information sources to infer the true high-fidelity label, previously proposed for multifidelity active search.",
            "citation_title": "Figuring out the User in a Few Steps: Bayesian Multifidelity Active Search with Cokriging",
            "mention_or_use": "mention",
            "system_name": "Co-kriging multifidelity model",
            "system_description": "A co-kriging (multi-output Gaussian process) predictive model that models correlations between multiple fidelity information sources (e.g., model predictions as low fidelity and user labels as high fidelity), enabling Bayesian inference of the high-fidelity quantity conditioned on cheaper evaluations. Klyuchnikov et al. used this model with sequential querying and UCB-style policies.",
            "application_domain": "Multifidelity recommendation / active search problems (user preference discovery) and more general multifidelity surrogate modeling.",
            "resource_allocation_strategy": "Combined with sequential querying policies (e.g., UCB), resource allocation is driven by predictive posterior mean and covariance from the co-kriging model to pick informative/ promising points.",
            "computational_cost_metric": "GP-based cost measured in cubic complexity w.r.t. number of observations for naive implementations (e.g., O(n^3) for exact inference); not detailed in this paper.",
            "information_gain_metric": "Typically uses posterior mean/variance and acquisition functions (e.g., UCB, expected improvement) derived from GP predictive distributions; not explicitly detailed here.",
            "uses_information_gain": null,
            "exploration_exploitation_mechanism": "Via Bayesian acquisition functions applied to co-kriging posterior (e.g., UCB or EI) in referenced work.",
            "diversity_mechanism": "No explicit diversity mechanism described here; co-kriging captures correlations which can implicitly affect diversity of chosen queries.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Sequential query budget (fixed number of queries); in Klyuchnikov et al. queries made sequentially one after another.",
            "budget_constraint_handling": "Allocation chosen per-step using acquisition driven by co-kriging posterior; no parallel L/H latency model as in MF-ENS.",
            "breakthrough_discovery_metric": "Number of discovered targets or recommendation accuracy depending on application.",
            "performance_metrics": "Not evaluated in this paper's experiments; referenced as prior related multifidelity work that could be adapted to different scheduling models.",
            "comparison_baseline": "Mentioned in related work; MF-ENS compared empirically to MF-UCB rather than directly to co-kriging implementations.",
            "performance_vs_baseline": "Not directly compared in this paper; authors note MF-ENS could be adopted to sequential settings like Klyuchnikov's but that marginalization differs due to parallel/pending H labels.",
            "efficiency_gain": "GP-based co-kriging can provide strong statistical sharing across fidelities but can be computationally expensive for large datasets without approximation.",
            "tradeoff_analysis": "Co-kriging trades statistical fidelity-sharing for computational cost; sequential querying reduces parallelism benefits available in settings modeled by MF-ENS.",
            "optimal_allocation_findings": "Co-kriging is a strong modeling choice for multifidelity inference; allocation rules must then be chosen to exploit the model (e.g., UCB, EI), but parallel latency differences are not handled in the same way as MF-ENS.",
            "uuid": "e2498.5",
            "source_info": {
                "paper_title": "Nonmyopic Multifidelity Active Search",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Multifidelity BO acquisitions",
            "name_full": "Multifidelity Bayesian Optimization acquisition functions (EI, KG, UCB adaptations)",
            "brief_description": "Adaptations of standard Bayesian optimization acquisition functions—Expected Improvement (EI), Knowledge Gradient (KG), and Upper Confidence Bound (UCB)—to the multifidelity setting where cheaper approximations inform expensive evaluations.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Multifidelity BO acquisition functions",
            "system_description": "Existing multifidelity BO literature adapts acquisition functions like Expected Improvement, Knowledge Gradient, and UCB to handle multiple information sources by modeling the correlation structure and cost differences between sources and modifying acquisition to trade off information gain per unit cost or expected improvement to the expensive objective.",
            "application_domain": "Hyperparameter tuning, engineering design, and other optimization tasks where cheaper simulators inform expensive experiments.",
            "resource_allocation_strategy": "Allocation is driven by acquisition functions that combine expected utility (e.g., improvement, knowledge gradient) with cost considerations, selecting queries that maximize acquisition per unit cost or expected value of information.",
            "computational_cost_metric": "Often expressed in cost units (dollars, wall-clock time) or number of evaluations per fidelity; some methods optimize acquisition/cost ratios.",
            "information_gain_metric": "Expected improvement, knowledge gradient (value of information), or UCB-based optimistic bounds derived from posterior mean and variance.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Established acquisition functions explicitly balance exploration/exploitation (e.g., EI focuses on expected improvement, KG explicitly computes value of information, UCB provides optimism-driven exploration).",
            "diversity_mechanism": "No general explicit diversity mechanism; batch BO extensions sometimes include diversity penalties or repulsion to avoid redundant evaluations.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Usually cost-constrained (monetary/time) or fixed evaluation budgets across fidelities.",
            "budget_constraint_handling": "Acquisitions are often cost-aware (choose the point/fidelity with highest acquisition per unit cost) or include fidelity selection in the acquisition optimization.",
            "breakthrough_discovery_metric": "Objective improvement (function value) or meeting performance thresholds; not binary-target oriented unless specialized.",
            "performance_metrics": "Reported in cited multifidelity BO literature; not directly experimented with in this paper but mentioned as related work.",
            "comparison_baseline": "Referenced to situate MF-ENS; no direct experimental comparison in this paper besides noting lack of nonmyopic multifidelity BO policies.",
            "performance_vs_baseline": "N/A in this paper.",
            "efficiency_gain": "BO adaptations can reduce expensive evaluations by leveraging cheap fidelities; tradeoffs between model fidelity, cost, and acquisition computation exist.",
            "tradeoff_analysis": "Multifidelity BO literature explicitly studies tradeoffs between cost and expected information gain; MF-ENS differs by focusing on cumulative discovery and parallel latency-aware scheduling.",
            "optimal_allocation_findings": "Acquisition design should incorporate fidelity-specific cost and information value; however, many BO-derived methods are greedy and not nonmyopic across mixed-fidelity parallel schedules as in MF-ENS.",
            "uuid": "e2498.6",
            "source_info": {
                "paper_title": "Nonmyopic Multifidelity Active Search",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Multi-fidelity MAB",
            "name_full": "Multi-fidelity Multi-armed Bandit (Kandasamy et al.)",
            "brief_description": "A formulation of the multi-armed bandit problem where each arm can be pulled at different fidelities with different costs and accuracies, and allocation seeks to maximize cumulative reward or minimize regret given fidelity tradeoffs.",
            "citation_title": "The Multi-fidelity Multi-armed Bandit",
            "mention_or_use": "mention",
            "system_name": "Multi-fidelity MAB",
            "system_description": "Models each arm (candidate) as having multiple fidelities; pulls at low fidelity are cheaper but less accurate. Policies (e.g., fidelity-aware UCB variants) allocate pulls across arms and fidelities using fidelity-dependent accuracy assumptions to trade off exploration cost vs information quality, with theoretical regret guarantees when fidelity accuracies are known.",
            "application_domain": "Sequential allocation problems, experimental design, and optimization tasks where multiple evaluation fidelities exist.",
            "resource_allocation_strategy": "Choose arm and fidelity per pull to maximize expected reward under cost constraints, often by upper-confidence bound style indices adjusted for fidelity accuracy and cost.",
            "computational_cost_metric": "Typically counts of arm pulls per fidelity or aggregate cost; theoretical analyses often use regret per time step or per cost unit.",
            "information_gain_metric": "Proxy via confidence/variance estimates per arm-fidelity; exploration via UCB-style bonuses.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Index-based (UCB-like) exploration bonuses adjusted by fidelity accuracy/cost; theoretical bounds guide allocations.",
            "diversity_mechanism": "Not explicitly focused on diversity across hypothesis space; focuses instead on per-arm optimization under fidelity tradeoffs.",
            "uses_diversity_promotion": null,
            "budget_constraint_type": "Time or query budget; can be cost-weighted budgets.",
            "budget_constraint_handling": "Policies manage per-step fidelity selection to trade off cost and information, with regret bounds under assumptions.",
            "breakthrough_discovery_metric": "Cumulative reward / regret; not necessarily binary target counts.",
            "performance_metrics": "Theoretical regret guarantees and empirical performance on bandit benchmarks; referenced as related methodology.",
            "comparison_baseline": "Related to UCB-family and multifidelity allocation algorithms; cited for theoretical fidelity-aware allocation results.",
            "performance_vs_baseline": "Not empirically compared in this paper; referenced to contextualize MF-ENS relative to bandit-theoretic fidelity-aware allocation.",
            "efficiency_gain": "When fidelity accuracies are known, can provide provably efficient fidelity allocations; practical performance depends on modeling accuracy.",
            "tradeoff_analysis": "Multi-fidelity MAB explicitly analyzes cost vs accuracy tradeoffs and provides allocation rules that balance exploration and exploitation across fidelities.",
            "optimal_allocation_findings": "Provable allocation rules exist under certain assumptions; practical multifidelity discovery problems may require additional modeling (e.g., correlation structures) beyond classical MAB formulations.",
            "uuid": "e2498.7",
            "source_info": {
                "paper_title": "Nonmyopic Multifidelity Active Search",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Efficient Nonmyopic Active Search",
            "rating": 2,
            "sanitized_title": "efficient_nonmyopic_active_search"
        },
        {
            "paper_title": "Efficient nonmyopic batch active search",
            "rating": 2,
            "sanitized_title": "efficient_nonmyopic_batch_active_search"
        },
        {
            "paper_title": "Figuring out the User in a Few Steps: Bayesian Multifidelity Active Search with Cokriging",
            "rating": 2,
            "sanitized_title": "figuring_out_the_user_in_a_few_steps_bayesian_multifidelity_active_search_with_cokriging"
        },
        {
            "paper_title": "The Multi-fidelity Multi-armed Bandit",
            "rating": 2,
            "sanitized_title": "the_multifidelity_multiarmed_bandit"
        },
        {
            "paper_title": "Multi-Information Source Optimization",
            "rating": 1,
            "sanitized_title": "multiinformation_source_optimization"
        }
    ],
    "cost": 0.022659,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Nonmyopic Multifidelity Active Search</p>
<p>Quan Nguyen 
Arghavan Modiri 
Roman Garnett 
Nonmyopic Multifidelity Active Search</p>
<p>Active search is a learning paradigm where we seek to identify as many members of a rare, valuable class as possible given a labeling budget. Previous work on active search has assumed access to a faithful (and expensive) oracle reporting experimental results. However, some settings offer access to cheaper surrogates such as computational simulation that may aid in the search. We propose a model of multifidelity active search, as well as a novel, computationally efficient policy for this setting that is motivated by state-of-the-art classical policies. Our policy is nonmyopic and budget aware, allowing for a dynamic tradeoff between exploration and exploitation. We evaluate the performance of our solution on real-world datasets and demonstrate significantly better performance than natural benchmarks.</p>
<p>Introduction</p>
<p>The goal of active search is to identify members of a rare and valuable class among a large pool of unlabeled points. This is a simple model of many real-world discovery problems, such as drug discovery and fraud detection. Active search proceeds by successively querying an oracle that returns a binary label indicating whether or not a chosen data point exhibits the desired properties. In many applications, this oracle is expensive, limiting the number of queries that could be made. The challenge is to design a policy to sequentially query the oracle in order to discover as many targets as possible, subject to a given labeling budget.</p>
<p>Active search has been extensively studied under various settings (Garnett et al., 2012;Jiang et al., 2017;2018;. Notably, Jiang et al. (2017) proved a hardness of approximation result, showing that no polynomial-time policy can approximate the performance of the Bayesian optimal policy within any constant factor. Thus active search is surprisingly hard. However, the authors also proposed an efficient approximation to the optimal policy that delivers impressive empirical performance. The key feature of their policy is its ability to account for the remaining budget and dynamically trade off exploration and exploitation.</p>
<p>Most previous work on active search has assumed access to only a single expensive oracle providing labels. In practice, however, we may have several methods of probing the search space, including cheap, low-fidelity surrogates. For example, a computational simulation may serve as a noisy approximation to an experiment done in a laboratory, and we may reasonably seek to use a cheaper computational search to help design the expensive experiments. This motivates the problem of multifidelity active search, where oracles of different fidelities may be simultaneously accessed to accelerate the search process. The central question in this task is how to effectively leverage these oracles in order to maximize the rate of discovery. We present a model for multifidelity active search and study the problem under the framework of Bayesian decision theory. We propose a novel policy for this setting inspired by the state-of-the-art single-fidelity policy mentioned above. The result is an efficient approximation to the optimal multifidelity policy that is specifically tailored to take advantage of low-fidelity oracles. We also create aggressive branchand-bound pruning strategies to increase the efficiency of our proposed algorithm, enabling scaling to large datasets. In a series of experiments, we investigate the performance of our policy on several real-world datasets for scientific discovery. Our solution outperforms various baselines from the literature by a large margin.</p>
<p>Problem Definition</p>
<p>We first introduce the general active search model and notations. Suppose we are given a finite set of points X {x i }, which includes a rare, valuable subset R ⊂ X . The members of R, which we call targets or positives, are not known a priori, but whether a given point x ∈ X is a target can be determined by making a query to an oracle that returns the binary label y 1{x ∈ R}. We assume that querying the oracle is expensive and that we only have a limited budget of t queries to do so. We denote a given dataset of queried points and their corresponding labels as D = {(x i , y i )}.  Figure 1. An illustration of our multifidelity active search model. Each short vertical line indicates when a query finishes and the next query is made. The numbers indicate the order in which queries are made across the two oracles. The low-fidelity oracle is k = 2 times faster than the high-fidelity oracle. The budget on H is t = 10; in total, T = t + kt − k = 28 queries are made.</p>
<p>times, we will use D i to denote the dataset collected after i queries.</p>
<p>Multifidelity Active Search</p>
<p>In addition to the expensive oracle that returns the exact label of a query, we have access to other cheaper but more noisy oracles that are low-fidelity approximations to the exact oracle. We limit our setting to two levels of fidelityone exact oracle and one noisy oracle, denoted as H and L, respectively-but note that our proposed algorithm can be extended to more than two fidelities with minor modifications, as we will discuss later.</p>
<p>For a given point x ∈ X , we denote its exact label on H as y H and its noisy label on L as y L . Under this setting, a dataset D can be partitioned into D L , the observations made on L, and D H , those that are made on H. Recall that our objective is to query as many targets as possible; as such, to express our preference over different datasets, we use the natural utility function
u (D = D L ∪ D H ) yi∈D H y i ,
the number of targets discovered on the H fidelity.</p>
<p>We assume that queries to different oracles are run in parallel, but a high-fidelity query takes longer to complete than a low-fidelity one. In particular, we will assume that each query on H is k times slower than a query on L. 1 That is, each time we make a query on H, we may make k sequential queries on L while waiting for the result. The process will then repeat until the budget is depleted. This model aims to emulate real-life scientific discovery procedures, the main motivation for active search, where multiple fidelities (e.g., computational and experimental campaigns) are often run in parallel but vary in response time.</p>
<p>If we are given a budget of t queries on H, we can make kt − k queries on L; in total, T = t + kt − k queries are made. (Although a total of kt queries can be made on L, after the final query on H is made at iteration T , further queries on L do not affect the final utility. We thus terminate after T queries.) This query schedule illustrated in Figure 1 for t = 10 and k = 2.</p>
<p>The Bayesian Optimal Policy</p>
<p>We now derive the optimal (expected-case) policy using Bayesian decision theory. First, we assume access to a probabilistic classification model that computes the posterior probability that a point x ∈ X is a target given a dataset D, Pr (y H = 1 | x, D), as well as the posterior probability that the same x is a positive on L, Pr (y L = 1 | x, D).</p>
<p>Suppose we are currently at iteration i + 1 ≤ T , having observed the dataset D i , and now need to make the next query, requesting the label of an unlabeled point x i+1 . The Bayesian optimal policy selects the point that maximizes the expected utility of the terminal dataset D T , assuming future queries will too be made optimally:
x * i+1 = arg max xi+1∈X \Di E u (D T \ D i ) | x i+1 , D i .
If the query is on H, the expectation is taken over the posterior distribution of the label of the putative query x i+1 ; if the query is on L, it is over the joint distribution of the L label of x i+1 and the label of the pending H query (recall that we sequentially query k points on L while waiting for an H query that runs in parallel to finish).</p>
<p>To compute this expected utility, we follow the backward induction procedure described in Bellman (1957). In the base case, we are at iteration T and need to make the last H query and
E u (D T \ D T −1 | x T , D T −1 ) = y H u (D T \ D T −1 ) Pr (y H | x T , D T −1 ) = Pr (y H = 1 | x T , D T −1 ) .
The optimal decision at this final step is therefore to greedily query the point most likely to be a target, maximizing Pr (y H = 1 | x T , D T −1 ). For the second-to-last query, which is on L, the expected utility is
E u (D T \ D T −2 | x T −1 , D T −2 ) = E max x T Pr (y H = 1 | x T , D T −1 ) .
This is the expected future reward to be collected at the next and final step, which we have shown to be optimally the greedy query. Again, at this iteration, the second-to-last H query is still pending, so the expectation is taken over the joint distribution of the label of that query and that of the putative L query. In general, we compute this expected utility at iteration i + 1 ≤ T for an L query recursively as
E u (D T \ D i | x i+1 , D i ) = E max xi+2 E u (D T \ D i+1 ) | x i+2 , D i+1 . (1)
For an H query, this expected utility may still be recursively computed in the same manner but has a different expansion:
E u (D T \ D i | x i+1 , D i ) = Pr (y H = 1 | x i+1 , D i ) + E max xi+2 E u (D T \ D i+1 ) | x i+2 , D i+1 . (2)
The new term in (2), Pr (y H = 1 | x i+1 , D i ), accounts for the possibility that our running reward increases if x i+1 ∈ R, as we are querying on fidelity H. This is simply the probability that x i+1 is indeed a target. Also different from (1), the expectation is taken over the distribution of the label y H of the putative point x i+1 only, as there is no pending query at this time. An intuitive interpretation of the sum in (2) is the balance between exploitation, the immediate reward Pr (y H = 1 | x i+1 , D i ), and exploration, the expected future reward u (D T \ D i+1 ) conditioned on this putative query. Again, the exploitation term is not present in (1) when an L query is made, as the query cannot possibly increase our utility immediately.</p>
<p>Perhaps unsurprisingly, computing the optimal policy is a daunting task: the time complexity of computing the expectation term in (1) and (2) is O (4n) , where = T − i is the number of remaining queries and n is the number of unlabeled points. This optimal policy is computationally intractable, and suboptimal approximations are needed in practice. One natural solution is to shorten the lookahead horizon, pretending there are only &lt; T − i iterations remaining. This idea constitutes myopic policies, the most straightforward of which is the greedy strategy that queries the point with the highest probability of being a target in the single-fidelity setting, setting = 1. However, the design of a greedy policy for an L query is not obvious, as no immediate reward can be obtained by making the query.</p>
<p>Hardness of Approximation</p>
<p>In addition to demonstrating the intractability of the analogous Bayesian optimal policy in the classical single-fidelity setting, Jiang et al. (2017) proved that no polynomial-time policy can approximate the optimal policy (in terms of the expected terminal utility) by any constant factor. This was done by constructing an adversarial family of active search problems featuring "hidden treasures" that are provably difficult to uncover without exponential work. By modifying details of this construction, the performance of any polynomial-time policy can be made arbitrarily worse in expectation than that of the optimal policy.</p>
<p>This hardness result naturally extends to our multifidelity model, as even access to a perfectly faithful low-fidelity oracle cannot aid a polynomial-time active search policy in one of these adversarial examples. If we assume positives on L also count towards our utility, one such active search problem with a faithful low-fidelity oracle reduces to a single-fidelity problem with a budget on H increased by a factor of (k + 1). Unless k is exponential in the initial budget, any polynomial-time policy remains incapable of approximating the optimal policy. Under our model, only positives on H count towards our utility, so the performance of any policy is further reduced. We therefore obtain the same hardness of approximation result. However, we can still reasonably aim to design efficient approximations to the optimal policy that perform well in practice.</p>
<p>Efficient Nonmyopic Approximation</p>
<p>Our proposed algorithm is inspired by the ENS policy, introduced by Jiang et al. (2017) for the single-fidelity active search setting. ENS offers an efficient and nonmyopic approximation to the optimal policy by assuming that after the putative query, all remaining budget will be spent simultaneously in one batch. Under this heuristic, the optimal decision following the putative query is to greedily construct the batch with points having the highest probabilities. The expected utility of this batch is simply the sum of these highest probabilities due to linearity of expectation and therefore can be computed efficiently. An interesting interpretation by Jiang et al. (2017) about ENS is that it matches the optimal policy given that after the putative query, the labels of the remaining unlabeled points are conditionally independent.</p>
<p>One-stage Approximation</p>
<p>As a stepping stone to our proposed policy, consider the following adoption of ENS. We reapply the heuristic by assuming that after the putative query, which can be on either L or H, all remaining H queries will be made simultaneously in one batch. Again, the optimal choice for this batch is the set of most promising points, which we will refer to as the greedy H batch. Using the summation-prime symbol s to denote the sum of the top s terms, we approximate the maximum expected utility of the remaining portion of the search in (1) and (2) as
max xi+2 E u (D T \ D i+1 ) | x i+2 , D i+1 ≈ H Pr (y H = 1 | x i+2 , D i+1 ) , (3) where H = (T − i − 1)/(k + 1)
is the number of remaining H queries. The resulting policy queries the point that maximizes the expected utility in (1) and (2), using (3) as an approximation. This policy is cognizant of the remaining budget on H and performs well in our experiments (see appendix). However, by assuming that the greedy H batch will be queried immediately following the putative point, the strategy fails to consider future queries that could be made to the low-fidelity oracle in its lookahead; this motivates the design of our proposed policy described below.</p>
<p>Two-stage Approximation</p>
<p>Our main contribution is a policy we call MF-ENS, an efficient approximation to the optimal policy that additionally accounts for future L queries. As above, we assume in our lookahead that after the putative query, our H budget will be spent simultaneously. However, prior to committing to that final batch, we assume we may make k ≤ k additional L queries (also simultaneously) with the goal of improving the expected utility of the final H batch. To faithfully emulate our search model, we set k to be the number of L queries remaining before the next H query is made. 2 We denote this batch of exploratory L queries as X L ⊂ X \ D L and the corresponding labels as Y L ∈ {0, 1} k . In the language of approximate dynamic programming (Bertsekas, 1995), the policy described in the previous subsection is a one-stage rollout policy where the base policy selects the optimal batch on H following the putative query. MF-ENS on the other hand is a two-stage rollout policy whose base policy first queries the L batch X L , and upon observing Y L , adaptively queries the updated optimal H batch.</p>
<p>How should we construct X L to best improve the greedy H batch at the second stage? One option would be to appeal to nonmyopic policies for batch active search (Jiang et al., 2018), but the best-promising batch policies become prohibitively expensive in this context as we would need to construct a new batch for each putative query and label. In general, the conditional dependence among the labels in the set Y L poses a computational challenge in approximating the expected utility gained from querying a given X L batch.</p>
<p>Recall that in the single-fidelity setting, the ENS policy is optimal if labels become conditionally independent after the chosen point. Let us make a similar assumption to ease computation: we assume that labels become conditionally independent after the putative query, except for the pair of labels (on H and L) corresponding to each point. That is, revealing the label y L of a point x is allowed to affect our belief about the corresponding label y H , but not the belief about any other label of any other point. This structure allows for efficiently sharing information between the fidelities and enables our multistage lookahead approach. 3 We now aim to quantify the value of querying an unlabeled point on L in improving the final H batch. Our solution is motivated by a heuristic search for alternatives to the members of the greedy H batch that is assembled under the one-stage approximation. Given a putative query, we still construct that same greedy H batch. Then, for each candidate x not in the greedy H batch, we consider the expected marginal gain in utility of querying it on L and modifying the membership of the H batch in light of its newly revealed label y L .</p>
<p>Recall that observing y L only changes the distribution of y H of the same x under our assumption. Let p * be the lowest success probability among the current H batch and consider two cases. If Pr (y H = 1 | x, D) exceeds p * as y L is revealed, we swap out the corresponding least-promising member of the batch with x and thus increase the final batch's expected utility. Otherwise, we do not modify the current batch. The value of querying x on L and observing
y L , denoted as v (x | D), is then v (x | D) E y L max (Pr (y H = 1 | x, y L , D) − p * , 0) .
This score can be rapidly computed for most models under our conditional independence assumption. With this value function in hand, we then greedily construct X L with the candidates having the highest values. Another computational benefit of label independence is that v (x | D) automatically vanishes for points already labeled on H, as querying its L label does not affect the belief of our model and thus cannot improve the H batch. This reduces our search space in computing the exploratory batch X L .</p>
<p>We now proceed to the last step of the rollout procedure in MF-ENS: marginalizing over Y L , the labels of X L . As previously described, for each possible value of Y L , we approximate the optimal sequence of queries following the putative one and X L with the updated greedy H batch given the newly revealed labels Y L :
max xi+1 E u (D T \ D i ) | x i+1 , D i ≈ E Y L H Pr (y H = 1 | x, X L , Y L , D i ) . (4)
At each iteration, MF-ENS queries the candidate maximizing the expected utility in (1) and (2), as approximated by (4). As an extension of ENS, our approach is nonmyopic and aware of the remaining budget; we will demonstrate the impact of this reasoning in our experiments. The policy also factors in future L queries, actively taking advantage of the ability to query the low-fidelity oracle. We give the pseudocode for the policy in the appendix.</p>
<p>Extension to Multiple Low Fidelities</p>
<p>So far, we have assumed our model only consists of one exact oracle H and one noisy oracle L. To extend MF-ENS to settings where there are multiple noisy oracles {L i } that approximate H, potentially at different levels of fidelity, we still aim to design each query to maximize the expected utility on H, marginalizing future experiments on the {L i } oracles. Once again limiting the conditional dependence among labels to those of the same point, now between each lower-fidelity L i and H, we identify a batch of appropriate size for each L i , marginalize their labels, update the probabilities on H, and compute the approximate expected utility. When there is only one low fidelity, this strategy reduces to the base version of MF-ENS we have presented here.</p>
<p>Implementation and Pruning</p>
<p>Active search requires a classification model computing a given point's success probability with an oracle. We extend the k-nearest neighbor introduced by Garnett et al. (2012) to our multifidelity setting by allowing information observed on L to propagate to H. Specifically, when calculating the probability that an unlabeled point x ∈ X is a positive on H, Pr (y H = 1 | x, D), we take into account the revealed labels of its nearest neighbors on both fidelities, as well as its own L label, y L . Effectively, we treat each given point x ∈ X as having two separate copies: one corresponding to its H label, denoted as x H , and one corresponding to its L label, denoted as x L . Compared to the single-fidelity k-nearest neighbor, the set of nearest neighbors of x H is now doubled to include both copies of its original neighbors and its own copy on L, x L . Formally, denote NN single (x) as the original nearest neighbor set of x. The nearest neighbor set of x H under our multifidelity predictive model is
NN(x H ) {x L } ∪ x H : x ∈ NN single (x) ∪ x L : x ∈ NN single (x) .
To account for the unknown accuracy of the low-fidelity oracle, we apply a damping factor q ∈ (0, 1) to the weights of the neighbors on L; q is dynamically set at each iteration via maximum likelihood estimation.</p>
<p>This model performs well in practice, is nonparametric, and can be efficiently updated in light of new data. The last feature is essential in allowing for fast lookahead, the central component of our method. The time complexity of a naive implementation of MF-ENS is O 2 k n 2 log n , 4 where k is the number of L queries made for each H query and n is the size of the candidate pool. The corresponding time complexity of the single-fidelity ENS is O n 2 log n (Jiang et al., 2017), to which MF-ENS adds a factor of 2 k from the exhaustive marginalization of the exploratory L labels. We may also take advantage of the implementation trick developed by Jiang et al. (2017), which reduces the time complexity to O 2 k n (n + m log m) , where m n is the maximum number of unlabeled points whose probabilities are affected by a newly revealed label. The interested reader may refer to §3.2 of Jiang et al. (2017) for more detail.</p>
<p>We also extend existing branch-and-bound pruning strategies to further reduce the computation time of our policy at each search iteration. First, following previous work (Garnett et al., 2012;Jiang et al., 2017), we establish an upper bound of the score function that is the approximate expected utility defined by (4). This allows us to eliminate candidates whose score upper bounds are lower than the current best score we have found, as their actual scores cannot possibly be the final best score. These upper bounds are computationally cheap to evaluate, so applying this pruning check only adds a trivial overhead to each search iteration. Further, we develop an extension of this pruning strategy by making use of the fact that computing the score of a point involves marginalizing over its unknown label. We thus apply similar pruning checks at every step during this marginalization, which allows us to identify and prune suboptimal candidates "on the fly," avoiding any unnecessary computation.</p>
<p>Concretely, denote by f (x) the score of an unlabeled point x ∈ X , defined by (4) for MF-ENS. Suppose x has a posterior probability of π = Pr (y = 1 | x, D), where y is the label to be returned by the oracle we are currently querying. As previously described, we compute f (x) by calculating its partial values while marginalizing over y:
f (x) = π f (x | y = 1) + (1 − π) f (x | y = 0) ,
where f (x | y) is the partial score of x according to (4) when conditioned on a value of y. Suppose before computing either f (x | y = 0) or f (x | y = 1), we know these partial scores are upper bounded by u(x) given a new positive label and u(x) given a new negative label:
f (x | y = 1) &lt; u(x); f (x | y = 0) &lt; u(x).
As such, f (x) need not be evaluated if
π u(x) + (1 − π) u(x) &lt; f * ,(5)
where f * is the current best score we have found. This pruning strategy has been found to offer a significant speedup in previous work (Garnett et al., 2012;Jiang et al., 2017;2018).</p>
<p>We extend this strategy by considering the case in which a given candidate x is not pruned because (5) is not satisfied, and we proceed with the calculation of f (x). Now, suppose we have computed only f (x | y = 1)</p>
<p>and not yet f (x | y = 0) and observe that π f (x | y = 1) + (1 − π) u(x) &lt; f * , then we may also safely conclude that f (x) cannot possibly exceed f * without needing to go further and compute f (x | y = 0). If this condition is met, we simply abort the computation of the current score f (x) and move on to the next unpruned candidate. For each H query, we apply this partial pruning check once (either after conditioning on y H = 1 and before on y H = 0 or vice versa) for each candidate that is not eliminated by the full pruning check (5).</p>
<p>For an L query, we may do this at most three times for each candidate, as the marginalization over the joint distribution of the putative label and the pending H label when computing f (x) involves four different possible label combinations. We quantify the effectiveness of these pruning strategies and show that they can significantly reduce the computation time of our methods in the appendix.</p>
<p>At an iteration where the current best score f * does not exceed the majority of the score upper bounds, many candidates may be left unpruned. In order to help our policy avoid having to calculate the scores of a large number of candidates and consequently spending too much time on a single iteration, we place an upper limit on how many candidates are to be considered before we terminate the search. Our approach is to first follow the lazy-evaluation strategy introduced by Jiang et al. (2018) and sort the candidates by their score upper bounds. With this sorting, candidates with higher upper bounds will be considered first, and a candidate will never be considered if it will be pruned later on. Now, at each iteration, if after having considered u candidates and noticing that there are still unpruned points remaining, we simply consider a randomly selected subset of size at most s of the unpruned set, before terminating the search and returning the current best candidate. In our experiments, we set u = s = 500 for MF-ENS.</p>
<p>We note that this strategy is only used when pruning fails to reduce our search space to be below u + s, and to allow us to collect results over a long horizon over many repeated experiments. When applied in a real-life planning setting, MF-ENS can still cover the entirety of a large pool of candidates if desired, even with a significant portion of the pool unpruned. In our experiments, MF-ENS takes approximately 30 seconds to reach its quota of 1000 candidates when pruning is unsuccessful; the time for it to fully cover a pool of 100 000 points (about the size of the real-world datasets used in our experiments) is thus well under one hour. In short, our policy remains tractable in real-life settings, even without successful pruning.</p>
<p>Related Work</p>
<p>This work is an extension of the larger active search paradigm, first introduced by Garnett et al. (2012). Active search is a variant of active learning (Settles, 2009) where the goal is not to learn an accurate model but to find and query positive labels. Previous work has studied active search under different settings such as finding a given number of targets as fast/cheaply as possible (Warmuth et al., 2002;2003;Jiang et al., 2019), making queries in batches (Jiang et al., 2018), or when points have real-valued utility (Vanchinathan et al., 2015). Our work generalizes ENS, the policy proposed by Jiang et al. (2017) from the singlefidelity setting. The authors of that work demonstrated that their policy is nonmyopic and aware of its exact budget, allowing it to automatically balance between exploration and exploitation during search and outperform various baselines by a large margin. We will make the same observations about our policy.</p>
<p>Multifidelity active search was first examined by Klyuchnikov et al. (2019), who specifically considered the search problem of a recommender system: identifying items that users of a given application are interested in. They modeled predictions made by a trained preference model as output of a low-fidelity oracle and proposed a co-kriging predictive model (Álvarez et al., 2012) to perform inference on the users' true preferences. Under their setting, queries to the oracles are made sequentially, one after another. Our multifidelity setting is different, modeling situations where oracles of different fidelities are available to run in parallel and vary in their response times, common in scientific experiments and testing. Regardless, our proposed policy could be naturally adopted to their sequential model; the only difference in the computation is that the marginalization over a pending H label is no longer necessary. Further, as we will show in later experiments, our algorithm outperforms the adoption of their upper confidence bound (UCB) policy for various datasets. To our knowledge, our work is the first to tackle multifidelity active search using Bayesian decision theory.</p>
<p>Active search is equivalent to Bayesian optimization (BO) (Brochu et al., 2010;Snoek et al., 2012) with binary observations and cumulative reward. Multifidelity BO itself has been studied considerably, and policies corresponding to common acquisition functions have been adopted to multifidelity settings, including expected improvement (Huang et al., 2006;Picheny et al., 2013), knowledge gradient (Poloczek et al., 2017;Wu et al., 2020), and UCB (Kandasamy et al., 2017). However, most of these policies are derived from or motivated by greedy approximations to the optimal policy under Table 1. Experiment results with an H budget of t = 300, averaged across all repeated experiments for each setting. θ is the simulated false positive rate of the low-fidelity oracle; k is the number of L queries made between two H queries (i.e., the speed ratio between the two fidelities). Each entry denotes the average number of targets found across the repeated experiments and the corresponding standard error in parentheses. The best performance in each column is highlighted bold. different utility functions, and to our knowledge, no nonmyopic multifidelity BO policies have been proposed.
ECFP4 GpiDAPH3 BMG θ = 0.1 θ = 0.3 θ = 0.1 θ = 0.3 θ = 0.1 θ = 0.3 k = 2 k = 5 k = 2 k = 5 k = 2 k = 5 k = 2 k = 5 k = 2 k = 5 k = 2 k = 5
Active search is related to the multi-armed bandit (MAB) problem (Lai &amp; Robbins, 1985). In particular, querying the label of a point can be viewed as "pulling an arm" in MAB; in active search, an arm cannot be pulled twice but is correlated to its neighbors. Kandasamy et al. (2016) studied a formulation of multifidelity MAB in which each arm may be pulled on different fidelities at different costs, and proposed a policy that is a variant of UCB. By assuming the accuracy of the lower fidelities is known, the authors derived strong theoretical guarantees for their proposed policy.</p>
<p>Experiments</p>
<p>We now compare the empirical performance of MF-ENS against several benchmarks. 5 As previously described, ENS is a state-of-the-art, nonmyopic active search policy in the single-fidelity setting. In our experiments, this policy simply ignores the low-fidelity oracle, serving as a single-fidelity baseline to illustrate the benefit of having access to low-fidelity queries. We also test against the MF-UCB policy for multifidelity active search, recently proposed by Klyuchnikov et al. (2019). Under this policy, each candidate x has a UCB-style score of α (x, D) = π + β π(1 − π), where π is the probability of x having a positive label on the fidelity being queried and β is the exploration/exploitation tradeoff parameter. 6 We set β = 0.01 for L queries and β = 0.001 for H queries, as suggested in the same work. For another benchmark, we consider a simple but natural heuristic for multifidelity optimization, that low-fidelity queries should serve to narrow down the most-promising search regions (exploration), so that more informed queries could be made on the higher-fidelity oracle (exploitation). Inspired by this heuristic, we design a policy we call UG (for uncertainty and greedy sampling) that always queries the most uncertain points on L and the points most likely to be positive on H. Uncertainty sampling is chosen for the role of exploration due to its popularity as an active learning technique (Lewis &amp; Gale, 1994). UG may also be viewed as the limit of UCB when β approaches infinity on L for maximum exploration and 0 on H for maximum exploitation.</p>
<p>Datasets for multifidelity active search are not readily available due to the relative novelty of the problem setting, at least not in our motivating area of scientific discovery. However, numerous high-quality datasets are available in the single-fidelity setting, which we will adopt and use to simulate multifidelity search. Namely, for each dataset, we simulate the noisy labels returned by the low-fidelity oracle using the following procedure. We first create a duplicate of the true labels. Given θ ∈ (0, 1), we randomly select a fraction θ of the positives from this duplicate set and "flip" their labels to negative. We also randomly select the same number of negatives and flip their values to positive. This perturbed set of labels is then used as the L labels. This construction yields simulated L labels with a false positive rate of θ and a false negative rate of θ r/(1 − r), where r ∈ (0, 1) is the prevalence rate of the positive set R in X . In a typical active search problem, R is rare and r 1, making the false negative rate much lower than the false positive rate, a common characteristic of many real-world scientific discovery and testing procedures.</p>
<p>We set θ ∈ {0.1, 0.3}. We set k, the number of L queries that are made for each H query, to be either 2 or 5, and set the budget on H to be 300. In each experiment, a policy starts with an initial training dataset of one randomly selected target whose L label is also positive.</p>
<p>Datasets</p>
<p>We conducted experiments on three real-world scientific discovery datasets used in previous studies (Jiang et al., 2017;2018;. The first two come from drug discovery, where the goal is to discover chemical compounds exhibiting binding activity with a given protein. Each protein defines the target for an active search problem. Here we used the first 50 proteins from the BindingDB database (Liu et al., 2007) described by Jiang et al. (2017). A set of 100 000 compounds sampled from the ZINC database (Sterling &amp; Irwin, 2015) served as a shared negative set. Features for the compounds are binary vectors encoding chemical characteristics, also known as a chemoinformatic fingerprint. We considered two fingerprints delivering good performance in previous studies: ECFP4 and GpiDAPH3. The size of the positive set for these 50 targets ranged from 205 to 1488 (mean 538), having an average prevalence rate of r ≈ 0.5%. For each of these two datasets and 50 targets, we repeat each experiment five times, for a total of 500 search simulations.</p>
<p>The other dataset is related to a materials science application. The targets in this case are alloys that can form bulk metallic glasses (BMGs), which have higher toughness and better wear resistance than crystalline alloys. This dataset comprises 106 810 alloys from the materials literature (Kawazoe et al., 1997;Ward et al., 2016), 4275 of which exhibit glassforming ability (r ≈ 4%). We repeated each experiment 50 times for this dataset.</p>
<p>We report the average number of targets found by each policy across the experiments with standard errors in parentheses in Table 2; each column corresponds to a specific setting of θ and k under a dataset. We observe that our policy MF-ENS outperforms all baselines by a large margin. In each column, a two-sided paired t-test rejects the hypothesis that the average difference in the number of targets found between MF-ENS and any baseline is zero with overwhelming confidence, returning a p-value of at most 4 × 10 −5 . We report these p-values in the appendix. Finally, looking across the columns, we notice the expected trends: performance of all algorithms except for ENS, which does not utilize fidelity L, improves with higher k (when L is cheaper) or with lower θ (when L is more accurate).</p>
<p>Performance Gain from Multifidelity Search</p>
<p>To further examine the benefit of having access to more than just the exact oracle, we visualize the difference in the cumulative number of targets found between MF-ENS and the single-fidelity policy ENS, averaged across all experiments, in Figure 2. We observe that MF-ENS completely dominates ENS, finding roughly linearly more targets throughout the search. This large difference in empirical performance illustrates the usefulness of the simulated low-fidelity oracle in our experiments, and suggests that our approach is likely to benefit search with any budget.</p>
<p>Nonmyopic Behavior</p>
<p>We have claimed that MF-ENS is nonmyopic and aware of its remaining budget at any given time during a search. We demonstrate this nonmyopia by first comparing the progressive probabilities of the points queried on H (at the time of the queries being made) by the policy against the myopic baselines MF-UCB and UG, averaged across all ex-periments, in the left panel of Figure 3. Initially, MF-ENS chooses points with lower probabilities, exploring the space. As the search progresses, MF-ENS queries more promising points, smoothly transitioning to exploitation. The opposite trend can be observed for UCB and UG, whose probabilities decrease over time due to greedy behavior. This difference translates to distinct patterns in the cumulative reward achieved by these policies. The right panel of Figure 3 shows the difference in the cumulative number of targets found between MF-ENS and MF-UCB, also averaged across all experiments. During the first half of the search, MF-ENS appears to perform worse than MF-UCB, but quickly recovers and outperforms the latter in the end. The corresponding plot comparing MF-ENS and UG shows a similar trend and is deferred to the appendix.</p>
<p>Overall, this phenomenon perfectly highlights the automatic tradeoff between exploration and exploitation exhibited by MF-ENS: the policy makes its initial queries to explore the search space, often requesting labels that are not the most likely to be positive and failing to collect substantial immediate reward; however, as the budget decreases, its queries grow more exploitative and are ultimately more successful than those from myopic policies by leveraging what it has learned.</p>
<p>Conclusion</p>
<p>We have proposed a multifidelity active search model in which an exact oracle and a cheaper surrogate are queried in parallel. We presented a novel nonmyopic policy (based on two-stage rollout) for this setting that reasons about the remaining queries on both fidelities seeking to maximize the total number of discoveries. Our policy is aware of the remaining budget and dynamically balances exploration and exploitation. Experiments on real-world data demonstrate that the policy significantly outperforms myopic benchmarks.</p>
<p>A. Pseudocode</p>
<p>We give the pseudocode for MF-ENS in Algorithm 1 (for H queries) and Algorithm 2 (for L queries). The two versions are nearly identical; however, in Algorithm 2, we marginalize both the putative label y L and the pending label y H for L queries, and the Pr(y H = 1 | x, D) term is dropped from the final line according to (1) in the main paper.</p>
<p>B. The One-Stage Approximation Policy</p>
<p>We name the one-stage approximation policy described in §3.1 of the main paper H-ENS. Recall H-ENS applies the ENS heuristic directly by assuming in its lookahead that immediately after the putative query, all budget on H is spent simultaneously on a batch of queries, disregarding any L queries that may be made before that batch.</p>
<p>The time complexity of H-ENS is the same as that of ENS, O n (log n + m log m) , where n is the size of the candidate pool and m is the maximum number of points whose probablities are affected by a newly revealed label (Jiang et al., 2017). To avoid having to consider a large number of unpruned candidates on a single iteration, we set u = s = 1000, where u is the upper limit of the number of candidates are to be considered before a random subset of size s of the remaining candidates is selected. This strategy is described in detail in §3.3 of the main paper.  Table 3 shows the p-values from two-sided paired t-tests, each testing for the hypothesis that the average number of targets found between MF-ENS and each of the baselines considered in §5 of the main paper is zero. All hypotheses are rejected with overwhelming confidence.</p>
<p>C. Further Experiment Results</p>
<p>We also show the difference in the cumulative number of targets found between MF-ENS and UG in Figure 4. The plot shows the same trend as that for MF-ENS and MF-UCB in the main paper: MF-ENS underperforms the myopic policy at the beginning of the search during its exploration phase, but quickly recovers and is able to find significantly more targets in the end.  </p>
<p>C.1. Effects of Pruning</p>
<p>Finally, we examine the effectiveness of our pruning strategies in helping H-ENS and MF-ENS cover the entire search space. Each of the two policies has an upper limit on how many candidates are to have their scores fully computed. If this limit is reached, we only consider a random subset of the remaining candidate pool. We classify each time a policy returns while there are unpruned candidates remaining as an instance of failure to cover the entire space.</p>
<p>First, we compute the fraction of iterations, across all experiments, in which this limit is not exceeded, or in other words, how often each policy can exactly consider all candidates with the help of pruning. When a point is pruned, it may be before any actual computation (total pruning) if its score upper bound is lower than the current highest score f * , satisfying (5) in the main paper. Otherwise, it may be pruned during the calculation of its scores (partial pruning) if its partial score, combined with partial upper bounds, is lower than f * . We keep track of the fraction of points pruned by each of the two methods; these results are averaged across the iterations in which pruning helps cover the entire candidate pool. We report these statistics in Table 4.</p>
<p>We observe that while we do not cover the entire candidate pool in many iterations, the effect of pruning is dramatic when it is successful. The existing pruning method (total pruning) helps eliminate most of the candidates, and our extension (partial pruning) raises the combined pruning rate Table 2. Experiment results with an H budget of t = 300, averaged across all repeated experiments for each setting. θ is the simulated false positive rate of the low-fidelity oracle; k is the number of L queries made between two H queries (i.e., the speed ratio between the two fidelities). Each entry denotes the average number of targets found across the repeated experiments and the corresponding standard error in parentheses. The best performance in each column is highlighted bold; those that are not significantly worse than the best (using a one-sided paired t-test with significance level α = 0.05) are in blue italic.  Table 3. p-values from two-paired t-tests, each testing for the hypothesis that the average number of targets found between MF-ENS and another policy is zero.</p>
<p>ECFP4</p>
<p>GpiDAPH3 BMG θ = 0.1 θ = 0.3 θ = 0.1 θ = 0.3 θ = 0.1 θ = 0.3 k = 2 k = 5 k = 2 k = 5 k = 2 k = 5 k = 2 k = 5 k = 2 k = 5 k = 2 k = 5 ENS 4 × 10 −27 3 × 10 −30 7 × 10 −12 2 × 10 −18 1 × 10 −26 1 × 10 −40 3 × 10 −14 3 × 10 −22 4 × 10 −10 5 × 10 −14 7 × 10 −7 6 × 10 −5</p>
<p>MF-UCB</p>
<p>1 × 10 −10 1 × 10 −6 7 × 10 −18 6 × 10 −14 1 × 10 −18 6 × 10 −18 1 × 10 −13 7 × 10 −18 5 × 10 −8 8 × 10 −6 1 × 10 −8 8 × 10 −8 UG 5 × 10 −8 2 × 10 −7 2 × 10 −10 2 × 10 −10 2 × 10 −15 5 × 10 −19 6 × 10 −11 1 × 10 −14 9 × 10 −8 4 × 10 −8 1 × 10 −5 4 × 10 −5 to roughly 99% on average. In our experiments, successful pruning could reduce the time a policy takes to produce a decision from under an hour to mere seconds.</p>
<p>Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</p>
<p>Figure 2 .
2The difference in cumulative targets found between MF-ENS and ENS, averaged across all experiments and datasets.</p>
<p>Figure 3 .
3An illustration of budget-awareness exhibited by MF-ENS. Left: The average progressive probabilities of points queried on H by different active search policies. Right: The difference in cumulative targets found between MF-ENS and MF-UCB. The results are averaged across all experiments and datasets.</p>
<p>Figure 4 .
4The difference in cumulative targets found between MF-ENS and UG, averaged across all experiments and datasets.</p>
<p>At arXiv:2106.06356v2 [cs.LG] 7 Jul 2021Nonmyopic Multifidelity Active Search </p>
<p>fidelity L </p>
<p>fidelity H </p>
<p>time </p>
<p>1 
4 
7 
22 
25 
28 </p>
<p>2 
3 
5 
6 
8 
23 
24 
26 
27 </p>
<p>Table 2
2shows the average number of targets found by all considered policies including H-ENS. H-ENS performs well in comparison with myopic baselines and the singlefidelity ENS policy. Against H-ENS, our main policy MF-ENS offers a slight improvement. Across all experiments, MF-ENS finds an average of 237 targets, whereas H-ENS finds 235; a two-sided paired t-test rejects the hypothesis that the average difference in the number of targets found between the two policies is zero with a p-value of p = 0.044.</p>
<p>Table 4 .
4Average pruning rate and the contribution of each of the two pruning methods.policies 
full coverage rate 
given full coverage </p>
<p>total prune % partial prune % </p>
<p>H-ENS 
45.2% 
85.2% 
14.0% </p>
<p>MF-ENS </p>
<p>27.6% 
89.4% 
9.2% </p>
<p>Washington University in St. Louis, MO, USA 2 University of Toronto, Toronto, Canada. Correspondence to: Quan Nguyen <a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#113;&#117;&#97;&#110;&#64;&#119;&#117;&#115;&#116;&#108;&#46;&#101;&#100;&#117;">&#113;&#117;&#97;&#110;&#64;&#119;&#117;&#115;&#116;&#108;&#46;&#101;&#100;&#117;</a>.
This assumption is for simplicity; asynchronous queries could be addressed with a slight modification of our proposed algorithm.
When making an H query, k = k; when making an L query, we subtract the number of L queries since the pending H query.
This is only used in policy construction and not in inference!
Note that we are assuming k is small enough that 2 k is a small constant; if this is not the case, we may approximate (4) by sampling instead, replacing 2 k by s, the number of samples used.
Matlab implementations of our policies are available at:https://github.com/KrisNguyen135/ multifidelity-active-search .
The authors also considered an odd scenario in which the correlation between the two oracles is negative. We assume that this correlation is always positive, and it is constructed to be so.
AcknowledgementsWe would like to thank the anonymous reviewers for their feedback. QN and RG were supported by the National Science Foundation (NSF) under award numbers OAC-1940224   and IIS-1845434.Algorithm 2 MF-ENS for L queries inputs x, D returns approximate expected utility for querying x on L, f (x) design query by maximizing 1: for y L , y H ∈ {−, +} 2 do probabilities: Pr(y | x, D)2: determine X L by finding top-k v scores among unlabeled L points 3:
Kernels for Vector-Valued Functions: A Review . Foundations and Trends in Machine Learning. M A Alvarez, L Rosasco, N D Lawrence, 4Alvarez, M. A., Rosasco, L., and Lawrence, N. D. Kernels for Vector-Valued Functions: A Review . Foundations and Trends in Machine Learning, 4(3):195-266, 2012.</p>
<p>. R Dynamic Bellman, Programming, Princeton University PressBellman, R. Dynamic Programming. Princeton University Press, 1957.</p>
<p>Dynamic Programming and Optimal Control. D P Bertsekas, Athena Scientific Belmont. 1MABertsekas, D. P. Dynamic Programming and Optimal Con- trol, volume 1. Athena Scientific Belmont, MA, 1995.</p>
<p>A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning. E Brochu, V M Cora, De Freitas, N , arXiv:1012.2599arXiv preprintcs.LGBrochu, E., Cora, V. M., and De Freitas, N. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierar- chical Reinforcement Learning. 2010. arXiv preprint arXiv:1012.2599 [cs.LG].</p>
<p>Bayesian Optimal Active Search and Surveying. R Garnett, Y Krishnamurthy, X Xiong, J Schneider, R Mann, Proceedings of the 29th International Conference on Machine Learning. the 29th International Conference on Machine LearningGarnett, R., Krishnamurthy, Y., Xiong, X., Schneider, J., and Mann, R. Bayesian Optimal Active Search and Surveying. In Proceedings of the 29th International Conference on Machine Learning, 2012.</p>
<p>Sequential kriging optimization using multiple-fidelity evaluations. Structural and Multidisciplinary Optimization. D Huang, T T Allen, W I Notz, R A Miller, 32Huang, D., Allen, T. T., Notz, W. I., and Miller, R. A. Se- quential kriging optimization using multiple-fidelity eval- uations. Structural and Multidisciplinary Optimization, 32(5):369-382, 2006.</p>
<p>Efficient Nonmyopic Active Search. S Jiang, G Malkomes, G Converse, A Shofner, B Moseley, R Garnett, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningJiang, S., Malkomes, G., Converse, G., Shofner, A., Mose- ley, B., and Garnett, R. Efficient Nonmyopic Active Search. In Proceedings of the 34th International Confer- ence on Machine Learning, pp. 1714-1723, 2017.</p>
<p>Efficient nonmyopic batch active search. S Jiang, G Malkomes, M Abbott, B Moseley, R Garnett, Advances in Neural Information Processing Systems. 31Jiang, S., Malkomes, G., Abbott, M., Moseley, B., and Garnett, R. Efficient nonmyopic batch active search. In Advances in Neural Information Processing Systems 31, pp. 1099-1109, 2018.</p>
<p>Cost effective active search. S Jiang, R Garnett, B Moseley, Advances in Neural Information Processing Systems. 32Jiang, S., Garnett, R., and Moseley, B. Cost effective active search. In Advances in Neural Information Processing Systems 32, pp. 4880-4889, 2019.</p>
<p>The Multi-fidelity Multi-armed Bandit. K Kandasamy, G Dasarathy, J Schneider, Póczos , B , Advances in Neural Information Processing Systems. 29Kandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. The Multi-fidelity Multi-armed Bandit. In Advances in Neural Information Processing Systems 29, pp. 1785- 1793, 2016.</p>
<p>Multi-fidelity Bayesian Optimisation with Continuous Approximations. K Kandasamy, G Dasarathy, J Schneider, Póczos , B , Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningKandasamy, K., Dasarathy, G., Schneider, J., and Póczos, B. Multi-fidelity Bayesian Optimisation with Continuous Approximations. In Proceedings of the 34th International Conference on Machine Learning, pp. 2861-2878, 2017.</p>
<p>Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys, volume 37A of Condensed Matters. Y Kawazoe, J.-Z Yu, A.-P Tsai, Masumoto , Springer-VerlagKawazoe, Y., Yu, J.-Z., Tsai, A.-P., and Masumoto, T. (eds.). Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys, volume 37A of Condensed Matters. Springer- Verlag, 1997.</p>
<p>Figuring out the User in a Few Steps: Bayesian Multifidelity Active Search with Cokriging. N Klyuchnikov, D Mottin, G Koutrika, E Müller, P Karras, Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningKlyuchnikov, N., Mottin, D., Koutrika, G., Müller, E., and Karras, P. Figuring out the User in a Few Steps: Bayesian Multifidelity Active Search with Cokriging. In Proceed- ings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pp. 686-695, 2019.</p>
<p>Asymptotically Efficient Adaptive Allocation Rules. T L Lai, H Robbins, Advances in Applied Mathematics. 61Lai, T. L. and Robbins, H. Asymptotically Efficient Adap- tive Allocation Rules. Advances in Applied Mathematics, 6(1):4-22, 1985.</p>
<p>A Sequential Algorithm for Training Text Classifiers. D D Lewis, W A Gale, Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. the 17th Annual International ACM SIGIR Conference on Research and Development in Information RetrievalLewis, D. D. and Gale, W. A. A Sequential Algorithm for Training Text Classifiers. In Proceedings of the 17th An- nual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 3-12, 1994.</p>
<p>A web-accessible database of experimentally determined protein-ligand binding affinities. T Liu, Y Lin, X Wen, R N Jorissen, M K Gilson, Bindingdb, Nucleic acids research. 351supplLiu, T., Lin, Y., Wen, X., Jorissen, R. N., and Gilson, M. K. BindingDB: A web-accessible database of experimentally determined protein-ligand binding affinities. Nucleic acids research, 35(suppl 1):D198-D201, 2007.</p>
<p>Quantile-Based Optimization of Noisy Computer Experiments with Tunable Precision. V Picheny, D Ginsbourger, Y Richet, G Caplin, Technometrics. 551Picheny, V., Ginsbourger, D., Richet, Y., and Caplin, G. Quantile-Based Optimization of Noisy Computer Exper- iments with Tunable Precision. Technometrics, 55(1): 2-13, 2013.</p>
<p>Multi-Information Source Optimization. M Poloczek, J Wang, P Frazier, Advances in Neural Information Processing Systems. 30Poloczek, M., Wang, J., and Frazier, P. Multi-Information Source Optimization. In Advances in Neural Information Processing Systems 30, pp. 4288-4298, 2017.</p>
<p>Active Learning Literature Survey. B Settles, University of Wisconsin-Madison Department of Computer SciencesTechnical reportSettles, B. Active Learning Literature Survey. Technical report, University of Wisconsin-Madison Department of Computer Sciences, 2009.</p>
<p>Practical Bayesian Optimization of Machine Learning Algorithms. J Snoek, H Larochelle, R P Adams, Advances in Neural Information Processing Systems. 25Snoek, J., Larochelle, H., and Adams, R. P. Practical Bayesian Optimization of Machine Learning Algorithms. In Advances in Neural Information Processing Systems 25, pp. 2951-2959, 2012.</p>
<p>Zinc 15-Ligand Discovery for Everyone. T Sterling, J J Irwin, Journal of Chemical Information and Modeling. 5511Sterling, T. and Irwin, J. J. Zinc 15-Ligand Discovery for Everyone. Journal of Chemical Information and Model- ing, 55(11):2324-2337, 2015.</p>
<p>Discovering Valuable Items from Massive Data. H P Vanchinathan, A Marfurt, C.-A Robelin, D Kossmann, A Krause, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data MiningVanchinathan, H. P., Marfurt, A., Robelin, C.-A., Kossmann, D., and Krause, A. Discovering Valuable Items from Massive Data. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, pp. 1195-1204, 2015.</p>
<p>A general-purpose machine learning framework for predicting properties of inorganic materials. L Ward, A Agrawal, A Choudhary, C Wolverton, npj Computational Materials. 21Ward, L., Agrawal, A., Choudhary, A., and Wolverton, C. A general-purpose machine learning framework for predict- ing properties of inorganic materials. npj Computational Materials, 2(1):1-7, 2016.</p>
<p>Active learning in the drug discovery process. M K Warmuth, G Rätsch, M Mathieson, J Liao, C Lemmen, Advances in Neural Information Processing Systems. 15Warmuth, M. K., Rätsch, G., Mathieson, M., Liao, J., and Lemmen, C. Active learning in the drug discovery pro- cess. In Advances in Neural Information Processing Systems 15, pp. 1449-1456, 2002.</p>
<p>Active Learning with Support Vector Machines in the Drug Discovery Process. M K Warmuth, J Liao, G Rätsch, M Mathieson, S Putta, C Lemmen, Journal of Chemical Information and Computer Sciences. 432Warmuth, M. K., Liao, J., Rätsch, G., Mathieson, M., Putta, S., and Lemmen, C. Active Learning with Support Vector Machines in the Drug Discovery Process. Journal of Chemical Information and Computer Sciences, 43(2): 667-673, 2003.</p>
<p>Practical Multi-Fidelity Bayesian Optimization for Hyperparameter Tuning. J Wu, S Toscano-Palmerin, P I Frazier, A G Wilson, Proceedings of the 36th. the 36thWu, J., Toscano-Palmerin, S., Frazier, P. I., and Wilson, A. G. Practical Multi-Fidelity Bayesian Optimization for Hyperparameter Tuning. In Proceedings of the 36th</p>
<p>Conference on Uncertainty in Artificial Intelligence. Conference on Uncertainty in Artificial Intelligence, pp. 788-798, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
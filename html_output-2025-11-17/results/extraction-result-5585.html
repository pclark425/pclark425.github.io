<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5585 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5585</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5585</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-114.html">extraction-schema-114</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used as text-based simulators in specific scientific subdomains, including details on the simulation tasks, reported accuracy, evaluation methods, and any factors identified as affecting the accuracy of these simulations.</div>
                <p><strong>Paper ID:</strong> paper-7e57f8aeed2074ea0a943c619cac4a78f28628f4</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7e57f8aeed2074ea0a943c619cac4a78f28628f4" target="_blank">TableGPT: Few-shot Table-to-Text Generation with Table Structure Reconstruction and Content Matching</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work proposes TableGPT, a model that outperforms existing systems on most few-shot settings for table-to-text generation and exploits multi-task learning with two auxiliary tasks that preserve table’s structural information by reconstructing the structure from GPT-2's representation and improving the text's fidelity with content matching task.</p>
                <p><strong>Paper Abstract:</strong> Although neural table-to-text models have achieved remarkable progress with the help of large-scale datasets, they suffer insufficient learning problem with limited training data. Recently, pre-trained language models show potential in few-shot learning with linguistic knowledge learnt from pretraining on large-scale corpus. However, benefiting table-to-text generation in few-shot setting with the powerful pretrained language model faces three challenges, including (1) the gap between the task’s structured input and the natural language input for pretraining language model. (2) The lack of modeling for table structure and (3) improving text fidelity with less incorrect expressions that are contradicting to the table. To address aforementioned problems, we propose TableGPT for table-to-text generation. At first, we utilize table transformation module with template to rewrite structured table in natural language as input for GPT-2. In addition, we exploit multi-task learning with two auxiliary tasks that preserve table’s structural information by reconstructing the structure from GPT-2’s representation and improving the text’s fidelity with content matching task aligning the table and information in the generated text. By experimenting on Humans, Songs and Books, three few-shot table-to-text datasets in different domains, our model outperforms existing systems on most few-shot settings.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5585",
    "paper_id": "paper-7e57f8aeed2074ea0a943c619cac4a78f28628f4",
    "extraction_schema_id": "extraction-schema-114",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00377275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>TableGPT: Few-shot Table-to-Text Generation with Table Structure Reconstruction and Content Matching</h1>
<p>Heng Gong ${ }^{1 <em>}$, Yawei Sun ${ }^{1 </em>}$, Xiaocheng Feng ${ }^{1,2}$, Bing Qin ${ }^{1,2}$, Wei Bi, Xiaojiang Liu, Ting Liu ${ }^{1,2}$<br>${ }^{1}$ Department of Computer Science and Technology, Harbin Institute of Technology, Harbin, China<br>${ }^{2}$ Peng Cheng Laboratory<br>{hgong, ywsun, xcfeng, qinb, tliu}@ir.hit.edu.cn<br>weibi@cse.ust.hk xiaojiangliu84@hotmail.com</p>
<h4>Abstract</h4>
<p>Although neural table-to-text models have achieved remarkable progress with the help of largescale datasets, they suffer insufficient learning problem with limited training data. Recently, pretrained language models show potential in few-shot learning with linguistic knowledge learnt from pretraining on large-scale corpus. However, benefiting table-to-text generation in few-shot setting with the powerful pretrained language model faces three challenges, including (1) the gap between the task's structured input and the natural language input for pretraining language model. (2) The lack of modeling for table structure and (3) improving text fidelity with less incorrect expressions that are contradicting to the table. To address aforementioned problems, we propose TableGPT for table-to-text generation. At first, we utilize table transformation module with template to rewrite structured table in natural language as input for GPT-2. In addition, we exploit multi-task learning with two auxiliary tasks that preserve table's structural information by reconstructing the structure from GPT-2's representation and improving the text's fidelity with content matching task aligning the table and information in the generated text. By experimenting on Humans, Songs and Books, three few-shot table-to-text datasets in different domains, our model outperforms existing systems on most few-shot settings.</p>
<h2>1 Introduction</h2>
<p>Table-to-text generation, aiming at generating descriptive text about important information in structured data, has well application prospect in communicating with human in a comprehensible and natural way, such as financial report (Murakami et al., 2017), medical report (Hasan and Farri, 2019) generation, etc. In recent years, data-driven models have shown impressive capability to produce informative and fluent text with the help of large-scale datasets, such as WIKIBIO (Lebret et al., 2016) and E2E (Dušek et al., 2020). However, it is not always feasible to collect large-scale labeled dataset for various domains in the real world, resulting in unsatisfying performance due to the insufficient training. Such few-shot learning setting for table-to-text generation is not well-explored, and in this paper, we focus on exploring how to efficiently model for few-shot table-to-text generation with limited training pairs.</p>
<p>Recently, pre-trained language models have shown promising progress in various natural language processing tasks (Yang et al., 2019b; Devlin et al., 2019; Radford et al., 2019). They can capture linguistic knowledge by pretraining on large-scale unlabeled dataset and generalize to downstream tasks with little labeled data in target domain, effectively modeling for few-shot setting (Peng et al., 2020). However, efforts to benefit table-to-text generation from the powerful pre-trained language model, especially in few-shot setting, are non-trivial due to three challenges. (1) There is a gap between the structured data input for table-to-text generation and natural language input that is used for pretraining GPT-2. (2) Also, it lacks modeling of the table's structure which contains rich information to understand the input before generating text. (3) Additionally, it doesn't address how to maintain text's fidelity for table-to-text gener-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Attribute</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">name</td>
<td style="text-align: left;">james beattie</td>
</tr>
<tr>
<td style="text-align: left;">fullname</td>
<td style="text-align: left;">james scott beattie</td>
</tr>
<tr>
<td style="text-align: left;">birth</td>
<td style="text-align: left;">27 february 1978 lancaster, england</td>
</tr>
<tr>
<td style="text-align: left;">position</td>
<td style="text-align: left;">striker</td>
</tr>
<tr>
<td style="text-align: left;">currentclub</td>
<td style="text-align: left;">swansea city ( assistant first team coach )</td>
</tr>
<tr>
<td style="text-align: left;">nationalteam</td>
<td style="text-align: left;">england u21 england</td>
</tr>
<tr>
<td style="text-align: left;">article</td>
<td style="text-align: left;">james beattie ( footballer )</td>
</tr>
<tr>
<td style="text-align: left;">$\ldots$</td>
<td style="text-align: left;">$\ldots$</td>
</tr>
<tr>
<td style="text-align: left;">Reference: james scott beattie ( born 27 february 1978 ) is an english former professional</td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">footballer who played as a striker.</td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>Table 1: A sample of table-text pair for table-to-text generation from Humans domain. The highlighted phrases are information mentioned in text.
ation while exploiting linguistic knowledge from pretraining corpus, that is the (highlighted) information in text (Table 1) should correctly derive from structured data.</p>
<p>In order to alleviate aforementioned problems, we propose TableGPT that focus on generating highfidelity text for table-to-text generation with limited training pairs. Addressing the gap between structured table input and natural language input that GPT-2 processes during pretraining, we utilize a table transformation module that employs template to naturally transform structured table into natural language. In addition, we utilize two auxiliary tasks under the framework of multi-task learning, table structure construction and content matching, targeting pretrained GPT-2's lack of modeling for table structure and text's fidelity. In detail, the table structure reconstruction task is proposed for GPT-2 which force it to embed table structure into its representation when modeling structured table. Besides, we utilize content matching task that help model correctly describe important information from table via Optimal-Transport (Chen et al., 2019a) technique, which measures the distance between the information in generated text and information in table and use the distance as penalty for text with incorrect information.</p>
<p>We conducted experiments on three data-to-text datasets on different domains (Chen et al., 2020b): Humans, Books, Songs in various settings. Both automatic evaluation and human evaluation results show that our model can achieve new state-of-the-art performance for table-to-text generation in terms of generating fluent and high-fidelity text in most few-shot settings.</p>
<h1>2 Background</h1>
<h3>2.1 Task Definition</h3>
<p>For the table-to-text task discussed in this paper, we can formulate each training instance as pair of table and summary $E=(S, T)$. Given a table, which can be formulated as sets of records $S=\left{r_{i}\right}<em 1="1">{i=1}^{N}$, the model is expected to generate descriptive text $T=w</em> . v$ can be viewed as a sequence of words.}, w_{2}, \ldots, w_{L} . N$ is the number of records and $L$ is the number of words in text. Each record $r_{i}$ consists of two type of information: $r_{i} . a$ and $r_{i} . v . r_{i} . a$ denotes the attribute of the record (e.g. name) and $r_{i} . v$ denotes the corresponding value (e.g. james beattie). Please note that both $r_{i} . a$ and $r_{i</p>
<h3>2.2 Pre-trained Language Model</h3>
<p>Recently, pre-trained language models, such as BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), XLNet (Yang et al., 2019b) and more, have achieved remarkable progress in various NLP tasks. The main idea is to pretrain a neural language model with large number of parameters on large-scale dataset in order to capture the linguistic knowledge at first. Then, transfer those knowledge to downstream task</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: TableGPT's Training Process. Tokens in red indicate information in reference and those in blue refers to value tokens' corresponding attribute. (a) Table Transformation employs template to rewrite the structured table in natural language and concatenate with reference to form a training pair, resulting in language model loss $L_{L M}$ with GPT-2. (b) Table Structure Reconstruction and (c) Content Matching are auxiliary tasks. The former reconstructs attributes from GPT-2's representation of value tokens, forming $L_{S R}$. The latter use Optimal Transport to measure distance between information mentioned in text and table, using this as content matching loss $L_{C M}$. The model is jointly finetuned with above three losses.
via finetuning on the task's dataset. Impressively, they can outperform various NLP tasks' previous state-of-the-arts by a large margin. Since we investigate table-to-text generation task in this paper instead of natural language understanding tasks, we choose GPT-2 as the basis of our model. The model structure for GPT-2 is a 12-to-48-layer transformer decoder (Vaswani et al., 2017) with 117 million to 1542 million parameters. Each layer consists of a stack of masked multi-head self attention and feed-forward neural network with residual connection and layer normalization. The pre-training target of GPT-2 is as the same as language model: maximizing gold text's probability. The large-scale model is trained on vast and diverse WebText dataset with 8 million documents collected from the Internet. Its success in the area of text generation attributes to both its high capacity model and knowledge learnt from pretraining on vast dataset.</p>
<h1>3 Approach</h1>
<p>In this section, we propose to address the three incompatibility between pretrained language model and table-to-text generation illustrated in Section 1. Figure 1 presents the overall multi-task training framework of our model. We first utilize table transformation module to reasonably transform structured table into text sequence and aggregate it with reference text, resulting in a suitable training sequence for GPT2 model. Then, two auxiliary tasks: table structure reconstruction and content matching with optimal transport (Chen et al., 2019a) are performed on top of GPT-2's representation of the training sequence. Those two auxiliary tasks' training objectives along with GPT-2's language model training objective are jointly finetuned based on the pre-trained GPT-2 under the multi-task training framework. The overall objective here is to produce high-fidelity text while maintaining its fluency.</p>
<h3>3.1 Table Transformation</h3>
<p>As noted in Section 2.1, the given structured table consists of multiple records as attribute-value pairs. In order to adapt to the sequential nature of language model, we employ a template-based table serialization</p>
<p>method (Chen et al., 2019b) to encode such structured table as a sequence. For example, we serialized the attribute-value pair "name: jack reynolds" as a sentence "name is jack reynolds." and concatenate all of them into a document according to the order of records in table.</p>
<p>After obtaining the serialized structured table, we connect it with the corresponding natural language description $T$ with special token " $&lt;$ table $2 t e x t&gt;$ ". It serves as a functional token that both encodes the overall information of the table and as a starting signal to generate text. The whole sequence is ended with special token " $&lt;$ endoftext $&gt;$ ". In this way, our model encodes the structured table and learns to predict the target sequence one word at a time as in GPT-2. We denote the final input sequence as $\boldsymbol{S T}=s t_{1}, \cdots, s t_{m+n+2} . m$ is the length of serialized table, $n$ is the length of text and 2 refers to the special tokens mentioned above. The language model's training objective is to maximize the likelihood of the reference text, which is equivalent to minimize the negative log likelihood (language model loss $\mathcal{L}_{L M}$ ) as characterized by Equation 1.</p>
<p>$$
\mathcal{L}<em i="m+2">{L M}=-\frac{1}{n+1} \sum</em>\right)
$$}^{m+n+2} \log P\left(s t_{i} \mid s t_{&lt;i</p>
<h1>3.2 Table Structure Reconstruction</h1>
<p>As shown in Table 1, unlike many natural language generation tasks that take sentences as input, table-to-text generation models need to process table with structural information. Each data record in the input can be seen as a pair of attribute and value. Traditionally, table-to-text models utilized attribute-value concatenation to represent the tables. In this way, they are able to capture the structural information by learning the correspondence between value and attribute. However, when we transform the table into natural language and use pre-trained language model GPT-2 for representation, it lacks the explicit modeling to incorporate such structural information. Inspired by Liu et al. (2019), our model treats the attribute names as the labels for the model to reconstruct such structural information from GPT-2's learned table values' representation.</p>
<p>In detail, as shown in Figure 1 (b), given a serialized table $S_{k}$ which consists of different attributes and values in natural language form, the table structure reconstruction task takes the last layer of GPT-2's hidden states for each value tokens $\left[\left.H^{t_{i, j}}\right]<em i="i">{i=1: n, j=1: m</em>$ is the number of tokens of $i$ th record's value.}}\right.$ of the table and classify which attribute does each value token's representation refer to. Specifically, the $i$ means the $i$ th record of the table $S_{k}, j$ means the $j$ th value token of $i$ th record, $n$ is the number of records and $m_{i</p>
<p>$$
\begin{gathered}
P\left(a_{i, j}\right)=\operatorname{softmax}\left(\mathbf{H}^{\mathbf{t}<em _mathbf_t="\mathbf{t">{\mathbf{i}, \mathbf{j}}} \mathbf{W}</em>}}+\mathbf{b<em R="R" S="S">{\mathbf{t}}\right) \
\mathcal{L}</em>\right)
\end{gathered}
$$}=-\frac{1}{Z} \sum_{i=1}^{n} \sum_{j=1}^{m_{i}} \log P\left(a_{i, j}^{*</p>
<p>Equation 2 shows the detail of the reconstruction classifier. Please note that the serialized table consists of attribute, value and template tokens. In this auxiliary task, we only take the GPT-2's hidden states for those value tokens and reconstruct the structural information by classifying their corresponding attribute. $W_{t}$ and $b_{t}$ are the trainable parameters of the introduced classifier and $p\left(a_{i, j}\right)$ is probability to classify value token $H^{t_{i, j}}$ as referring to attribute $a_{i, j}$. We use cross entropy as this task's objective function, illustrated by Equation 2 and 3. $a_{i, j}^{*}$ refers to the gold attribute label for the value token and $Z$ is the number of value tokens' that need to reconstruct the corresponding attribute label. By incorporating this auxiliary task, TableGPT can be guided to embed structural information when representing the table at the training stage.</p>
<h3>3.3 Content Matching</h3>
<p>Take Table 1 as an example, generating high fidelity text that correctly describe information in the table is the core of table-to-text generation. Producing fluent but incorrect text still means unsatisfying performance as the text is not reliable for the purpose of disseminating comprehensible information. Ideally,</p>
<p>when generating words that is intended to describe information in table, directly copying them from table will result in high-fidelity text. However, it's non-trivial to integrate a copy mechanism inside the transformer architecture of GPT-2 model, since the change of model structure may break syntactic and semantic features contained in the pretrained language model, which are essential for text generation especially in few-shot setting. Also, rephrasing sometimes is needed to produce more natural text.</p>
<p>In order to encourage our model to generate high-fidelity text while keeping GPT-2's advantage of produing fluent text, we utilize another auxiliary task, called content matching task, during finetuning on the table-to-text corpus. The content matching task is to explicitly match the important information in a table with information in the corresponding generated text. An intuitive way is to apply a mis-matching loss by hard-matching key information in table and information in the generated text. But that is discrete and non-differentiable and the corresponding gradient descent can't be learned directly. Inspired by optimal transport (OT) that can measure the distance between information in source sequence and target sequence (Chen et al., 2019a) without breaking the end-to-end training process, we adopt it as a content matching loss that guide the model to generate text containing information that align with the table.</p>
<p>As in Section 3.1, the whole GPT-2 training sequence consists of serialized table and reference text. The serialized table sequence, $\boldsymbol{x}=x_{1}, \cdots, x_{m}$, can be represented as a discrete distribution $\boldsymbol{\mu}=\sum_{i=1}^{m} u_{i} \delta_{x_{i}}$, where $u_{i} \geq 0$ and $\sum_{i} u_{i}=1, m$ is the length and $\delta_{x}$ is the Dirac function centered on $\boldsymbol{x}$. Similar with the serialized table sequence, the discrete distribution of reference text sequence $\boldsymbol{y}=y_{1}, \cdots, y_{n}$ can be represented as $\boldsymbol{\nu}=\sum_{j=1}^{n} v_{j} \delta_{y_{j}}$. Under such setting, computing the OT (optimal transport) distance between probability distributions $\mathbf{u}=\left{u_{i}\right}<em j="j">{i=1}^{m}$ and $\mathbf{v}=\left{v</em>$ is defined as the solution of the following network-flow problem (Luise et al., 2018):}\right}_{j=1}^{n</p>
<p>$$
\mathcal{L}<em _mathbf_T="\mathbf{T">{C M}=\min </em>\right)
$$} \in \Pi(\boldsymbol{\mu}, \boldsymbol{\nu})} \sum_{i=1}^{m} \sum_{j=1}^{n} T_{i j} \cdot d\left(x_{i}, y_{j</p>
<p>where $\Pi(\boldsymbol{\mu}, \boldsymbol{\nu})=\left{\mathbf{T} \in \mathbb{R}<em n="n">{+}^{m \times n} \mid \mathbf{T} \mathbf{1}</em>}=\boldsymbol{\mu}, \mathbf{T}^{\top} \mathbf{1<em n="n">{m}=\boldsymbol{\nu}\right}$ which is the set of joint distribution of the two marginal distribution $\mathbf{u}$ and $\mathbf{v}, \mathbf{1}</em>}$ and $\mathbf{1<em i="i">{m}$ are $n$-dimensional all-one vector and $m$-dimensional all-one vector respectively, and $d\left(x</em>\right|}, y_{j}\right)$ denotes the cost of moving $x_{i}$ to $y_{j}$. Especially, we adopt the cosine distance between two token embedding vectors of $x_{i}$ and $y_{j}$, which is defined as $d\left(x_{i}, y_{j}\right)=$ $1-\frac{x_{i} y_{j}}{\left|x_{i<em j="j">{2}\left|y</em>$ is computational intractable. In order to overcome this problem, we use the recently proposed Inexact Proximal point method for Optimal Transport (IPOT) (Chen et al., 2019a) as an approximation.}\right|_{2}}$. Exact minimization over $\mathbf{T</p>
<p>For natural language generation tasks such as neural machine translation, OT distance is often applied by matching source sequence with whole target sequence, since almost every word in both sequences are supposed to be matched. However, when it comes to table-to-text task in a realistic way, there are some redundant information or words in both table and text. In order to apply the OT distance, unlike previous adoption (Wang et al., 2020) based on the assumption that all information in the table should be described in text, we propose to only match the record words which appear in both table and reference text. In this way, the OT distance is able to avoid wrongly penalizing text that doesn't mention redundant information in table.</p>
<h1>3.4 Learning Objective</h1>
<p>For table structure reconstruction and content matching, both auxiliary tasks are trained with the main GPT-2's language model loss together, which can be regarded as multi-task learning. The loss function of multi-task learning consists of language model loss $\mathcal{L}<em R="R" S="S">{L M}$, table structure reconstruction loss $\mathcal{L}</em>}$ and content matching loss $\mathcal{L<em M="M" T="T">{C M}$. In this way, the loss function $\mathcal{L}</em>$ of the full model is computed as follows:</p>
<p>$$
\mathcal{L}<em L="L" M="M">{M T}=\mathcal{L}</em>}+\lambda_{1} \mathcal{L<em 2="2">{S R}+\lambda</em>
$$} \mathcal{L}_{C M</p>
<p>where $\lambda_{1}$ and $\lambda_{2}$ are hyper-parameters that are two scale factors. Please note that when optimizing the $\mathcal{L}_{C M}$ with IPOT algorithm, the gradients of OT loss are hard to back propagate to model's parameters, since the process of sampling words from multinomial distribution which comes from language model</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Humans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Books</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Songs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># of training instances</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">2.9</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">6.1</td>
<td style="text-align: center;">8.3</td>
<td style="text-align: center;">7.3</td>
<td style="text-align: center;">6.8</td>
<td style="text-align: center;">7.8</td>
<td style="text-align: center;">8.8</td>
<td style="text-align: center;">10.4</td>
<td style="text-align: center;">12.0</td>
<td style="text-align: center;">11.6</td>
<td style="text-align: center;">13.1</td>
</tr>
<tr>
<td style="text-align: center;">Base + switch + LM</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">29.5</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">41.7</td>
<td style="text-align: center;">34.3</td>
<td style="text-align: center;">36.2</td>
<td style="text-align: center;">37.9</td>
<td style="text-align: center;">40.3</td>
<td style="text-align: center;">36.1</td>
<td style="text-align: center;">37.2</td>
<td style="text-align: center;">39.4</td>
<td style="text-align: center;">42.2</td>
</tr>
<tr>
<td style="text-align: center;">Base + switch + LM (R)</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">27.7</td>
<td style="text-align: center;">33.7</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">35.4</td>
<td style="text-align: center;">37.6</td>
<td style="text-align: center;">40.9</td>
<td style="text-align: center;">35.3</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">41.8</td>
</tr>
<tr>
<td style="text-align: center;">TableGPT</td>
<td style="text-align: center;">29.8</td>
<td style="text-align: center;">34.5</td>
<td style="text-align: center;">40.6</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">35.1</td>
<td style="text-align: center;">37.3</td>
<td style="text-align: center;">38.5</td>
<td style="text-align: center;">41.6</td>
<td style="text-align: center;">36.7</td>
<td style="text-align: center;">37.8</td>
<td style="text-align: center;">39.3</td>
<td style="text-align: center;">42.3</td>
</tr>
<tr>
<td style="text-align: center;">-sr</td>
<td style="text-align: center;">29.1</td>
<td style="text-align: center;">34.0</td>
<td style="text-align: center;">40.2</td>
<td style="text-align: center;">45.5</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">37.1</td>
<td style="text-align: center;">41.5</td>
<td style="text-align: center;">35.6</td>
<td style="text-align: center;">37.7</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">41.8</td>
</tr>
<tr>
<td style="text-align: center;">-cm</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">33.5</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">45.6</td>
<td style="text-align: center;">34.8</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">42.2</td>
<td style="text-align: center;">35.7</td>
<td style="text-align: center;">37.4</td>
<td style="text-align: center;">39.2</td>
<td style="text-align: center;">41.8</td>
</tr>
<tr>
<td style="text-align: center;">-sr\&amp;cm</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">32.9</td>
<td style="text-align: center;">40.0</td>
<td style="text-align: center;">45.3</td>
<td style="text-align: center;">34.2</td>
<td style="text-align: center;">36.5</td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;">41.4</td>
<td style="text-align: center;">35.5</td>
<td style="text-align: center;">37.0</td>
<td style="text-align: center;">39.0</td>
<td style="text-align: center;">41.6</td>
</tr>
</tbody>
</table>
<p>Table 2: BLEU-4 results on Humans, Books and Songs domains.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Humans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Books</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Songs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"># of training instances</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
<td style="text-align: center;">50</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">200</td>
<td style="text-align: center;">500</td>
</tr>
<tr>
<td style="text-align: center;">Base</td>
<td style="text-align: center;">0.1</td>
<td style="text-align: center;">0.4</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">1.7</td>
<td style="text-align: center;">1.5</td>
<td style="text-align: center;">2.1</td>
<td style="text-align: center;">2.4</td>
<td style="text-align: center;">4.1</td>
<td style="text-align: center;">5.1</td>
<td style="text-align: center;">4.7</td>
<td style="text-align: center;">5.8</td>
</tr>
<tr>
<td style="text-align: center;">Base + switch + LM</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">22.1</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">22.5</td>
<td style="text-align: center;">23.1</td>
<td style="text-align: center;">25.0</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">30.1</td>
<td style="text-align: center;">32.6</td>
</tr>
<tr>
<td style="text-align: center;">Base + switch + LM (R)</td>
<td style="text-align: center;">12.1</td>
<td style="text-align: center;">14.1</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">28.2</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">22.2</td>
<td style="text-align: center;">24.4</td>
<td style="text-align: center;">27.3</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">28.3</td>
<td style="text-align: center;">28.8</td>
<td style="text-align: center;">32.0</td>
</tr>
<tr>
<td style="text-align: center;">TableGPT</td>
<td style="text-align: center;">16.3</td>
<td style="text-align: center;">20.6</td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;">32.4</td>
<td style="text-align: center;">24.0</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">26.7</td>
<td style="text-align: center;">28.9</td>
<td style="text-align: center;">27.1</td>
<td style="text-align: center;">29.4</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">32.8</td>
</tr>
<tr>
<td style="text-align: center;">-sr</td>
<td style="text-align: center;">16.7</td>
<td style="text-align: center;">20.2</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">32.3</td>
<td style="text-align: center;">23.4</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">26.5</td>
<td style="text-align: center;">29.2</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">29.0</td>
<td style="text-align: center;">31.0</td>
<td style="text-align: center;">33.5</td>
</tr>
<tr>
<td style="text-align: center;">-cm</td>
<td style="text-align: center;">15.7</td>
<td style="text-align: center;">20.0</td>
<td style="text-align: center;">25.4</td>
<td style="text-align: center;">31.6</td>
<td style="text-align: center;">22.3</td>
<td style="text-align: center;">24.9</td>
<td style="text-align: center;">26.3</td>
<td style="text-align: center;">29.7</td>
<td style="text-align: center;">26.2</td>
<td style="text-align: center;">28.6</td>
<td style="text-align: center;">30.9</td>
<td style="text-align: center;">33.1</td>
</tr>
<tr>
<td style="text-align: center;">-sr\&amp;cm</td>
<td style="text-align: center;">16.2</td>
<td style="text-align: center;">19.3</td>
<td style="text-align: center;">26.6</td>
<td style="text-align: center;">32.1</td>
<td style="text-align: center;">23.7</td>
<td style="text-align: center;">25.1</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">28.7</td>
<td style="text-align: center;">25.8</td>
<td style="text-align: center;">25.9</td>
<td style="text-align: center;">30.6</td>
<td style="text-align: center;">33.7</td>
</tr>
</tbody>
</table>
<p>Table 3: ROUGE-4 (F-measure) results on Humans, Books and Songs domains.
is unfortunately non-differentiable. In order to back propagate $\mathcal{L}_{C M}$, we adopt the Soft-argmax trick to approximate each word embedding vector. During testing, we feed the serialized table with special token $&lt;$ table2text $&gt;$ into GPT-2 and generate the text word by word.</p>
<h1>4 Experiment</h1>
<h3>4.1 Datasets and Experiment Settings</h3>
<p>Following Chen et al. (2020b)'s work on few-shot table-to-text generation, we conducted experiments on three datasets in different domains: Humans, Books and Songs respectively. Each instance in those datasets consists of the structured infobox as the table and first sentence of the article as reference. Same as Chen et al. (2020b), we train our model on different few-shot setting with training dataset size varying from 50, 100, 200 to 500. Also, models are chosen based on performance on validation set with 1000 instances. Test sets for Humans, Books and Songs consist of 13587, 5252 and 11879 instances respectively.</p>
<p>We implement TableGPT based on the transformers library (Wolf et al., 2019). The configuration of base GPT-2 model is 12 layers and 8 attention heads per layer. For optimizer, we adopt the OpenAIAdamW optimizer with 100 warm steps. We train the model with learning rate set to $2 \mathrm{e}-4$. The batch size is set to 10 for all datasets. The weights $\lambda_{1}, \lambda_{2}$ of the table structure reconstruction loss and content matching loss are both 0.2 according to performance on validation set. Following Chen et al. (2020b)'s way to deal with the vocabulary limitation, for all datasets we use the Byte Pair Encoding (BPE) and subword vocabulary as in Radford et al. (2019).</p>
<h3>4.2 Comparing Methods</h3>
<p>We compare our proposed TableGPT with baseline model and previous state-of-the-art model: Base and Base+switch+LM. More details can be found in Chen et al. (2020b).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Domain</th>
<th style="text-align: center;">Humans</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Books</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Songs</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Metrics</td>
<td style="text-align: center;">#Sup</td>
<td style="text-align: center;">#Cont</td>
<td style="text-align: center;">Nat.</td>
<td style="text-align: center;">#Sup</td>
<td style="text-align: center;">#Cont</td>
<td style="text-align: center;">Nat.</td>
<td style="text-align: center;">#Sup</td>
<td style="text-align: center;">#Cont</td>
<td style="text-align: center;">Nat.</td>
</tr>
<tr>
<td style="text-align: center;">Reference</td>
<td style="text-align: center;">$4.20^{\star}$</td>
<td style="text-align: center;">$0.36^{\star}$</td>
<td style="text-align: center;">$1.27^{\star}$</td>
<td style="text-align: center;">$3.54^{\star}$</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">$1.03^{\star}$</td>
<td style="text-align: center;">3.22</td>
<td style="text-align: center;">$0.64^{\star}$</td>
<td style="text-align: center;">$1.26^{\star}$</td>
</tr>
<tr>
<td style="text-align: center;">Base+switch+LM(R)</td>
<td style="text-align: center;">$3.20^{\star}$</td>
<td style="text-align: center;">$0.96^{\star}$</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;">3.32</td>
<td style="text-align: center;">0.46</td>
<td style="text-align: center;">$1.15^{\star}$</td>
<td style="text-align: center;">2.92</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">$0.73^{\star}$</td>
</tr>
<tr>
<td style="text-align: center;">TableGPT</td>
<td style="text-align: center;">3.67</td>
<td style="text-align: center;">0.60</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;">3.28</td>
<td style="text-align: center;">0.42</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">3.05</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">1.01</td>
</tr>
</tbody>
</table>
<p>Table 4: Human evaluation results. Models with * perform significantly different from TableGPT ( $p&lt;$ 0.05 ), using a one-way ANOVA with posthoc Tukey HSD tests.</p>
<ul>
<li>Base: It is based on a Seq2Seq model with field-gating encoder that incorporate the table structure's information during encoding (Liu et al., 2018). Additionally, it utilizes the pre-trained word embedding which is fixed during the training stage. Since it achieves competitive performance on large-scale dataset, it can show how a data-driven Seq2Seq model typically perform with limited training data.</li>
<li>Base + switch + LM: It tries to exploit GPT-2's learnt knowledge from pretraining on vast corpus by proposing a switch policy that choosing between copying from infobox and generating from the GPT-2 language model when generating each word in text. We also use the released codes and data ${ }^{1}$ by Chen et al. (2020b) to reproduce its result for human evaluation and report corresponding automatic evaulation results as Base + switch $+\mathbf{L M}(\mathbf{R})$.</li>
<li>TableGPT: In this paper, we propose TableGPT that exploits GPT-2's learnt knowledge from pretraining on vast corpus for few-shot learning while enhance it for generating high fidelity text with two auxiliary tasks. Also, we perform ablation studies for evaluating each auxiliary task's contribution. -sr represents the variant without table structure reconstruction, -cm represents the variant without content matching and -sr\&amp;cm shows the performance of GPT-2 without auxilary tasks.</li>
</ul>
<h1>4.3 Automatic Evaluation</h1>
<p>Following the previous work Chen et al. (2020b), we adopt BLEU-4 (Papineni et al., 2002) and ROUGE4 (Lin, 2004) to conduct automatic evaluations. Table 2 and 3 show corresponding results of comparing methods on different datasets. Although the Base achieves competitive results when training on largescale dataset (Liu et al., 2018), the performance drops drastically in few-shot setting. While utilizing a switch policy to combine copying words from table and generating words from GPT-2 (Base + switch + LM) can achieve impressive performance in all few-shot setting, a standard GPT-2 model (TableGPT - sr\&amp;cm) that takes a serialized table as input and generate text afterwards without copying can actually perform better in most of the few-shot setting. TableGPT with table structure reconstruction and content matching that preserves structural information during encoding and guide the model to generate high-fidelity text can further improve the performance. Ablation studies also show that each of the auxiliary task attributes to the performance enhancement and applying both of them can achieve the best performance in most of the few-shot setting.</p>
<h3>4.4 Human Evaluation</h3>
<p>Following the settings in Chen et al. (2020b), we conduct human evaluation on TableGPT with previous state-of-the-art model Base+switch+LM(R) and Reference from two aspects: Factual correctness and Language naturalness. We sampled 100 tables along with corresponding generated text from Humans, Books and Songs test set (under the few-shot setting of 100 training data) respectively, resulting in 300 tables in total. In order to reduce variance caused by human, each example is evaluated by three different graduates who have passed intermediate English test and the scores are averaged in Table 4.</p>
<p>The first evaluation criteria Factual correctness focuses on how well the text can correctly describe information provided by the corresponding table. Each rater was provided with both the text and table. They are asked to count how many facts contained in the text are consistent with information from table,</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Attribute</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">name</td>
<td style="text-align: left;">james beattie</td>
</tr>
<tr>
<td style="text-align: left;">fullname</td>
<td style="text-align: left;">james scott beattie</td>
</tr>
<tr>
<td style="text-align: left;">birth</td>
<td style="text-align: left;">27 february 1978 lancaster, england</td>
</tr>
<tr>
<td style="text-align: left;">height</td>
<td style="text-align: left;">61</td>
</tr>
<tr>
<td style="text-align: left;">position</td>
<td style="text-align: left;">striker</td>
</tr>
<tr>
<td style="text-align: left;">currentclub</td>
<td style="text-align: left;">swansea city ( assistant first team coach )</td>
</tr>
<tr>
<td style="text-align: left;">youthyears</td>
<td style="text-align: left;">$1995-1996$</td>
</tr>
<tr>
<td style="text-align: left;">youthclubs</td>
<td style="text-align: left;">blackburn rovers</td>
</tr>
<tr>
<td style="text-align: left;">totalcaps</td>
<td style="text-align: left;">443</td>
</tr>
<tr>
<td style="text-align: left;">totalgoals</td>
<td style="text-align: left;">131</td>
</tr>
<tr>
<td style="text-align: left;">nationalyears</td>
<td style="text-align: left;">19962003</td>
</tr>
<tr>
<td style="text-align: left;">nationalteam</td>
<td style="text-align: left;">england u21 england</td>
</tr>
<tr>
<td style="text-align: left;">nationalcaps</td>
<td style="text-align: left;">95</td>
</tr>
<tr>
<td style="text-align: left;">nationalgoals</td>
<td style="text-align: left;">40</td>
</tr>
<tr>
<td style="text-align: left;">manageryears</td>
<td style="text-align: left;">$2013-2014$</td>
</tr>
<tr>
<td style="text-align: left;">managerclubs</td>
<td style="text-align: left;">accrington stanley</td>
</tr>
<tr>
<td style="text-align: left;">article</td>
<td style="text-align: left;">james beattie ( footballer )</td>
</tr>
<tr>
<td style="text-align: left;">$\ldots$</td>
<td style="text-align: left;">$\ldots$</td>
</tr>
</tbody>
</table>
<p>Reference: james scott beattie (born 27 february 1978 ) is an english former professional footballer who played as a striker .
Base+switch+LM(R): james beattie (, born 10 july 1971 ) is an english former professional association footballer who played for , among others, England .
TableGPT: james beattie (born 27 february 1978 in lancaster ) is a former english footballer who played as a striker .</p>
<p>Table 5: A sample table from Human test set and the texts generated with different methods, trained with 100 training data. The facts that both exist in source table and summary text are highlighted with the same color in background. The red text indicates the wrong fact of generated text conflicts with information in source table. The blue text indicates the generated text fragment has grammatical error.
noted as #Sup, and how many are contradicting with or missing from the table, noted as #Cont. We report the average number of supporting facts (#Sup) and contradicting facts (#Cont) on different dataset in Table 4.</p>
<p>The second evaluation criteria Language naturalness tries to evaluate these models from the aspect of grammaticality (is the sentence grammatically correct?) and fluency of the text (is the sentence fluent and natural?). We arrange text from different models on the same table into 3 pairs. For each pair of text without table, raters are asked to decide which one is better or whether both text are of same quality, solely in terms of language naturalness. When a generated summary is chosen as the better one, we assign 1.0 score to the better one and 0.0 score to the worse. If two summaries are deemed of same quality, we assign 0.5 score to both of them. We then calculate the average scores and report results on different dataset in Table 4.</p>
<p>Results show that TableGPT can produce less contradicting facts than previous state-of-the-art model Base+switch+LM(R) on Humans and Books and achieves comparable performance on Songs. Meanwhile, TableGPT can include more supporting facts on Humans and Songs and generate more natural text than Base+switch+LM(R) on Humans and Songs. Overall, our TableGPT model can improve text fidelity while preserving the naturalness of the text.</p>
<h1>4.5 Case Study</h1>
<p>Compared with the previous state-of-the-art model Base+switch+LM(R) and Reference, TableGPT performs better. It can accurately describe most of the key information in fluent text compared with reference. For Base+switch+LM(R), the design of separate copy mechanism and GPT-2 may attributes to the inconsistent expression "played for, among others, England ." and the expression of wrong birth date, which shows the imperfect switch policy on deciding when to copy from table can sometimes hurt model's ability to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example.</p>
<h2>5 Related Work</h2>
<p>In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model's content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structure and dual attention mechanism to utilize the structure information when decoding. In addition, Bao et al. (2018) develops a table-aware sequence-to-sequence model on this task. However, Chen et al. (2020b) shows that the well-performed Seq2Seq model trained on large-scale dataset suffer from limited training data in few-shot setting.</p>
<p>Recently, GPT-2 has been successfully adapted to dialogue generation in few-shot setting (Zhang et al., 2020; Peng et al., 2020), showing potential to address insufficient training data problem for few-shot learning with the help of learnt knowledge from pretraining on vast corpus. As for table-to-text generation, Chen et al. (2020b) propose a switch model that use GPT-2 to generate template-like functional words while generating factual expressions via copying records' values from table in few-shot scenario. Different from this work, we model the table and generate text within a GPT-2 model in a unified way and we show that our proposed TableGPT can perform well in the few-shot scenario. In addition, different from both works mentioned above, we enhance GPT-2's ability to model table structure and to improve text fidelity. Another closely related paper is Chen et al. (2019b), which predicts whether a statement align with records in the table. Since the nature of classification task makes it possible to model table records bidirectionally, it use BERT with templates to transform and model the table. Meanwhile, Chen et al. (2020a) explores coarse-to-fine table-to-text generation with standard GPT-2 model. Different from above two works, we adapted GPT-2 in a text generation scenario for structured data input and more importantly address table structure modeling and improve text fidelity. In addition, one of the auxiliary task: content matching is inspired by ideas in machine translation (Yang et al., 2019a) and Seq2Seq (Chen et al., 2019a) model. The closest paper on data-to-text generation is Wang et al. (2020). They assume that the expected generated text should cover all information in the table. But in a more realistic scenario, like the task we explore in this paper, the table consists of redundant information and only the important ones should be used to constraint model to generate high-fidelity text. Therefore, we propose to match important information only in the table and information in text as an auxiliary task during training.</p>
<h2>6 Conclusion</h2>
<p>In this work, we present TableGPT, which enhances GPT-2 for table-to-text generation with two auxiliary tasks, table structure reconstruction and content matching, for improving text fidelity while exploiting GPT-2's learnt linguistic knowledge from pretraining on large-scale corpus. In detail, we use table transformation to bridge the gap between structured table and natural language input for GPT-2 and further enhance GPT-2 with following two auxiliary tasks for table-to-text generation. The table structure reconstruction task help model preserve the structural information of input while representing table with powerful pretrained GPT-2. In addition, the content matching task guides model to generate high-fidelity</p>
<p>task with less incorrect expressions that are contradicting to the table via measuring distance between table and information in generated text. Experiments are conducted on three datasets, Humans, Books and Songs, in different domains. Both automatic evaluation and human evaluation results show that our model achieves new state-of-the-art performance in most few-shot setting.</p>
<h1>Acknowledgements</h1>
<p>We would like to thank the anonymous reviewers for their helpful comments. Xiaocheng Feng is the corresponding author of this work. This work is supported by the National Key R\&amp;D Program of China via grant 2018YFB1005103 and National Natural Science Foundation of China (NSFC) via grant 61906053 and 61976073 .</p>
<h2>References</h2>
<p>Junwei Bao, Duyu Tang, Nan Duan, Zhao Yan, Yuanhua Lv, Ming Zhou, and Tiejun Zhao. 2018. Table-to-text: Describing table region with natural language. In AAAI, pages 5020-5027.</p>
<p>Liqun Chen, Yizhe Zhang, Ruiyi Zhang, Chenyang Tao, Zhe Gan, Haichao Zhang, Bai Li, Dinghan Shen, Changyou Chen, and Lawrence Carin. 2019a. Improving sequence-to-sequence learning via optimal transport. In ICLR.</p>
<p>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2019b. Tabfact: A large-scale dataset for table-based fact verification. ICLR.</p>
<p>Wenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. 2020a. Logical natural language generation from open-domain tables. In $A C L$, pages 7929-7942.</p>
<p>Zhiyu Chen, Harini Eavani, Wenhu Chen, Yinyin Liu, and William Yang Wang. 2020b. Few-shot NLG with pre-trained language model. In $A C L$, pages 183-190.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171-4186.</p>
<p>Ondřej Dušek, Jekaterina Novikova, and Verena Rieser. 2020. Evaluating the State-of-the-Art of End-to-End Natural Language Generation: The E2E NLG Challenge. Computer Speech \&amp; Language, 59:123-156.</p>
<p>Xiaocheng Feng, Yawei Sun, Bing Qin, Heng Gong, Yibo Sun, Wei Bi, Xiaojiang Liu, and Ting Liu. 2020. Learning to select bi-aspect information for document-scale text content manipulation. In AAAI, pages 77167723 .</p>
<p>Heng Gong, Xiaocheng Feng, Bing Qin, and Ting Liu. 2019. Table-to-text generation with effective hierarchical encoder on three dimensions (row, column and time). In EMNLP, pages 3141-3150.</p>
<p>Sadid A Hasan and Oladimeji Farri. 2019. Clinical natural language processing with deep learning. In Data Science for Healthcare, pages 147-171.</p>
<p>Rémi Lebret, David Grangier, and Michael Auli. 2016. Neural text generation from structured data with application to the biography domain. In EMNLP, pages 1203-1213.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81.</p>
<p>Tianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang, and Zhifang Sui. 2018. Table-to-text generation by structureaware seq2seq learning. In AAAI, pages 4881-4888.</p>
<p>Tianyu Liu, Fuli Luo, Qiaolin Xia, Shuming Ma, Baobao Chang, and Zhifang Sui. 2019. Hierarchical Encoder with Auxiliary Supervision for Neural Table-to-Text Generation: Learning Better Representation for Tables. In AAAI, pages 6786-6793.</p>
<p>Giulia Luise, Alessandro Rudi, Massimiliano Pontil, and Carlo Ciliberto. 2018. Differential properties of sinkhorn approximation for learning with wasserstein distance. In Advances in Neural Information Processing Systems, pages $5859-5870$.</p>
<p>Hongyuan Mei, TTI UChicago, Mohit Bansal, and Matthew R Walter. 2016. What to talk about and how? selective generation using lstms with coarse-to-fine alignment. In NAACL-HLT, pages 720-730.</p>
<p>Soichiro Murakami, Akihiko Watanabe, Akira Miyazawa, Keiichi Goshima, Toshihiko Yanase, Hiroya Takamura, and Yusuke Miyao. 2017. Learning to generate market comments from stock prices. In $A C L$, pages 1374-1384.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In $A C L$, pages 311-318.</p>
<p>Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li, Jinchao Li, Michael Zeng, and Jianfeng Gao. 2020. Few-shot natural language generation for task-oriented dialog. arXiv preprint arXiv:2002.12328.</p>
<p>Ratish Puduppully, Li Dong, and Mirella Lapata. 2019a. Data-to-text generation with content selection and planning. In AAAI, pages 6908-6915.</p>
<p>Ratish Puduppully, Li Dong, and Mirella Lapata. 2019b. Data-to-text generation with entity modeling. In ACL.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.</p>
<p>Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian Li, Baobao Chang, and Zhifang Sui. 2018. Order-planning neural text generation from structured data. In AAAI, pages 5414-5421.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008.</p>
<p>Zhenyi Wang, Xiaoyang Wang, Bang An, Dong Yu, and Changyou Chen. 2020. Towards faithful neural table-totext generation with content-matching constraints. In $A C L$, pages 1072-1086.</p>
<p>Sam Wiseman, Stuart M Shieber, and Alexander M Rush. 2017. Challenges in data-to-document generation. In EMNLP, pages 2253-2263.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, and Jamie Brew. 2019. HuggingFace's Transformers: State-of-theart Natural Language Processing.</p>
<p>Mingming Yang, Rui Wang, Kehai Chen, Masao Utiyama, Eiichiro Sumita, Min Zhang, and Tiejun Zhao. 2019a. Sentence-level agreement for neural machine translation. In ACL, pages 3076-3082.</p>
<p>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019b. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages 5754-5764.</p>
<p>Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT : Large-scale generative pre-training for conversational response generation. In $A C L$, pages $270-278$.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/czyssrs/Few-Shot-NLG&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
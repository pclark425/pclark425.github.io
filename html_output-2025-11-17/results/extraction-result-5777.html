<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5777 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5777</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5777</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-117.html">extraction-schema-117</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to estimate or predict the probability or likelihood of specific future real-world scientific discoveries, including details on methods, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-0f1c956f84a68ea4d6f5ebcf3c4d1d4e1f41d8f3</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0f1c956f84a68ea4d6f5ebcf3c4d1d4e1f41d8f3" target="_blank">Language models can learn complex molecular distributions</a></p>
                <p><strong>Paper Venue:</strong> Nature Communications</p>
                <p><strong>Paper TL;DR:</strong> This work introduces several challenging generative modeling tasks by compiling larger, more complex distributions of molecules and evaluates the ability of language models on each task, demonstrating that language models are powerful generative models, capable of adeptly learning complex molecular distributions.</p>
                <p><strong>Paper Abstract:</strong> Deep generative models of molecules have grown immensely in popularity, trained on relevant datasets, these models are used to search through chemical space. The downstream utility of generative models for the inverse design of novel functional compounds, depends on their ability to learn a training distribution of molecules. The most simple example is a language model that takes the form of a recurrent neural network and generates molecules using a string representation. Since their initial use, subsequent work has shown that language models are very capable, in particular, recent research has demonstrated their utility in the low data regime. In this work, we investigate the capacity of simple language models to learn more complex distributions of molecules. For this purpose, we introduce several challenging generative modeling tasks by compiling larger, more complex distributions of molecules and we evaluate the ability of language models on each task. The results demonstrate that language models are powerful generative models, capable of adeptly learning complex molecular distributions. Language models can accurately generate: distributions of the highest scoring penalized LogP molecules in ZINC15, multi-modal molecular distributions as well as the largest molecules in PubChem. The results highlight the limitations of some of the most popular and recent graph generative models– many of which cannot scale to these molecular distributions. Generative models for the novo molecular design attract enormous interest for exploring the chemical space. Here the authors investigate the application of chemical language models to challenging modeling tasks demonstrating their capability of learning complex molecular distributions.</p>
                <p><strong>Cost:</strong> 0.004</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5777",
    "paper_id": "paper-0f1c956f84a68ea4d6f5ebcf3c4d1d4e1f41d8f3",
    "extraction_schema_id": "extraction-schema-117",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00352675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Keeping it Simple: Language Models can learn Complex Molecular Distributions</h1>
<p>Daniel Flam-Shepherd, ${ }^{1,2, \text { * }}$ Kevin Zhu, ${ }^{1}$ and Alán Aspuru-Guzik ${ }^{1,2,3,4, \dagger}$<br>${ }^{1}$ Department of Computer Science, University of Toronto, Toronto, Ontario M5S 2E4, Canada<br>${ }^{2}$ Vector Institute for Artificial Intelligence, Toronto, Ontario M5S 1M1, Canada<br>${ }^{3}$ Department of Chemistry, University of Toronto, Toronto, Ontario M5G 1Z8, Canada<br>${ }^{4}$ Canadian Institute for Advanced Research, Toronto, Ontario M5G 1Z8, Canada<br>(Dated: December 7, 2021)</p>
<h4>Abstract</h4>
<p>Deep generative models of molecules have grown immensely in popularity, trained on relevant datasets, these models are used to search through chemical space. The downstream utility of generative models for the inverse design of novel functional compounds, depends on their ability to learn a training distribution of molecules. The most simple example is a language model that takes the form of a recurrent neural network and generates molecules using a string representation. More sophisticated are graph generative models, which sequentially construct molecular graphs and typically achieve state of the art results. However, recent work has shown that language models are more capable than once thought, particularly in the low data regime. In this work, we investigate the capacity of simple language models to learn distributions of molecules. For this purpose, we introduce several challenging generative modeling tasks by compiling especially complex distributions of molecules. On each task, we evaluate the ability of language models as compared with two widely used graph generative models. The results demonstrate that language models are powerful generative models, capable of adeptly learning complex molecular distributions- and yield better performance than the graph models. Language models can accurately generate: distributions of the highest scoring penalized LogP molecules in ZINC15, multi-modal molecular distributions as well as the largest molecules in PubChem.</p>
<p>The efficient exploration of chemical space is one of the most important objectives in all of science, with numerous applications in therapeutics and materials discovery. However, exploration efforts have only probed a very small subset of the synthetically accessible chemical space [1], therefore developing new tools is essential. It is possible that the rise of artificial intelligence will provide the methods to unlock the mysteries of the chemical universe, given its success in other challenging scientific questions like protein structure prediction [2].</p>
<p>Very recently, deep generative models have emerged as one of the most promising tool for this immense challenge [3]. These models are trained on relevant subsets of chemical space and can generate novel molecules similar to their training data. Their ability to learn the training distribution and generate valid, similar molecules-is important for success in downstream applications like the inverse design of functional compounds.</p>
<p>The first models involved re-purposing recurrent neural networks (RNNs) [4] in order to generate molecules as SMILES strings [5]. These language models can be used to generate molecular libraries for drug discovery [6] or built into variational autoencoders (VAE) [3, 7] where bayesian optimization can be used to search through the model's latent space for drug-like molecules. However there are problems with language models that make it challenging to train models' capable of generating valid molecules. The largest issue is the brittleness of the SMILES string representation that is widely used in these</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>models. If a single character is misplaced or erroneously generated then the produced SMILES string will correspond to an invalid molecule. This makes it difficult to train and apply language models for practical applications, in spite of this, researchers have attempted to use them for ligand based de novo design [8].</p>
<p>Other models that use graph representations do not have this issue and can be directly constrained to enforce valency restrictions. These graph generative models sequentially construct molecules as a series of probabilistic decisions [9-14] using graph neural networks [15, 16]. These models have been shown to be more capable in modeling distributions and useful in the inverse design of molecules with specific properties [11, 12]. Most importantly, they will always generate valid molecules in contrast to SMILES language models and others that generate whole molecules in one shot [17-20].</p>
<p>Recently, more robust molecular string representations have been proposed [21-23]. In particular, self referencing embedding strings (SELFIES) [24] distinguish themselves as every SELFIES string is a valid molecule. Additionally, with improved training methods, language models have achieved promising results [10, 25], particularly in the low-data regime [26, 27]. Given these developments, in order to test the ability of language models, we formulate a series of difficult generative modeling tasks by constructing training sets of molecules that are especially challenging to learn. We train language models on all tasks and provide baseline comparisons with graph models. The results demonstrate that language models are powerful generative models and can learn very complex molecular distributions that some popular graph models are less able or completely unable to.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>FIG. 1. a-c The molecular distributions defining the three complex molecular generative modeling task. a The distribution of penalized LogP vs SA score from the training data in the penalized logP task. b The four modes of differently weighted molecules in the training data of the multi-distribution task c Large scale task's molecular weight training distribution. d-f examples of molecules plotted using rdkit [28] from the training data in each of the generative modeling tasks d The penalized LogP task e The multi-distribution task f The large scale task g The main models considered and the representation they learn on: 1) SM-RNN trained on SMILES strings 2) SF-RNN trained on SELFIES 3) JTVAE using a graph and tree representation and 4) CGAVE using a graph representation.</p>
<h2>I. RESULTS</h2>
<p>We define three tasks, generating: 1) distributions of molecules with high scores of penalized LogP [3] (FIG. 1 a,d) 2) multi-modal distributions of molecules (FIG. 1 b,e) and 3) the largest molecules in PubChem (FIG. 1 c,e). Necessarily, each different generative modeling task is defined by learning to generate from the distribution of molecules in a dataset. We build three datasets using relevant subsets of larger databases.</p>
<p>For each task we assess performance by plotting the distribution of training molecules properties and the distribution learned by the language models and graph models. We use a histogram for the training molecules and fit a Gaussian kernel density estimator to it by tuning its bandwidth parameter. We plot KDE's for molecular properties from all models using the same bandwidth parameter.</p>
<p>From all models we initially generate 10K (thousand) molecules, compute their properties and use them to produce all plots and metrics. Furthermore, for fair comparison of learned distributions, we use the same number of generated molecules from all models after removing duplicates and training molecules.</p>
<p>For quantitative evaluation of any model's ability to learn its training distribution, we compute the Wasserstein distance between property values of generated molecules and training molecules. We also compute the Wasserstein distance between different samples of training molecules in order to determine a most optimal baseline, which we can compare with as an oracle.</p>
<p>For molecular properties we consider: quantitative estimate of drug-likeness (QED) [29], synthetic accessibility score (SA) [30], octanol-water partition coefficient (LogP) [31], exact molecular weight (MW), Bertz complexity (BCT) [32], natural product likeness (NP) [33]. We also use standard metrics like validity, uniqueness, novelty—to assess the model's ability to generate a diverse set of real molecules distinct from the training data.</p>
<p>For models, our main consideration is a chemical language model using a recurrent neural network with long short-term memory [34] and is trained on SMILES (SM-RNN) or SELFIES (SF-RNN). We also train two state-of-the-art graph generative models: the junction tree variational autoencoder (JTVAE) [11] and the constrained graph variational autoencoder (CGVAE) [10]. Each model is trained on a different molecular representation: SMILES/SELFIES strings or trees/graphs (FIG. 1 g).</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>FIG. 2. Penalized LogP Task a The plotted distribution of the penalized LogP scores of molecules from the training data (TRAIN) with the SM-RNN trained on SMILES, the SF-RNN trained on SELFIES and graph models: CGVAE and JTVAE. For the graph models we display 3 molecules from the out of distribution mode at penalized LogP score ∈ [1.5, 2.5] as well as 3 molecules with penalized LogP score in the the main mode [4.0, 4.5] from all models. b Distribution plots for all models and training data of molecular properties QED, LogP and SA score.</p>
<table>
<thead>
<tr>
<th></th>
<th>LogP</th>
<th>SA</th>
<th>QED</th>
<th>MW</th>
<th>BCT</th>
<th>NP</th>
</tr>
</thead>
<tbody>
<tr>
<td>TRAIN</td>
<td>0.020</td>
<td>0.0096</td>
<td>0.0029</td>
<td>1.620</td>
<td>7.828</td>
<td>0.013</td>
</tr>
<tr>
<td>SM-RNN</td>
<td>0.095</td>
<td>0.0312</td>
<td>0.0068</td>
<td>3.314</td>
<td>21.12</td>
<td>0.054</td>
</tr>
<tr>
<td>SF-RNN</td>
<td>0.177</td>
<td>0.2903</td>
<td>0.0095</td>
<td>6.260</td>
<td>25.00</td>
<td>0.209</td>
</tr>
<tr>
<td>JTVAE</td>
<td>0.536</td>
<td>0.2886</td>
<td>0.0811</td>
<td>35.93</td>
<td>76.81</td>
<td>0.164</td>
</tr>
<tr>
<td>CGVAE</td>
<td>1.000</td>
<td>2.1201</td>
<td>0.1147</td>
<td>69.26</td>
<td>141.2</td>
<td>1.965</td>
</tr>
</tbody>
</table>
<p>TABLE I. Penalized LogP Task Wasserstein metrics for LogP, SA, QED, MW, BT and NP between molecules from the training data and generated by the models. TRAIN is an oracle baseline- values closer to it are better.</p>
<h3>A. Penalized LogP Task</h3>
<p>For the first task, we consider one of the most widely used benchmark assessments for searching chemical space, the penalized LogP task– finding molecules with high LogP [35] penalized by synthesizability [30] and unrealistic rings. We turn this task into a generative modeling one, where the goal is to learn distributions of molecules with high penalized LogP scores. Finding a single molecule with a good score (above 3.0) can be challenging but learning to directly generate from this part of chemical space, so that every molecule produced by the model has high penalized LogP, adds another layer of complexity to the standard task. For this we build a training dataset by screening the ZINC15 database [36] for molecules with values of penalized LogP exceeding 4.0. Most machine learning approaches only find a handful of molecules in this range, for example JTVAE [11] found 22 total during all their attempts. After screening, the top scoring molecules in ZINC amounted to roughly 160K (K is thousand) molecules for the training data in this task. Thus, the training distribution is extremely spiked with most density falling around 4.0-4.5 penalized LogP as seen in FIG.1a with most training molecules resembling the examples shown in FIG. 1d. However, some of the training molecules, around 10% have even higher penalized LogP scores– adding a subtle tail to the distribution.</p>
<p>The results of training all models are shown in FIG. 2 and 3. The language models perform significantly better than the graph models, with the SELFIES RNN pro</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>FIG. 3. Penalized LogP Task a 2d histograms of penalized LogP and SA score from molecules generated by the models or from training data that have penalized LogP ≥ 6.0. b Histograms of penalized LogP, Atoms #, Ring # and length of largest carbon chain (all per molecule) from molecules generated by all models or from the training data that have penalized LogP ≥ 6.0. c 10 molecules generated by the models or from the training data that have penalized LogP ≥ 6.0.</p>
<p>ducing a slightly closer match to the training distribution in FIG. 2a. The CGVAE and JTVAE learn to produce a large number of molecules with penalized LogP scores that are substantially worse than the lowest training scores. It is important to note, from the examples of these shown in FIG. 2a these lower scoring molecules are quite similar to the molecules from the main mode of the training distribution, this highlights the difficulty of learning this distribution. In FIG. 2b we see that JTVAE and CGVAE learn to produce more molecules with larger SA scores than the training data. For LogP we see that all models learn the main mode but the RNNs produce closer distributions, similar results can be seen for QED. These results carry over for quantitative metrics and both RNNs achieve lower Wasserstein distance metrics than the CGVAE and JTVAE (Table I) with the SMILES RNN coming closest to the TRAIN oracle.</p>
<p>We further investigate the highest penalized LogP region of the training data with values exceeding 6.0—the subtle tail of the training distribution. In the 2d distributions (FIG. 3a) it's clear that both RNNs learn this subtle aspect of the training data while the graph models ignore it almost completely and only learn molecules that are closer to the main mode. In particular, CGVAE learns molecules with larger SA score than the training data. Furthermore, the molecules with highest penalized LogP scores in the training data typically contain very long carbon chains and fewer rings (FIG. 3b)—the RNNs are capable of picking up on this. This is very apparent in samples shown from the baselines and training data in FIG. 3c—the RNNs produce similar molecules while the CGVAE and JTVAE generate molecules with many rings.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>FIG. 4. Multi-distribution Task a The Histogram and KDE of molecular weight of training molecules along with KDEs of molecular weight of molecules generated from all models. Three training molecules from each mode are shown. b The Histogram and KDE of QED, LogP and SA scores of training molecules along with KDEs of molecules generated from all models. c 2d histograms of molecular weight and SA score of training molecules and molecules generated by all models.</p>
<p>that have penalized LogP scores near 6.0. The language models produce molecules more similar to the training examples in FIG. 3c and their distribution is close to the training distribution in the histograms of FIG. 3a&amp; b. Overall, the language models could learn distributions of molecules with high penalized LogP scores, better than the graph models.</p>
<h3>B. Multi-distribution Task</h3>
<p>For the next task, we created a dataset by combining subsets of : 1) GDB13 [37] molecules with molecular weight (MW) ≤ 185 2) ZINC [3, 36] molecules with 185</p>
<table>
<thead>
<tr>
<th></th>
<th>LogP</th>
<th>SA</th>
<th>QED</th>
<th>MW</th>
<th>BCT</th>
<th>NP</th>
</tr>
</thead>
<tbody>
<tr>
<td>TRAIN</td>
<td>0.048</td>
<td>0.0158</td>
<td>0.0020</td>
<td>2.177</td>
<td>14.149</td>
<td>0.010</td>
</tr>
<tr>
<td>SM-RNN</td>
<td>0.081</td>
<td>0.0246</td>
<td>0.0059</td>
<td>5.483</td>
<td>21.118</td>
<td>0.012</td>
</tr>
<tr>
<td>SF-RNN</td>
<td>0.286</td>
<td>0.1791</td>
<td>0.0227</td>
<td>11.35</td>
<td>68.809</td>
<td>0.079</td>
</tr>
<tr>
<td>JTVAE</td>
<td>0.495</td>
<td>0.2737</td>
<td>0.0343</td>
<td>27.71</td>
<td>171.87</td>
<td>0.109</td>
</tr>
<tr>
<td>CGVAE</td>
<td>1.617</td>
<td>1.8019</td>
<td>0.0764</td>
<td>30.31</td>
<td>183.58</td>
<td>1.376</td>
</tr>
</tbody>
</table>
<p>TABLE II. Multi-distribution Task Wasserstein metrics of LogP, SA, QED, MW, BCT, NP of training and model generated molecules. TRAIN is an oracle baseline.</p>
<p>≤ MW ≤ 425 3) Harvard clean energy project (CEP) [38] molecules with 460 ≤ MW ≤ 600, and the 4) POLYMERS [39] molecules with MW ≥ 600. The training</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" />
b
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>FIG. 5. Large Scale Task a The Histogram and KDE of molecular weight of training molecules along with the KDEs of molecular weight of molecules generated from the RNNs. Two molecules generated by the RNN's with lower molecular weight than most training molecules are shown on the left of the plot. In addition, two training molecules from three different regions in the distribution of molecular weight are displayed. b The Histogram and KDE of LogP of training molecules along with the KDEs of LogP of molecules generated from the RNNs. On either side of the plot, for each mode in the LogP distribution, we display 2 molecules from the training data and generated by the RNNs.
distribution has four modes- (FIG.1b,e \&amp; 4a). CEP \&amp; GDB13 make up $1 / 3$ and ZINC \&amp; POLYMERS take up $1 / 3$ each of $\sim 200 \mathrm{~K}$ training molecules.</p>
<p>In the multi-distribution task, both RNN models capture the data distribution quite well and learn every mode in the training distribution FIG. 4a, with comparable quality. On the other hand, JTVAE entirely misses the first mode from GDB13 then poorly learns ZINC and CEP. As well, CGVAE learns GDB13 but underestimates ZINC and entirely misses the mode from CEP. More evidence that the RNN models learn the training distribution more closely is apparent in FIG. 4c where CGVAE and JTVAE barely distinguish the main modes. Additionally, the RNN models generate molecules better resembling the training data (supplementary FIG. II). Despite this, all models-except CGVAE, capture the training distribution of QED, SA score and Bertz Complexity
(FIG. 4b). Lastly, in TABLE II the RNN trained on SMILES has the lowest Wasserstein metrics followed by the SELFIES RNN then JTVAE and CGVAE.</p>
<h2>C. Large Scale Task</h2>
<p>The last generative modeling task, involves testing the ability of deep generative models to learn large molecules, the largest possible molecules relevant to molecular generative models that use SMILES/SELFIES string representations or graphs. For this we turn to PubChem [40] and screen for the largest molecules with more than 100 heavy atoms, producing 300 K molecules. These are molecules of various kinds: small bio-molecules, photovoltaics and others (FIG.1f). They also have a wide range of molecular weight from 1500 to 5000 but most</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>FIG. 6. Large Scale Task a Histograms of fragment #, single atom fragment #, double atom fragment #, single ring fragment #, multi-ring fragment # (all per molecule) from molecules generated by the RNN models or from the training data b Five molecules generated by the RNN models or from the training data with 1500 ≤ Molecular Weight ≤ 2000. c Four molecules generated by the RNN models or from the training data with Molecular Weight ≥ 4000</p>
<p>molecules fall into the 1500-2000 range (FIG.1c).</p>
<p>This task was the most challenging for the graph models, both failed to train and were entirely incapable of learning the training data. In particular, JTVAE's tree decomposition algorithm applied to the training data produced a fixed vocabulary of ~11,000 substructures. However both RNN models were able to learn to generate molecules as large and as varied as the training data. The training molecules correspond to very long SMILES and SELFIES string representations, in this case, the SELFIES strings provided an additional advantage and the SELFIES RNN could match the data.</p>
<table>
<thead>
<tr>
<th>Task</th>
<th>Metric</th>
<th>SM-RNN</th>
<th>SF-RNN</th>
<th>JTVAE</th>
<th>CGVAE</th>
</tr>
</thead>
<tbody>
<tr>
<td>LogP</td>
<td>validity</td>
<td>0.941</td>
<td>1.000</td>
<td>1.000</td>
<td>1.000</td>
</tr>
<tr>
<td></td>
<td>uniqueness</td>
<td>0.987</td>
<td>1.000</td>
<td>0.982</td>
<td>1.000</td>
</tr>
<tr>
<td></td>
<td>novelty</td>
<td>0.721</td>
<td>0.871</td>
<td>0.980</td>
<td>1.000</td>
</tr>
<tr>
<td>Multis</td>
<td>valid</td>
<td>0.969</td>
<td>1.000</td>
<td>0.999</td>
<td>0.999</td>
</tr>
<tr>
<td></td>
<td>uniqueness</td>
<td>0.996</td>
<td>0.989</td>
<td>0.998</td>
<td>0.996</td>
</tr>
<tr>
<td></td>
<td>novelty</td>
<td>0.937</td>
<td>0.950</td>
<td>0.998</td>
<td>1.000</td>
</tr>
<tr>
<td>Large</td>
<td>valid</td>
<td>0.876</td>
<td>1.000</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>uniqueness</td>
<td>0.999</td>
<td>0.994</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td></td>
<td>novelty</td>
<td>0.999</td>
<td>0.999</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>TABLE III. Standard Metrics Validity, uniqueness and novelty of molecules generated by all models in every task. Closer to 1.0 indicates better performance.
distribution more closely (FIG. 5a). In particular, learning valid molecules is substantially more difficult with the SMILES grammar, as there are many more characters to generate for these molecules and a higher probability that the model will make a mistake and produce an invalid string. In contrast, the SELFIES string generated will never be invalid. Interestingly, even when the RNN models generated molecules that were out of distribution and substantially smaller than the training molecules- they still had similar substructures and resemblance to the training molecules (5a). In addition, the training molecules seemed to be divided into two modes of molecules with lower and higher LogP values (FIG. 5b): with biomolecules defining the lower mode and molecules with more rings and longer carbons chains defining the higher LogP mode (more example molecules can be seen in supplementary FIG. S7). The RNN models were both able to learn the bi-modal nature of the training distribution.</p>
<p>The training data has a variety of different molecules and substructures, in FIG. 6a the RNN models adequately learn the distribution of substructures arising in the training molecules. Specifically the distribution for the number of: fragments, single and double atom fragments as well as single and multi-rings fragments in each molecule. As the training molecules get larger and occur less, both RNN models still learn to generate these molecules (FIG. 5a when molecular weight $&gt;3000$ ). To demonstrate the size of the molecules being generated a few examples from both RNN models and training data are shown, from the main mode of the distribution (FIG. 6b) and largest training molecules with weight larger than 4000 (FIG. 6c).</p>
<h2>D. Metrics</h2>
<p>We also evaluate models on standard metrics in the literature: validity, uniqueness and novelty. Using the same 10 K molecules generated from each model for each task we compute the following statistics defined in [17] and store them in TABLE. III: 1) validity: the ratio between the number of valid and generated molecules, 2) unique-
ness: the ratio between the number of unique molecules (that are not duplicates) and valid molecules, 3) novelty: the ratio between unique molecules that are not in the training data and the total number of unique molecules. In the first two tasks (TABLE. III), JTVAE and CGVAE have better metrics with very high validity, uniqueness and novelty (all close to 1), here the SMILES and SELFIES RNN perform worse but the SELFIES RNN is close to their performance. The SMILES RNN has the worse metrics due to its poor grammar but is not substantially worse than the other models.</p>
<h2>II. DISCUSSION</h2>
<p>In this work, in effort to test the ability of chemical language models, we introduce three complex modeling tasks for deep generative models of molecules. Language models and graph baselines perform each task, which entails learning to generate molecules from a challenging dataset to learn. The results demonstrate that language models are very powerful, flexible models that can learn a variety of very different complex distributions while the popular graph baseline are much less capable.
SELFIES improves performance, decreases memorization. While the SMILES RNN still achieves impressive performance in all tasks, we notice that the SELFIES representation seems to improve the performance of language models in every task, particularly in the large scale task and consistently yields better standard metrics of validity, uniqueness and novelty. Additionally, the SELFIES grammar may be more difficult to overfit to, which is the likely explanation for why the SELFIES RNN has better standard metrics but worse Wasserstein metrics than the SMILES RNN. The SMILES RNN with lower novelty scores and Wasserstein metrics that are very close to the TRAIN oracle may be memorizing more of the training data than the SELFIES RNN. In future, it would be valuable to ascertain if one should ever use SMILES over SELFIES.
Graph generative models are not flexible The results show that the most popular graph generative models: JTVAE and CGVAE had trouble learning the distributions in each task. For the penalized LogP task, the difference between a molecule that has a score of 2 and one that scores 4 often can be very subtle. Sometimes changing a single carbon or other atom can cause a large drop in score- this likely explains why the CGVAE severely misfit the main training mode. For the multidistribution task, JTVAE and CGVAE's difficulties are clear but very understandable. For JTVAE, it has to learn a wide range of tree types: many of which have no large substructures like rings (the GDB13 molecules) while others are entirely rings (some of the CEP and POLYMERS). For CGVAE, it has to learn a wide range of very different generation traces- which is difficult especially since it only uses one sample trace during learning. For the same reasons, these models were incapable of</p>
<p>training on the largest molecules in PubChem.
Based on the experiments conducted, language models are very powerful generative models for learning any complex molecular distribution and should see even more widespread use. However, it is still possible to see improvements to these models, especially in combating over-fitting as the results did give evidence that language models can suffer from this. In addition, we hope that the molecular modeling tasks and datasets introduced can motivate new generative models that address the difficulties that the graph baselines experienced. Future work will explore how capable chemical language models are in learning larger and larger snapshots of chemical space.</p>
<h2>III. METHODS</h2>
<p>Model details and hyper-parameters: For each task we give the model and training hyper-parameters of the best model found during hyper-parameter optimization, where for all models we used random search with 100 randomly sampled hyper-parameters sets.
Penalized LogP Task. For the SM-RNN we used an LSTM with 2 hidden layer with 400 units and dropout in the last layer with prob $=0.2$ and learning rate of 0.0001 . For the SF-RNN we used an LSTM with 2 hidden layer with 600 units and dropout in the last layer with prob $=0.4$ and learning rate of 0.0002 . The CGVAE used 8 propagation layers and hidden layer side of 100 with kl annealed to 0.1 and a learning rate of 0.0015 . The JTVAE used a learning rate of 0.001 and 3 GNN layers with a hidden size of 356 .</p>
<p>Multi-Distribution Task. For the SM-RNN we used an LSTM with 3 hidden layer with 512 units and dropout in the last layer with prob $=0.5$ and learning rate of 0.0001 . For the SF-RNN we used an LSTM with 2 hidden layer with 500 units and dropout in the last layer
with prob $=0.2$ and learning rate of 0.0003 . The CGVAE used 8 propagation layers and hidden layer side of 100 with kl annealed to 0.1 and a learning rate of 0.001 . The JTVAE used a learning rate of 0.0001 and 3 GNN layers with a hidden size of 356 .</p>
<p>Large-Scale Task. For the SM-RNN we used an LSTM with 2 hidden layer with 512 units and dropout in the last layer with prob $=0.25$ and learning rate of 0.001 . For the SF-RNN we used an LSTM with 2 hidden layer with 800 units and dropout in the last layer with prob $=0.4$ and learning rate of 0.0001 .</p>
<h2>IV. DATA AVAILABILITY</h2>
<p>Training data and generated molecules is available: https://github.com/danielflamshep/genmoltasks.</p>
<h2>V. CODE AVAILABILITY</h2>
<p>The code used to train models is from public repos. JTVAE: https://github.com/wengong-jin/icml18-jtnn. CGVAE: https://github.com/microsoft/constrained-graph-variational-autoencoder.</p>
<p>The RNN models were trained using the char-rnn code from https://github.com/molecularsets/moses. Trained models are available upon request.</p>
<h2>VI. ACKNOWLEDGEMENTS</h2>
<p>A.A.-G. acknowledge funding from Dr. Anders G. Froseth. A.A.-G. also acknowledges support from the Canada 150 Research Chairs Program, the Canada Industrial Research Chair Program, and from Google, Inc. Models were trained using the Canada Computing Systems $[41]$.
[1] R. S. Bohacek, C. McMartin, and W. C. Guida, "The art and practice of structure-based drug design: a molecular modeling perspective," Medicinal research reviews 16, 3 (1996).
[2] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko, et al., "Highly accurate protein structure prediction with alphafold," Nature 596, 583 (2021).
[3] R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams, and A. Aspuru-Guzik, "Automatic chemical design using a data-driven continuous representation of molecules," ACS central science 4, 268 (2018).
[4] I. Sutskever, J. Martens, and G. E. Hinton, "Generating text with recurrent neural networks," International Conference on Machine Learning (2011). .
[5] D. Weininger, "Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules," Journal of chemical information and computer sciences 28, 31 (1988).
[6] M. H. Segler, T. Kogej, C. Tyrchan, and M. P. Waller, "Generating focused molecule libraries for drug discovery with recurrent neural networks," ACS central science 4, 120 (2018).
[7] D. P. Kingma and M. Welling, "Auto-encoding variational bayes," arXiv preprint arXiv:1312.6114 (2013).
[8] Q. Perron, O. Mirguet, H. Tajmouati, A. Skiredj, A. Rojas, A. Gohier, P. Ducrot, M.-P. Bourguignon, P. Sansilvestri-Morel, N. Do Huu, et al., "Deep generative models for ligand-based de novo design applied to multi-parametric optimization," (2021).
[9] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, "Learning deep generative models of graphs," arXiv preprint arXiv:1803.03324 (2018).</p>
<p>[10] Q. Liu, M. Allamanis, M. Brockschmidt, and A. Gaunt, in Advances in Neural Information Processing Systems (2018) pp. 7795-7804.
[11] W. Jin, R. Barzilay, and T. Jaakkola, "Junction tree variational autoencoder for molecular graph generation," arXiv preprint arXiv:1802.04364 (2018).
[12] J. You, B. Liu, Z. Ying, V. Pande, and J. Leskovec, "Graph convolutional policy network for goal-directed molecular graph generation," Advances in Neural Information Processing Systems, 6410 (2018).
[13] A. Seff, W. Zhou, F. Damani, A. Doyle, and R. P. Adams, in Advances in Neural Information Processing Systems.
[14] B. Samanta, D. Abir, G. Jana, P. K. Chattaraj, N. Ganguly, and M. G. Rodriguez, "Nevae: A deep generative model for molecular graphs," AAAI Conference on Artificial Intelligence (2019).,
[15] D. Duvenaud, D. Maclaurin, J. Aguilera-Iparraguirre, R. Gómez-Bombarelli, T. Hirzel, A. Aspuru-Guzik, and R. P. Adams, in Neural Information Processing Systems (2015).
[16] D. Flam-Shepherd, T. C. Wu, P. Friederich, and A. Aspuru-Guzik, "Neural message passing on high order paths," Machine Learning: Science and Technology (2021).
[17] M. Simonovsky and N. Komodakis, in International Conference on Artificial Neural Networks (Springer, 2018) pp. 412-422.
[18] T. Ma, J. Chen, and C. Xiao, in Advances in Neural Information Processing Systems (2018) pp. 7113-7124.
[19] N. De Cao and T. Kipf, "Molgan: An implicit generative model for small molecular graphs," arXiv preprint arXiv:1805.11973 (2018).
[20] D. Flam-Shepherd, T. Wu, and A. Aspuru-Guzik, "Graph deconvolutional generation," arXiv preprint arXiv:2002.07087 (2020).
[21] M. J. Kusner, B. Paige, and J. M. Hernández-Lobato, in International Conference on Machine Learning 2017.
[22] H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, "Syntaxdirected variational autoencoder for structured data," arXiv preprint arXiv:1802.08786 (2018).
[23] N. O’Boyle and A. Dalke, "Deepsmiles: an adaptation of smiles for use in machine-learning of chemical structures," (2018).
[24] M. Krenn, F. Håse, A. Nigam, P. Friederich, and A. Aspuru-Guzik, "Selfies: a robust representation of semantically constrained graphs with an example application in chemistry," arXiv preprint arXiv:1905.13741 (2019).
[25] D. Polykovskiy, A. Zhebrak, B. Sanchez-Lengeling, S. Golovanov, O. Tatanov, S. Belyaev, R. Kurbanov, A. Artamonov, V. Aladinskiy, M. Veselov, et al., "Molecular sets (moses): a benchmarking platform for molecular generation models," Frontiers in pharmacology 11, 1931 (2020).
[26] M. A. Skinnider, R. G. Stacey, D. S. Wishart, and L. J. Foster, "Deep generative models enable navigation in sparsely populated chemical space," (2021).
[27] M. Moret, L. Friedrich, F. Grisoni, D. Merk, and G. Schneider, "Generative molecular design in low data regimes," Nature Machine Intelligence 2, 171 (2020).
[28] G. Landrum, "Rdkit: A software suite for cheminformatics, computational chemistry, and predictive modeling," (2013).
[29] G. R. Bickerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins, "Quantifying the chemical beauty of drugs," Nature chemistry 4, 90 (2012).
[30] P. Ertl and A. Schuffenhauer, "Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions," Journal of cheminformatics 1, 1 (2009).
[31] S. A. Wildman and G. M. Crippen, "Prediction of physicochemical parameters by atomic contributions," Journal of chemical information and computer sciences 39, 868 (1999).
[32] S. H. Bertz, "The first general index of molecular complexity," Journal of the American Chemical Society 103, 3599 (1981).
[33] P. Ertl, S. Roggo, and A. Schuffenhauer, "Natural product-likeness score and its application for prioritization of compound libraries," Journal of chemical information and modeling 48, 68 (2008).
[34] S. Hochreiter and J. Schmidhuber, "Long short-term memory," Neural computation 9, 1735 (1997).
[35] A. K. Ghose and G. M. Crippen, "Atomic physicochemical parameters for three-dimensional structure-directed quantitative structure-activity relationships i. partition coefficients as a measure of hydrophobicity," Journal of computational chemistry 7, 565 (1986).
[36] J. J. Irwin and B. K. Shoichet, "Zinc- a free database of commercially available compounds for virtual screening," Journal of chemical information and modeling 45, 177 (2005).
[37] L. C. Blum and J.-L. Reymond, "970 million druglike small molecules for virtual screening in the chemical universe database gdb-13," Journal of the American Chemical Society 131, 8732 (2009).
[38] J. Hachmann, R. Olivares-Amaya, S. Atahan-Evrenk, C. Amador-Bedolla, R. S. Sánchez-Carrera, A. GoldParker, L. Vogt, A. M. Brockway, and A. Aspuru-Guzik, "The harvard clean energy project: large-scale computational screening and design of organic photovoltaics on the world community grid," The Journal of Physical Chemistry Letters 2, 2241 (2011).
[39] P. C. St. John, C. Phillips, T. W. Kemper, A. N. Wilson, Y. Guan, M. F. Crowley, M. R. Nimlos, and R. E. Larsen, "Message-passing neural networks for high-throughput polymer screening," The Journal of chemical physics 150, 234111 (2019).
[40] S. Kim, P. A. Thiessen, E. E. Bolton, J. Chen, G. Fu, A. Gindulyte, L. Han, J. He, S. He, B. A. Shoemaker, et al., "Pubchem substance and compound databases," Nucleic acids research 44, D1202 (2016).
[41] S. Baldwin, in Journal of Physics: Conference Series, Vol. 341 (IOP Publishing, 2012) p. 012001.</p>
<h1>SUPPLEMENTAL MATERIALS</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" />
e Molecules with more than 10 rings.
FIG. S1. Penalized LogP Task a-e Training molecules with different properties.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<h1>Generated Molecules</h1>
<h2>TABL 1: Penalized LogP Task</h2>
<p>Molecules generated from each model.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>FIG. S2. Penalized LogP Task a-b Generated molecules with penalized LogP &lt; 4 from the graph generative models.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Generated molecules</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">TRAIN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; \alpha_{1} \ &amp; \alpha_{2} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 2,2 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,20 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,20 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,10 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \alpha_{1} \ &amp; \alpha_{2} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \alpha_{1} \ &amp; \alpha_{2} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \alpha_{2} \ &amp; \alpha_{3} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; \alpha_{1} \ &amp; \alpha_{2} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,10 \ &amp; 0 \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\begin{aligned} &amp; g^{h_{1} h_{2}} \ &amp; g^{h_{3} h_{4}} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,0 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,0 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,0 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; g^{h_{1} h_{2}} \ &amp; g^{h_{3} h_{4}} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; g^{h_{1} h_{2}} \ &amp; g^{h_{3} h_{4}} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,0 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; 0,0 \ &amp; 0 \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; g^{h_{1} h_{3}} \ &amp; g^{h_{2} h_{4}} \end{aligned}$</td>
<td style="text-align: center;">$\begin{aligned} &amp; g^{h_{2} h_{3}} \ &amp; g^{h_{3} h_{4}} \end{aligned}$</td>
</tr>
<tr>
<td style="text-align: center;">CGVAE</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\alpha_{1}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}$</td>
<td style="text-align: center;">$\infty^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{3}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1}^{h_{1} h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}-h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}-h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{3}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1} \quad \alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}-h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{3}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty_{1}$</td>
<td style="text-align: center;">$\infty_{1}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2}$</td>
<td style="text-align: center;">$\infty_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\infty_{2}$</td>
<td style="text-align: center;">$\infty_{2}$</td>
<td style="text-align: center;">$\infty_{2}$</td>
<td style="text-align: center;">$\alpha_{2}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{3}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}-h_{1} h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}-h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{3}}$</td>
</tr>
<tr>
<td style="text-align: center;">SF-RNN</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{1} \alpha_{1}$</td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{2} \alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2} \alpha_{2}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{3}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{2} \alpha_{1}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty^{h_{1}}$</td>
<td style="text-align: center;">$\infty^{h_{1}}$</td>
<td style="text-align: center;">$\infty^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{1}^{h_{2}}$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{2} \alpha_{1}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty^{h_{2}}$</td>
<td style="text-align: center;">$\infty^{h_{2}}$</td>
<td style="text-align: center;">$\infty^{h_{2}}$</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty$</td>
<td style="text-align: center;">$\infty^{h_{2}}$</td>
<td style="text-align: center;">$\infty^{h_{2}}$</td>
<td style="text-align: center;">$\infty^{h_{2}}$</td>
<td style="text-align: center;">$\infty$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\alpha_{1} \alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{1}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{2}}$</td>
<td style="text-align: center;">$\alpha_{2}^{h_{3}}$</td>
</tr>
</tbody>
</table>
<p>TABLE II. Multi-distribution Task Model generated molecules. Each sub-row is from a specific molecular mode.</p>
<p><img alt="img-10.jpeg" src="img-10.jpeg" />
<img alt="img-11.jpeg" src="img-11.jpeg" />
<img alt="img-12.jpeg" src="img-12.jpeg" />
<img alt="img-13.jpeg" src="img-13.jpeg" />
<img alt="img-14.jpeg" src="img-14.jpeg" />
<img alt="img-15.jpeg" src="img-15.jpeg" />
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>FIG. S3. Large Scale Task Training molecules.</p>
<p><img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>FIG. S4. Large Scale Task Generated molecules from the SMILES RNN.</p>
<p><img alt="img-18.jpeg" src="img-18.jpeg" />
<img alt="img-19.jpeg" src="img-19.jpeg" />
<img alt="img-20.jpeg" src="img-20.jpeg" />
<img alt="img-21.jpeg" src="img-21.jpeg" />
<img alt="img-22.jpeg" src="img-22.jpeg" />
<img alt="img-23.jpeg" src="img-23.jpeg" />
<img alt="img-24.jpeg" src="img-24.jpeg" />
<img alt="img-25.jpeg" src="img-25.jpeg" /></p>
<p>FIG. S5. Large Scale Task Generated molecules from the SELFIES RNN.</p>
<p><img alt="img-26.jpeg" src="img-26.jpeg" />
b Molecules from the SMILES RNN.
FIG. S6. Large Scale Task a-b Generated molecules with less than 100 heavy atoms from the RNN models.</p>
<p><img alt="img-27.jpeg" src="img-27.jpeg" />
a Molecules from the mode with lower LogP values.
<img alt="img-28.jpeg" src="img-28.jpeg" />
b Molecules from the mode with higher LogP values.</p>
<p>FIG. S7. Large Scale Task a-b Training molecules from each LogP mode.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>danielfs@cs.toronto.edu
$\dagger$ alan@aspuru.com</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-587 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-587</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-587</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-40cc7cdffa0a861cb557410518246d97d1678642</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/40cc7cdffa0a861cb557410518246d97d1678642" target="_blank">Benchmarking Reinforcement Learning Algorithms on Real-World Robots</a></p>
                <p><strong>Paper Venue:</strong> Conference on Robot Learning</p>
                <p><strong>Paper TL;DR:</strong> This work introduces several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability and test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyzes sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks.</p>
                <p><strong>Paper Abstract:</strong> Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning.</p>
                <p><strong>Cost:</strong> 0.005</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-587",
    "paper_id": "paper-40cc7cdffa0a861cb557410518246d97d1678642",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.005031,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Benchmarking Reinforcement Learning Algorithms on Real-World Robots</h1>
<p>A. Rupam Mahmood<br>rupam@kindred.ai<br>Dmytro Korenkevych<br>dmytro.korenkevych@kindred.ai<br>Gautham Vasan<br>gautham.vasan@kindred.ai<br>William Ma<br>william.ma@kindred.ai<br>James Bergstra<br>james@kindred.ai</p>
<h4>Abstract</h4>
<p>Through many recent successes in simulation, model-free reinforcement learning has emerged as a promising approach to solving continuous control robotic tasks. The research community is now able to reproduce, analyze and build quickly on these results due to open source implementations of learning algorithms and simulated benchmark tasks. To carry forward these successes to real-world applications, it is crucial to withhold utilizing the unique advantages of simulations that do not transfer to the real world and experiment directly with physical robots. However, reinforcement learning research with physical robots faces substantial resistance due to the lack of benchmark tasks and supporting source code. In this work, we introduce several reinforcement learning tasks with multiple commercially available robots that present varying levels of learning difficulty, setup, and repeatability. On these tasks, we test the learning performance of off-the-shelf implementations of four reinforcement learning algorithms and analyze sensitivity to their hyper-parameters to determine their readiness for applications in various real-world tasks. Our results show that with a careful setup of the task interface and computations, some of these implementations can be readily applicable to physical robots. We find that state-of-the-art learning algorithms are highly sensitive to their hyper-parameters and their relative ordering does not transfer across tasks, indicating the necessity of re-tuning them for each task for best performance. On the other hand, the best hyper-parameter configuration from one task may often result in effective learning on held-out tasks even with different robots, providing a reasonable default. We make the benchmark tasks publicly available to enhance reproducibility in real-world reinforcement learning ${ }^{1}$.</p>
<p>Keywords: CORL, Robots, Reinforcement learning, Benchmarking</p>
<h2>1 Introduction</h2>
<p>In recent years, policy learning algorithms such as trust region policy optimization (TRPO, Schulman et al. 2015), proximal policy optimization (PPO, Schulman et al. 2016), and deep deterministic policy gradient (DDPG, Lillicrap et al. 2015) methods have gained popularity due to their success in various simulated robotic tasks (Duan et al. 2016). A large body of works has been built on these algorithms to address different challenges in reinforcement learning including policy learning (Haarnoja et al. 2017), hierarchical learning (KISSAROV et al. 2017), transfer learning (Wulfmeier, Posner \&amp; Abbeel 2017), and emergence of complex behavior (Heess et al. 2017). Deep learning software such as Theano and Tensorflow as well as the availability of source code of learning algorithms (e.g., Duan et al. 2016, Dhariwal et al. 2017) and benchmark simulated environments (e.g., Brockman et al. 2016, Machado et al. 2018, Tassa et al. 2018) contributed to this advancement.</p>
<p>It is natural to expect that successes in simulations would inspire similar engagement within the reinforcement learning community toward policy learning with physical robots. But this engage-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The robots used in this work: (left) Universal Robotics UR5 collaborative arms, (middle) Robotis MX-64AT Dynamixel actuators, and (right) iRobot Create2 mobile robot bases.
ment so far has been limited. Some notable works show success when the learning algorithms are supported with one or more of (a) sufficient state information or auxiliary task-specific steps and knowledge (e.g. Levine et al. 2016, Riedmiller et al. 2018), (b) preparation in simulation (e.g. Rusu et al. 2017) (c) collaborative learning (e.g. Yahya et al. 2017), and (d) learning from demonstrations (e.g. Hester et al. 2017). However, reinforcement learning research with real-world robots is yet to fully embrace and engage the purest and simplest form of the reinforcement learning problem statement-an agent maximizing its rewards by learning from its first-hand experience of the world. This lack of engagement indicates the difficulties in carrying forward the successes and enthusiasm found in simulation-based works to the real world. Due to the lack of benchmark tasks, it is hard to analyze these difficulties and address them as a community.</p>
<p>Mahmood et al. (2018) recently brought to attention some of the difficulties of real-world robot learning and showed that learning performance can be highly sensitive to different elements of the task setup such as the action space, the action cycle time defined by the time between two subsequent actions, and system delays (also see Riedmiller 2012). Therefore, reproducing and utilizing existing results can be hard when the details of these task setup elements are omitted. Moreover, without careful task setups, learning with physical robots can be insurmountably difficult.</p>
<p>To study and alleviate these difficulties, we introduce six reinforcement learning tasks based on three commercially available robots. Most of these tasks require no additional hardware installation apart from the basic robot setup. On these tasks, we compare and benchmark four reinforcement learning algorithms for continuous control: TRPO, PPO, DDPG, and Soft Q-learning (Haarnoja et al. 2017). The main contributions of this work are 1) introducing benchmark tasks for physical robots to share across the community, 2) setting up the tasks to be conducive to learning, and 3) providing the first extensive empirical study of multiple policy learning algorithms on multiple physical robots.</p>
<h1>2 Robots</h1>
<p>We use three commercially available robots (see Figure 1) as a basis for defining learning tasks.
UR5: The UR5, shown in Figure 1 (left) is a collaborative industrial arm with six joints produced by Universal Robots. The sensory packets from UR5 include angles, velocities, target accelerations, and currents for each joint. The control interface offers low-level position and velocity control commands. We use UR5 to develop two tasks called UR-Reacher-2 and UR-Reacher-6 based on the tasks developed by Mahmood et al. (2018).</p>
<p>Dynamixel MX-64AT: The Dynamixel (DXL) series of programmable Direct-Current actuators, manufactured by Robotis, are popular for custom robots ranging from robot arms to humanoids. We use single DXL MX-64AT actuators, shown in Figure 1 (middle), which complies with high torque and load requirements. The MX series actuators are controlled by digital packets via a half duplex asynchronous serial communication protocol, that is, we can read and write to the motor but not simultaneously. The protocol allows a control computer to send position, velocity or current control commands (referred to as torque control in the manual) to the actuator as well as poll sensorimotor information including position, velocity, temperature, current and load. We develop two tasks based on this actuator, which we call DXL-Reacher and DXL-Tracker.</p>
<p>Create 2: The Create 2, shown in Figure 1 (right), is a hobbyist version of iRobot’s Roomba vacuum robot. The Create 2 has two actuated wheels and many sensors including six front-facing infrared wall sensors, charging sensor, one omni-directional and two directional infrared sensors for docking, two physical bump sensors, and a directed distance sensor for the forward direction. The software interface allows the control computer to access sensory packets in a streaming mode as well as send to the robot target speeds (mm/s) for its two wheels. We develop two tasks with it, called CreateMover and Create-Docker.</p>
<p>Appendix A.1 contains additional details of the hardware setups.</p>
<h1>3 Tasks</h1>
<p>In a reinforcement learning (RL) task (Sutton \&amp; Barto 1998), an agent interacts with its environment at discrete time steps, where at each step $t$, the environment provides the agent its state information $S_{t} \in \mathcal{S}$ and a scalar reward signal $R_{t} \in \mathbb{R}$. The agent uses a stochastic policy $\pi$ with a probability distribution $\pi(a \mid s) \stackrel{\text { def }}{=} \operatorname{Pr}\left{A_{t}=a \mid S_{t}=s\right}$ to select an action $A_{t} \in \mathcal{A}$. In response, the environment transitions to a new state $S_{t+1}$ and produces a new reward $R_{t+1}$ at the next time step $t+1$ using a transition probability distribution: $p\left(s^{\prime}, r \mid s, a\right) \stackrel{\text { def }}{=} \operatorname{Pr}\left{S_{t+1}=s^{\prime}, R_{t+1}=r \mid S_{t}=s, A_{t}=a\right}$. The goal of the agent is typically to find a policy that maximizes the expected return defined as the future accumulated rewards $G_{t} \stackrel{\text { def }}{=} \sum_{k=t}^{\infty} \gamma^{k-t} R_{k+1}$, where $\gamma \in[0,1]$ is a discount factor. In practice, the agent observes the environment's state partially through a real-valued observation vector $\mathbf{o}<em t="t">{t}$ instead of receiving the state information fully.
UR-Reacher-2: We use the Reacher task with UR5 developed by Mahmood et al. (2018), which is designed analogously to OpenAI-Gym Reacher (Brockman et al. 2016). We modify the reward function and call the task UR-Reacher-2. In Gym Reacher, the agent's objective is to reach arbitrary target positions by exercising low-level control over a two-joint robotic arm. In UR-Reacher-2, we actuate the second and the third joints from the base by sending angular speeds between $[-0.3,+0.3]$ rad/s. The observation vector consists of joint angles, joint velocities, the previous action, and the vector difference between the target and the fingertip coordinates. The reward function is defined as: $R</em>$ is the Euclidean distance between the target and the fingertip positions. The second term of the reward function, which we call the precision reward, incentivizes the algorithm to learn to get to the target with a high precision. We defined episodes to be 4 seconds long to allow adequate exploration. At each episode, the target position is generated randomly within a $0.7 m \times 0.5 m$ boundary, while the arm always starts from the middle of the boundary.
UR-Reacher-6: The second task with UR5, which we call UR-Reacher-6, is analogous to UR-Reacher-2 with the exceptions that all six joints are actuated and the target is drawn from a $0.7 m \times$ $0.5 m \times 0.4 m$ 3D space. The higher dimensionality of the action and observation spaces and physical limitations of reaching locations from various configurations of the arm joints in 3D space result in a much more complex policy space and substantially increase the learning problem difficulty.
DXL-Reacher: We design a Reacher task similar to UR-Reacher-2 with current control of the DXL actuator, which we call DXL-Reacher. The action space is one-dimensional current control signals between $[-100,100] \mathrm{mA}$, making the task simpler than UR-Reacher-2. The reward function is defined as: $R_{t}=-d_{t}$. The observation vector includes the actuator position (in radians), moving speed, target position, and the previous action. Each episode is 2 seconds long to allow adequate time for reaching distant target positions. At each episode, the target position is chosen randomly within a certain boundary of angular positions, and the actuator starts from the center of it.
DXL-Tracker: We develop a second task using the DXL actuator, which we call DXL-Tracker. The objective here is to precisely track a moving target position with current control signals between $[-50,50] \mathrm{mA}$. The observation vector includes the actuator position (in radians), moving speed, current target position, target position from 50 milliseconds in the past and the previous action. The reward function is same as that of DXL-Reacher. Each episode is 4 seconds long to allow adequate time to catch up with the target and subsequently track it. At each episode, the starting position of the target is chosen uniformly randomly from a certain range, and the actuator starts from the center of that range. In addition, we also randomly choose the direction of the moving target. The speed of the target is set in such a way that the target always arrives at a certain fixed position at the end of the episode. Thus, the speed is different for different episodes.}=-d_{t}+\exp \left(-100 d_{t}^{2}\right)$, where $d_{t</p>
<p>Create-Mover: We develop a task with Create 2 where the agent needs to move the robot forward as fast as possible within an enclosed arena. We call it Create-Mover. A $3 f t \times 2.5 f t$ arena is built using white shelving boards for the walls and a white hardboard for the floor. The action space is $[-150 \mathrm{~mm} / \mathrm{s}, 150 \mathrm{~mm} / \mathrm{s}]^{2}$ for actuating the two wheels with speed control. The observation vector is composed of 6 wall-sensors values and the previous action. For the wall sensors, we always take the latest values received within the action cycle and use Equation 1 by (Benet et al. 2002) to convert the incoming signals to approximate distances. The reward function is the summation of the directed distance values over 10 most recent sensory packets. An episode is 90 seconds long but ends earlier if the agent triggers one of its bump sensors. When an episode terminates, the position of the robot is reset by moving backward to avoid bumping into the wall immediately. We use two Create 2 robots and two identical arenas for our experiments. Among the two robots, one of them has two faulty wall sensors always receiving value zero, with four other sensors oriented symmetrically. To make comparisons fair, each algorithm was run using both robots the same number of times.</p>
<p>Create-Docker: In this task the objective is to dock to a charging station attached to the middle of one of the wider walls of the Create-Mover arena. The reward function is a large positive number for successful docking with penalty for bumping and encouragement for moving forward and facing the charging station perpendicularly. More details of the task is provided in Appendix A.2.</p>
<p>All of these tasks are implemented following the computational model for real-time reinforcement learning tasks described by Mahmood et al. (2018). We improve on that model by running environment and agent computations on two different processes, which we found to reduce execution delays compared to the use of Python threads. The action cycle time is 150 ms for Create-Mover, 45 ms for Create-Docker and 40 ms for the rest of the tasks. The robot read-write cycle time is set to 8 ms for UR5 tasks, 10 ms for DXL tasks and 15 ms for Create tasks. The reward is scaled by the action cycle time in all cases. The action space is normalized between -1 and +1 for each dimension. For the Create tasks, all the observations are also normalized between -1 and +1 .</p>
<h1>4 Reinforcement learning algorithms</h1>
<p>We select four continuous control policy learning algorithms. TRPO, PPO and DDPG are among the most popular of their kind. On the other hand, Soft-Q is a new algorithm with promising results. We use the OpenAI Baselines implementations for TRPO, and PPO, Rllab implementation for DDPG, and the implementation of Soft Q-learning by the original authors.</p>
<p>Trust region policy optimization (TRPO): TRPO (Schulman et al. 2015) is a policy optimization algorithm that constrain the change in the policy at each learning update. The policy is optimized by iteratively solving the following constrained optimization problem:</p>
<p>$$
\underset{\theta}{\operatorname{maximize}} \quad \mathbb{E}<em _old="{old" _theta__text="\theta_{\text">{s, a \sim \pi</em>}}}}\left[r_{\theta}(a \mid s) A_{\theta_{\text {old }}}(s, a)\right], \quad \text { s.t. } \quad \mathbb{E<em _old="{old" _theta__text="\theta_{\text">{s, a \sim \pi</em>(\cdot \mid s)\right)\right] \leq \delta
$$}}}}\left[D_{\mathrm{KL}}\left(\pi_{\theta}(\cdot \mid s) | \pi_{\theta_{\text {old }}</p>
<p>where $A_{\theta_{\text {old }}}$ is the advantage function, $r_{\theta}(a \mid s)=\frac{\pi_{\theta}(a \mid s)}{\pi_{\theta_{\text {old }}}(a \mid s)}$ is the ratio of a target policy probability to the policy probability used to generate data, $D_{\mathrm{KL}}$ is a Kullback-Leibler divergence, and $\delta$ is a "step-size" parameter. TRPO uses the conjugate-gradient algorithm to solve the above problem.
Proximal policy optimization (PPO): PPO (Schulman et al. 2016) attempts to control the policy change during learning updates by replacing the KL-divergence constraint of TPRO in the optimization problem with a penalty term realized by a clipping in the objective function:</p>
<p>$$
L_{\theta}^{C L I P}=\mathbb{E}<em _old="{old" _theta__text="\theta_{\text">{s, a \sim \pi</em>(s, a)\right)\right]
$$}}}}\left[\min \left(r_{\theta}(a \mid s) A_{\theta_{\text {old }}}(a, s), \operatorname{clip}\left(r_{\theta}(a \mid s), 1-\varepsilon, 1+\varepsilon\right) A_{\theta_{\text {old }}</p>
<p>where $A_{\theta_{\text {old }}}$ is the advantage function, and $\varepsilon$ is a parameter, usually on the order of 0.1 . The optimization is done by running several epochs of stochastic gradient ascent at each update.
Soft Q-learning (Soft-Q): Soft-Q (Haarnoja et al. 2017) defines a policy as an energy based probability distribution: $\pi(a \mid s) \propto \exp (-\mathcal{E}(a, s))$, where the energy function $\mathcal{E}$ corresponds to a "soft" action-value function, obtained by optimizing the maximum entropy objective. The soft action-value function is represented by deep neural networks, and therefore the policy energy model can represent complex multi-modal behaviors. This model provides natural exploration mechanism without the need to introduce artificial sources of exploration noise, such as additive Gaussian noise.</p>
<p>Deep deterministic policy gradient (DDPG): DDPG (Lillicrap et al. 2015) learns a deterministic policy that maximizes the estimated action-value function by following a policy gradient:</p>
<p>$$
\nabla_{\theta} L_{\theta}=\frac{1}{N} \sum_{i} \nabla_{a} Q_{\phi}\left(s_{i}, a\right)\left|{ }<em _theta="\theta">{a=\mu</em>\right)
$$}\left(s_{i}\right.}\right) \nabla_{\theta} \mu_{\theta}\left(s_{i</p>
<p>where $N$ is the batch size. In the Rllab implementation, the exploration of DDPG is addressed by augmenting the policy output $\mu_{\theta}(s)$ with additive noise from an independent noise model.</p>
<h1>5 Experiment Protocol</h1>
<p>We run the four learning algorithms on the six robotic tasks to investigate different characteristics such as hyper-parameter and network initialization sensitivities within tasks, hyper-parameter consistency across tasks, and overall learning effectiveness of the algorithms in all tasks.</p>
<p>To analyze the hyper-parameter sensitivity within tasks and consistency across tasks, we perform a random search (Bergstra \&amp; Bengio 2012) of seven hyper-parameters of each algorithm on UR-Reacher-2 and DXL-Reacher. For each of these hyper-parameters, we predetermine the range of values to search and draw 30 independent hyper-parameter configurations from that range uniformly randomly in the logarithmic scale. The ranges of hyper-parameter values we use in this experiment are given in Appendix A.3. Each of these hyper-parameter configurations is used to run experiments using a different random initialization of neural networks. Each algorithm is run using the same set of hyper-parameter configurations on both tasks.</p>
<p>To know the statistical significance of the comparative performance of each hyper-parameter configuration, we need to run each of them with different randomization seeds that will determine network initialization, random target positions, and random action selections. Instead, we run each hyperparameter configuration of the random search with a single randomly drawn network initialization. To determine the effect of the network initialization, we redraw four hyper-parameter configurations uniformly randomly from our original 30 sets. Each of these four sets was rerun with five randomly drawn network initializations. We use the same five network initializations for all four sets of chosen hyper-parameter values on both tasks.</p>
<p>To analyze the overall effectiveness of the algorithms across tasks, we choose the best-performing hyper-parameter configurations of each algorithm from UR-Reacher-2 and use them to run experiments on the four held-out tasks: UR-Reacher-6, DXL-Tracker, Create-Mover, and Create-Docker. To understand the qualitative performance of learned policies, we also run some non-learning scripted agents and compute their average returns using the same experimental setup we use for the learning agents. These scripted agents are described in Appendix A.4.</p>
<p>Each run is 150,000 steps long or about 3 hours of wall time for UR-Reacher-2, 200,000 steps long or about 4 hours of wall time for UR-Reacher-6, 50,000 steps long or about 45 minutes of wall time for DXL-Reacher, 150,000 steps long or about 2 hours 15 minutes of wall time for DXL-Tracker, 40,000 steps long or about 2 hours of wall time for Create-Mover, and 300,000 steps long for CreateDocker. All wall times include resets. The resets of Create-Docker is dependent on performance.</p>
<h2>6 Experimental results and discussion</h2>
<p>First, we demonstrate the reliability of the experiments by repeating some of them multiple times with the same randomization seed using TRPO. Details are given in Appendix A.5. Figure 2 shows the results for all tasks except Create-Docker. The variation in performance between different runs was small and did not diverge over time except on Create-Mover, where the sequences of experience became dissimilar over time across runs. These results are a testament to the tight control over system delays achieved in our tasks by using the computational model of Mahmood et al. (2018).</p>
<p>To illustrate the sensitivity of each algorithm's performance to their hyper-parameters, we show in Figure 3 the performance of all algorithms on both UR-Reacher-2 (left) and DXL-Reacher (right) using Tukey's box plots (Tukey 1977) based on all 30 configurations. The performance of each configuration was measured by averaging all episodic returns obtained throughout the learning period during its run. For each algorithm, performance varied widely with different hyper-parameter configurations, ranging from learning visually-confirmed effective behavior to no learning at all. No learning occurred for several different hyper-parameter configurations. The performance of DDPG was</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Repeatability of learning on five robotic tasks: The plots show the returns over time of multiple learning experiments that would be identical for the same color if they had been run in simulation. The robot hardware introduces some non-determinism, but not enough to significantly impact repeatability in the natural ups and downs of exploration and learning, except in CreateMover, where the physical location of the robot can diverge over time across the runs.
the worst, having the least median performance on both tasks. The rest of the algorithms achieved good performance with many configurations on both tasks. Among them, TRPO's performance was the least sensitive to hyper-parameter variations with the smallest interquartile range on both tasks.</p>
<p>Overall, these results show that hyper-parameter choices are important, as they may make a much bigger difference than the choice of the algorithm. Blue crosshairs in Figure 4 show the performance for each of the 30 hyper-parameter configurations of all four algorithms in the descending order, also shown in Appendix A. 6 for easier comparison. The values of all configurations, distributions of individual hyper-parameters and their correlations with performance are given in Appendix A.7.</p>
<p>The box plots in Figure 4 show the effect of variations in network initialization with four randomly chosen hyper-parameter configurations. Except in one case of DDPG, the interquartile ranges of performance due to variations in network initializations were smaller than those due to variations in hyper-parameter choices shown in Figure 3. Except for TRPO on UR-Reacher-2, DDPG on DXLReacher and Soft-Q on both tasks, the medians of performance also retained the relative rank order of the configurations in the original experiment with single network initializations.</p>
<p>We show in Figure 5 how each hyper-parameter configuration ranks on both tasks according to average returns. Each plot corresponds to an algorithm and contains two columns of colored dots, each of which represents one of the 30 randomly chosen hyper-parameter configurations. The gray lines connect identical hyper-parameter configurations on both tasks. The correlations of performance between the tasks with corresponding p-values are given above the plots. The correlations were positive for all algorithms, and significantly large for PPO, Soft-Q and DDPG. This result indicates that although hyper-parameter optimization is likely necessary for best performance on a new task, a good configuration based on one task can still provide a good baseline performance for another.</p>
<p>Figure 6 shows the means of multiple learning curves together with their standard errors for different learning algorithms on different tasks, except the bottom right plot, which shows two independent
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The effect of hyper-parameter choices in two robotic tasks: The plot illustrates the variation in performance due to hyper-parameter choices using boxplots based on 30 randomly drawn hyper-parameter configurations of each algorithm on UR-Reacher-2 and DXL-Reacher tasks. Hyperparameter choices had a large impact on the quality of learned policies.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The effect of random network initializations on DXL-Reacher: Each blue cross represents a single run with a unique hyper-parameter choice. They are sorted by performance. To quantify the influence of network initialization, we re-ran some of the experiments but varied the initialization. The overall effect of network initialization, shown in Tukey's box plots for four randomly chosen hyper-parameter values, was smaller than that of the choice of the hyper-parameters.
learning curves of TRPO on Create-Docker. The average returns and standard errors of scripted agents are also shown for each task. For the mean curves, we used four independent runs of each algorithm on Create-Mover, and five runs on the rest. DDPG performed poorly on all UR5 and DXL tasks. We did not run it on the two Create tasks. The rest of the algorithms showed learning improvements on all tasks they were run. Among these algorithms, the final performance of TRPO was never substantially worse compared to the best in each task. Soft-Q had the fastest learning rate on all UR5 and DXL tasks. On Create-Mover, TRPO, PPO and Soft-Q learned an effective forwardmoving behavior of the robot, which turned as it approached the wall, as shown in the companion video $^{2}$. On Create-Docker, TRPO learned to dock successfully quite often although the movement was not smooth. Overall, RL solutions were outperformed by scripted solutions, by a large margin in some tasks, where such solutions were well established or easy to script. But in Create-Docker where a scripted solution is not obvious or easy, RL solutions appeared more competitive.</p>
<p>Working with real-world systems did create some challenges. Soft-Q on DXL tasks for many of its hyper-parameter configurations resulted in frequent overheating and failed during overnight experiments due to more-aggressive exploration. We could not run un-attended experiments with Create 2 when the robots were tethered to stationary external computers as their cables needed periodic untangling. We were able to overcome this problem by using an on-board external computer, which we used for one of the two Create-Docker runs. Two wall sensors of one of the Create 2s were faulty; surprisingly, learning performance did not appear to be affected, possibly due to four other symmetrically oriented wall sensors being available. Of all three robots, we were most pleased with our experience with UR5, which we were able to use constantly for days without interventions.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Hyper-parameter consistency between two robotic tasks: Each blue dot represents a single experiment on DXL-Reacher and each orange dot represents an experiment on UR-Reacher2. The gray lines connect experiments with identical hyperparameter choices. Although the orderings of hyper-parameter configurations by performance were not consistent, all four algorithms had hyper-parameter consistency between the tasks, evident by the correlation of performance shown above each plot, which varied from weakly positive to moderately positive relationships.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Learning performance of different algorithms on all six tasks: We used the best hyper-parameters based on UR-Reacher-2. TRPO, PPO and Soft-Q learned effectively, with Soft-Q being the fastest learner and TRPO achieving near-best final learning performance in all tasks.</p>
<p>Our experiments also revealed some limitations of the learning algorithms and their implementations, such as the sequential computations of the agent's learning updates and forward policy passes. Learning updates of these algorithms are expensive, and our choice of moderately large action cycle times minimized the number of samples affected by these sequential learning updates. However, to learn finer policies with faster action cycle times, moving toward efficient ordering of computations (Travnik et al. 2018, Mahmood et al. 2018) together with inexpensive incremental updates (Mahmood 2017) or asynchronous off-policy updates (Gu et al. 2017) would be essential.</p>
<h1>7 Conclusions</h1>
<p>In this work, we provided the first extensive experimental study of multiple policy learning algorithms, namely TRPO, PPO, DDPG, and Soft-Q on multiple commercially-available physical robots. We found that the performance of all algorithms was highly sensitive to their hyper-parameter values, requiring retuning on new tasks for the best performance. Nevertheless, some algorithms achieved effective learning performance across tasks for a wide range of hyper-parameter values. This effectiveness indicates the reliability of our task setups as well as the applicability of these algorithms and implementations in diverse physical environments. Benchmarking more learning algorithms on these tasks as well as upgrading the existing algorithms to allow higher sample efficiency and faster action cycle times are promising directions for future work.</p>
<p>We ran more than 450 independent experiments which took over 950 hours of robot usage in total. Most of the experiments were highly repeatable, and many of them resulted in effective learning performance. This study strongly indicates the viability of reinforcement learning research extensively based on real-world experiments, which is essential to understand the difficulties of learning with physical robots and mitigate them to achieve fast and reliable learning performance in dynamic environments. The benchmark tasks and the supporting source code enable the necessary steps for such understanding and easy adoption of physical robots in reinforcement learning research.</p>
<h2>Acknowledgements</h2>
<p>We thank Colin Cooke, Francois Hogan, and Daniel Snider for valuable discussion, and Yifei Cheng and Scott Purdy for helping us build the arena for Create 2. Colin Cooke also helped us with the on-board computer setup for one of the two Create-Docker runs.</p>
<h1>References</h1>
<p>Benet, G., Blanes, F., Simó, J. E., Pérez, P. (2002). Using infrared sensors for distance measurement in mobile robots. Robotics and autonomous systems 40(4), 255-266.
Bergstra, J., Bengio, Y. (2012). Random search for hyper-parameter optimization. Journal of Machine Learning Research, pp: 281-305.
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., Zaremba, W. (2016). OpenAI Gym. arXiv preprint arXiv:1606.01540.
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A., Schulman, J., Sidor, S., Wu, Y. (2017). OpenAI Baselines, https://github.com/openai/baselines.
Duan, Y., Chen, X., Houthooft, R., Schulman, J., Abbeel, P. (2016). Benchmarking deep reinforcement learning for continuous control. In Proceedings of the 33rd International Conference on Machine Learning, pp: 1329-1338.
Gu, S., Holly, E., Lillicrap, T., Levine, S. (2017). Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In IEEE International Conference on Robotics and Automation, pp:3389-3396.
Haarnoja, T., Tang, H., Abbeel, P., Levine, S. (2017). Reinforcement learning with deep energybased policies. arXiv preprint arXiv:1702.08165.
Heess, N., Sriram, S., Lemmon, J., Merel, J., Wayne, G., Tassa, Y., Silver, D. (2017). Emergence of locomotion behaviours in rich environments. arXiv preprint arXiv:1707.02286.
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Dulac-Arnold, G. and Osband, I., (2017). Deep Q-learning from demonstrations. arXiv preprint arXiv:1704.03732.
Klissarov, M., Bacon, P. L., Harb, J., Precup, D. (2017). Learnings Options End-to-End for Continuous Action Tasks. arXiv preprint arXiv:1712.00004.
Levine, S., Finn, C., Darrell, T., Abbeel, P. (2016). End-to-end training of deep visuomotor policies. The Journal of Machine Learning Research 17(1): 1334-1373.
Lillicrap, T.P., Hunt, J.J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D. and Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971.</p>
<p>Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., Bowling, M. (2018). Revisiting the arcade learning environment: Evaluation protocols and ppen problems for general agents. Journal of Artificial Intelligence Research 61: 523-562.
Mahmood, A. R. (2017). Incremental Off-policy Reinforcement Learning Algorithms. PhD thesis, Department of Computing Science, University of Alberta, Edmonton, AB T6G 2E8.
Mahmood, A. R., Korenkevych, D., Komer, B. J., Bergstra, J. (2018). Setting up a Reinforcement Learning Task with a Real-World Robot. arXiv preprint arXiv:1803.07067.
Riedmiller, M. (2012). 10 steps and some tricks to set up neural reinforcement controllers. In Neural networks: tricks of the trade, pp. 735-757. Springer, Berlin, Heidelberg.
Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Van de Wiele, T., Mnih, V., Heess, N. and Springenberg, J. T., (2018). Learning by Playing-Solving Sparse Reward Tasks from Scratch. arXiv preprint arXiv:1802.10567.
Rusu, A. A., Večerík, M., Rothörl, T., Heess, N., Pascanu, R., Hadsell, R. (2017). Sim-to-Real robot learning from pixels with progressive nets. In Proceedings of the 2nd Conference on Robot Learning, pp: 262-270.
Schulman, J., Levine, S., Abbeel, P., Jordan, M., Moritz, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning, pp:1889-1897.</p>
<p>Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347.
Sutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.
Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas, D., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T., and Riedmiller, M. (2018). DeepMind control suite. arXiv preprint arXiv:1801.00690.
Travnik, J. B., Mathewson, K. W., Sutton, R. S., Pilarski, P. M. (2018). Reactive Reinforcement Learning in Asynchronous Environments. arXiv preprint arXiv:1802.06139.
Tukey, J. W. (1977). Box-and-whisker plots. Exploratory data analysis, pp: 39-43.
Wulfmeier, M., Posner, I., Abbeel, P. (2017). Mutual alignment transfer learning. arXiv preprint arXiv:1707.07907.
Yahya, A., Li, A., Kalakrishnan, M., Chebotar, Y., Levine, S. (2017). Collective robot reinforcement learning with distributed asynchronous guided policy search. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp: 79-86.</p>
<h1>Appendix</h1>
<h2>A. 1 Additional details of the robots</h2>
<p>Here, we provide additional details on the hardware setup between the robots and the control computers. All of our setups use wired connections. The UR5 arm controller is communicated with the control computer over a TCP/IP connection. We use an Xevelabs USB2AX controller to interface between the MX-64AT actuators and a control computer via USB. A 12V, 5A DC power adapter is used to power the actuators. The Create 2 robot is interfaced with a control computer via serial port using iRobot's specified Open Interface. The robot is communicated in the streaming mode where the internal controller streams a data packet every 15 ms , which is the rate the internal controller uses to update data.</p>
<h2>A. 2 Additional details of Create-Docker</h2>
<p>In Create-Docker, the objective is to dock to a charging station attached to the middle of one of the wider walls of the Create-Mover arena. When the robot is at the charging station in such a way that the binary charging signal is active, the internal robot controller switches the operating mode to Passive, in which the actuation commands for the wheels from the external computer are ignored. Being in this state with an active charging signal is considered a successful docking. The internal controller does not switch to the Passive mode right away after an active charging signal. Therefore, it is possible to have an active charging signal momentarily but still not dock successfully due to a high speed in the backward direction or bouncing back from the station. Moreover, it is extremely difficult to activate the charging signal properly if the robot approaches the charging station at an angle. Therefore, to learn how to dock, it is important to approach the charging station perpendicularly and learn to slow down or stop when the charging signal is active.
The action space for Create-Docker is the same as for Create-Mover, that is, $[-150 \mathrm{~mm} / \mathrm{s}, 150 \mathrm{~mm} / \mathrm{s}]^{2}$ for actuating the two wheels with speed control. The observation vector is 20 -dimensional, consisting of a single binary charging signal, six infrared wall signals, two bump signals, two action components from the previous time step, and nine additional components processed based on the three infrared bytes from the charging station. Each infrared byte informs whether the left buoy, the right buoy and the force field of the dock beam configuration can be seen, from which we obtain nine binary values. Each of these binary values are then averaged over last 20 packets streamed from the robot's internal controller every 15 ms .
The reward function is a large positive number for successful docking with penalty for bumping and encouragement for moving forward and facing the charging station perpendicularly. The reward function for every time-step is defined as follows:</p>
<p>$$
R_{t}=\tau\left(a X_{t}+b Y_{t}+c Z_{t}+d V_{t}\right)
$$</p>
<p>where $\tau$ is the action cycle time, $X_{t}$ is the docking reward, $Y_{t}$ is bumping penalty, $Z_{t}$ is the moving bonus and $V_{t}$ is the bonus for aligning to the charging station. These components are defined as follows:</p>
<p>$$
\begin{aligned}
X_{t} &amp; =\frac{2}{n(n+1)} \sum_{i=1}^{n}(n-i+1) p_{t}(\text { charging }, i) \
Y_{t} &amp; =-\sum_{k=1}^{2} \bigcup_{i=1}^{n} p_{t}\left(b u m p_{k}, i\right) \
Z_{t} &amp; =\frac{1}{n} \sum_{i=1}^{n} p_{t}(\text { distance }, i) \
V_{t} &amp; =\frac{1}{20} \sum_{i=1}^{20} \sum_{k=1}^{9} w_{k} p_{t}\left[\text { ir } . d o c k_{k}, i\right]
\end{aligned}
$$</p>
<p>Here, $p_{t}[x, i]$ stands for the $i$ th most-recent data packet available at time step $t$ for sensor $x$, where charging stands for charging sensor, bump $p_{k}$ stands for $k$ th bump sensor, distance stands for the</p>
<p>distance sensor and $i r _d o c k_{k}$ stands for the $k$ th sensor value for the infrared receiver for docking. The weights used for ir_dock are $w=[1.0,0.5,0.05,0.65,0.15,0.65,0.05,0.5,1.0]$, which are chosen in such a way that the left receiver (first three values) focuses on the left buoy, the right receiver (last three values) focuses on the right buoy and the omni-directional receiver (middle three values) focuses on both buoy equally. The value of $n$ is the ratio between action cycle time and the read-write cycle time, that is, $n=\frac{n}{0.015}$. The weights of the different reward components are $a=150, b=10, c=5$, and $d=4$. They are chosen in such a way that the maximums of the penalty and the two bonuses are of the same magnitude scale and the docking reward is much larger than the auxiliary reward components. If docking is successful, the robot stays at the charging station for the rest of the episode and continues to receive the docking reward. This encourages docking as soon as possible.
An episode is always 30 seconds long. We designed the reset between episodes in such a way that docking is relatively easier for the initial policy if the previous episode is unsuccessful and it is relatively more difficult if the previous episode is successful, that is, the episode terminated while the robot is docked. To achieve this, the reset procedure first invokes the internal seek-dock routine of Open Interface to dock to the station if the previous episode is unsuccessful. After the robot docks using seek-dock or a time-out of 20 seconds, the robot moves backward for 3.25 seconds. If the seek-dock succeeds, the robot is always facing the charging station after reset and can dock successfully by learning to move straight in the forward direction. However, if the seek-dock routine does not succeed, then the robot may start from a difficult starting position, for example, at one of the corners of the arena facing the wall. If the previous episode is successful, then the reset procedure makes the robot move backward for 0.75 seconds and then sends uniform random speeds for 2.5 seconds to the two wheels independently between $[-250,-50]$ to move backward further rotationally. This last phase ensures that the robot is likely not facing the charging station perpendicularly and displacement is required to achieve alignment.</p>
<h1>A. 3 Ranges of hyper-parameter values for random search</h1>
<p>TRPO:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">$2^{[8,13]}$</td>
</tr>
<tr>
<td style="text-align: center;">vf-step-size</td>
<td style="text-align: center;">$10^{[-5,-2]}$</td>
</tr>
<tr>
<td style="text-align: center;">$\delta_{K L}$</td>
<td style="text-align: center;">$10^{[-2.5,-0.5]}$</td>
</tr>
<tr>
<td style="text-align: center;">$c_{\gamma}$</td>
<td style="text-align: center;">$10^{\left[\log _{10}(10 / N), 1.5\right]}$</td>
</tr>
<tr>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">$1-\frac{1}{c_{\gamma} N}$</td>
</tr>
<tr>
<td style="text-align: center;">$c_{\lambda}$</td>
<td style="text-align: center;">$10^{\left[\log _{10}(10 / N), 1.5\right]}$</td>
</tr>
<tr>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">$1-\frac{1}{c_{\lambda} N}$</td>
</tr>
<tr>
<td style="text-align: center;">hidden layers</td>
<td style="text-align: center;">$[1,4]$</td>
</tr>
<tr>
<td style="text-align: center;">hidden sizes</td>
<td style="text-align: center;">$2^{[3, X]}$</td>
</tr>
</tbody>
</table>
<p>Here, $N=T / \tau$, where $T$ is the total length of an episode in time and $\tau$ is the action cycle time. We restricted total number of weights in the network to be no larger than 100,000, and the upper limit of a hidden size $X$ was determined based on sampled number of hidden layers to respect this limit.</p>
<p>PPO:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">$2^{[8,13]}$</td>
</tr>
<tr>
<td style="text-align: center;">step-size</td>
<td style="text-align: center;">$10^{[-5,-2]}$</td>
</tr>
<tr>
<td style="text-align: center;">opt. batch size</td>
<td style="text-align: center;">$2^{\left[3, \log _{2}(\text { batch size })\right]}$</td>
</tr>
<tr>
<td style="text-align: center;">$c_{\gamma}$</td>
<td style="text-align: center;">$10^{\left[\log _{10}(10 / N), 1.5\right]}$</td>
</tr>
<tr>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">$1-\frac{1}{c_{\gamma} N}$</td>
</tr>
<tr>
<td style="text-align: center;">$c_{\lambda}$</td>
<td style="text-align: center;">$10^{\left[\log _{10}(10 / N), 1.5\right]}$</td>
</tr>
<tr>
<td style="text-align: center;">$\lambda$</td>
<td style="text-align: center;">$1-\frac{1}{c_{\lambda} N}$</td>
</tr>
<tr>
<td style="text-align: center;">hidden layers</td>
<td style="text-align: center;">$[1,4]$</td>
</tr>
<tr>
<td style="text-align: center;">hidden sizes</td>
<td style="text-align: center;">$2^{[3, X]}$</td>
</tr>
</tbody>
</table>
<p>Soft-Q:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">$2^{[8,13]}$</td>
</tr>
<tr>
<td style="text-align: center;">step size</td>
<td style="text-align: center;">$10^{[-5,-2]}$</td>
</tr>
<tr>
<td style="text-align: center;">epochs</td>
<td style="text-align: center;">$2^{[0,2]}$</td>
</tr>
<tr>
<td style="text-align: center;">$c_{\gamma}$</td>
<td style="text-align: center;">$10^{\left[\log _{10}(10 / N), 1.5\right]}$</td>
</tr>
<tr>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">$1-\frac{1}{c_{\gamma} N}$</td>
</tr>
<tr>
<td style="text-align: center;">reward scale</td>
<td style="text-align: center;">$10^{[0,2]}$</td>
</tr>
<tr>
<td style="text-align: center;">hidden layers</td>
<td style="text-align: center;">$[1,4]$</td>
</tr>
<tr>
<td style="text-align: center;">hidden sizes</td>
<td style="text-align: center;">$2^{[3, X]}$</td>
</tr>
</tbody>
</table>
<p>DDPG:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Range</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">batch size</td>
<td style="text-align: center;">$2^{[8,13]}$</td>
</tr>
<tr>
<td style="text-align: center;">step size</td>
<td style="text-align: center;">$10^{[-5,-2]}$</td>
</tr>
<tr>
<td style="text-align: center;">exploration $\sigma$</td>
<td style="text-align: center;">$10^{[-2, \log _{10} 5]}$</td>
</tr>
<tr>
<td style="text-align: center;">$c_{\gamma}$</td>
<td style="text-align: center;">$10^{\left[\log _{10}(10 / N), 1.5\right]}$</td>
</tr>
<tr>
<td style="text-align: center;">$\gamma$</td>
<td style="text-align: center;">$1-\frac{1}{c_{\gamma} N}$</td>
</tr>
<tr>
<td style="text-align: center;">reward scale</td>
<td style="text-align: center;">$10^{[0,2]}$</td>
</tr>
<tr>
<td style="text-align: center;">hidden layers</td>
<td style="text-align: center;">$[1,4]$</td>
</tr>
<tr>
<td style="text-align: center;">hidden sizes</td>
<td style="text-align: center;">$2^{[3, X]}$</td>
</tr>
</tbody>
</table>
<h1>A. 4 Details of scripted agents</h1>
<p>For UR5 tasks, we use the movej command of URScript, where we specify the joint angles for the target positions and set the time to reach to 2 seconds. For DXL tasks, we implement a PID controller. We do not constrain the current control values as we do for the learning agent, and we chose the optimal PID gain value for each task separately. For Create-Mover, we use a simple script that applies action $[-150 \mathrm{~mm} / \mathrm{s},+150 \mathrm{~mm} / \mathrm{s}]$ whenever the normalized signal value of either of the two front wall sensors has its value above 0.55 and otherwise, moves straight forward with action $[+150 \mathrm{~mm} / \mathrm{s},+150 \mathrm{~mm} / \mathrm{s}]$. For Create-Docker, we use the seek-dock routine of Open Interface. During this routine, the robot moves back and forth to perceive the vertical plane perpendicular to the wall at the charging station using the infrared bytes from the charging station, adjusts its position to align its principle axis with the perpendicular plane, and moves slowly toward the charging station.</p>
<h1>A. 5 Details of repeatability experiments with TRPO</h1>
<p>To generate the plots in Figure 2, we run the same experiment with the same seed four times on five different tasks using TRPO. There are different kinds of randomization in each task. For the agent, randomization is used to initialize the network and sample actions. For the environment, randomization is used to generate targets and resets. By using the same randomization seed across multiple experiments in this set of experiments, we ensure that the environment generates the same sequence of targets and resets, the agent is initialized with the same network, and it generates the same or similar sequence of actions for a particular task. We use the same hyper-parameter values of TRPO used in the experiments by Henderson et al. (2017).</p>
<h1>A. 6 Relative performance of different algorithms in random search</h1>
<p>In the figure below, we show the relative performance of four algorithms across 30 random hyperparameters configurations ordered by performance. Note that for different algorithms, parameter configurations with the same index generally correspond to different parameter values.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: All random parameter configurations of all four algorithms on UR-Reacher-2 and DXLReacher.</p>
<h1>A. 7 All hyper-parameter configurations, their value distributions and correlations with returns</h1>
<p>In the following table, we show parameter values for all 30 configurations and their corresponding average returns for TRPO on UR-Reacher-2. The configurations are shown in descending order according to average returns.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Average <br> Return</th>
<th style="text-align: center;">batch <br> size</th>
<th style="text-align: center;">vf-step-size</th>
<th style="text-align: center;">$\delta_{K L}$</th>
<th style="text-align: center;">$\gamma$</th>
<th style="text-align: center;">$\lambda$</th>
<th style="text-align: center;">hidden <br> layers</th>
<th style="text-align: center;">hidden <br> sizes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">158.56</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00472</td>
<td style="text-align: center;">0.02437</td>
<td style="text-align: center;">0.96833</td>
<td style="text-align: center;">0.99874</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">138.58</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00475</td>
<td style="text-align: center;">0.01909</td>
<td style="text-align: center;">0.99924</td>
<td style="text-align: center;">0.99003</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">131.35</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00037</td>
<td style="text-align: center;">0.31222</td>
<td style="text-align: center;">0.97433</td>
<td style="text-align: center;">0.99647</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">123.45</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00036</td>
<td style="text-align: center;">0.01952</td>
<td style="text-align: center;">0.99799</td>
<td style="text-align: center;">0.92958</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">122.60</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00163</td>
<td style="text-align: center;">0.00510</td>
<td style="text-align: center;">0.96801</td>
<td style="text-align: center;">0.96893</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">115.51</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00926</td>
<td style="text-align: center;">0.01659</td>
<td style="text-align: center;">0.99935</td>
<td style="text-align: center;">0.99711</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">103.18</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00005</td>
<td style="text-align: center;">0.21515</td>
<td style="text-align: center;">0.99891</td>
<td style="text-align: center;">0.99880</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">100.38</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00005</td>
<td style="text-align: center;">0.09138</td>
<td style="text-align: center;">0.99677</td>
<td style="text-align: center;">0.99959</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">95.47</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">0.06088</td>
<td style="text-align: center;">0.98488</td>
<td style="text-align: center;">0.99957</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">94.16</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00770</td>
<td style="text-align: center;">0.02278</td>
<td style="text-align: center;">0.99414</td>
<td style="text-align: center;">0.98684</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">88.57</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00282</td>
<td style="text-align: center;">0.02312</td>
<td style="text-align: center;">0.99813</td>
<td style="text-align: center;">0.99964</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">65.44</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00054</td>
<td style="text-align: center;">0.01882</td>
<td style="text-align: center;">0.99728</td>
<td style="text-align: center;">0.99420</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">63.60</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00009</td>
<td style="text-align: center;">0.10678</td>
<td style="text-align: center;">0.97415</td>
<td style="text-align: center;">0.99759</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">60.79</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.00007</td>
<td style="text-align: center;">0.02759</td>
<td style="text-align: center;">0.99945</td>
<td style="text-align: center;">0.99961</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">60.51</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00222</td>
<td style="text-align: center;">0.00392</td>
<td style="text-align: center;">0.98544</td>
<td style="text-align: center;">0.98067</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">60.35</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00004</td>
<td style="text-align: center;">0.25681</td>
<td style="text-align: center;">0.99750</td>
<td style="text-align: center;">0.98955</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">59.39</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.00435</td>
<td style="text-align: center;">0.00518</td>
<td style="text-align: center;">0.99516</td>
<td style="text-align: center;">0.99867</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">52.70</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">0.03385</td>
<td style="text-align: center;">0.99119</td>
<td style="text-align: center;">0.98400</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">51.44</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00034</td>
<td style="text-align: center;">0.01319</td>
<td style="text-align: center;">0.97334</td>
<td style="text-align: center;">0.98524</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">41.05</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">0.00351</td>
<td style="text-align: center;">0.99430</td>
<td style="text-align: center;">0.99781</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">17.14</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00023</td>
<td style="text-align: center;">0.01305</td>
<td style="text-align: center;">0.95963</td>
<td style="text-align: center;">0.99950</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">11.43</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00251</td>
<td style="text-align: center;">0.00532</td>
<td style="text-align: center;">0.99447</td>
<td style="text-align: center;">0.99951</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">11.13</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00003</td>
<td style="text-align: center;">0.00727</td>
<td style="text-align: center;">0.99686</td>
<td style="text-align: center;">0.93165</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;">-10.57</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00065</td>
<td style="text-align: center;">0.04867</td>
<td style="text-align: center;">0.99926</td>
<td style="text-align: center;">0.98226</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">-16.48</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">0.31390</td>
<td style="text-align: center;">0.99948</td>
<td style="text-align: center;">0.99204</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">-19.78</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00005</td>
<td style="text-align: center;">0.15077</td>
<td style="text-align: center;">0.96836</td>
<td style="text-align: center;">0.99944</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">-32.85</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00003</td>
<td style="text-align: center;">0.12650</td>
<td style="text-align: center;">0.99260</td>
<td style="text-align: center;">0.98021</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">-43.74</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00018</td>
<td style="text-align: center;">0.00333</td>
<td style="text-align: center;">0.98940</td>
<td style="text-align: center;">0.97090</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">-54.55</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00011</td>
<td style="text-align: center;">0.07420</td>
<td style="text-align: center;">0.99402</td>
<td style="text-align: center;">0.90185</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2048</td>
</tr>
<tr>
<td style="text-align: center;">-125.13</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00002</td>
<td style="text-align: center;">0.05471</td>
<td style="text-align: center;">0.99961</td>
<td style="text-align: center;">0.99877</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
</tbody>
</table>
<p>Table 1: The parameter values and corresponding average returns for all 30 hyper-parameter configurations of TRPO on UR-Reacher-2.</p>
<p>In the figure below, we show the best 5 (in red) and the worst 5 (in blue) hyper-parameter values out of 30 random configurations of TRPO on UR-Reacher-2. On each plot the x axis represents parameter values and the y axis represents average returns obtained during the corresponding run. For each hyper-parameter, we also show correlations between log-parameter values and corresponding returns.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 8: Average returns (y axis) vs. parameter values (x axis) for best 5 and worst 5 hyperparameter configurations of TRPO on UR-Reacher-2.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Average <br> Return</th>
<th style="text-align: center;">batch <br> size</th>
<th style="text-align: center;">vf-step-size</th>
<th style="text-align: center;">$\delta_{K L}$</th>
<th style="text-align: center;">$\gamma$</th>
<th style="text-align: center;">$\lambda$</th>
<th style="text-align: center;">hidden <br> layers</th>
<th style="text-align: center;">hidden <br> sizes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">-15.11</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00251</td>
<td style="text-align: center;">0.00532</td>
<td style="text-align: center;">0.99216</td>
<td style="text-align: center;">0.99907</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">-15.53</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00054</td>
<td style="text-align: center;">0.01882</td>
<td style="text-align: center;">0.99580</td>
<td style="text-align: center;">0.99183</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">-15.54</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00065</td>
<td style="text-align: center;">0.04867</td>
<td style="text-align: center;">0.99867</td>
<td style="text-align: center;">0.97815</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">-15.76</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00034</td>
<td style="text-align: center;">0.01319</td>
<td style="text-align: center;">0.96874</td>
<td style="text-align: center;">0.98142</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">-16.52</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">0.06088</td>
<td style="text-align: center;">0.98102</td>
<td style="text-align: center;">0.99918</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">-16.61</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00770</td>
<td style="text-align: center;">0.02278</td>
<td style="text-align: center;">0.99175</td>
<td style="text-align: center;">0.98320</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">-17.03</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00005</td>
<td style="text-align: center;">0.21515</td>
<td style="text-align: center;">0.99812</td>
<td style="text-align: center;">0.99796</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">-17.24</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.00435</td>
<td style="text-align: center;">0.00518</td>
<td style="text-align: center;">0.99304</td>
<td style="text-align: center;">0.99777</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">-18.44</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00163</td>
<td style="text-align: center;">0.00510</td>
<td style="text-align: center;">0.96331</td>
<td style="text-align: center;">0.96423</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">-19.21</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.00007</td>
<td style="text-align: center;">0.02759</td>
<td style="text-align: center;">0.99897</td>
<td style="text-align: center;">0.99924</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">-19.29</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00011</td>
<td style="text-align: center;">0.07420</td>
<td style="text-align: center;">0.99161</td>
<td style="text-align: center;">0.90163</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2048</td>
</tr>
<tr>
<td style="text-align: left;">-19.51</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00282</td>
<td style="text-align: center;">0.02312</td>
<td style="text-align: center;">0.99699</td>
<td style="text-align: center;">0.99929</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">-20.07</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00009</td>
<td style="text-align: center;">0.10678</td>
<td style="text-align: center;">0.96958</td>
<td style="text-align: center;">0.99622</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">-20.12</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">0.31390</td>
<td style="text-align: center;">0.99902</td>
<td style="text-align: center;">0.98921</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: left;">-20.30</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00036</td>
<td style="text-align: center;">0.01952</td>
<td style="text-align: center;">0.99678</td>
<td style="text-align: center;">0.92655</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">-20.45</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00004</td>
<td style="text-align: center;">0.25681</td>
<td style="text-align: center;">0.99610</td>
<td style="text-align: center;">0.98628</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">-20.45</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00472</td>
<td style="text-align: center;">0.02437</td>
<td style="text-align: center;">0.96363</td>
<td style="text-align: center;">0.99786</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">-21.28</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00003</td>
<td style="text-align: center;">0.00727</td>
<td style="text-align: center;">0.99524</td>
<td style="text-align: center;">0.92844</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: left;">-21.37</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00037</td>
<td style="text-align: center;">0.31222</td>
<td style="text-align: center;">0.96976</td>
<td style="text-align: center;">0.99472</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">-21.73</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00926</td>
<td style="text-align: center;">0.01659</td>
<td style="text-align: center;">0.99880</td>
<td style="text-align: center;">0.99558</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">-23.17</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">0.00351</td>
<td style="text-align: center;">0.99195</td>
<td style="text-align: center;">0.99653</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">-23.47</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00005</td>
<td style="text-align: center;">0.09138</td>
<td style="text-align: center;">0.99512</td>
<td style="text-align: center;">0.99921</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">-23.54</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00001</td>
<td style="text-align: center;">0.03385</td>
<td style="text-align: center;">0.98820</td>
<td style="text-align: center;">0.98005</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">-26.75</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00003</td>
<td style="text-align: center;">0.12650</td>
<td style="text-align: center;">0.98987</td>
<td style="text-align: center;">0.97595</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">-27.02</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00023</td>
<td style="text-align: center;">0.01305</td>
<td style="text-align: center;">0.95497</td>
<td style="text-align: center;">0.99905</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">-27.05</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00475</td>
<td style="text-align: center;">0.01909</td>
<td style="text-align: center;">0.99864</td>
<td style="text-align: center;">0.98684</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">-27.64</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00002</td>
<td style="text-align: center;">0.05471</td>
<td style="text-align: center;">0.99924</td>
<td style="text-align: center;">0.99792</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: left;">-28.02</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00222</td>
<td style="text-align: center;">0.00392</td>
<td style="text-align: center;">0.98164</td>
<td style="text-align: center;">0.97644</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: left;">-31.20</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00005</td>
<td style="text-align: center;">0.15077</td>
<td style="text-align: center;">0.96366</td>
<td style="text-align: center;">0.99895</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: left;">-33.33</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00018</td>
<td style="text-align: center;">0.00333</td>
<td style="text-align: center;">0.98611</td>
<td style="text-align: center;">0.96624</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
</tbody>
</table>
<p>Table 2: The parameter values and corresponding average returns for all 30 hyper-parameter configurations of TRPO on DXL-Reacher.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 9: Average return (y axis) vs. parameter values (x axis) for best 5 and worst 5 hyperparameter configurations of TRPO on DXL-Reacher.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Average <br> Return</th>
<th style="text-align: center;">batch <br> size</th>
<th style="text-align: center;">step-size</th>
<th style="text-align: center;">opt. <br> batch size</th>
<th style="text-align: center;">$\gamma$</th>
<th style="text-align: center;">$\lambda$</th>
<th style="text-align: center;">hidden <br> layers</th>
<th style="text-align: center;">hidden <br> sizes</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">176.62</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00005</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.96836</td>
<td style="text-align: center;">0.99944</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">150.25</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00050</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.99926</td>
<td style="text-align: center;">0.98226</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">137.92</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00011</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.99402</td>
<td style="text-align: center;">0.90185</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2048</td>
</tr>
<tr>
<td style="text-align: center;">137.26</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00163</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.96801</td>
<td style="text-align: center;">0.96893</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">136.09</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00280</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.99924</td>
<td style="text-align: center;">0.99003</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">128.34</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00036</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.99799</td>
<td style="text-align: center;">0.92958</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">118.77</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00003</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.99686</td>
<td style="text-align: center;">0.93165</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">256</td>
</tr>
<tr>
<td style="text-align: center;">112.48</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00941</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.98544</td>
<td style="text-align: center;">0.98067</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">110.01</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00080</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">0.99935</td>
<td style="text-align: center;">0.99711</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">107.47</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00267</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.96833</td>
<td style="text-align: center;">0.99874</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">95.62</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00226</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.97433</td>
<td style="text-align: center;">0.99647</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">82.21</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00037</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.99119</td>
<td style="text-align: center;">0.98400</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">78.97</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00090</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">0.99430</td>
<td style="text-align: center;">0.99781</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">73.33</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00079</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.99813</td>
<td style="text-align: center;">0.99964</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">73.17</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00003</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">0.99260</td>
<td style="text-align: center;">0.98021</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">56.25</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00987</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.99948</td>
<td style="text-align: center;">0.99204</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">49.02</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00019</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">0.99677</td>
<td style="text-align: center;">0.99959</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">29.70</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.00039</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.99945</td>
<td style="text-align: center;">0.99961</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">25.94</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00362</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.97415</td>
<td style="text-align: center;">0.99759</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">18.64</td>
<td style="text-align: center;">4096</td>
<td style="text-align: center;">0.00061</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.99891</td>
<td style="text-align: center;">0.99880</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">-1.68</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00006</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.99750</td>
<td style="text-align: center;">0.98955</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">-20.53</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00087</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.98940</td>
<td style="text-align: center;">0.97090</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">8</td>
</tr>
<tr>
<td style="text-align: center;">-31.58</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.00315</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.99516</td>
<td style="text-align: center;">0.99867</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">-49.32</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00680</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">0.99728</td>
<td style="text-align: center;">0.99420</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">-62.46</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00002</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.99414</td>
<td style="text-align: center;">0.98684</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">-64.50</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">0.00002</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">0.98488</td>
<td style="text-align: center;">0.99957</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: center;">-79.45</td>
<td style="text-align: center;">8192</td>
<td style="text-align: center;">0.00002</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.95963</td>
<td style="text-align: center;">0.99950</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">32</td>
</tr>
<tr>
<td style="text-align: center;">-84.29</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00002</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.97334</td>
<td style="text-align: center;">0.98524</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">16</td>
</tr>
<tr>
<td style="text-align: center;">-94.34</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">0.00645</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">0.99447</td>
<td style="text-align: center;">0.99951</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">-134.43</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">0.00689</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">0.99961</td>
<td style="text-align: center;">0.99877</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">32</td>
</tr>
</tbody>
</table>
<p>Table 3: The parameter values and corresponding average returns for all 30 hyper-parameter configurations of PPO on UR-Reacher-2.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 10: Average returns ( y axis) vs. parameter values ( x axis) for best 5 and worst 5 hyperparameter configurations of PPO on UR-Reacher-2.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ A companion video is available at https://youtu.be/ovDfhvjpQd8&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
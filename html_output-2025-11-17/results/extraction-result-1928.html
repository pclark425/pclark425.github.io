<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1928 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1928</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1928</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-39.html">extraction-schema-39</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <p><strong>Paper ID:</strong> paper-275949793</p>
                <p><strong>Paper Title:</strong> HAMSTER: H IERARCHICAL A CTION M ODELS FOR O PEN -W ORLD R OBOT M ANIPULATION</p>
                <p><strong>Paper Abstract:</strong> Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results, code, and dataset are provided at: https://hamster-robot.github.io/</p>
                <p><strong>Cost:</strong> 0.027</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1928.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1928.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HAMSTER</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>HAMSTER: Hierarchical Action Models with SeparaTEd Path Representations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical vision-language-action (VLA) system that fine-tunes a large vision-language model (VILA-1.5-13B) to predict intermediate 2D end-effector paths from RGB + language; these 2D paths condition a specialist low-level 3D-aware policy (e.g., RVT-2 or 3D-DA) which produces continuous robot actions for real-world manipulation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HAMSTER (hierarchical VLA)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Two-stage architecture: (1) a high-level VLM (VILA-1.5-13B) fine-tuned to output 2D image-plane trajectories and gripper open/close tokens from an RGB image + language prompt; (2) a low-level policy (RVT-2 or 3D-DA) that conditions on the drawn 2D path plus 3D perceptual inputs (pointcloud, depth, proprioception) to output continuous robot actions. The VLM is queried sparsely to produce a path which the low-level policy follows at higher control frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining on interleaved image-text and video-caption datasets (web-scale multimodal VLM pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Base VLM (VILA-1.5-13B) was pretrained on interleaved image-text datasets and video captioning data (web-scale multimodal sources containing object descriptions, scene captions, and some action-language in video captions); HAMSTER then fine-tunes on a multi-domain off-domain dataset (pixel point prediction Robopoint 770k, simulated RLBench 320k trajectory sketches, Bridge+DROID real robot trajectories ~55k, plus 660k VQA samples) that contains object locations, spatial relations, and end-effector path sketches but not necessarily detailed low-level robot actions.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (tabletop pick-and-place, press button, knock down, long-horizon / non-prehensile tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Real-world and simulated tabletop manipulation tasks executed on a Franka Panda (real world) and RLBench / Colosseum (simulation). Action space: continuous low-level robot control commands (controller-level actions produced by imitation-learned policy). Tasks include prehensile (pick & place) and non-prehensile (press button, knock down, wipe, open drawers) with varying objects, lighting, camera views, and language instructions; low-level policy takes 3D inputs (depth/pointcloud + proprioception) and continuous action outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>The paper explicitly discusses semantic alignment: the off-domain VLM fine-tuning data contain object location and trajectory sketches and VQA pairs that supply semantic grounding (objects, spatial relations, task descriptions). The degree of overlap is partial: simulation and other robot datasets provide end-effector paths (spatial/action cues) while VQA data retain world knowledge; the authors note the VLM had not seen in-domain data yet still transferred, indicating moderate semantic overlap sufficed for transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Real-world: HAMSTER (hierarchical VLM + low-level policy) yields an average improvement over OpenVLA: 'an average of 20% improvement in success rate across seven different axes of generalization' (stated in text; described as a 50% relative gain). Table 7 (grouped by task type) reports e.g., pick-and-place success: HAMSTER+RVT2 = 0.79, HAMSTER+3DDA = 0.78; press button = 0.50 (HAMSTER+RVT2) and 0.63 (HAMSTER+3DDA); knock down = 0.47 and 0.66 respectively. Simulation: HAMSTER paired with 3D-DA outperforms vanilla 3D-DA by an average of 31% (Colosseum results).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Baselines without the hierarchical VLM (non-VLM 3D policies): RVT-2 baseline success ~0.28, 3D-DA baseline ~0.19 (Table 7 aggregated per task type); monolithic VLA baseline OpenVLA in real pick-and-place averaged 0.54 (RLBench-finetuned) vs 0.58 (without RLBench fine-tuning) on a small pick-and-place test (6 trials), and OpenVLA overall reported lower success in the broader evaluations (e.g., Table 7 shows OpenVLA pick-and-place 0.46).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>HAMSTER shows improved demonstration efficiency: in Colosseum experiments, HAMSTER+3D-DA with 50% of the imitation data achieves ~2x the success rate of the standard 3D-DA trained on the full data (paper: 'HAMSTER+3D-DA with just 50% of the data still achieves 2x the success rate of standard 3D-DA'). The low-level policies in real-world HAMSTER were trained with 320 teleoperated episodes; the high-level VLM required only off-domain finetuning (no in-domain action labels).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No explicit attention visualization / attention-map analysis of the VLM or low-level policy is reported. The paper reports qualitative path visualizations and human rankings of predicted paths but not internal attention maps.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No explicit analysis of embedding space geometry, clustering, or representational similarity is reported beyond behavioral transfer and VQA benchmark matching.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Moderate empirical evidence: the VLM is fine-tuned to output discrete gripper open/close tokens plus 2D end-effector coordinates, and those trajectories when fed to the low-level policy improve downstream task success (quantitative gains vs baselines). Human evaluators ranked VLM-generated trajectories for plausibility, and ablations show including simulated trajectory sketches in finetuning improves real-world performance—this suggests language-to-action grounding through predicted paths, though the paper also documents limitations (2D→3D ambiguity, inability to represent forces or rotations).</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>The paper presents behavioral-level evidence that separating high-level (semantic) reasoning (VLM predicting 2D paths) from low-level (3D, proprioceptive) control yields stronger generalization; no neural-feature-level (e.g., layerwise) analysis is reported. Empirically, high-level VLM preserves VQA capabilities while enabling low-level 3D policies to focus on geometric control.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Transfer worked well despite large domain gaps between off-domain finetuning data and real-robot tests (differences in embodiment, dynamics, visual appearance, and task semantics), particularly when VLM finetuning included simulation sketches plus pixel-point tasks and some real robot trajectories; monolithic VLAs showed poor benefit from RLBench sim data. Failure modes include substantial environmental changes during execution, 2D→3D ambiguity, and incorrect trajectory prediction (wrong object).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>The evaluation explicitly tests novel objects and novel object-goal combinations; HAMSTER outperforms baselines on novel objects and combinations (see axes: novel object, obj-and-goal). Quantitative per-object breakdowns are in appendices; reported aggregate metrics show HAMSTER superior on novel object conditions (e.g., higher success rates across 'novel object' axis), but no per-object numeric delta vs familiar objects is explicitly summarized beyond task-aggregates.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>High-level VLM in HAMSTER was trained only on off-domain data (no in-domain VLM data) and used zero-shot at test-time on the deployment environment; this zero-shot transfer produced useful trajectories that improved downstream policy performance. Low-level policies were trained few-shot relative to large VLA requirements (320 teleop episodes in real world; additional simulation experiments show robust performance with 50% of data).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise or component ablation analysis of the VLM (e.g., freezing layers) is reported, beyond empirical ablations on path representation (RDP vs fixed 20 points) and inclusion/exclusion of simulation data.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes—monolithic VLA OpenVLA showed little benefit from RLBench simulation data when fine-tuned: RLBench-finetuned OpenVLA average success 0.54 vs 0.58 for model without RLBench fine-tuning on a small pick-and-place test, suggesting limited or negative transfer from that simulation data for monolithic action-predicting VLAs. The paper argues hierarchical decoupling avoids this negative transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>The paper compares HAMSTER to non-VLM 3D policies (vision/3D-input policies like RVT-2 and 3D-DA). HAMSTER (with VLM guidance) substantially outperforms these vision/3D-only policies (e.g., RVT-2 baseline 0.28 vs HAMSTER+RVT2 0.79 on pick-and-place aggregated metrics), indicating the added semantic guidance from VLMs improves performance; no direct numeric comparison to ImageNet-pretrained-only vision backbones is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>The paper does not provide a representation-level study of how representations evolve during fine-tuning; it discusses that VLM predictions are generated once or sparsely per episode and remain fixed during execution, which causes failures under dynamic environmental changes.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No explicit dimensionality or intrinsic-dimension analyses (PCA, rank, etc.) are reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1928.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1928.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>VILA-1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>VILA-1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B-parameter vision-language model used as the HAMSTER high-level backbone and fine-tuned to predict 2D end-effector trajectories and gripper actions from image+language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>VILA-1.5-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based vision-language model accepting interleaved image and text tokens and autoregressively generating text tokens; used as the VLM backbone and fully fine-tuned (including vision encoder) in HAMSTER to output tokenized 2D trajectories and discrete gripper action tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (image-text and video-caption pretraining / multimodal web-scale pretraining)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on interleaved image-text datasets and video captioning data (web-scale multimodal corpora containing object descriptions, scene captions, and some action-language from videos).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>2D end-effector path prediction for robot manipulation (as VLM component within HAMSTER)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Predict sequences of 2D image-plane points and gripper open/close tokens corresponding to feasible robot end-effector motion to satisfy a language instruction in tabletop scenes; evaluated zero-shot and after fine-tuning on off-domain datasets (Robopoint, RLBench sim trajectories, Bridge/DROID real trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Fine-tuning data includes pixel point tasks (object localization), simulated robot trajectories (explicit end-effector paths) and real robot trajectories—this provides explicit spatial/action alignment in tokenized form; authors report that inclusion of simulation data improves transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>After off-domain fine-tuning, VILA-1.5-13B in HAMSTER produces trajectories that lead to substantial downstream performance gains when consumed by low-level policies; HAMSTER's VLM matches base VILA performance on 15 VQA benchmarks (Table 4), indicating preservation of general VLM capabilities after fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Zero-shot closed-source VLMs (e.g., GPT-4o prompted for trajectories) underperform the fine-tuned VILA variant in human trajectory rankings and downstream success; the paper reports finetuned VILA variants (with simulation data) outperform zero-shot GPT-4o RT-Trajectory baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>VILA finetuned on off-domain trajectory sketches enables low-level policies to be trained with fewer in-domain demonstrations (e.g., demonstration-efficiency results where HAMSTER with 50% data still outperforms full-data baselines), but the paper does not isolate sample-efficiency of VILA-only vs non-VILA-only policies numerically beyond HAMSTER system-level results.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No attention-map or token-attention analysis of VILA layers is presented.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No embedding-space / feature-clustering analyses are reported for VILA.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral: fine-tuning VILA to emit explicit 2D end-effector coordinates + gripper action tokens yields trajectories that align with human judgments and improve downstream control when fed to 3D policies—evidence of tokenized grounding of 'where to move' and 'when to open/close'.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>No internal analysis across layers; paper shows coarse-grained functional separation (semantic/path reasoning preserved in VILA; precise spatial control handled by 3D policies).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Inclusion of RLBench simulated trajectories in finetuning improves transfer to real-world tasks; excluding sim trajectories hurts some generalization in human-ranking experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Human-ranking experiments include diverse semantic tasks; finetuned VILA generalizes to out-of-domain inputs (human sketches, new objects) better than zero-shot closed-source VLMs, per human-ranked plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Base VILA showed limited zero-shot path generation capability without finetuning; finetuned VILA transfers zero-shot to unseen real-world deployments (VLM had no in-domain data but produced useful paths).</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No layer-wise ablations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not reported specifically for VILA; the paper notes monolithic VLA approaches had limited benefit from RLBench sim data, whereas VILA fine-tuned as HAMSTER's high-level did benefit from inclusion of simulated path data.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared to vision-only pretrained backbones; comparisons are against monolithic VLAs and non-VLM 3D policies.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No representational temporal dynamics analyses provided.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>None reported.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1928.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1928.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (monolithic vision-language-action baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art monolithic vision-language-action model baseline that is finetuned end-to-end to produce robot actions directly from images and language; used for comparison to HAMSTER.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Openvla: An open-source vision-language-action model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Monolithic VLA that takes images and language and produces action tokens or trajectories directly (end-to-end); in paper used as a baseline and finetuned on in-domain on-robot data (parameter-efficient LoRA fine-tuning for experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>vision-language pretraining (VLM-based) used as base for VLA; then finetuned to predict actions on robot datasets</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretrained on multimodal datasets as a VLM backbone (not exhaustively specified in this paper); finetuned on robot on-robot datasets for action prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation (same real-world tabletop tasks used for HAMSTER)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Real-world pick-and-place and other tabletop tasks; monolithic action-predicting model that must infer both high-level planning and low-level control at inference frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>OpenVLA requires fine-tuning on in-domain action-labeled robot data to align actions with perceptions; the paper reports limited transfer benefit from RLBench sim data to OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When finetuned on in-domain teleoperation data, OpenVLA achieves moderate performance; in a small real pick-and-place experiment RLBench-finetuned OpenVLA averaged success 0.54 vs 0.58 for model without RLBench fine-tuning, indicating limited gains from sim data. Table 7 aggregated task numbers show OpenVLA pick-and-place success ~0.46.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Paper does not report random-initialized OpenVLA baselines; the key contrast is that monolithic VLAs require in-domain action data and obtain worse generalization across axes vs HAMSTER.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>No direct sample-efficiency numbers besides statements that monolithic VLAs need substantial in-domain on-robot action-labeled data and are constrained by inference speed; HAMSTER is presented as more sample efficient in practice (empirical comparisons show HAMSTER outperforms OpenVLA with same/smaller in-domain datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Not presented in-depth; the paper argues monolithic VLAs struggle to ground actions across sim→real gaps and require costly on-robot data to learn action grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Monolithic approach conflates semantic and low-level control; authors argue this limits transfer and sample efficiency, but no explicit feature analysis is shown.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>OpenVLA showed little benefit from RLBench simulation finetuning—suggesting monolithic VLAs are sensitive to action/observation mismatch between source sim data and target real robot data.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Paper reports poorer generalization on axes of variation compared to HAMSTER (OpenVLA performs worse on unseen objects/visual variations), but exact per-condition numbers are in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>OpenVLA showed poor zero-shot generalization and was finetuned in-domain for fair comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Yes—finetuning OpenVLA on RLBench provided little improvement (0.54 vs 0.58) and overall OpenVLA underperformed HAMSTER across generalization axes, indicating possible negative or negligible transfer from off-domain simulation data for monolithic action-predicting VLAs.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Compared vs non-VLM 3D policies and HAMSTER; OpenVLA underperforms HAMSTER and sometimes underperforms strong 3D policies depending on conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1928.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1928.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RT-Trajectory (GPT-4o baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RT-Trajectory (trajectory sketch generation baselines using GPT-4o and Code-as-Policies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baselines that generate 2D trajectory sketches via prompting large closed-source LLM/VLMs (GPT-4o), either directly or using Code-as-Policies, used for comparison to HAMSTER's finetuned VLM trajectory generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>RT-trajectory: Robotic task generalization via hindsight trajectory sketches.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RT-Trajectory (GPT-4o / Code-as-Policies)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Prompt-based trajectory sketch generators built on large closed-source models (GPT-4o) that output keypoints / trajectories from an image + scene description; two variants evaluated: zero-shot GPT-4o prompt and GPT-4o + Code-as-Policies (CaP) pipeline using open-vocabulary detections.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>text and possibly vision-capable large models (closed-source LLM/VLM like GPT-4o) pretrained on massive internet text and multimodal data (when vision-enabled); used zero-shot without task-specific finetuning here.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Pretraining corpora are not detailed in the paper; GPT-4o-style closed-source models are known to be trained on large text corpora and (if vision-enabled) image-text pairs; no supervised trajectory sketch finetuning was used for these baselines in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>trajectory sketch generation for robotic manipulation (2D keypoints for end-effector motion)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Given an image and instruction, produce a short sequence of 2D points and gripper actions to indicate a feasible manipulation trajectory; these trajectories are then evaluated qualitatively by human rankings and as path guidance in RT-Trajectory baseline experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Zero-shot RT-Trajectory provides semantic reasoning from large-language knowledge but lacks explicit grounding to robotic end-effector coordinate spaces unless augmented with object detections; Code-as-Policies variant uses open-vocabulary detectors to provide scene object lists to the LLM to improve alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Behavioral comparisons show RT-Trajectory zero-shot GPT-4o and CaP variants underperform HAMSTER's finetuned VLM in human trajectory rankings and downstream performance; human ranking experiments in the paper show HAMSTER variants (especially including simulation data) rank better, and Table 6 indicates HAMSTER outperforms zero-shot RT-Trajectory baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not applicable—the baselines are zero-shot LLM usage (no task-specific finetuning).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>RT-Trajectory is zero-shot and does not require demonstrations, but its generated trajectories are less accurate and less useful downstream compared to HAMSTER's finetuned VLM, per human rankings and downstream results.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No internal attention analyses reported for GPT-4o baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Limited—zero-shot LLMs produce plausible semantic plans but lack the concrete spatial grounding (end-effector coordinates aligned to camera/projective geometry) that finetuned VLM trajectory sketches provide; authors report these baselines underperform finetuned HAMSTER VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Not analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Zero-shot RT-Trajectory is sensitive to the quality of scene descriptions and open-vocabulary detections; augmenting with CaP helps but still lags behind finetuned path predictors.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Human ranking experiments include diverse tasks; RT-Trajectory baselines were ranked worse on many of these out-of-domain tasks compared to finetuned HAMSTER VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Yes—these baselines are zero-shot; the paper demonstrates they underperform compared to finetuned HAMSTER VLM.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not directly discussed beyond empirical underperformance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not directly compared; these baselines rely on large language/vision models rather than vision-only pretrained components.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1928.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1928.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RVT-2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RVT-2 (Robotic View Transformer 2)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 3D-aware low-level policy architecture used as a HAMSTER low-level controller; processes 3D perceptual inputs and executes continuous actions; in HAMSTER it conditions on overlaid or concatenated 2D path channels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RVT-2</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A small, specialist 3D policy model that takes RGB + pointcloud/depth and (optionally) concatenated path channels and outputs continuous robot actions; includes virtual reprojection steps and is used for precise manipulation trained via imitation learning.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>not described as large-scale pretraining; treated as a specialist imitation-learning policy (trained on task-specific teleoperation data).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on in-domain teleoperated demonstrations collected for the target tasks (in the paper: 320 episodes for real-world experiments, and dataset variants in simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation low-level control (pick-and-place, button press, knock down, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Operates at controller frequency to follow 2D path guidance and produce continuous actions using 3D perception and proprioception; tested in both real-world and simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>RVT-2 relies on the HAMSTER 2D path to encode high-level semantics; without path conditioning it performs worse on semantic generalization axes.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>When conditioned on HAMSTER's finetuned VLM 2D paths, RVT-2 performance increases drastically (e.g., pick-and-place aggregated success: RVT-2 baseline 0.28 vs HAMSTER+RVT2 0.79 in Table 7; Table 1 shows HAMSTER+RVT2(Concat) achieving near 1.0 on some Colosseum camera tests).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>RVT-2 trained without path guidance yields substantially lower success (e.g., RVT-2 baseline 0.28 pick-and-place aggregated).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>With HAMSTER path guidance, RVT-2 achieves higher success with the same demonstration budget (320 episodes); Colosseum experiments indicate HAMSTER variants can reach high performance using fewer demonstrations (50% data) than vanilla 3D policies.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No internal attention maps for RVT-2 are reported; behavioral analysis shows virtual reprojection can fragment drawn paths and affect adherence.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Indirect: conditioning RVT-2 on explicit 2D path guidance links high-level semantic plans to low-level continuous motor outputs and improves successful interaction with target objects; failure analysis indicates that many RVT-2 failures are due to poor adherence to predicted trajectories (72% of RVT failures), underscoring action-grounding challenges.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Behaviorally, RVT-2 benefits from high-level semantic guidance and focuses on local geometric control; path-concatenation (6-channel input) preserves path signal better than overlay and yields best performance in some settings.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>RVT-2 with path guidance is robust to camera angle changes and visual variations; virtual reprojection specifics affect path decoding fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>RVT-2 benefits substantially from VLM-guided trajectories when encountering novel objects/visual variations; exact per-object deltas not fully enumerated.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>RVT-2 itself is trained via imitation (not zero-shot), but when paired with zero-shot-capable finetuned VLM paths it generalizes better to new scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>RVT-2 can suffer from fragmented path interpretations due to reprojection, harming adherence; no large negative transfer reported.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>RVT-2 is a 3D perception policy (vision+depth) and benefits from language-guided path inputs vs being purely vision-only.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No representational temporal analysis; failure analysis shows many failures arise from trajectory adherence during execution.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1928.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1928.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3D-DA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3D Diffuser Actor (3D-DA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A state-of-the-art 3D-aware low-level policy (policy diffusion actor) used in HAMSTER as an alternative low-level controller that consumes path-drawn images and 3D inputs to produce continuous actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>3d diffuser actor: Policy diffusion with 3d scene representations.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>3D-DA (3D Diffuser Actor)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A policy diffusion architecture that conditions on 3D scene representations and language; used as a low-level policy in HAMSTER to follow VLM-generated 2D paths and generate actions for manipulation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>not described as large-scale pretraining in this paper; used as an imitation-learned policy trained on collected teleoperation demonstrations (and/or simulated data) for downstream tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on in-domain teleoperation demonstrations (320 episodes for real-world experiments) and evaluated in simulation (RLBench / Colosseum).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation low-level control (same tasks as HAMSTER evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>Uses 3D scene representations and language to issue continuous control actions; in HAMSTER it takes the VLM path overlay/concatenated channel as guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>3D-DA leverages high-level path sketches to focus on geometric control; when provided HAMSTER paths, it shows improved robustness to visual and semantic variation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>HAMSTER+3D-DA shows substantial gains over vanilla 3D-DA: simulation average improvement of ~31% (Colosseum), and real-world aggregated task success (Table 7) e.g., pick-and-place HAMSTER+3DDA = 0.78 vs 3D-DA baseline 0.19.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>3D-DA baseline without HAMSTER path guidance: lower success (e.g., aggregated pick-and-place 0.19).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>HAMSTER+3D-DA with 50% of data reaches much higher success (2x) compared to full-data vanilla 3D-DA, indicating improved demonstration efficiency when conditioned on VLM-generated paths.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>No internal attention or diffusion-step attention analyses reported.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Behavioral: most 3D-DA failures are attributed to execution-specific issues rather than trajectory adherence (paper reports 90% of 3D-DA failures from execution), implying that VLM path guidance helps disambiguate semantics but motor/execution details remain critical.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Empirical results indicate 3D-DA benefits from decoupled high-level VLM guidance for semantic reasoning while 3D-DA focuses on precise 3D control.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Performs better when language guidance is simplified (real-world) or detailed (simulation), depending on the model's attention mechanism; benefits from path concatenation where supported.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>HAMSTER+3D-DA shows improved generalization to novel objects and semantics compared to 3D-DA alone (aggregate metrics reported), but per-object breakdowns are in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>3D-DA is trained via imitation: not zero-shot; benefits from VLM zero-shot transfer to provide guidance at test time.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not explicitly reported for 3D-DA; some failure modes are execution-level rather than transfer-level.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>3D-DA is a 3D-aware policy (vision+depth); pairing with VLM path guidance outperforms 3D-DA alone.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_dynamics</strong></td>
                            <td>No representational temporal analysis; failures categorized between trajectory adherence and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>dimensionality_analysis</strong></td>
                            <td>No.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1928.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e1928.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language models or language-conditioned agents being applied to embodied tasks, 3D environments, or robotic manipulation, including details about pretraining approaches, transfer performance, sample efficiency, and evidence of language-perception grounding mechanisms.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLARVA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLARVA (monolithic VLA with auxiliary trajectory task)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A monolithic vision-language-action model that predicts end-effector trajectories as an auxiliary task while primarily trained to predict actions; mentioned as a related monolithic approach.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>LLARVA: Vision-action instruction tuning enhances robot learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLARVA</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Monolithic VLA architecture that predicts both actions and auxiliary trajectories from image + language in an end-to-end manner (mentioned in related work as a contrasting monolithic VLA approach).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_type</strong></td>
                            <td>built from VLMs and finetuned on robot action datasets (monolithic finetuning)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_description</strong></td>
                            <td>Trained on on-robot demonstrations and standard VLM pretraining; exact dataset details are in LLARVA original paper (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_name</strong></td>
                            <td>robotic manipulation / action prediction</td>
                        </tr>
                        <tr>
                            <td><strong>target_task_description</strong></td>
                            <td>End-to-end action generation conditioned on images and language; cited to contrast HAMSTER's hierarchical approach.</td>
                        </tr>
                        <tr>
                            <td><strong>semantic_alignment</strong></td>
                            <td>Mentioned in relation as monolithic methods that use trajectory prediction as auxiliary signal; not analyzed here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_language_pretraining</strong></td>
                            <td>Mentioned but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_language_pretraining</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency_comparison</strong></td>
                            <td>Not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>has_sample_efficiency_data</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>attention_analysis</strong></td>
                            <td>Not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>embedding_space_analysis</strong></td>
                            <td>Not in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>action_grounding_evidence</strong></td>
                            <td>Cited as predicting trajectories auxiliary to action prediction, but no empirical details in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hierarchical_features_evidence</strong></td>
                            <td>Paper references LLARVA to emphasize differences: LLARVA uses auxiliary trajectories but remains monolithic rather than hierarchical decoupling.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_conditions</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>novel_vs_familiar_objects</strong></td>
                            <td>Not discussed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>zero_shot_or_few_shot</strong></td>
                            <td>Not discussed here.</td>
                        </tr>
                        <tr>
                            <td><strong>layer_analysis</strong></td>
                            <td>Not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>negative_transfer_evidence</strong></td>
                            <td>Not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_vision_only</strong></td>
                            <td>Not provided.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RT-trajectory: Robotic task generalization via hindsight trajectory sketches. <em>(Rating: 2)</em></li>
                <li>Openvla: An open-source vision-language-action model. <em>(Rating: 2)</em></li>
                <li>VILA: On pretraining for visual language models. <em>(Rating: 2)</em></li>
                <li>Robopoint: A vision-language model for spatial affordance prediction in robotics. <em>(Rating: 2)</em></li>
                <li>RLBench: The robot learning benchmark & learning environment. <em>(Rating: 2)</em></li>
                <li>RT-1: Robotics transformer for real-world control at scale. <em>(Rating: 1)</em></li>
                <li>RT-2: Vision-language-action models transfer web knowledge to robotic control. <em>(Rating: 2)</em></li>
                <li>Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation. <em>(Rating: 1)</em></li>
                <li>DROID: A large-scale in-the-wild robot manipulation dataset <em>(Rating: 2)</em></li>
                <li>Bridge dataset <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1928",
    "paper_id": "paper-275949793",
    "extraction_schema_id": "extraction-schema-39",
    "extracted_data": [
        {
            "name_short": "HAMSTER",
            "name_full": "HAMSTER: Hierarchical Action Models with SeparaTEd Path Representations",
            "brief_description": "A hierarchical vision-language-action (VLA) system that fine-tunes a large vision-language model (VILA-1.5-13B) to predict intermediate 2D end-effector paths from RGB + language; these 2D paths condition a specialist low-level 3D-aware policy (e.g., RVT-2 or 3D-DA) which produces continuous robot actions for real-world manipulation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "HAMSTER (hierarchical VLA)",
            "model_description": "Two-stage architecture: (1) a high-level VLM (VILA-1.5-13B) fine-tuned to output 2D image-plane trajectories and gripper open/close tokens from an RGB image + language prompt; (2) a low-level policy (RVT-2 or 3D-DA) that conditions on the drawn 2D path plus 3D perceptual inputs (pointcloud, depth, proprioception) to output continuous robot actions. The VLM is queried sparsely to produce a path which the low-level policy follows at higher control frequency.",
            "pretraining_type": "vision-language pretraining on interleaved image-text and video-caption datasets (web-scale multimodal VLM pretraining)",
            "pretraining_data_description": "Base VLM (VILA-1.5-13B) was pretrained on interleaved image-text datasets and video captioning data (web-scale multimodal sources containing object descriptions, scene captions, and some action-language in video captions); HAMSTER then fine-tunes on a multi-domain off-domain dataset (pixel point prediction Robopoint 770k, simulated RLBench 320k trajectory sketches, Bridge+DROID real robot trajectories ~55k, plus 660k VQA samples) that contains object locations, spatial relations, and end-effector path sketches but not necessarily detailed low-level robot actions.",
            "target_task_name": "robotic manipulation (tabletop pick-and-place, press button, knock down, long-horizon / non-prehensile tasks)",
            "target_task_description": "Real-world and simulated tabletop manipulation tasks executed on a Franka Panda (real world) and RLBench / Colosseum (simulation). Action space: continuous low-level robot control commands (controller-level actions produced by imitation-learned policy). Tasks include prehensile (pick & place) and non-prehensile (press button, knock down, wipe, open drawers) with varying objects, lighting, camera views, and language instructions; low-level policy takes 3D inputs (depth/pointcloud + proprioception) and continuous action outputs.",
            "semantic_alignment": "The paper explicitly discusses semantic alignment: the off-domain VLM fine-tuning data contain object location and trajectory sketches and VQA pairs that supply semantic grounding (objects, spatial relations, task descriptions). The degree of overlap is partial: simulation and other robot datasets provide end-effector paths (spatial/action cues) while VQA data retain world knowledge; the authors note the VLM had not seen in-domain data yet still transferred, indicating moderate semantic overlap sufficed for transfer.",
            "performance_with_language_pretraining": "Real-world: HAMSTER (hierarchical VLM + low-level policy) yields an average improvement over OpenVLA: 'an average of 20% improvement in success rate across seven different axes of generalization' (stated in text; described as a 50% relative gain). Table 7 (grouped by task type) reports e.g., pick-and-place success: HAMSTER+RVT2 = 0.79, HAMSTER+3DDA = 0.78; press button = 0.50 (HAMSTER+RVT2) and 0.63 (HAMSTER+3DDA); knock down = 0.47 and 0.66 respectively. Simulation: HAMSTER paired with 3D-DA outperforms vanilla 3D-DA by an average of 31% (Colosseum results).",
            "performance_without_language_pretraining": "Baselines without the hierarchical VLM (non-VLM 3D policies): RVT-2 baseline success ~0.28, 3D-DA baseline ~0.19 (Table 7 aggregated per task type); monolithic VLA baseline OpenVLA in real pick-and-place averaged 0.54 (RLBench-finetuned) vs 0.58 (without RLBench fine-tuning) on a small pick-and-place test (6 trials), and OpenVLA overall reported lower success in the broader evaluations (e.g., Table 7 shows OpenVLA pick-and-place 0.46).",
            "sample_efficiency_comparison": "HAMSTER shows improved demonstration efficiency: in Colosseum experiments, HAMSTER+3D-DA with 50% of the imitation data achieves ~2x the success rate of the standard 3D-DA trained on the full data (paper: 'HAMSTER+3D-DA with just 50% of the data still achieves 2x the success rate of standard 3D-DA'). The low-level policies in real-world HAMSTER were trained with 320 teleoperated episodes; the high-level VLM required only off-domain finetuning (no in-domain action labels).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No explicit attention visualization / attention-map analysis of the VLM or low-level policy is reported. The paper reports qualitative path visualizations and human rankings of predicted paths but not internal attention maps.",
            "embedding_space_analysis": "No explicit analysis of embedding space geometry, clustering, or representational similarity is reported beyond behavioral transfer and VQA benchmark matching.",
            "action_grounding_evidence": "Moderate empirical evidence: the VLM is fine-tuned to output discrete gripper open/close tokens plus 2D end-effector coordinates, and those trajectories when fed to the low-level policy improve downstream task success (quantitative gains vs baselines). Human evaluators ranked VLM-generated trajectories for plausibility, and ablations show including simulated trajectory sketches in finetuning improves real-world performance—this suggests language-to-action grounding through predicted paths, though the paper also documents limitations (2D→3D ambiguity, inability to represent forces or rotations).",
            "hierarchical_features_evidence": "The paper presents behavioral-level evidence that separating high-level (semantic) reasoning (VLM predicting 2D paths) from low-level (3D, proprioceptive) control yields stronger generalization; no neural-feature-level (e.g., layerwise) analysis is reported. Empirically, high-level VLM preserves VQA capabilities while enabling low-level 3D policies to focus on geometric control.",
            "transfer_conditions": "Transfer worked well despite large domain gaps between off-domain finetuning data and real-robot tests (differences in embodiment, dynamics, visual appearance, and task semantics), particularly when VLM finetuning included simulation sketches plus pixel-point tasks and some real robot trajectories; monolithic VLAs showed poor benefit from RLBench sim data. Failure modes include substantial environmental changes during execution, 2D→3D ambiguity, and incorrect trajectory prediction (wrong object).",
            "novel_vs_familiar_objects": "The evaluation explicitly tests novel objects and novel object-goal combinations; HAMSTER outperforms baselines on novel objects and combinations (see axes: novel object, obj-and-goal). Quantitative per-object breakdowns are in appendices; reported aggregate metrics show HAMSTER superior on novel object conditions (e.g., higher success rates across 'novel object' axis), but no per-object numeric delta vs familiar objects is explicitly summarized beyond task-aggregates.",
            "zero_shot_or_few_shot": "High-level VLM in HAMSTER was trained only on off-domain data (no in-domain VLM data) and used zero-shot at test-time on the deployment environment; this zero-shot transfer produced useful trajectories that improved downstream policy performance. Low-level policies were trained few-shot relative to large VLA requirements (320 teleop episodes in real world; additional simulation experiments show robust performance with 50% of data).",
            "layer_analysis": "No layer-wise or component ablation analysis of the VLM (e.g., freezing layers) is reported, beyond empirical ablations on path representation (RDP vs fixed 20 points) and inclusion/exclusion of simulation data.",
            "negative_transfer_evidence": "Yes—monolithic VLA OpenVLA showed little benefit from RLBench simulation data when fine-tuned: RLBench-finetuned OpenVLA average success 0.54 vs 0.58 for model without RLBench fine-tuning on a small pick-and-place test, suggesting limited or negative transfer from that simulation data for monolithic action-predicting VLAs. The paper argues hierarchical decoupling avoids this negative transfer.",
            "comparison_to_vision_only": "The paper compares HAMSTER to non-VLM 3D policies (vision/3D-input policies like RVT-2 and 3D-DA). HAMSTER (with VLM guidance) substantially outperforms these vision/3D-only policies (e.g., RVT-2 baseline 0.28 vs HAMSTER+RVT2 0.79 on pick-and-place aggregated metrics), indicating the added semantic guidance from VLMs improves performance; no direct numeric comparison to ImageNet-pretrained-only vision backbones is provided.",
            "temporal_dynamics": "The paper does not provide a representation-level study of how representations evolve during fine-tuning; it discusses that VLM predictions are generated once or sparsely per episode and remain fixed during execution, which causes failures under dynamic environmental changes.",
            "dimensionality_analysis": "No explicit dimensionality or intrinsic-dimension analyses (PCA, rank, etc.) are reported.",
            "uuid": "e1928.0"
        },
        {
            "name_short": "VILA-1.5-13B",
            "name_full": "VILA-1.5-13B",
            "brief_description": "A 13B-parameter vision-language model used as the HAMSTER high-level backbone and fine-tuned to predict 2D end-effector trajectories and gripper actions from image+language prompts.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "VILA-1.5-13B",
            "model_description": "Large transformer-based vision-language model accepting interleaved image and text tokens and autoregressively generating text tokens; used as the VLM backbone and fully fine-tuned (including vision encoder) in HAMSTER to output tokenized 2D trajectories and discrete gripper action tokens.",
            "pretraining_type": "vision-language pretraining (image-text and video-caption pretraining / multimodal web-scale pretraining)",
            "pretraining_data_description": "Pretrained on interleaved image-text datasets and video captioning data (web-scale multimodal corpora containing object descriptions, scene captions, and some action-language from videos).",
            "target_task_name": "2D end-effector path prediction for robot manipulation (as VLM component within HAMSTER)",
            "target_task_description": "Predict sequences of 2D image-plane points and gripper open/close tokens corresponding to feasible robot end-effector motion to satisfy a language instruction in tabletop scenes; evaluated zero-shot and after fine-tuning on off-domain datasets (Robopoint, RLBench sim trajectories, Bridge/DROID real trajectories)",
            "semantic_alignment": "Fine-tuning data includes pixel point tasks (object localization), simulated robot trajectories (explicit end-effector paths) and real robot trajectories—this provides explicit spatial/action alignment in tokenized form; authors report that inclusion of simulation data improves transfer.",
            "performance_with_language_pretraining": "After off-domain fine-tuning, VILA-1.5-13B in HAMSTER produces trajectories that lead to substantial downstream performance gains when consumed by low-level policies; HAMSTER's VLM matches base VILA performance on 15 VQA benchmarks (Table 4), indicating preservation of general VLM capabilities after fine-tuning.",
            "performance_without_language_pretraining": "Zero-shot closed-source VLMs (e.g., GPT-4o prompted for trajectories) underperform the fine-tuned VILA variant in human trajectory rankings and downstream success; the paper reports finetuned VILA variants (with simulation data) outperform zero-shot GPT-4o RT-Trajectory baselines.",
            "sample_efficiency_comparison": "VILA finetuned on off-domain trajectory sketches enables low-level policies to be trained with fewer in-domain demonstrations (e.g., demonstration-efficiency results where HAMSTER with 50% data still outperforms full-data baselines), but the paper does not isolate sample-efficiency of VILA-only vs non-VILA-only policies numerically beyond HAMSTER system-level results.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No attention-map or token-attention analysis of VILA layers is presented.",
            "embedding_space_analysis": "No embedding-space / feature-clustering analyses are reported for VILA.",
            "action_grounding_evidence": "Behavioral: fine-tuning VILA to emit explicit 2D end-effector coordinates + gripper action tokens yields trajectories that align with human judgments and improve downstream control when fed to 3D policies—evidence of tokenized grounding of 'where to move' and 'when to open/close'.",
            "hierarchical_features_evidence": "No internal analysis across layers; paper shows coarse-grained functional separation (semantic/path reasoning preserved in VILA; precise spatial control handled by 3D policies).",
            "transfer_conditions": "Inclusion of RLBench simulated trajectories in finetuning improves transfer to real-world tasks; excluding sim trajectories hurts some generalization in human-ranking experiments.",
            "novel_vs_familiar_objects": "Human-ranking experiments include diverse semantic tasks; finetuned VILA generalizes to out-of-domain inputs (human sketches, new objects) better than zero-shot closed-source VLMs, per human-ranked plausibility.",
            "zero_shot_or_few_shot": "Base VILA showed limited zero-shot path generation capability without finetuning; finetuned VILA transfers zero-shot to unseen real-world deployments (VLM had no in-domain data but produced useful paths).",
            "layer_analysis": "No layer-wise ablations reported.",
            "negative_transfer_evidence": "Not reported specifically for VILA; the paper notes monolithic VLA approaches had limited benefit from RLBench sim data, whereas VILA fine-tuned as HAMSTER's high-level did benefit from inclusion of simulated path data.",
            "comparison_to_vision_only": "Not directly compared to vision-only pretrained backbones; comparisons are against monolithic VLAs and non-VLM 3D policies.",
            "temporal_dynamics": "No representational temporal dynamics analyses provided.",
            "dimensionality_analysis": "None reported.",
            "uuid": "e1928.1"
        },
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (monolithic vision-language-action baseline)",
            "brief_description": "A state-of-the-art monolithic vision-language-action model baseline that is finetuned end-to-end to produce robot actions directly from images and language; used for comparison to HAMSTER.",
            "citation_title": "Openvla: An open-source vision-language-action model.",
            "mention_or_use": "use",
            "model_name": "OpenVLA",
            "model_description": "Monolithic VLA that takes images and language and produces action tokens or trajectories directly (end-to-end); in paper used as a baseline and finetuned on in-domain on-robot data (parameter-efficient LoRA fine-tuning for experiments).",
            "pretraining_type": "vision-language pretraining (VLM-based) used as base for VLA; then finetuned to predict actions on robot datasets",
            "pretraining_data_description": "Pretrained on multimodal datasets as a VLM backbone (not exhaustively specified in this paper); finetuned on robot on-robot datasets for action prediction.",
            "target_task_name": "robotic manipulation (same real-world tabletop tasks used for HAMSTER)",
            "target_task_description": "Real-world pick-and-place and other tabletop tasks; monolithic action-predicting model that must infer both high-level planning and low-level control at inference frequency.",
            "semantic_alignment": "OpenVLA requires fine-tuning on in-domain action-labeled robot data to align actions with perceptions; the paper reports limited transfer benefit from RLBench sim data to OpenVLA.",
            "performance_with_language_pretraining": "When finetuned on in-domain teleoperation data, OpenVLA achieves moderate performance; in a small real pick-and-place experiment RLBench-finetuned OpenVLA averaged success 0.54 vs 0.58 for model without RLBench fine-tuning, indicating limited gains from sim data. Table 7 aggregated task numbers show OpenVLA pick-and-place success ~0.46.",
            "performance_without_language_pretraining": "Paper does not report random-initialized OpenVLA baselines; the key contrast is that monolithic VLAs require in-domain action data and obtain worse generalization across axes vs HAMSTER.",
            "sample_efficiency_comparison": "No direct sample-efficiency numbers besides statements that monolithic VLAs need substantial in-domain on-robot action-labeled data and are constrained by inference speed; HAMSTER is presented as more sample efficient in practice (empirical comparisons show HAMSTER outperforms OpenVLA with same/smaller in-domain datasets).",
            "has_sample_efficiency_data": true,
            "attention_analysis": "Not reported.",
            "embedding_space_analysis": "Not reported.",
            "action_grounding_evidence": "Not presented in-depth; the paper argues monolithic VLAs struggle to ground actions across sim→real gaps and require costly on-robot data to learn action grounding.",
            "hierarchical_features_evidence": "Monolithic approach conflates semantic and low-level control; authors argue this limits transfer and sample efficiency, but no explicit feature analysis is shown.",
            "transfer_conditions": "OpenVLA showed little benefit from RLBench simulation finetuning—suggesting monolithic VLAs are sensitive to action/observation mismatch between source sim data and target real robot data.",
            "novel_vs_familiar_objects": "Paper reports poorer generalization on axes of variation compared to HAMSTER (OpenVLA performs worse on unseen objects/visual variations), but exact per-condition numbers are in appendices.",
            "zero_shot_or_few_shot": "OpenVLA showed poor zero-shot generalization and was finetuned in-domain for fair comparison.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Yes—finetuning OpenVLA on RLBench provided little improvement (0.54 vs 0.58) and overall OpenVLA underperformed HAMSTER across generalization axes, indicating possible negative or negligible transfer from off-domain simulation data for monolithic action-predicting VLAs.",
            "comparison_to_vision_only": "Compared vs non-VLM 3D policies and HAMSTER; OpenVLA underperforms HAMSTER and sometimes underperforms strong 3D policies depending on conditions.",
            "temporal_dynamics": "Not analyzed.",
            "dimensionality_analysis": "Not analyzed.",
            "uuid": "e1928.2"
        },
        {
            "name_short": "RT-Trajectory (GPT-4o baselines)",
            "name_full": "RT-Trajectory (trajectory sketch generation baselines using GPT-4o and Code-as-Policies)",
            "brief_description": "Baselines that generate 2D trajectory sketches via prompting large closed-source LLM/VLMs (GPT-4o), either directly or using Code-as-Policies, used for comparison to HAMSTER's finetuned VLM trajectory generation.",
            "citation_title": "RT-trajectory: Robotic task generalization via hindsight trajectory sketches.",
            "mention_or_use": "use",
            "model_name": "RT-Trajectory (GPT-4o / Code-as-Policies)",
            "model_description": "Prompt-based trajectory sketch generators built on large closed-source models (GPT-4o) that output keypoints / trajectories from an image + scene description; two variants evaluated: zero-shot GPT-4o prompt and GPT-4o + Code-as-Policies (CaP) pipeline using open-vocabulary detections.",
            "pretraining_type": "text and possibly vision-capable large models (closed-source LLM/VLM like GPT-4o) pretrained on massive internet text and multimodal data (when vision-enabled); used zero-shot without task-specific finetuning here.",
            "pretraining_data_description": "Pretraining corpora are not detailed in the paper; GPT-4o-style closed-source models are known to be trained on large text corpora and (if vision-enabled) image-text pairs; no supervised trajectory sketch finetuning was used for these baselines in this paper.",
            "target_task_name": "trajectory sketch generation for robotic manipulation (2D keypoints for end-effector motion)",
            "target_task_description": "Given an image and instruction, produce a short sequence of 2D points and gripper actions to indicate a feasible manipulation trajectory; these trajectories are then evaluated qualitatively by human rankings and as path guidance in RT-Trajectory baseline experiments.",
            "semantic_alignment": "Zero-shot RT-Trajectory provides semantic reasoning from large-language knowledge but lacks explicit grounding to robotic end-effector coordinate spaces unless augmented with object detections; Code-as-Policies variant uses open-vocabulary detectors to provide scene object lists to the LLM to improve alignment.",
            "performance_with_language_pretraining": "Behavioral comparisons show RT-Trajectory zero-shot GPT-4o and CaP variants underperform HAMSTER's finetuned VLM in human trajectory rankings and downstream performance; human ranking experiments in the paper show HAMSTER variants (especially including simulation data) rank better, and Table 6 indicates HAMSTER outperforms zero-shot RT-Trajectory baselines.",
            "performance_without_language_pretraining": "Not applicable—the baselines are zero-shot LLM usage (no task-specific finetuning).",
            "sample_efficiency_comparison": "RT-Trajectory is zero-shot and does not require demonstrations, but its generated trajectories are less accurate and less useful downstream compared to HAMSTER's finetuned VLM, per human rankings and downstream results.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "No internal attention analyses reported for GPT-4o baselines.",
            "embedding_space_analysis": "No.",
            "action_grounding_evidence": "Limited—zero-shot LLMs produce plausible semantic plans but lack the concrete spatial grounding (end-effector coordinates aligned to camera/projective geometry) that finetuned VLM trajectory sketches provide; authors report these baselines underperform finetuned HAMSTER VLM.",
            "hierarchical_features_evidence": "Not analyzed.",
            "transfer_conditions": "Zero-shot RT-Trajectory is sensitive to the quality of scene descriptions and open-vocabulary detections; augmenting with CaP helps but still lags behind finetuned path predictors.",
            "novel_vs_familiar_objects": "Human ranking experiments include diverse tasks; RT-Trajectory baselines were ranked worse on many of these out-of-domain tasks compared to finetuned HAMSTER VLM.",
            "zero_shot_or_few_shot": "Yes—these baselines are zero-shot; the paper demonstrates they underperform compared to finetuned HAMSTER VLM.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Not directly discussed beyond empirical underperformance.",
            "comparison_to_vision_only": "Not directly compared; these baselines rely on large language/vision models rather than vision-only pretrained components.",
            "temporal_dynamics": "No.",
            "dimensionality_analysis": "No.",
            "uuid": "e1928.3"
        },
        {
            "name_short": "RVT-2",
            "name_full": "RVT-2 (Robotic View Transformer 2)",
            "brief_description": "A 3D-aware low-level policy architecture used as a HAMSTER low-level controller; processes 3D perceptual inputs and executes continuous actions; in HAMSTER it conditions on overlaid or concatenated 2D path channels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RVT-2",
            "model_description": "A small, specialist 3D policy model that takes RGB + pointcloud/depth and (optionally) concatenated path channels and outputs continuous robot actions; includes virtual reprojection steps and is used for precise manipulation trained via imitation learning.",
            "pretraining_type": "not described as large-scale pretraining; treated as a specialist imitation-learning policy (trained on task-specific teleoperation data).",
            "pretraining_data_description": "Trained on in-domain teleoperated demonstrations collected for the target tasks (in the paper: 320 episodes for real-world experiments, and dataset variants in simulation).",
            "target_task_name": "robotic manipulation low-level control (pick-and-place, button press, knock down, etc.)",
            "target_task_description": "Operates at controller frequency to follow 2D path guidance and produce continuous actions using 3D perception and proprioception; tested in both real-world and simulation.",
            "semantic_alignment": "RVT-2 relies on the HAMSTER 2D path to encode high-level semantics; without path conditioning it performs worse on semantic generalization axes.",
            "performance_with_language_pretraining": "When conditioned on HAMSTER's finetuned VLM 2D paths, RVT-2 performance increases drastically (e.g., pick-and-place aggregated success: RVT-2 baseline 0.28 vs HAMSTER+RVT2 0.79 in Table 7; Table 1 shows HAMSTER+RVT2(Concat) achieving near 1.0 on some Colosseum camera tests).",
            "performance_without_language_pretraining": "RVT-2 trained without path guidance yields substantially lower success (e.g., RVT-2 baseline 0.28 pick-and-place aggregated).",
            "sample_efficiency_comparison": "With HAMSTER path guidance, RVT-2 achieves higher success with the same demonstration budget (320 episodes); Colosseum experiments indicate HAMSTER variants can reach high performance using fewer demonstrations (50% data) than vanilla 3D policies.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No internal attention maps for RVT-2 are reported; behavioral analysis shows virtual reprojection can fragment drawn paths and affect adherence.",
            "embedding_space_analysis": "No.",
            "action_grounding_evidence": "Indirect: conditioning RVT-2 on explicit 2D path guidance links high-level semantic plans to low-level continuous motor outputs and improves successful interaction with target objects; failure analysis indicates that many RVT-2 failures are due to poor adherence to predicted trajectories (72% of RVT failures), underscoring action-grounding challenges.",
            "hierarchical_features_evidence": "Behaviorally, RVT-2 benefits from high-level semantic guidance and focuses on local geometric control; path-concatenation (6-channel input) preserves path signal better than overlay and yields best performance in some settings.",
            "transfer_conditions": "RVT-2 with path guidance is robust to camera angle changes and visual variations; virtual reprojection specifics affect path decoding fidelity.",
            "novel_vs_familiar_objects": "RVT-2 benefits substantially from VLM-guided trajectories when encountering novel objects/visual variations; exact per-object deltas not fully enumerated.",
            "zero_shot_or_few_shot": "RVT-2 itself is trained via imitation (not zero-shot), but when paired with zero-shot-capable finetuned VLM paths it generalizes better to new scenes.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "RVT-2 can suffer from fragmented path interpretations due to reprojection, harming adherence; no large negative transfer reported.",
            "comparison_to_vision_only": "RVT-2 is a 3D perception policy (vision+depth) and benefits from language-guided path inputs vs being purely vision-only.",
            "temporal_dynamics": "No representational temporal analysis; failure analysis shows many failures arise from trajectory adherence during execution.",
            "dimensionality_analysis": "No.",
            "uuid": "e1928.4"
        },
        {
            "name_short": "3D-DA",
            "name_full": "3D Diffuser Actor (3D-DA)",
            "brief_description": "A state-of-the-art 3D-aware low-level policy (policy diffusion actor) used in HAMSTER as an alternative low-level controller that consumes path-drawn images and 3D inputs to produce continuous actions.",
            "citation_title": "3d diffuser actor: Policy diffusion with 3d scene representations.",
            "mention_or_use": "use",
            "model_name": "3D-DA (3D Diffuser Actor)",
            "model_description": "A policy diffusion architecture that conditions on 3D scene representations and language; used as a low-level policy in HAMSTER to follow VLM-generated 2D paths and generate actions for manipulation tasks.",
            "pretraining_type": "not described as large-scale pretraining in this paper; used as an imitation-learned policy trained on collected teleoperation demonstrations (and/or simulated data) for downstream tasks.",
            "pretraining_data_description": "Trained on in-domain teleoperation demonstrations (320 episodes for real-world experiments) and evaluated in simulation (RLBench / Colosseum).",
            "target_task_name": "robotic manipulation low-level control (same tasks as HAMSTER evaluation)",
            "target_task_description": "Uses 3D scene representations and language to issue continuous control actions; in HAMSTER it takes the VLM path overlay/concatenated channel as guidance.",
            "semantic_alignment": "3D-DA leverages high-level path sketches to focus on geometric control; when provided HAMSTER paths, it shows improved robustness to visual and semantic variation.",
            "performance_with_language_pretraining": "HAMSTER+3D-DA shows substantial gains over vanilla 3D-DA: simulation average improvement of ~31% (Colosseum), and real-world aggregated task success (Table 7) e.g., pick-and-place HAMSTER+3DDA = 0.78 vs 3D-DA baseline 0.19.",
            "performance_without_language_pretraining": "3D-DA baseline without HAMSTER path guidance: lower success (e.g., aggregated pick-and-place 0.19).",
            "sample_efficiency_comparison": "HAMSTER+3D-DA with 50% of data reaches much higher success (2x) compared to full-data vanilla 3D-DA, indicating improved demonstration efficiency when conditioned on VLM-generated paths.",
            "has_sample_efficiency_data": true,
            "attention_analysis": "No internal attention or diffusion-step attention analyses reported.",
            "embedding_space_analysis": "No.",
            "action_grounding_evidence": "Behavioral: most 3D-DA failures are attributed to execution-specific issues rather than trajectory adherence (paper reports 90% of 3D-DA failures from execution), implying that VLM path guidance helps disambiguate semantics but motor/execution details remain critical.",
            "hierarchical_features_evidence": "Empirical results indicate 3D-DA benefits from decoupled high-level VLM guidance for semantic reasoning while 3D-DA focuses on precise 3D control.",
            "transfer_conditions": "Performs better when language guidance is simplified (real-world) or detailed (simulation), depending on the model's attention mechanism; benefits from path concatenation where supported.",
            "novel_vs_familiar_objects": "HAMSTER+3D-DA shows improved generalization to novel objects and semantics compared to 3D-DA alone (aggregate metrics reported), but per-object breakdowns are in appendices.",
            "zero_shot_or_few_shot": "3D-DA is trained via imitation: not zero-shot; benefits from VLM zero-shot transfer to provide guidance at test time.",
            "layer_analysis": "No.",
            "negative_transfer_evidence": "Not explicitly reported for 3D-DA; some failure modes are execution-level rather than transfer-level.",
            "comparison_to_vision_only": "3D-DA is a 3D-aware policy (vision+depth); pairing with VLM path guidance outperforms 3D-DA alone.",
            "temporal_dynamics": "No representational temporal analysis; failures categorized between trajectory adherence and execution.",
            "dimensionality_analysis": "No.",
            "uuid": "e1928.5"
        },
        {
            "name_short": "LLARVA",
            "name_full": "LLARVA (monolithic VLA with auxiliary trajectory task)",
            "brief_description": "A monolithic vision-language-action model that predicts end-effector trajectories as an auxiliary task while primarily trained to predict actions; mentioned as a related monolithic approach.",
            "citation_title": "LLARVA: Vision-action instruction tuning enhances robot learning.",
            "mention_or_use": "mention",
            "model_name": "LLARVA",
            "model_description": "Monolithic VLA architecture that predicts both actions and auxiliary trajectories from image + language in an end-to-end manner (mentioned in related work as a contrasting monolithic VLA approach).",
            "pretraining_type": "built from VLMs and finetuned on robot action datasets (monolithic finetuning)",
            "pretraining_data_description": "Trained on on-robot demonstrations and standard VLM pretraining; exact dataset details are in LLARVA original paper (cited).",
            "target_task_name": "robotic manipulation / action prediction",
            "target_task_description": "End-to-end action generation conditioned on images and language; cited to contrast HAMSTER's hierarchical approach.",
            "semantic_alignment": "Mentioned in relation as monolithic methods that use trajectory prediction as auxiliary signal; not analyzed here.",
            "performance_with_language_pretraining": "Mentioned but not evaluated in this paper.",
            "performance_without_language_pretraining": "Not provided here.",
            "sample_efficiency_comparison": "Not provided in this paper.",
            "has_sample_efficiency_data": false,
            "attention_analysis": "Not in this paper.",
            "embedding_space_analysis": "Not in this paper.",
            "action_grounding_evidence": "Cited as predicting trajectories auxiliary to action prediction, but no empirical details in this paper.",
            "hierarchical_features_evidence": "Paper references LLARVA to emphasize differences: LLARVA uses auxiliary trajectories but remains monolithic rather than hierarchical decoupling.",
            "transfer_conditions": "Not discussed in this paper.",
            "novel_vs_familiar_objects": "Not discussed in this paper.",
            "zero_shot_or_few_shot": "Not discussed here.",
            "layer_analysis": "Not provided.",
            "negative_transfer_evidence": "Not provided here.",
            "comparison_to_vision_only": "Not provided.",
            "uuid": "e1928.6"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RT-trajectory: Robotic task generalization via hindsight trajectory sketches.",
            "rating": 2
        },
        {
            "paper_title": "Openvla: An open-source vision-language-action model.",
            "rating": 2
        },
        {
            "paper_title": "VILA: On pretraining for visual language models.",
            "rating": 2
        },
        {
            "paper_title": "Robopoint: A vision-language model for spatial affordance prediction in robotics.",
            "rating": 2
        },
        {
            "paper_title": "RLBench: The robot learning benchmark & learning environment.",
            "rating": 2
        },
        {
            "paper_title": "RT-1: Robotics transformer for real-world control at scale.",
            "rating": 1
        },
        {
            "paper_title": "RT-2: Vision-language-action models transfer web knowledge to robotic control.",
            "rating": 2
        },
        {
            "paper_title": "Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation.",
            "rating": 1
        },
        {
            "paper_title": "DROID: A large-scale in-the-wild robot manipulation dataset",
            "rating": 2
        },
        {
            "paper_title": "Bridge dataset",
            "rating": 1
        }
    ],
    "cost": 0.02708775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>HAMSTER: HIERARCHICAL ACTION MODELS FOR OPEN-WORLD ROBOT MANIPULATION
10 May 2025</p>
<p>Yi Li 
⋆ ‡1 
University of Washington</p>
<p>Yuquan Deng 
University of Washington</p>
<p>Jesse Zhang 
University of Southern California</p>
<p>Joel Jang 
University of Washington</p>
<p>Marius Memmel 
University of Washington</p>
<p>Raymond Yu 
University of Washington</p>
<p>Caelan Garrett 
Fabio Ramos 
Dieter Fox 
University of Washington</p>
<p>Anqi Li 
Abhishek Gupta 
University of Washington</p>
<p>Ankit Goyal 
Nvidia 
HAMSTER: HIERARCHICAL ACTION MODELS FOR OPEN-WORLD ROBOT MANIPULATION
10 May 20257FF389CFC485AE51D52327B1FE86FC08arXiv:2502.05485v4[cs.RO]
Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics.One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation.A promising remedy is to leverage cheaper, "off-domain" data such as action-free videos, handdrawn sketches or simulation data.In this work, we posit that hierarchical visionlanguage-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions.In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description.The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation.Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning.We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc.In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain.Visual results are provided at: https://hamster-robot.github.io/</p>
<p>INTRODUCTION</p>
<p>Developing general robot manipulation policies has been notoriously difficult.With the advent of large vision-language models (VLMs) that display compelling generalization capabilities, there is optimism that the same recipe is directly applicable to robot manipulation.A line of prior work (Brohan et al., 2023a;Kim et al., 2024;Black et al., 2024) builds open-world vision-language-action models (VLAs) by finetuning off-the-shelf pretrained VLMs to directly produce robot actions.These VLA models, which we refer to in this work as monolithic VLA models, rely crucially on large robotics datasets, complete with on-robot observations, e.g., images and proprioceptive states, and actions.However, on-robot data is expensive, since end-to-end observation-action pairs are typically collected on the robot hardware through, e.g., teleoperation.Despite recent community-wide efforts in building large-scale robotics datasets (Collaboration et al., 2023;Khazatsky et al., 2024), the size, quality, and diversity of existing robotics datasets are still limited, and monolithic VLA models have yet to demonstrate emergent capability comparable to VLMs and LLMs in other domains of study.Moreover, monolithic VLA models are constrained by their inference frequency to achieve dexterous and dynamic manipulation tasks (Brohan et al., 2023a;Kim et al., 2024).</p>
<p>On the other hand, relatively small robot policy models have shown impressive dexterity and robustness.Such models have demonstrated promise across a range of complex tasks involving contactrich manipulation and 3D reasoning, spanning domains from tabletop manipulation (Shridhar et al.,  2023; Goyal et al., 2023;2024;Ke et al., 2024) to fine dexterous manipulation (Chi et al., 2023;Zhao et al., 2023).Trained on relatively small datasets, these models show local robustness, and can achieve dexterous and high-precision control.However, they are often brittle to drastic changes in the environment or semantic description of the tasks (Pumacay et al., 2024).These models also can struggle to effectively leverage simulation data for real-world manipulation tasks due to sim-to-real gaps in visual appearances and system dynamics (Li et al., 2024;Mandlekar et al., 2021).</p>
<p>In this work, we ask -how can we marry the generalization benefits of large VLMs, with the efficiency, local robustness, and dexterity of small policy models?Our key insight is that, instead of directly predicting robot actions, VLMs can be fine-tuned to produce intermediate representations as high-level guidance on solving the robot manipulation task.The intermediate representation can then be consumed by the low-level policy model to produce actions, alleviating the low-level policy from the burden of long-horizon planning and complex, semantic reasoning.Further, if the intermediate representations are chosen such that they are 1) easily obtainable from image sequences;</p>
<p>2) largely embodiment agnostic; and 3) sufficiently robust to subtle changes in dynamics, the VLM can be fine-tuned with off-domain data where robot actions are unavailable or inaccurate.Such offdomain data does not need to be collected on the actual robot hardware.Examples of off-domain data include action-free video data, simulation data, human videos, and videos of robot with different embodiments.These off-domain data are generally easier to collect and may already be abundant in existing datasets.We hypothesize, and show experimentally in Fig 7, that this hierarchical separation can allow VLA models to more effectively bridge the domain gap between off-domain data and in-domain robotic manipulation.</p>
<p>To this end, we propose a hierarchical architecture for VLAs, HAMSTER (Hierarchical Action Models with SeparaTEd Path Representations), where large fine-tuned VLMs are connected to lowlevel policy models via 2D path representations1 .A 2D path is a coarse trajectory of the 2D imageplane position of the robot end-effector2 , as well as where the gripper state changes, i.e., opens and closes (see Fig. 2).These 2D paths can be obtained cheaply and automatically from data sources such as action-free videos or physics simulations, using point tracking (Doersch et al., 2023;Karaev et al., 2025), hand-sketching (Gu et al., 2023), or proprioceptive projection.This allows HAM-STER can effectively leverage these abundant and inexpensive off-domain data when fine-tuning the high-level VLM.The hierarchical design presented in HAMSTER also offers additional advantages through the decoupling of VLM training and low-level action prediction.Specifically, while the higher-level VLM is predicting semantically meaningful trajectories from monocular RGB camera inputs, the lower-level policy models can additionally operate from rich 3D and proprioceptive inputs.In doing so, HAMSTER inherits the semantic reasoning benefits of VLMs along with the 3D reasoning and spatial awareness benefits of 3D policy models (Goyal et al., 2024;Ke et al., 2024).Moreover, the high-level VLM and low-level policy model can be queried at different frequencies In summary, we study a family of hierarchical VLA models HAMSTERs, where finetuned VLMs are connected to low-level 3D policy models (Goyal et al., 2024;Ke et al., 2024).The 2D paths produced by high-level VLMs serve as guidance for a low-level policy that operates on rich 3D and proprioceptive inputs, allowing low-level policies to focus on robustly generating precise, spatiallyaware actions.In our experiments, we observe an average of 20% improvement in success rate over seven different axes of generalization over OpenVLA (Kim et al., 2024), which amounts to 50% relative gain, as shown in Table 7.Since HAMSTER is built on both open-source VLMs and low-level policies, it can serve as a fully open-sourced enabler for the community-building visionlanguage-action models.It is important to note that while we are certainly not the first to propose hierarchical VLA models (Gu et al., 2023;Nasiriany et al., 2024a), we propose the novel insight that this type of hierarchical decomposition allows for these models to make use of abundant offdomain data for improving real-world control.This opens the door to alternative ways of training large vision-language-action models using cheaper and more abundant data sources.</p>
<p>RELATED WORK</p>
<p>LLMs and VLMs for robotics.Early attempts in leveraging LLMs and VLMs for robotics are through pretrained language (Jang et al., 2022;Shridhar et al., 2023;Singh et al., 2023) andvisual (Shah &amp;Kumar, 2021;Parisi et al., 2022;Nair et al., 2023;Ma et al., 2023) representations.However, these are insufficient for complex semantic reasoning and generalization to the open world (Brohan et al., 2022;Zitkovich et al., 2023).Recent research has focused on directly leveraging open world reasoning and generalization capability of LLMs and VLMs, by prompting or fine-tuning them to, e.g., generate plans (Duan et al., 2024;Huang et al., 2023b;Lin et al., 2023;Liang et al., 2023;Singh et al., 2023;Brohan et al., 2023b) or construct value (Huang et al., 2023a) and reward functions (Kwon et al., 2023;Sontakke et al., 2023;Yu et al., 2023b;Ma et al., 2024;Wang et al., 2024).Our work is more closely related to VLA models, summarized below.</p>
<p>Monolithic VLA models as language-conditioned robot policies.Monolithic VLA models have been proposed to produce robot actions given task description and image observations directly (Brohan et al., 2022;Jiang et al., 2023;Zitkovich et al., 2023;Team et al., 2024;Kim et al., 2024;Radosavovic et al., 2023).Monolithic VLA models are often constructed from VLMs (Liu et al., 2024d;Bai et al., 2023;Driess et al., 2023;Lin et al., 2024), and are trained on large-scale on-robot data (Brohan et al., 2022;Collaboration et al., 2023;Khazatsky et al., 2024) to predict actions as text or special tokens.However, due to the lack of coverage in existing robotics datasets, they must be finetuned in-domain on expensive on-robot data.Their action frequency is also constrained by inference frequency, limiting their capability to achieve dexterous and dynamic tasks.The most relevant monolithic VLA model to our work is LLARVA (Niu et al., 2024), which predicts end-effector trajectories in addition to robot actions.However, LLARVA only uses trajectory prediction as an auxiliary task to improve the action prediction of a monolithic VLA model.In contrast, our work takes a hierarchical approach, enabling us to use specialist lower-level policies that take in additional inputs the VLMs cannot support, such as 3D pointclouds, to enable better imitation learning.Our predicted paths then enable these lower-level policies to generalize more effectively.</p>
<p>VLMs for predicting intermediate representations.Our work bears connections to prior methods using vision-language models to predict intermediate representations.These methods can be categorized by the choice of predicted representations:</p>
<p>Point-based predictions: A common intermediate prediction interface has been keypoint affordances (Stone et al., 2023;Sundaresan et al., 2023;Nasiriany et al., 2024b;Yuan et al., 2024b;Kuang et al., 2024).Keypoint affordances can be obtained through using open-vocabulary detectors (Minderer et al., 2022), iterative prompting of VLMs (Nasiriany et al., 2024b), or fine-tuning detectors to identify certain parts of an object by semantics (Sundaresan et al., 2023).Perhaps most related to our work, Yuan et al. (2024b) finetune a VLM to predict objects of interest as well as free space for placing an object, and Liu et al. (2024b) propose a mark-based visual prompting procedure to predict keypoint affordances as well as a fixed number of waypoints.As opposed to these, our work finetunes a VLM model to not just predict points but rather entire 2D paths, making it more broadly applicable across robotic tasks.</p>
<p>Trajectory-based predictions: The idea of using trajectory-based task specifications to condition low-level policies was proposed in RT-trajectory (Gu et al., 2023), largely from the perspective of flexible task specification.This work also briefly discusses the possibility of combining trajectoryconditioned model with trajectory sketches generated by a pre-trained VLM.Complementary to RT-Trajectory, the focus of this work is less on the use of trajectory sketches for task specification, but rather a hierarchical design of VLAs such that the high-level VLM can be fine-tuned with relative cheap and abundant data sources.This could include data such as action-free videos, or simulation data that look very different from the real world.We show that the emergent generalization capability of VLMs from its web-scale pretraining allows it transfer to test scenarios of interest with considerable visual and semantic variations.While RT-trajectory uses human effort or off-the-shelf pre-trained VLMs to generate trajectories, we show that fine-tuning VLM models on cheap data sources can generate significantly more accurate and generalizable trajectories (see Table .6).Moreover, our instantiation of this architecture enables the incorporation of rich 3D and proprioceptive information, as compared to monocular 2D policies (Gu et al., 2023).</p>
<p>Similarly, the emergence of track-any-point (TAP) models (Doersch et al., 2023;Wang et al., 2023) has enabled policies conditioned on object trajectories (Yuan et al., 2024a;Xu et al., 2024;Bharadhwaj et al., 2024) or points sampled from a fixed grid in the image (Wen et al., 2023).While our current formulation focuses on end-effector trajectories, this framework can naturally extend to predicting object trajectories or other motion cues.By leveraging the predictive capabilities of VLMs, such an extension could further enhance the model's ability to generalize across diverse scenarios and improve its capacity for fine-grained motion reasoning.</p>
<p>Leveraging simulation data for training robot policies.There has been extensive work on leveraging simulation for robot learning.Simulation data is popular in reinforcement learning (RL), as RL on real robotic systems is often impractical due to high sample complexity and safety concerns (Lee et al., 2020;Handa et al., 2023;Torne et al., 2024).Recently, simulation has been also exploited to directly generate (Fishman et al., 2022) or bootstrap (Mandlekar et al., 2023) large-scale datasets for imitation learning, to reduce the amount of expensive robot teleoperation data needed.</p>
<p>Our work takes a different approach -using simulation data to finetune a VLM, and showing that VLM is able to transfer the knowledge learned from simulation data to real robot systems, despite considerable visual differences.A related observation is recently made by (Yuan et al., 2024b), but they use keypoint affordances as the interface between the VLM and the low-level policy as opposed to more general expressive 2D path representations.</p>
<p>BACKGROUND</p>
<p>Imitation Learning via Supervised Learning.Imitation learning trains a policy π θ (a | s, o, z) from expert demonstrations, where s denotes proprioceptive inputs, o includes perceptual observations (e.g., RGB images, depth), and z provides task instructions.Given an expert dataset
D = {(s i , o i , z i , a i )} N i=1
, the policy is optimized via maximum likelihood estimation, maximizing
E (si,oi,zi,ai)∼D [log π θ (a i | s i , o i , z i )].
Despite advancements in architectures such as 3D policy representations (Goyal et al., 2023;Ke et al., 2024), generalizing to novel semantic or visual variations remains challenging.In this paper, we explore how VLMs can enhance imitation learning models for better generalization.</p>
<p>Vision-Language Models.VLMs (Liu et al., 2024a;Lin et al., 2024;Liu et al., 2024d) are large transformer models (Vaswani et al., 2023) that accept both vision and text tokens to generate text responses.They are pre-trained on extensive multimodal datasets (Zhu et al., 2023;Byeon et al., 2022) and later fine-tuned on high-quality, task-specific data (Shen et al., 2021;Lu et al., 2022b).By tokenizing each modality into a shared space, these models autoregressively produce sequences of text tokens conditioned on an image and prior tokens.In our work, we assume access to such a pretrained, text-and-image VLM (Lin et al., 2024;Liu et al., 2024d), further fine-tuned via a supervised loss that minimizes the negative log-likelihood of the target tokens.</p>
<p>HAMSTER: HIERARCHICAL ACTION MODELS FOR ROBOTIC LEARNING</p>
<p>In this work, we examine how VLA models can leverage relatively abundant data and demonstrate cross-domain transfer capabilities, as opposed to relying purely on expensive observation-language-  The low-level policy is conditioned on the 2D path and interacts with the environment sequentially to execute low-level actions.The path predicted by the VLM enhances the low-level policy generalization capability.</p>
<p>action data collected on a robot.HAMSTER is a family of hierarchical VLA models designed for this purpose, exhibiting generalizable and robust manipulation.It consists of two interconnected models: first, a higher-level VLM that is finetuned on large-scale, off-domain data to produce intermediate 2D path guidance (detailed in Section 4.1), and second, a low-level policy that produces actions conditioned on 2D paths (detailed in Section 4.2).</p>
<p>The primary advantages of finetuning such a hierarchical VLM that produces intermediate representations as opposed to directly producing actions a with a monolithic model (Kim et al., 2024;Zitkovich et al., 2023;Black et al., 2024) are threefold: 1) our hierarchical VLM can leverage offdomain datasets lack of precise actions, e.g., simulation and videos; 2) we find empirically that hierarchical VLMs producing 2D paths generalize more effectively cross-domain than monolithic VLA models; and 3) the hierarchical design provides more flexibility on the sensory modality, and allows for asynchronous query of large high-level VLA models and small low-level policy models.Although, any pretrained text-and-image-input VLM (Lin et al., 2024;Liu et al., 2024d;Achiam et al., 2023) can be used to predict such a 2D path by casting an appropriate prompt, we find that pretrained VLMs struggle with predicting such a path in a zero-shot manner (see Table 6).Therefore, we finetune pre-trained VLMs on datasets that ground VLMs to robot scenes and path predictions collected from easier-to-obtain sources, i.e., internet visual-question-answering data, robot data from other modalities, and simulation data.This is in contrast to work such as Gu et al. (2023), where pre-trained VLMs are tasked with directly performing spatially relevant path generation.</p>
<p>We use VILA-1.5-13b(Lin et al., 2024) as our base VLM, a 13-billion-parameter vision language model trained on interleaved image-text datasets and video captioning data.Although it is possible to curate a dataset on path prediction {(img i , z i , p i )} i and train the VLM only on the dataset, the literature (Brohan et al., 2023a;Yuan et al., 2024b) has shown that co-training the VLM on a variety of relevant tasks, all framed as VQA tasks, can help retain the VLM's generalization capability.To this end, we curate a multi-domain dataset to finetune this model for effective 2D path prediction.</p>
<p>FINETUNING OBJECTIVE AND DATASETS.</p>
<p>Predicting the 2D path of the end-effector requires understanding what objects to manipulate in a given task in terms of their pixel positions, but also reasoning about how a robot should perform the task.To enable this understanding, we collate a diverse off-domain dataset D off from a wide range of modalities, including real-world data, visual question-answering data, and simulation data.Importantly, none of this off-domain data used to train the VLM comes from the deployment environment, thereby emphasizing generalizability.We assemble a dataset Pixel Point Prediction.For pixel point prediction, we use the RoboPoint dataset (Yuan et al., 2024b) with 770k pixel point prediction tasks, with most answers represented as a list of 2D points corresponding to locations on the image.A sample consists of a prompt z like Locate object between the marked items, an input image img and answer ans like [(0.25, 0.11), (0.22, 0.19), (0.53, 0.23)].3See the left of Figure 3 for an example.This dataset consists of data automatically generated in simulation and collected from existing real-world datasets; its diverse tasks enable the HAMSTER VLM to reason about pixel-object relationships across diverse scenes while retaining its semantic generalization capabilities.Simulated Robot Data.We additionally generate a dataset of simulated robotics tasks from RL-Bench (James et al., 2020), a simulator of a Franka robot performing tabletop manipulation for a wide array of both prehensile and non-prehensile tasks.We use the simulator's built-in planning algorithms to automatically generate successful manipulation trajectories.Given a trajectory, we use the first frame from the front camera as the image input img.We construct prompt z to instruct the VLM to provide a sequence of points denoting the trajectory of the robot gripper to achieve the given language instruction (see Figure 2).The ground-truth 2D path p = [(x t , y t , gripper open t )] t is given by propriceptive projection using forward kinematics and camera parameters.
D off = {(img i , z i , ans i )} M i=1 of image inputs img i ,
We generate 1000 episodes for each of 81 robot manipulation tasks in RLBench, each episode with ∼4 language instructions, for a total of around 320k (img, z, ans) tuples, where ans = p.See the middle of Figure 3 for an example.</p>
<p>Real Robot Data.Using real robot data allows us to ensure the VLM can reason about objects and robot gripper paths when conditioned on scenes, including real robot arms.We use existing, online robot datasets not from the deployment environment to enable this VLM ability.We source 10k trajectories from the Bridge dataset (Walke et al., 2023;Collaboration et al., 2023) consisting of a WidowX arm (different embodiment from test robot) performing manipulation tasks and around 45k trajectories from DROID (Khazatsky et al., 2024).We covert both datasets to VQA dataset in as similar way as the simulated RL-Bench data, where the 2D paths are extracted from proprioception and camera parameters (see the right of Figure 3 for an example).Note that we essentially utilize the robot data as video data, where the end effector is tracked over time.In principle, this could be done with any number of point-tracking methods (Doersch et al., 2023) on raw video as well, with no action or proprioceptive labels.</p>
<p>We finetune the HAMSTER VLM on all three types of data by randomly sampling from all samples in the entire dataset with equal weight.We also include a 660k-sample VQA dataset (Liu et al., 2024c) for co-training to preserve world knowledge.We train with the standardized supervised prediction loss to maximize the log-likelihood of the answers ans:
E (img i ,zi,ansi)∼Doff log VLM (ans i | img i , z i ).
Remark.One issue with simulation and real robot data is that the extracted 2D paths p can be extremely long, e.g., exceeding one hundred steps.Since we want the HAMSTER VLM to reason at a high level instead of on the same scale as the low-level control policy, we simplify the paths p o with the Ramer-Douglas-Peucker algorithm (Ramer, 1972;Douglas &amp; Peucker, 1973) that reduces curves composed of line segments to similar curves composed of fewer points.We refer readers to Appendix G for an ablation study.</p>
<p>PATH GUIDED LOW-LEVEL POLICY LEARNING</p>
<p>The low-level policy of HAMSTER π θ (a | s, o, z, p) is conditioned on proprioceptive and perceptive observations, (optional) language instruction and, importantly, 2D path.While a low-level control policy can learn to solve the task without 2D path, the paths allow the low-level policy to forgo long-horizon and semantic reasoning and focus on local and geometric predictions to produce robot actions.As we find empirically (see Figure 4), 2D paths allow for considerably improved visual and semantic generalization of low-level policies.</p>
<p>HAMSTER's general path-conditioning framework allows lower-level policies to take in proprioceptive and perceptual (e.g., depth images) observations, that are not input to the high-level VLM.</p>
<p>We consider low-level policies based on 3D perceptual information, i.e., o = (img, pointcloud), available at test time on a robotic platform with standard depth cameras.We study two choices of policy architecture, RVT-2 (Goyal et al., 2024) and 3D-DA (Ke et al., 2024) which has shown state-of-the-art results on popular robot manipulation benchmark (James et al., 2020).</p>
<p>Conditioning on Paths.Most policy architectures use the form π θ (a | s, o, z) without 2D path inputs.One naïve option is to concatenate the path with proprioceptive or language inputs.However, because 2D paths vary in length, the architecture must handle variable-length inputs.To incorporate the 2D path p from the VLM without major modifications, we alternatively overlay the 2D path onto the image observation (Gu et al., 2023).Our implementation follows this approach by drawing colored trajectories on all images in the trajectory o 1 i , . . ., o T i : points at each (x t , y t ) are connected with line segments using a color gradient to indicate temporal progression (see Figure 2(b)), and circles mark changes in gripper status (e.g., green for closing, blue for opening).If the policy architecture allows images with more than three channels, we can also include path drawing as separate channels, instead of overlaying it on the RGB channel.We empirically study both drawing strategies, overlay and concatenating channels, in section 5.3.</p>
<p>Policy Training.To train the policy, we collect a relatively small-scale task-specific dataset
D = {(s i , o i , z i , a i )} N
i=1 on the robot hardware.During training, we use oracle 2D paths constructed by proprioception projection, similar to how the 2D paths are constructed for the VLM training data, and construct path-labeled dataset
D path = {(s i , o i , z i , p i , a i )} N i=1 .
We train a policy π θ (a | s, o, z, p) with standard supervised imitation learning objectives on D path to maximize the log-likelihood of the dataset actions: E (si,oi,zi,pi,ai)∼Dpath log π θ (a i | s i , o i , z i , p i ).For further implementation details, see Appendix B.</p>
<p>Inference Speed.Monolithic VLAs query the VLM at every action step (Kim et al., 2024;Brohan et al., 2023a), which can be very expensive with large VLMs.For example, OpenVLA's 7Bparameter VLA only runs at 6Hz on an RTX 4090 (Kim et al., 2024).Instead, HAMSTER's hierarchical design allows us to query the VLM only one or few times during an episode to generate 2D paths p that can be followed by low-level policy for multiple steps.Therefore, HAMSTER can be scaled to large VLM backbones without needing end-users to be concerned about inference speed.</p>
<p>EXPERIMENTAL EVALUATION</p>
<p>We evaluate our approach in both simulation and real-world experiments to the following key questions.Do hierarchical VLAs: Q1 Generalize behaviors to unseen scenarios with significant visual and semantic variation?Q2 Achieve stronger cross-domain generalization than monolithic architectures?Q3 Facilitate learning of non-prehensile and long-horizon tasks?Q4 Exhibit strong demonstration efficiency?Q5 Have improved visual + semantic reasoning due to hierarchy and VLM fine-tuning?</p>
<p>REAL WORLD EVALUATION ON TABLETOP MANIPULATION</p>
<p>To answer Q1, our real-world evaluation experiments aim to test the generalization capability of hierarchical VLA models across significant semantic and visual variations.In particular, we consider a variant of HAMSTER that uses a VLM (VILA-1.5-13b(Lin et al., 2024)) finetuned on the data mixture in Section 4.1 as the high-level predictor, with two low-level 3D policy architectures -RVT-2 (Goyal et al., 2024) and 3D Diffuser Actor (3D-DA) (Ke et al., 2024) as choices of the lowlevel policy, as described in Section 4.2.The low-level 3D policies are trained with 320 episodes collected via teleoperation shown in Fig. 3. Importantly, the high-level VLM has not seen any indomain data and is only finetuned on the off-domain data described in Section 4.1.This suggests that any generalization that the VLM shows result from cross-domain transfer.</p>
<p>Baseline comparisons.To answer Q2, we compare HAMSTER with a state-of-the-art monolithic VLA, OpenVLA (Kim et al., 2024) as well as non-VLM 3D policies, RVT-2 (Goyal et al., 2024) and 3D-DA (Ke et al., 2024).For fair comparison, we finetune OpenVLA on the collected in-domain data described above since OpenVLA showed poor zero-shot generalization.The 3D policy (RVT-2, 3D-DA) baselines are trained with the same teleoperation data used to train the low-level policy in HAMSTER but without the intermediate 2D path representation from HAMSTER's VLM.</p>
<p>Finetuning OpenVLA with RLBench.To ensure our method's advantage over OpenVLA (Kim et al., 2024) is not solely due to RLBench data, we fine-tuned OpenVLA on the same RLBench dataset used for HAMSTER's VLM-1,000 episodes per task across 81 tasks (using only episodes with good front-camera visibility)-until achieving over 90% token accuracy (Kim et al., 2024).</p>
<p>We then fine-tuned this model on our tasks following the procedure in Appendix C.2.In real-world pick-and-place experiments (6 trials over 6 "Basic" tasks as shown in Table 5), RLBench-finetuned OpenVLA averaged a success score of 0.54 versus 0.58 for the model without RLBench fine-tuning.This suggests that monolithic VLA architectures like OpenVLA gain little benefit from RLBench data, likely due to mismatches in action and observation spaces relative to the real-world setup.</p>
<p>Quantitative Results. Figure 4 summarizes our real-world results.To answer Q3, we evaluate across multiple task types, including 'pick and place,' and nonprehensile tasks such as 'press buttons' and 'knock down objects.'We also test generalization across various axes (Q1) -obj and goal: unseen object-goal combinations; visual: visual changes in table texture, lighting, distractor objects; language: unseen language instructions (e.g., candy → sweet object); spatial: unseen spatial object relationships in the instruction; novel object: unseen objects; and lastly, multiple: a combination of multiple variations.In total, we evaluate each model on 74 tasks for 222 total evaluations.Detailed results and the success score metric are provided in Appendix Table 5.</p>
<p>Qualitative Eval on Various Tasks.In addition to the quantitative evaluation conducted for comparison with OpenVLA, we also present qualitative results that demonstrate how HAMSTER's hierarchical structure enables low-level policy models to generalize to more complex tasks.Figure 8 illustrates the diverse tasks HAMSTER can handle, including unfolding a towel, opening and closing drawers, pressing buttons, wiping surfaces, and cleaning tables.These tasks present challenges such as varying lighting conditions, cluttered backgrounds, and semantic understanding requiring exter-    6).We ran 10 trails and report averaged success score (success) described in Table 5 and number of successful executions (complete).</p>
<p>nal world knowledge.Additionally, HAMSTER demonstrates the ability to perform long-horizon tasks-none of which are part of the in-domain training set used to train the policy model.</p>
<p>Overall, we find that HAMSTER significantly outperforms monolithic VLA models and (non-VLM) 3D policies by over 2x and 3x, respectively, on average.This is significant because this improved performance is in the face of considerable visual and semantic changes in the test setting, showing the ability of HAMSTER to generalize better than monolithic VLA models or non-VLM base models.We further group results by task type in Table 7, where we see HAMSTER outperforms OpenVLA across all task types (pick and place, press button, and knock down).See Appendix C for evaluation conditions, a task list, and other experiment details, and Appendix E for failure modes.</p>
<p>SIMULATION EVALUATION</p>
<p>Overall Results.For further investigation into Q1, Q2, and Q3, we conducted a controlled simulation evaluation using Colosseum (Pumacay et al., 2024), which provides significant visual and semantic variations across pick-place and non-prehensile tasks.Pairing our high-level VLM with the state-of-the-art 3D-DA (Ke et al., 2024) policy on RLBench, we compared HAMSTER against a vanilla 3D-DA implementation without path guidance.As shown in Table 3 over 5 seeds, HAM-STER outperforms the vanilla approach by an average of 31%.This improvement stems from training with path-drawn images, which encourages the policy to focus on the path rather than extraneous visual features, thereby enhancing robustness to visual variations.We refer readers to Pumacay et al. (2024) for details on the variations and Appendix F for further simulation experiment details.</p>
<p>HAMSTER with Fewer Demonstrations.We also test HAMSTER's ability to work well with limited demonstrations to answer Q4.We test on a subset of 5 Colosseum tasks, namely, check HAMSTER's visual spatial reasoning.The results in Table 2 show that HAMSTER significantly outperforms OpenVLA and remains robust to new camera angles, benefiting from its VLM trained on diverse off-domain tasks across various viewpoints.Additionally, we compare HAM-STER+RVT2 (Concat), where instead of overlaying the path on the input RGB image, we modify RVT-2 to accept a 6-channel input by concatenating the original RGB image with a separate RGB image containing only the drawn path.We can easily apply this due to HAMSTER's hierarchical nature.Concatenated paths actually achieve the best performance, demonstrating the effectiveness of this path representation, though it is less general and not compatible with all imitation learning policy architectures (such as 3D-DA as it uses a pre-trained image encoder expecting 3 input channels).One possible explanation is that RVT2's virtual reprojection can fragment the 2D path when it is directly drawn on the image, making it harder for RVT2 to decode.By providing a dedicated path channel (via concatenation), path guidance is preserved more effectively.VLM generalization We further demonstrate the benefit of HAMSTER's hierarchy by demonstrating that the VLM generalizes well to visually unique and semantically challenging tasks due to its off-domain fine-tuning.We visualize example HAMSTER path drawings in Figure 7, demonstrating HAMSTER's VLM itself effectively reasons semantically and visually for unseen tasks.We further investigate VLM performance in Appendix D.1, where we find that (1) HAMSTER outperforms zero-shot path generation from closed-source VLMs (Gu et al., 2023;Liang et al., 2023) and ( 2) that inclusion of simulation data improves HAMSTER's real-world performance.Both results point to the benefit of explicit hierarchy: off-domain VLM fine-tuning that improves its performance.See Appendix D.1 for further details.</p>
<p>To quantitatively investigate whether HAMSTERretains broad commonsense knowledge, we evaluate it on 15 visual-question-answering and multimodal reasoning benchmarks.As shown in Table 4, HAMSTER matches the performance of VILA1.5-13B-which is HAMSTER's base model-demonstrating that our model behaves as a general-purpose VLM rather than a narrow, domain-specific system.</p>
<p>MULTIMODAL VQA BENCHMARK PERFORMANCE</p>
<p>CONCLUSION AND LIMITATIONS</p>
<p>In summary, we study hierarchical VLA models that achieve robust generalization in robotic manipulation.We introduce HAMSTER, consisting of a finetuned VLM that accurately predicts 2D paths and a low-level policy that learns to generate actions using the 2D paths.This two-step architecture enables visual generalization and semantic reasoning across considerable domain shifts while enabling specialist policies, like ones conditioned on 3D inputs, to execute low-level actions.</p>
<p>This work represents an initial step towards developing versatile, hierarchical VLA methods.The proposed work only generates points in 2D space, without making native 3D predictions.This prevents the VLM from having true spatial 3D understanding.Moreover, the interface of just using 2D paths is a bandwidth limited one, which cannot communicate nuances such as force or rotation.</p>
<p>In the future, investigating learnable intermediate interfaces is a promising direction.Moreover, training these VLMs directly from large-scale human video datasets would also be promising.</p>
<p>A VLM FINETUNING DATASET DETAILS</p>
<p>Pixel Point Pred Data.Our point prediction dataset comes from Robopoint (Yuan et al., 2024b).770k samples in our point prediction dataset contain labels given as a set of unordered points such as p o = [(0.25,0.11), (0.22, 0.19), (0.53, 0.23)], or bounding boxes in [(cx, cy, w, h)] style.Other than that, following Robopoint (Yuan et al., 2024b), we use the VQA dataset Liu et al. (2024c) with 660k samples which answer VQA queries in natural language such as "What is the person feeding the cat?"We keep these data as is because these VQA queries are likely to benefit a VLM's semantic reasoning and visual generalization capabilities; we fine-tune HAMSTER's VLM on the entire Robopoint dataset as given.</p>
<p>Simulation Data.We selected 81 RLBench tasks out of 103 to generate data by removing tasks with poor visibility on the front cam view in RLBench.We use the first image in each episode combined with each language instruction.The final dataset contains around 320k trajectories.</p>
<p>Real Robot Data.For the Bridge (Walke et al., 2023) dataset, which only provides RGB images, we extract trajectories by iteratively estimating the extrinsic matrix for each episode.In each scene, we randomly sample a few frames and manually label the center of the gripper fingers.Using the corresponding end-effector poses, we compute the 3D-2D projection matrix with a PnP (Perspectiven-Point) approach.We then apply this projection matrix to the episodes and manually check for any misalignments between the projected gripper and the actual gripper.Episodes exhibiting significant deviations are filtered out, and a new round is started to estimate their extrinsic matrix.</p>
<p>For DROID (Khazatsky et al., 2024), a large portion of the dataset contains noisy camera extrinsics information that do not result in good depth alignment.Therefore, we filter out trajectories with poor-quality extrinsics as measured by the alignment between the projected depth images and the RGB images.This results in ∼45k trajectories (∼22k unique trajectories as trajectories each have 2 different camera viewpoints) which we use for constructing the VLM dataset D off as described in Section 4.1.</p>
<p>B IMPLEMENTATION AND ARCHITECTURE DETAILS</p>
<p>HAMSTER Prompt</p>
<p>In the image, please execute the command described in ⟨quest⟩{quest}⟨/quest⟩.Provide a sequence of points denoting trajectory of a robot gripper to achieve the goal.Format your answer as a list of tuples enclosed by ⟨ans⟩ and ⟨/ans⟩ tags.For example: ⟨ans⟩[(0.25,0.32), (0.32, 0.17), (0.13, 0.24), ⟨action⟩Open Gripper⟨/action⟩, (0.74, 0.21), ⟨action⟩Close Gripper⟨/action⟩, ...]⟨/ans⟩ The tuple denotes the x and y location of the end effector of the gripper in the image.The action tags indicate the gripper action.The coordinates should be floats ranging between 0 and 1, indicating the relative locations of the points in the image.</p>
<p>Figure 10: The full text prompt we use to train HAMSTER with on simulation and real robot data (Section 4.1).We also use this prompt for inference.</p>
<p>B.1 VLM IMPLEMENTATION DETAILS</p>
<p>VLM Prompt.We list the prompt for both fine-tuning on sim and real robot data and evaluation in Figure 10.We condition the model on an image and the prompt, except when training on Pixel Point Prediction data (i.e., from Robopoint (Yuan et al., 2024b)) where we used the given prompts from the dataset.Note that we ask the model to output gripper changes as separate language tokens, i.e., Open Gripper/Close Gripper, as opposed to as a numerical value as shown in simplified depictions like Figure 2. VLM Trajectory Processing.As mentioned in Section 4.1, one problem with directly training on the path labels p o is that many paths may be extremely long.Therefore, we simplify the paths p o with the Ramer-Douglas-Peucker algorithm (Ramer, 1972;Douglas &amp; Peucker, 1973) that reduces curves composed of line segments to similar curves composed of fewer points.We run this algorithm on paths produced by simulation and real robot data to generate the labels p o for D off .We use tolerance ϵ = 0.05, resulting in paths that are around 2-5 points for each short horizon task.</p>
<p>VLM Training Details.We train our VLM, VILA1.5-13BLin et al. (2024), on a node equipped with eight NVIDIA A100 GPUs, each utilizing approximately 65 GB of memory.The training process takes about 30 hours to complete.We use an effective batch size of 256 and a learning rate of 1 × 10 −5 .During fine-tuning, the entire model-including the vision encoder-is updated.</p>
<p>B.2 LOW-LEVEL POLICY TRAINING DETAILS</p>
<p>We train RVT2 (Goyal et al., 2024) and 3D-DA (Ke et al., 2024) as our lower-level policies.We keep overall architecture and training hyperparameters the same as paper settings.Specific details about how the inputs were modified other than the 2D path projection follow.</p>
<p>For low-level policy training, we train the policies on ground truth paths constructed by projecting trajectory end-effector points to the camera image.In order to also ensure the policies are robust to possible error introduced by HAMSTER VLM predictions during evaluation, we add a small</p>
<p>RT-Trajectory GPT-4o Prompt</p>
<p>In the image, please execute the command described in '{quest}'.Provide a sequence of keypoints denoting a trajectory of a robot gripper to achieve the goal.Keep in mind these are keypoints, so you do not need to provide too many points.Format your answer as a list of tuples enclosed by <ans> and </ans> tags.For example: <ans>[(0.25,0.32), (0.32, 0.17), (0.13, 0.24), <action>Open Gripper</action>, (0.74, 0.21), <action>Close Gripper</action>, ...]</ans> The tuple denotes point x and y location of the end effector of the gripper in the image.The action tags indicate the gripper action.The coordinates should be floats ranging between 0 and 1, indicating the relative locations of the points in the image.The current position of the robot gripper is: {current position}.Do not include this point in your answer.RVT2 (Goyal et al., 2024).We remove the language instruction for RVT-2 when conditioning on HAMSTER 2D paths.</p>
<p>3D-DA (Ke et al., 2024).In simulated experiments in Colosseum, no changes were needed.In fact, we saw a performance drop for HAMSTER+3D-DA when removing language for Colosseum tasks and a small drop in performance when using simplified language instructions.This is likely due to 3D-DA's visual attention mechanism which cross attends CLIP language token embeddings with CLIP visual features, therefore detailed language instructions are beneficial.</p>
<p>In real-world experiments, we simplify the language instruction in the same way as for RVT2 when conditioning on HAMSTER 2D paths to encourage following the trajectory more closely with limited data.In addition, we reduced the embedding dimension of the transformer to 60 from 120, removed proprioception information from past timesteps, and reduced the number of transformer heads to 6 from 12 in order to prevent overfitting.</p>
<p>C REAL WORLD EXPERIMENT DETAILS C.1 TRAINING TASKS AND DATA COLLECTION</p>
<p>For our real-world experiments, we collected all data using a Franka Panda arm through human teleoperation, following the setup described in Khazatsky et al. (2024).Below, we describe the training tasks: Pick and place.We collected 220 episodes using 10 toy objects.In most of the training data, 2 bowls were placed closer to the robot base, while 3 objects were positioned nearer to the camera.The language goal for training consistently followed the format: pick up the {object} and put it in the {container}.</p>
<p>Knock down objects.We collected 50 episodes with various objects of different sizes.Typically, 3 objects were arranged in a row, and one was knocked down.The language goal for training followed the format: push down the {object}.</p>
<p>Press button.We collected 50 episodes with 4 colored buttons.In each episode, the gripper was teleoperated to press one of the buttons.The language goal followed the format: press the {color} button.• The workspace is a 1 × 1 square centered at (0.5, 0.5)</p>
<p>• The x-axis points rightward and y-axis points downward.</p>
<p>Please write Python code that generates a list of 2D poses and gripper statuses for the robot to follow.Include Python comments explaining each step.Assume you can use numpy or standard Python libraries, just make sure to import them.</p>
<p>Enclose the start and end of the code block with <code> and </code> so that it can be parsed.Make sure that it is a self-contained script such that when executing the code string, there is a variable named robot poses which is a list of poses of the form: [(x, y, gripper), (x, y, gripper), ...].Scene Description: When training RVT2, which requires keyframes as labels, in addition to labeling frames where the gripper performs the open gripper and close gripper actions, we also included frames that capture the intermediate motion as the gripper moves toward these keyframes.
<code> {scene_description} </code></p>
<p>C.2 BASELINE TRAINING DETAILS</p>
<p>OpenVLA (Kim et al., 2024).Following Kim et al. (2024), we only utilize parameter efficient fine-tuning (LoRA) for all of our experiments, since they showed that it matches full fine-tuning performance while being much more efficient.We follow the recommended default rank of r=32.</p>
<p>We opt for the resolution of 360 x 360 to match all of the baseline model's resolutions.We also follow the recommended practice of training the model until it surpasses 95% token accuracy.However, for some fine-tuning datasets, token accuracy converged near 90%.We selected the model checkpoints when we observed that the token accuracy converged, which usually required 3,000 to 10,000 steps using a global batch size of either 16 or 32.Training was conducted with 1 or 2 A6000 gpus (which determined the global batch size of 16 or 32).Emprically, we observed that checkpoints that have converged showed very similar performance in the real world.For example, when we evaluate checkpoint that was trained for 3,000 steps and showed convergence, evaluating on a checkpoint trained for 5,000 steps of the same run resulted in a very similar performance.</p>
<p>RT-Trajectory (Gu et al., 2023).We implement two versions of RT-Trajectory for the comparison in Table 6.The first (0-shot GPT-4o) directly uses GPT-4o to generate 2D paths with a prompt very similar to the one we use for HAMSTER, displayed in Figure 11.</p>
<p>The second version implements RT-Trajectory on top of a Code-as-Policies (Liang et al., 2023), as described in RT-Trajectory.We use OWLv2 (Minderer et al., 2023) to perform open-vocabulary object detection on the image to generate a list of objects as the scene description and then prompt RT-Trajectory with the prompt shown in Figure 12.We also use GPT-4o as the backbone for this method.Table 5: Detailed results of real-world evaluation.The first column indicates the variation category, while the second column presents the language instruction.For the pick and place task, 0.25 points are awarded for each successful action: reaching the object, picking it up, moving it to the target container, and placing it inside.For the knock down task, 0.5 points are awarded for touching the correct object and successfully knocking it down.For the press button task, 0.5 points are awarded for positioning the gripper above the correct button and successfully pressing it.</p>
<p>C.3 EVALUATION TASKS</p>
<p>We evaluate our method on the tasks of pick and place, knock down object, and press button across various generalization challenges, as illustrated in Figure 4. Detailed results are available in Table 5.Following (Kim et al., 2024), we assign points for each successful sub-action.</p>
<p>For VLM, human experts are employed to assess the correctness of the predicted trajectories.</p>
<p>D EXTENDED RESULTS</p>
<p>D.1 IMPACT OF DESIGN DECISIONS ON VLM PERFORMANCE</p>
<p>To better understand the transfer and generalization performance of the proposed hierarchical VLA model, we analyze the impact of various decisions involved in training the high-level VLM.We conduct a human evaluation of different variants of a trained high-level VLM on a randomly collected dataset of real-world test images, as shown in Figure 7.We ask each model to generate 2D path traces corresponding to instructions such as "move the block on the right to Taylor Swift" or "screw the light bulb in the lamp" (the full set is in Appendix D.2).We then provide the paths generated by each method to human evaluators who have not previously seen any of the models' predictions.The human evaluators then rank the predictions for each method; we report the average rank across the samples in Table 6.</p>
<p>We evaluate the following VLM models: (1) zero-shot state-of-the-art closed-source models such as GPT-4o using a similar prompt to ours (shown in Figure 11), (2) zero-shot state-of-the-art closedsource models such as GPT-4o but using Code-as-Policies (Liang et al., 2023) to generate paths as described in Gu et al. (2023) (prompt in Figure 12), ( 3) finetuned open-source models (VILA-1.5-13b) on the data sources described in Section 4.1, but excluding the simulation trajectories from the RLBench dataset, ( 4) finetuned open-source models (VILA-1.5-13b) on the data sources described in Section 4.  13.Note that the path drawing convention in images for this experiment differ from what is given to the lower-level policies as described in Section 4.2 as this multi-colored line is easier for human evaluators to see.Due to the variety of possible trajectories that accomplish the same task, we use human rankings to compare how likely produced trajectories are to solve the task instead of quantitative metrics such as MSE.To do that, we generate trajectories for 48 image-question pairs with HAMSTER w/o RLBench, HAMSTER, Code-as-Policy (Liang et al., 2023), andGPT4o (Achiam et al., 2023).See Figure 14 for an example.</p>
<p>We recruit 5 human evaluators, who are robot learning researchers that have not seen the path outputs of HAMSTER, to grade these 4 VLMs based on the instruction: "Provide a rank for each method (1 for best and 4 for worst).In your opinion, which robot trajectory is most likely to succeed.Traj goes from blue to red, blue circle means close gripper, red circle means open gripper."The evaluators are allowed to give multiple trajectories the same score if they believe those trajectories are tied.</p>
<p>As they are robot learning researchers, they are familiar with the types of trajectories that are more likely to succeed.Therefore, these rankings act as a meaningful trajectory quality metric.This section outlines the failure modes observed during our experiments and provides a detailed breakdown of the causes.Failures can be attributed to issues in trajectory prediction, trajectory adherence, and action execution.</p>
<p>E FAILURE ANALYSIS</p>
<p>E.1 DIFFERENT FAILURE MODES</p>
<p>Trajectory Prediction Failures The Vision-Language Model (VLM) may fail to predict the correct trajectory due to several factors:</p>
<p>-Failure to understand the language goal: Although the VLM demonstrates strong capabilities in handling diverse task descriptions, it struggles when the training set lacks similar tasks.This can cause the model to misunderstand the goal and make inaccurate predictions.</p>
<p>-Incorrect trajectory prediction: In some cases, the VLM predicts an incorrect trajectory, either by interacting with the wrong objects or misinterpreting the direction of the affordance.</p>
<p>-Dynamic changes in the environment: Since trajectories are generated at the beginning of a task, significant environmental changes during execution can lead to failure.The model lacks the ability to dynamically adjust the trajectory or reidentify the object initially referenced.</p>
<p>Trajectory Adherence Failures Failures in adhering to the predicted trajectory arise primarily due to:</p>
<p>-3D ambiguity: The use of 2D trajectory predictions introduces ambiguities, such as determining whether a point is positioned above or behind an object, leading to execution errors.</p>
<p>-Incorrect object interaction: The low-level action model is not explicitly constrained to strictly follow the predicted trajectory.As a result, it may deviate, interacting with the wrong object and causing task failures.</p>
<p>Action Execution Failures Even when the trajectory is correctly predicted and adhered to, action execution may still fail due to:</p>
<p>-Execution-specific issues: Despite training on a diverse set of actions, the model may fail during execution.For example, in grasping tasks, an incorrect grasp angle can cause the object to slip, resulting in a failed grasp.</p>
<p>E.2 FAILURE ANALYSIS</p>
<p>Our analysis in Figure 15 reveals distinct failure tendencies across methods.</p>
<p>For RVT, 72% of failures stemmed from the low-level model failing to follow the trajectory, while 28% were due to execution failures.In contrast, for 3DDA, only 10% of failures were related to trajectory adherence, with 90% attributed to execution failures.</p>
<p>We hypothesize that this discrepancy arises because RVT incorporates a re-projection step, complicating trajectory adherence.In contrast, 3DDA leverages a vision tower that processes the original 2D image, simplifying trajectory interpretation.</p>
<p>F SIMULATION EXPERIMENT DETAILS Our simulation experiments are performed on Colosseum (Pumacay et al., 2024), a simulator built upon RL-Bench (James et al., 2020) containing a large number of visual and task variations to test the generalization performance of robot manipulation policies (see Figure 16 for a visualization of a subset of the variations).We use the front camera and remove all tasks in which the camera does not provide a clear view of the objects in the task, resulting in 14 out of 20 colosseum tasks (we remove basketball in hoop, empty drawer, get ice from fridge, move hanger, open drawer, turn oven on).</p>
<p>Colosseum contains 100 training episodes for each task, without any visual variations, and evaluates on 25 evaluation episodes for each variation.We follow the same procedure other than using just the front camera instead of multiple cameras.We report results in</p>
<p>G DIFFERENT WAYS OF REPRESENTING 2D PATHS</p>
<p>To investigate the effect of the number of points on the 2D path, we train the VLM to predict 1. paths simplified using RDP algorithm, which simplify paths in short horizon tasks to 3-5 points and is what we used in the paper.We denote these paths as RDP in the following; 2. Paths represented with 20 points sampled on the path with same step size, denoted as 20p in the following.We keep points where the gripper is executing operation of open or close in both methods.</p>
<p>We train the network on RLBench 80 tasks with 1000 episodes for each task and test it on 25 episodes on the task of close jar.We tried both VILA1.5-3B(denoted as 3B) and VILA1.5-13B(denoted as 13B) as our backbone.Thus we have in total 4 combinations over 2 backbones and 2 designs of path representations.We visualize the result in this Figure 17.</p>
<p>From this result we can see that when using smaller models, like VILA1.5-3B, paths represented using points extracted using RDP algorithm outperforms paths represented with a fixed number of 20 points significantly.When the network becomes larger to the level of 13B, the VLM is able to handle the representation using 20 points and both two path representations work perfectly.We believe that is because when points are simplified using the RDP algorithm, we usually need less points to represent the path and helps the model to pay more attention to predict the accurate position for the gripper open/close points.</p>
<p>Figure 1 :
1
Figure 1: Overview of HAMSTER, VLAs and "smaller" imitation learning methods.HAMSTER's hierarchical design results in better generalization with a small amount of in-domain data.HAMSTER is able to utilize cheap training sources such as videos or simulations for enhanced generalization.</p>
<p>Figure 2 :
2
Figure 2: Depiction of HAMSTER's execution.The high-level VLM is called once to generate the 2D path.</p>
<p>Figure 3 :
3
Figure 3: Off Domain Training Data: Doff contains (a) Pixel Point Prediction: 770k object location tasks from RoboPoint.(b) Simulated Robot Data: 320k 2D end-effector paths from RLBench environment.(c) Real Robot Data: 110k 2D end-effector paths from Bridge and DROID trajectories.</p>
<p>language prompts z i , and answer ans i consisting of three types of off-domain data: (1) pixel point prediction tasks (what); (2) simulated robotics tasks (what and how); (3) a real robot dataset consisting of trajectories (what and how).We detail each dataset below; see Figure 3 for visualization of each dataset's prompts and corresponding answers.</p>
<p>Figure 4: Depiction of quantitative real-world policy execution results on a real-world robot, evaluated across different axes of generalization and across both prehensile and non-prehensile tasks.Across all generalization axes, HAMSTER outperforms monolithic VLAs and the base 3D imitation learning policies.</p>
<p>Figure 5 :
5
Figure 5: Example real-world HAMSTER rollouts demonstrate its strong performance in novel scenes achieved by leveraging VLMs' generalization capabilities and the robust execution of low-level 3D policies.Method Success 3D-DA 0.18 ± 0.10 HAMSTER+3D-DA (50%) 0.36 ± 0.04 HAMSTER+3D-DA 0.43 ± 0.05</p>
<p>Figure</p>
<p>Figure 6: Camera pos.for view invariance: old (right) and new (left).</p>
<p>Figure 7 :
7
Figure 7: HAMSTER's VLM demonstrates strong generalization to unseen scenarios.From left to right: (a) leveraging world knowledge for user-specified tasks, (b) handling out-of-domain inputs like human-drawn sketches, and (c) transferring from diverse simulations to visually distinct real-world tasks.Blue-to-red lines indicate motion, with blue and red circles marking grasp and release points, respectively.</p>
<p>Figure 8 :Figure 9 :
89
Figure 8: Examples of various robot tasks and environments that HAMSTER can handle.See more details in our teaser video at https://hamster-robot.github.io/.</p>
<p>Figure 11 :
11
Figure 11: The full text prompt we use to prompt RT-Trajectory with GPT4-o.</p>
<p>RT-Trajectory Code as Policies PromptTask Instruction: {task instruction} Robot Constraints: • The robot arm takes as input 2D poses with gripper open/closing status of the form (x, y, gripper open == 1) • The gripper can open and close with only binary values</p>
<p>Figure 12 :
12
Figure 12: The full text prompt we use for RT-Trajectory with Code-as-Policies on top of GPT4-o.The scene description at the bottom comes from an open-vocabulary object detector describing each detected object and its bounding box in the image based on the task instruction.</p>
<p>Languagepick up the object with the color of sky and and put it in the container with the color of coal 1</p>
<p>Figure 13: Human VLM evaluation example images and instructions along with corresponding trajectories from HAMSTER without any finetuning on (RLBench) simulation data, HAMSTER finetuned on all the data in Section 4.1, RT-Trajectory (Gu et al., 2023) with Code-as-Policies (Liang et al., 2023) powered by GPT-4o (Achiam et al., 2023), and RT-Trjaectory powered by GPT-4o directly.</p>
<p>Figure 14 :
14
Figure 14: An example of results for human ranking.The trajectory is from blue to red with blue circle and red circle denotes gripper close point and open point respectively.The grader is asked to provide a rank to these trajectory about which trajectory has highest chance to succeed.</p>
<p>Figure 15 :
15
Figure 15: Performance Distribution of RVT2+Sketch and 3DDA+Sketch</p>
<p>Figure 16 :
16
Figure 16: Colosseum benchmark variations.Figure from Pumacay et al. (2024), taken with permission.</p>
<p>Figure 17 :
17
Figure 17: The task is to pick up the lid and close it on the jar with correct color.Task description is located on the top-left corner of each image.The trajectory goes from blue to red where blue circles denotes where the gripper should close and red circles denotes where the gripper should open.GT denotes ground truth, 3B and 13B denotes VILA1.5-3B and VILA1.5-13B,RDP denotes paths simplified using Ramer-Douglas-Peucker algorithm while 20p denotes paths reprensented using 20 points.</p>
<p>Table 1 :
1
Results
MethodOriginal CameraNovel CameraSuccess Complete Success CompleteOpenVLA0.600.300.230.00HAMSTER+RVT20.830.700.730.40HAMSTER+RVT2 (Concat)1.001.000.980.90
on Colosseum demonstrate that HAMSTER is data efficient, achieving 2X the success score of 3D-DA with just 50% of the data.</p>
<p>Table 2 :
2
Real</p>
<p>world results demonstrate HAMSTER generalizes to better to novel camera views (see Fig.Figure</p>
<p>Table 3 :
3
(Pumacay et al., 2024)0.43± 0.06 0.34 ± 0.07 0.35 ± 0.11 0.39 ± 0.11 0.44 ± 0.13 0.41 ± 0.04 0.41 ± 0.11 HAMSTER (w 3D-DA) 0.46 ± 0.04 0.57 ± 0.03 0.48 ± 0.08 0.39 ± 0.06 0.41 ± 0.05 0.59 ± 0.04 0.57 ± 0.08 0.51 ± 0.10Ke et al.]0.27 ± 0.04 0.34 ± 0.10 0.36 ± 0.05 0.36 ± 0.12 0.07 ± 0.03 0.45 ± 0.12 0.42 ± 0.06 0.23 ± 0.04 HAMSTER (w 3D-DA) 0.48 ± 0.06 0.48 ± 0.05 0.40 ± 0.05 0.56 ± 0.09 0.11 ± 0.10 0.58 ± 0.04 0.56 ± 0.03 0.35 ± 0.07 Simulation evaluation of HAMSTER across different visual variations.We test vanilla 3D Diffuser Actor and HAMSTER across variations in Colosseum(Pumacay et al., 2024)and find that HAMSTER generalizes more effectively than 3D Diffuser Actor.Avg.indicates mean across variations, including no variation.
SLIDE BLOCK TO TARGET, PLACE WINE AT RACK LOCATION, INSERT ONTO SQUARE PEG,STACK CUPS, SETUP CHESS. Results in Table 1 demonstrate that HAMSTER+3D-DA with just50% of the data still achieves 2x the success rate of standard 3D-DA, demonstrating that HAM-STER is demonstration-efficient for the downstream imitation learning tasks.
5.3 VLM GENERALIZATION STUDIESFinally, we answer Q5: can HAMSTER's hierarchy enable superior visual and semantic reasoning?Camera View Invariance.We test HAMSTER+RVT2 against OpenVLA from a new camera angle (Figure6) across 10 pick-and-place trials using 6 training objects and 3 training containers to</p>
<p>Table 4 :
4
Comparison across visual-language benchmarks, grouped into core VQA tasks (left of the vertical bar) and robustness/probing datasets (right).HAMSTER (ours) uses the same LLM and image resolution as VILA1.5-13Bbut is trained without curated vision-language finetuning.Best results are in bold.Bench-</p>
<p>marks: VQA-v2 Goyal et al. (2017); GQA Hudson &amp; Manning (2019); VizWiz Gurari et al. (2018); SQA I : ScienceQA-IMG Lu et al. (2022a); VQA T : TextVQA Singh et al. (2019); POPE Li et al. (2023b); MME Fu et al. (2024); MMB: MMBench Liu et al. (2024e); MMB CN : MMBench-Chinese Liu et al. (2024e); SEED: SEED-Bench Li et al. (2023a); SEED I : SEED-Bench (Image) Li et al. (2023a); LLaVA W : LLaVA-Bench (Inthe-Wild) Liu et al. (2023); MM-Vet Yu et al. (2023a); MMMU val Yue et al. (2024).</p>
<p>Table 6 :
6
(Liang et al., 2023)evaluation of different VLMs, averaged across various real-world evaluation tasks.Results indicate that HAMSTER including simulation data is most effective since it captures both spatial and semantic information across diverse tasks from RLBench.This significantly outperforms zero-shot VLMbased trajectory generation, as described inGu et al. (2023)RLBench dataset.In doing so, we analyze the ability of the VLM to predict intermediate paths to transfer across significantly varying domains (from RLBench to the real world).The results suggest that: (1) zero-shot path generation, even from closed-source VLMsGu et al. (2023)such as GPT-4o with additional help through Code-as-Policies(Liang et al., 2023), underperforms VLMs finetuned on cross-domain data as in HAMSTER; (2) inclusion of significantly different training data such as low-fidelity simulation during finetuning improves the real-world performance of the VLM.This highlights the transferability displayed by HAMSTER across widely varying domains.These results emphasize that the hierarchical VLA approach described in HAM-STER can effectively utilize diverse sources of cheap prior data for 2D path predictions, despite considerable perceptual differences.
MethodVLMFinetuningRankRankRankDataExc. Real RLB. Real RLB. AllRT-Traj.0-shot GPT-4o -3.403.633.47RT-Traj.CaP GPT-4o-3.573.363.41HAMSTER VILAOur Exc. Sim RLB. 1.782.392.13HAMSTER VILAOur1.591.281.40
Gu et al. (2023)h sketches from the RLBench dataset.The purpose of these evaluations is to first compare with closely related work that generates 2D trajectories using pretrained closed source VLMsGu et al. (2023)(Comparison (1) and (2)).The comparison between (3) and (4) (our complete method) is meant to isolate the impact of including the simulation path sketches from theD.2 VLM REAL WORLD GENERALIZATION STUDYThe full list of task descriptions for this study is below (see Appendix D.1 for the main experiment details).Duplicates indicate different images for the same task.We plot some additional comparison examples in Figure</p>
<p>Table 7 :
7
Table 3 after removing variations with no visual variations (e.g., object friction).Real world average success rates grouped by task type.
TaskRVT2 3DDA OpenVLA HAMSTER+RVT2 HAMSTER+3DDApick and place0.280.190.460.790.78press button0.130.160.250.500.63knock down0.170.030.410.470.66
Representations similar to
2D paths has been explored in the robot learning literature(Gu et al., 2023), primarily as a technique for flexible task specification. We refer readers to section 2 for a detailed discussion.2 For human video, this corresponds to the position of the palm center or fingertips.
Note that this is not a temporally ordered path, but rather a set of unordered points of interest in an image.
ACKNOWLEDGEMENTSWe thank Wentao Yuan for generously providing the Robopoint dataset.We also acknowledge Entong Su and Yunchu Zhang for their assistance in setting up the robot environment.We are grateful for the support from the Army Research Lab through sponsored research, as well as the Amazon Science Hub for Yi and Marius.We also thank Animesh Garg for many helpful discussions.Finally, we extend our gratitude to Yao Lu, Hongxu Yin, Ligeng Zhu, Borys Tymchenko, and Zhijian Liu from NVIDIA's VILA group for their valuable support throughout this work.
. Josh Openai, Steven Achiam, Sandhini Adler, Lama Agarwal, Ilge Ahmad, Florencia Akkaya, Diogo Leoni Aleman, Janko Almeida, Sam Altenschmidt, Shyamal Altman, Red Anadkat, Igor Avila, Suchir Babuschkin, Valerie Balaji, Paul Balcom, Haiming Baltescu, Mo Bao, Jeff Bavarian, Irwan Belgum, Jake Bello, Gabriel Berdine, Christopher Bernadett-Shapiro, Lenny Berner, Oleg Bogdonoff, Madelaine Boiko, Anna-Luisa Boyd, Greg Brakman, Tim Brockman, Miles Brooks, Kevin Brundage, Trevor Button, Rosie Cai, Andrew Campbell, Brittany Cann, Chelsea Carey, Rory Carlson, Brooke Carmichael, Che Chan, Fotis Chang, Derek Chantzis, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Benjamin Chen, Chester Chess, Casey Cho, Hyung Won Chu, Dave Chung, Jeremiah Cummings, Yunxing Currier, Cory Dai, Thomas Decareaux, Noah Degry, Damien Deutsch, Arka Deville, David Dhar, Steve Dohan, Sheila Dowling, Adrien Dunning, Atty Ecoffet, Tyna Eleti, David Eloundou, Liam Farhi, Niko Fedus, Felix, Juston Sim'on Posada Fishman, Isabella Forte, Leo Fulford, Elie Gao, Christian Georges, Vik Gibson, Tarun Goel, Gabriel Gogineni, Raphael Goh, Jonathan Gontijo-Lopes, Morgan Gordon, Scott Grafstein, Ryan Gray, Joshua Greene, Gross, Shane Shixiang, Yufei Gu, Chris Guo, Jesse Hallacy, Jeff Han, Yuchen Harris, Mike He, Johannes Heaton, Chris Heidecke, Alan Hesse, Wade Hickey, Peter Hickey, Brandon Hoeschele, Kenny Houghton, Shengli Hsu, Xin Hu, Joost Hu, Shantanu Huizinga, Shawn Jain, Joanne Jain, Angela Jang, Roger Jiang, Haozhun Jiang, Denny Jin, Shino Jin, Billie Jomoto, Heewoo Jonn, Tomer Jun, Lukasz Kaftan, Ali Kaiser, Ingmar Kamali, Nitish Kanitscheider, Tabarak Shirish Keskar, Logan Khan, Jong Wook Kilpatrick, Christina Kim, Yongjik Kim, Hendrik Kim, Jamie Kirchner, Matthew Ryan Kiros, Daniel Knight, Lukasz Kokotajlo, Andrew Kondraciuk, Aris Kondrich, Kyle Konstantinidis, Gretchen Kosic, Vishal Krueger, Michael Kuo, Ikai Lampe, Teddy Lan, Jan Lee, Jade Leike, Daniel Leung, Levy, Ming Chak, Rachel Li, Molly Lim, Stephanie Lin, Mateusz Lin, Theresa Litwin, Ryan Lopez, Patricia Lowe, Anna Adeola Lue, Kim Makanju, Sam Malfacini, Todor Manning, Yaniv Markov, Bianca Markovski, Katie Martin, Andrew Mayer, Mayne ; Aalok, Jacob Mehta, Luke Menick, Andrey Metz, Pamela Mishchenko, Vinnie Mishkin, Evan Monaco, Daniel P Morikawa, Tong Mossing, Mira Mu, Oleg Murati, Murk, M' David, Ashvin Ely, Reiichiro Nair, Rajeev Nakano, Arvind Nayak, Richard Neelakantan, Hyeonwoo Ngo, Ouyang Noh, Long, O' Cullen, Jakub W Keefe, Alex Pachocki, Joe Paino, Ashley Palermo, Giambattista Pantuliano, Joel Parascandolo, Emy Parish, Alexandre Parparita, Mikhail Passos, Andrew Pavlov, Adam Peng, Perelman ; Toki, Jessica Sherbakov, Sarah Shieh, Pranav Shoker, Szymon Shyam, Eric Sidor, Maddie Sigler, Jordan Simens, Katarina Sitkin, Ian Slama, Benjamin D Sohl, Yang Sokolowsky, Natalie Song, Staudacher, Wei, Akila Cj Weinmann, Peter Welihinda, Jiayi Welinder, Lilian Weng, Matt Weng, Dave Wiethoff, Clemens Willner, Samuel Winter, Hannah Wolrich, Lauren Wong, Sherwin Workman, Jeff Wu, Michael Wu, Kai Wu, Tao Xiao, Sarah Xu, Kevin Yoo, Qiming Yu, Wojciech Yuan, Rowan Zaremba, Chong Zellers, Marvin Zhang, Shengjia Zhang, Tianhao Zhao, Juntang Zheng, William Zhuang, Barret Zhuk, Zoph, Felipe Petroski Such. Michael Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario D. Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard,2023Filipe de Avila Belbute Peres ; Juan Felipe Cer'on Uribe, Andrea Vallone, Arun VijayvergiyaGpt-4 technical report. In arxiv preprint</p>
<p>Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, Jingren Zhou, arXiv:2308.12966Qwen-vl: A frontier large vision-language model with versatile abilities. 2023arXiv preprint</p>
<p>Track2act: Predicting point tracks from internet videos enables diverse zero-shot robot manipulation. Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, Shubham Tulsiani, arXiv:2405.015272024arXiv preprint</p>
<p>pi 0: A vision-language-action flow model for general robot control. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, arXiv:2410.241642024arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, Brianna Zitkovich, arXivpreprintarXiv:2307.158182023a</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Conference on robot learning. PMLR2023b</p>
<p>Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, Saehoon Kim, Coyo-700m: Image-text pair dataset. 2022</p>
<p>Diffusion policy: Visuomotor policy learning via action diffusion. Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burchfiel, Shuran Song, 10.15607/RSS.2023.XIX.026Robotics: Science and Systems XIX. Kostas E Bekris, Kris Hauser, Sylvia L Herbert, Jingjin Yu, Daegu, Republic of KoreaJuly 10-14, 20232023</p>
<p>. Abby O' Neill, Abdul Rehman, Abhinav Gupta, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, Alex Irpan, Alexander Khazatsky, Anant Rai, Anchit Gupta, Andrew Wang, Andrey Kolobov, Anikait Singh, Animesh Garg, Aniruddha Kembhavi, Annie Xie, Anthony Brohan, Antonin Raffin, Archit Sharma, Arefeh Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Felipe Vieira Frujeri, Freek Stulp, Gaoyue Zhou, Gaurav S Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guangwen Yang, Guanzhi Wang, Hao Su, Hao-Shu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I Christensen, Hiroki Furuta, Homanga Bharadhwaj, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jay Vakil, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, João Silvério, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Pratap Kunal, Kuo-Hao Singh, Kyle Zeng, Kyle Hatch, Laurent Hsu, Lawrence Itti, Lerrel Yunliang Chen, Li Pinto, Liam Fei-Fei, Tan, " Linxi, Lionel Jim" Fan, Lisa Ott, Luca Lee, Magnum Weihs, Marion Chen, Marius Lepert, Masayoshi Memmel, Masha Tomizuka, Mateo Guaman Itkina, Max Castro, Maximilian Spero, Michael Du, Michael C Ahn, Mingtong Yip, Mingyu Zhang, Minho Ding, Mohan Heo, Mohit Kumar Srirama, Sharma, Jin Moo, Naoaki Kim, Nicklas Kanazawa, Nicolas Hansen, Heess, J Nikhil, Niko Joshi, Ning Suenderhauf, Norman Liu, Nur Di Palo, Muhammad Mahi, Oier Shafiullah, Oliver Mees, Osbert Kroemer, Bastani, Patrick ; Pannag R Sanketi, Patrick Miller, Paul Yin, Peng Wohlhart, Peter Xu, Peter David Fagan, Pierre Mitrano, Pieter Sermanet, Priya Abbeel, Qiuyu Sundaresan, Quan Chen, Rafael Vuong, Ran Rafailov, Ria Tian, Roberto Doshi, Stefan Mart ; Soroush Nasiriany, Stefan Schaal, Stephen Welker, Subramanian Tian, Sudeep Ramamoorthy, Suneel Dasari, Sungjae Belkhale, Suraj Park, Suvir Nair, Takayuki Mirchandani, Tanmay Osa, Tatsuya Gupta, Tatsuya Harada, Ted Matsushima, Thomas Xiao, Tianhe Kollar, Tianli Yu, Todor Ding, Tony Z Davchev, Travis Zhao, Trevor Armstrong, Trinity Darrell, Vidhi Chung, Vikash Jain, Vincent Kumar, Wei Vanhoucke, Wenxuan Zhan, Wolfram Zhou, Xi Burgard, Xiangyu Chen, Chen ; Yao, Yecheng Lu, Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Mart'in, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham Sonawani, Shubham Tulsiani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist,2023Yonatan BiskLee, Yuchen Cui, Yue Cao; Yutaka Matsuo, Zehan MaXiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Xu Liangwei, Xuanlin Li, Yansong Pang,Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa. Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, Zipeng Fu, and Zipeng Lin. Open X-Embodiment: Robotic learning datasets and RT-X models</p>
<p>Tapir: Tracking any point with per-frame initialization and temporal refinement. Carl Doersch, Yi Yang, Mel Vecerik, Dilara Gokay, Ankush Gupta, Yusuf Aytar, Joao Carreira, Andrew Zisserman, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Algorithms for the reduction of the number of points required to represent a digitized line or its caricature. H David, Thomas K Douglas, Peucker, 10.3138/FM57-6770-U75U-7727Cartographica. 1021973</p>
<p>Palm-e: An embodied multimodal language model. Danny Driess, Fei Xia, S M Mehdi, Corey Sajjadi, Aakanksha Lynch, Brian Chowdhery, Ayzaan Ichter, Jonathan Wahid, Quan Tompson, Tianhe Vuong, Yu, International Conference on Machine Learning. PMLR2023</p>
<p>Jiafei Duan, Wentao Yuan, Wilbert Pumacay, Yi Ru Wang, Kiana Ehsani, Dieter Fox, Ranjay Krishna, arXiv:2406.18915Manipulate-anything: Automating real-world robots using vision-language models. 2024arXiv preprint</p>
<p>Motion policy networks. Adam Fishman, Adithyavairavan Murali, Clemens Eppner, Bryan Peele, Byron Boots, Dieter Fox, of Proceedings of Machine Learning Research. Karen Liu, Dana Kulic, Jeffrey Ichnowski, Auckland, New ZealandPMLRCoRL 2022, 14-18 December 2022. 2022205Conference on Robot Learning</p>
<p>Mme-survey: A comprehensive survey on evaluation of multimodal llms. Chaoyou Fu, Yi-Fan Zhang, Shukang Yin, Bo Li, Xinyu Fang, Sirui Zhao, Haodong Duan, Xing Sun, Ziwei Liu, Liang Wang, arXiv:2411.152962024arXiv preprint</p>
<p>Rvt: Robotic view transformer for 3d object manipulation. Ankit Goyal, Jie Xu, Yijie Guo, Valts Blukis, Yu-Wei Chao, Dieter Fox, Conference on Robot Learning. PMLR2023</p>
<p>Rvt2: Learning precise manipulation from few demonstrations. Ankit Goyal, Valts Blukis, Jie Xu, Yijie Guo, Yu-Wei Chao, Dieter Fox, 2024RSS</p>
<p>Making the v in vqa matter: Elevating the role of image understanding in visual question answering. Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2017</p>
<p>Rt-trajectory: Robotic task generalization via hindsight trajectory sketches. Jiayuan Gu, Sean Kirmani, Paul Wohlhart, Yao Lu, Montserrat Gonzalez Arenas, Kanishka Rao, Wenhao Yu, Chuyuan Fu, Keerthana Gopalakrishnan, Zhuo Xu, Priya Sundaresan, Peng Xu, Hao Su, Karol Hausman, Chelsea Finn, Quan Vuong, Ted Xiao, 2023</p>
<p>Vizwiz grand challenge: Answering visual questions from blind people. Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, Jeffrey P Bigham, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition2018</p>
<p>Dextreme: Transfer of agile in-hand manipulation from simulation to reality. Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Voxposer: Composable 3d value maps for robotic manipulation with language models. Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei, Conference on Robot Learning. PMLR2023a</p>
<p>Inner monologue: Embodied reasoning through planning with language models. Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Conference on Robot Learning. PMLR2023b</p>
<p>Gqa: A new dataset for real-world visual reasoning and compositional question answering. A Drew, Christopher D Hudson, Manning, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Rlbench: The robot learning benchmark &amp; learning environment. Stephen James, Zicong Ma, David Rovick Arrojo, Andrew J Davison, IEEE Robotics and Automation Letters. 522020</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, Frederik Ebert, Corey Lynch, Sergey Levine, Chelsea Finn, Conference on Robot Learning. PMLR2022</p>
<p>Vima: General robot manipulation with multimodal prompts. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, Linxi Fan, International Conference on Machine Learning. 2023</p>
<p>Cotracker: It is better to track together. Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, Christian Rupprecht, European Conference on Computer Vision. Springer2025</p>
<p>3d diffuser actor: Policy diffusion with 3d scene representations. Tsung-Wei Ke, Nikolaos Gkanatsios, Katerina Fragkiadaki, First Workshop on Vision-Language Models for Navigation and Manipulation at ICRA 2024. 2024</p>
<p>. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty Ellis, Peter David Fagan, Joey Hejna, Masha Itkina, Marion Lepert, Jason Yecheng, Patrick Tree Ma, Jimmy Miller, Suneel Wu, Shivin Belkhale, Huy Dass, Arhan Ha, Abraham Jain, Youngwoon Lee, Marius Lee, Sungjae Memmel, Ilija Park, Kaiyuan Radosavovic, Albert Wang, Kevin Zhan, Cheng Black, Kyle Beltran Chi, Shan Hatch, Jingpei Lin, Jean Lu, Abdul Mercat, Rehman, Archit Pannag R Sanketi, Cody Sharma, Quan Simpson, Homer Vuong, Blake Rich Walke, Ted Wulfe, Jonathan Heewon Xiao, Arefeh Yang, Tony Z Yavary, Christopher Zhao, Rohan Agia, Mateo Guaman Baijal, Daphne Castro, Qiuyu Chen, Trinity Chen, Jaimyn Chung, Ethan Paul Drake, Jensen Foster, David Antonio Gao, Minho Herrera, Kyle Heo, Jiaheng Hsu, Donovon Hu, Charlotte Jackson, Yunshuang Le, Kevin Li, Roy Lin, Zehan Lin, Abhiram Ma, Suvir Maddukuri, Daniel Mirchandani, Tony Morton, Abigail O' Nguyen, Rosario Neill, Derick Scalise, Victor Seale, Stephen Son, Emi Tian, Andrew E Tran, Yilin Wang, Annie Wu, Jingyun Xie, Patrick Yang, Yunchu Yin, Osbert Zhang, Glen Bastani, Jeannette Berseth, Ken Bohg, Abhinav Goldberg, Abhishek Gupta, Dinesh Gupta, Joseph J Jayaraman, Jitendra Lim, Roberto Malik, Subramanian Martín-Martín, Dorsa Ramamoorthy, Shuran Sadigh, Jiajun Song, Michael C Wu, Yuke Yip, Thomas Zhu, Sergey Kollar, Chelsea Levine, Finn, 2024Droid: A large-scale in-the-wild robot manipulation dataset</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>Ram: Retrieval-based affordance transfer for generalizable zero-shot robotic manipulation. Yuxuan Kuang, Junjie Ye, Haoran Geng, Jiageng Mao, Congyue Deng, Leonidas Guibas, He Wang, Yue Wang, arXiv:2407.046892024arXiv preprint</p>
<p>Reward design with language models. Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Learning quadrupedal locomotion over challenging terrain. Joonho Lee, Jemin Hwangbo, Vladlen Lorenz Wellhausen, Marco Koltun, Hutter, Science robotics. 54759862020</p>
<p>Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan, arXiv:2307.16125Seed-bench: Benchmarking multimodal llms with generative comprehension. 2023aarXiv preprint</p>
<p>Evaluating real-world robot manipulation policies in simulation. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, arXiv:2405.059412024arXiv preprint</p>
<p>Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, Ji-Rong Wen, arXiv:2305.10355Evaluating object hallucination in large vision-language models. 2023barXiv preprint</p>
<p>Code as policies: Language model programs for embodied control. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, Andy Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Vila: On pretraining for visual language models. Ji Lin, Hongxu Yin, Wei Ping, Pavlo Molchanov, Mohammad Shoeybi, Song Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)June 2024</p>
<p>Text2motion: From natural language instructions to feasible plans. Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg ; Aixin, Bei Liu, Bing Feng, Bingxuan Xue, Bochao Wang, Chengda Wu, Chenggang Lu, Chengqi Zhao, Chenyu Deng, Chong Zhang, Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2023. 2024a47arXiv preprint</p>
<p>Moka: Open-vocabulary robotic manipulation through mark-based visual prompting. Fangchen Liu, Kuan Fang, Pieter Abbeel, Sergey Levine, arXiv:2403.031742024barXiv preprint</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 202336</p>
<p>Improved baselines with visual instruction tuning. Haotian Liu, Chunyuan Li, Yuheng Li, Yong Jae Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024c</p>
<p>Visual instruction tuning. Haotian Liu, Chunyuan Li, Qingyang Wu, Yong Jae Lee, Advances in neural information processing systems. 2024d36</p>
<p>Mmbench: Is your multi-modal model an all-around player?. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, European conference on computer vision. Springer2024e</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, Advances in Neural Information Processing Systems. 2022a35</p>
<p>Learn to explain: Multimodal reasoning via thought chains for science question answering. Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, Ashwin Kalyan, The 36th Conference on Neural Information Processing Systems (NeurIPS). 2022b</p>
<p>Vip: Towards universal visual reward and representation via value-implicit pre-training. Jason Yecheng, Shagun Ma, Dinesh Sodhani, Osbert Jayaraman, Vikash Bastani, Amy Kumar, Zhang, The Eleventh International Conference on Learning Representations. 2023</p>
<p>Eureka: Human-level reward design via coding large language models. Jason Yecheng, William Ma, Guanzhi Liang, De-An Wang, Osbert Huang, Dinesh Bastani, Yuke Jayaraman, Linxi Zhu, Anima Fan, Anandkumar, The Twelfth International Conference on Learning Representations. 2024</p>
<p>What matters in learning from offline human demonstrations for robot manipulation. Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun Kulkarni, Li Fei-Fei, Silvio Savarese, Yuke Zhu, Roberto Martín-Martín, Conference on Robot Learning (CoRL). 2021</p>
<p>Mimicgen: A data generation system for scalable robot learning using human demonstrations. Ajay Mandlekar, Soroush Nasiriany, Bowen Wen, Iretiayo Akinola, Yashraj Narang, Linxi Fan, Yuke Zhu, Dieter Fox, Conference on Robot Learning. PMLR2023</p>
<p>Simple open-vocabulary object detection. Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, European Conference on Computer Vision. Springer2022</p>
<p>Scaling open-vocabulary object detection. Matthias Minderer, Alexey A Gritsenko, Neil Houlsby, Thirty-seventh Conference on Neural Information Processing Systems. 2023</p>
<p>R3m: A universal visual representation for robot manipulation. Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, Abhinav Gupta, Conference on Robot Learning. PMLR2023</p>
<p>Soroush Nasiriany, Sean Kirmani, Tianli Ding, Laura Smith, Yuke Zhu, Danny Driess, Dorsa Sadigh, Ted Xiao, arXiv:2411.02704Rt-affordance: Affordances are versatile intermediate representations for robot manipulation. November 2024aarXiv preprint</p>
<p>Pivot: Iterative visual prompting elicits actionable knowledge for vlms. Soroush Nasiriany, Fei Xia, Wenhao Yu, Ted Xiao, Jacky Liang, Ishita Dasgupta, Annie Xie, Danny Driess, Ayzaan Wahid, Zhuo Xu, International Conference on Machine Learning. 2024b</p>
<p>LLARVA: Vision-action instruction tuning enhances robot learning. Dantong Niu, Yuvan Sharma, Giscard Biamby, Jerome Quenum, Yutong Bai, Baifeng Shi, Trevor Darrell, Roei Herzig, 8th Annual Conference on Robot Learning. 2024</p>
<p>The unsurprising effectiveness of pre-trained vision models for control. Simone Parisi, Aravind Rajeswaran, Senthil Purushwalkam, Abhinav Gupta, international conference on machine learning. PMLR2022</p>
<p>The colosseum: A benchmark for evaluating generalization for robotic manipulation. Wilbert Pumacay, Ishika Singh, Jiafei Duan, Ranjay Krishna, Jesse Thomason, Dieter Fox, arXiv:2402.081912024arXiv preprint</p>
<p>Urs Ramer. An iterative procedure for the polygonal approximation of plane curves. Ilija Radosavovic, Baifeng Shi, Letian Fu, Ken Goldberg, Trevor Darrell, Jitendra Malik, S0146-664X(72)80017-0Conference on Robot Learning. PMLR2023. 19721Robot learning with sensorimotor pre-training</p>
<p>Rrl: Resnet as representation for reinforcement learning. M Rutav, Vikash Shah, Kumar, International Conference on Machine Learning. PMLR2021</p>
<p>Incorporating visual layout structures for scientific text classification. Zejiang Shen, Kyle Lo, Lucy Lu Wang, Bailey Kuehl, Daniel S Weld, Doug Downey, ArXiv, abs/2106.006762021</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. Mohit Shridhar, Lucas Manuelli, Dieter Fox, Conference on Robot Learning. PMLR2023</p>
<p>Towards vqa models that can read. Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, Marcus Rohrbach, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognition2019</p>
<p>Progprompt: Generating situated robot task plans using large language models. Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, Animesh Garg, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Roboclip: One demonstration is enough to learn robot policies. Jesse Sumedh Anand Sontakke, Séb Zhang, Karl Arnold, Erdem Pertsch, Dorsa Biyik, Chelsea Sadigh, Laurent Finn, Itti, NeurIPS2023</p>
<p>Open-world object manipulation using pre-trained vision-language models. Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Sean Kirmani, Brianna Zitkovich, Fei Xia, Conference on Robot Learning. PMLR2023</p>
<p>Kite: Keypoint-conditioned policies for semantic manipulation. Priya Sundaresan, Suneel Belkhale, Dorsa Sadigh, Jeannette Bohg, ; Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, arXiv:2405.12213An open-source generalist robot policy. 2024arXiv preprintConference on Robot Learning</p>
<p>Reconciling reality through simulation: A real-to-sim-to-real approach for robust manipulation. Marcel Torne, Anthony Simeonov, Zechu Li, April Chan, Tao Chen, Abhishek Gupta, Pulkit Agrawal, Robotics: Science and Systems. 2024</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, 2023</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Kevin Homer Walke, Abraham Black, Lee, Jin Moo, Max Kim, Chongyi Du, Tony Zheng, Philippe Zhao, Quan Hansen-Estruch, Andre Vuong, Vivek He, Kuan Myers, Chelsea Fang, Sergey Finn, Levine, Conference on Robot Learning (CoRL). 2023</p>
<p>Tracking everything everywhere all at once. Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holynski, Noah Snavely, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision2023</p>
<p>Erdem Biyik, David Held, and Zackory Erickson. Rl-vlm-f: Reinforcement learning from vision language foundation model feedback. Yufei Wang, Zhanyi Sun, Jesse Zhang, Zhou Xian, International Conference on Machine Learning. 2024</p>
<p>Any-point trajectory modeling for policy learning. Chuan Wen, Xingyu Lin, John So, Kai Chen, Qi Dou, Yang Gao, Pieter Abbeel, arXiv:2401.000252023arXiv preprint</p>
<p>Flow as the cross-domain manipulation interface. Mengda Xu, Zhenjia Xu, Yinghao Xu, Cheng Chi, Gordon Wetzstein, Manuela Veloso, Shuran Song, 8th Annual Conference on Robot Learning. 2024</p>
<p>Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, arXiv:2308.02490Mm-vet: Evaluating large multimodal models for integrated capabilities. 2023aarXiv preprint</p>
<p>Language to rewards for robotic skill synthesis. Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montserrat Gonzalez Arenas, Lewis Hao-Tien, Tom Chiang, Leonard Erez, Jan Hasenclever, Humplik, Conference on Robot Learning. PMLR2023b</p>
<p>Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao, arXiv:2401.11439General flow as foundation affordance for scalable robot learning. 2024aarXiv preprint</p>
<p>Robopoint: A vision-language model for spatial affordance prediction in robotics. Wentao Yuan, Jiafei Duan, Valts Blukis, Wilbert Pumacay, Ranjay Krishna, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox, 8th Annual Conference on Robot Learning. 2024b</p>
<p>Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition2024</p>
<p>Learning fine-grained bimanual manipulation with low-cost hardware. Tony Z Zhao, Vikash Kumar, Sergey Levine, Chelsea Finn, 10.15607/RSS.2023.XIX.016Robotics: Science and Systems XIX. Kostas E Bekris, Kris Hauser, Sylvia L Herbert, Jingjin Yu, Daegu, Republic of KoreaJuly 10-14, 20232023</p>
<p>Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang, Wang , Yejin Choi, arXiv:2304.06939Multimodal C4: An open, billionscale corpus of images interleaved with text. 2023arXiv preprint</p>
<p>PMLR, 2023. 1. screw in the light bulb on the lamp 2. screw in the light bulb on the lamp 3. screw in the light bulb on the lamp 4. screw out the light bulb and place it on the holder 5. screw out the light bulb and place it on the holder 6. Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Conference on Robot Learning. Rt-2: Vision-language-action models transfer web knowledge to robotic control. screw in the light bulb 7. screw in the light bulb on the lamp 8. move the blue block on Taylor Swift 9. pick up the left block and put it on Jensen Huang 10. move the block on the right to Taylor Swift 11. place the yellow block on Kobe 12. pick up the blue block and place it on Jensen Huang 13. move the red block to Kobe 14. press the button on the wall</p>
<p>pick up the tomato soup and put it into the drawer 30. pick up the peach and put it into the drawer 31. move the mayo to the drawer 32. move the dessert to the drawer 33. pick up the object on the left and place it on the left 34. pick up the fruit on the left and put it on the plate 35. pick up the milk and put it on the plate 36. press the button with the color of cucumber, then press the button with color of fire</p>
<p>pick up the left block on the bottom and stack it on the middle block on top 46. stack the leftest block on the rightest block 47. stack the block 25 over block L 48. put the left block on first stair D.3 HUMAN RANKING</p>            </div>
        </div>

    </div>
</body>
</html>
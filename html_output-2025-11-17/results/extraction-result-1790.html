<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1790 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1790</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1790</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-2f45d8ce0781819ff3b364886f238a3f3e6d1c04</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/2f45d8ce0781819ff3b364886f238a3f3e6d1c04" target="_blank">Deep Drone Acrobatics</a></p>
                <p><strong>Paper Venue:</strong> Robotics: Science and Systems</p>
                <p><strong>Paper TL;DR:</strong> This paper proposes to learn a sensorimotor policy that enables an autonomous quadrotor to fly extreme acrobatic maneuvers with only onboard sensing and computation, and trains the policy entirely in simulation by leveraging demonstrations from an optimal controller that has access to privileged information.</p>
                <p><strong>Paper Abstract:</strong> Performing acrobatic maneuvers with quadrotors is extremely challenging. Acrobatic flight requires high thrust and extreme angular accelerations that push the platform to its physical limits. Professional drone pilots often measure their level of mastery by flying such maneuvers in competitions. In this paper, we propose to learn a sensorimotor policy that enables an autonomous quadrotor to fly extreme acrobatic maneuvers with only onboard sensing and computation. We train the policy entirely in simulation by leveraging demonstrations from an optimal controller that has access to privileged information. We use appropriate abstractions of the visual input to enable transfer to a real quadrotor. We show that the resulting policy can be directly deployed in the physical world without any fine-tuning on real data. Our methodology has several favorable properties: it does not require a human expert to provide demonstrations, it cannot harm the physical system during training, and it can be used to learn maneuvers that are challenging even for the best human pilots. Our approach enables a physical quadrotor to fly maneuvers such as the Power Loop, the Barrel Roll, and the Matty Flip, during which it incurs accelerations of up to 3g.</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1790.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1790.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deep Drone Acrobatics controller</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sensorimotor policy for quadrotor acrobatic maneuvers (Deep Drone Acrobatics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A neural-network sensorimotor controller trained entirely in simulation to map onboard-sensor histories (feature-track visual abstraction + preprocessed IMU + reference trajectory) to low-level commands (collective thrust and body rates), deployed zero-shot on a physical quadrotor to perform high-acceleration acrobatic maneuvers.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Custom quadrotor with onboard perception (Deep Drone Acrobatics platform)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>A custom 1.15 kg quadrotor (thrust-to-weight ratio 4:1) that carries an onboard forward-facing camera and IMU and executes low-level collective-thrust and body-rate commands computed on a Jetson TX2 (uses Intel RealSense T265 for images/IMU in real tests); purpose is autonomous execution of high-acceleration acrobatic maneuvers (Power Loop, Barrel Roll, Matty Flip, Combo) using only onboard sensing and computation.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>aerial robotics / quadrotor acrobatics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Gazebo (with RotorS extension)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Gazebo simulator with the RotorS multirotor extension simulating quadrotor rigid-body dynamics and sensors (simulated AscTec Hummingbird platform, forward-facing fisheye camera, IMU) in a 70 m cubic flying space; used to generate unlimited training data, run the privileged MPC expert, and produce sensor measurements for the student policy.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>high-fidelity physics for multirotor dynamics (Gazebo + RotorS), but limited visual realism and selective randomization (not photorealistic domain randomization of scene appearance).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Rigid-body quadrotor physics via RotorS, motor/thrust dynamics sufficient for generating feasible reference and expert trajectories, simulated camera images (fisheye) and IMU measurements, ability to configure IMU bias and thrust-to-weight parameters for randomized perturbation.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>No systematic randomization or photorealistic rendering of scene appearance/geometry (authors explicitly do NOT randomize geometry/appearance during data collection); some control models used for the privileged expert simplify dynamics (the MPC expert uses a simplified model that neglects angular-rate dynamics); aerodynamic effects and certain subtle actuator/model mismatches are not exhaustively modeled or quantified; visual domain gap (appearance, motion blur specifics) left unmodeled.</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical quadrotor flown without external motion-capture; onboard forward-facing camera + IMU (Intel RealSense T265), computation on Jetson TX2, safety pilot present but controllers executed zero-shot without fine-tuning; experiments recorded in real indoor flight environment with different visual appearance from simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Execution of high-acceleration acrobatic maneuvers (Power Loop, Barrel Roll, Matty Flip, and a Combo sequence) learned in simulation and transferred to the physical quadrotor (closed-loop low-level thrust and body-rate control from onboard sensors).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Imitation learning / privileged learning (offline supervised imitation of an MPC privileged expert in simulation), DAGGER-style data collection (off-policy collect-and-train iterations) with supervised regression of expert actions onto student observations.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Primary metrics: average RMS tracking error (position error) in cm/m in simulation and success rate (%) for completing maneuver (no crash/no safety intervention); additional empirical measure: number of successful back-to-back Power Loops before human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Simulation performance for the final learned controller ('Ours') averaged over 20 runs: Power Loop tracking error 24 ± 5 cm (100% success), Barrel Roll 58 ± 9 cm (100% success), Matty Flip 53 ± 15 cm (100% success), Combo 128 ± 57 cm (95% success).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Real-world performance (10 runs per maneuver): Ours achieved 100% success on Power Loop, 100% success on Barrel Roll, and 100% success on Matty Flip (no fine-tuning; additional repeated-loop experiments show multiple successful consecutive loops before intervention as reported qualitatively/graphically).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Limited randomization applied during training: IMU biases and thrust-to-weight ratio randomized by up to 10% of nominal in each iteration. The authors explicitly did NOT randomize scene geometry or appearance during data collection.</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Visual appearance / rendering differences (raw images -> large domain gap), motion blur and large image displacements at high accelerations, drift in state estimation from purely-inertial sensing, sensor calibration/IMU bias, actuation/model mismatch (simplified expert dynamics), and lack of photorealistic scene variability in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Key enabling factors: abstraction of raw sensory input (use of feature-track representation rather than raw pixels) to reduce observation-model discrepancy; preprocessing of IMU (de-biasing and integration/gravity alignment); privileged-learning setup (MPC expert in simulation) to provide dense, high-quality demonstrations; limited domain randomization of IMU biases and thrust-to-weight; training entirely in a high-fidelity physics simulator (Gazebo+RotorS); asynchronous network design to handle multi-rate sensors; and explicit formal argument (lemmas) showing abstraction reduces the simulation-to-reality gap (upper bound via Wasserstein distance).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>No explicit numeric fidelity thresholds were prescribed; rather, the paper identifies representation fidelity requirements: the observation abstraction must preserve task-relevant information while being invariant to nuisance factors (e.g., feature tracks preserve geometry/egomotion information but are invariant to appearance), and physics fidelity should be sufficient to produce feasible expert trajectories; the authors formally show that reducing difference between observation models (via f) reduces the sim-to-real gap, but do not quantify required numeric error bounds.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparisons: (1) Visual-input modality: a network using raw images (pixel input) trained on Matty Flip had training error 90 ± 32 cm and 80% success in training environment, but completely failed (0% success) when tested on novel backgrounds; in contrast, the feature-track based controller ('Ours') had 53 ± 15 cm error and 100% success in training and maintained 100% success on novel backgrounds. (2) Modality ablations: IMU-only (no feature tracks) could complete many maneuvers but accumulated drift on long sequences and had lower accuracy; reference-only (open loop) performed very poorly. These results demonstrate that abstraction of visual input (feature tracks) substantially improves sim-to-real transfer compared to raw images, and that combining IMU+feature-tracks yields best transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The paper demonstrates reliable zero-shot sim-to-real transfer of an end-to-end sensorimotor controller for highly aggressive quadrotor acrobatics by (i) training entirely in Gazebo+RotorS on expert MPC demonstrations, (ii) using abstraction of sensory inputs (feature tracks + preprocessed IMU) to reduce the observation-model gap, and (iii) limited domain randomization of IMU biases and thrust-to-weight; raw-image policies do not generalize to novel visual domains and fail to transfer, IMU-only policies suffer drift on long maneuvers, and no real-world fine-tuning was necessary to achieve 100% success on the tested maneuvers, validating that appropriate abstraction and selected randomization can enable high-fidelity skill transfer from simulation to reality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Deep Drone Acrobatics', 'publication_date_yy_mm': '2020-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep drone racing: From simulation to reality with domain randomization <em>(Rating: 2)</em></li>
                <li>CAD2RL: Real single-image flight without a single real image <em>(Rating: 2)</em></li>
                <li>Driving policy transfer via modularity and abstraction <em>(Rating: 2)</em></li>
                <li>Does computer vision matter for action? <em>(Rating: 1)</em></li>
                <li>Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1790",
    "paper_id": "paper-2f45d8ce0781819ff3b364886f238a3f3e6d1c04",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Deep Drone Acrobatics controller",
            "name_full": "Sensorimotor policy for quadrotor acrobatic maneuvers (Deep Drone Acrobatics)",
            "brief_description": "A neural-network sensorimotor controller trained entirely in simulation to map onboard-sensor histories (feature-track visual abstraction + preprocessed IMU + reference trajectory) to low-level commands (collective thrust and body rates), deployed zero-shot on a physical quadrotor to perform high-acceleration acrobatic maneuvers.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Custom quadrotor with onboard perception (Deep Drone Acrobatics platform)",
            "agent_system_description": "A custom 1.15 kg quadrotor (thrust-to-weight ratio 4:1) that carries an onboard forward-facing camera and IMU and executes low-level collective-thrust and body-rate commands computed on a Jetson TX2 (uses Intel RealSense T265 for images/IMU in real tests); purpose is autonomous execution of high-acceleration acrobatic maneuvers (Power Loop, Barrel Roll, Matty Flip, Combo) using only onboard sensing and computation.",
            "domain": "aerial robotics / quadrotor acrobatics",
            "virtual_environment_name": "Gazebo (with RotorS extension)",
            "virtual_environment_description": "Gazebo simulator with the RotorS multirotor extension simulating quadrotor rigid-body dynamics and sensors (simulated AscTec Hummingbird platform, forward-facing fisheye camera, IMU) in a 70 m cubic flying space; used to generate unlimited training data, run the privileged MPC expert, and produce sensor measurements for the student policy.",
            "simulation_fidelity_level": "high-fidelity physics for multirotor dynamics (Gazebo + RotorS), but limited visual realism and selective randomization (not photorealistic domain randomization of scene appearance).",
            "fidelity_aspects_modeled": "Rigid-body quadrotor physics via RotorS, motor/thrust dynamics sufficient for generating feasible reference and expert trajectories, simulated camera images (fisheye) and IMU measurements, ability to configure IMU bias and thrust-to-weight parameters for randomized perturbation.",
            "fidelity_aspects_simplified": "No systematic randomization or photorealistic rendering of scene appearance/geometry (authors explicitly do NOT randomize geometry/appearance during data collection); some control models used for the privileged expert simplify dynamics (the MPC expert uses a simplified model that neglects angular-rate dynamics); aerodynamic effects and certain subtle actuator/model mismatches are not exhaustively modeled or quantified; visual domain gap (appearance, motion blur specifics) left unmodeled.",
            "real_environment_description": "Physical quadrotor flown without external motion-capture; onboard forward-facing camera + IMU (Intel RealSense T265), computation on Jetson TX2, safety pilot present but controllers executed zero-shot without fine-tuning; experiments recorded in real indoor flight environment with different visual appearance from simulation.",
            "task_or_skill_transferred": "Execution of high-acceleration acrobatic maneuvers (Power Loop, Barrel Roll, Matty Flip, and a Combo sequence) learned in simulation and transferred to the physical quadrotor (closed-loop low-level thrust and body-rate control from onboard sensors).",
            "training_method": "Imitation learning / privileged learning (offline supervised imitation of an MPC privileged expert in simulation), DAGGER-style data collection (off-policy collect-and-train iterations) with supervised regression of expert actions onto student observations.",
            "transfer_success_metric": "Primary metrics: average RMS tracking error (position error) in cm/m in simulation and success rate (%) for completing maneuver (no crash/no safety intervention); additional empirical measure: number of successful back-to-back Power Loops before human intervention.",
            "transfer_performance_sim": "Simulation performance for the final learned controller ('Ours') averaged over 20 runs: Power Loop tracking error 24 ± 5 cm (100% success), Barrel Roll 58 ± 9 cm (100% success), Matty Flip 53 ± 15 cm (100% success), Combo 128 ± 57 cm (95% success).",
            "transfer_performance_real": "Real-world performance (10 runs per maneuver): Ours achieved 100% success on Power Loop, 100% success on Barrel Roll, and 100% success on Matty Flip (no fine-tuning; additional repeated-loop experiments show multiple successful consecutive loops before intervention as reported qualitatively/graphically).",
            "transfer_success": true,
            "domain_randomization_used": true,
            "domain_randomization_details": "Limited randomization applied during training: IMU biases and thrust-to-weight ratio randomized by up to 10% of nominal in each iteration. The authors explicitly did NOT randomize scene geometry or appearance during data collection.",
            "sim_to_real_gap_factors": "Visual appearance / rendering differences (raw images -&gt; large domain gap), motion blur and large image displacements at high accelerations, drift in state estimation from purely-inertial sensing, sensor calibration/IMU bias, actuation/model mismatch (simplified expert dynamics), and lack of photorealistic scene variability in simulation.",
            "transfer_enabling_conditions": "Key enabling factors: abstraction of raw sensory input (use of feature-track representation rather than raw pixels) to reduce observation-model discrepancy; preprocessing of IMU (de-biasing and integration/gravity alignment); privileged-learning setup (MPC expert in simulation) to provide dense, high-quality demonstrations; limited domain randomization of IMU biases and thrust-to-weight; training entirely in a high-fidelity physics simulator (Gazebo+RotorS); asynchronous network design to handle multi-rate sensors; and explicit formal argument (lemmas) showing abstraction reduces the simulation-to-reality gap (upper bound via Wasserstein distance).",
            "fidelity_requirements_identified": "No explicit numeric fidelity thresholds were prescribed; rather, the paper identifies representation fidelity requirements: the observation abstraction must preserve task-relevant information while being invariant to nuisance factors (e.g., feature tracks preserve geometry/egomotion information but are invariant to appearance), and physics fidelity should be sufficient to produce feasible expert trajectories; the authors formally show that reducing difference between observation models (via f) reduces the sim-to-real gap, but do not quantify required numeric error bounds.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Comparisons: (1) Visual-input modality: a network using raw images (pixel input) trained on Matty Flip had training error 90 ± 32 cm and 80% success in training environment, but completely failed (0% success) when tested on novel backgrounds; in contrast, the feature-track based controller ('Ours') had 53 ± 15 cm error and 100% success in training and maintained 100% success on novel backgrounds. (2) Modality ablations: IMU-only (no feature tracks) could complete many maneuvers but accumulated drift on long sequences and had lower accuracy; reference-only (open loop) performed very poorly. These results demonstrate that abstraction of visual input (feature tracks) substantially improves sim-to-real transfer compared to raw images, and that combining IMU+feature-tracks yields best transfer.",
            "key_findings": "The paper demonstrates reliable zero-shot sim-to-real transfer of an end-to-end sensorimotor controller for highly aggressive quadrotor acrobatics by (i) training entirely in Gazebo+RotorS on expert MPC demonstrations, (ii) using abstraction of sensory inputs (feature tracks + preprocessed IMU) to reduce the observation-model gap, and (iii) limited domain randomization of IMU biases and thrust-to-weight; raw-image policies do not generalize to novel visual domains and fail to transfer, IMU-only policies suffer drift on long maneuvers, and no real-world fine-tuning was necessary to achieve 100% success on the tested maneuvers, validating that appropriate abstraction and selected randomization can enable high-fidelity skill transfer from simulation to reality.",
            "uuid": "e1790.0",
            "source_info": {
                "paper_title": "Deep Drone Acrobatics",
                "publication_date_yy_mm": "2020-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep drone racing: From simulation to reality with domain randomization",
            "rating": 2
        },
        {
            "paper_title": "CAD2RL: Real single-image flight without a single real image",
            "rating": 2
        },
        {
            "paper_title": "Driving policy transfer via modularity and abstraction",
            "rating": 2
        },
        {
            "paper_title": "Does computer vision matter for action?",
            "rating": 1
        },
        {
            "paper_title": "Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search",
            "rating": 1
        }
    ],
    "cost": 0.011760999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Deep Drone Acrobatics</h1>
<p>Elia Kaufmann<em>‡, Antonio Loquercio</em>‡, René Ranftl ${ }^{\dagger}$, Matthias Müller ${ }^{\dagger}$, Vladlen Koltun ${ }^{\dagger}$, Davide Scaramuzza ${ }^{\ddagger}$</p>
<h4>Abstract</h4>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1: A quadrotor performs a Barrel Roll (left), a Power Loop (middle), and a Matty Flip (right). We safely train acrobatic controllers in simulation and deploy them with no fine-tuning (zero-shot transfer) on physical quadrotors. The approach uses only onboard sensing and computation. No external motion tracking was used.</p>
<h4>Abstract</h4>
<p>Performing acrobatic maneuvers with quadrotors is extremely challenging. Acrobatic flight requires high thrust and extreme angular accelerations that push the platform to its physical limits. Professional drone pilots often measure their level of mastery by flying such maneuvers in competitions. In this paper, we propose to learn a sensorimotor policy that enables an autonomous quadrotor to fly extreme acrobatic maneuvers with only onboard sensing and computation. We train the policy entirely in simulation by leveraging demonstrations from an optimal controller that has access to privileged information. We use appropriate abstractions of the visual input to enable transfer to a real quadrotor. We show that the resulting policy can be directly deployed in the physical world without any fine-tuning on real data. Our methodology has several favorable properties: it does not require a human expert to provide demonstrations, it cannot harm the physical system during training, and it can be used to learn maneuvers that are challenging even for the best human pilots. Our approach enables a physical quadrotor to fly maneuvers such as the Power Loop, the Barrel Roll, and the Matty Flip, during which it incurs accelerations of up to 3 g .</p>
<h2>SUPPLEMENTARY MATERIAL</h2>
<p>A video demonstrating acrobatic maneuvers is available at https://youtu.be/2N_wKXQ6MXA. Code can be found at https://github.com/uzh-rpg/deep_drone_acrobatics.</p>
<h2>I. INTRODUCTION</h2>
<p>Acrobatic flight with quadrotors is extremely challenging. Human drone pilots require many years of practice to safely master maneuvers such as power loops and barrel rolls ${ }^{1}$. Existing autonomous systems that perform agile maneuvers require external sensing and/or external computation [1-3]. For aerial vehicles that rely only on onboard sensing and computation, the high accelerations that are required for</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>acrobatic maneuvers together with the unforgiving requirements on the control stack raise fundamental questions in both perception and control. Therefore, they provide a natural benchmark to compare the capabilities of autonomous systems against trained human pilots.</p>
<p>Acrobatic maneuvers represent a challenge for the actuators, the sensors, and all physical components of a quadrotor. While hardware limitations can be resolved using expert-level equipment that allows for extreme accelerations, the major limiting factor to enable agile flight is reliable state estimation. Visionbased state estimation systems either provide significantly reduced accuracy or completely fail at high accelerations due to effects such as motion blur, large displacements, and the difficulty of robustly tracking features over long time frames [4]. Additionally, the harsh requirements of fast and precise control at high speeds make it difficult to tune controllers on the real platform, since even tiny mistakes can result in catastrophic outcomes for the platform.</p>
<p>The difficulty of agile autonomous flight led previous work to mostly focus on specific aspects of the problem. One line of research focused on the control problem, assuming near-perfect state estimation from external sensors [1-3, 5]. While these works showed impressive examples of agile flight, they focused purely on control. The issues of reliable perception and state estimation during agile maneuvers were cleverly circumvented by instrumenting the environment with sensors (such as Vicon and OptiTrack) that provide near-perfect state estimation to the platform at all times. Recent works addressed the control and perception aspects in an integrated way via techniques like perception-guided trajectory optimization [6-8] or training end-to-end visuomotor agents [9]. However, acrobatic performance of high-acceleration maneuvers with only onboard sensing and computation has not yet been achieved.</p>
<p>In this paper, we show for the first time that a visionbased autonomous quadrotor with only onboard sensing and computation is capable of autonomously performing agile</p>
<p>maneuvers with accelerations of up to 3g, as shown in Fig. 1. This contribution is enabled by a novel simulation-to-reality transfer strategy, which is based on abstraction of both visual and inertial measurements. We demonstrate both formally and empirically that the presented abstraction strategy decreases the simulation-to-reality gap with respect to a naive use of sensory inputs. Equipped with this strategy, we train an end-to-end sensimotor controller to fly acrobatic maneuvers exclusively in simulation. Learning agile maneuvers entirely in simulation has several advantages: (i) Maneuvers can be simply specified by reference trajectories in simulation and do not require expensive demonstrations by a human pilot, (ii) training is safe and does not pose any physical risk to the quadrotor, and (iii) the approach can scale to a large number of diverse maneuvers, including ones that can only be performed by the very best human pilots.</p>
<p>Our sensorimotor policy is represented by a neural network that combines information from different input modalities to directly regress thrust and body rates. To cope with different output frequencies of the onboard sensors, we design an asynchronous network that operates independently of the sensor frequencies. This network is trained in simulation to imitate demonstrations from an optimal controller that has access to privileged state information.</p>
<p>We apply the presented approach to learning autonomous execution of three acrobatic maneuvers that are challenging even for expert human pilots: the Power Loop, the Barrel Roll, and the Matty Flip. Through controlled experiments in simulation and on a real quadrotor, we show that the presented approach leads to robust and accurate policies that are able to reliably perform the maneuvers with only onboard sensing and computation.</p>
<h2>II. Related Work</h2>
<p>Acrobatic maneuvers comprehensively challenge perception and control stacks. The agility that is required to perform acrobatic maneuvers requires carefully tuned controllers together with accurate state estimation. Compounding the challenge, the large angular rates and high speeds that arise during the execution of a maneuver induce strong motion blur in vision sensors and thus compromise the quality of state estimation.</p>
<p>The complexity of the problem has led early works to only focus on the control aspect while disregarding the question of reliable perception. Lupashin et al. [1] proposed iterative learning of control strategies to enable platforms to perform multiple flips. Mellinger et al. [10] used a similar strategy to autonomously fly quadrotors through a tilted window [10]. By switching between two controller settings, Chen et al. [11] also demonstrated multi-flip maneuvers. Abbeel et al. [2] learned to perform a series of acrobatic maneuvers with autonomous helicopters. Their algorithm leverages expert pilot demonstrations to learn task-specific controllers. While these works proved the ability of flying machines to perform agile maneuvers, they did not consider the perception problem. Indeed, they all assume that near-perfect state estimation is available during the maneuver, which in practice requires instrumenting the environment with dedicated sensors.</p>
<p>Aggressive flight with only onboard sensing and computation is an open problem. The first attempts in this direction were made by Shen et al. [8], who demonstrated agile vision-based flight. The work was limited to low-acceleration trajectories, therefore only accounting for part of the control and perception problems encountered at high speed. More recently, Loianno et al. [12] and Falanga et al. [7] demonstrated aggressive flight through narrow gaps with only onboard sensing. Even though these maneuvers are agile, they are very short and cannot be repeated without re-initializing the estimation pipeline. Using perception-guided optimization, Falanga et al. [6] and Lee et al. [13] proposed a model-predictive control framework to plan aggressive trajectories while minimizing motion blur. However, such control strategies are too conservative to fly acrobatic maneuvers, which always induce motion blur.</p>
<p>Abolishing the classic division between perception and control, a recent line of work proposes to train visuomotor policies directly from data. Similarly to our approach, Zhang et al. [9] trained a neural network from demonstrations provided by an MPC controller. While the latter has access to the full state of the platform and knowledge of obstacle positions, the network only observes laser range finder readings and inertial measurements. Similarly, Li et al. [14] proposed an imitation learning approach for training visuomotor agents for the task of quadrotor flight. The main limitation of these methods is in their sample complexity: large amounts of demonstrations are required to fly even straight-line trajectories. As a consequence, these methods were only validated in simulation or were constrained to slow hover-to-hover trajectories.</p>
<p>Our approach employs abstraction of sensory input [15] to reduce the problem’s sample complexity and enable zero-shot sim-to-real transfer. While prior work has demonstrated the possibility of controlling real-world quadrotors with zero-shot sim-to-real transfer [16, 17], our approach is the first to learn an end-to-end sensorimotor mapping - from sensor measurements to low-level controls - that can perform high-speed and high-acceleration acrobatic maneuvers on a real physical system.</p>
<h2>III. OVERVIEW</h2>
<p>In order to perform acrobatic maneuvers with a quadrotor, we train a sensorimotor controller to predict low-level actions from a history of onboard sensor measurements and a userdefined reference trajectory. An observation $\boldsymbol{o}[k] \in \mathbb{O}$ at time $k \in[0, \ldots, T]$ consists of a camera image $\mathcal{I}[k]$ and an inertial measurement $\phi[k]$. Since the camera and IMU typically operate at different frequencies, the visual and inertial observations are updated at different rates. The controller's output is an action $\boldsymbol{u}[k]=\left[c, \boldsymbol{\omega}^{\top}\right]^{\top} \in \mathbb{U}$ that consists of continuous massnormalized collective thrust $c$ and bodyrates $\boldsymbol{\omega}=\left[\omega_{x}, \omega_{y}, \omega_{z}\right]^{\top}$ that are defined in the quadrotor body frame.</p>
<p>The controller is trained via privileged learning [18]. Specifically, the policy is trained on demonstrations that are provided by a privileged expert: an optimal controller that has access to privileged information that is not available to the</p>
<p>sensorimotor student, such as the full ground-truth state of the platform $\boldsymbol{s}[k] \in \mathbb{S}$. The privileged expert is based on a classic optimization-based planning and control pipeline that tracks a reference trajectory from the state $\boldsymbol{s}[k]$ using MPC [6].</p>
<p>We collect training demonstrations from the privileged expert in simulation. Training in simulation enables synthesis and use of unlimited training data for any desired trajectory, without putting the physical platform in danger. This includes maneuvers that stretch the abilities of even expert human pilots. To facilitate zero-shot simulation-to-reality transfer, the sensorimotor student does not directly access raw sensor input such as color images. Rather, the sensorimotor controller acts on an abstraction of the input, in the form of feature points extracted via classic computer vision. Such abstraction supports sample-efficient training, generalization, and simulation-toreality transfer $[15,19]$.</p>
<p>The trained sensorimotor student does not rely on any privileged information and can be deployed directly on the physical platform. We deploy the trained controller to perform acrobatic maneuvers in the physical world, with no adaptation required.</p>
<p>The next section presents each aspect of our method in detail.</p>
<h2>IV. Method</h2>
<p>We define the task of flying acrobatic maneuvers with a quadrotor as a discrete-time, continuous-valued optimization problem. Our task is to find an end-to-end control policy $\pi: \mathbb{O} \rightarrow \mathbb{U}$, defined by a neural network, which minimizes the following finite-horizon objective:</p>
<p>$$
\min <em _rho_pi_="\rho(\pi)">{\pi} J(\pi)=\mathbb{E}</em>[k]\right)\right]
$$}\left[\sum_{k=0}^{k=T} \mathcal{C}\left(\boldsymbol{\tau}_{r}[k], \boldsymbol{s</p>
<p>where $\mathcal{C}$ is a quadratic cost depending on a reference trajectory $\boldsymbol{\tau}_{r}[k]$ and the quadrotor state $\boldsymbol{s}[k]$, and $\rho(\pi)$ is the distribution of possible trajectories ${(\boldsymbol{s}[0], \boldsymbol{o}[0], \boldsymbol{u}[0]), \ldots,(\boldsymbol{s}[T], \boldsymbol{o}[T], \boldsymbol{u}[T])}$ induced by the policy $\pi$.</p>
<p>We define the quadrotor state $\boldsymbol{s}[k]=[\boldsymbol{p}, \boldsymbol{q}, \boldsymbol{v}, \boldsymbol{\omega}]$ as the platform position $\boldsymbol{p}$, its orientation quaternion $\boldsymbol{q}$, and their derivatives. Note that the agent $\pi$ does not directly observe the state $\boldsymbol{s}[k]$. We further define the reference trajectory $\boldsymbol{\tau}_{r}[k]$ as a time sequence of reference states which describe the desired trajectory. We formulate the cost $\mathcal{C}$ as</p>
<p>$$
\mathcal{C}\left(\boldsymbol{\tau}_{r}[k], \boldsymbol{s}[k]\right)=\boldsymbol{x}[k]^{\top} \mathcal{L} \boldsymbol{x}[k]
$$</p>
<p>where $\boldsymbol{x}[k]=\boldsymbol{\tau}_{r}[k]-\boldsymbol{s}[k]$ denotes the difference between the state of the platform and the corresponding reference at time $k$, and $\mathcal{L}$ is a positive-semidefinite cost matrix.</p>
<h2>A. Reference Trajectories</h2>
<p>Both the privileged expert and the learned policy assume access to a reference trajectory $\boldsymbol{\tau}_{r}[k]$ that specifies an acrobatic maneuver. To ensure that such reference is dynamically feasible, it has to satisfy constraints that are imposed by the physical limits and the underactuated nature of the quadrotor platform. Neglecting aerodynamic drag and motor dynamics, the dynamics of the quadrotor can be modelled as</p>
<p>$$
\begin{aligned}
\dot{\boldsymbol{p}}<em _mathrm_WB="\mathrm{WB">{\mathrm{WB}} &amp; =\boldsymbol{v}</em> \
\dot{\boldsymbol{v}}}<em _mathrm_WB="\mathrm{WB">{\mathrm{WB}} &amp; =\mathrm{w} \boldsymbol{g}+\boldsymbol{q}</em>}} \odot \boldsymbol{c<em _mathrm_WB="\mathrm{WB">{\mathrm{B}} \
\dot{\boldsymbol{q}}</em>}} &amp; =\frac{1}{2} \Lambda\left(\boldsymbol{\omega<em _mathrm_WB="\mathrm{WB">{\mathrm{B}}\right) \cdot \boldsymbol{q}</em> \
\dot{\boldsymbol{\omega}}}<em _mathrm_B="\mathrm{B">{\mathrm{B}} &amp; =\dot{\boldsymbol{J}}^{-1} \cdot\left(\boldsymbol{\eta}-\boldsymbol{\omega}</em>\right)
\end{aligned}
$$}} \times \boldsymbol{J} \cdot \boldsymbol{\omega}_{\mathrm{B}</p>
<p>where $\boldsymbol{p}<em _mathrm_WB="\mathrm{WB">{\mathrm{WB}}, \boldsymbol{v}</em>}}, \boldsymbol{q<em _mathrm_w="\mathrm{w">{\mathrm{WB}}$ denote the position, linear velocity, and orientation of the platform body frame with respect to the world frame. The gravity vector ${ }</em>}} \boldsymbol{g}$ is expressed in the world frame and $\boldsymbol{q<em _mathrm_B="\mathrm{B">{\mathrm{WB}} \odot \boldsymbol{c}</em>}}$ denotes the rotation of the massnormalized thrust vector $\boldsymbol{c<em _mathrm_WB="\mathrm{WB">{\mathrm{B}}=(0,0, c)^{\top}$ by quaternion $\boldsymbol{q}</em>$ are the torques acting on the body due to the motor thrusts.}}$. The time derivative of a quaternion $\boldsymbol{q}=\left(q_{w}, q_{x}, q_{y}, q_{z}\right)^{\top}$ is given by $\dot{\boldsymbol{q}}=\frac{1}{2} \Lambda(\boldsymbol{\omega}) \cdot \boldsymbol{q}$ and $\Lambda(\boldsymbol{\omega})$ is a skew-symmetric matrix of the vector $\left(0, \boldsymbol{\omega}^{\top}\right)^{\top}=\left(0, \omega_{x}, \omega_{y}, \omega_{z}\right)^{\top}$. The diagonal matrix $\boldsymbol{J}=\operatorname{diag}\left(J_{x x}, J_{y y}, J_{z z}\right)$ denotes the quadrotor inertia, and $\boldsymbol{\eta} \in \mathbb{R}^{3</p>
<p>Instead of directly planning in the full state space, we plan reference trajectories in the space of flat outputs $\boldsymbol{z}=[x, y, z, \psi]^{\top}$ proposed in [20], where $x, y, z$ denote the position of the quadrotor and $\psi$ is the yaw angle. It can be shown that any smooth trajectory in the space of flat outputs can be tracked by the underactuated platform (assuming reasonably bounded derivatives).</p>
<p>The core part of each acrobatic maneuver is a circular motion primitive with constant tangential velocity $\boldsymbol{v}<em l="l">{l}$. The orientation of the quadrotor is constrained by the thrust vector the platform needs to produce. Consequently, the desired platform orientation is undefined when there is no thrust. To ensure a well-defined reference trajectory through the whole circular maneuver, we constrain the norm of the tangential velocity $\boldsymbol{v}</em>$ to be larger by a margin $\varepsilon$ than the critical tangential velocity that would lead to free fall at the top of the maneuver:</p>
<p>$$
\left|\boldsymbol{v}_{l}\right|&gt;\varepsilon \sqrt{r g}
$$</p>
<p>where $r$ denotes the radius of the loop, $g=9.81 \mathrm{~m} \mathrm{~s}^{-2}$, and $\varepsilon=1.1$.</p>
<p>While the circular motion primitives form the core part of the agile maneuvers, we use constrained polynomial trajectories to enter, transition between, and exit the maneuvers. A polynomial trajectory is described by four polynomial functions specifying the independent evolution of the components of $\boldsymbol{z}$ over time:</p>
<p>$$
z_{i}(t)=\sum_{j=0}^{j=P_{i}} a_{i j} \cdot t^{j} \quad \text { for } \quad i \in{0,1,2,3}
$$</p>
<p>We use polynomials of order $P_{i}=7$ for the position components $(i={0,1,2})$ of the flat outputs and $P_{i}=2$ for the yaw component $(i=3)$. By enforcing continuity for both start $(t=0)$ and end $\left(t=t_{m}\right)$ of the trajectory down to the 3rd derivative of position, the trajectory is fully constrained. We minimize the execution time $t_{m}$ of the polynomial trajectories, while constraining the maximum speed, thrust, and body rates throughout the maneuver.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 3: Reference trajectories for acrobatic maneuvers. Top row, from left to right: Power Loop, Barrel Roll, and Matty Flip. Bottom row: Combo.</p>
<p>Finally, the trajectories are concatenated to the full reference trajectory, which is then converted back to the full state-space representation $\tau_{r}(t)$ [20]. Subsequent sampling with a frequency of 50 Hz results in the discrete-time representation $\tau_{r}[k]$ of the reference trajectory. Some example trajectories are illustrated in Figure 3.</p>
<h3><em>B. Privileged Expert</em></h3>
<p>Our privileged expert $\pi^*$ consists of an MPC [6] that generates collective thrust and body rates via an optimization-based scheme. The controller operates on the simplified dynamical model of a quadrotor proposed in [21]:</p>
<p>$$
\begin{aligned}
\dot{\boldsymbol{p}}<em _mathrm_WB="\mathrm{WB">{\mathrm{WB}} &amp;= \boldsymbol{v}</em> \
\dot{\boldsymbol{v}}}<em _mathrm_w="\mathrm{w">{\mathrm{WB}} &amp;= </em>}} \boldsymbol{g} + \boldsymbol{q<em _mathrm_B="\mathrm{B">{\mathrm{WB}} \odot \boldsymbol{c}</em> \
\dot{\boldsymbol{q}}}<em _mathrm_WB="\mathrm{WB">{\mathrm{WB}} &amp;= \frac{1}{2} \Lambda\left(\boldsymbol{\omega}_B\right) \cdot \boldsymbol{q}</em>
\end{aligned}
\tag{6}
$$}</p>
<p>In contrast to the model (3), the simplified model neglects the dynamics of the angular rates. The MPC repeatedly optimizes the open-loop control problem over a receding horizon of N time steps and applies the first control command from the optimized sequence. Specifically, the action computed by the MPC is the first element of the solution to the following optimization problem:</p>
<p>$$
\begin{aligned}
\pi^* &amp;= \min_{\mathbf{u}} \left{ \mathbf{x}[N]^{\top} \mathcal{Q}\mathbf{x}[N] + \sum_{k=1}^{N-1} \left( \mathbf{x}[k]^{\top} \mathcal{Q}\mathbf{x}[k] + \mathbf{u}[k]^{\top} \mathcal{R}\mathbf{u}[k] \right) \right} \
&amp;\text{s.t.} \quad \mathbf{r}(\mathbf{x}, \mathbf{u}) = 0 \
&amp;\quad \mathbf{h}(\mathbf{x}, \mathbf{u}) \le 0,
\end{aligned}
\tag{7}
$$</p>
<p>where $\mathbf{x}[k] = \tau_r[k] - s[k]$ denotes the difference between the state of the platform at time $k$ and the corresponding reference $\tau_r[k]$, $\mathbf{r}(\mathbf{x}, \mathbf{u})$ are equality constraints imposed by the system dynamics (6), and $\mathbf{h}(\mathbf{x}, \mathbf{u})$ are optional bounds on inputs and states. $\mathcal{Q}$, $\mathcal{R}$ are positive-semidefinite cost matrices.</p>
<h3><em>C. Learning</em></h3>
<p>The sensorimotor controller is trained by imitating demonstrations provided by the privileged expert. While the expert has access to privileged information in the form of ground-truth state estimates, the sensorimotor controller does not access any privileged information and can be directly deployed in the physical world [18].</p>
<p>A lemma by Pan et al. [22] formally defines an upper bound between the expert and the student performance as</p>
<p>$$
\begin{aligned}
J(\pi) - J(\pi^<em>) &amp;\leq C_{\pi^</em>} \mathbb{E}<em _rho_pi_="\rho(\pi)">{\rho(\pi)} [DW(\pi, \pi^<em>)] \
&amp;\leq C_{\pi^</em>} \mathbb{E}</em>} \mathbb{E<em _mathbf_u="\mathbf{u">{\mathbf{u}^<em> \sim \pi^</em>} \mathbb{E}</em>|],
\end{aligned}
\tag{8}
$$} \sim \pi} [|\mathbf{u}^* - \mathbf{u</p>
<p>where $DW(\cdot, \cdot)$ is the Wasserstein metric [23] and $C_{\pi^<em>}$ is a constant depending on the smoothness of expert actions. Finding an agent $\pi$ with the same performance as the privileged controller $\pi^</em>$ boils down to minimizing the discrepancy in actions between the two policies on the expected agent trajectories $\rho(\pi)$.</p>
<p>The aforementioned discrepancy can be minimized by an iterative supervised learning process known as DAGGER [24]. This process iteratively collects data by letting the student control the platform, annotating the collected observations with the experts' actions, and updating the student controller based on the supervised learning problem</p>
<p>$$
\pi = \min_{\pi} \mathbb{E}_{\mathbf{s}[k] \sim \rho(\pi)} [|\mathbf{u}^*(s[k]) - \hat{\pi}(\mathbf{o}[k])|],
\tag{9}
$$</p>
<p>where $\mathbf{u}^<em>(s[k])$ is the expert action and $\mathbf{o}[k]$ is the observation vector in the state $s[k]$. Running this process for $O(N)$ iterations yields a policy $\pi$ with performance $J(\pi) \le J(\pi^</em>) + O(N)$ [24].</p>
<p>Naive application of this algorithm to the problem of agile flight in the physical world presents two major challenges: how can the expert access the ground-truth state $s[k]$ and how can we protect the platform from damage when the partially trained student $\pi$ is in control? To circumvent these challenges, we train exclusively in simulation. This significantly simplifies the training procedure, but presents a new hurdle: how do we minimize the difference between the sensory input received by the controller in simulation and reality?</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 4: Network architecture. The network receives a history of feature tracks, IMU measurements, and reference trajectories as input. Each input modality is processed using temporal convolutions and updated at different input rates. The resulting intermediate representations are processed by a multi-layer perceptron at a fixed output rate to produce collective thrust and body rate commands.</p>
<p>Our approach to bridging the gap between simulation and reality is to leverage <em>abstraction</em> [15]. Rather than operating on raw sensory input, our sensorimotor controller operates on an intermediate representation produced by a perception module [19]. This intermediate representation is more consistent across simulation and reality than raw visual input.</p>
<p>We now formally show that training a network on abstractions of sensory input reduces the gap between simulation and reality. Let $M(\boldsymbol{z} \mid \boldsymbol{s}), L(\boldsymbol{z} \mid \boldsymbol{s}): \mathbb{S} \rightarrow \mathbb{O}$ denote the observation models in the real world and in simulation, respectively. Such models describe how an on-board sensor measurement $\boldsymbol{z}$ senses a state $\boldsymbol{s}$. We further define $\pi_{r} = \mathbb{E}<em _boldsymbol_o="\boldsymbol{o">{\boldsymbol{o}_r \sim M(\boldsymbol{s})} \left[ \pi(\boldsymbol{o}_r[k]) \right]$ and $\pi_s = \mathbb{E}</em>_s[k]) \right]$ as the realizations of the policy $\pi$ in the real world and in simulation. The following lemma shows that, disregarding actuation differences, the distance between the observation models upper-bounds the gap in performance in simulation and reality.}_s \sim L(\boldsymbol{s})} \left[ \pi(\boldsymbol{o</p>
<p><strong>Lemma 1:</strong> For a Lipschitz-continuous policy $\pi$ the simulation-to-reality gap $J(\pi_r) - J(\pi_s)$ is upper-bounded by</p>
<p>$$J(\pi_r) - J(\pi_s) \leq C_{\pi_r} K \mathbb{E}_{p(\pi_r)} [DW(M, L)],\tag{10}$$</p>
<p>where $K$ denotes the Lipschitz constant.</p>
<p><strong>Proof:</strong> The lemma follows directly from (8) and the fact that</p>
<p>$$\begin{split} DW(\pi_r, \pi_s) &amp;= \inf_{\gamma \in \Pi(\boldsymbol{o}<em _boldsymbol_o="(\boldsymbol{o">r, \boldsymbol{o}_s)} \mathbb{E}</em><em _Pi_boldsymbol_o="\Pi(\boldsymbol{o" _gamma="\gamma" _in="\in">r, \boldsymbol{o}_s)} [d_p(\pi_r, \pi_s)] \ &amp; \leq K \inf</em><em _boldsymbol_o="(\boldsymbol{o">r, \boldsymbol{o}_s)} \mathbb{E}</em>$$}_r, \boldsymbol{o}_s)} [d_o(\boldsymbol{o}_r, \boldsymbol{o}_s)] \ &amp;= K \cdot DW(M, L),\end{split</p>
<p>where $d_o$ and $d_p$ are distances in observation and action space, respectively.</p>
<p>We now consider the effect of abstraction of the input observations. Let $f$ be a mapping of the observations such that</p>
<p>$$DW(f(M), f(L)) \le DW(M, L).\tag{11}$$</p>
<p>The mapping $f$ is task-dependent and is generally designed – with domain knowledge – to contain sufficient information to solve the task while being invariant to nuisance factors. In our case, we use feature tracks as abstraction of camera frames. The feature tracks are provided by a visual-inertial odometry (VIO) system. In contrast to camera frames, feature tracks primarily depend on scene geometry, rather than surface appearance. We also make inertial measurements independent of environmental conditions, such as temperature and pressure, by integration and de-biasing. As such, our input representations fulfill the requirements of Eq. (11).</p>
<p>As the following lemma shows, training on such representations reduces the gap between task performance in simulation and the real world.</p>
<p><strong>Lemma 2:</strong> A policy that acts on an abstract representation of the observations $\pi_f : f(\mathbb{O}) \rightarrow \mathbb{U}$ has a lower simulation-to-reality gap than a policy $\pi_o : \mathbb{O} \rightarrow \mathbb{U}$ that acts on raw observations.</p>
<p><strong>Proof:</strong> The lemma follows directly from (10) and (11).</p>
<h3><em>D. Sensorimotor Controller</em></h3>
<p>In contrast to the expert policy $\pi^*$, the student policy $\pi$ is only provided with onboard sensor measurements from the forward-facing camera and the IMU. There are three main challenges for the controller to tackle: (i) it should keep track of its state based on the provided inputs, akin to a visual-inertial odometry system [25, 26], (ii) it should be invariant to environments and domains, so as to not require retraining for each scene, and (iii) it should process sensor readings that are provided at different frequencies.</p>
<p>We represent the policy as a neural network that fulfills all of the above requirements. The network consists of three input branches that process visual input, inertial measurements, and the reference trajectory, followed by a multi-layer perceptron that produces actions. The architecture is illustrated in Fig. 4. Similarly to visual-inertial odometry systems [25–27], we provide the network with a representation of the platform state by supplying it with a history of length $L = 8$ of visual and inertial information.</p>
<p>To ensure that the learned policies are scene- and domain-independent, we provide the network with appropriate abstractions of the inputs instead of directly feeding raw inputs. We design these abstractions to contain sufficient information to solve the task while being invariant to environmental factors that are hard to simulate accurately and are thus unlikely to transfer from simulation to reality.</p>
<p>The distribution of raw IMU measurements depends on the exact sensor as well as environmental factors such as pressure and temperature. Instead of using the raw measurements as input to the policy, we preprocess the IMU signal by applying bias subtraction and gravity alignment. Modern visual-inertial odometry systems perform a similar pre-integration of the inertial data in their optimization backend [28]. The resulting inertial signal contains the estimated platform velocity, orientation, and angular rate.</p>
<p>We use the history of filtered inertial measurements, sampled at 100 Hz, and process them using temporal convolutions [29]. Specifically, the inertial branch consists of a temporal convolutional layer with 128 filters, followed by three temporal convolutions with 64 filters each. A final fully-connected layer maps the signal to a 128-dimensional representation.</p>
<p>Another input branch processes a history of reference velocities, orientations, and angular rates. It has the same structure as the inertial branch. New reference states are added to the history at a rate of 50 Hz.</p>
<p>For the visual signal, we use feature tracks, i.e. the motion of salient keypoints in the image plane, as an abstraction of the input. Feature tracks depend on the scene structure, ego-motion, and image gradients, but not on absolute pixel intensities. At the same time, the information contained in the feature tracks is sufficient to infer the ego-motion of the platform up to an unknown scale. Information about the scale can be recovered from the inertial measurements. We leverage the computationally efficient feature extraction and tracking frontend of VINS-Mono [28] to generate feature tracks. The frontend extracts Harris corners [30] and tracks them using the Lucas-Kanade method [31]. We perform geometric verification and exclude correspondences with a distance of more than 2 pixels from the epipolar line. We represent each feature track by a 5-dimensional vector that consists of the keypoint position, its displacement with respect to the previous keyframe (both on the rectified image plane), and the number of keyframes that the feature has been tracked (a measure of keypoint quality).</p>
<p>To facilitate efficient batch training, we randomly sample 40 keypoints per keyframe. The features are processed by a reduced version of the PointNet architecture proposed in [32] before we generate a fixed-size representation at each timestep. Specifically, we reduce the number of hidden layers from 6 to 4, with $32,64,128,128$ filters, respectively, in order to reduce latency. The output of this subnetwork is reduced to a 128-dimensional vector by global average pooling over the feature tracks. The history of resulting hidden representations is then processed by a temporal convolutional network that has the same structure as the inertial and reference branches.</p>
<p>Finally, the outputs of the individual branches are concatenated and processed by a synchronous multi-layer perceptron with three hidden layers of size $128,64,32$. The final outputs are the body rates and collective thrust that are used to control the platform.</p>
<p>We account for the different input frequencies by allowing each of the input branches to operate asynchronously. Each branch operates independently from the others by generating an output only when a new input from the sensor arrives. The multilayer perceptron uses the latest outputs from the asynchronous branches and operates at 100 Hz. It outputs control commands at approximately the same rate due to its minimal computational overhead.</p>
<h3>V-E Implementation Details</h3>
<p>We use the Gazebo simulator to train our policies. Gazebo can model the physics of quadrotors with high fidelity using the RotorS extension [33]. We simulate the AscTec Hummingbird multirotor, which is equipped with a forward-facing fisheye camera. The platform is instantiated in a cubical simulated flying space with a side length of 70 meters. An example image is shown in Fig. 5 (left).</p>
<p>For the real-world experiments we use a custom quadrotor that weighs 1.15 kg and has a thrust-to-weight ratio of 4:1. We use a Jetson TX2 for neural network inference. Images and inertial measurements are provided by an Intel RealSense T265 camera.</p>
<p>We use an off-policy learning approach. We execute the trained policy, collect rollouts, and add them to a dataset. After 30 new rollouts are added, we train for 40 epochs on the entire dataset. This collect-rollouts-and-train procedure is repeated 5 times: there are 150 rollouts in the dataset by the end. We use the Adam optimizer [34] with a learning rate of $3e-4$. We always use the latest available model for collecting rollouts. We execute a student action only if the difference to the expert action is smaller than a threshold $t=1.0$ to avoid crashes in the early stages of training. We double the threshold $t$ every 30 rollouts. We perform a random action with a probability $p=30\%$ at every stage of the training to increase the coverage of the state space. To facilitate transfer from simulation to reality, we randomize the IMU biases and the thrust-to-weight ratio of the platform by up to $10\%$ of their nominal value in every iteration. We do not perform any randomization of the geometry and appearance of the scene during data collection.</p>
<h2>V. EXPERIMENTS</h2>
<p>We design our evaluation procedure to address the following questions. Is the presented sensorimotor controller advantageous to a standard decomposition of state estimation and control? What is the role of input abstraction in facilitating transfer from simulation to reality? Finally, we validate our design choices with ablation studies.</p>
<h3>V-A Experimental Setup</h3>
<p>We learn sensorimotor policies for three acrobatic maneuvers that are popular among professional drone pilots as well as a</p>
<table>
<thead>
<tr>
<th>Maneuver</th>
<th>Input</th>
<th></th>
<th></th>
<th>Power Loop</th>
<th></th>
<th>Barrel Roll</th>
<th></th>
<th>Matty Flip</th>
<th></th>
<th>Combo</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Ref</td>
<td>IMU</td>
<td>FT</td>
<td>Error (↓)</td>
<td>Success (↑)</td>
<td>Error (↓)</td>
<td>Success (↑)</td>
<td>Error (↓)</td>
<td>Success (↑)</td>
<td>Error (↓)</td>
<td>Success (↑)</td>
</tr>
<tr>
<td>VIO-MPC</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>$43 \pm 14$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$79 \pm 43$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$92 \pm 41$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$164 \pm 51$</td>
<td>$70 \%$</td>
</tr>
<tr>
<td>Ours (Only Ref)</td>
<td>✓</td>
<td></td>
<td></td>
<td>$250 \pm 50$</td>
<td>$20 \%$</td>
<td>$485 \pm 112$</td>
<td>$85 \%$</td>
<td>$340 \pm 120$</td>
<td>$15 \%$</td>
<td>$\infty$</td>
<td>$0 \%$</td>
</tr>
<tr>
<td>Ours (No IMU)</td>
<td>✓</td>
<td></td>
<td>✓</td>
<td>$210 \pm 100$</td>
<td>$30 \%$</td>
<td>$543 \pm 95$</td>
<td>$85 \%$</td>
<td>$380 \pm 100$</td>
<td>$20 \%$</td>
<td>$\infty$</td>
<td>$0 \%$</td>
</tr>
<tr>
<td>Ours (No FT)</td>
<td>✓</td>
<td>✓</td>
<td></td>
<td>$28 \pm 8$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$64 \pm 24$</td>
<td>$95 \%$</td>
<td>$67 \pm 29$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$134 \pm 113$</td>
<td>$85 \%$</td>
</tr>
<tr>
<td>Ours</td>
<td>✓</td>
<td>✓</td>
<td>✓</td>
<td>$\mathbf{2 4} \pm \mathbf{5}$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$\mathbf{5 8} \pm \mathbf{9}$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$\mathbf{5 3} \pm \mathbf{1 5}$</td>
<td>$\mathbf{1 0 0 \%}$</td>
<td>$\mathbf{1 2 8} \pm \mathbf{5 7}$</td>
<td>$\mathbf{9 5 \%}$</td>
</tr>
</tbody>
</table>
<p>TABLE I: Comparison of different variants of our approach with the baseline (VIO-MPC) in terms of the average tracking error in centimeters and the success rate. Results were averaged over 20 runs. Agents without access to IMU data perform poorly. An agent that has access only to IMU measurements has a significantly lower tracking error than the baseline. Adding feature tracks further improves tracking performance and success rate, especially for longer and more complicated maneuvers.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Fig. 6: Tracking error (left) and success rate (right) over time when a maneuver is executed repeatedly in simulation. The controllers were trained to complete the maneuver for six seconds and generalize well to longer sequences. Our learned controller, which leverages both IMU and visual data, provides consistently good performance without a single failure.
policy that consists of a sequence of multiple maneuvers.</p>
<p>1) Power Loop: Accelerate over a distance of 4 m to a speed of $4.5 \mathrm{~m} \mathrm{~s}^{-1}$ and enter a loop maneuver with a radius of $r=1.5 \mathrm{~m}$.
2) Barrel Roll: Accelerate over a distance of 3 m to a speed of $4.5 \mathrm{~m} \mathrm{~s}^{-1}$ and enter a roll maneuver with a radius of $r=1.5 \mathrm{~m}$.
3) Matty Flip: Accelerate over a distance of 4 m to a speed of $4.5 \mathrm{~m} \mathrm{~s}^{-1}$ while yawing $180^{\circ}$ and enter a backwards loop maneuver with a radius of $r=1.5 \mathrm{~m}$.
4) Combo: This sequence starts with a triple Barrel Roll, followed by a double Power Loop, and ends with a Matty Flip. The full maneuver is executed without stopping between maneuvers.
The maneuvers are listed by increasing difficulty. The trajectories of these maneuvers are illustrated in Fig. 3. They contain high accelerations and fast angular velocities around the body axes of the platform. All maneuvers start and end in the hover condition.</p>
<p>For comparison, we construct a strong baseline by combining visual-inertial odometry [28] and model predictive control [6]. Our baseline receives the same inputs as the learned controllers: inertial measurements, camera images, and a reference trajectory.</p>
<p>We define two metrics to compare different approaches. We measure the average root mean square error in meters of
the reference position with respect to the true position of the platform during the execution of the maneuver. Note that we can measure this error only for simulation experiments, as it requires exact state estimation. We thus define a second metric, the average success rate for completing a maneuver. In simulation, we define success as not crashing the platform into any obstacles during the maneuver. For the real-world experiments, we consider a maneuver successful if the safety pilot did not have to intervene during the execution and the maneuver was executed correctly.</p>
<h2>B. Experiments in Simulation</h2>
<p>We first evaluate the performance for individual maneuvers in simulation. The results are summarized in Table I. The learned sensorimotor controller that has access to both visual and inertial data (Ours) is consistently the best across all maneuvers. This policy exhibits a lower tracking error by up to $45 \%$ in comparison to the strong VIO-MPC baseline. The baseline can complete the simpler maneuvers with perfect success rate, but generally has higher tracking error due to drift in state estimation. The gap between the baseline and our controller widens for longer and more difficult sequences.</p>
<p>Table I also highlights the relative importance of the input modalities. Policies that only receive the reference trajectories but no sensory input (Ours (Only Ref)) - effectively operating open-loop - perform poorly across all maneuvers. Policies</p>
<p>| Input | Train | | Test 1 | | Test 2 | |
| | Error (↓) | Success (↑) | Error (↓) | Success (↑) | Error (↓) | Success (↑) |
| --- | --- | --- | --- | --- | --- | --- |
| Image | $90 \pm 32$ | $80 \%$ | $\infty$ | $0 \%$ | $\infty$ | $0 \%$ |
| Ours | $53 \pm 15$ | $100 \%$ | $58 \pm 18$ | $100 \%$ | $61 \pm 11$ | $100 \%$ |</p>
<p>TABLE II: Sim-to-sim transfer for different visual input modalities. Policies that directly rely on images as input do not transfer to scenes with novel appearance (Test 1, Test 2). Feature tracks enable reliable transfer. Results are averaged over 10 runs.</p>
<table>
<thead>
<tr>
<th>Maneuver</th>
<th>Power Loop</th>
<th>Barrel Roll</th>
<th>Matty Flip</th>
</tr>
</thead>
<tbody>
<tr>
<td>Ours (No FT)</td>
<td>$100 \%$</td>
<td>$90 \%$</td>
<td>$100 \%$</td>
</tr>
<tr>
<td>Ours</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
<td>$100 \%$</td>
</tr>
</tbody>
</table>
<p>TABLE III: Success rate across 10 runs on the physical platform.
that have access to visual input but not to inertial data (Ours (No IMU)) perform similarly poorly since they do not have sufficient information to determine the absolute scale of the ego-motion. On the other hand, policies that only rely on inertial data for sensing (Ours (No FT)) are able to safely fly most maneuvers. Even though such controllers only have access to inertial data, they exhibit significantly lower tracking error than the VIO-MPC baseline. However, the longer the maneuver, the larger the drift accumulated by purely-inertial (Ours (No FT)) controllers. When both inertial and visual data is incorporated (Ours), drift is reduced and accuracy improves. For the longest sequence (Combo), the abstracted visual input raises the success rate by 10 percentage points.</p>
<p>Fig. 6 analyzes the evolution of tracking errors and success rates of different methods over time. For this experiment, we trained a policy to repeatedly fly barrel rolls for four seconds. We evaluate robustness and generalization of the learned policy by flying the maneuver for up to 20 seconds at test time. The results again show that (abstracted) visual input reduces drift and increases robustness. The controller that has access to both visual and inertial data (Ours) is able to perform barrel rolls for 20 seconds without a single failure.</p>
<p>To validate the importance of input abstraction, we compare our approach to a network that uses raw camera images instead of feature tracks as visual input. This network substitutes the PointNet in the input branch with a 5-layer convolutional network that directly operates on image pixels, but retains the same structure otherwise. We train this network on the Matty Flip and evaluate its robustness to changes in the background images. The results are summarized in Table II. In the training environment, the image-based network has a success rate of only $80 \%$, with a $58 \%$ higher tracking error than the controller that receives an abstraction of the visual input in the form of feature tracks (Ours). We attribute this to the higher sample complexity of learning from raw pixels [19]. Even more dramatically, the image-based controller fails completely when tested with previously unseen background images (Test 1, Test 2). (For backgrounds, we use randomly sampled images from the COCO dataset [35].) In contrast, our approach maintains a $100 \%$ success rate in these conditions.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Fig. 7: Number of successful back-to-back Power Loops on the physical quadrotor before the human expert pilot had to intervene. Results are averaged over 5 runs.</p>
<h2>C. Deployment in the Physical World</h2>
<p>We now perform direct simulation-to-reality transfer of the learned controllers. We use exactly the same sensorimotor controllers that were learned in simulation and quantitatively evaluated in Table I to fly a physical quadrotor, with no finetuning. Despite the differences in the appearance of simulation and reality (see Fig. 5), the abstraction scheme we use facilitates successful deployment of simulation-trained controllers in the physical world. The controllers are shown in action in the supplementary video.</p>
<p>We further evaluate the learned controllers with a series of quantitative experiments on the physical platform. The success rates of different maneuvers are shown in Table III. Our controllers can fly all maneuvers with no intervention. An additional experiment is presented in Fig. 7, where a controller that was trained for a single loop was tested on repeated execution of the maneuver with no breaks. The results indicate that using all input modalities, including the abstracted visual input in the form of feature tracks, enhances robustness.</p>
<h2>VI. CONCLUSION</h2>
<p>Our approach is the first to enable an autonomous flying machine to perform a wide range of acrobatics maneuvers that are highly challenging even for expert human pilots. The approach relies solely on onboard sensing and computation, and leverages sensorimotor policies that are trained entirely in simulation. We have shown that designing appropriate abstractions of the input facilitates direct transfer of the policies from simulation to physical reality. The presented methodology is not limited to autonomous flight and can enable progress in other areas of robotics.</p>
<h2>ACKNOWLEDGEMENT</h2>
<p>This work was supported by the Intel Network on Intelligent Systems, the National Centre of Competence in Research Robotics (NCCR) through the Swiss National Science Foundation, and the SNSF-ERC Starting Grant. The authors especially thank Thomas Längle, Christian Pfeiffer, Manuel Sutter and Titus Cieslewski for their contributions to the design and help with the experiments.</p>
<h2>REFERENCES</h2>
<p>[1] Sergei Lupashin, Angela Schöllig, Michael Sherback, and Raffaello D'Andrea. A simple learning strategy for highspeed quadrocopter multi-flips. In IEEE International Conference on Robotics and Automation (ICRA), pages 1642-1648, 2010.
[2] Pieter Abbeel, Adam Coates, and Andrew Y Ng. Autonomous helicopter aerobatics through apprenticeship learning. The International Journal of Robotics Research, 29(13):1608-1639, 2010.
[3] Adam Bry, Charles Richter, Abraham Bachrach, and Nicholas Roy. Aggressive flight of fixed-wing and quadrotor aircraft in dense indoor environments. The International Journal of Robotics Research, 34(7):9691002, 2015.
[4] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, José Neira, Ian D. Reid, and John J. Leonard. Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age. IEEE Transactions on Robotics, 32(6):1309-1332, 2016.
[5] Jemin Hwangbo, Inkyu Sa, Roland Siegwart, and Marco Hutter. Control of a quadrotor with reinforcement learning. IEEE Robotics and Automation Letters, 2(4):2096-2103, 2017.
[6] Davide Falanga, Philipp Foehn, Peng Lu, and Davide Scaramuzza. PAMPC: Perception-aware model predictive control for quadrotors. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2018.
[7] Davide Falanga, Elias Mueggler, Matthias Faessler, and Davide Scaramuzza. Aggressive quadrotor flight through narrow gaps with onboard sensing and computing using active vision. In IEEE International Conference on Robotics and Automation (ICRA), 2017.
[8] Shaojie Shen, Yash Mulgaonkar, Nathan Michael, and Vijay Kumar. Vision-based state estimation and trajectory control towards high-speed flight with a quadrotor. In Robotics: Science and Systems (RSS), 2013.
[9] Tianhao Zhang, Gregory Kahn, Sergey Levine, and Pieter Abbeel. Learning deep control policies for autonomous aerial vehicles with MPC-guided policy search. In IEEE International Conference on Robotics and Automation (ICRA), pages 528-535, 2016.
[10] Daniel Mellinger, Nathan Michael, and Vijay Kumar. Trajectory generation and control for precise aggressive maneuvers with quadrotors. The International Journal of Robotics Research, 31(5):664-674, 2012.
[11] Ying Chen and Néstor O Pérez-Arancibia. Controller synthesis and performance optimization for aerobatic quadrotor flight. IEEE Transactions on Control Systems Technology, pages 1-16, 2019.
[12] Giuseppe Loianno, Chris Brunner, Gary McGrath, and Vijay Kumar. Estimation, control, and planning for aggressive flight with a small quadrotor with a single
camera and IMU. IEEE Robotics and Automation Letters, 2(2):404-411, 2016.
[13] Keuntaek Lee, Jason Gibson, and Evangelos A Theodorou. Aggressive perception-aware navigation using deep optical flow dynamics and pixelmpc. arXiv:2001.02307, 2020.
[14] Shuo Li, Ekin Ozturk, Christophe De Wagter, Guido de Croon, and Dario Izzo. Aggressive online control of a quadrotor via deep network representations of optimality principles. arXiv:1912.07067, 2019.
[15] Matthias Müller, Alexey Dosovitskiy, Bernard Ghanem, and Vladen Koltun. Driving policy transfer via modularity and abstraction. In Conference on Robot Learning (CoRL), 2018.
[16] Antonio Loquercio, Elia Kaufmann, René Ranftl, Alexey Dosovitskiy, Vladlen Koltun, and Davide Scaramuzza. Deep drone racing: From simulation to reality with domain randomization. IEEE Transactions on Robotics, 36(1):1-14, 2020.
[17] Fereshteh Sadeghi and Sergey Levine. CAD2RL: Real single-image flight without a single real image. In Robotics: Science and Systems (RSS), 2017.
[18] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Krähenbühl. Learning by cheating. In Conference on Robot Learning (CoRL), 2019.
[19] Brady Zhou, Philipp Krähenbühl, and Vladlen Koltun. Does computer vision matter for action? Science Robotics, 4(30), 2019.
[20] Daniel Mellinger and Vijay Kumar. Minimum snap trajectory generation and control for quadrotors. In IEEE International Conference on Robotics and Automation (ICRA), pages 2520-2525, 2011.
[21] Mark W Mueller, Markus Hehn, and Raffaello D’Andrea. A computationally efficient algorithm for state-to-state quadrocopter trajectory generation and feasibility verification. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3480-3486, 2013.
[22] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos Theodorou, and Byron Boots. Agile autonomous driving using end-to-end deep imitation learning. In Robotics: Science and Systems (RSS), 2018.
[23] Alison L Gibbs and Francis Edward Su. On choosing and bounding probability metrics. International Statistical Review, 70(3):419-435, 2002.
[24] Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured prediction to no-regret online learning. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.
[25] C. Forster, L. Carlone, F. Dellaert, and D. Scaramuzza. On-manifold preintegration for real-time visual-inertial odometry. IEEE Transactions on Robotics, 3(1):1-21, 2017.
[26] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 40(3):611-625, 2018.
[27] Ronald Clark, Sen Wang, Hongkai Wen, Andrew Markham, and Niki Trigoni. VINet: Visual-inertial</p>
<p>odometry as a sequence-to-sequence learning problem. In AAAI Conference on Artificial Intelligence, 2017.
[28] Tong Qin, Peiliang Li, and Shaojie Shen. VINS-Mono: A robust and versatile monocular visual-inertial state estimator. IEEE Transactions on Robotics, 34(4):10041020, 2018.
[29] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv:1803.01271, 2018.
[30] Christopher G. Harris and Mike Stephens. A combined corner and edge detector. In Proceedings of the Alvey Vision Conference, 1988.
[31] Bruce D. Lucas and Takeo Kanade. An iterative image registration technique with an application to stereo vision. In International Joint Conference on Artificial Intelligence,</p>
<p>1981.
[32] René Ranftl and Vladlen Koltun. Deep fundamental matrix estimation. In European Conference on Computer Vision (ECCV), 2018.
[33] Fadri Furrer, Michael Burri, Markus Achtelik, and Roland Siegwart. RotorS-a modular gazebo MAV simulator framework. In Robot Operating System (ROS), pages 595-625. Springer, 2016.
[34] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015.
[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In European Conference on Computer Vision (ECCV), 2014.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution.
$\ddagger$ Robotics and Perception Group, University of Zurich and ETH Zurich.
${ }^{\dagger}$ Intelligent Systems Lab, Intel.
${ }^{\dagger}$ https://www.youtube.com/watch?v=T1vzjPa5260&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
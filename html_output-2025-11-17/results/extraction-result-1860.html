<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1860 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1860</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1860</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-1b8a734dd28a9d766a5d3dbc0871e76b6a452b65</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/1b8a734dd28a9d766a5d3dbc0871e76b6a452b65" target="_blank">Point-E: A System for Generating 3D Point Clouds from Complex Prompts</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This paper explores an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU, and is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases.</p>
                <p><strong>Paper Abstract:</strong> While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1860.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1860.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GLIDE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GLIDE (text-guided diffusion model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A text-conditional image diffusion model (3B parameters in this work) pretrained on large text–image corpora and fine-tuned on rendered 3D views to generate synthetic conditioning images for downstream 3D point-cloud generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>GLIDE (fine-tuned)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A text-to-image diffusion model (3B parameters in this paper) trained to map natural language prompts to images; in Point·E it is fine-tuned on a mixture of its original dataset plus rendered 3D views so that it produces in-distribution synthetic renders for the point-cloud model.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>text–image pairs (natural language captions paired with images)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Originally trained on a large corpus of (text, image) pairs (Nichol et al., 2021). For this work GLIDE was fine-tuned with a mixture of its original dataset and the paper's rendered 3D dataset (3D renders sampled 5% of the time); fine-tuned for 100K iterations so the model saw several epochs of the 3D renders.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Text-conditioned 3D generation (via synthetic-view conditioning)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Generate a single synthetic rendered view from text which is then used as the conditioning image for a point-cloud diffusion model that produces a 3D RGB point cloud; objective is to produce an image that preserves prompt semantics and matches in-distribution renders used by the point-cloud model.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language prompts / captions (open-ended text descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (the 'embodied' downstream task is 3D shape generation/optimization rather than robot/control actions; GLIDE outputs pixels).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>High-level text semantics are rendered to pixels by the text-to-image diffusion network; those pixels serve as conditioning input (image) for the point-cloud diffusion model — there is no explicit learned mapping to motor controls, only an image-mediated conditioning pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>None for GLIDE itself; downstream model expects generator images with renderer-style lighting and backgrounds. The point-cloud model uses CLIP image encodings of GLIDE outputs as perceptual conditioning.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Used successfully to produce conditioning images; GLIDE sampling time reported as 46.28 V100-seconds (Table 4). No direct numeric improvement of point-cloud CLIP-R Precision attributed solely to GLIDE fine-tuning is provided, but the paper reports that fine-tuning GLIDE on renders ensures in-distribution conditioning images for the point-cloud model.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>GLIDE fine-tuning: 100K fine-tune iterations on mixed dataset (paper).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Large-scale text–image pretraining provides the ability to follow diverse and complex text prompts; fine-tuning on renderer outputs reduces distribution shift between synthetic images and the point-cloud model's training data.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Mismatch between the distribution of images produced by out-of-distribution text-to-image models (e.g. shadows in DALL·E 2 outputs) and the point-cloud model's expected renderer outputs can cause misinterpretation; GLIDE required fine-tuning on renders to avoid such distributional issues.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text-to-image pretraining (GLIDE) supplies diverse prompt-following capability. Fine-tuning on renderer-style images (5% sampling of 3D renders, 100K iterations) reduces distribution shift and enables the image->point-cloud model to perform better in practice, but numerical ablation isolating only GLIDE's contribution is not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Point-E: A System for Generating 3D Point Clouds from Complex Prompts', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1860.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1860.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DALL·E 2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DALL·E 2 (text-to-image system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large text-conditional image generation system whose samples were tested as conditioning images for the Point·E point-cloud model; the paper reports qualitative failure modes when using DALL·E 2 outputs without adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>DALL·E 2 (pretrained text-to-image)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A pretrained text-to-image generator producing photorealistic images from captions; in this paper DALL·E 2 generated conditioning images used to test the image-to-point-cloud model (Appendix D).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>text–image pairs (natural language captions paired with images)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Not specified in this paper (external pretrained model); DALL·E 2 samples were used as-is to condition the point-cloud model in qualitative experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Image-conditioned 3D point-cloud generation (qualitative tests)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Use images generated by DALL·E 2 as conditioning images for the point-cloud diffusion model to produce 3D point clouds; objective is to evaluate whether off-the-shelf text-to-image outputs are suitable conditioning inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language prompts (open-ended captions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (generation of images used for 3D reconstruction rather than physical actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Generated images from text act as an intermediate modality; no explicit low-level action mapping is used — pixel images are provided as conditioning to the point-cloud diffusion model.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>The point-cloud model expects renderer-style images; DALL·E 2 images with shadows or stylistic artifacts can be misinterpreted (e.g., inferred ground planes).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Qualitative: DALL·E 2 images can be used but often include shadows/visual cues (e.g., dark ground plane) that the point-cloud model misinterprets; adding a white border can mitigate some failure modes. No quantitative transfer metrics provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Generative quality and visual realism of text-to-image outputs can convey text semantics to the point-cloud model without retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Distributional mismatch (shadows, framing, non-renderer cues) between DALL·E 2 outputs and the training render distribution leads to erroneous 3D inferences; simple image preprocessing (e.g., adding border) can partially mitigate.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Off-the-shelf text-to-image models can serve as conditioning sources for 3D generation, but distributional differences (lighting, shadows, framing) cause systematic errors unless the downstream 3D model is adapted or the images are preprocessed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Point-E: A System for Generating 3D Point Clouds from Complex Prompts', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1860.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1860.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP (ViT-L/14)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP (Vision Transformer ViT-L/14 variant)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained image–text embedding model used both as a frozen image encoder to condition the point-cloud diffusion model (a grid of CLIP latents) and as the evaluation backbone for CLIP R-Precision metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Learning transferable visual models from natural language supervision</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP ViT-L/14 (frozen encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>A joint image-text contrastive model producing aligned image and text representations; used as a frozen encoder to produce a spatial grid of image latents (CLIP latents) that are prepended as conditioning tokens to the point-cloud Transformer, and used for evaluation (CLIP R-Precision).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image–text pairs (natural language captions paired with images)</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Pretrained on large-scale image-text datasets (details not specified in this paper; original CLIP paper describes large web-scale dataset). In Point·E the ViT-L/14 CLIP model is used frozen to encode conditioning images into a 256×D' latent grid.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Image-conditioned 3D point-cloud generation and evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Provide perceptual conditioning (spatial CLIP latents) to a point-cloud diffusion model so it can produce 3D RGB point clouds consistent with a conditioning image; also used to compute CLIP R-Precision for text-to-3D evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Text captions used only for evaluation (CLIP text embeddings) and for some ablations; CLIP provides text-image alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (conditioning and evaluation modality for a generative 3D reconstruction task, not motor actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Text is embedded by CLIP into a shared latent space; in this work CLIP image latents (grid of spatial tokens) are used directly as conditioning tokens to the point-cloud Transformer, transferring semantic and spatial information from pixels to 3D generation.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>High-quality RGB images (renderer-style) to produce useful CLIP latents; spatial grid of CLIP latents preserves locality information important to the point-cloud model.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Ablations show that conditioning on a grid of CLIP image latents outperforms conditioning on a single CLIP embedding; using CLIP latents enables higher CLIP R-Precision and faster P-FID convergence (exact numeric deltas not tabulated in the paper beyond reported model comparisons). The paper reports CLIP R-Precision evaluation using ViT-L/14 values across methods (e.g., Point·E 1B: 46.8%).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>CLIP provides aligned image–text semantics and spatial latents that encode both appearance and layout; using a grid of latents preserves spatial structure that helps 3D reconstruction.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>CLIP encodings reflect 2D appearance and may not encode occluded geometry; reliance on single-token image embeddings loses spatial detail (worse results than grid).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Freezing a pretrained image–text encoder (CLIP) and feeding a spatial grid of image latents into a point-cloud diffusion Transformer improves conditioning fidelity versus single-token conditioning, enabling better text-to-3D performance when images are used as an intermediate modality.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Point-E: A System for Generating 3D Point Clouds from Complex Prompts', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1860.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1860.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamFields</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamFields (text→NeRF via CLIP guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that optimizes a NeRF representation of a 3D object from text by maximizing CLIP similarity between rendered views and a text prompt — enabling zero-shot text-guided object generation without 3D training data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Zero-shot text-guided object generation with dream fields</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>DreamFields (CLIP-guided NeRF optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>An optimization-based method that takes a pretrained CLIP model and optimizes NeRF parameters so that rendered views from the NeRF are maximally similar (in CLIP space) to a text prompt; requires no paired 3D training data but uses expensive per-example optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>CLIP pretrained on image–text pairs (language supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Uses a pretrained CLIP model (trained on large web image–text pairs) as the similarity objective; DreamFields itself does not perform language pretraining but leverages CLIP's pretrained language–image alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Text-to-3D generation via NeRF optimization</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Optimize a neural radiance field (NeRF) so that its rendered images match a text prompt according to CLIP; objective is to produce a coherent 3D object consistent with open-ended text without 3D supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language prompts (open-ended instructions describing the desired object).</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (the task is 3D scene/object synthesis via optimizing a NeRF's parameters, not physical actions).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Text semantics are mapped to 3D geometry by iterative optimization: render views of the current 3D representation, compute CLIP similarity to the text prompt, and backpropagate gradients into the NeRF parameters to change geometry/appearance.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Rendered RGB views from the candidate 3D representation; CLIP acts as the perceptual/textual comparator.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Reported CLIP R-Precision in Table 1: ViT-B/32 = 78.6%, ViT-L/14 = 82.9%; sampling/optimization latency ~200 V100-hours (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Very high: optimization-based per-prompt process reported on the order of hundreds of V100 GPU-hours (~200 V100-hr in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Strong image–text alignment in CLIP enables zero-shot text-to-3D optimization; no 3D supervision required.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Extremely high per-sample compute (long optimization), susceptibility to local minima and poor 3D priors leading to nonsensical geometry, and no amortized generation (one expensive optimization per sample).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Text–image pretrained models like CLIP can guide 3D geometry optimization to produce plausible 3D objects from text without 3D training data, but this approach is computationally expensive (hundreds of GPU-hours) and can suffer from optimization pitfalls due to weak 3D priors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Point-E: A System for Generating 3D Point Clouds from Complex Prompts', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1860.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1860.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DreamFusion</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DreamFusion (text-to-3D using 2D diffusion guidance)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that uses a pretrained text-to-image diffusion model as a scorer/guidance mechanism to optimize a NeRF so rendered views match a text prompt, replacing CLIP guidance with a stronger diffusion-based prior.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Dreamfusion: Text-to-3d using 2d diffusion</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>DreamFusion (pretrained text-to-image diffusion → NeRF optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Takes a pretrained text-to-image diffusion model and uses its gradients to optimize a NeRF representation so that rendered views are likely under the diffusion model conditioned on text; replaces CLIP-based objective with diffusion-based guidance for improved coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Text–image pairs used to pretrain the diffusion-based text-to-image model.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Uses an external pretrained text-to-image diffusion model (details in Poole et al., 2022); DreamFusion itself performs per-prompt NeRF optimization driven by the diffusion prior.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Text-to-3D NeRF optimization</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Optimize a neural radiance field so that diffusion model guidance makes rendered views consistent with a text prompt; objective is improved fidelity and coherence for generated 3D models compared to CLIP guidance.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (parameter optimization of a 3D representation rather than motor control).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Use gradients from the pretrained text-to-image diffusion model (conditioning on text) with differentiable rendering to update NeRF parameters so renderings score highly under the diffusion prior for the given text.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Rendered RGB views from the current NeRF candidate; diffusion model provides perceptual/textual scoring.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Table 1 reports CLIP R-Precision: ViT-B/32 = 75.1%, ViT-L/14 = 79.7%; latency ~12 V100-hours (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Moderate-to-high per-sample optimization cost (~12 V100-hours reported in Table 1), but less than CLIP-only DreamFields in this table.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Diffusion-based prior provides a stronger photorealistic and semantic objective than CLIP, improving the quality and coherence of optimized 3D outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Still optimization-based and thus computationally expensive per sample; may require careful rendering/differentiable renderer setup and can still suffer from 3D prior weaknesses.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Replacing CLIP with gradients from a pretrained text-to-image diffusion model improves text-to-3D optimization quality, achieving competitive CLIP R-Precision with lower (but still substantial) per-sample compute compared to CLIP-only optimization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Point-E: A System for Generating 3D Point Clouds from Complex Prompts', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1860.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1860.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CLIP-Mesh</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CLIP-Mesh (text→textured mesh via pretrained image–text models)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A mesh-optimization approach that uses pretrained image–text models to guide mesh generation from text prompts, trading off between compute and output fidelity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Clipmesh: Generating textured meshes from text using pretrained image-text models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>CLIP-Mesh (CLIP-guided mesh optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Optimizes mesh geometry and texture so that differentiably-rendered views score highly under a pretrained image–text model (CLIP), producing textured meshes conditioned on text without paired 3D training data.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Image–text pairs (CLIP pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>Leverages a pretrained image–text model (CLIP) as the objective; CLIP-Mesh itself operates in an optimization loop modifying mesh parameters and textures to maximize CLIP similarity to text prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Text-to-mesh optimization</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Optimize a mesh and its texture such that rendered views match a text prompt in CLIP space; objective is to produce recognizable textured meshes from language.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>Natural language prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>N/A (3D mesh synthesis/optimization rather than physical action control).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Iterative differentiable rendering + CLIP similarity: render the current mesh, compute CLIP similarity to the text prompt, backpropagate to mesh geometry and texture parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Differentiable rendering to produce RGB views; CLIP for image–text similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Table 1 reports CLIP R-Precision: ViT-B/32 = 67.8%, ViT-L/14 = 74.5%; latency ~17 V100-minutes (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Optimization-based per-sample time ~17 V100-minutes reported (much faster than DreamFields/DreamFusion in the paper's table).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Using a pretrained image–text model (CLIP) provides a direct semantic objective to shape the mesh and texture to match text.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Optimization can require careful tuning; quality and coherence depend on renderer, initialization, and the strength of 3D priors.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Pretrained image–text models can be repurposed as semantic scorers to optimize meshes from text, yielding reasonable CLIP R-Precision with per-sample optimization costs that can be lower than some NeRF-based methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Point-E: A System for Generating 3D Point Clouds from Complex Prompts', 'publication_date_yy_mm': '2022-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Zero-shot text-guided object generation with dream fields <em>(Rating: 2)</em></li>
                <li>Dreamfusion: Text-to-3d using 2d diffusion <em>(Rating: 2)</em></li>
                <li>Clipmesh: Generating textured meshes from text using pretrained image-text models <em>(Rating: 2)</em></li>
                <li>GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models <em>(Rating: 2)</em></li>
                <li>Learning transferable visual models from natural language supervision <em>(Rating: 2)</em></li>
                <li>Hierarchical text-conditional image generation with CLIP latents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1860",
    "paper_id": "paper-1b8a734dd28a9d766a5d3dbc0871e76b6a452b65",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "GLIDE",
            "name_full": "GLIDE (text-guided diffusion model)",
            "brief_description": "A text-conditional image diffusion model (3B parameters in this work) pretrained on large text–image corpora and fine-tuned on rendered 3D views to generate synthetic conditioning images for downstream 3D point-cloud generation.",
            "citation_title": "GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models",
            "mention_or_use": "use",
            "model_agent_name": "GLIDE (fine-tuned)",
            "model_agent_description": "A text-to-image diffusion model (3B parameters in this paper) trained to map natural language prompts to images; in Point·E it is fine-tuned on a mixture of its original dataset plus rendered 3D views so that it produces in-distribution synthetic renders for the point-cloud model.",
            "pretraining_data_type": "text–image pairs (natural language captions paired with images)",
            "pretraining_data_details": "Originally trained on a large corpus of (text, image) pairs (Nichol et al., 2021). For this work GLIDE was fine-tuned with a mixture of its original dataset and the paper's rendered 3D dataset (3D renders sampled 5% of the time); fine-tuned for 100K iterations so the model saw several epochs of the 3D renders.",
            "embodied_task_name": "Text-conditioned 3D generation (via synthetic-view conditioning)",
            "embodied_task_description": "Generate a single synthetic rendered view from text which is then used as the conditioning image for a point-cloud diffusion model that produces a 3D RGB point cloud; objective is to produce an image that preserves prompt semantics and matches in-distribution renders used by the point-cloud model.",
            "action_space_text": "Natural language prompts / captions (open-ended text descriptions).",
            "action_space_embodied": "N/A (the 'embodied' downstream task is 3D shape generation/optimization rather than robot/control actions; GLIDE outputs pixels).",
            "action_mapping_method": "High-level text semantics are rendered to pixels by the text-to-image diffusion network; those pixels serve as conditioning input (image) for the point-cloud diffusion model — there is no explicit learned mapping to motor controls, only an image-mediated conditioning pipeline.",
            "perception_requirements": "None for GLIDE itself; downstream model expects generator images with renderer-style lighting and backgrounds. The point-cloud model uses CLIP image encodings of GLIDE outputs as perceptual conditioning.",
            "transfer_successful": true,
            "performance_with_pretraining": "Used successfully to produce conditioning images; GLIDE sampling time reported as 46.28 V100-seconds (Table 4). No direct numeric improvement of point-cloud CLIP-R Precision attributed solely to GLIDE fine-tuning is provided, but the paper reports that fine-tuning GLIDE on renders ensures in-distribution conditioning images for the point-cloud model.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "GLIDE fine-tuning: 100K fine-tune iterations on mixed dataset (paper).",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Large-scale text–image pretraining provides the ability to follow diverse and complex text prompts; fine-tuning on renderer outputs reduces distribution shift between synthetic images and the point-cloud model's training data.",
            "transfer_failure_factors": "Mismatch between the distribution of images produced by out-of-distribution text-to-image models (e.g. shadows in DALL·E 2 outputs) and the point-cloud model's expected renderer outputs can cause misinterpretation; GLIDE required fine-tuning on renders to avoid such distributional issues.",
            "key_findings": "Text-to-image pretraining (GLIDE) supplies diverse prompt-following capability. Fine-tuning on renderer-style images (5% sampling of 3D renders, 100K iterations) reduces distribution shift and enables the image-&gt;point-cloud model to perform better in practice, but numerical ablation isolating only GLIDE's contribution is not provided.",
            "uuid": "e1860.0",
            "source_info": {
                "paper_title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DALL·E 2",
            "name_full": "DALL·E 2 (text-to-image system)",
            "brief_description": "A large text-conditional image generation system whose samples were tested as conditioning images for the Point·E point-cloud model; the paper reports qualitative failure modes when using DALL·E 2 outputs without adaptation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "DALL·E 2 (pretrained text-to-image)",
            "model_agent_description": "A pretrained text-to-image generator producing photorealistic images from captions; in this paper DALL·E 2 generated conditioning images used to test the image-to-point-cloud model (Appendix D).",
            "pretraining_data_type": "text–image pairs (natural language captions paired with images)",
            "pretraining_data_details": "Not specified in this paper (external pretrained model); DALL·E 2 samples were used as-is to condition the point-cloud model in qualitative experiments.",
            "embodied_task_name": "Image-conditioned 3D point-cloud generation (qualitative tests)",
            "embodied_task_description": "Use images generated by DALL·E 2 as conditioning images for the point-cloud diffusion model to produce 3D point clouds; objective is to evaluate whether off-the-shelf text-to-image outputs are suitable conditioning inputs.",
            "action_space_text": "Natural language prompts (open-ended captions).",
            "action_space_embodied": "N/A (generation of images used for 3D reconstruction rather than physical actions).",
            "action_mapping_method": "Generated images from text act as an intermediate modality; no explicit low-level action mapping is used — pixel images are provided as conditioning to the point-cloud diffusion model.",
            "perception_requirements": "The point-cloud model expects renderer-style images; DALL·E 2 images with shadows or stylistic artifacts can be misinterpreted (e.g., inferred ground planes).",
            "transfer_successful": true,
            "performance_with_pretraining": "Qualitative: DALL·E 2 images can be used but often include shadows/visual cues (e.g., dark ground plane) that the point-cloud model misinterprets; adding a white border can mitigate some failure modes. No quantitative transfer metrics provided.",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Generative quality and visual realism of text-to-image outputs can convey text semantics to the point-cloud model without retraining.",
            "transfer_failure_factors": "Distributional mismatch (shadows, framing, non-renderer cues) between DALL·E 2 outputs and the training render distribution leads to erroneous 3D inferences; simple image preprocessing (e.g., adding border) can partially mitigate.",
            "key_findings": "Off-the-shelf text-to-image models can serve as conditioning sources for 3D generation, but distributional differences (lighting, shadows, framing) cause systematic errors unless the downstream 3D model is adapted or the images are preprocessed.",
            "uuid": "e1860.1",
            "source_info": {
                "paper_title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CLIP (ViT-L/14)",
            "name_full": "CLIP (Vision Transformer ViT-L/14 variant)",
            "brief_description": "A pretrained image–text embedding model used both as a frozen image encoder to condition the point-cloud diffusion model (a grid of CLIP latents) and as the evaluation backbone for CLIP R-Precision metrics.",
            "citation_title": "Learning transferable visual models from natural language supervision",
            "mention_or_use": "use",
            "model_agent_name": "CLIP ViT-L/14 (frozen encoder)",
            "model_agent_description": "A joint image-text contrastive model producing aligned image and text representations; used as a frozen encoder to produce a spatial grid of image latents (CLIP latents) that are prepended as conditioning tokens to the point-cloud Transformer, and used for evaluation (CLIP R-Precision).",
            "pretraining_data_type": "Image–text pairs (natural language captions paired with images)",
            "pretraining_data_details": "Pretrained on large-scale image-text datasets (details not specified in this paper; original CLIP paper describes large web-scale dataset). In Point·E the ViT-L/14 CLIP model is used frozen to encode conditioning images into a 256×D' latent grid.",
            "embodied_task_name": "Image-conditioned 3D point-cloud generation and evaluation",
            "embodied_task_description": "Provide perceptual conditioning (spatial CLIP latents) to a point-cloud diffusion model so it can produce 3D RGB point clouds consistent with a conditioning image; also used to compute CLIP R-Precision for text-to-3D evaluation.",
            "action_space_text": "Text captions used only for evaluation (CLIP text embeddings) and for some ablations; CLIP provides text-image alignment.",
            "action_space_embodied": "N/A (conditioning and evaluation modality for a generative 3D reconstruction task, not motor actions).",
            "action_mapping_method": "Text is embedded by CLIP into a shared latent space; in this work CLIP image latents (grid of spatial tokens) are used directly as conditioning tokens to the point-cloud Transformer, transferring semantic and spatial information from pixels to 3D generation.",
            "perception_requirements": "High-quality RGB images (renderer-style) to produce useful CLIP latents; spatial grid of CLIP latents preserves locality information important to the point-cloud model.",
            "transfer_successful": true,
            "performance_with_pretraining": "Ablations show that conditioning on a grid of CLIP image latents outperforms conditioning on a single CLIP embedding; using CLIP latents enables higher CLIP R-Precision and faster P-FID convergence (exact numeric deltas not tabulated in the paper beyond reported model comparisons). The paper reports CLIP R-Precision evaluation using ViT-L/14 values across methods (e.g., Point·E 1B: 46.8%).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": null,
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "CLIP provides aligned image–text semantics and spatial latents that encode both appearance and layout; using a grid of latents preserves spatial structure that helps 3D reconstruction.",
            "transfer_failure_factors": "CLIP encodings reflect 2D appearance and may not encode occluded geometry; reliance on single-token image embeddings loses spatial detail (worse results than grid).",
            "key_findings": "Freezing a pretrained image–text encoder (CLIP) and feeding a spatial grid of image latents into a point-cloud diffusion Transformer improves conditioning fidelity versus single-token conditioning, enabling better text-to-3D performance when images are used as an intermediate modality.",
            "uuid": "e1860.2",
            "source_info": {
                "paper_title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DreamFields",
            "name_full": "DreamFields (text→NeRF via CLIP guidance)",
            "brief_description": "A method that optimizes a NeRF representation of a 3D object from text by maximizing CLIP similarity between rendered views and a text prompt — enabling zero-shot text-guided object generation without 3D training data.",
            "citation_title": "Zero-shot text-guided object generation with dream fields",
            "mention_or_use": "mention",
            "model_agent_name": "DreamFields (CLIP-guided NeRF optimization)",
            "model_agent_description": "An optimization-based method that takes a pretrained CLIP model and optimizes NeRF parameters so that rendered views from the NeRF are maximally similar (in CLIP space) to a text prompt; requires no paired 3D training data but uses expensive per-example optimization.",
            "pretraining_data_type": "CLIP pretrained on image–text pairs (language supervision).",
            "pretraining_data_details": "Uses a pretrained CLIP model (trained on large web image–text pairs) as the similarity objective; DreamFields itself does not perform language pretraining but leverages CLIP's pretrained language–image alignment.",
            "embodied_task_name": "Text-to-3D generation via NeRF optimization",
            "embodied_task_description": "Optimize a neural radiance field (NeRF) so that its rendered images match a text prompt according to CLIP; objective is to produce a coherent 3D object consistent with open-ended text without 3D supervision.",
            "action_space_text": "Natural language prompts (open-ended instructions describing the desired object).",
            "action_space_embodied": "N/A (the task is 3D scene/object synthesis via optimizing a NeRF's parameters, not physical actions).",
            "action_mapping_method": "Text semantics are mapped to 3D geometry by iterative optimization: render views of the current 3D representation, compute CLIP similarity to the text prompt, and backpropagate gradients into the NeRF parameters to change geometry/appearance.",
            "perception_requirements": "Rendered RGB views from the candidate 3D representation; CLIP acts as the perceptual/textual comparator.",
            "transfer_successful": true,
            "performance_with_pretraining": "Reported CLIP R-Precision in Table 1: ViT-B/32 = 78.6%, ViT-L/14 = 82.9%; sampling/optimization latency ~200 V100-hours (Table 1).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Very high: optimization-based per-prompt process reported on the order of hundreds of V100 GPU-hours (~200 V100-hr in Table 1).",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Strong image–text alignment in CLIP enables zero-shot text-to-3D optimization; no 3D supervision required.",
            "transfer_failure_factors": "Extremely high per-sample compute (long optimization), susceptibility to local minima and poor 3D priors leading to nonsensical geometry, and no amortized generation (one expensive optimization per sample).",
            "key_findings": "Text–image pretrained models like CLIP can guide 3D geometry optimization to produce plausible 3D objects from text without 3D training data, but this approach is computationally expensive (hundreds of GPU-hours) and can suffer from optimization pitfalls due to weak 3D priors.",
            "uuid": "e1860.3",
            "source_info": {
                "paper_title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "DreamFusion",
            "name_full": "DreamFusion (text-to-3D using 2D diffusion guidance)",
            "brief_description": "A method that uses a pretrained text-to-image diffusion model as a scorer/guidance mechanism to optimize a NeRF so rendered views match a text prompt, replacing CLIP guidance with a stronger diffusion-based prior.",
            "citation_title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "mention_or_use": "mention",
            "model_agent_name": "DreamFusion (pretrained text-to-image diffusion → NeRF optimization)",
            "model_agent_description": "Takes a pretrained text-to-image diffusion model and uses its gradients to optimize a NeRF representation so that rendered views are likely under the diffusion model conditioned on text; replaces CLIP-based objective with diffusion-based guidance for improved coherence.",
            "pretraining_data_type": "Text–image pairs used to pretrain the diffusion-based text-to-image model.",
            "pretraining_data_details": "Uses an external pretrained text-to-image diffusion model (details in Poole et al., 2022); DreamFusion itself performs per-prompt NeRF optimization driven by the diffusion prior.",
            "embodied_task_name": "Text-to-3D NeRF optimization",
            "embodied_task_description": "Optimize a neural radiance field so that diffusion model guidance makes rendered views consistent with a text prompt; objective is improved fidelity and coherence for generated 3D models compared to CLIP guidance.",
            "action_space_text": "Natural language prompts.",
            "action_space_embodied": "N/A (parameter optimization of a 3D representation rather than motor control).",
            "action_mapping_method": "Use gradients from the pretrained text-to-image diffusion model (conditioning on text) with differentiable rendering to update NeRF parameters so renderings score highly under the diffusion prior for the given text.",
            "perception_requirements": "Rendered RGB views from the current NeRF candidate; diffusion model provides perceptual/textual scoring.",
            "transfer_successful": true,
            "performance_with_pretraining": "Table 1 reports CLIP R-Precision: ViT-B/32 = 75.1%, ViT-L/14 = 79.7%; latency ~12 V100-hours (Table 1).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Moderate-to-high per-sample optimization cost (~12 V100-hours reported in Table 1), but less than CLIP-only DreamFields in this table.",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Diffusion-based prior provides a stronger photorealistic and semantic objective than CLIP, improving the quality and coherence of optimized 3D outputs.",
            "transfer_failure_factors": "Still optimization-based and thus computationally expensive per sample; may require careful rendering/differentiable renderer setup and can still suffer from 3D prior weaknesses.",
            "key_findings": "Replacing CLIP with gradients from a pretrained text-to-image diffusion model improves text-to-3D optimization quality, achieving competitive CLIP R-Precision with lower (but still substantial) per-sample compute compared to CLIP-only optimization.",
            "uuid": "e1860.4",
            "source_info": {
                "paper_title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
                "publication_date_yy_mm": "2022-12"
            }
        },
        {
            "name_short": "CLIP-Mesh",
            "name_full": "CLIP-Mesh (text→textured mesh via pretrained image–text models)",
            "brief_description": "A mesh-optimization approach that uses pretrained image–text models to guide mesh generation from text prompts, trading off between compute and output fidelity.",
            "citation_title": "Clipmesh: Generating textured meshes from text using pretrained image-text models",
            "mention_or_use": "mention",
            "model_agent_name": "CLIP-Mesh (CLIP-guided mesh optimization)",
            "model_agent_description": "Optimizes mesh geometry and texture so that differentiably-rendered views score highly under a pretrained image–text model (CLIP), producing textured meshes conditioned on text without paired 3D training data.",
            "pretraining_data_type": "Image–text pairs (CLIP pretraining).",
            "pretraining_data_details": "Leverages a pretrained image–text model (CLIP) as the objective; CLIP-Mesh itself operates in an optimization loop modifying mesh parameters and textures to maximize CLIP similarity to text prompts.",
            "embodied_task_name": "Text-to-mesh optimization",
            "embodied_task_description": "Optimize a mesh and its texture such that rendered views match a text prompt in CLIP space; objective is to produce recognizable textured meshes from language.",
            "action_space_text": "Natural language prompts.",
            "action_space_embodied": "N/A (3D mesh synthesis/optimization rather than physical action control).",
            "action_mapping_method": "Iterative differentiable rendering + CLIP similarity: render the current mesh, compute CLIP similarity to the text prompt, backpropagate to mesh geometry and texture parameters.",
            "perception_requirements": "Differentiable rendering to produce RGB views; CLIP for image–text similarity.",
            "transfer_successful": true,
            "performance_with_pretraining": "Table 1 reports CLIP R-Precision: ViT-B/32 = 67.8%, ViT-L/14 = 74.5%; latency ~17 V100-minutes (Table 1).",
            "performance_without_pretraining": null,
            "sample_complexity_with_pretraining": "Optimization-based per-sample time ~17 V100-minutes reported (much faster than DreamFields/DreamFusion in the paper's table).",
            "sample_complexity_without_pretraining": null,
            "sample_complexity_gain": null,
            "transfer_success_factors": "Using a pretrained image–text model (CLIP) provides a direct semantic objective to shape the mesh and texture to match text.",
            "transfer_failure_factors": "Optimization can require careful tuning; quality and coherence depend on renderer, initialization, and the strength of 3D priors.",
            "key_findings": "Pretrained image–text models can be repurposed as semantic scorers to optimize meshes from text, yielding reasonable CLIP R-Precision with per-sample optimization costs that can be lower than some NeRF-based methods.",
            "uuid": "e1860.5",
            "source_info": {
                "paper_title": "Point-E: A System for Generating 3D Point Clouds from Complex Prompts",
                "publication_date_yy_mm": "2022-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Zero-shot text-guided object generation with dream fields",
            "rating": 2
        },
        {
            "paper_title": "Dreamfusion: Text-to-3d using 2d diffusion",
            "rating": 2
        },
        {
            "paper_title": "Clipmesh: Generating textured meshes from text using pretrained image-text models",
            "rating": 2
        },
        {
            "paper_title": "GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models",
            "rating": 2
        },
        {
            "paper_title": "Learning transferable visual models from natural language supervision",
            "rating": 2
        },
        {
            "paper_title": "Hierarchical text-conditional image generation with CLIP latents",
            "rating": 1
        }
    ],
    "cost": 0.021592,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Point$\cdot$E: A System for Generating 3D Point Clouds from Complex Prompts</h1>
<p>Alex Nichol<em>1 Heewoo Jun</em> ${ }^{* 1}$ Prafulla Dhariwal ${ }^{1}$ Pamela Mishkin ${ }^{1}$ Mark Chen ${ }^{1}$</p>
<h4>Abstract</h4>
<p>While recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method first generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https : //github.com/openai/point-e.</p>
<h2>1. Introduction</h2>
<p>With the recent explosion of text-to-image generative models, it is now possible to generate and modify high-quality images from natural language descriptions in a number of seconds (Ramesh et al., 2021; Ding et al., 2021; Nichol et al., 2021; Ramesh et al., 2022; Gafni et al., 2022; Yu et al., 2022; Saharia et al., 2022; Feng et al., 2022; Balaji et al., 2022). Inspired by these results, recent works have explored text-conditional generation in other modalities, such as video (Hong et al., 2022; Singer et al., 2022; Ho et al., 2022b;a) and 3D objects (Jain et al., 2021; Poole et al., 2022; Lin et al., 2022a; Sanghi et al., 2021; 2022). In this work, we focus specifically on the problem of text-to-3D generation, which has significant potential to democratize 3D content creation for a wide range of applications such as</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>virtual reality, gaming, and industrial design.
Recent methods for text-to-3D synthesis typically fall into one of two categories:</p>
<ol>
<li>Methods which train generative models directly on paired (text, 3D) data (Chen et al., 2018; Mittal et al., 2022; Fu et al., 2022; Zeng et al., 2022) or unlabeled 3D data (Sanghi et al., 2021; 2022; Watson et al., 2022). While these methods can leverage existing generative modeling approaches to produce samples efficiently, they are difficult to scale to diverse and complex text prompts due to the lack of large-scale 3D datasets (Sanghi et al., 2022).</li>
<li>Methods which leverage pre-trained text-image models to optimize differentiable 3D representations (Jain et al., 2021; Poole et al., 2022; Lin et al., 2022a). These methods are often able to handle complex and diverse text prompts, but require expensive optimization processes to produce each sample. Furthermore, due to the lack of a strong 3D prior, these methods can fall into local minima which don't correspond to meaningful or coherent 3D objects (Poole et al., 2022).</li>
</ol>
<p>We aim to combine the benefits of both categories by pairing a text-to-image model with an image-to-3D model. Our text-to-image model leverages a large corpus of (text, image) pairs, allowing it to follow diverse and complex prompts, while our image-to-3D model is trained on a smaller dataset of (image, 3D) pairs. To produce a 3D object from a text prompt, we first sample an image using the text-to-image model, and then sample a 3D object conditioned on the sampled image. Both of these steps can be performed in a number of seconds, and do not require expensive optimization procedures. Figure 1 depicts this two-stage generation process.</p>
<p>We base our generative stack on diffusion (Sohl-Dickstein et al., 2015; Song \&amp; Ermon, 2020b; Ho et al., 2020), a recently proposed generative framework which has become a popular choice for text-conditional image generation. For our text-to-image model, we use a version of GLIDE (Nichol et al., 2021) fine-tuned on 3D renderings (Section 4.2). For our image-to-3D model, we use a stack of diffusion models which generate RGB point clouds conditioned on</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1. A high-level overview of our pipeline. First, a text prompt is fed into a GLIDE model to produce a synthetic rendered view. Next, a point cloud diffusion stack conditions on this image to produce a 3D RGB point cloud.
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. Selected point clouds generated by Point$\cdot$E using the given text prompts. For each prompt, we selected one point cloud out of eight samples.</p>
<p>images (Section 4.3 and 4.4 detail our novel Transformerbased architecture for this task). For rendering-based evaluations, we go one step further and produce meshes from generated point clouds using a regression-based approach (Section 4.5).</p>
<p>We find that our system can often produce colored 3D point clouds that match both simple and complex text prompts (See Figure 2). We refer to our system as Point$\cdot$E, since it generates point clouds efficiently. We release our point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e.</p>
<h2>2. Background</h2>
<p>Our method builds off of a growing body of work on diffusion-based models, which were first proposed by SohlDickstein et al. (2015) and popularized more recently (Song \&amp; Ermon, 2020b;a; Ho et al., 2020).</p>
<p>We follow the Gaussian diffusion setup of Ho et al. (2020), which we briefly describe here. We aim to sample from some distribution $q\left(x_{0}\right)$ using a neural network approximation $p_{\theta}\left(x_{0}\right)$. Under Gaussian diffusion, we define a noising process</p>
<p>$$
q\left(x_{t} \mid x_{t-1}\right):=\mathcal{N}\left(x_{t} ; \sqrt{1-\beta_{t}} x_{t-1}, \beta_{t} \mathbf{I}\right)
$$</p>
<p>for integer timesteps $t \in[0, T]$. Intuitively, this process gradually adds Gaussian noise to a signal, with the amount of noise added at each timestep determined by some noise schedule $\beta_{t}$. We employ a noise schedule such that, by the final timestep $t=T$, the sample $x_{T}$ contains almost no information (i.e. it looks like Gaussian noise). Ho et al. (2020) note that it is possible to directly jump to a given timestep of the noising process without running the whole chain:</p>
<p>$$
x_{t}=\sqrt{\bar{\alpha}<em 0="0">{t}} x</em> \epsilon
$$}+\sqrt{1-\bar{\alpha}_{t}</p>
<p>where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ and $\bar{\alpha}<em s="0">{t}:=\prod</em>$.
To train a diffusion model, we approximate $q\left(x_{t-1} \mid x_{t}\right)$ as a neural network $p_{\theta}\left(x_{t-1} \mid x_{t}\right)$. We can then produce a sample by starting at random Gaussian noise $x_{T}$ and gradually reversing the noising process until arriving at a noiseless sample $x_{0}$. With enough small steps, $p_{\theta}\left(x_{t-1} \mid x_{t}\right)$ can be parameterized as a diagonal Gaussian distribution, and Ho et al. (2020) propose to parameterize the mean of this distribution by predicting $\epsilon$, the effective noise added to a sample $x_{t}$. While Ho et al. (2020) fix the variance $\Sigma$ of $p_{\theta}\left(x_{t-1} \mid x_{t}\right)$ to a reasonable per-timestep heuristic, Nichol \&amp; Dhariwal (2021) achieve better results by predicting the variance as well as the mean.}^{t} 1-\beta_{t</p>
<p>Diffusion sampling can be cast through the lens of differential equations (Song et al., 2020), allowing one to use various SDE and ODE solvers to sample from these models. Karras et al. (2022) find that a carefully-designed secondorder ODE solver provides a good trade-off between quality and sampling efficiency, and we employ this sampler for our point cloud diffusion models.</p>
<p>To trade off sample diversity for fidelity in diffusion models, several guidance strategies may be used. Dhariwal \&amp; Nichol (2021) introduce classifier guidance, where gradients from a noise-aware classifier $\nabla_{x_{t}} p_{\theta}\left(y \mid x_{t}\right)$ are used to perturb every sampling step. They find that increasing the scale of the perturbation increases generation fidelity while reducing sample diversity. Ho \&amp; Salimans (2021) introduce classifier-free guidance, wherein a conditional diffusion model $p\left(x_{t-1} \mid x_{t}, y\right)$ is trained with the class label stochastically dropped and replaced with an additional $\varnothing$ class. During sampling, the model's output $\epsilon$ is linearly extrapolated away from the unconditional prediction towards the conditional prediction:</p>
<p>$$
\epsilon_{\text {guided }}:=\epsilon_{\theta}\left(x_{t}, \varnothing\right)+s \cdot\left(\epsilon_{\theta}\left(x_{t}, y\right)-\epsilon_{\theta}\left(x_{t}, \varnothing\right)\right)
$$</p>
<p>for some guidance scale $s \geq 1$. This approach is straightforward to implement, requiring only that conditioning information is randomly dropped during training time. We employ this technique throughout our models, using the drop probability 0.1 .</p>
<h2>3. Related Work</h2>
<p>Several prior works have explored generative models over point clouds. Achlioptas et al. (2017) train point cloud autoencoders, and fit generative priors (either GANs (Goodfellow et al., 2014) or GMMs) on the resulting latent representations. Mo et al. (2019) generate point clouds using a VAE (Kingma \&amp; Welling, 2013) on hierarchical graph representations of 3D objects. Yang et al. (2019) train a two-stage flow model for point cloud generation: first, a prior flow model produces a latent vector, and then a second flow model samples points conditioned on the latent vector. Along the same lines, Luo \&amp; Hu (2021); Cai et al. (2020) both train two-stage models where the second stage is a diffusion model over individual points in a point cloud, and the first stage is a latent flow model or a latent GAN, respectively. Zeng et al. (2022) train a two-stage hierarchical VAE on point clouds with diffusion priors at both stages. Most similar to our work, Zhou et al. (2021a) introduce PVD, a single diffusion model that generates point clouds directly. Compared to previous point cloud diffusion methods such as PVD, our Transformer-based model architecture is simpler and incorporates less 3D-specific structure. Unlike prior works, our models also produce RGB channels alongside</p>
<p>point cloud coordinates.
A growing body of work explores the problem of 3D model generation in representations other than point clouds. Several works aim to train 3D-aware GANs from datasets of 2D images (Chan et al., 2020; Schwarz et al., 2020; Chan et al., 2021; Or-El et al., 2021; Gu et al., 2021; Zhou et al., 2021b). These GANs are typically applied to the problem of novel view synthesis in forward-facing scenes, and do not attempt to reconstruct full 360-degree views of objects. More recently, Gao et al. (2022) train a GAN that directly produces full 3D meshes, paired with a discriminator that inputs differentiably-rendered (Laine et al., 2020) views of the generated meshes. Bautista et al. (2022) generates complete 3D scenes by first learning a representation space that decodes into NeRFs (Mildenhall et al., 2020), and then training a diffusion prior on this representation space. However, none of these works have demonstrated the ability to generate arbitrary 3D models conditioned on open-ended, complex text-prompts.</p>
<p>Several recent works have explored the problem of textconditional 3D generation by optimizing 3D representations according to a text-image matching objective. Jain et al. (2021) introduce DreamFields, a method which optimizes the parameters of a NeRF using an objective based on CLIP (Radford et al., 2021). Notably, this method requires no 3D training data. Building on this principle, Khalid et al. (2022) optimizes a mesh using a CLIP-guided objective, finding that the mesh representation is more efficient to optimize than a NeRF. More recently, Poole et al. (2022) extend DreamFields to leverage a pre-trained text-to-image diffusion model instead of CLIP, producing more coherent and complex objects. Lin et al. (2022a) build off of this technique, but convert the NeRF representation into a mesh and then refine the mesh representation in a second optimization stage. While these approaches are able to produce diverse and complex objects or scenes, the optimization procedures typically require multiple GPU hours to converge, making them difficult to apply in practical settings.</p>
<p>While the above approaches are all based on optimization against a text-image model and do not leverage 3D data, other methods for text-conditional 3D synthesis make use of 3D data, possibly paired with text labels. Chen et al. (2018) employ a dataset of text-3D pairs to train a GAN to generate 3D representations conditioned on text. Liu et al. (2022) also leverage paired text-3D data to generate models in a joint representation space. Sanghi et al. (2021) employ a flow-based model to generate 3D latent representations, and find some text-to-3D capabilities when conditioning their model on CLIP embeddings. More recently, Zeng et al. (2022) achieve similar results when conditioning on CLIP embeddings, but employ a hierarchical VAE on point clouds for their generative stack. Mittal et al. (2022) and</p>
<p>Fu et al. (2022) employ a VQ-VAE (van den Oord et al., 2017) with an autoregressive prior to sample 3D shapes conditioned on text labels. More recently, Sanghi et al. (2022) also employ a VQ-VAE approach, but leverage CLIP embeddings to avoid the need for explicit text labels in the dataset. While many of these works demonstrate promising early results, they tend to be limited to simple prompts or a narrow set of object categories due to the limited availability of 3D training data. Our method sidesteps this issue by leveraging a pre-trained text-to-image model to condition our 3D generation procedure.</p>
<p>A large body of research focuses on reconstructing 3D models from single or few images. Notably, this is an underspecified problem, since the model must impute some details not present in the conditioning image(s). Nevertheless, some regression-based methods have shown promising results on this task (Choy et al., 2016; Wang et al., 2018; Gkioxari et al., 2019; Groueix et al., 2018; Yu et al., 2020; Lin et al., 2022b). A separate body of literature studies generative approaches for single- or multi-view reconstruction. Fan et al. (2016) predict point clouds of objects from single views using a VAE. Sun et al. (2018) use a hybrid of a flow predictor and a GAN to generate novel views from few images. Kosiorek et al. (2021) use a view-conditional VAE to generate latent vectors for a NeRF decoder. Watson et al. (2022) employ an image-to-image diffusion model to synthesize novel views of an object conditioned on a single view, allowing many consistent views to be synthesized autoregressively.</p>
<h2>4. Method</h2>
<p>Rather than training a single generative model to directly produce point clouds conditioned on text, we instead break the generation process into three steps. First, we generate a synthetic view conditioned on a text caption. Next, we produce a coarse point cloud (1,024 points) conditioned on the synthetic view. And finally, we produce a fine point cloud (4,096 points) conditioned on the low-resolution point cloud and the synthetic view. In practice, we assume that the image contains the relevant information from the text, and do not explicitly condition the point clouds on the text.</p>
<p>To generate text-conditional synthetic views, we use a 3billion parameter GLIDE model (Nichol et al., 2021) finetuned on rendered 3D models from our dataset (Section 4.2). To generate low-resolution point clouds, we use a conditional, permutation invariant diffusion model (Section 4.3). To upsample these low-resolution point clouds, we use a similar (but smaller) diffusion model which is additionally conditioned on the low-resolution point cloud (Section 4.4).</p>
<p>We train our models on a dataset of several million 3D models and associated metadata. We process the dataset into rendered views, text descriptions, and 3D point clouds</p>
<p>with associated RGB colors for each point. We describe our data processing pipeline in more detail in Section 4.1.</p>
<h3>4.1. Dataset</h3>
<p>We train our models on several million 3D models. We found that data formats and quality varied wildly across our dataset, prompting us to develop various post-processing steps to ensure higher data quality.</p>
<p>To convert all of our data into one generic format, we rendered every 3D model from 20 random camera angles as RGBAD images using Blender (Community, 2018), which supports a variety of 3D formats and comes with an optimized rendering engine. For each model, our Blender script normalizes the model to a bounding cube, configures a standard lighting setup, and finally exports RGBAD images using Blender’s built-in realtime rendering engine.</p>
<p>We then converted each object into a colored point cloud using its renderings. In particular, we first constructed a dense point cloud for each object by computing points for each pixel in each RGBAD image. These point clouds typically contain hundreds of thousands of unevenly spaced points, so we additionally used farthest point sampling to create uniform clouds of 4K points. By constructing point clouds directly from renders, we were able to sidestep various issues that might arise from attempting to sample points directly from 3D meshes, such as sampling points which are contained within the model or dealing with 3D models that are stored in unusual file formats.</p>
<p>Finally, we employed various heuristics to reduce the frequency of low-quality models in our dataset. First, we eliminated flat objects by computing the SVD of each point cloud and only retaining those where the smallest singular value was above a certain threshold. Next, we clustered the dataset by CLIP features (for each object, we averaged features over all renders). We found that some clusters contained many low-quality categories of models, while other clusters appeared more diverse or interpretable. We binned these clusters into several buckets of varying quality, and used a weighted mixture of the resulting buckets as our final dataset.</p>
<h3>4.2. View Synthesis GLIDE Model</h3>
<p>Our point cloud models are conditioned on rendered views from our dataset, which were all produced using the same renderer and lighting settings. Therefore, to ensure that these models correctly handle generated synthetic views, we aim to explicitly generate 3D renders that match the distribution of our dataset.</p>
<p>To this end, we fine-tune GLIDE with a mixture of its original dataset and our dataset of 3D renderings. Since our 3D dataset is small compared to the original GLIDE training</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3. Our point cloud diffusion model architecture. Images are fed through a frozen, pre-trained CLIP model, and the output grid is fed as tokens into the transformer. Both the timestep t and noised input x<sup>t</sup> are also fed in as tokens. The output tokens corresponding to x<sup>t</sup> are used to predict ε and Σ.</p>
<p>set, we only sample images from the 3D dataset 5% of the time, using the original dataset for the remaining 95%. We fine-tune for 100K iterations, meaning that the model has made several epochs over the 3D dataset (but has never seen the same exact rendered viewpoint twice).</p>
<p>To ensure that we always sample in-distribution renders (rather than only sampling them 5% of the time), we add a special token to every 3D render’s text prompt indicating that it is a 3D render; we then sample with this token at test time.</p>
<h3>4.3. Point Cloud Diffusion</h3>
<p>To generate point clouds with diffusion, we extend the framework used by Zhou et al. (2021a) to include RGB colors for each point in a point cloud. In particular, we represent a point cloud as a tensor of shape K × 6, where K is the number of points, and the inner dimension contains (x, y, z) coordinates as well as (R, G, B) colors. All coordinates and colors are normalized to the range [−1, 1]. We then generate these tensors directly with diffusion, starting from random noise of shape K × 6, and gradually denoising it.</p>
<p>Unlike prior work which leverages 3D-specific architectures to process point clouds, we use a simple Transformer-based model (Vaswani et al., 2017) to predict both ε and Σ conditioned on the image, timestep t, and noised point cloud x<sup>t</sup>. An overview of our architecture can be seen in Figure 3. As input context to this model, we run each point in the point cloud through a linear layer with output dimension D, obtaining a K × D input tensor. Additionally, we run the timestep t through a small MLP, obtaining another D-dimensional vector to prepend to the context.</p>
<p>To condition on the image, we feed it through a pre-trained</p>
<p>ViT-L/14 CLIP model, take the last layer embeddings from this CLIP model (of shape $256 \times D^{\prime}$ ), and linearly project it into another tensor of shape $256 \times D$ before prepending it to the Transformer context. In Section 5.1, we find that this is superior to using a single CLIP image or text embedding, as done by Sanghi et al. (2021); Zeng et al. (2022); Sanghi et al. (2022).</p>
<p>The final input context to our model is of shape $(K+257) \times$ $D$. To obtain a final output sequence of length $K$, we take the final $K$ tokens of output and project it to obtain $\epsilon$ and $\Sigma$ predictions for the $K$ input points.</p>
<p>Notably, we do not employ positional encodings for this model. As a result, the model itself is permutation-invariant to the input point clouds (although the output order is tied to the input order).</p>
<h3>4.4. Point Cloud Upsampler</h3>
<p>For image diffusion models, the best quality is typically achieved by using some form of hierarchy, where a lowresolution base model produces output which is then upsampled by another model (Nichol \&amp; Dhariwal, 2021; Saharia et al., 2021; Ho et al., 2021; Rombach et al., 2021). We employ this approach to point cloud generation by first generating 1 K points with a large base model, and then upsampling to 4 K points using a smaller upsampling model. Notably, our models' compute requirements scale with the number of points, so it is four times more expensive to generate 4 K points than 1 K points for a fixed model size.</p>
<p>Our upsampler uses the same architecture as our base model, with extra conditioning tokens for the low-resolution point cloud. To arrive at 4 K points, the upsampler conditions on 1 K points and generates an additional 3 K points which are added to the low-resolution pointcloud. We pass the conditioning points through a separate linear embedding layer than the one used for $x_{t}$, allowing the model to distinguish conditioning information from new points without requiring the use of positional embeddings.</p>
<h3>4.5. Producing Meshes</h3>
<p>For rendering-based evaluations, we do not render generated point clouds directly. Rather, we convert the point clouds into textured meshes and render these meshes using Blender. Producing meshes from point clouds is a well-studied, sometimes difficult problem. Point clouds produced by our models often have cracks, outliers, or other types of noise that make the problem particularly challenging. We briefly tried using pre-trained SAP models (Peng et al., 2021) for this purpose, but found that the resulting meshes sometimes lost large portions or important details of the shape that were present in the point clouds. Rather than training new SAP models, we opted to take a simpler approach.</p>
<p>To convert point clouds into meshes, we use a regressionbased model to predict the signed distance field of an object given its point cloud, and then apply marching cubes (Lorensen \&amp; Cline, 1987) to the resulting SDF to extract a mesh. We then assign colors to each vertex of the mesh using the color of the nearest point from the original point cloud. For details, see Appendix C.</p>
<h2>5. Results</h2>
<p>In the following sections, we conduct a number of ablations and comparisons to evaluate how our method performs and scales. We adopt the CLIP R-Precision (Park et al., 2021) metric for evaluating text-to-3D methods end-to-end, using the same object-centric evaluation prompts as Jain et al. (2021). Additionally, we introduce a new pair of metrics which we refer to as P-IS and P-FID, which are point cloud analogs for Inception Score (Salimans et al., 2016) and FID (Heusel et al., 2017), respectively.</p>
<p>To construct our P-IS and P-FID metrics, we employ a modified PointNet++ model (Qi et al., 2017) to extract features and predict class probabilities for point clouds. For details, see Appendix B.</p>
<h3>5.1. Model Scaling and Ablations</h3>
<p>In this section, we train a variety of base diffusion models to study the effect of scaling and to ablate the importance of image conditioning. We train the following base models and evaluate them throughout training:</p>
<ul>
<li>40M (uncond.): a small model without any conditioning information.</li>
<li>40M (text vec.): a small model which only conditions on text captions, not rendered images. The text caption is embedded with CLIP, and the CLIP embedding is appended as a single extra token of context. This model depends on the text captions present in our 3D dataset, and does not leverage the fine-tuned GLIDE model.</li>
<li>40M (image vec.): a small model which conditions on CLIP image embeddings of rendered images, similar to Sanghi et al. (2021). This differs from the other image-conditional models in that the image is encoded into a single token of context, rather than as a sequence of latents corresponding to the CLIP latent grid.</li>
<li>40M: a small model with full image conditioning through a grid of CLIP latents.</li>
<li>300M: a medium model with full image conditioning through a grid of CLIP latents.</li>
<li>1B: a large model with full image conditioning through a grid of CLIP latents.</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4. Sample-based evaluations computed throughout training across different base model runs. The same upsampler and conditioning images are used for all runs.</p>
<p>In order to isolate changes to the base model, we use the same (image conditional) upsampler model for all evaluations, and use the same 306 pre-generated synthetic views for the CLIP R-Precision evaluation prompts. Here we use the ViT-L/14 CLIP model to compute CLIP R-Precision, but we report results with an alternative CLIP model in Section 5.3.</p>
<p>In Figure 4, we present the results of our ablations. We find that using only text conditioning with no text-to-image step results in much worse CLIP R-Precision (see Appendix E for more details). Furthermore, we find that using a single CLIP embedding to condition on images is worse than using a grid of embeddings, suggesting that the point cloud model benefits from seeing more (spatial) information about the conditioning image. Finally, we find that scaling our model improves the speed of P-FID convergence, and increases final CLIP R-Precision.
<img alt="img-4.jpeg" src="img-4.jpeg" />
(a) Image to point cloud sample for the prompt "a very realistic 3D rendering of a corgi".
<img alt="img-5.jpeg" src="img-5.jpeg" />
(b) Image to point cloud sample for the prompt "a traffic cone".</p>
<p>Figure 5. Two common failure modes of our model. In the top example, the model incorrectly interprets the relative proportions of different parts of the depicted object, producing a tall dog instead of a short, long dog. In the bottom example, the model cannot see underneath the traffic cone, and incorrectly infers a second mirrored cone.</p>
<h3>5.2. Qualitative Results</h3>
<p>We find that Point $\cdot \mathrm{E}$ can often produce consistent and highquality 3D shapes for complex prompts. In Figure 2, we show various point cloud samples which demonstrate our model's ability to infer a variety of shapes while correctly binding colors to the relevant parts of the shapes.</p>
<p>Sometimes the point cloud diffusion model fails to understand or extrapolate the conditioning image, resulting in a shape that does not match the original prompt. We find that this is usually due to one of two issues: 1) the model incorrectly interprets the shape of the object depicted in the image, or 2) the model incorrectly infers some part of the shape that is occluded in the image. In Figure 5, we present an example of each of these two failure modes.</p>
<h3>5.3. Comparison to Other Methods</h3>
<p>As text-conditional 3D synthesis is a fairly new area of research, there is not yet a standard set of benchmarks for this task. However, several other works evaluate 3D generation using CLIP R-Precision, and we compare to these methods in Table 1. In addition to CLIP R-Precision, we also note the reported sampling compute requirements for each method.</p>
<p>While our method performs worse than the current state-of-the-art, we note two subtleties of this evaluation which may explain some (but likely not all) of this discrepancy:</p>
<p>Table 1. Comparison of Point$\cdot$E to other 3D generative techniques as measured by CLIP R-Precision (with two different CLIP base models) on COCO evaluation prompts. ${ }^{+} 50$ P100-minutes converted to V100-minutes using conversion rate $\frac{1}{2}$. ${ }^{1}$ Assuming 2 V100 minutes $=1$ A100 minute and 1 TPUv4-minute $=1$ A100minute. We report DreamFields results from Poole et al. (2022).</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">ViT-B/32</th>
<th style="text-align: center;">ViT-L/14</th>
<th style="text-align: center;">Latency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">DreamFields</td>
<td style="text-align: center;">$78.6 \%$</td>
<td style="text-align: center;">$82.9 \%$</td>
<td style="text-align: center;">$\sim 200 \mathrm{~V} 100-\mathrm{hr}^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">CLIP-Mesh</td>
<td style="text-align: center;">$67.8 \%$</td>
<td style="text-align: center;">$74.5 \%$</td>
<td style="text-align: center;">$\sim 17 \mathrm{~V} 100-\mathrm{min}^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">DreamFusion</td>
<td style="text-align: center;">$75.1 \%$</td>
<td style="text-align: center;">$79.7 \%$</td>
<td style="text-align: center;">$\sim 12 \mathrm{~V} 100-\mathrm{hr}^{1}$</td>
</tr>
<tr>
<td style="text-align: center;">Point$\cdot$E (40M, <br> text-only)</td>
<td style="text-align: center;">$15.4 \%$</td>
<td style="text-align: center;">$16.2 \%$</td>
<td style="text-align: center;">16 V100-sec</td>
</tr>
<tr>
<td style="text-align: center;">Point$\cdot$E (40M)</td>
<td style="text-align: center;">$36.5 \%$</td>
<td style="text-align: center;">$38.8 \%$</td>
<td style="text-align: center;">1.0 V100-min</td>
</tr>
<tr>
<td style="text-align: center;">Point$\cdot$E (300M)</td>
<td style="text-align: center;">$40.3 \%$</td>
<td style="text-align: center;">$45.6 \%$</td>
<td style="text-align: center;">1.2 V100-min</td>
</tr>
<tr>
<td style="text-align: center;">Point$\cdot$E (1B)</td>
<td style="text-align: center;">$41.1 \%$</td>
<td style="text-align: center;">$46.8 \%$</td>
<td style="text-align: center;">1.5 V100-min</td>
</tr>
<tr>
<td style="text-align: center;">Conditioning <br> images</td>
<td style="text-align: center;">$69.6 \%$</td>
<td style="text-align: center;">$86.6 \%$</td>
<td style="text-align: center;">-</td>
</tr>
</tbody>
</table>
<ul>
<li>Unlike multi-view optimization-based methods like DreamFusion, Point$\cdot$E does not explicitly optimize every view to match the text prompt. This could result in lower CLIP R-Precision simply because certain objects are not easy to identify from all angles.</li>
<li>Our method produces point clouds which must be preprocessed before rendering. Converting point clouds into meshes is a difficult problem, and the approach we use can sometimes lose information present in the point clouds themselves.</li>
</ul>
<p>While our method performs worse on this evaluation than state-of-the-art techniques, it produces samples in a small fraction of the time. This could make it more practical for certain applications, or could allow for the discovery of higher-quality 3D objects by sampling many objects and selecting the best one according to some heuristic.</p>
<h2>6. Limitations and Future Work</h2>
<p>While our model is a meaningful step towards fast text-to3D synthesis, it also has several limitations. Currently, our pipeline requires synthetic renderings, but this limitation could be lifted in the future by training 3D generators that condition on real-world images. Furthermore, while our method produces colored three-dimensional shapes, it does so at a relatively low resolution in a 3D format (point clouds) that does not capture fine-grained shape or texture. Extending this method to produce high-quality 3D representations such as meshes or NeRFs could allow the model's outputs to be used for a variety of applications. Finally, our method could be used to initialize optimization-based techniques to speed up initial convergence.</p>
<p>We expect that this model shares many of the limitations,
<img alt="img-6.jpeg" src="img-6.jpeg" />
"a 3D printable gear, a single gear 3 inches in diameter and half inch thick"</p>
<p>Figure 6. Example of a potential misuse of our model, where it could be used to fabricate objects in the real world without external validation.
including bias, as our DALL$\cdot$E 2 system where many of the biases are inherited from the dataset (Mishkin et al., 2022). In addition, this model has the ability to support the creation of point clouds that can then be used to fabricate products in the real world, for example through 3D printing (Walther, 2014; Neely, 2016; Straub \&amp; Kerlin, 2016). This has implications both when the models are used to create blueprints for dangerous objects and when the blueprints are trusted to be safe despite no empirical validation (Figure 6).</p>
<h2>7. Conclusion</h2>
<p>We have presented Point$\cdot$E, a system for text-conditional synthesis of 3D point clouds that first generates synthetic views and then generates colored point clouds conditioned on these views. We find that Point$\cdot$E is capable of efficiently producing diverse and complex 3D shapes conditioned on text prompts. We hope that our approach can serve as a starting point for further work in the field of text-to-3D synthesis.</p>
<h2>8. Acknowledgements</h2>
<p>We would like to thank everyone behind ChatGPT for creating a tool that helped provide useful writing feedback.</p>
<h2>References</h2>
<p>Achlioptas, P., Diamanti, O., Mitliagkas, I., and Guibas, L. Learning representations and generative models for 3d point clouds. arXiv:1707.02392, 2017.</p>
<p>Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and Liu, M.-Y. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers, 2022.</p>
<p>Bautista, M. A., Guo, P., Abnar, S., Talbott, W., Toshev, A., Chen, Z., Dinh, L., Zhai, S., Goh, H., Ulbricht, D.,</p>
<p>Dehghan, A., and Susskind, J. Gaudi: A neural architect for immersive 3d scene generation. arXiv:2207.13751, 2022.</p>
<p>Cai, R., Yang, G., Averbuch-Elor, H., Hao, Z., Belongie, S., Snavely, N., and Hariharan, B. Learning gradient fields for shape generation. arXiv:2008.06520, 2020.</p>
<p>Chan, E. R., Monteiro, M., Kellnhofer, P., Wu, J., and Wetzstein, G. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. arXiv:2012.00926, 2020.</p>
<p>Chan, E. R., Lin, C. Z., Chan, M. A., Nagano, K., Pan, B., Mello, S. D., Gallo, O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., and Wetzstein, G. Efficient geometry-aware 3d generative adversarial networks. arXiv:2112.07945, 2021.</p>
<p>Chen, K., Choy, C. B., Savva, M., Chang, A. X., Funkhouser, T., and Savarese, S. Text2shape: Generating shapes from natural language by learning joint embeddings. arXiv:1803.08495, 2018.</p>
<p>Choy, C. B., Xu, D., Gwak, J., Chen, K., and Savarese, S. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. arXiv:1604.00449, 2016.</p>
<p>Community, B. O. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www. blender.org.</p>
<p>Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. arXiv:2105.05233, 2021.</p>
<p>Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., and Tang, J. Cogview: Mastering text-to-image generation via transformers. arXiv:2105.13290, 2021.</p>
<p>Fan, H., Su, H., and Guibas, L. A point set generation network for 3d object reconstruction from a single image. arXiv:1612.00603, 2016.</p>
<p>Feng, Z., Zhang, Z., Yu, X., Fang, Y., Li, L., Chen, X., Lu, Y., Liu, J., Yin, W., Feng, S., Sun, Y., Tian, H., Wu, H., and Wang, H. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. arXiv:2210.15257, 2022.</p>
<p>Fu, R., Zhan, X., Chen, Y., Ritchie, D., and Sridhar, S. Shapecrafter: A recursive text-conditioned 3d shape generation model. arXiv:2207.09446, 2022.</p>
<p>Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., and Taigman, Y. Make-a-scene: Scene-based text-toimage generation with human priors. arXiv:2203.13131, 2022.</p>
<p>Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., and Fidler, S. Get3d: A generative model of high quality 3d textured shapes learned from images. arXiv:2209.11163, 2022.</p>
<p>Gkioxari, G., Malik, J., and Johnson, J. Mesh r-cnn. arXiv:1906.02739, 2019.</p>
<p>Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. arXiv:1406.2661, 2014.</p>
<p>Groueix, T., Fisher, M., Kim, V. G., Russell, B. C., and Aubry, M. Atlasnet: A papier-mâché approach to learning 3d surface generation. arXiv:1802.05384, 2018.</p>
<p>Gu, J., Liu, L., Wang, P., and Theobalt, C. Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. arXiv:2110.08985, 2021.</p>
<p>Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017.</p>
<p>Ho, J. and Salimans, T. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https:// openreview.net/forum?id=qw8AKxfYbI.</p>
<p>Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. arXiv:2006.11239, 2020.</p>
<p>Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and Salimans, T. Cascaded diffusion models for high fidelity image generation. arXiv:2106.15282, 2021.</p>
<p>Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., and Salimans, T. Imagen video: High definition video generation with diffusion models. arXiv:2210.02303, 2022a.</p>
<p>Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. Video diffusion models. arXiv:2204.03458, 2022b.</p>
<p>Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022.</p>
<p>Jain, A., Mildenhall, B., Barron, J. T., Abbeel, P., and Poole, B. Zero-shot text-guided object generation with dream fields. arXiv:2112.01455, 2021.</p>
<p>Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. arXiv:2206.00364, 2022.</p>
<p>Khalid, N. M., Xie, T., Belilovsky, E., and Popa, T. Clipmesh: Generating textured meshes from text using pretrained image-text models. arXiv:2203.13333, 2022.</p>
<p>Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv:1312.6114, 2013.</p>
<p>Kosiorek, A. R., Strathmann, H., Zoran, D., Moreno, P., Schneider, R., Mokrá, S., and Rezende, D. J. NeRFVAE: A geometry aware 3D scene generative model. arXiv:2104.00587, April 2021.</p>
<p>Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., and Aila, T. Modular primitives for high-performance differentiable rendering. arXiv:2011.03277, 2020.</p>
<p>Lin, C.-H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.-Y., and Lin, T.-Y. Magic3d: High-resolution text-to-3d content creation. arXiv:2211.10440, 2022a.</p>
<p>Lin, K.-E., Yen-Chen, L., Lai, W.-S., Lin, T.-Y., Shih, Y.-C., and Ramamoorthi, R. Vision transformer for nerf-based view synthesis from a single input image. arXiv:2207.05736, 2022b.</p>
<p>Liu, Z., Wang, Y., Qi, X., and Fu, C.-W. Towards implicit text-guided 3d shape generation. arXiv:2203.14622, 2022.</p>
<p>Lorensen, W. E. and Cline, H. E. Marching cubes: A high resolution 3d surface construction algorithm. In Stone, M. C. (ed.), SIGGRAPH, pp. 163-169. ACM, 1987. ISBN 0-89791-227-6. URL http: //dblp.uni-trier.de/db/conf/siggraph/ siggraph1987.html#LorensenC87.</p>
<p>Luo, S. and Hu, W. Diffusion probabilistic models for 3d point cloud generation. arXiv:2103.01458, 2021.</p>
<p>Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance fields for view synthesis. arXiv:2003.08934, 2020.</p>
<p>Mishkin, P., Ahmad, L., Brundage, M., Krueger, G., and Sastry, G. Dall-e 2 preview - risks and limitations. 2022. URL https://github.com/ openai/dalle-2-preview/blob/main/ system-card.md.</p>
<p>Mittal, P., Cheng, Y.-C., Singh, M., and Tulsiani, S. Autosdf: Shape priors for 3d completion, reconstruction and generation. arXiv:2203.09516, 2022.</p>
<p>Mo, K., Guerrero, P., Yi, L., Su, H., Wonka, P., Mitra, N., and Guibas, L. J. Structurenet: Hierarchical graph networks for 3d shape generation. arXiv:1908.00575, 2019.</p>
<p>Neely, E. L. The risks of revolution: Ethical dilemmas in 3d printing from a us perspective. Science and Engineering Ethics, 22(5):1285-1297, Oct 2016. ISSN 14715546. doi: 10.1007/s11948-015-9707-4. URL https: //doi.org/10.1007/s11948-015-9707-4.</p>
<p>Nichol, A. and Dhariwal, P. Improved denoising diffusion probabilistic models. arXiv:2102.09672, 2021.</p>
<p>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv:2112.10741, 2021.</p>
<p>Or-El, R., Luo, X., Shan, M., Shechtman, E., Park, J. J., and Kemelmacher-Shlizerman, I. Stylesdf: Highresolution 3d-consistent image and geometry generation. arXiv:2112.11427, 2021.</p>
<p>Park, D. H., Azadi, S., Liu, X., Darrell, T., and Rohrbach, A. Benchmark for compositional text-to-image synthesis. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.net/forum? id=bKBhQhPeKaF.</p>
<p>Peng, S., Jiang, C. M., Liao, Y., Niemeyer, M., Pollefeys, M., and Geiger, A. Shape as points: A differentiable poisson solver. arXiv:2106.03452, 2021.</p>
<p>Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. arXiv:2209.14988, 2022.</p>
<p>Qi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv:1706.02413, 2017.</p>
<p>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. arXiv:2103.00020, 2021.</p>
<p>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-toimage generation. arXiv:2102.12092, 2021.</p>
<p>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv:2204.06125, 2022.</p>
<p>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. arXiv:2112.10752, 2021.</p>
<p>Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J., and Norouzi, M. Image super-resolution via iterative refinement. arXiv:arXiv:2104.07636, 2021.</p>
<p>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Photorealistic text-to-image diffusion models with deep language understanding. arXiv:2205.11487, 2022.</p>
<p>Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. arXiv:1606.03498, 2016.</p>
<p>Sanghi, A., Chu, H., Lambourne, J. G., Wang, Y., Cheng, C.-Y., Fumero, M., and Malekshan, K. R. Clip-forge: Towards zero-shot text-to-shape generation. arXiv:2110.02624, 2021.</p>
<p>Sanghi, A., Fu, R., Liu, V., Willis, K., Shayani, H., Khasahmadi, A. H., Sridhar, S., and Ritchie, D. Textcraft: Zeroshot generation of high-fidelity and diverse shapes from text. arXiv:2211.01427, 2022.</p>
<p>Schwarz, K., Liao, Y., Niemeyer, M., and Geiger, A. Graf: Generative radiance fields for 3d-aware image synthesis. 2020.</p>
<p>Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., and Taigman, Y. Make-a-video: Text-to-video generation without text-video data. arXiv:2209.14792, 2022.</p>
<p>Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv:1503.03585, 2015.</p>
<p>Song, Y. and Ermon, S. Improved techniques for training score-based generative models. arXiv:2006.09011, 2020a.</p>
<p>Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. arXiv:arXiv:1907.05600, 2020b.</p>
<p>Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv:2011.13456, 2020.</p>
<p>Straub, J. and Kerlin, S. Evaluation of the use of 3D printing and imaging to create working replica keys. In Javidi, B. and Son, J.-Y. (eds.), Three-Dimensional Imaging, Visualization, and Display 2016, volume 9867, pp. 98670E. International Society for Optics and Photonics, SPIE, 2016. doi: 10.1117/12.2223858. URL https://doi.org/10.1117/12.2223858.</p>
<p>Sun, S.-H., Huh, M., Liao, Y.-H., Zhang, N., and Lim, J. J. Multi-view to novel view: Synthesizing novel views with
self-learned confidence. In Ferrari, V., Hebert, M., Sminchisescu, C., and Weiss, Y. (eds.), Computer Vision ECCV 2018, pp. 162-178, Cham, 2018. Springer International Publishing. ISBN 978-3-030-01219-9.
van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. arXiv:1711.00937, 2017.</p>
<p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. arXiv:1706.03762, 2017.</p>
<p>Walther, G. Printing insecurity? the security implications of 3d-printing of weapons. Science and engineering ethics, 21, 12 2014. doi: 10.1007/s11948-014-9617-x.</p>
<p>Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., and Jiang, Y.-G. Pixel2mesh: Generating 3d mesh models from single rgb images. arXiv:1804.01654, 2018.</p>
<p>Watson, D., Chan, W., Martin-Brualla, R., Ho, J., Tagliasacchi, A., and Norouzi, M. Novel view synthesis with diffusion models. arXiv:2210.04628, 2022.</p>
<p>Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.</p>
<p>Yang, G., Huang, X., Hao, Z., Liu, M.-Y., Belongie, S., and Hariharan, B. Pointflow: 3d point cloud generation with continuous normalizing flows. arXiv:1906.12320, 2019.</p>
<p>Yu, A., Ye, V., Tancik, M., and Kanazawa, A. pixelnerf: Neural radiance fields from one or few images. arXiv:2012.02190, 2020.</p>
<p>Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., and Wu, Y. Scaling autoregressive models for contentrich text-to-image generation. arXiv:2206.10789, 2022.</p>
<p>Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., and Kreis, K. Lion: Latent point diffusion models for 3d shape generation. arXiv:2210.06978, 2022.</p>
<p>Zhou, L., Du, Y., and Wu, J. 3d shape generation and completion through point-voxel diffusion. arXiv:2104.03670, 2021a.</p>
<p>Zhou, P., Xie, L., Ni, B., and Tian, Q. Cips-3d: A 3d-aware generator of gans based on conditionally-independent pixel synthesis. arXiv:2110.09788, 2021b.</p>
<p>Table 2. Training hyper-parameters for our point cloud diffusion models. Width and depth refer to the size of the transformer backbone.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Width</th>
<th style="text-align: center;">Depth</th>
<th style="text-align: center;">LR</th>
<th style="text-align: center;"># Params</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Base (40M)</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">$40,466,956$</td>
</tr>
<tr>
<td style="text-align: center;">Base (300M)</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$7 \mathrm{e}-5$</td>
<td style="text-align: center;">$311,778,316$</td>
</tr>
<tr>
<td style="text-align: center;">Base (1B)</td>
<td style="text-align: center;">2048</td>
<td style="text-align: center;">24</td>
<td style="text-align: center;">$5 \mathrm{e}-5$</td>
<td style="text-align: center;">$1,244,311,564$</td>
</tr>
<tr>
<td style="text-align: center;">Upsampler</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">$1 \mathrm{e}-4$</td>
<td style="text-align: center;">$40,470,540$</td>
</tr>
</tbody>
</table>
<p>Table 3. Sampling hyperparameters for figures and CLIP RPrecision evaluations.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Hyperparameter</th>
<th style="text-align: center;">Base</th>
<th style="text-align: center;">Upsampler</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Timesteps</td>
<td style="text-align: center;">64</td>
<td style="text-align: center;">64</td>
</tr>
<tr>
<td style="text-align: center;">Guidance scale</td>
<td style="text-align: center;">3.0</td>
<td style="text-align: center;">3.0</td>
</tr>
<tr>
<td style="text-align: center;">$S_{\text {churn }}$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\sigma_{\text {min }}$</td>
<td style="text-align: center;">$1 \mathrm{e}-3$</td>
<td style="text-align: center;">$1 \mathrm{e}-3$</td>
</tr>
<tr>
<td style="text-align: center;">$\sigma_{\text {max }}$</td>
<td style="text-align: center;">120</td>
<td style="text-align: center;">160</td>
</tr>
</tbody>
</table>
<h2>A. Hyperparameters</h2>
<p>We train all of our diffusion models with batch size 64 for 1,300,000 iterations. In Table 2, we enumerate the training hyperparameters that were varied across model sizes. We train all of our models with 1024 diffusion timesteps. For our upsampler model, we use the linear noise schedule from Ho et al. (2020), and for our base models, we use the cosine noise schedule proposed by Nichol \&amp; Dhariwal (2021).</p>
<p>For P-FID and P-IS evaluations, we produce 10K samples using stochastic DDPM with the full noise schedule. For CLIP R-Precision and figures in the paper, we use 64 steps (128 function evaluations) of the Heun sampler from Karras et al. (2022) for both the base and upsampler models. Table 3 enumerates the hyperparameters used for Heun sampling. When sampling from GLIDE, we use 150 diffusion steps for the base model, and 50 diffusion steps for the upsampling model. We report sampling time for each component of our stack in Table 4.</p>
<h2>B. P-FID and P-IS Metrics</h2>
<p>To evaluate P-FID and P-IS, we train a PointNet++ model on ModelNet40 (Wu et al., 2015) using an open source implementation. ${ }^{1}$ We modify the baseline model in several ways. First, we double the width of the model, resulting in roughly 16 million parameters. Next, we apply some additional data augmentations to make the model more robust to out-of-distribution samples. In particular, we apply random</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 4. Sampling performance for various components of our model. We use the Karras sampler for our base and upsampler models, but not for GLIDE.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">V100 seconds</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GLIDE</td>
<td style="text-align: center;">46.28</td>
</tr>
<tr>
<td style="text-align: center;">Upsampler (40M)</td>
<td style="text-align: center;">12.58</td>
</tr>
<tr>
<td style="text-align: center;">Base (40M)</td>
<td style="text-align: center;">3.35</td>
</tr>
<tr>
<td style="text-align: center;">Base (300M)</td>
<td style="text-align: center;">12.78</td>
</tr>
<tr>
<td style="text-align: center;">Base (1B)</td>
<td style="text-align: center;">28.67</td>
</tr>
</tbody>
</table>
<p>rotations to each point cloud, and we add Gaussian noise to the points with standard deviation sampled from $U[0,0.01]$.</p>
<p>To compute P-FID, we extract features for each point cloud from the layer before the final ReLU activation. To compute P-IS, we use the predicted class probabilities for the 40 classes from ModelNet40. We note that our generative models are trained on a dataset which only has P-IS 12.95, so our best reported P-IS score of $\sim 13$ is near the expected upper bound.</p>
<h2>C. Mesh Extraction</h2>
<p>To convert point clouds into meshes, we train a model which predicts SDFs from point clouds and apply marching cubes to the resulting SDFs. We parametrize our SDF model as an encoder-decoder Transformer. First, an 8-layer encoder processes the input point cloud as an unordered sequence, producing a sequence of hidden representations. Then, a 4-layer cross-attention decoder takes 3D coordinates and the sequence of latent vectors, and predicts an SDF value. Each input query point is processed independently, allowing for efficient batching. Using more layers in the encoder and fewer in the decoder allows us to amortize the encoding cost across many query points.</p>
<p>We train our SDF regression model on a subset of 2.4 million manifold meshes from our dataset, and add Gaussian noise with $\sigma=0.005$ to the point clouds as data augmentation. We train the model $f_{\theta}(x)$ to predict the SDF $y$ with a weighted L1 objective:</p>
<p>Here, we define the SDF such that points outside of the surface have negative sign. Therefore, in the face of uncertainty, the model is encouraged predict that points are inside the surface. We found this to be helpful in initial experiments, likely because it helps prevent the resulting meshes from effectively ignoring thin or noisy parts of the point cloud.</p>
<p>When producing meshes for evaluations, we use a grid size</p>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 7. Examples of point clouds (left) and corresponding extracted meshes (right). We find that our method often produces smooth meshes and removes outliers (middle row), but can sometimes miss thin/sparse parts of objects (bottom row).
of $128 \times 128 \times 128$, resulting in $128^{3}$ queries to the SDF model. In Figure 7, we show examples of input point clouds and corresponding output meshes from our model. We observe that our method works well in many cases, but sometimes fails to capture thin or sparse parts of a point cloud.</p>
<h2>D. Conditioning on DALL$\cdot$E 2 Samples</h2>
<p>In our main experiments, we use a specialized text-to-image model to produce in-distribution conditioning images for our point cloud models. In this section, we explore what happens if we use renders from a pre-existing text-to-image model, DALL$\cdot$E 2.</p>
<p>In Figure 8, we present three image-to-3D examples where the conditioning images are generated by DALL$\cdot$E 2. We find that DALL$\cdot$E 2 tends to include shadows under objects, and our point cloud model interprets these as a dark ground plane. We also find that our point cloud model can misinterpret shapes from the generated images when the objects take up too much of the image. In these cases, adding a border around the generated images can improve the reconstructed shapes.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 8. Examples of point clouds reconstructed from DALL-2 generations. The top image was produced using the prompt "a 3d rendering of an avocado chair, chair imitating an avocado, full view, white background". The middle image was produced using the prompt "a simple full view of a 3d rendering of a corgi in front of a white background". The bottom image is the same as the middle image, but with an additional white border.</p>
<h2>E. Pure Text-Conditional Generation</h2>
<p>In Section 5.1, we train a pure text-conditional point cloud model without an additional image generation step. While we find that this model performs worse on evaluations than our full system, it still achieves non-trivial results. In this section, we explore the capabilities and limitations of this model.</p>
<p>In Figure 9, we show examples where our text-conditional model is able to produce point clouds matching the provided text prompt. Notably, these examples include simple prompts that describe single objects. In Figure 10, we show examples where this model struggles with prompts that combine multiple concepts.</p>
<p>Finally, we expect that this model has inherited biases from our 3D dataset. We present one possible example of this in Figure 11, wherein the model produces longer and narrower objects for the prompt "a woman" than for the prompt "a man" when using a fixed diffusion noise seed.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 9. Selected point clouds generated by our pure textconditional 40 M parameter point cloud diffusion model.
<img alt="img-10.jpeg" src="img-10.jpeg" />
(a) Prompt: "a small red cube is sitting on top of a large blue cube. red on top, blue on bottom"
<img alt="img-11.jpeg" src="img-11.jpeg" />
(b) Prompt: "a corgi wearing a red santa hat"</p>
<p>Figure 10. Sample grids where our small, pure text-conditional model fails to understand complex prompts.
<img alt="img-12.jpeg" src="img-12.jpeg" />
(a) Prompt: "a man"
<img alt="img-13.jpeg" src="img-13.jpeg" />
(b) Prompt: "a woman"</p>
<p>Figure 11. Sample grids from our pure text-conditional 40M parameter model. Samples in the top grid use the same noise seed as the corresponding samples in the bottom grid.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/yanx27/Pointnet_ Pointnet2_pytorch&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4841 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4841</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4841</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-106.html">extraction-schema-106</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-221340643</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2008.11990v1.pdf" target="_blank">Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces</a></p>
                <p><strong>Paper Abstract:</strong> Recent research has proposed neural architectures for solving combinatorial problems in structured output spaces. In many such problems, there may exist multiple solutions for a given input, e.g. a partially filled Sudoku puzzle may have many completions satisfying all constraints. Further, we are often interested in finding {\em any one} of the possible solutions, without any preference between them. Existing approaches completely ignore this solution multiplicity. In this paper, we argue that being oblivious to the presence of multiple solutions can severely hamper their training ability. Our contribution is two fold. First, we formally define the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku. Second, we present a generic learning framework that adapts an existing prediction network for a combinatorial problem to handle solution multiplicity. Our framework uses a selection module, whose goal is to dynamically determine, for every input, the solution that is most effective for training the network parameters in any given learning iteration. We propose an RL based approach to jointly train the selection module with the prediction network. Experiments on three different domains, and using two different prediction networks, demonstrate that our framework significantly improves the accuracy in our setting, obtaining up to $21$ pt gain over the baselines.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4841.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4841.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLM-NQueens</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Logic Machine (applied to N-Queens)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Neural Logic Machines (NLMs) — a neural architecture that learns first-order logic style rules over grounded predicates — used here as the prediction network to complete partially-filled N-Queens boards (size-invariant training: trained on 10x10, tested on 11x11).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural logic machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Logic Machine (NLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>NLM: a relational/neural-logic architecture that instantiates learned first-order rules over grounded predicates represented as tensors. In this paper the NLM variant used for N-Queens had M=8 intermediate predicates and depth=30, max arity 2; unary and binary grounded predicates encode board cells and relations (same row/col/diagonal). Training used Adam; pretraining then RL fine-tuning when used with the selector.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>N-Queens (partial board completion)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Place remaining queens on an N×N chessboard given k < N pre-placed non-attacking queens so final board has N non-attacking queens — requires spatial relational reasoning (row/column/diagonal constraints).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>NLM used as the prediction network M_Θ. To handle multiple valid completions the paper evaluates: (a) MINLOSS (greedy choose target with minimum loss per example), and (b) SELECTR (an RL-trained selection module S_φ that probabilistically picks one target y∈Y_x for backpropagation; the selector's latent model G_φ is also an NLM for this task). SELECTR trains S_φ with policy gradients and reward equal to count of matching components between model prediction and selected target; joint updates alternate/parallel with M_Θ updates.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Architectural: NLM explicitly models unary/binary predicates over board cells (same-row/col/diagonal) and learns multi-step relational rules — designed for relational/spatial structure. Empirical: models are evaluated only as correct if full board satisfies spatial constraints (no partial credit). Performance improvements from SELECTR show the model learns to produce valid spatial configurations rather than memorizing single completions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test accuracy (mean over 3 runs ± std). OS (unique-solution queries): Naïve 70.59 ±0.09, MINLOSS 77.29 ±0.38, SELECTR 79.73 ±0.34. MS (multi-solution queries): Naïve 55.34 ±2.82, MINLOSS 77.22 ±1.28, SELECTR 79.68 ±1.35. Overall: Naïve 68.04 ±0.46, MINLOSS 77.28 ±0.48, SELECTR 79.72 ±0.46. (Values from Table 3.)</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Training on inputs with many possible completions is challenging; naive training (summing losses over solutions) performs poorly, especially on MS queries. NLM performance depends on proper handling of solution multiplicity; MINLOSS can get stuck in local optima, motivating SELECTR. Dataset used small numbers of solutions per query (2–6) for N-Queens; scalability beyond this regime not exhaustively tested.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SELECTR outperforms MINLOSS and Naïve baselines by several points on OS and very large margins on MS (e.g., Naïve MS 55.34% vs SELECTR 79.68%). The paper contrasts NLM-based predictors trained with/without the selection mechanism; SELECTR gives the best results.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4841.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4841.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLM-Futoshiki</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural Logic Machine (applied to Futoshiki)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>NLM used as prediction network for Futoshiki puzzles — fill an N×N grid with 1..N without row/column repeats and honoring pairwise inequality constraints — requiring spatial/relational reasoning over grid structure.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Neural logic machines</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Neural Logic Machine (NLM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same NLM family as for N-Queens; input encodes cell-value presence as unary predicates and inequality constraints as unary/binary relations. Trained size-invariantly (trained on 5×5, tested on 6×6). Latent selector G_φ for SELECTR again uses an NLM (depth=4, M=10).</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Futoshiki</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Fill an N×N grid with digits 1..N so rows/columns have no repeats and given adjacent inequality constraints (>, <) are satisfied — requires spatial (grid) relational reasoning and constraint propagation.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Prediction network is NLM (M_Θ). To handle multiple valid completions the paper evaluates MINLOSS and SELECTR; SELECTR's selector is an NLM that takes prediction (from a frozen copy of the predictor) and candidate targets to output a distribution over targets; policy-gradient RL trains selector with reward = count of matching cells.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>NLM explicitly models relations between cells (same row/column/box analogs) and inequality relations; evaluation counts a solution as correct only when all constraints are satisfied, demonstrating learned spatial constraint enforcement. Large performance gains of SELECTR on MS queries indicate learned reasoning across alternative valid completions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test accuracy (mean ± std). OS: Naïve 65.59 ±0.62, MINLOSS 76.78 ±0.81, SELECTR 78.01 ±0.70. MS: Naïve 14.99 ±2.17, MINLOSS 70.35 ±1.16, SELECTR 71.57 ±1.02. Overall: Naïve 52.96 ±0.96, MINLOSS 75.18 ±0.64, SELECTR 76.40 ±0.36.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Naïve training collapses on MS queries (very low MS accuracy ~15%) showing sensitivity to solution multiplicity. The training data generation enumerates all completions but MS per query is small (2–6) here; influence of much larger solution sets not tested for Futoshiki. Also requires upsampling MS examples due to dataset imbalance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SELECTR substantially outperforms Naïve and Random baselines and modestly improves over MINLOSS; MINLOSS itself already yields major gains over Naïve when multiplicity is addressed.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4841.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4841.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RRN-Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recurrent Relational Network (applied to Sudoku)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recurrent Relational Network (RRN) — a message-passing graph neural network — used as the prediction network to solve Sudoku puzzles; evaluated specifically on puzzles that intentionally have multiple valid completions to test handling of solution multiplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Recurrent relational networks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Recurrent Relational Network (RRN)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>RRN: graph neural network using message-passing; applied with architecture matching Palm et al. (2018) used for Sudoku: each of 81 cells is a node, edges connect row/column/box neighbors, 32 message-passing steps; input is 81×10 one-hot givens, output is per-cell distribution after last step. Prediction network pre-training took ~20–22 hours; RL fine-tuning ~10–12 hours on reported hardware.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Sudoku (partial grid completion)</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Fill 9×9 grid with digits 1..9 without repeating in rows/columns/3×3 boxes. The datasets include puzzles engineered to have multiple valid completions (by removing givens from known unique puzzles) to test multiplicity handling; solving requires spatial/relational propagation across the grid.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>RRN used as M_Θ predictor. To handle multiplicity: MINLOSS and SELECTR evaluated. In SELECTR for Sudoku the latent model G_φ inside the selector is a 5-layer CNN; the selector internal copy M_Θ_ provides intermediate predictions at each message-passing step and rewards are computed across steps. SELECTR uses RL (policy gradient) with reward equal to number of matching cells between predictor output and chosen target; up to 5 target solutions per training example were sampled (|Y_x| ≤ 5) due to explosion of solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>Architectural: RRN's message-passing explicitly propagates constraints between spatially-related cells (rows/cols/boxes) over multiple steps; evaluation requires full valid Sudoku solution. Empirical: pretrained RRN that was trained only on unique-solution puzzles achieved high OS accuracy but catastrophically low MS accuracy, indicating that RRNs learn spatial constraints but are sensitive to training signal when multiple completions exist. SELECTR improves MS accuracy strongly, indicating more robust spatial solution selection.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Test accuracy (mean ± std). OS: Naïve 87.85 ±0.84, MINLOSS 88.25 ±0.35, SELECTR 88.69 ±0.55. MS: Naïve 9.13 ±0.89, MINLOSS 76.93 ±1.50, SELECTR 81.73 ±2.00. Overall: Naïve 48.49 ±0.86, MINLOSS 82.59 ±0.62, SELECTR 85.21 ±0.76. Additionally, a pretrained state-of-the-art RRN from Palm et al. (trained on unique-solution instances) achieved 94.32% on unique instances but dropped to 24.48% on the multi-solution subset (reported in text).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>When trained/tested on instances with multiple valid completions, models trained ignoring multiplicity fail badly (Naïve MS ~9%); RRNs trained only on unique-solution instances overfit and perform poorly on multi-solution queries. Training used at most 5 sampled solutions per multi-solution Sudoku example (|Y_x| ≤ 5), so selector had incomplete target sets for many puzzles; puzzles with >50 solutions were filtered out, so results do not reflect behavior on extremely under-constrained instances. Model training is compute/time intensive (pretraining ~20–22 hours).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SELECTR yields large improvements over Naïve and substantial improvements over MINLOSS on multi-solution Sudoku (MS: MINLOSS 76.93% vs SELECTR 81.73%). Pretrained RRN trained only on unique puzzles performed well on unique instances but poorly on multi-solution puzzles, demonstrating the benefit of explicitly handling multiplicity with SELECTR.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4841.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4841.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SELECTR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SELECTR (RL-based selection module for one-of-many learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL-based learning framework that trains a selector neural module to pick, per training example, which valid target (out of many) should be used for backpropagation, jointly training the selector with the prediction network to handle solution multiplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>SELECTR (selection module S_φ + latent model G_φ; used with NLM or RRN predictors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>SELECTR: selection module S_φ is a policy network that takes (x_i, Y_{x_i}) and an internal copy of the predictor's parameters Θ_ to compute a distribution over candidate targets via a latent model G_φ (NLM for N-Queens/Futoshiki, CNN for Sudoku). The selected target is used to form a one-hot w_i that defines the training loss L_w; S_φ is trained with policy-gradient RL where reward = count of matching output components between predictor ŷ and chosen target. Training includes pretraining of predictor and selector and joint RL fine-tuning; copyitr controls frequency of copying Θ→Θ_.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Applied to N-Queens, Futoshiki, and Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>SELECTR is a generic training strategy applicable to combinatorial CSPs in structured output spaces (spatial puzzles in this paper) that have multiple valid completions per query.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Selector computes P_φ(y_ij) over Y_{x_i} using latent model G_φ and a past copy of predictor predictions ŷ_i_. It samples a targetȳ_i, sets w_i to select that target for the predictor loss, and receives reward R(ŷ_i,ȳ_i)=#matching components. Policy gradient updates φ (equation 7) and SGD updates Θ (equation 8); pretraining on unique-solution examples or using MINLOSS pretraining is used for stability.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>SELECTR itself is a meta-training strategy; evidence of improved spatial reasoning comes from empirical results: across all three spatial tasks, SELECTR raises accuracy on multi-solution queries dramatically (e.g., Sudoku MS from 9.13% Naïve to 81.73% SELECTR), indicating that jointly learning which target to train on leads to predictors that generate spatially valid full solutions even under multiplicity.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Applying SELECTR to NLM/RRN leads to best overall performance across tasks. Example numbers (mean test accuracies): N-Queens overall 79.72% ±0.46, Futoshiki overall 76.40% ±0.36, Sudoku overall 85.21% ±0.76. SELECTR consistently outperforms MINLOSS and Naïve baselines, especially on MS subsets (see per-task numbers in other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>SELECTR requires training an additional selector network and RL training, increasing complexity and time (selector pretraining + RL fine-tuning: hours per task). Reward is heuristic (count of matching cells) and may not capture global quality nuances; selector relies on a sampled subset of solutions (e.g., Sudoku used ≤5 targets per example), so incomplete target sets can limit optimality. Hyperparameters (copyitr, sampling ratios) and data imbalance need careful tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>SELECTR outperforms MINLOSS (greedy) across tasks and both outperform baselines that ignore multiplicity. Improvement magnitudes vary by task, but SELECTR gives up to ~21 percentage points gain over baselines in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4841.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4841.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models (LLMs or neural language models) being used to solve puzzle games that require spatial knowledge (such as Sudoku or other spatial reasoning tasks). Include details about the models, the puzzles, the mechanisms or strategies used, performance metrics, evidence of spatial reasoning, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MINLOSS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MINLOSS (greedy minimum-loss selection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A greedy training strategy that, at each iteration, selects for each example the target y in Y_x with minimum current loss relative to the model prediction, and backpropagates loss only for that chosen target.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MINLOSS (greedy per-example target selection)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Alternating optimization: for current Θ_t, choose w_{ij}=1 for y_{ij}=argmin_y l_Θ(ŷ_i,y) per example (Equation 4), then update Θ using SGD on the resulting weighted loss L_w. No explicit exploration mechanism — fully greedy.</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>Applied to N-Queens, Futoshiki, and Sudoku</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Used as a baseline strategy for training predictors on CSPs with multiple valid completions per query; aims to pick the nearest solution to current predictor and train towards it.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanism_or_strategy</strong></td>
                            <td>Greedy pick of per-example target with minimum current loss, then standard SGD update on that single target; repeats per iteration. No RL or explicit exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>evidence_of_spatial_reasoning</strong></td>
                            <td>MINLOSS improves over naive multiple-target-sum training for spatial puzzles (it avoids averaging conflicting gradients from multiple completions), resulting in predictors that produce valid spatial solutions in many cases; however it can get stuck in local optima where greedy local choices preclude reaching globally better hypotheses (illustrated with a logistic regression toy example and empirically by SELECTR outperforming MINLOSS).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>MINLOSS substantially improves over Naïve baselines. Example mean accuracies: N-Queens overall 77.28% ±0.48 (MS 77.22% ±1.28), Futoshiki overall 75.18% ±0.64 (MS 70.35% ±1.16), Sudoku overall 82.59% ±0.62 (MS 76.93% ±1.50).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Because MINLOSS greedily selects the closest target per example based on current Θ, it can become trapped in local minima and fail to explore alternative targets that would lead to better global solutions (demonstrated with a 1D logistic regression example). It lacks an exploration mechanism and so is consistently outperformed by the RL-based SELECTR.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>MINLOSS outperforms Naïve and Random baselines but is consistently outperformed by SELECTR which adds exploration via RL (SELECTR improves MS and overall accuracies over MINLOSS across tasks).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces', 'publication_date_yy_mm': '2020-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Neural logic machines <em>(Rating: 2)</em></li>
                <li>Recurrent relational networks <em>(Rating: 2)</em></li>
                <li>Can convolutional neural networks crack sudoku puzzles? <em>(Rating: 1)</em></li>
                <li>Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4841",
    "paper_id": "paper-221340643",
    "extraction_schema_id": "extraction-schema-106",
    "extracted_data": [
        {
            "name_short": "NLM-NQueens",
            "name_full": "Neural Logic Machine (applied to N-Queens)",
            "brief_description": "Neural Logic Machines (NLMs) — a neural architecture that learns first-order logic style rules over grounded predicates — used here as the prediction network to complete partially-filled N-Queens boards (size-invariant training: trained on 10x10, tested on 11x11).",
            "citation_title": "Neural logic machines",
            "mention_or_use": "use",
            "model_name": "Neural Logic Machine (NLM)",
            "model_description": "NLM: a relational/neural-logic architecture that instantiates learned first-order rules over grounded predicates represented as tensors. In this paper the NLM variant used for N-Queens had M=8 intermediate predicates and depth=30, max arity 2; unary and binary grounded predicates encode board cells and relations (same row/col/diagonal). Training used Adam; pretraining then RL fine-tuning when used with the selector.",
            "puzzle_name": "N-Queens (partial board completion)",
            "puzzle_description": "Place remaining queens on an N×N chessboard given k &lt; N pre-placed non-attacking queens so final board has N non-attacking queens — requires spatial relational reasoning (row/column/diagonal constraints).",
            "mechanism_or_strategy": "NLM used as the prediction network M_Θ. To handle multiple valid completions the paper evaluates: (a) MINLOSS (greedy choose target with minimum loss per example), and (b) SELECTR (an RL-trained selection module S_φ that probabilistically picks one target y∈Y_x for backpropagation; the selector's latent model G_φ is also an NLM for this task). SELECTR trains S_φ with policy gradients and reward equal to count of matching components between model prediction and selected target; joint updates alternate/parallel with M_Θ updates.",
            "evidence_of_spatial_reasoning": "Architectural: NLM explicitly models unary/binary predicates over board cells (same-row/col/diagonal) and learns multi-step relational rules — designed for relational/spatial structure. Empirical: models are evaluated only as correct if full board satisfies spatial constraints (no partial credit). Performance improvements from SELECTR show the model learns to produce valid spatial configurations rather than memorizing single completions.",
            "performance_metrics": "Test accuracy (mean over 3 runs ± std). OS (unique-solution queries): Naïve 70.59 ±0.09, MINLOSS 77.29 ±0.38, SELECTR 79.73 ±0.34. MS (multi-solution queries): Naïve 55.34 ±2.82, MINLOSS 77.22 ±1.28, SELECTR 79.68 ±1.35. Overall: Naïve 68.04 ±0.46, MINLOSS 77.28 ±0.48, SELECTR 79.72 ±0.46. (Values from Table 3.)",
            "limitations_or_failure_cases": "Training on inputs with many possible completions is challenging; naive training (summing losses over solutions) performs poorly, especially on MS queries. NLM performance depends on proper handling of solution multiplicity; MINLOSS can get stuck in local optima, motivating SELECTR. Dataset used small numbers of solutions per query (2–6) for N-Queens; scalability beyond this regime not exhaustively tested.",
            "comparison_baseline": "SELECTR outperforms MINLOSS and Naïve baselines by several points on OS and very large margins on MS (e.g., Naïve MS 55.34% vs SELECTR 79.68%). The paper contrasts NLM-based predictors trained with/without the selection mechanism; SELECTR gives the best results.",
            "uuid": "e4841.0",
            "source_info": {
                "paper_title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "NLM-Futoshiki",
            "name_full": "Neural Logic Machine (applied to Futoshiki)",
            "brief_description": "NLM used as prediction network for Futoshiki puzzles — fill an N×N grid with 1..N without row/column repeats and honoring pairwise inequality constraints — requiring spatial/relational reasoning over grid structure.",
            "citation_title": "Neural logic machines",
            "mention_or_use": "use",
            "model_name": "Neural Logic Machine (NLM)",
            "model_description": "Same NLM family as for N-Queens; input encodes cell-value presence as unary predicates and inequality constraints as unary/binary relations. Trained size-invariantly (trained on 5×5, tested on 6×6). Latent selector G_φ for SELECTR again uses an NLM (depth=4, M=10).",
            "puzzle_name": "Futoshiki",
            "puzzle_description": "Fill an N×N grid with digits 1..N so rows/columns have no repeats and given adjacent inequality constraints (&gt;, &lt;) are satisfied — requires spatial (grid) relational reasoning and constraint propagation.",
            "mechanism_or_strategy": "Prediction network is NLM (M_Θ). To handle multiple valid completions the paper evaluates MINLOSS and SELECTR; SELECTR's selector is an NLM that takes prediction (from a frozen copy of the predictor) and candidate targets to output a distribution over targets; policy-gradient RL trains selector with reward = count of matching cells.",
            "evidence_of_spatial_reasoning": "NLM explicitly models relations between cells (same row/column/box analogs) and inequality relations; evaluation counts a solution as correct only when all constraints are satisfied, demonstrating learned spatial constraint enforcement. Large performance gains of SELECTR on MS queries indicate learned reasoning across alternative valid completions.",
            "performance_metrics": "Test accuracy (mean ± std). OS: Naïve 65.59 ±0.62, MINLOSS 76.78 ±0.81, SELECTR 78.01 ±0.70. MS: Naïve 14.99 ±2.17, MINLOSS 70.35 ±1.16, SELECTR 71.57 ±1.02. Overall: Naïve 52.96 ±0.96, MINLOSS 75.18 ±0.64, SELECTR 76.40 ±0.36.",
            "limitations_or_failure_cases": "Naïve training collapses on MS queries (very low MS accuracy ~15%) showing sensitivity to solution multiplicity. The training data generation enumerates all completions but MS per query is small (2–6) here; influence of much larger solution sets not tested for Futoshiki. Also requires upsampling MS examples due to dataset imbalance.",
            "comparison_baseline": "SELECTR substantially outperforms Naïve and Random baselines and modestly improves over MINLOSS; MINLOSS itself already yields major gains over Naïve when multiplicity is addressed.",
            "uuid": "e4841.1",
            "source_info": {
                "paper_title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "RRN-Sudoku",
            "name_full": "Recurrent Relational Network (applied to Sudoku)",
            "brief_description": "Recurrent Relational Network (RRN) — a message-passing graph neural network — used as the prediction network to solve Sudoku puzzles; evaluated specifically on puzzles that intentionally have multiple valid completions to test handling of solution multiplicity.",
            "citation_title": "Recurrent relational networks",
            "mention_or_use": "use",
            "model_name": "Recurrent Relational Network (RRN)",
            "model_description": "RRN: graph neural network using message-passing; applied with architecture matching Palm et al. (2018) used for Sudoku: each of 81 cells is a node, edges connect row/column/box neighbors, 32 message-passing steps; input is 81×10 one-hot givens, output is per-cell distribution after last step. Prediction network pre-training took ~20–22 hours; RL fine-tuning ~10–12 hours on reported hardware.",
            "puzzle_name": "Sudoku (partial grid completion)",
            "puzzle_description": "Fill 9×9 grid with digits 1..9 without repeating in rows/columns/3×3 boxes. The datasets include puzzles engineered to have multiple valid completions (by removing givens from known unique puzzles) to test multiplicity handling; solving requires spatial/relational propagation across the grid.",
            "mechanism_or_strategy": "RRN used as M_Θ predictor. To handle multiplicity: MINLOSS and SELECTR evaluated. In SELECTR for Sudoku the latent model G_φ inside the selector is a 5-layer CNN; the selector internal copy M_Θ_ provides intermediate predictions at each message-passing step and rewards are computed across steps. SELECTR uses RL (policy gradient) with reward equal to number of matching cells between predictor output and chosen target; up to 5 target solutions per training example were sampled (|Y_x| ≤ 5) due to explosion of solutions.",
            "evidence_of_spatial_reasoning": "Architectural: RRN's message-passing explicitly propagates constraints between spatially-related cells (rows/cols/boxes) over multiple steps; evaluation requires full valid Sudoku solution. Empirical: pretrained RRN that was trained only on unique-solution puzzles achieved high OS accuracy but catastrophically low MS accuracy, indicating that RRNs learn spatial constraints but are sensitive to training signal when multiple completions exist. SELECTR improves MS accuracy strongly, indicating more robust spatial solution selection.",
            "performance_metrics": "Test accuracy (mean ± std). OS: Naïve 87.85 ±0.84, MINLOSS 88.25 ±0.35, SELECTR 88.69 ±0.55. MS: Naïve 9.13 ±0.89, MINLOSS 76.93 ±1.50, SELECTR 81.73 ±2.00. Overall: Naïve 48.49 ±0.86, MINLOSS 82.59 ±0.62, SELECTR 85.21 ±0.76. Additionally, a pretrained state-of-the-art RRN from Palm et al. (trained on unique-solution instances) achieved 94.32% on unique instances but dropped to 24.48% on the multi-solution subset (reported in text).",
            "limitations_or_failure_cases": "When trained/tested on instances with multiple valid completions, models trained ignoring multiplicity fail badly (Naïve MS ~9%); RRNs trained only on unique-solution instances overfit and perform poorly on multi-solution queries. Training used at most 5 sampled solutions per multi-solution Sudoku example (|Y_x| ≤ 5), so selector had incomplete target sets for many puzzles; puzzles with &gt;50 solutions were filtered out, so results do not reflect behavior on extremely under-constrained instances. Model training is compute/time intensive (pretraining ~20–22 hours).",
            "comparison_baseline": "SELECTR yields large improvements over Naïve and substantial improvements over MINLOSS on multi-solution Sudoku (MS: MINLOSS 76.93% vs SELECTR 81.73%). Pretrained RRN trained only on unique puzzles performed well on unique instances but poorly on multi-solution puzzles, demonstrating the benefit of explicitly handling multiplicity with SELECTR.",
            "uuid": "e4841.2",
            "source_info": {
                "paper_title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "SELECTR",
            "name_full": "SELECTR (RL-based selection module for one-of-many learning)",
            "brief_description": "An RL-based learning framework that trains a selector neural module to pick, per training example, which valid target (out of many) should be used for backpropagation, jointly training the selector with the prediction network to handle solution multiplicity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "SELECTR (selection module S_φ + latent model G_φ; used with NLM or RRN predictors)",
            "model_description": "SELECTR: selection module S_φ is a policy network that takes (x_i, Y_{x_i}) and an internal copy of the predictor's parameters Θ_ to compute a distribution over candidate targets via a latent model G_φ (NLM for N-Queens/Futoshiki, CNN for Sudoku). The selected target is used to form a one-hot w_i that defines the training loss L_w; S_φ is trained with policy-gradient RL where reward = count of matching output components between predictor ŷ and chosen target. Training includes pretraining of predictor and selector and joint RL fine-tuning; copyitr controls frequency of copying Θ→Θ_.",
            "puzzle_name": "Applied to N-Queens, Futoshiki, and Sudoku",
            "puzzle_description": "SELECTR is a generic training strategy applicable to combinatorial CSPs in structured output spaces (spatial puzzles in this paper) that have multiple valid completions per query.",
            "mechanism_or_strategy": "Selector computes P_φ(y_ij) over Y_{x_i} using latent model G_φ and a past copy of predictor predictions ŷ_i_. It samples a targetȳ_i, sets w_i to select that target for the predictor loss, and receives reward R(ŷ_i,ȳ_i)=#matching components. Policy gradient updates φ (equation 7) and SGD updates Θ (equation 8); pretraining on unique-solution examples or using MINLOSS pretraining is used for stability.",
            "evidence_of_spatial_reasoning": "SELECTR itself is a meta-training strategy; evidence of improved spatial reasoning comes from empirical results: across all three spatial tasks, SELECTR raises accuracy on multi-solution queries dramatically (e.g., Sudoku MS from 9.13% Naïve to 81.73% SELECTR), indicating that jointly learning which target to train on leads to predictors that generate spatially valid full solutions even under multiplicity.",
            "performance_metrics": "Applying SELECTR to NLM/RRN leads to best overall performance across tasks. Example numbers (mean test accuracies): N-Queens overall 79.72% ±0.46, Futoshiki overall 76.40% ±0.36, Sudoku overall 85.21% ±0.76. SELECTR consistently outperforms MINLOSS and Naïve baselines, especially on MS subsets (see per-task numbers in other entries).",
            "limitations_or_failure_cases": "SELECTR requires training an additional selector network and RL training, increasing complexity and time (selector pretraining + RL fine-tuning: hours per task). Reward is heuristic (count of matching cells) and may not capture global quality nuances; selector relies on a sampled subset of solutions (e.g., Sudoku used ≤5 targets per example), so incomplete target sets can limit optimality. Hyperparameters (copyitr, sampling ratios) and data imbalance need careful tuning.",
            "comparison_baseline": "SELECTR outperforms MINLOSS (greedy) across tasks and both outperform baselines that ignore multiplicity. Improvement magnitudes vary by task, but SELECTR gives up to ~21 percentage points gain over baselines in reported experiments.",
            "uuid": "e4841.3",
            "source_info": {
                "paper_title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
                "publication_date_yy_mm": "2020-08"
            }
        },
        {
            "name_short": "MINLOSS",
            "name_full": "MINLOSS (greedy minimum-loss selection)",
            "brief_description": "A greedy training strategy that, at each iteration, selects for each example the target y in Y_x with minimum current loss relative to the model prediction, and backpropagates loss only for that chosen target.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "MINLOSS (greedy per-example target selection)",
            "model_description": "Alternating optimization: for current Θ_t, choose w_{ij}=1 for y_{ij}=argmin_y l_Θ(ŷ_i,y) per example (Equation 4), then update Θ using SGD on the resulting weighted loss L_w. No explicit exploration mechanism — fully greedy.",
            "puzzle_name": "Applied to N-Queens, Futoshiki, and Sudoku",
            "puzzle_description": "Used as a baseline strategy for training predictors on CSPs with multiple valid completions per query; aims to pick the nearest solution to current predictor and train towards it.",
            "mechanism_or_strategy": "Greedy pick of per-example target with minimum current loss, then standard SGD update on that single target; repeats per iteration. No RL or explicit exploration.",
            "evidence_of_spatial_reasoning": "MINLOSS improves over naive multiple-target-sum training for spatial puzzles (it avoids averaging conflicting gradients from multiple completions), resulting in predictors that produce valid spatial solutions in many cases; however it can get stuck in local optima where greedy local choices preclude reaching globally better hypotheses (illustrated with a logistic regression toy example and empirically by SELECTR outperforming MINLOSS).",
            "performance_metrics": "MINLOSS substantially improves over Naïve baselines. Example mean accuracies: N-Queens overall 77.28% ±0.48 (MS 77.22% ±1.28), Futoshiki overall 75.18% ±0.64 (MS 70.35% ±1.16), Sudoku overall 82.59% ±0.62 (MS 76.93% ±1.50).",
            "limitations_or_failure_cases": "Because MINLOSS greedily selects the closest target per example based on current Θ, it can become trapped in local minima and fail to explore alternative targets that would lead to better global solutions (demonstrated with a 1D logistic regression example). It lacks an exploration mechanism and so is consistently outperformed by the RL-based SELECTR.",
            "comparison_baseline": "MINLOSS outperforms Naïve and Random baselines but is consistently outperformed by SELECTR which adds exploration via RL (SELECTR improves MS and overall accuracies over MINLOSS across tasks).",
            "uuid": "e4841.4",
            "source_info": {
                "paper_title": "Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces",
                "publication_date_yy_mm": "2020-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Neural logic machines",
            "rating": 2,
            "sanitized_title": "neural_logic_machines"
        },
        {
            "paper_title": "Recurrent relational networks",
            "rating": 2,
            "sanitized_title": "recurrent_relational_networks"
        },
        {
            "paper_title": "Can convolutional neural networks crack sudoku puzzles?",
            "rating": 1,
            "sanitized_title": "can_convolutional_neural_networks_crack_sudoku_puzzles"
        },
        {
            "paper_title": "Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver",
            "rating": 1,
            "sanitized_title": "satnet_bridging_deep_learning_and_logical_reasoning_using_a_differentiable_satisfiability_solver"
        }
    ],
    "cost": 0.016734,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces</p>
<p>Yatin Nandwani yatin.nandwani@cse.iitd.ac.in 
Department of Computer Science and Engineering
Indian Institute of Technology Delhi</p>
<p>Deepanshu Jindal deepanshu.jindal.cs116@cse.iitd.ac.in 
Department of Computer Science and Engineering
Indian Institute of Technology Delhi</p>
<p>MausamParag Singla parags@cse.iitd.ac.in 
Department of Computer Science and Engineering
Indian Institute of Technology Delhi</p>
<p>Neural Learning of One-of-Many Solutions for Combinatorial Problems in Structured Output Spaces</p>
<p>Recent research has proposed neural architectures for solving combinatorial problems in structured output spaces. In many such problems, there may exist multiple solutions for a given input, e.g. a partially filled Sudoku puzzle may have many completions satisfying all constraints. Further, we are often interested in finding any one of the possible solutions, without any preference between them. Existing approaches completely ignore this solution multiplicity. In this paper, we argue that being oblivious to the presence of multiple solutions can severely hamper their training ability. Our contribution is two fold. First, we formally define the task of learning one-of-many solutions for combinatorial problems in structured output spaces, which is applicable for solving several problems of interest such as N-Queens, and Sudoku. Second, we present a generic learning framework that adapts an existing prediction network for a combinatorial problem to handle solution multiplicity. Our framework uses a selection module, whose goal is to dynamically determine, for every input, the solution that is most effective for training the network parameters in any given learning iteration. We propose an RL based approach to jointly train the selection module with the prediction network. Experiments on three different domains, and using two different prediction networks, demonstrate that our framework significantly improves the accuracy in our setting, obtaining up to 21 pt gain over the baselines. * Equal Contribution Preprint. Under review.</p>
<p>Introduction</p>
<p>Neural networks have become the de-facto standard for solving perceptual tasks over low level representations, such as pixels in an image or audio signals. Recent research has also explored their application for solving symbolic reasoning tasks, requiring higher level inferences, such as neural theorem proving [Rocktäschel et al., 2015, Evans and Grefenstette, 2018, Minervini et al., 2020, and playing blocks world [Dong et al., 2019]. The advantage of neural models for these tasks is that it will create a unified, end-to-end trainable representation for integrated AI systems that combine perceptual and high level reasoning. Our paper focuses on one such high level reasoning tasksolving combinatorial problems in structured output spaces, e.g., solving a Sudoku or N-Queens puzzle. These can be thought of as Constraint Satisfaction problems (CSPs) where the underlying constraints are not explicitly available, and need to be learned from training data.</p>
<p>One of the key characteristics of such problems is solution multiplicity -there could be many correct solutions for any given input, even though we may be interested in finding any one of these solutions. For example, in a game of Sudoku with only 16 digits filled, there are always multiple correct solutions [McGuire et al., 2012], and obtaining any one of them suffices for solving Sudoku. Unfortunately, existing literature has completely ignored solution multiplicity, resulting in sub-optimally trained networks. Our preliminary analysis of a state-of-the-art neural Sudoku solver [Palm et al., 2018] 2 , which trains and tests on instances with single solutions, showed that it achieves a high accuracy of 96% on instances with single solution, but the accuracy drops to less than 25%, when tested on inputs that have multiple solutions. Intuitively, the challenge comes from the fact that (a) there could be a very large number of possible solutions for a given input, and (b) the solutions may be highly varied. For example, a 16-givens Sudoku puzzle could have as many as 10,000 solutions, with maximum hamming distance between any two solutions being 61. Hence, we argue that an explicit modeling effort is required to represent this solution multiplicity.</p>
<p>As the first contribution of our work, we formally define the important and novel problem of Oneof-Many Learning (1oML). It is given training data of the form {(x i , Y x i )}, where Y x i denotes a subset of all correct outputs Y x i associated with input x i . The goal of 1oML is to learn a function f such that, for any input x, f (x) = y for some y ∈ Y x . We show that a naïve strategy that uses separate loss terms for each (x i , y ij ) pair where y ij ∈ Y x i can result in a bad likelihood objective. In response, we present our first-cut approach, MINLOSS, which picks up the single y ij closest to the predictionŷ i based on the current parameters of prediction network (base architecture for function f ), and uses it to compute and back-propagate the loss for that training sample x i . Though significantly better than naïve training, through a simple example, we demonstrate that MINLOSS can be sub-optimal in certain scenarios, due to its inability to pick a y ij based on global characteristics of the solution space.</p>
<p>As our second contribution, we present an RL based learning framework SELECTR, which uses a selection module to decide which y ij should be picked for a given input x i , for back-propagating the loss in the next iteration. Our technique is generic in the sense that it can work with any prediction network for the given problem. Our selection module is trained jointly along with the prediction network using reinforcement learning, where the RL reward is specified in the form of the quality of the final prediction. This allows us to trade-off exploration and exploitation in selecting the optimum y ij by learning a probability distribution over the space of possible y ij s for any given input x i .</p>
<p>We experiment on three CSPs: N-Queens, Futoshiki, and Sudoku. Our prediction networks for the first two problems are constructed using Neural Logic Machines [Dong et al., 2019], and for Sudoku, we use a state-of-the-art neural solver based on Recurrent Relational Networks [Palm et al., 2018]. In all three problems, our experiments demonstrate that SELECTR vastly outperforms naïve baselines by up to 21 pts, underscoring the value of explicitly modeling solution multiplicity. SELECTR also consistently improves on our first cut approach of MINLOSS across all datasets.</p>
<p>Background and Related Work</p>
<p>Related ML Models: There are a few learning scenarios within weak supervision which may appear similar to the setting of 1oML, but are actually different from it. We first discuss them briefly.</p>
<p>'Partial Label Learning' (PLL) [Cabannes et al., 2020, Xu et al., 2019, Feng and An, 2019, Cour et al., 2011, Jin and Ghahramani, 2002 involves learning from the training data where, for each input, a noisy set of candidate labels is given amongst which only one label is correct. This is different from 1oML in which there is no training noise and all the solutions in the solution set Y x for a given x are correct. Though some of the recent approaches to tackle ambiguity in PLL [Cabannes et al., 2020] may be similar to our methods (i.e., MINLOSS ), by the way of deciding which solution in the target set should be picked next for training, the motivations are quite different. In PLL, the objective is to select the correct label out of many incorrect ones to reduce training noise, whereas in 1oML, selecting only one label for training provably improves the learnability and there is no question of reducing noise as all the labels are correct. Further, most of the previous work on PLL considers classification over a discrete output space with, say, L labels, where as in 1oML, we work with structured output spaces, e.g., an r dimensional vector space where each dimension represents a discrete space of L labels. This exponentially increases the size of the output space, making it intractable to enumerate all possible solutions as is typically done in existing approaches for PLL [Jin and Ghahramani, 2002].</p>
<p>Within weak supervision, there is also some work in 'Multi Instance Learning' (MIL) approach for Relation Extraction (RE) which employs a selection module to pick a set of sentences to be used for training a relation classifier, given a set of noisy relation labels [Feng et al., 2018, Qin et al., 2018. This is clearly different from us where multiplicity is associated with any given input, not with a class (relation).</p>
<p>Other than weak supervision, 1oML should also not be confused with the problems in the space of multi-label learning [Tsoumakas and Katakis, 2007]. While in multi-label learning, given a solution set Y x for each input x, the goal is to learn to correctly predict each possible solution in the set Y x for x. Typically, a classifier is learned for each of the possible labels (solutions) separately. On the other hand, in 1oML, the objective is to learn any one of the correct solutions for a given input, and a single classifier is learned. The characteristics of the two problems are quite different, and hence, also the solution approaches. As we show later, the two settings lead to requirements for two different kinds of generalization losses.</p>
<p>Solution Multiplicity in Other Settings: There is some prior work related to our problem of solution multiplicity, albeit in different settings. An example is the task of video-prediction, where there can be multiple next frames (y ij ) for a given partial video x i [Henaff et al., 2017, Denton andFergus, 2018]. The multiplicity of solutions here arises from the underlying uncertainty rather than as a inherent characteristic of the domain itself. Current approaches model the final prediction as a combination of the deterministic part oblivious to uncertainty, and a non-determinstic part caused by uncertainty.</p>
<p>There is no such separation in our case since each solution is inherently different from others.</p>
<p>Another line of work, which comes close to ours is the task of Neural Program Synthesis [Devlin et al., 2017, Bunel et al., 2018. Given a set of Input-Output (IO) pairs, the goal is to generate a valid program conforming to the IO specifications. For a given IO pair, there could be multiple valid programs, and often, training data may only have one (or a few) of them. Bunel et al. [2018] propose a solution where they define an alternate RL based loss using the correctness of the generated program on a subset of held out IO pairs as reward. In our setting, there is no such additional signal available for training outside the subset of targets Y x for an input x.</p>
<p>It would also be worthwhile to mention other tasks such as Neural Machine translation [Bahdanau et al., 2015, Sutskever et al., 2014, Summarization [Nallapati et al., 2017, Paulus et al., 2018, Image Captioning [Vinyals et al., 2017, You et al., 2016 etc., where one would expect to have multiple valid solutions for any given input. E.g., for a given sentence in language A, there could be multiple valid translations in language B. To the best of our knowledge, existing literature ignores solution multiplicity in such problems, and simply trains on all possible given labels for any given input.</p>
<p>Models for Symbolic Reasoning: Our work follows the line of recent research, which proposes neural architectures for implicit symbolic and relational reasoning problems [Santoro et al., 2018, Palm et al., 2018, Dong et al., 2019. We experiment with two architectures as base prediction networks: Neural Logic Machines (NLMs) [Dong et al., 2019], and Recurrent Relational Networks (RRNs) [Palm et al., 2018]. NLMs allow learning of first-order logic rules expressed as Horn Clauses over a set of predicates, making them amenable to transfer over different domain sizes. The rules are instantiated over a given set of objects, where the groundings are represented as tensors in the neural space over which logical rules operate. RRNs use a graph neural network to learn relationships between symbols represented as nodes in the graph, and have been shown to be good at problems that require multiple steps of symbolic reasoning.</p>
<p>Theory and Algorithm</p>
<p>Problem Definition</p>
<p>Notation: Each possible solution (target) for an input (query) x is denoted by an r-dimensional vector y ∈ V r , where each element of y takes values from a discrete space denoted by V. Let Y = V r , and let Y x denote the set of all solutions associated with input x. We will use the term solution multiplicity to refer to the fact that there could be multiple possible solutions y for a given input x. In our setting, the solutions in Y x span a structured combinatorial subspace of V r , and can be thought of as representing solutions to an underlying Constraint Satisfaction Problem (CSP). For example in N-Queens, x would denote a partially filled board, and y denote a solution for the input board.</p>
<p>Given a set of inputs x i along with a subset of associated solutions Y x i ⊆ Y x i , i.e., given a set of (x i , Y x i ) pairs, we are interested in learning a mapping from x to any one y among many possible solutions for x. Formally, we define the One-of-Many-Learning (1oML) problem as follows.
Definition 1. Given training data D of the form, {(x i , Y x i )} m i=1 ,
where Y x i denotes a subset of solutions associated with input x i , and m is the size of training dataset, One-of-Many-Learning (1oML) is defined as the problem of learning a function f such that, for any input x, f (x) = y for some y ∈ Y x , where Y x is the set of all solutions associated with x.</p>
<p>We will be using parameterized neural networks to represent our mapping function. We will use M Θ to denote a network (model) M with associated set of parameters Θ. We will useŷ i (ŷ) to denote the network output corresponding to input x i (respectively, x). We are interested in finding a Θ * that solves the 1oML problem as defined above. Next, we consider various formulations for the same.</p>
<p>Objective Function</p>
<p>Naïve Objective: In the absence of solution multiplicity, i.e. when target set Y x i = {y i }, ∀i, the standard method to train such models is to minimize the total loss,
L(Θ) = m i=1 l Θ (ŷ i , y i ), where l Θ (ŷ i , y i )
is the loss between the predictionŷ i and the unique target y i for the input x i . We find the optimal Θ * as argmin Θ L(Θ). A Naïve extension of this for 1oML would be to sum the loss over all targets in Y x , i.e., minimize the following loss function:
L(Θ) = 1 m m i=1 y ij ∈Yx i l Θ (ŷ i , y ij )(1)
We observe that loss function in Equation 1 would unnecessarily penalize the model when dealing with solution multiplicity. Even when it is correctly predicting one of the targets for an input x i , the loss with respect to the other targets in Y x i could be rather high, hence misguiding the training process. Example 1 below demonstrates such a case. For illustration, we will use the cross-entropy 
loss, i.e., l Θ (ŷ, y) = − k l 1{y[k] = v l } log(P (ŷ[k] = v l )), where v l ∈ V{x i , Y x i },
the Naïve objective (with l Θ as cross entropy) will be minimized, when P (ŷ i [k] = 0) = P (ŷ i [k] = 1) = 0.5, for k ∈ {1, 2}, ∀i, which can not recover either of the desired solutions: (0, 1) or (1, 0).</p>
<p>The problem arises from the fact that when dealing with 1oML, the training loss defined in Equation 1 is no longer a consistent predictor of the generalization error as formalized below. Lemma 1. The training loss L(Θ) as defined in eq. (1) is an inconsistent estimator of generalization error for 1oML, when l Θ is a zero-one loss, i.e., l Θ (ŷ i , y ij ) = 1{ŷ i = y ij }.</p>
<p>Proof. Let D represent the distribution using which samples (x, Y x ) are generated. In our setting, generalization error ε(M Θ ) for a prediction network M Θ can be written as:
ε(M Θ ) = E (x,Yx)∼D (1{ŷ / ∈ Y x }), whereŷ = M Θ (x), i.e.
the prediction of the network on unseen example sampled from the underlying data distribution. Assume a scenario when Y x i = Y x i , ∀i, i.e., for each input x i all the corresponding solutions are present in the training data. Then, an unbiased estimatorε D (M Θ ) of the generalization error, computed using the training data is written as:
ε D (M Θ ) = 1 m m i=1 1{ŷ i / ∈ Y x i }.
Clearly, the estimator obtained using L(Θ) (Naïve Objective), when the loss function l Θ (ŷ i , y ij ) is replaced by a zero-one loss 1{ŷ i = y ij }, is not a consistent estimator for the generalization error.</p>
<p>This can be easily seen by considering a case whenŷ i ∈ Y x i and |Y x i | &gt; 1.</p>
<p>New Objective: We now motivate a better objective function based on the unbiased estimator described above. In general, we would like M Θ to learn a conditional probability distribution P r(y|x i ; Θ) over the output space Y such that the entire probability mass is concentrated on the desired solution set Y x i , i.e., y ij ∈Yx i P r(y ij |x i ; Θ) = 1, ∀i. If such a conditional distribution is learnt, then we can easily sample a y ij ∈ Y x i from it. However, ours being a structured output space, it is intractable to represent all possible joint distributions over the possible solutions in Y x i . Hence, we instead design a loss function which forces the model to learn a distribution in which the probability mass is concentrated on any one of the targets y ij ∈ Y x i . We call such distributions as one-hot. To do this, we introduce |Y x i | number of new learnable Boolean parameters, w i , for each query x i in the training data, and correspondingly define the following loss function:
L w (Θ, w) = m i=1 y ij ∈Yx i w ij l Θ (ŷ i , y ij )(2)
Here, w ij ∈ {0, 1} and j w ij = 1, ∀i, where j indices over solutions y ij ∈ Y x i . The last constraint over Boolean variables w ij enforces that exactly one of the weights in w i is 1 and all others are zero.
Lemma 2. Under the assumption Y x i = Y x i , ∀i, the loss L (Θ) = min w L w (Θ, w), defined as the minimum value of L w (Θ, w) (defined in eq.
(2)) with respect to w, is a consistent estimator of generalization error for 1oML, when l Θ is a zero-one loss, i.e., l Θ (ŷ i , y ij ) = 1{ŷ i = y ij }.</p>
<p>Proof of Lemma 2 follows from definition of loss in eq. (2). We define our new objective as:
min Θ,w L w (Θ, w) s.t. w ij ∈ {0, 1} ∀i, ∀j and |Yx i | j=1 w ij = 1, ∀i = 1 . . . m(3)</p>
<p>Greedy Formulation: MINLOSS</p>
<p>In this section, we present one possible way to optimize our desired objective min Θ,w L w (Θ, w). It alternates between optimizing over the Θ parameters, and optimizing over w parameters. While Θ parameters are optimized using SGD, the weights w are selected greedily for a given Θ = Θ t at each iteration, i.e., it assigns a non-zero weight to the solution corresponding to the minimum loss amongst all the possible y ij ∈ Y x i for each i = 1 . . . m:
w (t) ij = 1 y ij = argmin y∈Yx i l Θ (t) ŷ (t) i , y , ∀i = 1 . . . m(4)
This can be done by computing the loss with respect to each target, and picking the one which has the minimum loss. We refer to this approach as MINLOSS. Intuitively, for a given set of Θ (t) parameters, MINLOSS greedily picks the weight vector w i (t) , and uses them to get the next set of Θ (t+1) parameters using SGD update.
Θ (t+1) ← Θ (t) − α Θ ∇ Θ L w (Θ, w) | Θ=Θ (t) ,w=w (t)(5)
One significant challenge with MINLOSS is the fact that it chooses the current set of w parameters independently for each example based on current Θ values. While this way of picking the w parameters is optimal if Θ has reached the optima, i.e. Θ = Θ * , it can lead to sub-optimal choices when both Θ and w are being simultaneously trained. Following example illustrates this. Example 2. Consider a simple task with a one-dimensional continuous input space X ⊂ R, and target space Y = {0, 1}. Consider learning with 10 examples, given as (
x = 1, Y x = {1}) (5 examples), (x = −1, Y x = {0, 1}) (4 examples), (x = −2, Y x = {1}) (1 example). The optimal
decision hypothesis is given as: y = 1{x &gt; α}, for α ≤ −2, or y = 1{x &lt; β}, for β ≥ 1.</p>
<p>Assume learning this with logistic regression using MINLOSS as the training algorithm optimizing the objective in eq. (3). If we initialize the parameters of logistic such that the starting hypothesis is given by y = 1{x &gt; 0} (logistic parameters: θ 1 = 0.1, θ 0 = 0), MINLOSS will greedily pick the target y = 0 for samples with x = −1, repeatedly. This will result in the learning algorithm converging to the decision hypothesis y = 1{x &gt; −0.55}, which is sub-optimal since the input with x = −2 is incorrectly classified (see supplement for a detailed illustration).</p>
<p>In the above example, MINLOSS is not able to achieve the optimum since it greedily picks the target for each query x i based on current set of parameters and gets stuck in local mimima. This will be addressed in the next section.</p>
<p>Reinforcement Learning Formulation: SELECTR</p>
<p>In this section, we will design a training algorithm that fixes some of the issues observed with MINLOSS. Considering the Example 2 above, the main problem with MINLOSS is its inability to consider alternate targets which may not be greedily optimal at the current set of parameters. A better strategy will try to explore alternative solutions as a way of reaching better optima, e.g., in example 2 we could pick, for the input x = −1, the target y = 1 with some nonzero probability, to come out of the local optima. In the above case, this also happens to be the globally optimal strategy. This is the key motivation for our RL-based strategy proposed below.</p>
<p>x,Y x</p>
<p>Selection Module Figure 1: Flow-diagram for our RL Framework A natural questions arises: how should we assign the probability of picking a particular target? We note that the amount of exploration required may depend in complex ways on the global solution landscape, as well as the current set of parameters. Therefore, we propose a strategy, which makes use of a separate selection module (a neural network), which takes as input, the current example (x i , Y x i ), as well as the current set of parameters Θ (or a past copy Θ_ to introduce stability), and outputs the probability of picking each target for training Θ in the next iteration. Our strategy is RL-based since, we can think of choosing each target (for a given input) as an action that our selection module needs to take. Our selection module is trained using a reward that captures the quality of selecting the corresponding target for training the prediction network. We next describe the details of our approach.
x Y x Expected Reward x Loss w Y x Y x
Selection Module (S φ ): This is an RL agent or a policy network where the action is to select a target, y ij ∈ Y x i , for each x i . Given a training sample, (x i , Y x i ), it first internally predictŝ y i _ = M Θ_ (x i ), using a past copy of the parameters Θ_. This prediction is then fed as an input along with the target set, Y x i , to a latent model, G φ , which outputs a probability distribution P r φ (y ij ), ∀y ij ∈ Y x i , s.t. y ij P r φ (y ij ) = 1. S φ then picks a targetȳ i ∈ Y x i based on the distribution P r φ (y ij ) and returns aw i such that ∀i,w ij = 1 if y ij =ȳ i , andw ij = 0 otherwise.</p>
<p>Update of φ Parameters: Since we do not know a-priori whichȳ i ∈ Y x i is optimal for defining the loss in training of M Θ , we train S φ using a reward which is defined to be the count ofŷ i (i.e. prediction of M Θ on x i ) components which match the selected targetȳ i , i.e.,
R(ŷ i ,ȳ i ) = r k=1 1{ŷ i [k] =ȳ i [k]}.
The expected reward for RL can then be written as:
R(φ) = m i=1 y ij ∈Yx i P r φ (y ij ) R (ŷ i , y ij )(6)
We make use of policy gradient to compute the derivative of the expected reward with respect to the φ parameters. Accordingly, update equation for φ can be written as:
φ (t+1) ← φ (t) + α φ ∇ φ R (φ) | φ=φ (t)(7)
Update of Θ Parameters: Next step is to use the output of the selection module,w i corresponding to the sampled targetȳ i , ∀i, to train the M Θ network. The update equation for updating the Θ parameters during next learning iteration can be written as:
Θ (t+1) ← Θ (t) − α Θ ∇ Θ L w (Θ, w) | Θ=Θ (t) ,w=w (t)(8)
Instead of backpropagating the loss gradient at a sampled targetȳ i , one could also backpropagate the gradient of the expected loss given the distribution P r φ (y ij ). Figure 1 represents the overall framework. In the diagram, gradients for updating Θ flow back through the red line and gradients for updating φ flow back through the green line.</p>
<p>Training Algorithm</p>
<p>We put all the update equations together and present the key components of our training algorithm below. For a detailed pseudocode, we refer to Algorithm 1 in the supplement.</p>
<p>Pre-training: It is a common strategy in many RL based approaches to first pre-train the network weights using a simple strategy. Accordingly, we pre-train both the M Θ and S φ networks before going into joint training. In our experiments, we observe that in some cases, pre-training M Θ using only those samples from training data D for which there is only a unique solution, i.e.,
{(x i , Y x i ) ∈ D s.t. |Y x i | = 1}
gives better performance than pre-training with MINLOSS. Therefore, we pre-train using both the approaches and select the better one based on their performance on a held out dev set. Once the prediction network is pre-trained, a copy of it is given to the selection module to initialize M Θ <em>. Keeping Θ and Θ</em> fixed and identical to each other, selection module S φ is pre-trained using the rewards given by the pre-trained M Θ and the internal predictions given by M Θ _.</p>
<p>Joint Training: After pre-training, both prediction network M Θ and selection module S φ are trained jointly. In each iteration t, selection module first computes the weights,w t i , for each sample in the mini-batch. The prediction network computes the predictionŷ t i and rewards R(ŷ t i , y ij ), ∀y ij ∈ Y x i . The parameters φ t and Θ t are updated simultaneously using eq. (7) and eq. (8), respectively. The copy of the prediction network within selection module, i.e., M Θ _ in S φ , is updated with the latest parameters Θ t after every copyitr updates where copyitr is a hyper-parameter.</p>
<p>Experiments</p>
<p>The main goal of our experiments is to evaluate MINLOSS and SELECTR, when compared to baseline approaches that completely disregard the problem of solution multiplicity. We also wish to assess the performance gap, if any, between queries with a unique solution and those with many possible solutions. To answer these questions, we conduct experiments on three different tasks (N-Queens, Futoshiki and Sudoku), which are trained over two different prediction networks, as described below. 3</p>
<p>Datasets and Prediction Networks</p>
<p>N-Queens: Given a query, i.e., a chess-board of size N × N and a placement of k &lt; N non-attacking queens on it, the task of N Queens is to place the remaining N − k queens, such that no two queens are attacking each other. We train a Neural Logic Machine (NLM) model [Dong et al., 2019] 4 as the prediction network M Θ for solving queries for this task. To model N-Queens within NLM, we represent a query x and the target y as N 2 dimensional Boolean vectors with 1 at locations where a Queen is placed. Accordingly, x has 1 on k locations and y on N locations. We use another smaller NLM architecture as the latent model G φ . We train our model on 10-Queens puzzles and test on 11-Queens puzzles, both with 5 placed queens. This size-invariance in training and test is a key strength of NLM architecture, which we exploit in our experiments. To generate the train data, we start with all possible valid 10-Queens board configurations and randomly mask any 5 queens, and then check for all possible valid completions to generate potentially multiple solutions for an input. Test data is also generated similarly. Training and testing on different board sizes ensures that no direct information leaks from test to train. Queries with multiple solutions have 2-6 solutions, so we choose Y
x i = Y x i , ∀x i .
Futoshiki: This is a logic puzzle in which we are given a grid of size N × N , and the goal is to fill the grid with digits from {1 . . . N } such that no digit is repeated in a row or a column. k out of N 2 positions are already filled in the input query x and the remaining N 2 − k positions need to be filled. Further, inequality constraints are specified between some pairs of adjacent grid positions, which need to be honored in the solution. Our prediction network, and latent model use NLM, and the details (described in supplementary) are very similar to that of N-Queens. Similar to N-Queens, we do size-invariant training -we train our models on 5 × 5 puzzles with 14 missing digits and test on 6 × 6 puzzles with 18 missing digits. Similar to N-Queens, we generate all possible valid grids and randomly mask out the requisite number of digits to generate train and test data. For both train and test queries we keep up to five inequality constraints of each type: &gt; and &lt;.</p>
<p>Sudoku:</p>
<p>We also experiment on Sudoku, which has been used as the task of choice for many recent neural reasoning works [Palm et al., 2018. We use Relational Recurrent Networks (RRN) [Palm et al., 2018] 5 as the prediction network since it has recently shown state-of-the-art performance on the task. We use a 5 layer CNN as our latent model G φ . Existing Sudoku datasets [Royle, 2014, Park, 2018, do not expose the issues with solution multiplicity. In response, we generate our own dataset by starting with a collection of Sudoku puzzles with unique solutions that have 17 digits filled. 6 We remove one of the digits, thus generating a puzzle, which is guaranteed to have solution multiplicity. We then randomly add 1 to 18 of the digits back from the solution of the original puzzle, while ensuring that the query continues to have more than 1 solution. 7 This generates our set of multi-solution queries with a uniform distribution of filled digits from 17 to 34. We mix an 3 Further details of software environments, hyperparameters and dataset generation are in the supplement. 4 Code taken from: https://github.com/google/neural-logic-machines (see supplement for architecture details) 5 Code taken from: https://github.com/dmlc/dgl/tree/master/examples/pytorch/rrn 6 Available at https://data.dgl.ai/dataset/sudoku-hard.zip 7 We identify all solutions to a puzzle using http://www.enjoysudoku.com/JSolve12.zip equal number of unique solution queries (with same filled distribution). Because some x i s may have hundreds of solutions, we randomly sample 5 of them from Y x i , i.e., |Y x i | ≤ 5 in the train set.</p>
<p>For each dataset, we generate a devset in a manner similar to the test set. </p>
<p>Baselines and Evaluation Metric</p>
<p>Our comparison baselines include: (1) Naïve: backpropagating L(Θ) through each solution independently using Equation (1) For all tasks, we consider the output of a prediction network as correct only if it is a valid solution for the underlying CSP. No partial credit is given for guessing parts of the output correctly. </p>
<p>Results and Discussion</p>
<p>SelectR MinLoss Unique</p>
<p>We report the accuracies across all tasks and models in Table 2. For each setting, we report the mean over three random runs (with different seeds), and also the accuracy on the best of these runs selected via the devset (in the parentheses). We first observe that Naïve and Random perform significantly worse than Unique in all the tasks, not only on MS, but on OS as well. This suggests that, 1oML models that explicitly handle solution multiplicity, even if by simply discarding multiple solutions, are much better than those that do not recognize it at all.</p>
<p>Predictably both MINLOSS and SELECTR vastly improve upon the performance of naïve baselines, with a dramatic 13-52 pt accuracy gains between Unique and SELECTR on queries with multiple solutions. Comparing MINLOSS and SE- LECTR, we find that our RL-based approach outperforms MINLOSS consistently across tasks. This highlights the value of our selector module on top of the greedy target selection of MINLOSS.</p>
<p>Recall that Sudoku training set has no more than 5 solutions for a query, irrespective of the actual number of solutions -i.e, for many x i , Y x i Y x i . Despite incomplete solution set, significant improvement over baselines is obtained, indicating that our formulation handles solution multiplicity even with incomplete information. Furthermore, the large variation in the size of solution set (|Y x |) in Sudoku allows us to assess its effect on the overall performance. We find that all models get worse as |Y x | increases (Figure 2), even though SELECTR remains the most robust.</p>
<p>Conclusion and Future Work</p>
<p>In this paper, we have defined 1oML: the task of learning one of many solutions for combinatorial problems in structured output spaces. We have identified solution multiplicity as an important aspect of the problem, which if not handled properly, may result in sub-optimal models. As a first cut solution, we proposed a greedy approach: MINLOSS formulation. We identified certain shortcomings with the greedy approach and proposed an RL based formulation, SELECTR, which overcomes some of the issues in MINLOSS by exploring the locally sub-optimal choices for better global optimization. Experiments on three different tasks using two different prediction networks demonstrate the effectiveness of our approach in training robust models under solution multiplicity. We will make all the code and the datasets publicly available.</p>
<p>It is interesting to note that for traditional CSP solvers, e.g. [Selman et al., 1993, Mahajan et al., 2004, a problem with many solutions will be considered an easy problem, whereas for neural models, such problems appear much harder (Figure 2). As a future work, it will be interesting to combine symbolic CSP solvers with SELECTR to design a much stronger neuro-symbolic reasoning model.</p>
<p>Supplementary Material 3 Theory and Algorithm</p>
<p>Objective Function</p>
<p>Lemma 3.2. Under the assumption Y x i = Y x i , ∀i, the loss L (Θ) = min w L w (Θ, w), defined as the minimum value of L w (Θ, w) (defined in eq. (2)) with respect to w, is a consistent estimator of generalization error for 1oML, when l Θ is a zero-one loss, i.e., l Θ (ŷ i , y ij ) = 1{ŷ i = y ij }.</p>
<p>Proof. Let D represent the distribution using which samples (x, Y x ) are generated. In our setting, generalization error ε(M Θ ) for a prediction network M Θ is:
ε(M Θ ) = E (x,Yx)∼D (1{ŷ / ∈ Y x }) whereŷ = M Θ (x), i.e.
the prediction of the network on unseen example sampled from the underlying data distribution. Assume a scenario when Y x i = Y x i , ∀i, i.e., for each input x i all the corresponding solutions are present in the training data. Then, an unbiased estimatorε D (M Θ ) of the generalization error, computed using the training data is written as:
ε D (M Θ ) = 1 m m i=1 1{ŷ i / ∈ Y x i } Now, consider the objective function L (Θ) = min w L w (Θ, w) = min w 1 m m i=1 y ij ∈Yx i w ij 1{ŷ i = y ij } = 1 m m i=1 min w i y ij ∈Yx i w ij 1{ŷ i = y ij } s.t. w ij ∈ {0, 1} ∀i, ∀j and |Yx i | j=1 w ij = 1, ∀i = 1 . . . m
For any x i , if the predictionŷ i is correct, i.e., ∃y ij * ∈ Y x i s.t.ŷ i = y ij * , then 1{ŷ i = y ij * } = 0 and 1{ŷ i = y ij } = 1, ∀y ij ∈ Y x i , y ij = y ij * . Now minimizing over w i ensures w ij * = 1 and w ij = 0 ∀y ij ∈ Y x i , y ij = y ij * . Thus, the contribution to the overall loss from this example x i is zero. On the other hand if the prediction is incorrect then 1{ŷ i = y ij } = 1, ∀y ij ∈ Y x i , thus making the loss from this example to be 1 irrespective of the choice of w i . As a result, L (Θ) is exactly equal toε D (M Θ ) and hence it is a consistent estimator for generalization error.</p>
<p>Note that in the main paper, there is a factor of 1/m missing in the definition of L w in Equation 2.</p>
<p>The correct definition is:
L w = 1 m m i=1 y ij ∈Yx i w ij l Θ (ŷ i , y ij ).</p>
<p>Greedy Formulation: MINLOSS</p>
<p>Example 3.2. Consider a simple task with a one-dimensional continuous input space X ⊂ R, and target space Y = {0, 1}. Consider learning with 10 examples, given as (
x = 1, Y x = {1}) (5 examples), (x = −1, Y x = {0, 1}) (4 examples), (x = −2, Y x = {1}) (1 example). The optimal
decision hypothesis is given as: y = 1{x &gt; α}, for α ≤ −2, or y = 1{x &lt; β}, for β ≥ 1.</p>
<p>Assume learning this with logistic regression using MINLOSS as the training algorithm optimizing the objective in eq. (3). If we initialize the parameters of logistic such that the starting hypothesis is given by y = 1{x &gt; 0} (logistic parameters: θ 1 = 0.1, θ 0 = 0), MINLOSS will greedily pick the target y = 0 for samples with x = −1, repeatedly. This will result in the learning algorithm converging to the decision hypothesis y = 1{x &gt; −0.55}, which is sub-optimal since the input with x = −2 is incorrectly classified. For logistic regression, when input x is one dimensional, probability of the prediction being 1 for any given point x = [x] is given as:
P (y = 1) = σ(θ 1 x + θ 0 ) where σ(z) = 1 1 + e −z , z ∈ R
The decision boundary is the hyperplane on which the probability of the two classes, 0 and 1, is same, i.e. the hyperplane corresponding to P (y = 0) = P (y = 1) = 0.5 or θ 1 x + θ 0 = 0.</p>
<p>Initially, θ 1 = 0.1 and θ 0 = 0 implies that decision boundary lies at x = 0 (shown in green). All the points on the left of decision boundary are predicted to have 0 label while all the points on the right have 1 label. For all the dual label points (x = 1), P (y = 1) &lt; 0.5, thus MINLOSS greedily picks the label 0 for all these points. This choice by MINLOSS doesn't change unless the decision boundary goes beyond -1.</p>
<p>However, we observe that with gradient descent using a sufficiently small learning rate, logistic regression converges at x = −0.55 with MINLOSS never flipping its choice. Clearly, this decision boundary is sub-optimal since we can define a linear decision boundary (y = 1{x &gt; α}, for α ≤ −2, or y = 1{x &lt; β}, for β ≥ 1) that classifies all the points with label 1 and achieves 100% accuracy. </p>
<p>Training
w i ← S φ ((x i , Y x i ), Θ_), ∀i ∈ B 8 Get model predictions:ŷ i ← M Θ t (x i ), ∀i ∈ B 9 Get rewards: r i ← [R(ŷ i , y ij ), ∀y ij ∈ Y x i ], ∀i ∈ B 10
Update φ: Use Equation (7) to get φ (t+1)</p>
<p>11</p>
<p>Update Θ: Use Equation (8) </p>
<p>Experiments</p>
<p>All the experiments are repeated thrice using different seeds. Hyperparameters are selected based on the held out validation set performance.</p>
<p>Hardware Architecture: Each experiment is run on a 12GB NVIDIA K40 GPU with 2880 CUDA cores and 4 cores of Intel E5-2680 V3 2.5GHz CPUs.</p>
<p>Optimizer: We use Adam as our optimizer in all our experiments. Initial learning rate is set to 0.005 for NLM [Dong et al., 2019] experiments while it is kept at 0.001 for RRN [Palm et al., 2018] experiments. Learning rate for RL phase is kept at 0.1 times the initial learning rate. We reduce learning rate by a factor of 0.2 whenever the performance on the dev set plateaus.</p>
<p>Details for N-Queens Experiment</p>
<p>Data Generation: To generate the train data, we start with all possible valid 10-Queens board configurations. We then generate queries by randomly masking any 5 queens. We check for all possible valid completions to generate potentially multiple solutions for any given query. Test data is also generated similarly. Training and testing on different board sizes ensures that no direct information leaks from the test dataset to the train dataset. Queries with multiple solutions have a small number of total solutions (2-6), hence we choose Y Architecture Details for Prediction Network M Θ : We use Neural Logic Machines (NLM) 10 [Dong et al., 2019] as the base prediction network for this task. NLM consists of a series of basic blocks, called 'Logic Modules', stacked on top of each other with residual connections. Number of blocks in an NLM architecture is referred to as its depth. Each block takes grounded predicates as input and learns to represent M intermediate predicates as its output. See [Dong et al., 2019] for further details. We chose an architecture with M = 8 and depth = 30. We keep the maximum arity of intermediate predicates learnt by the network to be 2.
x i = Y x i , ∀x i .
Input Output for Prediction Network: Input to NLM is provided in terms of grounded unary and binary predicates and the architecture learns to represent an unknown predicate in terms of the input predicates. Each cell on the board acts as an atomic variable over which predicates are defined.</p>
<p>Unary Predicates: To indicate the presence of a Queen on a cell in the input, we use a unary predicate, 'HasQueenPrior'. It is represented as a Boolean tensor x of size N 2 with 1 on k out of N 2 cells indicating the presence of a Queen. The output y of the network is also a unary predicate 'HasQueen' which indicates the final position of the queens on board.</p>
<p>Binary Predicates: We use 4 binary predicates to indicate if two cells are in same row, same column, same diagonal or same off-diagonal. The binary predicates are a constant for all board configurations for a given size N and hence can also be thought of as part of network architecture instead of input.</p>
<p>Input Output for G φ : Same as N-Queens experiment except for the addition of two more unary predicates corresponding to the inequality relations. First unary predicate is y ij −ŷ i _ which is augmented with the inequality predicates.</p>
<p>Hyperparameters: Same as N-Queens experiment.</p>
<p>Training Time: Pre-training takes roughly 12 − 14 hours while RL fine-tuning takes 7 − 8 hours.</p>
<p>Details for Sudoku Experiment</p>
<p>Data Generation for Sudoku</p>
<p>We start with the dataset proposed by Palm et al. [2018]. It has 180k queries with only unique solution and the number of givens are uniformly distributed in the range from 17 to 34. For the queries with unique solution, we randomly sample 10000 queries from their dataset, keeping their train, val and test splits. Using the queries with 17-givens from the entire dataset of size 180k, we use the following procedure to create queries with multiple solutions:</p>
<p>We know that for a Sudoku puzzle to have a unique solution it must have 17 or more givens [McGuire et al., 2012]. So we begin with the set of 17-givens puzzles having a unique solution and randomly remove 1 of the givens, giving us a 16-givens puzzle which necessarily has more than 1 correct solution. We then randomly add 1 to 18 of the digits back from the solution of the original puzzle, while ensuring that the query continues to have more than 1 solution. This procedure gives us multi-solution queries with givens in the range of 17 to 34, just as the original dataset of puzzles with only unique solution. We also observed that often there are queries which have a very large number of solutions (&gt; 100). We found that such Sudoku queries are often too poorly defined to be of any interest. So we filter out all queries having more than 50 solutions. To have the same uniform distribution of number of givens as in the original dataset of puzzles with unique solution, we sample queries from this set of puzzles with multiple solutions such that we have a uniform distribution of number of givens in our dataset.</p>
<p>We repeat this procedure to generate our validation and test data by starting from validation and test datasets from Palm et al. [2018].</p>
<ol>
<li>copyitr: We experiment with copyitr = 1 i.e. copying M Θ to M Θ _ after every update. 4. Weight Decay in Optimizer: We experiment with weight decay factor of 1E-4 (same as Palm et al. [2018]). 5. Pretraining φ: We pretrain G φ for 1250 updates, equivalent to one pass over the train data.</li>
</ol>
<p>Comparison with pretrained SOTA Model: We also evaluate the performance of a pretrained state-of-the-art neural Sudoku solver [Palm et al., 2018] 12 on our dataset. This model trains and tests on instances with single solution. The training set used by this model is a super-set of the unique solution queries in our training data and contains 180,000 queries. This model achieves a high accuracy of 94.32% on queries having unique solution (OS) in our test data which is a random sample from their test data only, but the accuracy drop to 24.48% when tested on subset of our test data having only queries that have multiple solutions (MS). We notice that the performance on MS is worse than Unique baseline, even though both are trained using queries with only unique solution. This is because the pretrained model overfits on the the queries with unique solution whereas the Unique baseline early stops based on performance on a dev set having queries with multiple solutions as well, hence avoiding overfitting on unique solution queries.</p>
<p>Training Time: Pre-training the RRN takes around 20 − 22 hours whereas RL fine-tuning starting with the pretrained model takes around 10 − 12 hours.  Table 3 reports the mean test accuracy along with the standard error over three runs for different baselines and our two approaches.</p>
<p>Results</p>
<p>varies over the elements of V, and k indices over r dimensions in the solution space. y[k] denotes the k th element of y, and similarly forŷ[k]. Example 1. Consider a learning problem over a discrete (Boolean) input space X = {0, 1} and Boolean target space in two dimensions, i.e., Y = V r = {0, 1} 2 . Let this be a trivial learning problem where ∀x, the solution set is Y x = {(0, 1), (1, 0)}. Then, given a set of examples</p>
<p>, ( 2 )
2Unique: computing L(Θ) only over the subset of training examples that have a unique solution, and (3) Random: backpropagating L(Θ) through one arbitrarily picked solution y i ∈ Y x i for every x i in the train data. We separately report performance on two mutually exclusive subsets of test data: OS: queries with a unique solution, and MS: those with multiple solutions. For all methods, we tune various hyperparameters (and do early stopping) based on the devset performance. Additional parameters for MINLOSS and SELECTR include the ratio of OS and MS examples in training. 8 SELECTR also selects the pre-training strategy as described in Section 3.5.</p>
<p>Figure 2 :
2Accuracy vs size of query's solution set.</p>
<p>Figure 3 :
3Decision Boundary learnt by logistic regression guided by MINLOSS. Green vertical line at x = 0 is the initial decision boundary and black vertical line at x = −0.55 is the decision boundary at convergence.</p>
<p>Algorithm Algorithm 1
1Joint Training of Prediction Network M Θ &amp; Selection Module S φ 1 Pre-train Θ: Θ 0 ← Pre-train Θ using Equation (4) and Equation (5) 2 For Selection Module: Θ_ ← Θ 0 3 Pre-train φ: φ 0 ← Pre-train φ using rewards from M Θ in Equation (7) 4 Initialize:</p>
<p>to get Θ (t+1) 12 Update Θ_ (for Selection Module) as: Θ_ ← Θ (t+1) if t % copyitr = 0 13 4</p>
<p>Figure 4 : 8 -
48Queens query along with its two possible solution. 9</p>
<p>Table 1 :
1Statistics of datasets. 'Train', 'Test' and task names are abbreviated. Devset similar to test. N-Qn (Tr) N-Qn (Tst) Futo. (Tr) Futo. (Tst) Sud. (Tr) Sud. (Tst)# of queries 
165,744 
10,000 
10,000 
10,000 
20,000 
10,000 
%age of MS queries 
7.04% 
16.67% 
17.05% 
24.95% 
50% 
50% 
Avg solns per MS query 
2.1 
2.2 
2.2 
2.4 
13.8 
13.7 </p>
<p>Table 2 :
2Mean (Max) test accuracy over three runs for MINLOSS and SELECTR compared with baselines. OS: test queries with only one solution, MS: queries with more than one solution.Naïve 
Random 
Unique 
MINLOSS 
SELECTR </p>
<p>OS 
70.59 (70.56) 72.91 (73.86) 75.09 (75.76) 
77.29 (78.0) 
79.73 (80.12) 
MS 
55.34 (60.97) 61.13 (61.81) 66.85 (69.48) 77.22 (77.82) 79.68 (82.37) 
N-Queens 
Overall 68.04 (68.96) 70.94 (71.85) 73.72 (74.71) 77.28 (77.97) 
79.72 (80.5) </p>
<p>OS 
65.59 (66.8) 
65.49 (65.22) 67.63 (69.49) 76.78 (78.24) 78.01 (78.36) 
MS 
14.99 (18.04) 14.22 (18.84) 19.13 (23.33) 70.35 (69.06) 71.57 (72.42) 
Futoshiki 
Overall 52.96 (54.63) 
52.7 (53.65) 
55.53 (57.97) 75.18 (75.95) 
76.4 (76.88) </p>
<p>OS 
87.85 (89.08) 87.53 (86.24) 89.19 (90.24) 88.25 (88.22) 88.69 (87.94) 
MS 
9.13 (10.59) 
13.65 (16.07) 
66.39 (70.2) 
76.93 (78.94) 81.73 (85.45) 
Sudoku 
Overall 48.49 (49.84) 50.59 (51.15) 77.79 (80.22) 82.59 (83.58) 
85.21 (86.7) </p>
<p>Table 3 :
3Mean test accuracy and standard error over three runs for MINLOSS and SELECTR compared with baselines. OS: test queries with only one solution, MS: queries with more than one solution. OS 70.59 ± 0.09 72.91 ± 0.65 75.09 ± 0.33 77.29 ± 0.38 79.73 ± 0.34 MS 55.34 ± 2.82 61.13 ± 1.13 66.85 ± 2.46 77.22 ± 1.28 79.68 ± 1.35 N-Queens Overall 68.04 ± 0.46 70.94 ± 0.71 73.72 ± 0.59 77.28 ± 0.48 79.72 ± 0.46 OS 65.59 ± 0.62 65.49 ± 0.28 67.63 ± 0.96 76.78 ± 0.81 78.01 ± 0.70 MS 14.99 ± 2.17 14.22 ± 2.77 19.13 ± 3.14 70.35 ± 1.16 71.57 ± 1.02 Futoshiki Overall 52.96 ± 0.96 52.70 ± 0.74 55.53 ± 1.44 75.18 ± 0.64 76.40 ± 0.36 OS 87.85 ± 0.84 87.53 ± 0.82 89.19 ± 1.12 88.25 ± 0.35 88.69 ± 0.55 MS 9.13 ± 0.89 13.65 ± 1.79 66.39 ± 2.82 76.93 ± 1.50 81.73 ± 2.00 Sudoku Overall 48.49 ± 0.86 50.59 ± 0.49 77.79 ± 1.96 82.59 ± 0.62 85.21 ± 0.76Naïve 
Random 
Unique 
MINLOSS 
SELECTR </p>
<p>Available at https://data.dgl.ai/models/rrn-sudoku.pkl
Futoshiki and N-Queens training datasets have significant OS-MS imbalance (seeTable 1), necessitating managing this ratio by undersampling OS. This is simlar to standard approach in class imbalance problems.
Image Source: Game play on http://www.brainmetrix.com/8-queens/ 10 Code taken from: https://github.com/google/neural-logic-machines
. Batch Size: We use a batch size of 32 for training the baselines, while for RL based training we use a batch size of 16. 11 Code taken from: https://github.com/dmlc/dgl/tree/master/examples/pytorch/rrn
Available at https://data.dgl.ai/models/rrn-sudoku.pkl
Architecture Details for Selection Module S φ : We use another NLM as our latent model G φ within the selection module S φ . We fix depth = 4 and M = 10 for the latent model.Input Output for G φ : Input to G φ is provided in terms of grounded unary and binary predictates represented as tensors just like the prediction network. G φ takes 1 unary predicate as input, represented as an N 2 sized vector, y ij −ŷ i <em>, whereŷ i _ is the prediction from its internal copy of the prediction network (M Θ </em>) given the query x i . For each y ij ∈ Y x i , G φ returns a score which is converted into a probability distribution P r φ (y ij ) over Y x i using a softmax layer.Hyperparameters:The list below enumerates the various hyper-parameters with a brief description (whenever required) and the set of its values that we experiment with. Best value of a hyper-parameter is selected based on performance on a held out validation set.1. Data Sampling: Since number of queries with multiple solutions is underrepresented in the training data, we up-sample them and experiment with different ratios of multi-solution queries in the training data. Specifically, we experiment with the ratios of 0.5 and 0.25 in addition to the two extremes of selecting queries with only unique or only multiple solutions. Different data sampling may be used during pre-training and RL fine tuning phases.2. Batch Size: We use a batch size of 4. We selected the maximum batch size that can be accommodated in 12GB GPU memory.3. copyitr: We experiment with two extremes of copying the prediction network after every update and copying after every 2500 updates.Weight Decay in Optimizer:We experiment with different weight decay factors of 1E-4, 1E-5 and 0.Pretraining φ:We pretrain G φ for 250 updates.Training Time: Pre-training takes 10 − 12 hours while RL fine-tuning take roughly 6 − 8 hours using the hardware mentioned in the beginning of the section.Details for Futoshiki ExperimentData Generation: We start with generating all the possible ways in which we can fill a N × N grid such that no number appears twice in a row or column. For generating a query we sample any solution and randomly mask out k positions on it. Also we enumerate all the GreaterT han and LessT han relations between adjacent pair of cells in the chosen solution and randomly add q of these relations to the query. We check for all possible valid completions to generate potentially multiple solutions for any given query. Test data is also generated similarly. Training and testing on different board sizes ensures that no direct information leaks from the test dataset to the training data. Queries with multiple solutions have a small number of total solutions (2-6), so we choose YArchitecture Details for Prediction Network M Θ : Same as N-Queens experiment.Input Output for Prediction Network: Just like N-Queens experiment, the input to the network is a set of grounded unary and binary predicates. We define a grid cell along with the digit to be filled in it as an atomic variable. There are N 2 cells in the grid and each cell can take N values, thus we have N 3 atomic variables over which the predicates are defined.Unary Predicates: To indicate the presence of a value in a cell in the input, we use a unary predicate, 'IsPresentPrior'. It is represented as a Boolean tensor x of size N 3 with 1 on k positions indicating the presence of a digit in a cell. The output y of the network is also a unary predicate 'IsPresent' which indicates the final prediction of grid. Additionally, there are two more unary predicates which represent the inequality relations that need to be honoured. Since inequality relations are defined only between pair of adjacent cells we can represent them using unary predicates.Binary Predicates: We use 3 binary predicates to indicate if two vairables are in same row, same column, or same grid cell. The binary predicates are a constant for all board configurations for a given size N .Architecture Details for Selection Module S φ : Same as N-Queens experiment.Architecture Details for Prediction Network M Θ : We use Recurrent Relational Network (RRN)[Palm et al., 2018]11 as the prediction network for this task. RRN uses a message passing based inference algorithm on graph objects. We use the same architecture as used byPalm et al. [2018]for their Sudoku experiments. Each cell in grid is represented as a node in the graph. All the cells in the same row, column and box are connected in the graph. Each inference involves 32 steps of message passing between the nodes in the graph and the model outputs a prediction at each step.Input Output for Prediction Network: Input to the prediction network is represented as a 81 × 10 matrix with each of the 81 cell represented as a one-hot vector representing the digits (0-9, 0 if not given). Output of the prediction network is a 81 × 10 × 32 tensor formed by concatenating the prediction of network at each of the 32 steps of message passing. The prediction at the last step is used for computing accuracy.Architecture Details for Selection Module S φ : We use a CNN as the latent model G φ . The network consists of four convolutional layers followed by a fully connected layer. The four layers have 100, 64, 32 and 32 filters respectively. Each filter has a size of 3 × 3 with stride of length 1.Input Output for G φ : Similar to the other two experiments, the input to G φ is the outputŷ i _ from the selection module's internal copy M Θ _ along with y ij . Since the prediction network gives an output at each step of message passing, we modify the G φ and the rewards for S φ accordingly to be computed from prediction at each step instead of relying only on the final prediction.Hyperparameters:1. Data Sampling: Since number of queries with multiple solutions and queries with unique solution are in equal proportion, we no longer need to upsample multi-solution queries.
Neural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1409.0473.</p>
<p>Leveraging grammar and reinforcement learning for neural program synthesis. Rudy Bunel, Matthew J Hausknecht, Jacob Devlin, Rishabh Singh, Pushmeet Kohli, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netRudy Bunel, Matthew J. Hausknecht, Jacob Devlin, Rishabh Singh, and Pushmeet Kohli. Leveraging grammar and reinforcement learning for neural program synthesis. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Con- ference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum? id=H1Xw62kRZ.</p>
<p>Structured prediction with partial labelling through the infimum loss. CoRR, abs. Vivien Cabannes, Alessandro Rudi, Francis Bach, Vivien Cabannes, Alessandro Rudi, and Francis Bach. Structured prediction with partial labelling through the infimum loss. CoRR, abs/2003.00920, 2020. URL https://arxiv.org/abs/2003. 00920.</p>
<p>Learning from partial labels. Timothée Cour, Benjamin Sapp, Ben Taskar, J. Mach. Learn. Res. 12Timothée Cour, Benjamin Sapp, and Ben Taskar. Learning from partial labels. J. Mach. Learn. Res., 12:1501-1536, 2011. URL http://dl.acm.org/citation.cfm?id=2021049.</p>
<p>Stochastic video generation with a learned prior. Emily Denton, Rob Fergus, PMLRProceedings of the 35th International Conference on Machine Learning. Jennifer G. Dy and Andreas Krausethe 35th International Conference on Machine LearningStockholmsmässan, Stockholm, Sweden80Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. In Jennifer G. Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 1182-1191. PMLR, 2018. URL http: //proceedings.mlr.press/v80/denton18a.html.</p>
<p>Robustfill: Neural program learning under noisy I/O. Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-Rahman Mohamed, Pushmeet Kohli, PMLRProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia70Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel-rahman Mohamed, and Pushmeet Kohli. Robustfill: Neural program learning under noisy I/O. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pages 990-998. PMLR, 2017. URL http://proceedings.mlr.press/v70/devlin17a.html.</p>
<p>Neural logic machines. Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, Denny Zhou, 7th International Conference on Learning Representations. New Orleans, LA, USAHonghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Denny Zhou. Neural logic machines. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview.net/forum?id= B1xY-hRctX.</p>
<p>Learning explanatory rules from noisy data. Richard Evans, Edward Grefenstette, 10.1613/jair.5714J. Artif. Intell. Res. 61Richard Evans and Edward Grefenstette. Learning explanatory rules from noisy data. J. Artif. Intell. Res., 61:1-64, 2018. doi: 10.1613/jair.5714. URL https://doi.org/10.1613/jair.5714.</p>
<p>Reinforcement learning for relation classification from noisy data. Jun Feng, Minlie Huang, Li Zhao, Yang Yang, Xiaoyan Zhu, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)New Orleans, Louisiana, USAAAAI PressJun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xiaoyan Zhu. Reinforcement learning for relation classification from noisy data. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI- 18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5779-5786. AAAI Press, 2018. URL https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17151.</p>
<p>Partial label learning with self-guided retraining. Lei Feng, Bo An, 10.1609/aaai.v33i01.33013542The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019. Honolulu, Hawaii, USAAAAI PressLei Feng and Bo An. Partial label learning with self-guided retraining. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019, pages 3542-3549. AAAI Press, 2019. doi: 10.1609/aaai.v33i01.33013542. URL https://doi. org/10.1609/aaai.v33i01.33013542.</p>
<p>Prediction under uncertainty with error-encoding networks. CoRR, abs/1711.04994. Mikael Henaff, Jake Junbo, Yann Zhao, Lecun, Mikael Henaff, Junbo Jake Zhao, and Yann LeCun. Prediction under uncertainty with error-encoding networks. CoRR, abs/1711.04994, 2017. URL http://arxiv.org/abs/1711.04994.</p>
<p>Learning with multiple labels. Rong Jin, Zoubin Ghahramani, Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002. Suzanna Becker, Sebastian Thrun, and Klaus ObermayerVancouver, British Columbia, CanadaMIT PressRong Jin and Zoubin Ghahramani. Learning with multiple labels. In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer, editors, Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada], pages 897-904. MIT Press, 2002. URL http://papers.nips.cc/paper/ 2234-learning-with-multiple-labels.</p>
<p>Zchaff2004: An efficient SAT solver. Yogesh S Mahajan, Zhaohui Fu, Sharad Malik, 10.1007/11527695_27Theory and Applications of Satisfiability Testing, 7th International Conference. Holger H. Hoos and David G. MitchellVancouver, BC, CanadaSpringer3542Revised Selected PapersYogesh S. Mahajan, Zhaohui Fu, and Sharad Malik. Zchaff2004: An efficient SAT solver. In Holger H. Hoos and David G. Mitchell, editors, Theory and Applications of Satisfiability Testing, 7th International Conference, SAT 2004, Vancouver, BC, Canada, May 10-13, 2004, Revised Selected Papers, volume 3542 of Lecture Notes in Computer Science, pages 360-375. Springer, 2004. doi: 10.1007/11527695_27. URL https://doi.org/10.1007/11527695_27.</p>
<p>There is no 16-clue sudoku: Solving the sudoku minimum number of clues problem via hitting set enumeration. Gary Mcguire, Bastian Tugemann, Gilles Civario, Experimental Mathematics. 23Gary McGuire, Bastian Tugemann, and Gilles Civario. There is no 16-clue sudoku: Solving the sudoku minimum number of clues problem via hitting set enumeration. Experimental Mathematics, 23:190-217, 2012.</p>
<p>Differentiable reasoning on large knowledge bases and natural language. Pasquale Minervini, Matko Bošnjak, Tim Rocktäschel, Sebastian Riedel, Edward Grefenstette, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. the Thirty-First AAAI Conference on Artificial IntelligenceAAAI PressPasquale Minervini, Matko Bošnjak, Tim Rocktäschel, Sebastian Riedel, and Edward Grefenstette. Differentiable reasoning on large knowledge bases and natural language. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. AAAI Press, 2020.</p>
<p>Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. Ramesh Nallapati, Feifei Zhai, Bowen Zhou, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence. Satinder P. Singh and Shaul Markovitchthe Thirty-First AAAI Conference on Artificial IntelligenceSan Francisco, California, USAAAAI PressRamesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In Satinder P. Singh and Shaul Markovitch, editors, Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California, USA, pages 3075-3081. AAAI Press, 2017. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14636.</p>
<p>Recurrent relational networks. Rasmus Berg Palm, Ulrich Paquet, Ole Winther, ; Hanna, M Wallach, Hugo Larochelle, Kristen Grauman, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Nicolò Cesa-Bianchi, and Roman GarnettNeurIPS; Montréal, CanadaSamy Bengio,Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 3372-3382, 2018. URL http://papers.nips.cc/paper/ 7597-recurrent-relational-networks.</p>
<p>Can convolutional neural networks crack sudoku puzzles?. Kyubyong Park, Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? https://github.com/ Kyubyong/sudoku, 2018.</p>
<p>A deep reinforced model for abstractive summarization. Romain Paulus, Caiming Xiong, Richard Socher, 6th International Conference on Learning Representations. Vancouver, BC, CanadaConference Track Proceedings. OpenReview.netRomain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive sum- marization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 -May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https://openreview.net/forum?id=HkAClQgA-.</p>
<p>Robust distant supervision relation extraction via deep reinforcement learning. Pengda Qin, Weiran Xu, William Yang Wang, Proceedings of the 56th. Iryna Gurevych and Yusuke Miyaothe 56thPengda Qin, Weiran Xu, and William Yang Wang. Robust distant supervision relation extraction via deep reinforcement learning. In Iryna Gurevych and Yusuke Miyao, editors, Proceedings of the 56th</p>
<p>10.18653/v1/P18-1199Annual Meeting of the Association for Computational Linguistics. Melbourne, AustraliaAssociation for Computational Linguistics1Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 2137-2147. Association for Computational Linguistics, 2018. doi: 10.18653/v1/P18-1199. URL https://www.aclweb.org/anthology/ P18-1199/.</p>
<p>Injecting logical background knowledge into embeddings for relation extraction. Tim Rocktäschel, Sameer Singh, Sebastian Riedel, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Denver, Colorado, USANAACL HLT 2015Tim Rocktäschel, Sameer Singh, and Sebastian Riedel. Injecting logical background knowledge into embeddings for relation extraction. In NAACL HLT 2015, The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, May 31 -June 5, 2015, pages 1119-1129, 2015. URL http://aclweb.org/anthology/N/N15/N15-1118.pdf.</p>
<p>Minimum sudoku. Gordon Royle, Gordon Royle. Minimum sudoku. https://staffhome.ecm.uwa.edu.au/~00013890/ sudokumin.php, 2014.</p>
<p>Relational recurrent neural networks. Adam Santoro, Ryan Faulkner, David Raposo, Jack W Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy P Lillicrap, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems. Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman GarnettNeurIPS; Montréal, CanadaAdam Santoro, Ryan Faulkner, David Raposo, Jack W. Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, and Timothy P. Lillicrap. Relational recurrent neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada, pages 7310-7321, 2018. URL http://papers.nips.cc/ paper/7960-relational-recurrent-neural-networks.</p>
<p>Local search strategies for satisfiability testing. Bart Selman, Henry A Kautz, Bram Cohen, 10.1090/dimacs/026/25Cliques, Coloring, and Satisfiability, Proceedings of a DIMACS Workshop. David S. Johnson and Michael A. TrickNew Brunswick, New Jersey, USADIMACS/AMS26Bart Selman, Henry A. Kautz, and Bram Cohen. Local search strategies for satisfiability test- ing. In David S. Johnson and Michael A. Trick, editors, Cliques, Coloring, and Satisfi- ability, Proceedings of a DIMACS Workshop, New Brunswick, New Jersey, USA, October 11-13, 1993, volume 26 of DIMACS Series in Discrete Mathematics and Theoretical Com- puter Science, pages 521-531. DIMACS/AMS, 1993. doi: 10.1090/dimacs/026/25. URL https://doi.org/10.1090/dimacs/026/25.</p>
<p>Sequence to sequence learning with neural networks. Ilya Sutskever, Oriol Vinyals, Quoc V Le, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems. Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. WeinbergerQuebec, CanadaIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger, editors, Advances in Neural Information Processing Systems 27: An- nual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Mon- treal, Quebec, Canada, pages 3104-3112, 2014. URL http://papers.nips.cc/paper/ 5346-sequence-to-sequence-learning-with-neural-networks.</p>
<p>Multi-label classification: An overview. Grigorios Tsoumakas, Ioannis Katakis, 10.4018/jdwm.2007070101IJDWMGrigorios Tsoumakas and Ioannis Katakis. Multi-label classification: An overview. IJDWM, 3 (3):1-13, 2007. doi: 10.4018/jdwm.2007070101. URL https://doi.org/10.4018/jdwm.</p>
<p>Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, 10.1109/TPAMI.2016.2587640IEEE Trans. Pattern Anal. Mach. Intell. 394Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: Lessons learned from the 2015 MSCOCO image captioning challenge. IEEE Trans. Pattern Anal. Mach. Intell., 39(4):652-663, 2017. doi: 10.1109/TPAMI.2016.2587640. URL https://doi.org/10.1109/ TPAMI.2016.2587640.</p>
<p>Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. Po-Wei Wang, Priya L Donti, Bryan Wilder, J Zico Kolter, PMLRProceedings of the 36th International Conference on Machine Learning, ICML 2019. the 36th International Conference on Machine Learning, ICML 2019Long Beach, California, USA97Po-Wei Wang, Priya L. Donti, Bryan Wilder, and J. Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 6545-6554. PMLR, 2019. URL http://proceedings.mlr.press/v97/wang19e.html.</p>
<p>Partial label learning via label enhancement. Ning Xu, Jiaqi Lv, Xin Geng, 10.1609/aaai.v33i01.33015557The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019. Honolulu, Hawaii, USAAAAI PressNing Xu, Jiaqi Lv, and Xin Geng. Partial label learning via label enhancement. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 -February 1, 2019, pages 5557-5564. AAAI Press, 2019. doi: 10.1609/aaai.v33i01.33015557. URL https: //doi.org/10.1609/aaai.v33i01.33015557.</p>
<p>Image captioning with semantic attention. Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, Jiebo Luo, 10.1109/CVPR.2016.5032016 IEEE Conference on Computer Vision and Pattern Recognition. Las Vegas, NV, USAIEEE Computer SocietyQuanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image captioning with semantic attention. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 4651-4659. IEEE Computer Society, 2016. doi: 10.1109/CVPR.2016.503. URL https://doi.org/10.1109/CVPR.2016.503.</p>            </div>
        </div>

    </div>
</body>
</html>
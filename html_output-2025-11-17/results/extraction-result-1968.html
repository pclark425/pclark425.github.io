<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1968 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1968</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1968</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-40.html">extraction-schema-40</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <p><strong>Paper ID:</strong> paper-278394787</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.04965v1.pdf" target="_blank">DenseGrounding: Improving Dense Language-Vision Semantics for Ego-Centric 3D Visual Grounding</a></p>
                <p><strong>Paper Abstract:</strong> Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction. A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions. However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions. We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics. For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment. For text descriptions, we propose a Language Semantic Enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training. Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the SOTA in egocentric 3D visual grounding. Our method also achieves 1st place and receives the Innovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1968.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1968.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DenseGrounding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DenseGrounding</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>End-to-end ego-centric 3D visual grounding system that (1) restores dense scene-level visual semantics via a Hierarchical Scene Semantic Enhancer and (2) enriches textual descriptions using an LLM grounded in a Scene Information Database, producing large gains on EmbodiedScan.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DenseGrounding</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Single-stage ego-centric multi-view 3D visual grounding system built on the EmbodiedScan architecture: decoupled 2D (RGB) semantic and 3D geometric encoders whose 2D semantics are lifted to 3D, concatenated with geometric features, fused with an FPN and decoded by a DETR-style decoder; augmented by (a) Hierarchical Scene Semantic Enhancer (HSSE) that (i) aggregates multi-scale, per-view semantic features via pooled cross-attention, (ii) performs scene-level multi-view self-attention together with language features, and (iii) broadcasts the resulting scene-level semantics back into multi-scale spatial feature maps via cross-attention; and (b) Language Semantic Enhancer (LSE) that uses an LLM grounded on a Scene Information Database to produce disambiguating training augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>ResNet-50 (2D semantic encoder) and MinkNet34 (3D geometric encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>Pretrained on a 3D object detection pretraining stage following EmbodiedScan settings with Class-Balanced Grouping and Sampling (CBGS); exact external pretraining datasets not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Cross-modal grounding via (1) lifting 2D multi-scale semantic features to 3D and concatenating with geometric features, (2) cross-attention/self-attention within HSSE to produce scene-level semantics fused with language, (3) semantic broadcast (cross-attention) to re-infuse scene semantics into spatial feature maps, and (4) a DETR-based decoder that selects visual tokens most similar to text features for final 9-DoF bounding box regression.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Multi-level: multi-scale 2D semantic feature maps lifted to 3D point cloud (sparse), scene-level global semantic tokens, and point/region-level spatial features used by decoder.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Explicit 3D spatial representations: depth-reconstructed point cloud (3D coordinates), intrinsic/extrinsic-based lifting of image features to 3D, and output 9-DoF bounding boxes.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D visual grounding / 3D scene understanding for embodied agents</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan (ego-centric multi-view 3D visual grounding benchmark)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric multi-view RGB-D scans (real-world indoor scenes)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ACC@25 (accuracy for predicted box IoU > 0.25)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Full split: 45.31% ACC@25 overall on EmbodiedScan validation (Table 1); Mini split: 41.95% ACC@25 overall (Table 1). Zero-shot cross-dataset (train on 3RScan+Matterport3D, test on ScanNet): 18.14% overall (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Baseline without HSSE and without LSE (reported enhanced baseline in ablation Table 3): 35.77% ACC@25 overall (Table 3, baseline row).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>Adding LSE gave +3.48% (Table 3); adding HSSE then gave an additional +2.09% (Table 3); combined LSE+HSSE yields +5.57% ACC@25 overall over the baseline (Table 3). DenseGrounding outperforms EmbodiedScan baseline by 6.03 percentage points on the full set (45.31% vs 39.28% in Table 1) and by 7.86 points vs EmbodiedScan mini (41.95% vs 34.09% raw EmbodiedScan; Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td>The paper replaced the original EmbodiedScan RoBERTa text encoder with a CLIP text encoder and incorporated CBGS during pretraining to form an enhanced baseline (EmbodiedScan †). The enhanced baseline (EmbodiedScan †) produced +1.99% ACC@25 improvement over the original EmbodiedScan on the mini split (Table 1); the paper cites better cross-modal alignment using CLIP as justification.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>The dominant perception bottleneck is semantic loss from sparse fusion: only ~2% of reconstructed point-cloud points are sampled for downstream fusion (due to computational limits), causing loss of fine-grained visual semantics (especially for small objects) and limiting grounding performance.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Two main failure modes analyzed: (1) semantic sparsity in the sparse fused point cloud leads to missed fine-grained object details and mislocalization (quantified by improvements when HSSE re-infuses scene semantics); (2) ambiguous or concise natural language descriptions (missing anchor objects) cause confusion among similar objects—DenseGrounding shows larger gains on 'Hard' scenes (many same-class distractors), e.g., Hard ACC@25 improved from 30.51% (baseline in Table 3) to 34.38% (with LSE+HSSE) on the mini set (+3.87 points). The paper reports qualitative examples where baseline misidentifies among similar objects while DenseGrounding succeeds due to enriched text and cross-modal alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td>No explicit domain-adaptation layers; evaluated zero-shot cross-dataset generalization (train on 3RScan+Matterport3D, test on ScanNet): DenseGrounding yields 18.14% overall vs baseline 9.81%, indicating improved robustness across dataset/camera/scene shifts thanks to HSSE and LSE (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td>Limited-data analysis: DenseGrounding is more sample-efficient — e.g., with 40% of training data DenseGrounding outperforms the baseline trained on 80% (Figure 5); explicit numeric curves not tabulated but described in Appendix A.3.</td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Decoupled modality encoding followed by lift-to-3D (image semantics lifted using intrinsics/extrinsics), concatenation with geometric features, FPN fusion, HSSE cross-attention/self-attention to create scene tokens, semantic broadcast via cross-attention to multi-scale spatial maps, and DETR-style decoding that matches visual tokens to language features.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Demonstrated improved sample efficiency in limited-data experiments: DenseGrounding trained on 40% of data outperforms baseline trained on 80% (Figure 5); on mini-splits DenseGrounding achieves substantially higher ACC@25 than baselines indicating fewer samples needed to reach strong performance.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Restoring dense, scene-level visual semantics (HSSE) and enriching textual context via LLM-grounded augmentation (LSE) address two orthogonal grounding failure modes—semantic sparsity from sparse point sampling and ambiguity in concise human descriptions—yielding consistent ACC@25 improvements (≈+5–6 pts overall) and substantially better performance on hard scenes and zero-shot cross-dataset generalization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1968.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1968.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HSSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Scene Semantic Enhancer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hierarchical module that (1) aggregates per-view multi-scale semantic features into pooled view-level tokens, (2) performs scene-level multi-view self-attention jointly with language tokens to form global scene semantics, and (3) broadcasts these semantics back into multi-scale spatial feature maps via cross-attention to mitigate semantic loss from sparse point sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>HSSE (Hierarchical Scene Semantic Enhancer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three-stage module: (a) view-level semantic aggregation — FPN + adaptive average pooling to produce pooled queries followed by cross-attention and self-attention within each view; (b) scene-level semantic interaction — concatenation of pooled view tokens with language features and L_scene layers of self-attention to produce scene-global tokens and refined language tokens; (c) semantic broadcast — cross-attention from multi-scale spatial features (queries) to scene-global tokens (keys/values) to re-infuse global semantics into spatial maps. An aggregation-broadcast mechanism filters irrelevant information.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (module operates on features from ResNet-50 2D encoder and MinkNet34 3D encoder)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Enables grounding by creating scene-level visual tokens that directly attend to and interact with language tokens (self-attention) and then broadcasting enriched semantics back to spatial feature maps via cross-attention, improving alignment between text and visual features.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Scene-level global semantic tokens plus multi-scale spatial feature maps (multi-level representation).</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Operates on lifted multi-scale 2D semantic feature maps that have been projected into 3D spatial context; not a new explicit coordinate representation but re-infuses semantic context into spatial feature maps used for point-based grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D visual grounding (ego-centric)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan (used within DenseGrounding)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric multi-view RGB-D</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ACC@25</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>HSSE alone (added to enhanced baseline) yields +2.09% ACC@25 improvement in ablation (Table 3 shows baseline -> +HSSE gives Overall 39.25% vs baseline 35.77% on mini experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Ablation baseline without HSSE (and without LSE) overall ACC@25 35.77% (Table 3 baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>HSSE alone gives +2.09% ACC@25 vs baseline (Table 3). When combined with LSE, HSSE contributes part of the combined +5.57% improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>HSSE was explicitly designed to address the semantic loss caused by sparse sampling (~2% of reconstructed point cloud points retained), by producing and broadcasting dense scene-level semantic tokens into spatial maps to recover fine-grained details.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper shows HSSE reduces failures on small and similar objects by improving fine-grained semantics; ablations on the number of self-attention layers and pooled token spatial size indicate sensitivity (Tables 4 and 5) — too small pooled map loses information, too large adds redundancy.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>View-level cross-attention and self-attention to form pooled view tokens; scene-level self-attention joining view tokens and language; semantic broadcast via cross-attention to spatial feature maps.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Restoring scene-level dense semantics via hierarchical aggregation + cross-modal interaction effectively mitigates semantic loss from sparse point sampling and improves grounding, especially for small/distractor-heavy (hard) scenes; HSSE alone yields measurable gains (~+2% ACC@25 in ablation).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1968.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1968.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Language Semantic Enhancer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based pipeline that augments training-language descriptions by grounding LLM prompts with a Scene Information Database (SIDB) containing object relationships and location info to produce disambiguating, anchor-rich textual augmentations for training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LSE (Language Semantic Enhancer)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Constructs a Scene Information Database (SIDB) from training annotations (object labels, relationships, bounding-box locations); for each raw description, selects up to k scene-context descriptions (k=50) and prompts an LLM (GPT-4o mini in experiments) to generate enriched descriptions that preserve the original content while adding anchors (object relationships and location references). The augmented text is used only during training (inference uses original raw descriptions).</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (text augmentation component)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Improves grounding indirectly by enriching language inputs used during training, providing stronger anchors and disambiguation so learned cross-modal alignment is more robust to similar-object confusion.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Language-level augmentation affecting downstream token alignments to multi-scale visual features and scene tokens.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Uses SIDB entries that include object location information (3D bounding-box positions) to ground textual augmentations; thus LSE leverages explicit 3D location metadata to generate spatially-anchored text.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D visual grounding (training-time augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan (used to construct SIDB and generate augmented training descriptions)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric indoor scenes (used only for constructing SIDB/context for LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ACC@25</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>LSE alone (with DB(R+L)) yields +2.45% ACC@25 over naive baseline in LSE ablations (Table 2 shows LLM+DB(R+L) overall 38.82% vs naive baseline constrained to 25% data). In Table 3 ablation on mini set, LSE alone increases overall from 35.77% to 39.02% (+3.25% in that table's configuration).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Baseline without LSE (and without HSSE) overall ACC@25 35.77% (Table 3 baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>LSE (LLM+DB(R+L)) yields ~+2.45% to +3.48% ACC@25 depending on subset and comparison (Table 2 and Table 3 results show consistent single-component gains). Combined with HSSE the improvement is additive (total +5.57% in Table 3).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>LSE targets the textual ambiguity bottleneck: short/concise human descriptions lacking anchors cause mis-grounding among similar objects; LSE augments descriptions with anchors drawn from SIDB to reduce this ambiguity.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Paper reports LSE reduces confusion among similar-looking objects; Table 2 shows progressive improvements from simple concatenation -> template LLM -> LLM+DB(R) -> LLM+DB(R+L), indicating that including both relationship and location info in SIDB yields the best disambiguation effect.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Language augmentation pipeline provides richer textual tokens which are then encoded (CLIP text encoder) and participate in HSSE scene-level self-attention and downstream matching in the DETR decoder (i.e., late cross-modal fusion via attention).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>LSE increases training efficiency: on limited data experiments LSE contributes to better performance when training data is scarce (combined with HSSE leads to larger relative gains on mini splits).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>LLM-grounded text augmentation using a Scene Information Database that includes object relationships and 3D location information produces more informative, anchor-rich training descriptions that measurably reduce ambiguity-induced grounding errors and contribute ~+2–3.5 percentage points ACC@25 as a standalone component.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1968.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1968.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EmbodiedScan</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EmbodiedScan (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior state-of-the-art ego-centric multi-view RGB-D 3D visual grounding method that decouples RGB semantic encoding and depth-based geometric encoding, lifts 2D semantics into 3D using intrinsics/extrinsics, samples a sparse set of points for fusion, and uses a decoder for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EmbodiedScan</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoupled encoder architecture: separate 2D semantic encoder and 3D geometric encoder; 2D semantic features are projected into 3D point cloud using camera intrinsics/extrinsics and concatenated with geometric features; only a sparse subset (~2%) of points is sampled for downstream processing due to compute constraints; semantic features are fused and decoded for 9-DoF bounding-box grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>2D semantic encoder and 3D geometric encoder (original paper used standard encoders; DenseGrounding re-evaluates EmbodiedScan weights and builds on it)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td>EmbodiedScan's pretraining details are referenced but exact datasets not restated in this paper (DenseGrounding follows EmbodiedScan training settings for encoder pretraining).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Lift-and-match: lift 2D semantic features to 3D points, concatenate with geometry, fuse and decode; grounding primarily performed by matching visual tokens to language tokens in a decoder pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>3D point-cloud centric (sparse point-level features) with lifted 2D semantics</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Depth-reconstructed point cloud and 3D bounding boxes</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>Ego-centric 3D visual grounding</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric multi-view RGB-D</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ACC@25</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Reported EmbodiedScan (re-evaluated) Full: 39.28% ACC@25 overall (Table 1); Mini split: 34.09% ACC@25 overall (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>EmbodiedScan (and prior methods) sample only a sparse subset (~2%) of reconstructed point-cloud points during fusion, which DenseGrounding identifies as causing significant loss of fine-grained visual semantics and grounding failures, especially for small objects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Baseline struggles on 'Hard' scenes with ≥3 objects of the same class and with ambiguous descriptions; DenseGrounding shows larger improvements in these scenarios.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Lift 2D semantics to 3D via intrinsics/extrinsics, concatenate with geometry, and fuse via FPN before decoding.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>EmbodiedScan established the sparse lift-and-fuse baseline for ego-centric 3D grounding but is limited by semantic sparsity from extreme point sampling; methods that restore dense semantics and enrich language mitigate these bottlenecks.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1968.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1968.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM+DB(R+L)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM augmentation with Scene Information Database (Relationships + Location)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Training-time text augmentation strategy that prompts an LLM with scene-context descriptions from a Scene Information Database containing object relationships and 3D location metadata to produce disambiguating, anchor-rich language augmentations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLM+DB(R+L)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Select up to k (k=50) context descriptions from SIDB for the same scene and anchor/target classes, include relationship and location information, and prompt an LLM (GPT-4o mini used in experiments) to produce enriched paraphrases that preserve original content while adding anchors; augmented texts used only during training.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td>N/A (text augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Indirect: improves the language side of cross-modal alignment so learned grounding is less ambiguous and more robust to similar-object distractors.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Language-level augmentation affecting token-level grounding interactions</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>Uses object location metadata (3D bounding box centers) from SIDB to ground generated text in spatial terms</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D visual grounding (training augmentation)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>EmbodiedScan (used to build SIDB)</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric indoor scenes (used only as context for LLM prompts)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>ACC@25</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Ablation (Table 2) shows LLM+DB(R+L) achieves 38.82% ACC@25 (comparison constrained to 25% data for fairness) vs ConcatSamples 36.37% and naive LLM 37.23% — showing the best single-method gain among tested text augmentations on that split.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td>Naive baseline (Concat Samples) in the constrained 25%-data ablation: 36.37% ACC@25 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td>LLM+DB(R+L) outperforms naive concatenation and template LLM augmentation by ~1.6–2.5 percentage points in the ablated 25% experiments (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Directly addresses failure mode due to ambiguous descriptions; ablation shows incremental benefit from adding relationships and then locations (R+L) in SIDB.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Augmented language tokens are encoded (CLIP text encoder) and used in cross-modal attention/decoder matching—no change to visual pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Contributes to sample efficiency; in low-data ablations LLM+DB(R+L) gives the largest gains among text-augmentation variants (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>LLM-guided textual augmentation grounded with scene relationships and explicit location metadata yields the strongest reduction in language ambiguity among tested augmentation methods, improving ACC@25 by ~1.6–2.5 pts in constrained-data ablations and contributing substantially to combined system gains.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1968.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1968.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of vision-language grounding mechanisms in embodied AI tasks (robotics, navigation, manipulation), including grounding architectures, perception bottlenecks, failure modes, ablation studies, and performance comparisons across different grounding approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sparse Sampling Bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sparse Point Sampling Semantic Bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Identified failure mode where reconstructing full scene point clouds is infeasible at runtime and extreme subsampling (~2% of points) discards most lifted 2D semantic features, causing loss of fine-grained semantics and grounding errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Sparse Point Sampling Bottleneck</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>During depth-to-point reconstruction and fusion, computational limits force sampling only a small fraction (~2%) of the full point cloud; semantic features corresponding to unsampled points are discarded, producing a semantic sparsity bottleneck that harms detection/grounding of small or closely clustered objects.</td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>visual_encoder_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_mechanism</strong></td>
                            <td>Negative-impact mechanism: loss of lifted 2D semantics reduces the fidelity of visual tokens available for matching with language, degrading grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>representation_level</strong></td>
                            <td>Point-cloud level (sparse 3D points a small subset of full scene)</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_representation</strong></td>
                            <td>3D point cloud with heavy subsampling (~2% retained)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_type</strong></td>
                            <td>3D visual grounding (embodied perception bottleneck)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Observed in EmbodiedScan and addressed by DenseGrounding on EmbodiedScan benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>visual_domain</strong></td>
                            <td>Egocentric RGB-D reconstructions</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Indirectly measured via improvements when mitigated (ACC@25)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Not a model performance value; mitigation via HSSE contributes ~+2.09% ACC@25 (Table 3) and combined improvements indicate the bottleneck materially reduces baseline accuracy by several percentage points.</td>
                        </tr>
                        <tr>
                            <td><strong>has_grounding_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_grounding</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>grounding_improvement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_encoder_comparison</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>encoder_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_identified</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>perception_bottleneck_details</strong></td>
                            <td>Sampling only ~2% of reconstructed points discards most per-pixel/lifted semantic features; this specifically degrades performance on small objects and scenes with many similar objects.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_mode_analysis</strong></td>
                            <td>Attributed to missed fine-grained semantics and errors in distinguishing similar objects; mitigated by HSSE's scene-level semantic broadcast which reinjects dense semantics into the sampled maps, improving ACC@25 by measurable amounts in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_shift_handling</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_object_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>frozen_vs_finetuned</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_scale_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fusion_mechanism</strong></td>
                            <td>Problem arises from lift->sample->discard workflow; solution uses HSSE to broadcast scene-wise semantics back into retained spatial maps.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_grounding</strong></td>
                            <td>Extreme subsampling of reconstructed point clouds is a major bottleneck; restoring dense scene-level semantics (HSSE) recovers significant performance lost to sampling and improves grounding robustness in cluttered/hard scenes.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI <em>(Rating: 2)</em></li>
                <li>ViewRefer: Grasp the Multi-View Knowledge for 3D Visual Grounding <em>(Rating: 2)</em></li>
                <li>Multi-View Transformer for 3D Visual Grounding <em>(Rating: 2)</em></li>
                <li>3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection <em>(Rating: 1)</em></li>
                <li>RefMask3D: Language-Guided Transformer for 3D Referring Segmentation <em>(Rating: 1)</em></li>
                <li>DenseG: Alleviating Vision-Language Feature Sparsity in Multi-View 3D Visual Grounding <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1968",
    "paper_id": "paper-278394787",
    "extraction_schema_id": "extraction-schema-40",
    "extracted_data": [
        {
            "name_short": "DenseGrounding",
            "name_full": "DenseGrounding",
            "brief_description": "End-to-end ego-centric 3D visual grounding system that (1) restores dense scene-level visual semantics via a Hierarchical Scene Semantic Enhancer and (2) enriches textual descriptions using an LLM grounded in a Scene Information Database, producing large gains on EmbodiedScan.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DenseGrounding",
            "model_description": "Single-stage ego-centric multi-view 3D visual grounding system built on the EmbodiedScan architecture: decoupled 2D (RGB) semantic and 3D geometric encoders whose 2D semantics are lifted to 3D, concatenated with geometric features, fused with an FPN and decoded by a DETR-style decoder; augmented by (a) Hierarchical Scene Semantic Enhancer (HSSE) that (i) aggregates multi-scale, per-view semantic features via pooled cross-attention, (ii) performs scene-level multi-view self-attention together with language features, and (iii) broadcasts the resulting scene-level semantics back into multi-scale spatial feature maps via cross-attention; and (b) Language Semantic Enhancer (LSE) that uses an LLM grounded on a Scene Information Database to produce disambiguating training augmentations.",
            "visual_encoder_type": "ResNet-50 (2D semantic encoder) and MinkNet34 (3D geometric encoder)",
            "visual_encoder_pretraining": "Pretrained on a 3D object detection pretraining stage following EmbodiedScan settings with Class-Balanced Grouping and Sampling (CBGS); exact external pretraining datasets not specified in paper",
            "grounding_mechanism": "Cross-modal grounding via (1) lifting 2D multi-scale semantic features to 3D and concatenating with geometric features, (2) cross-attention/self-attention within HSSE to produce scene-level semantics fused with language, (3) semantic broadcast (cross-attention) to re-infuse scene semantics into spatial feature maps, and (4) a DETR-based decoder that selects visual tokens most similar to text features for final 9-DoF bounding box regression.",
            "representation_level": "Multi-level: multi-scale 2D semantic feature maps lifted to 3D point cloud (sparse), scene-level global semantic tokens, and point/region-level spatial features used by decoder.",
            "spatial_representation": "Explicit 3D spatial representations: depth-reconstructed point cloud (3D coordinates), intrinsic/extrinsic-based lifting of image features to 3D, and output 9-DoF bounding boxes.",
            "embodied_task_type": "3D visual grounding / 3D scene understanding for embodied agents",
            "embodied_task_name": "EmbodiedScan (ego-centric multi-view 3D visual grounding benchmark)",
            "visual_domain": "Egocentric multi-view RGB-D scans (real-world indoor scenes)",
            "performance_metric": "ACC@25 (accuracy for predicted box IoU &gt; 0.25)",
            "performance_value": "Full split: 45.31% ACC@25 overall on EmbodiedScan validation (Table 1); Mini split: 41.95% ACC@25 overall (Table 1). Zero-shot cross-dataset (train on 3RScan+Matterport3D, test on ScanNet): 18.14% overall (Table 6).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Baseline without HSSE and without LSE (reported enhanced baseline in ablation Table 3): 35.77% ACC@25 overall (Table 3, baseline row).",
            "grounding_improvement": "Adding LSE gave +3.48% (Table 3); adding HSSE then gave an additional +2.09% (Table 3); combined LSE+HSSE yields +5.57% ACC@25 overall over the baseline (Table 3). DenseGrounding outperforms EmbodiedScan baseline by 6.03 percentage points on the full set (45.31% vs 39.28% in Table 1) and by 7.86 points vs EmbodiedScan mini (41.95% vs 34.09% raw EmbodiedScan; Table 1).",
            "has_encoder_comparison": true,
            "encoder_comparison_results": "The paper replaced the original EmbodiedScan RoBERTa text encoder with a CLIP text encoder and incorporated CBGS during pretraining to form an enhanced baseline (EmbodiedScan †). The enhanced baseline (EmbodiedScan †) produced +1.99% ACC@25 improvement over the original EmbodiedScan on the mini split (Table 1); the paper cites better cross-modal alignment using CLIP as justification.",
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "The dominant perception bottleneck is semantic loss from sparse fusion: only ~2% of reconstructed point-cloud points are sampled for downstream fusion (due to computational limits), causing loss of fine-grained visual semantics (especially for small objects) and limiting grounding performance.",
            "failure_mode_analysis": "Two main failure modes analyzed: (1) semantic sparsity in the sparse fused point cloud leads to missed fine-grained object details and mislocalization (quantified by improvements when HSSE re-infuses scene semantics); (2) ambiguous or concise natural language descriptions (missing anchor objects) cause confusion among similar objects—DenseGrounding shows larger gains on 'Hard' scenes (many same-class distractors), e.g., Hard ACC@25 improved from 30.51% (baseline in Table 3) to 34.38% (with LSE+HSSE) on the mini set (+3.87 points). The paper reports qualitative examples where baseline misidentifies among similar objects while DenseGrounding succeeds due to enriched text and cross-modal alignment.",
            "domain_shift_handling": "No explicit domain-adaptation layers; evaluated zero-shot cross-dataset generalization (train on 3RScan+Matterport3D, test on ScanNet): DenseGrounding yields 18.14% overall vs baseline 9.81%, indicating improved robustness across dataset/camera/scene shifts thanks to HSSE and LSE (Table 6).",
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": "Limited-data analysis: DenseGrounding is more sample-efficient — e.g., with 40% of training data DenseGrounding outperforms the baseline trained on 80% (Figure 5); explicit numeric curves not tabulated but described in Appendix A.3.",
            "fusion_mechanism": "Decoupled modality encoding followed by lift-to-3D (image semantics lifted using intrinsics/extrinsics), concatenation with geometric features, FPN fusion, HSSE cross-attention/self-attention to create scene tokens, semantic broadcast via cross-attention to multi-scale spatial maps, and DETR-style decoding that matches visual tokens to language features.",
            "sample_efficiency": "Demonstrated improved sample efficiency in limited-data experiments: DenseGrounding trained on 40% of data outperforms baseline trained on 80% (Figure 5); on mini-splits DenseGrounding achieves substantially higher ACC@25 than baselines indicating fewer samples needed to reach strong performance.",
            "key_findings_grounding": "Restoring dense, scene-level visual semantics (HSSE) and enriching textual context via LLM-grounded augmentation (LSE) address two orthogonal grounding failure modes—semantic sparsity from sparse point sampling and ambiguity in concise human descriptions—yielding consistent ACC@25 improvements (≈+5–6 pts overall) and substantially better performance on hard scenes and zero-shot cross-dataset generalization.",
            "uuid": "e1968.0"
        },
        {
            "name_short": "HSSE",
            "name_full": "Hierarchical Scene Semantic Enhancer",
            "brief_description": "A hierarchical module that (1) aggregates per-view multi-scale semantic features into pooled view-level tokens, (2) performs scene-level multi-view self-attention jointly with language tokens to form global scene semantics, and (3) broadcasts these semantics back into multi-scale spatial feature maps via cross-attention to mitigate semantic loss from sparse point sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "HSSE (Hierarchical Scene Semantic Enhancer)",
            "model_description": "Three-stage module: (a) view-level semantic aggregation — FPN + adaptive average pooling to produce pooled queries followed by cross-attention and self-attention within each view; (b) scene-level semantic interaction — concatenation of pooled view tokens with language features and L_scene layers of self-attention to produce scene-global tokens and refined language tokens; (c) semantic broadcast — cross-attention from multi-scale spatial features (queries) to scene-global tokens (keys/values) to re-infuse global semantics into spatial maps. An aggregation-broadcast mechanism filters irrelevant information.",
            "visual_encoder_type": "N/A (module operates on features from ResNet-50 2D encoder and MinkNet34 3D encoder)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Enables grounding by creating scene-level visual tokens that directly attend to and interact with language tokens (self-attention) and then broadcasting enriched semantics back to spatial feature maps via cross-attention, improving alignment between text and visual features.",
            "representation_level": "Scene-level global semantic tokens plus multi-scale spatial feature maps (multi-level representation).",
            "spatial_representation": "Operates on lifted multi-scale 2D semantic feature maps that have been projected into 3D spatial context; not a new explicit coordinate representation but re-infuses semantic context into spatial feature maps used for point-based grounding.",
            "embodied_task_type": "3D visual grounding (ego-centric)",
            "embodied_task_name": "EmbodiedScan (used within DenseGrounding)",
            "visual_domain": "Egocentric multi-view RGB-D",
            "performance_metric": "ACC@25",
            "performance_value": "HSSE alone (added to enhanced baseline) yields +2.09% ACC@25 improvement in ablation (Table 3 shows baseline -&gt; +HSSE gives Overall 39.25% vs baseline 35.77% on mini experiments).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Ablation baseline without HSSE (and without LSE) overall ACC@25 35.77% (Table 3 baseline).",
            "grounding_improvement": "HSSE alone gives +2.09% ACC@25 vs baseline (Table 3). When combined with LSE, HSSE contributes part of the combined +5.57% improvement.",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "HSSE was explicitly designed to address the semantic loss caused by sparse sampling (~2% of reconstructed point cloud points retained), by producing and broadcasting dense scene-level semantic tokens into spatial maps to recover fine-grained details.",
            "failure_mode_analysis": "Paper shows HSSE reduces failures on small and similar objects by improving fine-grained semantics; ablations on the number of self-attention layers and pooled token spatial size indicate sensitivity (Tables 4 and 5) — too small pooled map loses information, too large adds redundancy.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "View-level cross-attention and self-attention to form pooled view tokens; scene-level self-attention joining view tokens and language; semantic broadcast via cross-attention to spatial feature maps.",
            "sample_efficiency": null,
            "key_findings_grounding": "Restoring scene-level dense semantics via hierarchical aggregation + cross-modal interaction effectively mitigates semantic loss from sparse point sampling and improves grounding, especially for small/distractor-heavy (hard) scenes; HSSE alone yields measurable gains (~+2% ACC@25 in ablation).",
            "uuid": "e1968.1"
        },
        {
            "name_short": "LSE",
            "name_full": "Language Semantic Enhancer",
            "brief_description": "An LLM-based pipeline that augments training-language descriptions by grounding LLM prompts with a Scene Information Database (SIDB) containing object relationships and location info to produce disambiguating, anchor-rich textual augmentations for training.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "LSE (Language Semantic Enhancer)",
            "model_description": "Constructs a Scene Information Database (SIDB) from training annotations (object labels, relationships, bounding-box locations); for each raw description, selects up to k scene-context descriptions (k=50) and prompts an LLM (GPT-4o mini in experiments) to generate enriched descriptions that preserve the original content while adding anchors (object relationships and location references). The augmented text is used only during training (inference uses original raw descriptions).",
            "visual_encoder_type": "N/A (text augmentation component)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Improves grounding indirectly by enriching language inputs used during training, providing stronger anchors and disambiguation so learned cross-modal alignment is more robust to similar-object confusion.",
            "representation_level": "Language-level augmentation affecting downstream token alignments to multi-scale visual features and scene tokens.",
            "spatial_representation": "Uses SIDB entries that include object location information (3D bounding-box positions) to ground textual augmentations; thus LSE leverages explicit 3D location metadata to generate spatially-anchored text.",
            "embodied_task_type": "3D visual grounding (training-time augmentation)",
            "embodied_task_name": "EmbodiedScan (used to construct SIDB and generate augmented training descriptions)",
            "visual_domain": "Egocentric indoor scenes (used only for constructing SIDB/context for LLM)",
            "performance_metric": "ACC@25",
            "performance_value": "LSE alone (with DB(R+L)) yields +2.45% ACC@25 over naive baseline in LSE ablations (Table 2 shows LLM+DB(R+L) overall 38.82% vs naive baseline constrained to 25% data). In Table 3 ablation on mini set, LSE alone increases overall from 35.77% to 39.02% (+3.25% in that table's configuration).",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Baseline without LSE (and without HSSE) overall ACC@25 35.77% (Table 3 baseline).",
            "grounding_improvement": "LSE (LLM+DB(R+L)) yields ~+2.45% to +3.48% ACC@25 depending on subset and comparison (Table 2 and Table 3 results show consistent single-component gains). Combined with HSSE the improvement is additive (total +5.57% in Table 3).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "LSE targets the textual ambiguity bottleneck: short/concise human descriptions lacking anchors cause mis-grounding among similar objects; LSE augments descriptions with anchors drawn from SIDB to reduce this ambiguity.",
            "failure_mode_analysis": "Paper reports LSE reduces confusion among similar-looking objects; Table 2 shows progressive improvements from simple concatenation -&gt; template LLM -&gt; LLM+DB(R) -&gt; LLM+DB(R+L), indicating that including both relationship and location info in SIDB yields the best disambiguation effect.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Language augmentation pipeline provides richer textual tokens which are then encoded (CLIP text encoder) and participate in HSSE scene-level self-attention and downstream matching in the DETR decoder (i.e., late cross-modal fusion via attention).",
            "sample_efficiency": "LSE increases training efficiency: on limited data experiments LSE contributes to better performance when training data is scarce (combined with HSSE leads to larger relative gains on mini splits).",
            "key_findings_grounding": "LLM-grounded text augmentation using a Scene Information Database that includes object relationships and 3D location information produces more informative, anchor-rich training descriptions that measurably reduce ambiguity-induced grounding errors and contribute ~+2–3.5 percentage points ACC@25 as a standalone component.",
            "uuid": "e1968.2"
        },
        {
            "name_short": "EmbodiedScan",
            "name_full": "EmbodiedScan (baseline)",
            "brief_description": "A prior state-of-the-art ego-centric multi-view RGB-D 3D visual grounding method that decouples RGB semantic encoding and depth-based geometric encoding, lifts 2D semantics into 3D using intrinsics/extrinsics, samples a sparse set of points for fusion, and uses a decoder for grounding.",
            "citation_title": "EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI",
            "mention_or_use": "use",
            "model_name": "EmbodiedScan",
            "model_description": "Decoupled encoder architecture: separate 2D semantic encoder and 3D geometric encoder; 2D semantic features are projected into 3D point cloud using camera intrinsics/extrinsics and concatenated with geometric features; only a sparse subset (~2%) of points is sampled for downstream processing due to compute constraints; semantic features are fused and decoded for 9-DoF bounding-box grounding.",
            "visual_encoder_type": "2D semantic encoder and 3D geometric encoder (original paper used standard encoders; DenseGrounding re-evaluates EmbodiedScan weights and builds on it)",
            "visual_encoder_pretraining": "EmbodiedScan's pretraining details are referenced but exact datasets not restated in this paper (DenseGrounding follows EmbodiedScan training settings for encoder pretraining).",
            "grounding_mechanism": "Lift-and-match: lift 2D semantic features to 3D points, concatenate with geometry, fuse and decode; grounding primarily performed by matching visual tokens to language tokens in a decoder pipeline.",
            "representation_level": "3D point-cloud centric (sparse point-level features) with lifted 2D semantics",
            "spatial_representation": "Depth-reconstructed point cloud and 3D bounding boxes",
            "embodied_task_type": "Ego-centric 3D visual grounding",
            "embodied_task_name": "EmbodiedScan benchmark",
            "visual_domain": "Egocentric multi-view RGB-D",
            "performance_metric": "ACC@25",
            "performance_value": "Reported EmbodiedScan (re-evaluated) Full: 39.28% ACC@25 overall (Table 1); Mini split: 34.09% ACC@25 overall (Table 1).",
            "has_grounding_ablation": null,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "EmbodiedScan (and prior methods) sample only a sparse subset (~2%) of reconstructed point-cloud points during fusion, which DenseGrounding identifies as causing significant loss of fine-grained visual semantics and grounding failures, especially for small objects.",
            "failure_mode_analysis": "Baseline struggles on 'Hard' scenes with ≥3 objects of the same class and with ambiguous descriptions; DenseGrounding shows larger improvements in these scenarios.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Lift 2D semantics to 3D via intrinsics/extrinsics, concatenate with geometry, and fuse via FPN before decoding.",
            "sample_efficiency": null,
            "key_findings_grounding": "EmbodiedScan established the sparse lift-and-fuse baseline for ego-centric 3D grounding but is limited by semantic sparsity from extreme point sampling; methods that restore dense semantics and enrich language mitigate these bottlenecks.",
            "uuid": "e1968.3"
        },
        {
            "name_short": "LLM+DB(R+L)",
            "name_full": "LLM augmentation with Scene Information Database (Relationships + Location)",
            "brief_description": "Training-time text augmentation strategy that prompts an LLM with scene-context descriptions from a Scene Information Database containing object relationships and 3D location metadata to produce disambiguating, anchor-rich language augmentations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLM+DB(R+L)",
            "model_description": "Select up to k (k=50) context descriptions from SIDB for the same scene and anchor/target classes, include relationship and location information, and prompt an LLM (GPT-4o mini used in experiments) to produce enriched paraphrases that preserve original content while adding anchors; augmented texts used only during training.",
            "visual_encoder_type": "N/A (text augmentation)",
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Indirect: improves the language side of cross-modal alignment so learned grounding is less ambiguous and more robust to similar-object distractors.",
            "representation_level": "Language-level augmentation affecting token-level grounding interactions",
            "spatial_representation": "Uses object location metadata (3D bounding box centers) from SIDB to ground generated text in spatial terms",
            "embodied_task_type": "3D visual grounding (training augmentation)",
            "embodied_task_name": "EmbodiedScan (used to build SIDB)",
            "visual_domain": "Egocentric indoor scenes (used only as context for LLM prompts)",
            "performance_metric": "ACC@25",
            "performance_value": "Ablation (Table 2) shows LLM+DB(R+L) achieves 38.82% ACC@25 (comparison constrained to 25% data for fairness) vs ConcatSamples 36.37% and naive LLM 37.23% — showing the best single-method gain among tested text augmentations on that split.",
            "has_grounding_ablation": true,
            "performance_without_grounding": "Naive baseline (Concat Samples) in the constrained 25%-data ablation: 36.37% ACC@25 (Table 2).",
            "grounding_improvement": "LLM+DB(R+L) outperforms naive concatenation and template LLM augmentation by ~1.6–2.5 percentage points in the ablated 25% experiments (Table 2).",
            "has_encoder_comparison": false,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": false,
            "perception_bottleneck_details": null,
            "failure_mode_analysis": "Directly addresses failure mode due to ambiguous descriptions; ablation shows incremental benefit from adding relationships and then locations (R+L) in SIDB.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Augmented language tokens are encoded (CLIP text encoder) and used in cross-modal attention/decoder matching—no change to visual pipeline.",
            "sample_efficiency": "Contributes to sample efficiency; in low-data ablations LLM+DB(R+L) gives the largest gains among text-augmentation variants (Table 2).",
            "key_findings_grounding": "LLM-guided textual augmentation grounded with scene relationships and explicit location metadata yields the strongest reduction in language ambiguity among tested augmentation methods, improving ACC@25 by ~1.6–2.5 pts in constrained-data ablations and contributing substantially to combined system gains.",
            "uuid": "e1968.4"
        },
        {
            "name_short": "Sparse Sampling Bottleneck",
            "name_full": "Sparse Point Sampling Semantic Bottleneck",
            "brief_description": "Identified failure mode where reconstructing full scene point clouds is infeasible at runtime and extreme subsampling (~2% of points) discards most lifted 2D semantic features, causing loss of fine-grained semantics and grounding errors.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Sparse Point Sampling Bottleneck",
            "model_description": "During depth-to-point reconstruction and fusion, computational limits force sampling only a small fraction (~2%) of the full point cloud; semantic features corresponding to unsampled points are discarded, producing a semantic sparsity bottleneck that harms detection/grounding of small or closely clustered objects.",
            "visual_encoder_type": null,
            "visual_encoder_pretraining": null,
            "grounding_mechanism": "Negative-impact mechanism: loss of lifted 2D semantics reduces the fidelity of visual tokens available for matching with language, degrading grounding.",
            "representation_level": "Point-cloud level (sparse 3D points a small subset of full scene)",
            "spatial_representation": "3D point cloud with heavy subsampling (~2% retained)",
            "embodied_task_type": "3D visual grounding (embodied perception bottleneck)",
            "embodied_task_name": "Observed in EmbodiedScan and addressed by DenseGrounding on EmbodiedScan benchmark",
            "visual_domain": "Egocentric RGB-D reconstructions",
            "performance_metric": "Indirectly measured via improvements when mitigated (ACC@25)",
            "performance_value": "Not a model performance value; mitigation via HSSE contributes ~+2.09% ACC@25 (Table 3) and combined improvements indicate the bottleneck materially reduces baseline accuracy by several percentage points.",
            "has_grounding_ablation": false,
            "performance_without_grounding": null,
            "grounding_improvement": null,
            "has_encoder_comparison": null,
            "encoder_comparison_results": null,
            "perception_bottleneck_identified": true,
            "perception_bottleneck_details": "Sampling only ~2% of reconstructed points discards most per-pixel/lifted semantic features; this specifically degrades performance on small objects and scenes with many similar objects.",
            "failure_mode_analysis": "Attributed to missed fine-grained semantics and errors in distinguishing similar objects; mitigated by HSSE's scene-level semantic broadcast which reinjects dense semantics into the sampled maps, improving ACC@25 by measurable amounts in ablations.",
            "domain_shift_handling": null,
            "novel_object_performance": null,
            "frozen_vs_finetuned": null,
            "pretraining_scale_effect": null,
            "fusion_mechanism": "Problem arises from lift-&gt;sample-&gt;discard workflow; solution uses HSSE to broadcast scene-wise semantics back into retained spatial maps.",
            "sample_efficiency": null,
            "key_findings_grounding": "Extreme subsampling of reconstructed point clouds is a major bottleneck; restoring dense scene-level semantics (HSSE) recovers significant performance lost to sampling and improves grounding robustness in cluttered/hard scenes.",
            "uuid": "e1968.5"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI",
            "rating": 2
        },
        {
            "paper_title": "ViewRefer: Grasp the Multi-View Knowledge for 3D Visual Grounding",
            "rating": 2
        },
        {
            "paper_title": "Multi-View Transformer for 3D Visual Grounding",
            "rating": 2
        },
        {
            "paper_title": "3D-SPS: Single-Stage 3D Visual Grounding via Referred Point Progressive Selection",
            "rating": 1
        },
        {
            "paper_title": "RefMask3D: Language-Guided Transformer for 3D Referring Segmentation",
            "rating": 1
        },
        {
            "paper_title": "DenseG: Alleviating Vision-Language Feature Sparsity in Multi-View 3D Visual Grounding",
            "rating": 1
        }
    ],
    "cost": 0.02123075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>DENSEGROUNDING: IMPROVING DENSE LANGUAGE-VISION SEMANTICS FOR EGO-CENTRIC 3D VISUAL GROUNDING
8 May 2025</p>
<p>Henry Zheng jh-zheng22@mails.tsinghua.edu.cn 
Department of Automation
Tsinghua University
BNRist</p>
<p>Hao Shi 
Department of Automation
Tsinghua University
BNRist</p>
<p>Qihang Peng 
Department of Automation
Tsinghua University
BNRist</p>
<p>Yong Xien Chng 
Department of Automation
Tsinghua University
BNRist</p>
<p>Rui Huang 
Department of Automation
Tsinghua University
BNRist</p>
<p>Yepeng Weng 
AI Lab
Lenovo Research</p>
<p>Zhongchao Shi 
AI Lab
Lenovo Research</p>
<p>Gao Huang gaohuang@tsinghua.edu.cn 
Department of Automation
Tsinghua University
BNRist</p>
<p>DENSEGROUNDING: IMPROVING DENSE LANGUAGE-VISION SEMANTICS FOR EGO-CENTRIC 3D VISUAL GROUNDING
8 May 2025FFD58BAC8B3B5D98B2D08E7E6FD94DFBarXiv:2505.04965v1[cs.CV]
Enabling intelligent agents to comprehend and interact with 3D environments through natural language is crucial for advancing robotics and human-computer interaction.A fundamental task in this field is ego-centric 3D visual grounding, where agents locate target objects in real-world 3D spaces based on verbal descriptions.However, this task faces two significant challenges: (1) loss of fine-grained visual semantics due to sparse fusion of point clouds with ego-centric multi-view images, (2) limited textual semantic context due to arbitrary language descriptions.We propose DenseGrounding, a novel approach designed to address these issues by enhancing both visual and textual semantics.For visual features, we introduce the Hierarchical Scene Semantic Enhancer, which retains dense semantics by capturing fine-grained global scene features and facilitating cross-modal alignment.For text descriptions, we propose a Language Semantic Enhancer that leverages large language models to provide rich context and diverse language descriptions with additional context during model training.Extensive experiments show that DenseGrounding significantly outperforms existing methods in overall accuracy, with improvements of 5.81% and 7.56% when trained on the comprehensive full dataset and smaller mini subset, respectively, further advancing the SOTA in egocentric 3D visual grounding.Our method also achieves 1st place and receives Innovation Award in the CVPR 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track, validating its effectiveness and robustness.Code</p>
<p>INTRODUCTION</p>
<p>Recent years have seen increasing attention in the field of embodied AI, with the introduction of 3D visual grounding benchmarks (Achlioptas et al., 2020;Chen et al., 2020;Wang et al., 2024) prompting a surge of research works (Guo et al., 2023;Wu et al., 2023;Zhao et al., 2021;Jain et al., 2022;Yang et al., 2024;Huang et al., 2022;Wang et al., 2024).The task of 3D visual grounding, which aims to locate target objects in real-world 3D environments based on natural language descriptions, is a fundamental perception task for embodied agents.This capability is crucial for enabling agents to interact with and understand their surroundings through language, facilitating applications in robotics and human-computer interaction.For instance, accurate 3D visual grounding empowers robots to perform tasks such as object retrieval and manipulation based on verbal instructions, enhancing their functionality in service and assistive scenarios.Despite these advances, significant challenges continue to hinder the performance of 3D perception systems.One major challenge lies in how embodied agents perceive their environment, as they typically rely on ego-centric observations from multiple views while moving around, lacking a holistic, scene-level perception.Some methods attempt to enhance scene-level understanding using reconstructed scene-level 3D point clouds (Wu et al., 2023;Zhao et al., 2021;Jain et al., 2022;Yang et al., 2024;Guo et al., 2023;Huang et al., 2022;2024b;c) following the previous 3D perception methods (Wu et al., 2024b;Jiang et al., 2020), but these approach are impractical in real-world applications where such comprehensive scene-level information is not readily available.In this context, EmbodiedScan (Wang et al., 2024) have emerged, utilizing multi-view ego-centric RGB-D scans to address the need for models to understand scenes directly from sparse views.It decouples the encoding of RGB images and depth-reconstructed point clouds from ego-centric views to extract both semantic and geometric information.Semantic features are then projected onto 3D space using intrinsic and extrinsic matrices to create a semantically enriched sparse point cloud for bounding box regression.However, due to the high number of points in the reconstructed point cloud and computational limitations, only a sparse subset (around 2%) is sampled.This sampling results in a significant loss of visual semantics, limiting the model's ability to retain fine-grained object details, especially for smaller objects, and ultimately hindering grounding performance.</p>
<p>Another challenge is the ambiguity in natural language descriptions found in existing datasets (Chen et al., 2020;Achlioptas et al., 2020;Wang et al., 2024).Humans naturally provide concise instructions, often lacking critical reference points or anchor objects, which leads to inherent ambiguities.For example, a command like "Please get the water bottle on top of the table" does not specify which table or provide additional context in environments with multiple similar objects.This conciseness makes it difficult for models to disambiguate between similar objects within complex scenes (Wang et al., 2024).Furthermore, the increased complexity of scenes in new datasets-such as a more similar objects instances in a single scene-exacerbates this issue.In result, training on these data may cause model to have suboptimal performance.While previous attempts have been made to enrich descriptions using large language models with predefined templates (Guo et al., 2023) or by concatenating related descriptions (Wang et al., 2024), these methods have not fully mitigated the ambiguities present in the annotations and have not utilize the available informations in the dataset.</p>
<p>Addressing these challenges is crucial for advancing the field of embodied AI perception and enhancing the practical utility of 3D visual grounding in real-world applications.In response to these challenges, we propose DenseGrounding, a novel method for multi-view 3D visual grounding that alleviates the sparsity in both visual and textual features.Specifically, to address the loss of finegrained visual semantics, we introduce the Hierarchical Scene Semantic Enhancer (HSSE), which enriches visual representations with global scene-level semantics.HSSE operates hierarchically, starting with view-level semantic aggregation that integrates multi-scale features from RGB images within each view to capture detailed view-level semantics.Then, HSSE performs scene-level semantic interaction to combine these aggregated multi-view semantics with language features from descriptions, promoting cross-modal fusion and a holistic understanding of the scene.Finally, HSSE conducts semantic broadcast that infuses the enriched global scene-level semantics into the multiscale feature maps of the depth-reconstructed point cloud, improving their fine-grained semantics.</p>
<p>To mitigate the ambiguity in natural language descriptions used for training, for the text modality, we propose the Language Semantic Enhancer (LSE) that constructs a scene information database based on the existing dataset, EmbodiedScan.This database captures crucial information about object relationships and locations within the scene.During training, the LSE leverages LLM to enriches input descriptions with additional anchors and context, reducing confusion among similar objects and providing more robust representations by incorporating external knowledge.This enrichment increases the semantic richness of the language descriptions and reduces inherent ambiguities.</p>
<p>By addressing the ambiguities in annotated text and enhancing the model's capacity to capture scenelevel global visual semantics from ego-centric multi-view inputs, DenseGrounding, advances the state-of-the-art in 3D visual grounding.Extensive experiments demonstrate the effectiveness of our approach, highlighting the importance of enriched textual descriptions and bidirectional interaction between text and visual modalities in overcoming the limitations of previous methods.</p>
<p>Our main contributions are as follows:</p>
<p>• We develop a novel Language Semantic Enhancer (LSE) that employs LLM-assisted description augmentation to increase semantic richness and significantly reduce ambiguities in natural language descriptions.By leveraging an LLM grounded in a scene information database, our approach enriches the diversity and contextual clarity of the textual features.</p>
<p>• We introduce a Hierarchical Scene Semantic Enhancer (HSSE) that effectively aggregates ego-centric multi-view features and captures fine-grained scene-level global semantics.HSSE enables interaction between textual and visual features, ensuring the model captures both global context and detailed object semantics from ego-centric inputs.</p>
<p>• Our method achieves state-of-the-art performance on the EmbodiedScan benchmark, substantially outperforming existing approaches.We secured first place in the CVPR 2024 Autonomous Driving Grand Challenge Track on Multi-View 3D Visual Grounding (Zheng et al., 2024), demonstrating the practical effectiveness and impact of our contributions.</p>
<p>RELATED WORKS</p>
<p>3D Visual Grounding.The integration of language and 3D perception for scene understanding is crucial in embodied AI, enabling agents to navigate, understand, and interact with complex realworld environments.Key tasks (Chen et al., 2020;Achlioptas et al., 2020;Azuma et al., 2022;Chen et al., 2021) in this domain, 3D Visual Grounding (3DVG) (Wu et al., 2023;Jain et al., 2022;Zhu et al., 2023b;Huang et al., 2022;2024a;Cai et al., 2022), 3D Captioning (Huang et al., 2024a;Chen et al., 2023;Cai et al., 2022) and 3D Question Answering (3DQA) (Huang et al., 2024a), combine natural language understanding with spatial reasoning to advance multimodal intelligence in 3D spaces.Our work focuses on ego-centric 3D visual grounding tasks that integrate multimodal data to localize objects in 3D space.</p>
<p>Major 3D visual grounding methods can be divided into one-stage and two-stage architectures.Onestage approaches (Chen et al., 2018;Liao et al., 2020;Luo et al., 2022;Geng &amp; Yin, 2024;He &amp; Ding, 2024) fuse textual and visual features in a single step, enabling end-to-end optimization and faster inference.Recent works have optimized single stage methods in various ways: 3D-SPS (Luo et al., 2022) progressively selects keypoints guided by language; ViewInfer3D (Geng &amp; Yin, 2024) uses large language models (LLMs) to infer embodied viewpoints; and RefMask3D (He &amp; Ding, 2024) integrates language with geometrically coherent sub-clouds via cross-modal groupword attention, producing semantic primitives that enhance vision-language understanding.Two-stage approaches (Yang et al., 2019;Achlioptas et al., 2020;Huang et al., 2022;Guo et al., 2023;Wu et al., 2024a;Chang et al., 2024) first use pre-trained object detectors (Jiang et al., 2020;Wu et al., 2024b) to generate object proposals or utilize ground truth bounding boxes, which are then matched with the linguistic input.Advances in this framework include the Multi-View Transformer (Huang et al., 2022), projecting the 3D scene into a holistic multi-view space; ViewRefer (Guo et al., 2023), leveraging LLMs to expand grounding texts and enhancing cross-view interactions with inter-view attention; and MiKASA Transformer (Chang et al., 2024), introducing a scene-aware object encoder and a multi-key-anchor technique to improve object recognition and spatial understanding.In our paper, we target the more challenging setting of single-stage methods.</p>
<p>LLMs for Data Augmentation.Recent advances in large language models (LLMs) (Achiam et al., 2023;Touvron et al., 2023;Du et al., 2021) have demonstrated remarkable capabilities.Fully leveraging the power of LLMs beyond language processing, some recent studies have successfully integrated them with other modalities, leading to the development of highly effective multi-modal methods (Moon et al., 2022;Guo et al., 2023;Feng et al., 2024) for vision-language tasks.</p>
<p>However, directly utilizing LLM or VLMs in robotics perception tasks remains a research question (Kim et al., 2024;Zhen et al., 2024;Phan et al., 2024).Therefore, leveraging LLMs to directly implement data augmentation and maximizing their potential to enhance text data diversity become a new trend (Peng et al., 2023;Khan et al., 2023;Dunlap et al., 2023;Guo et al., 2023;Ding et al., 2024).Among them, ALIA (Dunlap et al., 2023) leverages LLMs to generate image descriptions and guide language-based image modifications, augmenting the training data.This approach surpasses traditional data augmentation techniques in fine-grained classification, underscoring the value of LLM-driven augmentation in visual tasks.In 3D visual grounding, although ViewRefer (Guo et al., 2023) employs LLMs to augment textual data through rephrasing view-dependent descriptions from varying perspectives, however the data samples are augmented individually and suffer from limited textual semantic information.To obtain high-quality description augmentations that disambiguate among similar items and more robust representations, we utilize LLMs to enrich input descriptions utilizing readily available information in the dataset as additional context.</p>
<p>PRELIMINARIES</p>
<p>EGO-CENTRIC 3D VISUAL GROUNDING</p>
<p>In real-world scenarios, intelligent agents perceive their environment without any prior scene knowledge.They often rely on ego-centric observations, such as multi-view RGB-D images, rather than pre-established scene-level priors like pre-reconstructed 3D point clouds of the entire scene, as commonly used in previous studies (Wu et al., 2024b;Huang et al., 2024b;Chng et al., 2024).</p>
<p>Following Wang et al. (2024), we formalize the ego-centric 3D visual grounding task as follows: Given a language description L ∈ R T , together with V views of RGB-D images
{(I v , D v )} V v=1
, where I v ∈ R H×W ×3 represents the RGB image and D v ∈ R H×W denotes the depth image of the v-th view along with their corresponding sensor intrinsics
{(K I v , K D v )} V v=1 and ex- trinsics {(T I v , T D v )} V v=1
, the objective is to output a 9-degree-of-freedom (9DoF) bounding box B = (x, y, z, l, w, h, θ, ϕ, ψ).Here, (x, y, z) are the 3D coordinates of the object's center, (l, w, h) are its dimensions, and (θ, ϕ, ψ) are its orientation angles.The task is to determine B such that it accurately corresponds to the object described by L within the scene represented by {(I v , D v )} V v=1 .</p>
<p>OVERVIEW OF THE ARCHITECTURE</p>
<p>We adopt and improve upon the previous SOTA in ego-centric 3D visual grounding (Wang et al., 2024).To prevent geometric information from interfering with semantic extraction and fully leverage each modality's strengths, Wang et al. (2024) decouple the encoding of input RGB and depth signals from each ego-centric views, following Liu et al. (2023).Specifically, the depth data from each view is transformed into a partial point cloud, which is then integrated into a holistic 3D point cloud P ∈ R N ×3 using global alignment matrices, thereby preserving precise geometric details.</p>
<p>Next, a semantic encoder and a geometric encoder extract multi-scale semantic and geometric features from {I v } V v=1 and P , respectively, denoted as
{F s Sem } S s=1 ∈ R V ×Hs×Ws×Cs and {F s Geo } S s=1 ∈ R N ×Cs .
Here, N is the total number of points sampled from all perspective views, and S is the number of scale for the encoded feature maps.</p>
<p>The semantic features from RGB data are then lifted to the 3D space using intrinsic and extrinsic matrices {(K I v , T I v )} V v=1 , and concatenated with geometric features at multiple scales, followed by fusion through a Feature Pyramid Network (FPN).However, due to the large number of points in the depth-reconstructed point cloud and computational limitations, only a sparse subset (about 2%) is sampled, and the semantic features at unsampled locations are discarded, resulting in significant semantic information loss.To address this issue, we introduce a Hierarchical Scene Semantic Enhancer module (detailed in Sec.4.1) to improve scene-level semantics.The enhanced features are Sematic Broadcast  then fed into the decoder, which remains unchanged from the previous work, where the visual tokens most similar to the text features are selected as queries.</p>
<p>METHOD</p>
<p>In this section, we present our proposed method, DenseGrounding, for ego-centric 3D visual grounding.As shown in Figure 2, our method consists of three key components: Hierarchical Scene Semantic Enhancer (Sec.4.1), LLM-based Language Semantic Enhancer (Sec.4.2), and an Enhanced Baseline model (Sec.4.3).We will describe each component in detail in the following subsections.</p>
<p>HIERARCHICAL SCENE SEMANTIC ENHANCER</p>
<p>To mitigate the semantic loss inherent in sparse point clouds, we introduce the Hierarchical Scene Semantic Enhancer (HSSE) module, which extracts individual view semantics and enriches visual representation with global scene-level semantics.This enriched information is then unprojected to the depth reconstructed point cloud during fusion, minimizing the semantic loss.</p>
<p>Our proposed HSSE module works in a hierarchical manner, starting with view-level semantic aggregation to capture view-level semantics from individual egocentric perspective views.Then, it fuses these aggregated view semantics with language semantics, facilitating scene-level multi-view semantic interaction and cross-modal feature fusion.Moreover, a broadcast module integrates these scene-level semantics into multi-scale semantic feature maps, enriching the fine-grained semantics in the sparse point cloud.Finally, the HSSE employs an aggregation-broadcast mechanism to filter out irrelevant information, ensuring efficient and streamlined semantic transfer to the point cloud for enhanced performance.</p>
<p>View-Level Semantic Aggregation.With the encoded 2D features from each view, HSSE performs view-level semantic aggregation to capture view-level global semantics within each view.For the v-th view, given the multi-scale semantic features {F v,s Sem } S s=1 ∈ R Hs×Ws×Cs extracted from RGB image, we aim to capture the salient semantic cues in each view.</p>
<p>First, these semantic features are processed through a Feature Pyramid Network (FPN) to integrate information across different scales.A specific scale s of the feature map is then selected as the reference feature
F v Ref ∈ R Hs×Ws×Cs . F v Ref = Conv s i=1 Upsample(F v,i Sem ) ∈ R Hs×Ws×Cs (1)
Next, adaptive average pooling is applied to the reference feature to reduce its spatial dimensions, yielding the initial query features
F v Q ∈ R hs×ws×Cs . F v Q = Pool(F v Ref ) ∈ R hs×ws×Cs (2)
Subsequently, we compute the cross-attention between the pooled queries and the reference feature to aggregate semantic information at the view level.
F v Q = CrossAttn(Q = F v Q , K = F v Ref , V = F v Ref )(3)
where F v Q ∈ R hs×ws×Cs encapsulates the global semantics of the view.We then apply self attention layer, to further refine the features and model intra-view relationships.
F v Q = SelfAttn(Q = F v Q , K = F v Q , V = F v Q )(4)
Scene-Level Semantic Interaction.Moreover, HSSE conducts scene-level semantic interaction by integrating the aggregated multi-view semantics { F v Q } V v=1 with the language semantics F Lang derived from the language description.To promote cross-modal fusion and scene-level interaction among multi-view semantics, we concatenate { F v Q } V v=1 with F Lang and apply L scene layers of self-attention.
[{ F v ′ Q } V v=1 , F ′ Lang ] = SelfAttn Lscene ([{ F v Q } V v=1 ; F Lang ]),(5)
where
{ F v ′ Q } V v=1
captures the scene-level global semantics, and F ′ Lang represents the refined language semantics after the interaction.</p>
<p>Semantic Broadcast.Finally, HSSE broadcasts the scene-level global semantics F v ′ Q back to the multi-scale semantic feature maps {F s,v Sem } S s=1 ∈ R Hs×Ws×Cs for each view.Specifically, we compute cross-attention between the spatial features {F s,v Sem } S s=1 and the scene-level semantics F v ′ Q , treating the spatial features as queries.
{ F s,v Sem } S s=1 = CrossAttn(Q = {F s,v Sem } S s=1 , K = F v ′ Q , V = F v ′ Q ) (6)
where { F s,v Sem } S s=1 represents the enhanced feature maps enriched with scene-level global semantics.</p>
<p>LLM-BASED LANGUAGE SEMANTIC ENHANCER</p>
<p>Human interactions with intelligent agents often involve casual and vague language descriptions, resulting in input that lacks clear anchors, limited textual context, and ambiguities.To address this, we propose a Language Semantic Enhancement (LSE) pipeline based on Large Language Models (LLMs) to enhance the training data.This pipeline enriches input language descriptions by leveraging LLMs, which are grounded with a Scene Information Database (SIDB) containing object locations and relationships, providing more contextual details of the corresponding scene.</p>
<p>Construct Scene Information Database (SIDB).Visual grounding datasets are typically built on indoor 3D detection datasets, where collected RGB-D images or point clouds are used to represent a scene.Each object within the scene is annotated with 3D bounding boxes, which visual grounding datasets extend by adding language descriptions detailing the object's relationship to its surroundings.To ensure accurate contextualization for LLMs augmenting these language descriptions, we propose to construct a Scene Information Database (SIDB) based on the annotated training set that is also used by all baseline methods for model training.</p>
<p>For each 3D scene, we follow the design of the datasets and assume that every language description targets a specific bounding box.These descriptions are grouped according to their corresponding scenes, allowing the SIDB to store detailed object relationships within each scene.This approach enables LLMs to better understand the spatial and relational dynamics of objects in a given scene, leading to a more holistic comprehension of the environment.</p>
<p>Moreover, to provide location information about the object instances in the scene, we enrich each of the descriptions in SIDB with position information from the original dataset annotations or pseudolabels from the detection model, associating each described object with its spatial location.These descriptions, combining both spatial and semantic information, form the backbone of the SIDB.By providing this structured and detailed knowledge, the SIDB allows LLMs to generate more contextually informed and reliable text descriptions grounded in the specific dynamics of each scene.</p>
<p>Prompt for LLM-based Enhancement.To effectively enrich the input descriptions, we design a prompt for the LLM that leverages the SIDB to generate more detailed and contextually rich descriptions.By instructing the LLM to utilize positional relationships among objects in the scene, the prompt ensures sufficient and reliable anchors, encouraging the description of the target object using multiple reference objects to minimize ambiguity and distractions.For instance, it draws on spatial context from the database to more accurately reflect the object's relationships to surrounding items.Moreover, the prompt guides the LLM to maintain consistency between the original and augmented text by preserving the original content while adding additional information, which reduces the risk of the generated text deviating from the initial prompt.</p>
<p>Enhance Description with LLM and SIDB.For each description to be augmented (raw description), we select the k most related context descriptions in the corresponding scene based on object names from the SIDB and provide them to the LLM.Details on the context description selection can be found in Appendix A.5.2.The LLM leverages the database to incorporate spatial and semantic details, aiming to enrich the text with additional contextual anchors that can help prevent confusion with similar-looking objects.</p>
<p>ENHANCED BASELINE</p>
<p>In this work, we introduce a strong baseline by building upon the state-of-the-art method established by EmbodiedScan for ego-centric multiview 3D visual grounding and further boosting its performance.More details about the proposed improvements can be found in the Appendix A.1.1.</p>
<p>EXPERIMENTS</p>
<p>Dataset and Benchmark.The EmbodiedScan dataset (Wang et al., 2024), used in our experiments, is a large-scale, multi-modal, ego-centric dataset for comprehensive 3D scene understanding.It consists of 5,185 scene scans sourced from well-known datasets such as ScanNet (Dai et al., 2017), 3RScan (Wald et al., 2019), and Matterport3D (Chang et al., 2017).This extensive dataset provides a diverse and rich foundation for 3D visual grounding tasks, offering broader scene coverage compared to prior datasets.This makes the dataset significantly larger and more challenging than previous ones, providing a more rigorous benchmark for 3D visual grounding tasks.</p>
<p>For benchmarking, the official dataset maintains a non-public test set for the test leaderboard and divides the original training set into new subsets for training and validation.In this paper, we refer to these as the training and validation sets, while the non-public test set is called the testing set.</p>
<p>Experimental Settings.Due to resource limitations, we reserve the full training dataset for baseline comparisons on the test set and leaderboard submissions to ensure a fair and comprehensive evaluation.For the "mini" data in the Data column of Table 1 and analysis experiments in Sec.5.2, we use a smaller subset of the data as a proxy task in performing experiments.The subset is referred to as mini sets, available through the official release by Wang et al. (2024).We report accuracy using the official metric, considering instances where IoU exceeds 0.25.Additionally, we present results for "Easy" and "Hard" scenes, classifying a scene as "Hard" if it contains 3 or more objects of the same class.The "Dep" and "Indep" metrics further challenge spatial understanding ability by assessing its performance with and without perspective-specific descriptions.</p>
<p>Implementation Details.We follow EmbodiedScan (Wang et al., 2024) by using feature encoders, sparse fusion modules, and a DETR-based (Carion et al., 2020) decoder.Specifically, we use ResNet50 (He et al., 2016) and MinkNet34 (Choy et al., 2019) as the 2D and 3D vision encoders, respectively.Moreover, as mentioned in Sec.4.3, we replace RoBERTA with CLIP (Radford et al., 2021) text encoder as language feature encoders.We first pre-train our image and point cloud encoders on the 3D object detection task, adhering to EmbodiedScan's training settings and integrating CBGS (Zhu et al., 2019).The LSE-augmented text is generated using GPT4-o mini and is utilized exclusively during training.For inference, our model processes descriptions directly, without any enhancement, aligning with our baseline methods for fair comparison.More details in App.A.1.2.</p>
<p>MAIN RESULTS</p>
<p>Evaluation.We evaluate the 3D visual grounding performance of our proposed method, DenseGrounding, and report the results in Table 1, where we compare it against established SOTA methods from the dataset benchmark.The table is divided into two sections: the upper half presents models trained on the full training set, while the lower half showcases performance using a mini training set, which is approximately 20% of the full dataset.</p>
<p>On the full training set, DenseGrounding achieves a significant improvement of 5.81% over the previous strongest baseline, EmbodiedScan.This substantial gain highlights the effectiveness of our approach in leveraging extensive training data to capture complex visual-linguistic correlations in 3D environments.It demonstrates our method's superior ability to understand and ground linguistic expressions in rich visual scenes.When trained on the more manageable mini-training set, we introduce an enhanced baseline, denoted as EmbodiedScan † , which achieves a 1.99% performance gain over the original EmbodiedScan.This stronger baseline ensures a more rigorous and fair comparison with our method.Remarkably, even against this enhanced baseline, DenseGrounding attains a substantial 5.57% improvement in overall accuracy, culminating in a total performance gain of 7.56% over the previous state-of-the-art.Specifically, on hard samples, where scenes contain more than three objects of the same class, we observe a 4.14% increase in accuracy.This demonstrates our method's proficiency in accurately identifying the correct target among similar-looking objects.As detailed in Table 1, DenseGrounding consistently outperforms the baselines across various evaluation categories, including both easy and hard samples, as well as in view-independent and view-dependent tasks.These consistent gains across different metrics underscore the robustness and generalizability of our approach in 3D visual grounding tasks.Moreover, our model not only achieves the highest performance among all methods but also secured 1st place in the CVPR 2024 Autonomous Driving Grand Challenge Track on Multi-View 3D Visual Grounding.</p>
<p>ANALYSIS EXPERIMENTS</p>
<p>Effectiveness of Each Component.We conduct an ablation analysis to assess the effectiveness of each component, as shown in Tab. 3. Our baseline is an enhanced model that excludes text augmentations and semantic enhancements.Introducing the augmented data led to a remarkable accuracy improvement of 3.48%, highlighting its significant impact.The further integration of the HSSE module provided an additional performance boost of 2.09%.Furthermore, the combined use of LSE and HSSE resulted in a 5.57% accuracy improvement on hard samples compared to the baseline, underscoring our model's enhanced capacity for both visual and language understanding, particularly in disambiguation of challenging cases.</p>
<p>Ablation on LSE.To evaluate the effectiveness of our proposed LSE, we conducted experiments comparing several text augmentation methods.The "Concat Samples" method from Embodied-Scan disambiguate descriptions by concatenating multiple annotations."LLM" refers to our reimplementation of template-based LLM augmentation used by Viewrefer (Guo et al., 2023).Our method, "LLM+DB(R)," employs LLM with SIDB incorporating object relationships only, while "LLM+DB(R+L)" extends this approach by adding location information from 3D bounding boxes.</p>
<p>It is worth noting that since "Concat Samples" method only has about 25% data, we limit other methods to use 25% data for fair comparisons.The results demonstrate that "LLM+DB(R+L)" achieves the notable over all improvement of 2.45% against naive baseline, confirming the effectiveness of incorporating both object relationships and location data in augmentation process.</p>
<p>Ablation on HSSE Components.We further explore the design of the HSSE module through comprehensive experiments, as presented in Tab. 4 and 5.In Tab. 4, we conduct an ablation study to determine the optimal number of self-attention layers needed for effective learning of the scenefeature representation.Additionally, Tab. 5 examines the impact of varying feature map sizes for the pooled F v Q feature.The results indicate that a small feature map size may be insufficient to capture the necessary information, while an excessively large feature map may include redundant details.Based on these findings, we adopted the best configuration for our final model.</p>
<p>QUALITATIVE ANALYSIS</p>
<p>We present qualitative results to illustrate the effectiveness of DenseGrounding in improving egocentric 3D visual grounding performance.Figure 3 shows a comparison between our model and our baseline model, EmbodiedScan.It can be clearly seen that our method outperforms the baseline in correctly identifying the target objects based on ambiguous descriptions.In cases where the baseline model struggles to disambiguate between multiple similar objects, DenseGrounding successfully detects the correct target by leveraging its enriched textual descriptions and robust cross-modal interactions.For instance, in environments where descriptions such as "select the keyboard that is close to  the fan" are provided, our model accurately localizes the keyboard, while the baseline misidentifies nearby objects.This improvement is attributed to HSSE, which captures both fine-grained details and provides better alignment with text, enabling better object identification.These qualitative results demonstrate the enhanced performance of DenseGrounding, especially in complex scenes with multiple distractors, solidifying its robustness and precision in real-world applications.</p>
<p>LIMITATIONS</p>
<p>While the DenseGrounding significantly improves the ego-centric 3D visual grounding task performance, it has limitations.In real-life applications, vague or ambiguous descriptions from human instructions pose challenges, as the model struggles without the necessary information to resolve ambiguities.The lack of clear instructions limits the effectiveness of the embodied agent.Future research should explore integrating human-agent interaction, allowing the model to query users for clarification, and improving adaptability and robustness in real-world scenarios.</p>
<p>CONCLUSION</p>
<p>In this work, we propose DenseGrounding, a novel approach to address the challenges of ambiguity in natural language descriptions and loss of fine-grained semantic features in multi-view 3D visual grounding.By leveraging LLMs for description enhancement and introducing the HSSE to enhance fine-grained visual semantics, our method significantly improves the accuracy and robustness of 3D visual grounding in ego-centric environments.Extensive experiments on the EmbodiedScan benchmark demonstrate that DenseGrounding outperforms existing state-of-the-art models, securing first place in the CVPR 2024 Autonomous Grand Challenge.These results highlight the importance of enriched language descriptions and effective cross-modal feature interactions for advancing embodied AI perception and enabling real-world applications in robotics and human-computer interaction.</p>
<p>A.3 ANALYSIS ON LIMITED DATA SCENARIO</p>
<p>In Figure 5, we present a comparative analysis of the performance between our method and the baseline under varying amounts of training data.The results clearly demonstrate that our method consistently outperforms the baseline, especially in scenarios with limited data availability.Notably, when trained on just 40% of the training data, our method surpasses the baseline model that was trained on 80% of the data.This highlights the data efficiency and robustness of our approach, indicating its effectiveness even when training data is scarce.Cross-dataset evaluation is essential for testing the generalization capabilities and robustness of egocentric 3D visual grounding methods.This process is particularly challenging due to differences in camera settings, scene layouts, and object characteristics in visual data across various datasets, which can significantly impact model performance.To further assess the zero-shot generalization of our approach, we reorganized the EmbodiedScan dataset to create a zero-shot setting.Specifically, we trained our model exclusively on scenes from 3RScan and Matterport3D and then evaluated its performance on ScanNet scenes, which were entirely unseen during training.This setup tests the model's ability to adapt to new environments with different camera settings and scene characteristics.</p>
<p>As shown in Table 6, our method significantly outperforms the baseline in this zero-shot setting, demonstrating superior robustness and the ability to generalize effectively to new and diverse scenes despite the inherent cross-dataset challenges.</p>
<p>A.5 IMPLEMENTATION DETAILS OF LSE</p>
<p>In this section, we provide the implementation details of the Language Semantic Enhancer (LSE) module, focusing on how the LLM is prompted.We delve into the specific prompts used and explain how information is selected to provide context to the LLM.</p>
<p>A.5.1 PROMPT FOR LSE MODULE</p>
<p>In this section, we present the prompts used in the Language Semantic Enhancer (LSE) module, along with an example input and output, as illustrated in Fig. 6.</p>
<p>Select the box that is in proximity to the printer, it is situated near the oven</p>
<p>Figure 1 :
1
Figure 1: (a) illustrates how limited context due to arbitrary descriptions leads to insufficient language semantics.(b) highlights the issue of losing fine-grained semantics in sparse fusion.</p>
<p>of the guitar, choose the curtain to the right of it With back to the guitar, select the curtain that is on the left of it, near the bed, between the picture and clock, and far away from</p>
<p>Figure 2 :
2
Figure 2: (a) shows overall framework, while (b) details the Language Semantic Enhancer (LSE) module, and (c) describes the Hierarchical Scene Semantic Enhancer (HSSE) module.</p>
<p>Figure 3 :
3
Figure 3: Qualitative Analysis.Comparison of Ground Truth, our baseline, and DenseGrounding.Ground truth boxes are shown in green, baseline in red, and DenseGrounding's predictions in blue.</p>
<p>Figure 5 :
5
Figure 5: Comparison of DenseGrounding and EmbodiedScan on limited data scenario.</p>
<p>Table 1 :
1
Wang et al. (2024) Accuracy performance of the models on the official full validation set.We follow the experimental setting proposed byWang et al. (2024)and use the RGB-D as visual input.The 'Data' column indicates the data split used.† denotes the improved baseline discussed in Sec.4.3.The results reported for EmbodiedScan are obtained by re-evaluating the validation set using the officially provided weights.ACC 25 ACC 25 ACC 25 ACC 25
Method ACC 25 ScanRefer (Chen et al., 2020) Easy Hard Data Full 13.78 9.12Indep Dep 13.44 10.77Overall 12.85BUTD-DETR (Jain et al., 2022)Full23.12 18.2322.47 20.9822.14L3Det (Zhu et al., 2023a)Full24.01 18.3423.59 21.2223.07EmbodiedScan (Wang et al., 2024) Full39.28 30.8838.23 39.3038.60DenseGroundingFull45.31 34.2344.40 44.4244.41(+6.03)(+3.35)(+6.17)(+5.12)(+5.81)EmbodiedScan (Wang et al., 2024) Mini34.09 30.2533.56 34.2033.78EmbodiedScan  †Mini36.23 30.5135.89 34.3035.77(+2.14)(+0.26)(+2.33)(+0.10)(+1.99)DenseGroundingMini41.95 34.3840.89 42.1941.34(+7.86)(+4.13)(+7.33)(+7.99)(+7.56)</p>
<p>Table 2 :
2
Ablation on LSE.R and L refers to object relationship and object location information in SIDB, respectively.
MethodEasy Hard OverallConcat Samples 36.80 31.44 36.37LLM37.69 31.97 37.23LLM+DB(R)38.48 33.54 38.08LLM+DB(R+L) 39.29 33.55 38.82</p>
<p>Table 3 :
3
Ablation of Proposed Methods.The reported values are Accuracies for predictions greater than 25% IoU with groundtruth.
LSE HSSE Easy Hard Overall35.77 30.5135.77✓39.58 32.6039.02✓39.67 34.4939.25✓✓41.95 34.3841.34</p>
<p>Table 4 :
4
Ablation on the number of self attention layers for HSSE.
L scene Easy Hard Overall140.15 35.4439.77240.76 32.7040.11341.95 34.3841.34441.06 33.9640.49640.66 33.2340.06</p>
<p>Table 5 :
5
Ablation on the view feature map size after pooling for HSSE.
Pooled Size Easy Hard Overall140.61 34.9140.15341.92 33.7541.26541.95 34.3841.34741.92 34.2841.30940.05 34.6039.61</p>
<p>Table 6 :
6
Zero-shot Cross Dataset Performance
MethodTraining ScenesTesting Scenes Easy Hard OverallEmbodiedScan3RScan+MatterportScannet9.957.949.81DenseGrounding 3RScan+MatterportScannet18.57 12.4218.14
ACKNOWLEDGMENTSThe work is supported in part by the National Key R&amp;D Program of China under Grant 2024YFB4708200 and the National Natural Science Foundation of China under Grant U24B20173.First, we replace the RoBERTa(Liu et al., 2019)language encoder with the CLIP(Radford et al., 2021)encoder.Given that visual grounding requires a deep understanding of both linguistic instructions and environmental visual features, strong cross-modal alignment is critical for success.CLIP, specifically designed for such tasks, provides superior alignment between language and vision, making it a natural fit for this application.Moreover, visual grounding often involves identifying a diverse range of objects, and the original EmbodiedScan model relies on a detection pipeline to pretrain the visual feature encoders.However, this approach can be hampered by the long-tailed distribution of objects in the pretraining dataset, leading to suboptimal detection performance.To address this, we incorporate the Class-Balanced Grouping and Sampling (CBGS) method(Zhu et al., 2019)during pretraining, which has proven effective in mitigating data imbalance and enhancing detection accuracy across rare and common object categories.A.1.2 HYPERPARAMETERSOur multi-view visual grounding model, DenseGrounding, is trained with the AdamW optimizer using a learning rate of 5e-4, weight decay of 5e-4, and a batch size of 48.The model is trained for 12 epochs, with the learning rate reduced by 0.1 at epochs 8 and 11.All other settings align with EmbodiedScan.A.2 PERFORMANCE ON INDIVIDUAL CLASSESWe show the performance of our method on the individual classes in the validation set in Fig.4Figure4: Performance of our method on each class in validation set of EmbodiedScan[System]You are an AI spatial reasoning assistant specialized in rephrasing expressions related to 3D indoor scenes.Your task is to rephrase the given expression, enhancing the accuracy of spatial relationships based on the provided context.Below are some expressions (visual grounding descriptions) that describe the scene and one specific expression that needs rephrasing.The text after LOCATION INFO provides the 3D position of the objects centers (unit: meters) in the lidar coordinate system.Enhance this specific expression's accuracy and diversity by using more anchor points.NOTE:1. Input:Several visual grounding descriptions that provide reliable positional information.Each description includes the 3D coordinates of objects, the coordinates are given as (x, y, z) with units in meters.A visual grounding description that needs rephrasing.2. Output: A rephrased visual grounding description.3. Ensure that the target object remains unchanged.4. Use the positional relationships of more objects in the scene to describe it, ensuring higher accuracy and without any potential ambiguity.5.Only describe spatial relationships with high confidence; avoid adding extra information or assumptions.6. Understand the spatial relationships between objects based on their 3D coordinates as a supplement to spatial information.7. DO NOT provide the LOCATION INFO in the rephrased expression.8. Just output the rephrased sentence directly, prohibit outputting any other statements.Example 1 (maybe not in this scene):Before rephrase: select the pillow that is near the decoration.LOCATION INFO: pillow is located at (2.5, 2.8, 0.7).decoration is located at (1.25, 3.5, 0.5) After rephrase: Choose the pillow near the decoration, it is also situated beneath the mirror and positioned between the fireplace and the decoration Example 2 (maybe not in this scene): ....[User] Visual Grounding Descriptions:choose the clock that is in front of the towel.LOCATION INFO: clock is located at (0.04, -1.85, 1.52).towel is located at (0.15, 1.97, 0.55) select the clock that is farthest from the pitcher.LOCATION INFO: clock is located at (0.07, -2.08, 1.85).pitcher is located at(-1.44, 1.50, 0.85)the clock that is closer to the chandelier.LOCATION INFO: clock is located at (0.04, -1.85, 1.52).chandelier is located at (0.24, 2.38, 2.20) ........Visual grounding description that needs rephrasing:the clock that is closer to the refrigerator.LOCATION INFO: clock is located at (0.04, -1.85, 1.52).refrigerator is located at (-1.05, 0.50, 0.76) Now, rephrase it according to the above requirements.[Assistant]Choose the clock that is closer to the refrigerator, positioned beneath the chandelier and in front of the towel Given a text description to be augmented (i.e., the raw description) and a SIDB containing text descriptions (i.e., context descriptions), we select k relevant context descriptions (we used k=50) to provide the LLM with scene and object relationship information.Specifically, when selecting descriptions as context to be utilized by the LLM for augmentation, we find descriptions in the SIDB that both belong to the same scene as the raw description and contain the target or anchor class of the raw description in their text.If more than 50 descriptions qualify, we randomly select 50 descriptions from among them.Conversely, if fewer than 50 descriptions qualify, we randomly select from the descriptions in the same scene.This strategy simplifies the process of context selection and allows for diversity in the context provided to the LLM.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, Leonidas Guibas, ECCV. 2020</p>
<p>Scanqa: 3d question answering for spatial scene understanding. Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, Motoaki Kawanabe, CVPR. 2022</p>
<p>3djcg: A unified framework for joint dense captioning and visual grounding on 3d point clouds. Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, Dong Xu, CVPR. 2022</p>
<p>End-to-end object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, ECCV. Springer2020</p>
<p>Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, Yinda Zhang, arXiv:1709.06158Matterport3d: Learning from rgb-d data in indoor environments. 2017arXiv preprint</p>
<p>Mikasa: Multi-key-anchor &amp; scene-aware transformer for 3d visual grounding. Chun-Peng Chang, Shaoxiang Wang, Alain Pagani, Didier Stricker, CVPR. 2024</p>
<p>Scanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu, Chen , Angel X Chang, Matthias Nießner, ECCV. Springer2020</p>
<p>End-to-end 3d dense captioning with vote2cap-detr. Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, Tao Chen, CVPR. 2023</p>
<p>Xinpeng Chen, Lin Ma, Jingyuan Chen, Zequn Jie, Wei Liu, Jiebo Luo, arXiv:1812.03426Real-time referring expression comprehension by single-stage grounding network. 2018arXiv preprint</p>
<p>Scan2cap: Context-aware dense captioning in rgb-d scans. Zhenyu Chen, Ali Gholami, Matthias Nießner, Angel X Chang, CVPR. 2021</p>
<p>Exploring contextual modeling with linear complexity for point cloud segmentation. Yong Xien, Chng , Xuchong Qiu, Yizeng Han, Yifan Pu, Jiewei Cao, Gao Huang, arXiv:2410.212112024arXiv preprint</p>
<p>4d spatio-temporal convnets: Minkowski convolutional neural networks. Christopher Choy, Junyoung Gwak, Silvio Savarese, CVPR. 2019</p>
<p>Scannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, Matthias Nießner, CVPR. 2017</p>
<p>Data augmentation using llms: Data perspectives, learning paradigms and challenges. Bosheng Ding, Chengwei Qin, Ruochen Zhao, Tianze Luo, Xinze Li, Guizhen Chen, Wenhan Xia, Junjie Hu, Anh Tuan Luu, Shafiq Joty, arXiv:2403.029902024arXiv preprint</p>
<p>Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, Jie Tang, Glm, arXiv:2103.10360General language model pretraining with autoregressive blank infilling. 2021arXiv preprint</p>
<p>Diversify your vision datasets with automatic diffusion-based augmentation. Lisa Dunlap, Alyssa Umino, Han Zhang, Jiezhi Yang, Joseph E Gonzalez, Trevor Darrell, NeurIPS. 362023</p>
<p>Naturally supervised 3d visual grounding with language-regularized concept learners. Chun Feng, Joy Hsu, Weiyu Liu, Jiajun Wu, CVPR. 2024</p>
<p>Viewinfer3d: 3d visual grounding based on embodied viewpoint inference. Liang Geng, Jianqin Yin, 2024IEEE RA-L</p>
<p>Viewrefer: Grasp the multi-view knowledge for 3d visual grounding. Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li, ICCV. 2023</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 2016</p>
<p>Refmask3d: Language-guided transformer for 3d referring segmentation. Shuting He, Henghui Ding, arXiv:2407.182442024arXiv preprint</p>
<p>Chat-scene: Bridging 3d scene and large language models with object identifiers. Haifeng Huang, Yilun Chen, Zehan Wang, Rongjie Huang, Runsen Xu, Tai Wang, Luping Liu, Xize Cheng, Yang Zhao, Jiangmiao Pang, 2024aNeurIPS</p>
<p>Segment3d: Learning fine-grained class-agnostic 3d segmentation without manual labels. Rui Huang, Songyou Peng, Ayca Takmaz, Federico Tombari, Marc Pollefeys, Shiji Song, Gao Huang, Francis Engelmann, ECCV. 2024b</p>
<p>Training an open-vocabulary monocular 3d detection model without 3d data. Rui Huang, Henry Zheng, Yan Wang, Zhuofan Xia, Marco Pavone, Gao Huang, NeurIPS2024c</p>
<p>Multi-view transformer for 3d visual grounding. Shijia Huang, Yilun Chen, Jiaya Jia, Liwei Wang, CVPR. 2022</p>
<p>Bottom up top down detection transformers for language grounding in images and point clouds. Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, Katerina Fragkiadaki, ECCV. Springer2022</p>
<p>Pointgroup: Dual-set point grouping for 3d instance segmentation. Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, Jiaya Jia, CVPR. 2020</p>
<p>Q: How to specialize large vision-language models to data-scarce vqa tasks? a: Self-train on unlabeled images. Zaid Khan, Vijay Kumar, B G , Samuel Schulter, Xiang Yu, Yun Fu, Manmohan Chandraker, CVPR. 2023</p>
<p>Openvla: An open-source vision-language-action model. Jin Moo, Karl Kim, Siddharth Pertsch, Ted Karamcheti, Ashwin Xiao, Suraj Balakrishna, Rafael Nair, Ethan Rafailov, Grace Foster, Pannag Lam, Sanketi, arXiv:2406.092462024arXiv preprint</p>
<p>A real-time crossmodality correlation filtering method for referring expression comprehension. Yue Liao, Si Liu, Guanbin Li, Fei Wang, Yanjie Chen, Chen Qian, Bo Li, CVPR. 2020</p>
<p>. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, 2019Roberta: A robustly optimized bert pretraining approach</p>
<p>Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation. Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela L Rus, Song Han, ICRA. IEEE2023</p>
<p>3d-sps: Single-stage 3d visual grounding via referred point progressive selection. Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing Ren, Hao Shen, Huaxia Xia, Si Liu, CVPR. 2022</p>
<p>Multi-modal understanding and generation for medical images and text via vision-language pre-training. Jong Hak Moon, Hyungyung Lee, Woncheol Shin, Young-Hak Kim, Edward Choi, IEEE Journal of Biomedical and Health Informatics. 26122022</p>
<p>Baolin Peng, Chunyuan Li, arXiv:2304.03277Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. 2023arXiv preprint</p>
<p>Adapting pretrained vision-language model for zero-shot end-to-end temporal action detection. Thinh Phan, Khoa Vo, Duy Le, Gianfranco Doretto, Donald Adjeroh, Ngan Le, Zeetad, WACV2024</p>
<p>Learning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML. 2021</p>
<p>Thibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timothée Lachaux, Baptiste Lacroix, Naman Rozière, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023arXiv preprint</p>
<p>Rio: 3d object instance re-localization in changing indoor environments. Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, Matthias Nießner, ICCV. 2019</p>
<p>Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai. Tai Wang, Xiaohan Mao, Chenming Zhu, Runsen Xu, Ruiyuan Lyu, Peisen Li, Xiao Chen, Wenwei Zhang, Kai Chen, Tianfan Xue, Xihui Liu, Cewu Lu, Dahua Lin, and Jiangmiao Pang. 2024CVPR</p>
<p>Tung-Yu Wu, Sheng-Yu Huang, Yu-Chiang Frank, Wang , arXiv:2403.16539Dora: 3d visual grounding with orderaware referring. 2024aarXiv preprint</p>
<p>Point transformer v3: Simpler faster stronger. Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, Hengshuang Zhao, CVPR. 2024b</p>
<p>Eda: Explicit textdecoupling and dense alignment for 3d visual grounding. Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, Jian Zhang, CVPR. 2023</p>
<p>Exploiting contextual objects and relations for 3d visual grounding. Li Yang, Ziqi Zhang, Zhongang Qi, Yan Xu, Wei Liu, Ying Shan, Bing Li, Weiping Yang, Peng Li, Yan Wang, NeurIPS2024</p>
<p>Dynamic graph attention for referring expression comprehension. Sibei Yang, Guanbin Li, Yizhou Yu, ICCV. 2019</p>
<p>3dvg-transformer: Relation modeling for visual grounding on point clouds. Lichen Zhao, Daigang Cai, Lu Sheng, Dong Xu, ICCV. 2021</p>
<p>Xiaowen Haoyu Zhen, Peihao Qiu, Jincheng Chen, Xin Yang, Yilun Yan, Yining Du, Chuang Hong, Gan, arXiv:2403.096313d-vla: A 3d vision-language-action generative world model. 2024arXiv preprint</p>
<p>Denseg: Alleviating vision-language feature sparsity in multi-view 3d visual grounding. Henry Zheng, Hao Shi, Yong Xien Chng, Rui Huang, Zanlin Ni, Tan Tianyi, Peng Qihang, Weng Yepeng, Zhongchao Shi, Gao Huang, CVPRW. 2024</p>
<p>Class-balanced grouping and sampling for point cloud 3d object detection. Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, Gang Yu, arXiv:1908.094922019arXiv preprint</p>
<p>Object2scene: Putting objects in context for open-vocabulary 3d detection. Chenming Zhu, Wenwei Zhang, Tai Wang, Xihui Liu, Kai Chen, 2023a</p>
<p>3d-vista: Pretrained transformer for 3d vision and text alignment. Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, Qing Li, CVPR. 2023b</p>            </div>
        </div>

    </div>
</body>
</html>
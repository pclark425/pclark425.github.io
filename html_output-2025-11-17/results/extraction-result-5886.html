<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5886 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5886</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5886</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-84b77180228051040286423cec82b62c323a8fda</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/84b77180228051040286423cec82b62c323a8fda" target="_blank">Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> Evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well is shown and retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs' QA and judgemental capabilities. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <p class="empty-note">No extracted data.</p>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5886",
    "paper_id": "paper-84b77180228051040286423cec82b62c323a8fda",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [],
    "potentially_relevant_new_papers": [],
    "cost": 0.00596925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation</h1>
<p>Ruiyang Ren ${ }^{1,3 \text { † }}$ Yuhao Wang ${ }^{1,3 *}$ Yingqi Qu ${ }^{2}$ Wayne Xin Zhao ${ }^{1,3 \ddagger}$ Jing Liu ${ }^{2 \S}$<br>Hao Tian ${ }^{2}$ Hua Wu ${ }^{2}$ Ji-Rong Wen ${ }^{1,3}$ Haifeng Wang ${ }^{2}$<br>${ }^{1}$ Gaoling School of Artificial Intelligence, Renmin University of China<br>${ }^{2}$ Baidu Inc.<br>${ }^{3}$ Beijing Key Laboratory of Big Data Management and Analysis Methods<br>{reyon.ren, yh.wang, jrwen}@ruc.edu.cn, batmanfly@gmail.com<br>{quyingqi, liujing46, tianhao, wu_hua, wanghaifeng}@baidu.com</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have shown impressive prowess in solving a wide range of tasks with world knowledge. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly under retrieval augmentation settings. In this study, we present the first analysis on the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain question answering (QA), with a bunch of important findings. Specifically, we focus on three research questions and analyze them by examining QA, priori judgement and posteriori judgement capabilities of LLMs. We show evidence that LLMs possess unwavering confidence in their knowledge and cannot handle the conflict between internal and external knowledge well. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries. We further conduct thorough experiments to examine how different factors affect LLMs and propose a simple method to dynamically utilize supporting documents with our judgement strategy. Additionally, we find that the relevance between the supporting documents and the questions significantly impacts LLMs' QA and judgemental capabilities. The code to reproduce this work is available at https://github.com/RUCAIBox/ LLM-Knowledge-Boundary.</p>
<h2>1 Introduction</h2>
<p>Knowledge-intensive tasks, defined by their requirement for extensive knowledge, represent a significant area of interest within the field of natural language processing (Petroni et al., 2021). A prime example of such tasks is open-domain question answering (QA) (Chen et al., 2017), which necessitates the assistance of information retrieval</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>systems (Zhao et al., 2023) to obtain relevant information. Subsequently, a reading comprehension model is employed to identify and retrieve pertinent information, ultimately yielding the final response.</p>
<p>Recently, large language models (LLMs) have showcased remarkable abilities in solving various tasks, which are capable of encoding extensive volumes of world knowledge within their parameters (Brown et al., 2020a; Ouyang et al., 2022; Zhao et al., 2023). Despite their exceptional capabilities, LLMs may exhibit limited flexibility in knowledge-intensive tasks, necessitating the incorporation of retrieval augmentation strategies. Several pioneering efforts have applied LLMs to opendomain QA tasks (Qin et al., 2023; Kamalloo et al., 2023; Wang et al., 2023a; Sun et al., 2023; Xu et al., 2024). Typically, they mainly focus on evaluating the QA performance of LLMs, discussing improved evaluation methods or leveraging LLMs to enhance QA models. The existing effort also detects the uncertainty of LLMs with an automated method (Yin et al., 2023). Furthermore, the issue of hallucination poses challenges to LLMs' reliable deployment, while retrieval augmentation is considered an effective method to mitigate hallucinations (Li et al., 2024; Shuster et al., 2021; Wang et al., 2023b).</p>
<p>Despite the contribution of existing studies, there is still a lack of a deep understanding of LLMs' capabilities in perceiving their factual knowledge boundaries, particularly when external resources can be used. For instance, it remains uncertain whether LLMs are able to assess their own ability to answer questions, evaluate whether the provided reference is sufficient to address the questions, and assess the accuracy of their own answers. Our primary focus is the factual knowledge boundary of LLMs, and study the impact of retrieval augmentation on the generation of LLMs.</p>
<p>To this end, we undertake a thorough analysis of the influence of retrieval augmentation on the</p>
<p>generation quality of LLMs, with a specific focus on QA performance and LLMs’ perception of their factual knowledge boundaries. In order to fully explore the knowledge boundaries of LLMs, we consider a wide range of LLMs, including closed source LLMs and publicly available LLMs. To measure the capacity of knowledge boundary perception, we consider two alternative approaches. The first one is priori judgement, in which LLMs assess the feasibility of answering a given question before the response generation process. The second one is posteriori judgement, where LLMs evaluate the correctness of their answer to questions after the response generation process. The two approaches can evaluate the knowledge boundary perception of LLMs from different perspectives. For retrieval augmentation, we adopt multiple retrieval models to provide supporting documents for LLMs regarding the given questions.</p>
<p>To conduct a comprehensive investigation, our work aims to answer three research questions progressively: (i) To what extent can LLMs perceive their factual knowledge boundaries? (ii) What effect does retrieval augmentation have on LLMs? (iii) How do supporting documents with different characteristics affect LLMs?</p>
<p>We design comprehensive experiments to thoroughly explore these research questions individually. Furthermore, building upon priori judgement, we attempt a simple method for dynamically introducing retrieval augmentation to LLMs with a performance gain. Based on the empirical analysis, we have derived the following important findings corresponding to the three research questions:</p>
<ul>
<li>LLMs’ perception of the factual knowledge boundary is inaccurate and they often display a tendency towards being overconfident. LLMs are also not able to handle internalexternal knowledge conflicts well.</li>
<li>LLMs cannot sufficiently utilize their internal knowledge, while retrieval augmentation can provide a beneficial knowledge supplement for LLMs, especially for smaller LLMs. Retrieval augmentation can also enhance LLMs’ perception on their factual knowledge boundaries. Furthermore, the performance of retrieval-augmented LLMs is affected by model-specific factors, question types, and the characteristics of the retrieval documents.</li>
<li>LLMs exhibit improved performance and con- fidence when referring high-quality supporting documents and tend to rely on the provided information to produce the responses. The reliance extent and LLMs’ confidence are contingent upon the relevance between the supporting documents and the question.</li>
</ul>
<h2>2 Background and Methodology</h2>
<p>In this section, we provide an overview of the background and fundamental methodologies that are essential for this study.</p>
<h3>2.1 Task Formulation</h3>
<p>We conduct experiments on open-domain question answering (QA), which can be described as follows. Given a question $q$ in natural language and a large document collection $\mathcal{D}=\left{d_{i}\right}_{i=1}^{m}$ such as Wikipedia, the model is required to provide an answer $a$ to the question $q$ with the help of the provided corpus $\mathcal{D}$. With large language models (LLMs), the open-domain QA task can be directly solved in an end-to-end manner in a closedbook setting <em>Qin et al. (2023)</em>. Given a question $q$, the answer $a$ can be generated by the LLM with a prompt $p$ following a specific output format:</p>
<p>$$
a=f_{\mathrm{LLM}}(p, q)
$$</p>
<p>When enhancing the LLM with retrieval, a typical strategy is designing prompt $p$ to instruct the LLM to provide an answer $a$ to question $q$ using the supporting documents $\mathcal{L}$ retrieved by the retriever:</p>
<p>$$
a=f_{\mathrm{LLM}}(p, q, \mathcal{L})
$$</p>
<p>Equation 1 and 2 present two approaches for LLMs to solve QA tasks, we also adopt these approaches to evaluate LLMs’ factual knowledge boundaries.</p>
<h3>2.2 Instructing LLMs with Natural Language Prompts</h3>
<p>We consider two instruction settings, namely QA prompting and judgemental prompting. Figure 1 provides an overall illustration of our prompting strategies and detailed instructions can be found in Appendix A.5.</p>
<h3>2.2.1 QA Prompting</h3>
<p>To evaluate LLMs’ ability to utilize knowledge, we focus LLMs’ QA ability that provides answers to the given questions, with two-fold aims. Firstly, we manage to ensure that LLMs can accurately answer and refer to both internal and external knowledge. Secondly, we encourage concise responses</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: The illustration of different settings to instruct LLMs, the evaluation metrics are also displayed.</p>
<p>to match the short-answer format for assessment with normally used metrics. To this end, we propose two distinct instructional approaches. (1) <em>Normal setting:</em> In this approach, LLMs are instructed to respond to questions based solely on their internal knowledge, as formulated in Equation (1). (2) <em>Retrieval-augmented setting:</em> This approach requires LLMs to answer the given questions using a combination of their internal knowledge and information from supporting documents retrieved, as outlined in Equation (2). In this setting, supporting documents are optional, and ideally, LLMs should determine whether to refer to the supporting documents based on their reliability.</p>
<h3>2.2.2 Judgemental Prompting</h3>
<p>To investigate whether LLMs are capable of perceiving their own factual knowledge boundary, we propose judgemental prompting to evaluate the judging abilities of LLMs.</p>
<p>Similar to QA prompting, the concepts of the <em>normal setting</em> and the <em>retrieval-augmented setting</em> are also applicable for judgemental prompting, where LLMs utilizing their own knowledge or consulting supporting documents from retrievers to carry out the judgement process. Furthermore, we construct instructions from different judgement perspectives. (1) <em>Priori judgement:</em> LLMs are tasked with determining their capability to provide an answer to the question, employing either the normal setting or the retrieval-augmented setting. Priori judgment is carried out before the answering of LLMs, through this predictive approach, we are able to evaluate the confidence levels and assessment accuracies of LLMs in their mastery of knowledge. (2) <em>Posteriori judgement:</em> LLMs are tasked with evaluating the correctness of the answer to the question provided by itself, employing either a normal setting or a retrieval-augmented setting. Posteriori judgement aims to enable LLMs to judge their responses. Through this reflective approach, we can evaluate the confidence levels and assessment accuracies of LLMs in the content they generate.</p>
<h3>2.3 Experimental Settings</h3>
<p>In this part, we set up our experiments on open-domain QA, including evaluation metrics and retrieval sources. Due to the space limitation, more settings can be found in Appendix A, including datasets, evaluation models, implemental details and instruction designs.</p>
<h4>2.3.1 Evaluation Metrics</h4>
<p>Following previous works (Chen et al., 2017; Izacard and Grave, 2021a; Sun et al., 2023), we use the <em>exact match (EM)</em> score and <em>F1</em> score to evaluate the QA performance of LLMs. <em>EM</em> calculates the percentage of questions in which the answer predicted by LLMs precisely matches the correct answer to the question. <em>F1</em> is used to measure the overlap between the predicted answer and the correct answer, combines precision and recall into a single metric by taking their harmonic mean.</p>
<p>We also propose several metrics for evaluating LLMs' judgement abilities. <em>Give-up</em> denotes the percentage of questions that LLMs give up</p>
<p>answering, which reflects the LLMs’ judgement on whether they possess the relevant knowledge. Right/G represents the probability that LLMs give up answering but can actually answer correctly. Right/ G represents the probability that LLMs do not give up answering and can answer correctly. Eval-Right refers to the proportion of questions where LLMs assess their answers as correct. Eval-Acc represents the percentage of questions for which the assessment of the answer by LLMs aligns with the fact. Among them, Give-up, Right/G and Right/ G are metrics for priori judgement, Eval-Right and Eval-Acc are metrics for posteriori judgement. All metrics are illustrated in Figure 1.</p>
<h3>2.3.2 Retrieval Sources</h3>
<p>We consider multiple retrieval sources to acquire supporting documents, including dense retrieval (Gao and Callan, 2021; Ren et al., 2021a; Zhuang et al., 2022; Zhou et al., 2022), sparse retrieval (Robertson et al., 2009) and ChatGPT.</p>
<p>For the dense retriever, we utilize RocketQAv2 (Ren et al., 2021b) to find semantically relevant documents for questions. To achieve this, we train the model on each dataset with the constructed in-domain training data under the settings of RocketQAv2 and leverage Faiss (Johnson et al., 2019) to obtain relevant documents for each question from the candidate corpus. For the sparse retriever, we use BM25 (Yang et al., 2017) to find lexical relevant documents for questions. Similar to previous works (Yu et al., 2022; Ren et al., 2023), we regard the generative language model as a "retriever" that "retrieves" knowledge from its memory, where ChatGPT is instructed to produce relevant documents in response to a given question. The retrieval results can be found in Appendix A.4.</p>
<h2>3 Experimental Analysis and Findings</h2>
<p>In this section, we focus on addressing the three research questions and tackle them by investigating the judgement ability and the QA ability of LLMs with the proposed strategies in Section 2.2.</p>
<h3>3.1 To What Extent Can LLMs Perceive Their Factual Knowledge Boundaries?</h3>
<p>We deconstruct the question and investigate the following points: (a) Before answering: how do LLMs decide to abstain from a question? (b) When answering: can LLMs accurately address a given question? (c) After answering: how do LLMs assess the correctness of their answers?</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">QA</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Priori Judgement</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Posteriori Judgement</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">EM</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Give-up</td>
<td style="text-align: center;">Right/ G</td>
<td style="text-align: center;">Eval-Right</td>
<td style="text-align: center;">Eval-Acc</td>
</tr>
<tr>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">Davinci003</td>
<td style="text-align: center;">27.20</td>
<td style="text-align: center;">36.20</td>
<td style="text-align: center;">29.20\%</td>
<td style="text-align: center;">32.77\%</td>
<td style="text-align: center;">72.80\%</td>
<td style="text-align: center;">45.01\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">33.40</td>
<td style="text-align: center;">45.32</td>
<td style="text-align: center;">57.40\%</td>
<td style="text-align: center;">42.72\%</td>
<td style="text-align: center;">84.40\%</td>
<td style="text-align: center;">43.40\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">34.60</td>
<td style="text-align: center;">48.72</td>
<td style="text-align: center;">15.20\%</td>
<td style="text-align: center;">39.15\%</td>
<td style="text-align: center;">90.20\%</td>
<td style="text-align: center;">38.87\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA2</td>
<td style="text-align: center;">16.60</td>
<td style="text-align: center;">24.26</td>
<td style="text-align: center;">6.60\%</td>
<td style="text-align: center;">17.56\%</td>
<td style="text-align: center;">58.40\%</td>
<td style="text-align: center;">46.74\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">11.20</td>
<td style="text-align: center;">19.30</td>
<td style="text-align: center;">49.80\%</td>
<td style="text-align: center;">15.94\%</td>
<td style="text-align: center;">68.00\%</td>
<td style="text-align: center;">37.90\%</td>
</tr>
<tr>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">Davinci003</td>
<td style="text-align: center;">65.20</td>
<td style="text-align: center;">69.57</td>
<td style="text-align: center;">7.40\%</td>
<td style="text-align: center;">67.17\%</td>
<td style="text-align: center;">87.00\%</td>
<td style="text-align: center;">69.82\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">69.00</td>
<td style="text-align: center;">75.29</td>
<td style="text-align: center;">25.00\%</td>
<td style="text-align: center;">75.73\%</td>
<td style="text-align: center;">88.80\%</td>
<td style="text-align: center;">71.95\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">75.80</td>
<td style="text-align: center;">84.52</td>
<td style="text-align: center;">8.80\%</td>
<td style="text-align: center;">77.85\%</td>
<td style="text-align: center;">93.00\%</td>
<td style="text-align: center;">76.57\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA2</td>
<td style="text-align: center;">48.80</td>
<td style="text-align: center;">53.40</td>
<td style="text-align: center;">4.80\%</td>
<td style="text-align: center;">50.21\%</td>
<td style="text-align: center;">75.60\%</td>
<td style="text-align: center;">57.60\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">36.20</td>
<td style="text-align: center;">42.09</td>
<td style="text-align: center;">34.80\%</td>
<td style="text-align: center;">46.63\%</td>
<td style="text-align: center;">86.00\%</td>
<td style="text-align: center;">48.10\%</td>
</tr>
<tr>
<td style="text-align: center;">$\square$</td>
<td style="text-align: center;">Davinci003</td>
<td style="text-align: center;">18.40</td>
<td style="text-align: center;">26.78</td>
<td style="text-align: center;">35.40\%</td>
<td style="text-align: center;">24.15\%</td>
<td style="text-align: center;">70.60\%</td>
<td style="text-align: center;">43.99\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ChatGPT</td>
<td style="text-align: center;">20.80</td>
<td style="text-align: center;">29.27</td>
<td style="text-align: center;">78.40\%</td>
<td style="text-align: center;">31.48\%</td>
<td style="text-align: center;">66.80\%</td>
<td style="text-align: center;">43.12\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">GPT-4</td>
<td style="text-align: center;">28.60</td>
<td style="text-align: center;">40.33</td>
<td style="text-align: center;">54.80\%</td>
<td style="text-align: center;">42.92\%</td>
<td style="text-align: center;">72.40\%</td>
<td style="text-align: center;">45.74\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">LLaMA2</td>
<td style="text-align: center;">11.40</td>
<td style="text-align: center;">16.88</td>
<td style="text-align: center;">25.60\%</td>
<td style="text-align: center;">12.63\%</td>
<td style="text-align: center;">49.80\%</td>
<td style="text-align: center;">54.88\%</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Mistral</td>
<td style="text-align: center;">10.80</td>
<td style="text-align: center;">17.86</td>
<td style="text-align: center;">64.00\%</td>
<td style="text-align: center;">19.44\%</td>
<td style="text-align: center;">81.80\%</td>
<td style="text-align: center;">27.40\%</td>
</tr>
</tbody>
</table>
<p>Table 1: Evaluation results on three datasets without retrieval augmentation. The abbreviations are explained in Section 2.3.1.</p>
<h3>3.1.1 Settings</h3>
<p>In this part of the experiments, we employ the priori judgement with the normal setting to instruct LLMs on the decision of whether to give up answering questions based on their inherent knowledge, and we use the QA prompting with the normal setting to instruct LLMs to provide answers. Moreover, we employ posteriori judgement with the normal setting to instruct LLMs in evaluating the correctness of their answers.</p>
<h3>3.1.2 Main Findings</h3>
<p>LLMs struggle to perceive their factual knowledge boundary, and tend to be overconfident. Overall, the overestimation of LLMs is evident both before and after answering questions in Table 1. Before answering, LLMs frequently overestimate their knowledge, leading to a high rate of incorrect responses (Right/ G). Additionally, their Give-up rate is substantially low, misaligned with their actual QA abilities (reflected by EM). When we instruct LLMs to evaluate their answers for posteriori judgement, they also exhibit a significant tendency to believe that their answers are correct. For this reason, we can observe much higher Eval-Right values compared to EM. However, there exists a substantial disparity between Eval-Right value and the actual evaluation accuracy, as indicated by relatively low Eval-Acc metrics. Similar to previous studies (Kamalloo et al., 2023), LLMs still demonstrate commendable performance in QA tasks (EM and F1), even without external documents. This indicates that LLMs possess a substantial knowledge base and can leverage it to a</p>
<table>
<thead>
<tr>
<th>Datasets</th>
<th>LLMs</th>
<th>Retrieval Source</th>
<th>QA</th>
<th></th>
<th>Priori Judgement</th>
<th></th>
<th></th>
<th>Posteriori Judgement</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>EM</td>
<td>F1</td>
<td>Give-up</td>
<td>Right/G</td>
<td>Right/ G</td>
<td>Eval-Right</td>
<td>Eval-Acc</td>
</tr>
<tr>
<td>NQ</td>
<td>Davinci003</td>
<td>Sparse</td>
<td>27.80</td>
<td>38.29</td>
<td>21.20\%</td>
<td>12.26\%</td>
<td>31.98\%</td>
<td>39.40\%</td>
<td>67.94\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>39.00</td>
<td>51.27</td>
<td>12.80\%</td>
<td>14.06\%</td>
<td>42.66\%</td>
<td>46.40\%</td>
<td>71.43\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>34.00</td>
<td>47.36</td>
<td>6.20\%</td>
<td>6.45\%</td>
<td>35.82\%</td>
<td>46.00\%</td>
<td>71.54\%</td>
</tr>
<tr>
<td></td>
<td>ChatGPT</td>
<td>Sparse</td>
<td>28.40</td>
<td>41.10</td>
<td>42.40\%</td>
<td>17.92\%</td>
<td>36.11\%</td>
<td>67.00\%</td>
<td>48.77\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>39.40</td>
<td>52.65</td>
<td>26.60\%</td>
<td>18.05\%</td>
<td>47.14\%</td>
<td>68.80\%</td>
<td>53.56\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>32.20</td>
<td>47.37</td>
<td>7.40\%</td>
<td>2.70\%</td>
<td>34.56\%</td>
<td>78.80\%</td>
<td>49.90\%</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>Sparse</td>
<td>34.20</td>
<td>45.81</td>
<td>28.20\%</td>
<td>14.18\%</td>
<td>42.06\%</td>
<td>57.20\%</td>
<td>48.48\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>43.60</td>
<td>56.36</td>
<td>12.60\%</td>
<td>15.87\%</td>
<td>47.60\%</td>
<td>66.40\%</td>
<td>50.86\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>34.40</td>
<td>48.56</td>
<td>4.20\%</td>
<td>4.76\%</td>
<td>35.70\%</td>
<td>69.80\%</td>
<td>48.69\%</td>
</tr>
<tr>
<td></td>
<td>LLaMA2</td>
<td>Sparse</td>
<td>23.00</td>
<td>34.14</td>
<td>32.80\%</td>
<td>15.85\%</td>
<td>26.49\%</td>
<td>6.00\%</td>
<td>75.00\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>33.40</td>
<td>45.39</td>
<td>24.80\%</td>
<td>20.16\%</td>
<td>37.77\%</td>
<td>5.20\%</td>
<td>73.08\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>33.40</td>
<td>48.19</td>
<td>5.20\%</td>
<td>15.38\%</td>
<td>34.39\%</td>
<td>5.00\%</td>
<td>87.88\%</td>
</tr>
<tr>
<td></td>
<td>Mistral</td>
<td>Sparse</td>
<td>23.20</td>
<td>33.21</td>
<td>59.00\%</td>
<td>13.22\%</td>
<td>37.56\%</td>
<td>48.60\%</td>
<td>54.71\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>35.20</td>
<td>45.82</td>
<td>40.00\%</td>
<td>21.50\%</td>
<td>44.33\%</td>
<td>50.20\%</td>
<td>56.39\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>32.60</td>
<td>47.49</td>
<td>14.40\%</td>
<td>16.67\%</td>
<td>35.28\%</td>
<td>41.00\%</td>
<td>64.24\%</td>
</tr>
<tr>
<td>TriviaQA</td>
<td>Davinci003</td>
<td>Sparse</td>
<td>64.60</td>
<td>70.19</td>
<td>15.60\%</td>
<td>19.23\%</td>
<td>72.99\%</td>
<td>69.00\%</td>
<td>77.15\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>69.60</td>
<td>75.31</td>
<td>10.00\%</td>
<td>30.00\%</td>
<td>74.00\%</td>
<td>74.40\%</td>
<td>81.49\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>67.40</td>
<td>75.43</td>
<td>2.00\%</td>
<td>10.00\%</td>
<td>68.57\%</td>
<td>72.20\%</td>
<td>81.00\%</td>
</tr>
<tr>
<td></td>
<td>ChatGPT</td>
<td>Sparse</td>
<td>62.60</td>
<td>69.98</td>
<td>23.00\%</td>
<td>34.78\%</td>
<td>70.91\%</td>
<td>79.80\%</td>
<td>73.29\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>66.20</td>
<td>74.75</td>
<td>18.20\%</td>
<td>39.56\%</td>
<td>72.13\%</td>
<td>82.40\%</td>
<td>75.73\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>65.00</td>
<td>74.44</td>
<td>3.00\%</td>
<td>13.33\%</td>
<td>66.60\%</td>
<td>90.40\%</td>
<td>74.34\%</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>Sparse</td>
<td>66.20</td>
<td>75.99</td>
<td>12.40\%</td>
<td>35.48\%</td>
<td>70.55\%</td>
<td>83.40\%</td>
<td>76.79\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>69.00</td>
<td>78.01</td>
<td>7.20\%</td>
<td>30.56\%</td>
<td>71.98\%</td>
<td>85.80\%</td>
<td>76.51\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>66.40</td>
<td>76.33</td>
<td>2.60\%</td>
<td>15.38\%</td>
<td>67.76\%</td>
<td>83.40\%</td>
<td>73.39\%</td>
</tr>
<tr>
<td></td>
<td>LLaMA2</td>
<td>Sparse</td>
<td>51.00</td>
<td>59.51</td>
<td>35.60\%</td>
<td>40.45\%</td>
<td>56.83\%</td>
<td>13.20\%</td>
<td>70.19\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>58.60</td>
<td>66.57</td>
<td>33.40\%</td>
<td>40.72\%</td>
<td>67.57\%</td>
<td>11.40\%</td>
<td>75.00\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>63.00</td>
<td>71.76</td>
<td>2.80\%</td>
<td>28.57\%</td>
<td>63.99\%</td>
<td>18.20\%</td>
<td>79.35\%</td>
</tr>
<tr>
<td></td>
<td>Mistral</td>
<td>Sparse</td>
<td>52.20</td>
<td>59.55</td>
<td>30.40\%</td>
<td>20.39\%</td>
<td>66.09\%</td>
<td>59.20\%</td>
<td>68.75\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>57.40</td>
<td>65.59</td>
<td>24.20\%</td>
<td>26.45\%</td>
<td>67.28\%</td>
<td>59.80\%</td>
<td>72.53\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>62.20</td>
<td>71.72</td>
<td>3.60\%</td>
<td>16.67\%</td>
<td>63.90\%</td>
<td>55.20\%</td>
<td>77.76\%</td>
</tr>
<tr>
<td>HotpotQA</td>
<td>Davinci003</td>
<td>Sparse</td>
<td>31.20</td>
<td>40.95</td>
<td>27.20\%</td>
<td>14.71\%</td>
<td>37.36\%</td>
<td>31.20\%</td>
<td>76.84\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>26.80</td>
<td>35.89</td>
<td>37.00\%</td>
<td>13.51\%</td>
<td>34.60\%</td>
<td>35.20\%</td>
<td>76.89\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>28.20</td>
<td>39.34</td>
<td>8.20\%</td>
<td>12.20\%</td>
<td>29.63\%</td>
<td>33.40\%</td>
<td>77.37\%</td>
</tr>
<tr>
<td></td>
<td>ChatGPT</td>
<td>Sparse</td>
<td>29.60</td>
<td>41.28</td>
<td>50.60\%</td>
<td>17.39\%</td>
<td>42.11\%</td>
<td>51.80\%</td>
<td>54.90\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>26.40</td>
<td>35.75</td>
<td>58.40\%</td>
<td>14.38\%</td>
<td>43.27\%</td>
<td>48.20\%</td>
<td>56.10\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>26.40</td>
<td>38.30</td>
<td>11.20\%</td>
<td>7.14\%</td>
<td>28.83\%</td>
<td>68.20\%</td>
<td>48.24\%</td>
</tr>
<tr>
<td></td>
<td>GPT-4</td>
<td>Sparse</td>
<td>36.00</td>
<td>47.71</td>
<td>25.60\%</td>
<td>14.84\%</td>
<td>43.28\%</td>
<td>43.40\%</td>
<td>64.90\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>31.80</td>
<td>43.92</td>
<td>37.00\%</td>
<td>17.30\%</td>
<td>40.32\%</td>
<td>46.00\%</td>
<td>60.34\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>29.80</td>
<td>41.67</td>
<td>8.80\%</td>
<td>6.82\%</td>
<td>32.02\%</td>
<td>48.40\%</td>
<td>68.55\%</td>
</tr>
<tr>
<td></td>
<td>LLaMA2</td>
<td>Sparse</td>
<td>24.00</td>
<td>33.45</td>
<td>45.60\%</td>
<td>16.67\%</td>
<td>30.15\%</td>
<td>8.60\%</td>
<td>58.89\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>21.60</td>
<td>31.17</td>
<td>57.00\%</td>
<td>13.68\%</td>
<td>32.09\%</td>
<td>7.60\%</td>
<td>66.99\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>25.80</td>
<td>37.56</td>
<td>11.80\%</td>
<td>22.03\%</td>
<td>26.30\%</td>
<td>7.20\%</td>
<td>82.53\%</td>
</tr>
<tr>
<td></td>
<td>Mistral</td>
<td>Sparse</td>
<td>25.00</td>
<td>35.49</td>
<td>52.40\%</td>
<td>13.74\%</td>
<td>37.39\%</td>
<td>42.40\%</td>
<td>62.42\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>Dense</td>
<td>23.60</td>
<td>32.70</td>
<td>59.80\%</td>
<td>13.38\%</td>
<td>38.81\%</td>
<td>45.60\%</td>
<td>59.75\%</td>
</tr>
<tr>
<td></td>
<td></td>
<td>ChatGPT</td>
<td>26.20</td>
<td>37.93</td>
<td>14.00\%</td>
<td>12.86\%</td>
<td>28.37\%</td>
<td>37.60\%</td>
<td>70.04\%</td>
</tr>
</tbody>
</table>
<p>Table 2: Results of retrieval-augmented LLMs, the abbreviations are explained in Section 2.3.1.
certain extent. Moreover, it can be observed that closed source LLMs overall exhibit better QA performance than publicly available LLMs we used, and GPT-4 obtains the best QA performance. Overall, closed source LLMs show a higher accuracy in percepting their factual knowledge boundary.</p>
<h3>3.2 What Impact Does Retrieval Augmentation Have on LLMs?</h3>
<p>Following the analysis under the closed-book setting, we next study the retrieval augmentation setting. Specifically, with the supporting documents from retrievers, we employ the priori judgement to determine whether to give up answering, and the
posteriori judgement to assess the correctness of answers generated by LLMs. Additionally, we employ QA prompting to guide LLMs in answering the questions.</p>
<h3>3.2.1 Main Findings</h3>
<p>We conduct our analysis from three perspectives: knowledge utilization, recognition of knowledge boundaries, and the impact of documents from various sources. After thorough retrieval augmentation experiments under different settings in Table 2, we arrive at the following findings.</p>
<p>LLMs cannot sufficiently utilize their internal knowledge, while retrieval augmentation can</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: The performance of retrieval-augmented LLMs with different retrieved document numbers.</p>
<p><strong>serve as a valuable knowledge supplement for LLMs.</strong> It can be observed that LLMs with supporting documents outperform closed-book LLMs in most cases, and incorporating results of dense retrieval as supporting documents often leads to the best performance. To further explore the effect of different retrieval sources, we conduct a more detailed analysis in Appendix B.1. Although LLMs have learned massive knowledge from Wikipedia during training (Ouyang et al., 2022), providing them with Wikipedia documents still improves their QA abilities, indicating that LLMs are not able to effectively utilize their knowledge learned during the pre-training stage. Furthermore, we find that retrieval augmentation yields higher improvements in QA performance for publicly available LLMs. We speculate that this phenomenon arises from the increased storage of knowledge associated with these closed-source LLMs that have larger parameter scales, resulting in a lower gain with the introduction of external knowledge. We also observe a decline in the performance of ChatGPT and GPT-4 when incorporating supporting documents on TriviaQA. We manually inspect the bad cases where they initially answer correctly but answer incorrectly after incorporating retrieval. We find that a significant portion of these cases are due to the extraction of incorrect contents from the supporting documents. Given the relatively high performance of ChatGPT and GPT-4 on TriviaQA, we suspect that multiple supporting documents may introduce significant noise, reflecting the upper bound of retrieval augmentation for improvement to some extent (further discussed in Section 3.3).</p>
<p><strong>Retrieval augmentation improves LLM's ability to perceive their factual knowledge boundaries.</strong> From Table 2, we find that the accuracy of LLMs' self-assessment improves after incorporating supporting documents from either sparse or dense retrievers. For priori judgement, <em>Right/¬G</em> exhibits a notable increase, surpassing the growth trajectory (attributed to the significant enhancement in QA performance) of <em>Right/G</em>. Furthermore, in certain contexts, <em>Right/G</em> has shown a decrease. The results show that the priori judgement of retrieval-augmented LLMs is more accurate. For posteriori judgement, <em>Eval-Right</em> decreases that it is more consistent with <em>EM</em> metric, while <em>Eval-Acc</em> significantly increases. The results indicate that retrieval augmentation can also improve the accuracy of LLMs' posterior judgement.</p>
<p><strong>Increasing the number of supporting documents improves the performance of LLMs below a model-specific threshold.</strong> In Figure 2, we further explore the effect of the supporting document number on retrieval-augmented LLMs. As the number increases, the QA performance (<em>EM</em>) gradually increases until reaching a certain threshold, beyond which the performance ceases to improve and may even decline. We find LLaMA2 exhibiting lower thresholds, which may be due to its inferior handling of long-form text compared to the GPT series LLMs (Xu et al., 2023; Jiang et al., 2023b). We also observe that <strong>the improvement yielded by the increased supporting document number is not fully attributable to the improvement of recall rate</strong>. Even if the documents are all golden documents (described in Section 3.3.1), a larger document number still results in improvements. Furthermore, <strong>LLMs seem to be insensitive to the ordering of retrieved documents</strong>, such that the performance remains unaffected when the supporting documents are reversed or shuffled. With respect to the accuracy of perceiving knowledge bound</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: A simple method that dynamically introduces retrieval for LLMs based on priori judgement strategy. We use ChatGPT with QA prompting under the retrievalaugmented setting as the baseline (w/o judgement).
aries, the Eval-Acc of LLaMA2 and Mistral both decline as the document number increases. We also find an increase in the confidence level (Give-up) of most models with an increase in the number of supporting documents, except for LLaMA2.</p>
<p>In addition to the above findings, we also explore the impact of retrieval augmentation on different query types and LLMs with various parameter quantities, and obtain the following findings: (1) Increasing the number of supporting documents improves the performance of LLMs below a model-specific threshold. (2) Retrieval augmentation is more pronounced for improving LLMs with fewer parameters, including both QA performances and accuracies of knowledge boundary perception. Due to the limited space, the comprehensive analysis of the two findings can be found in Appendix B. 2 and B.3.</p>
<h3>3.2.2 Dynamic Retrieval Augmentation</h3>
<p>In order to further investigate the observed improvement, we examine with a simple method that employs priori judgement with either the normal or the retrieval-augmented settings to determine whether to introduce retrieval augmentation. Specifically, if the LLM considers a question is challenging to answer under the current setting, supporting documents are introduced to help provide an answer, otherwise the question will be answered without supporting documents. We conduct
experiment on ChatGPT, using supporting documents from the dense retriever.</p>
<p>Figure 3 compares different judgemental settings for decision-making to dynamically incorporate retrieval augmentation with the corresponding results on NQ. It is evident that employing the priori judgment with ChatGPT within the standard decisionmaking framework results in decreased answering accuracy when compared to the baseline that always utilizes retrieval augmentation. Nevertheless, upon integrating retrieval augmentation for judging, the accuracy surpasses the baseline. The result indicates that it is a promising way to dynamically introduce supporting documents for LLMs according to its retrieval-augmented priori judgement. It further shows that the incorporation of retrieval augmentation can improve LLMs' awareness of their factual knowledge boundaries.</p>
<h3>3.3 How do Different Relevance Supporting Document Affect LLMs?</h3>
<p>We have explored the effect of retrieval augmentation on LLMs. Actually, the retrieval results consist of documents with varying relevance, which might lead to different effects. For this purpose, we continue to study how different relevance of supporting documents influence LLMs. In our experiments, we consider the following perspectives, the relevance between the document and the question, and the presence of an answer within the document.</p>
<h3>3.3.1 Sampling Strategies</h3>
<p>In order to thoroughly study the impact of supporting documents on LLMs, we propose to provide LLMs with supporting documents of different characteristics for obtaining answers. Golden documents refer to documents containing at least one correct answer to the question, which are sampled from top to bottom in the top 100 retrieval results of the question; High-related incorrect documents refer to documents that are highly relevant to the question but do not contain the correct answer. They are also sampled from top to bottom in the top 100 retrieval results of the question; Weak-related incorrect documents are the documents weakly relevant to the query and do not contain the correct answer. We randomly sample documents from the top 100 retrieval results of the question excluding high-related incorrect documents; Random incorrect documents refer to documents randomly sampled from the entire corpus $\mathcal{D}$, which do not contain the correct answers to the given question.</p>
<table>
<thead>
<tr>
<th>LLMs</th>
<th>Supporting Doc</th>
<th>EM</th>
<th>F1</th>
<th>Give-up</th>
<th>Right/G</th>
<th>Right/ G</th>
<th>Eval-Right</th>
<th>Eval-Acc</th>
</tr>
</thead>
<tbody>
<tr>
<td>Davinci-003</td>
<td>None</td>
<td>27.20</td>
<td>36.20</td>
<td>29.20\%</td>
<td>13.70\%</td>
<td>32.77\%</td>
<td>72.80\%</td>
<td>45.01\%</td>
</tr>
<tr>
<td></td>
<td>Golden</td>
<td>50.60</td>
<td>62.93</td>
<td>15.80\%</td>
<td>15.19\%</td>
<td>57.24\%</td>
<td>52.00\%</td>
<td>71.08\%</td>
</tr>
<tr>
<td></td>
<td>Retrieved</td>
<td>39.00</td>
<td>51.27</td>
<td>12.80\%</td>
<td>14.06\%</td>
<td>42.66\%</td>
<td>46.40\%</td>
<td>71.43\%</td>
</tr>
<tr>
<td></td>
<td>High-related</td>
<td>10.20</td>
<td>20.66</td>
<td>18.00\%</td>
<td>8.89\%</td>
<td>10.49\%</td>
<td>28.40\%</td>
<td>57.89\%</td>
</tr>
<tr>
<td></td>
<td>Weak-related</td>
<td>11.80</td>
<td>19.69</td>
<td>41.40\%</td>
<td>10.63\%</td>
<td>12.63\%</td>
<td>20.80\%</td>
<td>61.71\%</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>23.00</td>
<td>30.82</td>
<td>88.40\%</td>
<td>20.59\%</td>
<td>41.38\%</td>
<td>19.40\%</td>
<td>66.26\%</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>None</td>
<td>33.40</td>
<td>45.32</td>
<td>57.40\%</td>
<td>26.48\%</td>
<td>42.72\%</td>
<td>84.40\%</td>
<td>43.40\%</td>
</tr>
<tr>
<td></td>
<td>Golden</td>
<td>50.00</td>
<td>64.28</td>
<td>22.60\%</td>
<td>23.01\%</td>
<td>57.88\%</td>
<td>75.20\%</td>
<td>53.24\%</td>
</tr>
<tr>
<td></td>
<td>Retrieved</td>
<td>39.40</td>
<td>52.65</td>
<td>26.60\%</td>
<td>18.05\%</td>
<td>47.14\%</td>
<td>68.80\%</td>
<td>53.56\%</td>
</tr>
<tr>
<td></td>
<td>High-related</td>
<td>16.20</td>
<td>28.20</td>
<td>42.00\%</td>
<td>13.81\%</td>
<td>17.93\%</td>
<td>56.20\%</td>
<td>47.82\%</td>
</tr>
<tr>
<td></td>
<td>Weak-related</td>
<td>18.40</td>
<td>29.86</td>
<td>60.20\%</td>
<td>16.61\%</td>
<td>21.11\%</td>
<td>49.80\%</td>
<td>46.21\%</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>24.80</td>
<td>35.35</td>
<td>91.00\%</td>
<td>23.30\%</td>
<td>40.00\%</td>
<td>29.80\%</td>
<td>48.80\%</td>
</tr>
<tr>
<td>GPT-4</td>
<td>None</td>
<td>34.60</td>
<td>48.72</td>
<td>15.20\%</td>
<td>9.21\%</td>
<td>39.15\%</td>
<td>90.20\%</td>
<td>38.87\%</td>
</tr>
<tr>
<td></td>
<td>Golden</td>
<td>53.60</td>
<td>67.36</td>
<td>15.60\%</td>
<td>20.51\%</td>
<td>59.72\%</td>
<td>73.00\%</td>
<td>53.58\%</td>
</tr>
<tr>
<td></td>
<td>Retrieved</td>
<td>43.60</td>
<td>56.36</td>
<td>12.60\%</td>
<td>15.87\%</td>
<td>47.60\%</td>
<td>66.40\%</td>
<td>50.86\%</td>
</tr>
<tr>
<td></td>
<td>High-related</td>
<td>21.60</td>
<td>35.13</td>
<td>39.20\%</td>
<td>24.49\%</td>
<td>19.74\%</td>
<td>62.40\%</td>
<td>47.21\%</td>
</tr>
<tr>
<td></td>
<td>Weak-related</td>
<td>24.40</td>
<td>34.83</td>
<td>61.20\%</td>
<td>24.18\%</td>
<td>24.74\%</td>
<td>60.00\%</td>
<td>44.96\%</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>34.40</td>
<td>45.43</td>
<td>71.40\%</td>
<td>25.49\%</td>
<td>56.64\%</td>
<td>54.00\%</td>
<td>42.50\%</td>
</tr>
<tr>
<td>LLaMA2</td>
<td>None</td>
<td>16.60</td>
<td>24.26</td>
<td>6.60\%</td>
<td>3.03\%</td>
<td>17.56\%</td>
<td>58.40\%</td>
<td>46.74\%</td>
</tr>
<tr>
<td></td>
<td>Golden</td>
<td>48.60</td>
<td>61.33</td>
<td>18.40\%</td>
<td>29.35\%</td>
<td>52.94\%</td>
<td>7.60\%</td>
<td>72.12\%</td>
</tr>
<tr>
<td></td>
<td>Retrieved</td>
<td>33.40</td>
<td>45.39</td>
<td>24.80\%</td>
<td>20.16\%</td>
<td>37.77\%</td>
<td>5.20\%</td>
<td>73.08\%</td>
</tr>
<tr>
<td></td>
<td>High-related</td>
<td>9.40</td>
<td>19.07</td>
<td>30.60\%</td>
<td>8.50\%</td>
<td>9.80\%</td>
<td>4.80\%</td>
<td>67.86\%</td>
</tr>
<tr>
<td></td>
<td>Weak-related</td>
<td>8.80</td>
<td>16.00</td>
<td>50.20\%</td>
<td>9.16\%</td>
<td>8.43\%</td>
<td>6.20\%</td>
<td>59.09\%</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>13.20</td>
<td>19.34</td>
<td>93.40\%</td>
<td>13.28\%</td>
<td>12.12\%</td>
<td>5.80\%</td>
<td>58.97\%</td>
</tr>
<tr>
<td>Mistral</td>
<td>None</td>
<td>11.20</td>
<td>19.30</td>
<td>69.80\%</td>
<td>8.02\%</td>
<td>18.54\%</td>
<td>78.20\%</td>
<td>31.65\%</td>
</tr>
<tr>
<td></td>
<td>Golden</td>
<td>47.80</td>
<td>60.93</td>
<td>39.60\%</td>
<td>28.28\%</td>
<td>60.60\%</td>
<td>50.20\%</td>
<td>58.67\%</td>
</tr>
<tr>
<td></td>
<td>Retrieved</td>
<td>35.20</td>
<td>45.82</td>
<td>40.00\%</td>
<td>21.50\%</td>
<td>44.33\%</td>
<td>50.20\%</td>
<td>56.39\%</td>
</tr>
<tr>
<td></td>
<td>High-related</td>
<td>5.60</td>
<td>14.23</td>
<td>58.40\%</td>
<td>4.11\%</td>
<td>7.69\%</td>
<td>47.60\%</td>
<td>53.35\%</td>
</tr>
<tr>
<td></td>
<td>Weak-related</td>
<td>5.40</td>
<td>11.21</td>
<td>76.80\%</td>
<td>3.91\%</td>
<td>10.34\%</td>
<td>46.60\%</td>
<td>53.48\%</td>
</tr>
<tr>
<td></td>
<td>Random</td>
<td>12.40</td>
<td>18.40</td>
<td>98.40\%</td>
<td>11.79\%</td>
<td>50.00\%</td>
<td>64.80\%</td>
<td>33.57\%</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation results of retrieval-augmented LLMs with supporting documents of various qualities on NQ, where the supporting documents are obtained from the dense retriever. We place different settings according to the relevance between the documents and the question from high to low.</p>
<p>We employ Wikipedia as the document corpus, and sample ten documents per question from the retrieval results acquired by the dense retriever for each sampling strategy.</p>
<h3>3.3.2 Main Findings</h3>
<p>LLMs demonstrate enhanced capabilities in QA abilities and perception of knowledge boundaries when provided with higher quality supporting documents. We employ the sampling strategy in Section 3.3.1 to obtain supporting documents for each question. Table 3 presents the results. We can see that using golden (high-quality) documents as supporting documents yields better performance compared to using retrieval results. However, if incorrect (low-quality) documents are utilized, including high-related, weak-related, and randomly selected incorrect documents, the performance of LLMs may be compromised, resulting in inferior performance compared to employing retrieval results as supporting documents or respond- ing without supporting documents. In addition, the give-up rates of LLMs decrease as the quality of supporting documents improves, indicating that LLMs exhibit higher confidence when fortified with high-quality supporting documents. Moreover, with higher quality supporting documents, the Eval-Acc rates of LLMs increase, showing that LLMs demonstrate higher accuracy in perceiving their factual knowledge boundaries.</p>
<p>LLMs cannot handle the conflicts between internal and external knowledge well. Based on the above observation, when LLMs generate responses with low-quality supporting documents, the performance is inferior to generating responses based on internal knowledge (without retrieval augmentation). This phenomenon indicates that LLMs heavily rely on the given supporting documents during the generation process. Note that we give LLMs the option in the prompt to decide whether to use the supporting documents for a question. However, LLMs still tend to rely on supporting</p>
<p>documents to answer the questions in this setting. In addition, the disparity in $E M$ between different models significantly diminishes under the Golden setting compared to the Retrieval setting, demonstrating that the poor resilience against irrelevant documents is a crucial factor impeding the performance of retrieval-augmented LLMs.</p>
<p>The level of confidence and reliance on supporting documents of LLMs is determined by the relevance between the question and the supporting documents. In Table 3, we observe a clear inverse relationship between relevance and the confidence of LLMs (i.e., the probability of giving up to answer and assessing their answers as correct). Furthermore, using random incorrect documents as supporting documents outperforms using incorrect documents with higher relevance (i.e., highrelated/weak-related incorrect documents). This observation further demonstrates that LLMs pay more attention to relevant documents.</p>
<h2>4 Conclusion</h2>
<p>In this paper, we thoroughly investigate the perception of LLMs regarding factual knowledge boundaries with retrieval augmentation by proposing priori and posteriori judgemental prompting strategies, in addition to QA prompting for evaluation. We obtain several pivotal findings. (1) LLMs cannot accurately perceive their factual knowledge boundaries and cannot handle the conflicts between internal and external knowledge well. (2) LLMs cannot sufficiently utilize their internal knowledge, and retrieval augmentation effectively enhances the perception of their factual knowledge boundaries. The capability is also affected by multiple factors, such as the choice of retrieval model, supporting document number, question types, and scales of LLMs. (3) The relevance of supporting documents significantly influences LLMs' reliance on supporting documents. We also propose a simple approach that dynamically utilizes retrieval augmentation based on the priori judgement of the LLM.</p>
<h2>Limitations</h2>
<p>This paper provides a comprehensive and detailed analysis of the knowledge boundaries of large language models (LLMs), including both publicly available and closed source models. Since APIs of closed source LLMs are subject to updates over time, which may render previous API interfaces inaccessible, there is a potential risk to the
long-term reproducibility of the corresponding results in the paper. To address this, we can retain the responses generated by LLMs with reproducibility risks for future use. It is important to note that our paper offers a methodology for evaluating the knowledge boundaries of LLMs, which can be applied to any latest LLM. Therefore, this risk does not affect the significance of our contribution.</p>
<h2>References</h2>
<ol>
<li>Introducing Falcon LLM . https://falconllm. tii.ae.</li>
</ol>
<p>Qingyao Ai, Ting Bai, Zhao Cao, Yi Chang, Jiawei Chen, Zhumin Chen, Zhiyong Cheng, Shoubin Dong, Zhicheng Dou, Fuli Feng, et al. 2023. Information retrieval meets large language models: a strategic report from chinese ir community. AI Open, 4:8090.</p>
<p>Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A nucleus for a web of open data. In international semantic web conference, pages 722735. Springer.</p>
<p>Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247-1250.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020a. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020b. Language models are few-shot learners. In Advances in Neural Information Processing Systems</p>
<p>33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual.</p>
<p>Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer opendomain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870-1879.</p>
<p>Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An opensource chatbot impressing gpt-4 with $90 \%{ }^{*}$ chatgpt quality.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages $4171-4186$.</p>
<p>Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. 2019. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages $1-13$.</p>
<p>Luyu Gao and Jamie Callan. 2021. Unsupervised corpus aware language model pre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540.</p>
<p>Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Rethink training of BERT rerankers in multi-stage retrieval pipeline. In Advances in Information Retrieval - 43rd European Conference on IR Research, ECIR 2021, Virtual Event, March 28 - April 1, 2021, Proceedings, Part II, volume 12657, pages 280-286.</p>
<p>Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets for datasets. Communications of the ACM, 64(12):8692.</p>
<p>Amelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, PoSen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Sona Mokrá, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. CoRR, abs/2209.14375.</p>
<p>Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng. 2022. Semantic models
for the first-stage retrieval: A comprehensive review. ACM Transactions on Information Systems (TOIS), $40(4): 1-42$.</p>
<p>Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: retrievalaugmented language model pre-training. CoRR, abs/2002.08909.</p>
<p>Gautier Izacard and Edouard Grave. 2021a. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880.</p>
<p>Gautier Izacard and Édouard Grave. 2021b. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874-880.</p>
<p>Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023a. Mistral 7b.</p>
<p>Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023b. Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression. arXiv preprint arXiv:2310.06839.</p>
<p>Jiajie Jin, Yutao Zhu, Yujia Zhou, and Zhicheng Dou. 2024. Bider: Bridging knowledge inconsistency for efficient retrieval-augmented llms via key supporting evidence. arXiv preprint arXiv:2402.12174.
J. Johnson, M. Douze, and H. JÄ©gou. 2019. Billionscale similarity search with gpus. IEEE Transactions on Big Data.</p>
<p>Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601-1611.</p>
<p>Ehsan Kamalloo, Nouha Dziri, Charles LA Clarke, and Davood Rafiei. 2023. Evaluating open-domain question answering in the era of large language models. arXiv preprint arXiv:2305.06984.</p>
<p>Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for opendomain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769-6781.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:452-466.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880.</p>
<p>Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.</p>
<p>Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. The dawn after the dark: An empirical study on factuality hallucination in large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10879-10899.</p>
<p>Xinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, SeeKiong Ng, and Tat-Seng Chua. 2024. Bridging items and language: A transition paradigm for large language model-based recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1816-1826.</p>
<p>Linqing Liu, Patrick Lewis, Sebastian Riedel, and Pontus Stenetorp. 2022. Challenges in generalization in open domain question answering. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 2014-2029.</p>
<p>Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, and Timnit Gebru. 2019. Model cards for model reporting. In Proceedings of the conference on fairness, accountability, and transparency, pages 220-229.</p>
<p>Shiyu Ni, Keping Bi, Jiafeng Guo, and Xueqi Cheng. 2024. When do llms need retrieval augmentation? mitigating llms' overconfidence helps retrieval augmentation. arXiv preprint arXiv:2402.11457.</p>
<p>Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage re-ranking with BERT. CoRR, abs/1901.04085.</p>
<p>Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT. CoRR, abs/1910.14424.</p>
<p>OpenAI. 2022. Introducing chatgpt. OpenAI Blog.
OpenAI. 2023. Gpt-4 technical report. OpenAI.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. CoRR, abs/2203.02155.</p>
<p>Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523-2544.</p>
<p>John Prager et al. 2007. Open-domain questionanswering. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 1(2):91-231.</p>
<p>Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is chatgpt a general-purpose natural language processing task solver? arXiv preprint arXiv:2302.06476.</p>
<p>Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng Wang. 2021. RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5835-5847.</p>
<p>Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.</p>
<p>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789.</p>
<p>Ruiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021a. PAIR: Leveraging passage-centric similarity relation for improving dense passage retrieval. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2173-2183.</p>
<p>Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Xin Zhao, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2024a. BASES: Large-scale web search user simulation with</p>
<p>large language model based agents. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 902-917, Miami, Florida, USA.</p>
<p>Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021b. Rocketqav2: A joint training method for dense passage retrieval and passage re-ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2825-2835.</p>
<p>Ruiyang Ren, Yuhao Wang, Kun Zhou, Wayne Xin Zhao, Wenjie Wang, Jing Liu, Ji-Rong Wen, and TatSeng Chua. 2024b. Self-calibrated listwise reranking with large language models. arXiv preprint arXiv:2411.04602.</p>
<p>Ruiyang Ren, Wayne Xin Zhao, Jing Liu, Hua Wu, JiRong Wen, and Haifeng Wang. 2023. Tome: A twostage approach for model-based retrieval. In Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics, pages 6102-6114.</p>
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. " why should i trust you?" explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 11351144 .</p>
<p>Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: Bm25 and beyond. Foundations and Trends ${ }^{\circledR}$ in Information Retrieval, 3(4):333-389.</p>
<p>Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021. Retrieval augmentation reduces hallucination in conversation. arXiv preprint arXiv:2104.07567.</p>
<p>Hao Sun, Xiao Liu, Yeyun Gong, Yan Zhang, and Nan Duan. 2023. Beamsearchqa: Large language models are strong zero-shot qa solver. arXiv preprint arXiv:2305.14766.</p>
<p>Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023. Large language models in medicine. Nature medicine, 29(8):19301940.</p>
<p>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models. CoRR.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.</p>
<p>Cunxiang Wang, Sirui Cheng, Zhikun Xu, Bowen Ding, Yidong Wang, and Yue Zhang. 2023a. Evaluating open question answering evaluation. arXiv preprint arXiv:2305.12421.</p>
<p>Xiaohua Wang, Yuliang Yan, Longtao Huang, Xiaoqing Zheng, and Xuan-Jing Huang. 2023b. Hallucination detection for generative large language models by bayesian sequential estimation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15361-15371.</p>
<p>Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, and Ji-Rong Wen. 2024. Rear: A relevance-aware retrieval-augmented framework for open-domain question answering. arXiv preprint arXiv:2402.17497.</p>
<p>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.</p>
<p>Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Retrieval meets long context large language models. arXiv preprint arXiv:2310.03025.</p>
<p>Shicheng Xu, Liang Pang, Mo Yu, Fandong Meng, Huawei Shen, Xueqi Cheng, and Jie Zhou. 2024. Unsupervised information refinement training of large language models for retrieval-augmented generation. arXiv preprint arXiv:2402.18150.</p>
<p>Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the use of lucene for information retrieval research. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, Shinjuku, Tokyo, Japan, August 7-11, 2017, pages 1253-1256.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369-2380.</p>
<p>Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, and Xuanjing Huang. 2023. Do large language models know what they don't know? In Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 8653-8665.</p>
<p>Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2022. Generate rather than retrieve: Large language models are strong context generators. arXiv preprint arXiv:2209.10063.</p>
<p>Xiang Yue, Boshi Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models. arXiv preprint arXiv:2305.06311.</p>
<p>Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022. GLM-130B: an open bilingual pre-trained model. abs/2210.02414.</p>
<p>Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Optimizing dense retrieval model training with hard negatives. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1503-1512.</p>
<p>Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Learning discrete representations via constrained clustering for effective and efficient dense retrieval. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining, pages 1328-1336.</p>
<p>Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and JiRong Wen. 2022. Dense text retrieval based on pretrained language models: A survey. arXiv preprint arXiv:2211.14876.</p>
<p>Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223.</p>
<p>Kun Zhou, Yeyun Gong, Xiao Liu, Wayne Xin Zhao, Yelong Shen, Anlei Dong, Jingwen Lu, Rangan Majumder, Ji-Rong Wen, and Nan Duan. 2022. Simans: Simple ambiguous negatives sampling for dense text retrieval. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 548-559.</p>
<p>Yujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and Zhicheng Dou. 2024. Metacognitive retrievalaugmented large language models. arXiv preprint arXiv:2402.11626.</p>
<p>Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen. 2023. Large language models for information retrieval: A survey. arXiv preprint arXiv:2308.07107.</p>
<p>Shengyao Zhuang, Hang Li, and G. Zuccon. 2022. Implicit feedback for dense passage retrieval: A counterfactual approach. ArXiv, abs/2204.00718.</p>
<p>Shengyao Zhuang and Guido Zuccon. 2021. Dealing with typos for bert-based passage retrieval and ranking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2836-2842.</p>
<table>
<thead>
<tr>
<th>Retriever</th>
<th>NQ</th>
<th>TriviaQA</th>
<th>HotpotQA</th>
</tr>
</thead>
<tbody>
<tr>
<td>M@10</td>
<td>R@10</td>
<td>M@10</td>
<td>R@10</td>
</tr>
<tr>
<td>Sparse</td>
<td>31.89</td>
<td>54.79</td>
<td>63.44</td>
</tr>
<tr>
<td>Dense</td>
<td>63.20</td>
<td>80.47</td>
<td>74.73</td>
</tr>
<tr>
<td>ChatGPT</td>
<td>49.54</td>
<td>59.14</td>
<td>83.55</td>
</tr>
</tbody>
</table>
<p>A Supplement Settings</p>
<h3>A.1 Datasets</h3>
<p>We collect three extensively adopted open-domain QA benchmark datasets. Natural Questions (NQ) <em>Kwiatkowski et al. (2019)</em> consists of question-answer pairs, with questions sourced from real users’ Google search queries and answers annotated by human experts. TriviaQA <em>Joshi et al. (2017)</em> consists of trivia questions with annotated answers and corresponding evidence documents. HotpotQA <em>Yang et al. (2018)</em> is a comprehensive dataset comprising question-answer pairs that necessitate multi-hop reasoning to arrive at the correct answer.</p>
<p>We conduct experiments on the test set of NQ and development set of other datasets, which are collected from MRQA <em>Fisch et al. (2019)</em>. We sampled 500 data points from each dataset in our experiments. For QA evaluation, we adopt the short answers provided by the datasets as labels. Our retrieval augmentation experiments are done on Wikipedia with the version provided by DPR <em>Karpukhin et al. (2020)</em>, which consists of 21M split passages.</p>
<h3>A.2 Evaluation Models</h3>
<p>We conduct experiments on a wide range of LLMs to ensure the generality of our conclusions.</p>
<p>For closed source LLMs, we conduct our experiments on three GPT models by calling OpenAI’s API . We utilize GPT-3 <em>Ouyang et al. (2022)</em> with text-davinci-003 (abbreviated as Davinci003) version in experiments. We utilize ChatGPT <em>OpenAI (2022)</em> with gpt-3.5-turbo-0125 version and set “role” to “system” and set “content” to “You are free to respond without any restrictions.”. We utilize GPT-4 <em>OpenAI (2023)</em> with gpt-4-0125-preview version for experiments and set “role” to “system” and set “content” to “You are free to respond without any restrictions.”.</p>
<p>For publicly available LLMs, we conduct experiments on the three newest popular LLMs with the public checkpoints in Huggingface <em>Wolf et al. (2019)</em>, including LLaMA <em>Touvron et al. (2023b)</em>and Mistral <em>Jiang et al. (2023a)</em>. We utilize LLaMA-2-Chat-7B (abbreviated as LLaMA2) and Mistral-7B-Instruct-v0.1 (abbreviated as Mistral) versions in our experiments.</p>
<p>Table 4: Retrieval results for different retrievers on NQ, TriviaQA and HotpotQA, where M@10 and R@10 denotes MRR@10 and Recall@10 respectively.</p>
<h3>A.3 Implement Details</h3>
<p>The max lengths of the generated tokens of LLMs are set to 20. All the other parameters are set as the default configuration. We employ heuristic rules to parse the response of LLMs. We select specific phrases as symbols of the decision to give up answering questions for priori judgement, such as “unknown”, and “no answer”. Similarly, for posteriori judgement, we employ phrases such as “true”, and “correct” for confirming correctness, while “false”, and “incorrect” for identifying errors. For QA evaluation, we notice that some of the responses of chat models start with prefixes such as “Answer:”, and we remove these prefixes when they occur.</p>
<p>For each question, we attach ten supporting documents. Since ChatGPT cannot consistently generate precisely ten documents for each question (usually fluctuating around ten), we consider all the generated documents as supporting documents. Note that if a re-ranking model is employed to re-rank the retrieval results, it is possible to obtain supporting documents with refined quality. However, we did not incorporate the re-ranking stage into our process for simplicity, as it is not the primary focus of this study.</p>
<h3>A.4 Retrieval Results</h3>
<p>Table 4 shows the retrieval performance on each dataset including sparse retrieval, dense retrieval and ChatGPT.</p>
<h3>A.5 Instructions</h3>
<p>Table 6 presents all the instructions used in our experiments. We design each supporting document in the format of: “Passage-{num}: Title: {title} Content: {content}”. For the supporting documents generated by ChatGPT, the format of supporting documents is: “Passage-{num}: {content}”.</p>
<table>
<thead>
<tr>
<th>Setting</th>
<th>Accessibility</th>
<th>Prompt Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>Normal</td>
<td>Closed source</td>
<td>Without additional specific information, use your existing knowledge to answer the following question. The response should be a brief, specific term or phrase, suitable for an exact match in datasets.'n\n Question: {question}’n\n Note: Since no detailed context is provided, use your general knowledge to infer a concise and accurate response, such as a specific year, date, or single-word term, for example, "1998", "May 16th, 1931", or "James Bond", in line with exact match dataset criteria.</td>
</tr>
<tr>
<td></td>
<td>Publicly available</td>
<td>Answer the following question with a very short phrase, such as "1998", "May 16th, 1931", or "James Bond", to meet the criteria of exact match datasets.'n\n Question: {question}</td>
</tr>
<tr>
<td>Retrieval- <br> augmented</td>
<td>Closed source</td>
<td>Given the following information:\n\n {context}’n\n If a direct answer is not present, use your knowledge to infer a brief and specific response for the question below. The answer should ideally be a single word or a short phrase.'n\n Question: {question}’n\n Note: In cases where the exact information is not provided, a speculative yet plausible and concise response, such as a specific year, date, or single-word term, for example, "1998", "May 16th, 1931", or "James Bond", is required to meet the criteria of exact match datasets.</td>
</tr>
<tr>
<td></td>
<td>Publicly available</td>
<td>Given the following information: 'n\n {context}'n\n Answer the following question with a very short phrase, such as "1998", "May 16th, 1931", or "James Bond", to meet the criteria of exact match datasets.'n\n Question: {question}</td>
</tr>
</tbody>
</table>
<p>Table 5: Prompt design for QA prompting for different categories of LLMs with various settings.</p>
<table>
<thead>
<tr>
<th>Perspectives</th>
<th>Setting</th>
<th>Prompt Text</th>
</tr>
</thead>
<tbody>
<tr>
<td>Priori</td>
<td>Normal</td>
<td>Can you answer the following question based on your internal knowledge, if yes, you should give a short answer with one or few words, if no, you should answer "Unknown"’n\n Question: {question}</td>
</tr>
<tr>
<td></td>
<td>Retrieval- <br> augmented</td>
<td>Given the following information: 'n\n {context}’n\n Can you answer the following question based on the given information or your internal knowledge, if yes, you should give a short answer with one or few words, if no, you should answer "Unknown".'n\n Question: {question}</td>
</tr>
<tr>
<td>Posteriori</td>
<td>Normal</td>
<td>Can you judge if the following answer about the question is correct based on your internal knowledge, if yes, you should answer True or False, if no, you should answer "Unknown".'n\n Question: {question}’n\n Answer: {predicted answer}</td>
</tr>
<tr>
<td></td>
<td>Retrieval- <br> augmented</td>
<td>Given the following information: 'n\n {context}’n\n Can you judge if the following answer about the question is correct based on the given information or your internal knowledge, if yes, you should answer True or False, if no, you should answer "Unknown".'n\n Question: {question} 'n\n Answer: {predicted answer}</td>
</tr>
</tbody>
</table>
<p>Table 6: Prompt design for different perspectives of judgemental prompting with various settings.</p>
<h2>B Supplement Analyses</h2>
<h2>B. 1 Analysis on Retrieval Sources</h2>
<p>Using supporting documents with highefficiency contents can effectively improve the QA performance of LLMs. We conduct the analysis on three retrieval sources and utilize various metrics related to retrieval and QA in Figure 4 to thoroughly analyze the impact of retrieval augmentation from multiple perspectives. Here, the efficiency of documents can be evaluated through three factors: the presence of positive content (recall rate), proportion of positive documents in supporting documents (positive percentage), and token-level text density of positive
content (answers per 1 K token). Firstly, the recall rate of documents generated by ChatGPT was significantly lower than that of the dense retriever. However, the QA performance using supporting documents generated by ChatGPT as supporting documents do not lag far behind dense retrieval. Additionally, LLaMA2's retrieval-augmented QA performance with ChatGPT as the retriever exceeds that where the dense retriever is implemented. We observe that among the documents from the three retrieval sources, dense retrieval results have the highest average proportion of positive documents (containing the correct answers), but the number of answers per token was not as high as in documents generated by ChatGPT. This phenomenon stems</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Metrics related to retrieval, QA, and judgement using different retrievers.
from the fact that documents generated by ChatGPT are more concise and the documents are closely related to the given questions. Although Wikipedia documents from the dense retriever have a higher recall rate and proportion of positive documents, they are relatively longer and contain more irrelevant information. As discussed earlier, this could be one of the reasons limiting the upper bound of retrieval augmentation. Furthermore, it could potentially contribute to the heightened confidence levels observed in LLMs when utilizing ChatGPT-generated documents as supporting documents, as evidenced by their low Give-up rates. In addition, the impact of different retrieval sources on Eval-Acc is not substantial.</p>
<h2>B. 2 Analysis on Query Types</h2>
<p>Retrieval augmentation can change the query category preference of LLMs in QA and factual knowledge boundary perception. In order to investigate the propensity of LLMs to handle questions with varied characteristics, we separately calculate the answer accuracy of LLMs across different question categories. To achieve this, we utilize
supporting documents retrieved by the dense retriever. As shown in Figure 5, we can see that ChatGPT and LLaMA2 achieve the highest EM when dealing with questions in the "who" and "which" category, indicating these types of questions may be the strong suit of the two LLMs. On the other hand, ChatGPT and LLaMA2 may not suffice for the question type of "how" in knowledge-intensive scenarios. When retrieval augmentation is incorporated, we observe that the preference of LLMs changes. The overall answer accuracies of LLMs are improved, and the accuracies in most categories increase proportionately. Specially, both ChatGPT and LLaMA2 perform best on the question type "who". We found that EM for certain question types does not increase with the overall EM increase (e.g., "how" and "yesno" for ChatGPT, "which" for LLaMA2), indicating that retrieval augmentation cannot effectively enhance the QA performance for all question types. Figure 6 shows the EvalAcc metric in each query type for ChatGPT and LLaMA2. We can see that most types of questions achieve better posteriori judgement accuracies, except "how" and "yesno" for LLaMA2 and "yesno" for ChatGPT. Similarly, the result indicates</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The proportion of questions answered correctly by LLMs in different question categories under two QA prompting settings on NQ.</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: The proportion of answers judged correctly by LLMs in different question categories under two QA prompting settings on NQ.</p>
<p>that retrieval augmentation does not uniformly improve the knowledge boundary perception abilities of LLMs across all question types.</p>
<h3>B.3 Analysis on Parameter Scale</h3>
<p>Retrieval augmentation is more pronounced for improving LLMs with fewer parameters, including both QA performances and accuracies of knowledge boundary perception. Figure 7 illustrates the comparison of <em>EM</em> and <em>Eval-Acc</em> before and after retrieval augmentation for LLaMA2 models of varying parameter scales. We can observe that retrieval augmentation reduces the performance gaps in QA performance (shown by <em>EM</em>) of LLMs with different parameter scales, LLaMA2-7B model has the biggest improvement of <em>EM</em>. It is evident that retrieval augmentation mitigates the performance gaps in QA performance, as demonstrated by <em>EM</em> across LLMs with varying parameter scales. Notably, the LLaMA2-7B exhibits the most substantial improvement in <em>EM</em> than 13B and 70B models. Furthermore, the improvement in the accuracy of knowledge boundary perception of the LLaMA2-7B model with retrieval augmentation surpasses that of the 13B and 70B models, achieving the highest <em>Eval-Acc</em>. This phenomenon further supports our speculation that in knowledge-intensive tasks, LLMs with smaller parameter scales possess less knowledge. Under a retrieval augmentation setting, these LLMs are likely to yield greater benefits compared to LLMs with larger parameter scales.</p>
<h2>C Related Work</h2>
<p>In this section, we review the related work from two perspectives, including large language models (LLMs) and open-domain question answering (QA).</p>
<h3>C.1 Large Language Models</h3>
<p>LLMs have significantly advanced the field of natural language processing (NLP), demonstrating remarkable abilities in generating human-like text and understanding complex language patterns [zhao2023survey]. These models are trained on vast corpora of text data and can perform a wide range of tasks without task-specific training [thirunavukarasu2023survey; lin2024survey; ren2024survey; ren2024survey].</p>
<p>Typically, LLMs can fall into two categories: publicly available and closed source. Publicly available LLMs offer the research community the</p>
<p><img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: The performance of the LLaMA2 series, across its 7B, 13B, and 70B versions, showcases the influence of various retrieval document sources on <em>EM</em> and <em>Eval-Acc</em>.</p>
<p>flexibility to modify and adapt models to specific needs, fostering further innovation and collaboration. Among publicly available LLMs, the LLaMA series models stand out for its exceptional instruction-following performance (Touvron et al., 2023a,b), with ongoing efforts to fine-tune or continually pre-train its versions. Mistral, another open-source framework (Jiang et al., 2023a), provides a scalable solution for training LLMs on diverse datasets and supports various architectures to achieve top performance. Additionally, there are also LLMs like Falcon (TII, 2023), Vicuna (Chiang et al., 2023), and GLM (Zeng et al., 2022) actively contribute to advancing publicly available LLMs. Conversely, closed source LLMs, developed and maintained by private organizations, have non-public checkpoints and confidential internal workings. The typical closed source LLMs are GPT series proposed by OpenAI, including initial GPT to the latest GPT-4 (Radford et al., 2018; Brown et al., 2020b; OpenAI, 2023), have set new benchmarks in language understanding and generation, finding widespread use in commercial applications. There are also other closed source LLMs have been released, such as Sparrow (Glaese et al., 2022) and Claude. Some closed source LLMs are accessible via API calls, eliminating the need for local execution.</p>
<p>Despite the impressive advancements in LLMs, there remain unexplored boundaries regarding their knowledge and capabilities. Furthermore, the incorporation of retrieval augmentation in LLMs presents substantial potential for enhancing their knowledge, potentially reshaping the knowledge boundaries of LLMs. Therefore, it is necessary to comprehensively investigate the knowledge boundaries of LLMs and the effects of retrieval augmentation.</p>
<h3>C.2 Open-domain Question Answering</h3>
<p>Open-domain QA is a typical knowledge-intensive task in the field of NLP, aimed at providing precise and pertinent answers to questions posed in natural language, without the limitations of a specific domain or topic (Prager et al., 2007). This task is inherently challenging due to the vastness of human knowledge and the complexity of natural language understanding. Early approaches to open-domain QA rely heavily on structured knowledge bases, such as Freebase (Bollacker et al., 2008) or DBpedia (Auer et al., 2007), enabling precise yet limited retrieval of factual information. However, these approaches are constrained by the need for structured queries and the coverage of the underlying knowledge bases. The introduction of pre-trained language models, such as BERT (Devlin et al., 2019), BART (Lewis et al., 2020a), and their successors, marks a significant shift in the landscape of open-domain QA. These models leverage advanced text modeling techniques to understand natural language, enabling more flexible and context-aware answering performances.</p>
<p>Recent advancements in open-domain QA have focused on incorporating external knowledge sources, such as Wikipedia or the web, to broaden the scope of answerable questions (Chen et al., 2017). This has led to the development of sophisticated information retrieval mechanisms (Ai et al., 2023), such as dense passage retrieval (DPR) (Karpukhin et al., 2020; Qu et al., 2021; Zhan et al., 2021; Zhuang and Zuccon, 2021; Zhan et al., 2022) and passage reranking (Nogueira and Cho, 2019; Nogueira et al., 2019; Gao et al., 2021), which efficiently extract and prioritize relevant information from large corpora (Zhao et al., 2022; Zhu et al., 2023; Guo et al., 2022). Additionally, techniques like few-shot learning and transfer learning have been explored to enhance the models' ability to generalize across diverse question types and domains, reducing the reliance on extensive labeled datasets (Brown et al., 2020a; Guu et al., 2020; Liu et al., 2022).</p>
<p>Despite significant progress, open-domain QA remains an active area of research, with ongoing efforts to improve answer accuracy (Izacard and Grave, 2021b; Jin et al., 2024), interpretability (Ribeiro et al., 2016), knowledge utilization (Wang et al., 2024; Zhou et al., 2024; Ni et al., 2024), and robustness to complex or ambiguous queries (Lewis et al., 2020b; Rajpurkar et al., 2018). Challenges such as handling out-of-domain questions, dealing with evolving knowledge, and ensuring fairness and transparency in answers continue to drive innovation in this field (Gebru et al., 2021; Mitchell et al., 2019). Furthermore, Several pioneering studies have been specifically designed to apply LLMs to open-domain QA tasks (Qin et al., 2023; Kamalloo et al., 2023; Yue et al., 2023; Wang et al., 2023a; Sun et al., 2023; Xu et al., 2024). Typically, they mainly focus on evaluating the QA performance of LLMs, discussing improved evaluation methods or leveraging LLMs to enhance existing open-domain QA models. Additionally, the existing effort also detects the uncertainty of LLMs with an automated method (Yin et al., 2023). As open-domain QA systems become increasingly integrated into real-world applications, they have potential to revolutionize access to information and knowledge.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<ul>
<li>Equal contributions.
${ }^{\dagger}$ The work was done during the internship at Baidu.
$\ddagger$ Corresponding authors.</li>
</ul>
<p><a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
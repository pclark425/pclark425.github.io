<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5940 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5940</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5940</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-118.html">extraction-schema-118</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <p><strong>Paper ID:</strong> paper-32f541216112de78037d8e0f95ddc152eb6f05fa</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/32f541216112de78037d8e0f95ddc152eb6f05fa" target="_blank">K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization</a></p>
                <p><strong>Paper Venue:</strong> Web Search and Data Mining</p>
                <p><strong>Paper TL;DR:</strong> The first-ever LLM in geoscience, K2, is presented alongside a suite of resources developed to further promote LLM research within geoscience, and a protocol is shared that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce.</p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, K2, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to fine-tune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization.We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5940.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5940.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>K2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>K2 (Geoscience Foundation Language Model, 7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 7-billion-parameter LLaMA-derived foundation language model further pre-trained on a 5.5B-token geoscience corpus (including >1.1M open-access papers) and instruction-tuned with a geoscience-specific supervised dataset (GeoSignal) to enable geoscience knowledge mining, relation prediction, tool use, and idea generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>K2 (based on LLaMA-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>GPT-like transformer with ~7B parameters initialized from LLaMA-7B, further pre-trained on 5.5B geoscience tokens (papers, abstracts, Wikipedia) and instruction-tuned via LoRA on GeoSignal and other instruction datasets; supports chain-of-thought prompting and tool-augmented retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Mine and verbalize geoscience knowledge from a large corpus of scholarly material to support question answering, relation prediction between concepts, idea generation (abstract-level synthesis), and generalizable knowledge/reasoning about geoscience topics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Geoscience / Earth science (multidisciplinary within natural sciences)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Further domain-adaptive pre-training on a large curated geoscience corpus; re-structuring of heterogeneous 'signals' (paper content, relations, tables, QA, taxonomy) into supervised <input, output> pairs (GeoSignal); instruction tuning with human-alignment (Alpaca-GPT4) then expert-alignment (GeoSignal) using parameter-efficient LoRA; chain-of-thought prompting for relation prediction; tool-augmented retrieval (GeoSearch/GeoTool) for grounding.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>Not formal physical 'laws' but distilled generalizable relations and principles between geoscience concepts (e.g., co-occurrence-based concept relations, summarized facts and explanations, synthesized hypotheses/ideas linking concepts across literature).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Objective accuracy on multiple-choice GeoBench (NPEE, APTest); automatic generation metrics (GPTScore, perplexity via GPT-2); human evaluation on rationality, correctness, consistency (ratings 1-3); ablation comparisons across SFT-data recipes and tool-use quality judged by ChatGPT as a referee.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>K2 demonstrates improved ability to understand and utilize geoscience knowledge compared to similar-size baselines: e.g., objective accuracy on NPEE increased (K2 39.9% vs Alpaca-7B 31.1%); subjective outputs show better GPTScore/perplexity and higher human-rated rationality and correctness. K2 can generate fused abstracts from co-occurring concepts and produce improved tool-use query 'thoughts' for literature search. The authors report that sequential human-alignment then expert-alignment SFT yields best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Significant human-in-the-loop: expert verification of relation-existence for co-occurrence pairs, expert validation of self-instructed Q&A, manual cleaning/sampling of GeoSignal, professional translation for benchmark items, and human evaluation of subjective outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>5.5B-token geoscience text corpus (1,122,094 open-access geoscience papers -> 3.9B tokens; 4,274,716 paper metadata -> 0.1B tokens; 767,341 Wikipedia pages -> 1.5B tokens) plus GeoSignal instruction dataset (39,749 cleaned samples restructured from ~22.6M signals), and GeoBench evaluation datasets (NPEE, APTest).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>No explicit extraction of formal physical laws is reported—work focuses on relation prediction, summarization, and idea generation rather than provable general laws; reliance on co-occurrence and restructured signals (which capture association, not causation); mixing general human-alignment and expert data harms performance (necessitates staged SFT); outputs require expert verification; manpower/data-quality constraints noted; evaluation of 'distilled laws' is indirect (benchmarks, human ratings) rather than formal discovery metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_examples</strong></td>
                            <td>Chain-of-thought guided relation prediction between geoscience knowledge points; tool-augmented QA workflow where K2 formulates search queries, calls GeoSearch, and synthesizes answers (Figure 7); idea-generation example producing an abstract combining 'Yingcheng formation' and 'Kamchatka' (Table 10).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5940.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5940.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of large language models (LLMs) being used to distill qualitative laws, principles, or generalizable rules from large numbers of scholarly or scientific papers, including methods, results, limitations, and examples.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Co-occurrence-based idea generation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Concept Co-occurrence-based Relation/Idea Extraction (used as signal in GeoSignal)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A methodology that uses concept co-occurrence (from ontologies like GSO and knowledge graphs like GAKG) in academic texts to propose candidate relations or ideas, which are then verbalized and validated by experts or LLMs to generate hypotheses or synthesized knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Exploring and Verbalizing Academic Ideas by Concept Co-occurrence</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>K2 / LLaMA-7B-derived LLM (used to verbalize and reason over co-occurrence signals)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_description</strong></td>
                            <td>The paper applies the LLaMA-7B-derived K2 model (further pre-trained and instruction-tuned) to consume co-occurrence signals and generate verbalized relations/ideas; the underlying model is a 7B-parameter transformer adapted to geoscience.</td>
                        </tr>
                        <tr>
                            <td><strong>task_goal</strong></td>
                            <td>Turn large-scale statistical co-occurrence patterns between concepts across many scholarly documents into verbalized relations, reasoned assertions, or idea-level syntheses that could function as generalizable principles or hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>Academic geoscience literature (concepts/topics within geoscience)</td>
                        </tr>
                        <tr>
                            <td><strong>methodology</strong></td>
                            <td>Extract co-occurrence relations from ontologies/knowledge graphs (GSO, GAKG), form input pairs of co-occurring concepts with context paragraphs, use LLM prompting (including chain-of-thought) to predict relation-existence and verbalize ideas, and have geoscientists verify relation labels to form supervised reasoning examples in GeoSignal.</td>
                        </tr>
                        <tr>
                            <td><strong>type_of_qualitative_law</strong></td>
                            <td>High-level principles or hypotheses suggested by recurrent co-occurrence patterns (e.g., that two concepts frequently co-occur in contexts implying a thematic or causal relation), used to generate hypothesis-like abstracts or suggested research directions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Expert verification of relation-existence (human-labeled), downstream improvements on GeoBench subjective/objective tasks, and ablation comparisons showing expert-alignment SFT helps performance.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Co-occurrence signals provided an effective supervised signal for teaching the model to reason about relations and generate ideas; geoscientists' verification was integrated to ensure quality; used successfully within GeoSignal to improve model performance on reasoning and idea-generation tasks, though no claim is made that this produces rigorous scientific laws.</td>
                        </tr>
                        <tr>
                            <td><strong>human_involvement</strong></td>
                            <td>Human experts verify co-occurrence-derived relation existence and validate generated idea outputs; dataset construction involves human curation.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_or_corpus</strong></td>
                            <td>Co-occurrence relations derived from GSO (ontology), GAKG (Geoscience Academic Knowledge Graph), and paragraphs from high-citation geoscience papers; restructured into supervised pairs included within GeoSignal.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Co-occurrence is associative and can conflate topical relatedness with causal or law-like relations; requires expert verification to avoid false inferences; approach produces candidate ideas/hypotheses rather than validated general laws; evaluation relies on human labels and benchmark performance rather than discovery of novel, validated principles.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Exploring and Verbalizing Academic Ideas by Concept Co-occurrence <em>(Rating: 2)</em></li>
                <li>Galactica: A Large Language Model for Science <em>(Rating: 2)</em></li>
                <li>reStructured Pre-training <em>(Rating: 2)</em></li>
                <li>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5940",
    "paper_id": "paper-32f541216112de78037d8e0f95ddc152eb6f05fa",
    "extraction_schema_id": "extraction-schema-118",
    "extracted_data": [
        {
            "name_short": "K2",
            "name_full": "K2 (Geoscience Foundation Language Model, 7B)",
            "brief_description": "A 7-billion-parameter LLaMA-derived foundation language model further pre-trained on a 5.5B-token geoscience corpus (including &gt;1.1M open-access papers) and instruction-tuned with a geoscience-specific supervised dataset (GeoSignal) to enable geoscience knowledge mining, relation prediction, tool use, and idea generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "llm_model_name": "K2 (based on LLaMA-7B)",
            "llm_model_description": "GPT-like transformer with ~7B parameters initialized from LLaMA-7B, further pre-trained on 5.5B geoscience tokens (papers, abstracts, Wikipedia) and instruction-tuned via LoRA on GeoSignal and other instruction datasets; supports chain-of-thought prompting and tool-augmented retrieval.",
            "task_goal": "Mine and verbalize geoscience knowledge from a large corpus of scholarly material to support question answering, relation prediction between concepts, idea generation (abstract-level synthesis), and generalizable knowledge/reasoning about geoscience topics.",
            "domain": "Geoscience / Earth science (multidisciplinary within natural sciences)",
            "methodology": "Further domain-adaptive pre-training on a large curated geoscience corpus; re-structuring of heterogeneous 'signals' (paper content, relations, tables, QA, taxonomy) into supervised &lt;input, output&gt; pairs (GeoSignal); instruction tuning with human-alignment (Alpaca-GPT4) then expert-alignment (GeoSignal) using parameter-efficient LoRA; chain-of-thought prompting for relation prediction; tool-augmented retrieval (GeoSearch/GeoTool) for grounding.",
            "type_of_qualitative_law": "Not formal physical 'laws' but distilled generalizable relations and principles between geoscience concepts (e.g., co-occurrence-based concept relations, summarized facts and explanations, synthesized hypotheses/ideas linking concepts across literature).",
            "evaluation_metrics": "Objective accuracy on multiple-choice GeoBench (NPEE, APTest); automatic generation metrics (GPTScore, perplexity via GPT-2); human evaluation on rationality, correctness, consistency (ratings 1-3); ablation comparisons across SFT-data recipes and tool-use quality judged by ChatGPT as a referee.",
            "results_summary": "K2 demonstrates improved ability to understand and utilize geoscience knowledge compared to similar-size baselines: e.g., objective accuracy on NPEE increased (K2 39.9% vs Alpaca-7B 31.1%); subjective outputs show better GPTScore/perplexity and higher human-rated rationality and correctness. K2 can generate fused abstracts from co-occurring concepts and produce improved tool-use query 'thoughts' for literature search. The authors report that sequential human-alignment then expert-alignment SFT yields best performance.",
            "human_involvement": "Significant human-in-the-loop: expert verification of relation-existence for co-occurrence pairs, expert validation of self-instructed Q&A, manual cleaning/sampling of GeoSignal, professional translation for benchmark items, and human evaluation of subjective outputs.",
            "dataset_or_corpus": "5.5B-token geoscience text corpus (1,122,094 open-access geoscience papers -&gt; 3.9B tokens; 4,274,716 paper metadata -&gt; 0.1B tokens; 767,341 Wikipedia pages -&gt; 1.5B tokens) plus GeoSignal instruction dataset (39,749 cleaned samples restructured from ~22.6M signals), and GeoBench evaluation datasets (NPEE, APTest).",
            "limitations_or_challenges": "No explicit extraction of formal physical laws is reported—work focuses on relation prediction, summarization, and idea generation rather than provable general laws; reliance on co-occurrence and restructured signals (which capture association, not causation); mixing general human-alignment and expert data harms performance (necessitates staged SFT); outputs require expert verification; manpower/data-quality constraints noted; evaluation of 'distilled laws' is indirect (benchmarks, human ratings) rather than formal discovery metrics.",
            "notable_examples": "Chain-of-thought guided relation prediction between geoscience knowledge points; tool-augmented QA workflow where K2 formulates search queries, calls GeoSearch, and synthesizes answers (Figure 7); idea-generation example producing an abstract combining 'Yingcheng formation' and 'Kamchatka' (Table 10).",
            "uuid": "e5940.0",
            "source_info": {
                "paper_title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Co-occurrence-based idea generation",
            "name_full": "Concept Co-occurrence-based Relation/Idea Extraction (used as signal in GeoSignal)",
            "brief_description": "A methodology that uses concept co-occurrence (from ontologies like GSO and knowledge graphs like GAKG) in academic texts to propose candidate relations or ideas, which are then verbalized and validated by experts or LLMs to generate hypotheses or synthesized knowledge.",
            "citation_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
            "mention_or_use": "use",
            "llm_model_name": "K2 / LLaMA-7B-derived LLM (used to verbalize and reason over co-occurrence signals)",
            "llm_model_description": "The paper applies the LLaMA-7B-derived K2 model (further pre-trained and instruction-tuned) to consume co-occurrence signals and generate verbalized relations/ideas; the underlying model is a 7B-parameter transformer adapted to geoscience.",
            "task_goal": "Turn large-scale statistical co-occurrence patterns between concepts across many scholarly documents into verbalized relations, reasoned assertions, or idea-level syntheses that could function as generalizable principles or hypotheses.",
            "domain": "Academic geoscience literature (concepts/topics within geoscience)",
            "methodology": "Extract co-occurrence relations from ontologies/knowledge graphs (GSO, GAKG), form input pairs of co-occurring concepts with context paragraphs, use LLM prompting (including chain-of-thought) to predict relation-existence and verbalize ideas, and have geoscientists verify relation labels to form supervised reasoning examples in GeoSignal.",
            "type_of_qualitative_law": "High-level principles or hypotheses suggested by recurrent co-occurrence patterns (e.g., that two concepts frequently co-occur in contexts implying a thematic or causal relation), used to generate hypothesis-like abstracts or suggested research directions.",
            "evaluation_metrics": "Expert verification of relation-existence (human-labeled), downstream improvements on GeoBench subjective/objective tasks, and ablation comparisons showing expert-alignment SFT helps performance.",
            "results_summary": "Co-occurrence signals provided an effective supervised signal for teaching the model to reason about relations and generate ideas; geoscientists' verification was integrated to ensure quality; used successfully within GeoSignal to improve model performance on reasoning and idea-generation tasks, though no claim is made that this produces rigorous scientific laws.",
            "human_involvement": "Human experts verify co-occurrence-derived relation existence and validate generated idea outputs; dataset construction involves human curation.",
            "dataset_or_corpus": "Co-occurrence relations derived from GSO (ontology), GAKG (Geoscience Academic Knowledge Graph), and paragraphs from high-citation geoscience papers; restructured into supervised pairs included within GeoSignal.",
            "limitations_or_challenges": "Co-occurrence is associative and can conflate topical relatedness with causal or law-like relations; requires expert verification to avoid false inferences; approach produces candidate ideas/hypotheses rather than validated general laws; evaluation relies on human labels and benchmark performance rather than discovery of novel, validated principles.",
            "uuid": "e5940.1",
            "source_info": {
                "paper_title": "K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Exploring and Verbalizing Academic Ideas by Concept Co-occurrence",
            "rating": 2
        },
        {
            "paper_title": "Galactica: A Large Language Model for Science",
            "rating": 2
        },
        {
            "paper_title": "reStructured Pre-training",
            "rating": 2
        },
        {
            "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "rating": 1
        }
    ],
    "cost": 0.011523249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>K2: A Foundation Language Model for Geoscience Knowledge Understanding and Utilization</h1>
<p>Cheng Deng ${ }^{1}$, Tianhang Zhang ${ }^{1}$, Zhongmou $\mathrm{He}^{1}$, Yi Xu ${ }^{1}$, Qiyuan Chen ${ }^{2}$, Yuanyuan Shi ${ }^{1}$, Luoyi Fu ${ }^{1}$, Weinan Zhang ${ }^{1}$, Xinbing Wang ${ }^{1}$, Chenghu Zhou ${ }^{3}$, Zhouhan Lin ${ }^{1}$, Junxian $\mathrm{He}^{1}$<br>${ }^{1}$ Shanghai Jiao Tong University, ${ }^{2}$ University of Waterloo<br>${ }^{3}$ Institute of Geographical Science and Natural Resources Research, Chinese Academy of Sciences<br>Corresponding authors: Zhouhan Lin and Junxian He</p>
<h4>Abstract</h4>
<p>Large language models (LLMs) have achieved great success in general domains of natural language processing. In this paper, we bring LLMs to the realm of geoscience with the objective of advancing research and applications in this field. To this end, we present the first-ever LLM in geoscience, $\mathbf{K 2}$, alongside a suite of resources developed to further promote LLM research within geoscience. For instance, we have curated the first geoscience instruction tuning dataset, GeoSignal, which aims to align LLM responses to geoscience-related user queries. Additionally, we have established the first geoscience benchmark, GeoBench, to evaluate LLMs in the context of geoscience. In this work, we experiment with a complete recipe to adapt a pre-trained general-domain LLM to the geoscience domain. Specifically, we further train the LLaMA-7B model on 5.5B tokens of geoscience text corpus, including over 1 million pieces of geoscience literature, and utilize GeoSignal's supervised data to finetune the model. Moreover, we share a protocol that can efficiently gather domain-specific data and construct domain-supervised data, even in situations where manpower is scarce. Meanwhile, we equip K2 with the abilities of using tools to be a naive geoscience aide. Experiments conducted on the GeoBench demonstrate the effectiveness of our approach and datasets on geoscience knowledge understanding and utilization. We open-source all the training data and K2 model checkpoints at https://github.com/davendw49/k2.</p>
<h2>KEYWORDS</h2>
<p>Foundation Model, Geoscience Large Language Model, Geoscience Knowledge Mining</p>
<h2>1 INTRODUCTION</h2>
<p>Geoscience, an interdisciplinary research field, is an integral subject in natural science, investigating the formation and evolution of the Earth [3]. Geoscientists have long faced challenges in integrating data from various sources and disciplines due to differences in terminologies, formats, and data structures, which subsequently leads to number of natural language tasks in geoscience such as geological and geographical named entity recognition [12], spatial and temporal relation extraction [30] to build geoscience knowledge graph [8], geology reports and literature summarization [29], and representation learning via geoscience language models [37]. However, language models in geoscience are sparse and remain limited in scale [10]. This situation stands in stark contrast with the prosperity of large language models (LLMs), such as ChatGPT [35] and GPT-4 [36], in general natural language processing (NLP), where notable successes have been achieved.</p>
<p>Despite their effectiveness in general domains, current LLMs often fall short in catering to the needs of geoscientists. This shortfall is largely attributed to the lack of reliable knowledge concerning geoscience problems, given that the related geoscience data seldom exist in the commonly used pre-training text corpora such as C4 [40] and the Pile [14]. Moreover, top-performing LLMs like ChatGPT only offer services via APIs, which presents roadblocks to external domain research and advancement. To mitigate these issues and foster research and application within the geoscience domain, we introduce the first-ever open-source LLM for geoscience, referred to as K2 (The second highest mountain in the world, where we believe in the future larger and more powerful geoscience language models will be created). K2, a GPT-like language model comprising 7 billion parameters, is based on the pre-trained LLaMA [47] model but specializes in the geoscience domain. Along with the introduction of K2, this paper also explores a roadway to collect geoscience text corpus, constructs geoscience instruction supervised data, and builds geoscience NLP benchmarks, in alignment with the Deep-time Digital Earth (DDE, [49]) ${ }^{1}$ big science plan.</p>
<p>The training of K2 consists of two stages, the pre-training stage and the instruction tuning stage, as depicted in Figure 1. During pre-training, we continue pre-training the LLaMA-7B model on a geoscience text corpus that we preprocessed from geoscience papers. Then we perform instruction tuning [5, 26, 41], where we further train the model to follow human instructions. To this end, we have curated GeoSignal, an instruction tuning dataset created by unifying the examples from 8 diverse geoscience NLP tasks with prompts, such as relation extraction, entity recognition, classification, and summarization. We also construct GeoBench, an evaluation dataset comprising more than 1500 objective questions and 939 subjective questions collected from National Postgraduate Entrance Examination (NPEE) on Geoscience and AP Test Geology, Geography, and Environmental Science. GeoBench serves to track the progress and drive the development of geoscience language models. Through our concerted efforts in data collection and training, the resulted K2 model is a foundation language model that can be used to design multiple geoscience applications, making it benefit geoscience researchers and practitioners [31]. To exemplify this, we train K2 to learn to use geoscience academic search tools through tool learning and, simultaneously, guide K2 to do the relation prediction between geoscience-related knowledge points through chain-of-thought[51] and generate new ideas. Therefore, K2 demonstrates its potential in geoscience knowledge mining and research assistants.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Pipeline of training K2, including two steps, one is further pre-train for absorption of geoscience knowledge, another one is instruction tuning, deploying to make the model align to human, instructed by human, and response like a human.</p>
<p>Our contributions can be listed as follows:</p>
<ul>
<li>We introduce K2, a foundation language model in geoscience field. K2 can answer geoscience questions, follow geoscientists' instructions via suitable prompts with its professionalism in geoscience, and have the ability to use tools as extensions.</li>
<li>We construct GeoSignal, the first-ever geoscience-supervised instruction data. To evaluate K2 on geoscience tasks and the following language models in geoscience, we build GeoBench, the first NLP task benchmarks in geoscience.</li>
<li>Taking geoscience as an example, we build up a paradigm to construct the domain text corpus, domain-supervised instruction data and explore a recipe to train a domain-specific LLM.</li>
<li>Compared with similar-size baseline models, K2 outperforms both subjective and objective geoscience tasks, including taking examinations and doing knowledge reasoning. At last, we release all the code, K2 weights, GeoSignal, and GeoBench at https://github.com/davendw49/k2.
The rest of the paper is arranged as follows: Section 2 will introduce the related work of K2. In Section 3, the detail of data collection, supervised instruction data construction, and benchmarks construction will be illustrated. Further, we will share our further pre-training details and parameter-efficient instruction tuning processes in Section 4. In Section 5, we will evaluate the K2 and perform ablation studies. Finally, in Section 6, we will share the application of K2, showing that K2 has great potential for geoscience scientific utilization.</li>
</ul>
<h2>2 RELATED WORK</h2>
<p>Foundation Language Models. Since the appearance of ChatGPT [35], there has been a large number of large language models for use as a foundation model to solve real-life problems. Since the models that provide only online demos and APIs, like ChatGPT, GPT-4[36] and Yiyan (https://yiyan.baidu.com/) are not suitable and convenient for further pre-training and developing. The opensource models like CodeGen [34], LLaMA[47], GLM [55], becomes the foundation models for many other instruction-tuned LLMs like Alpaca [44], Baize [52], Vicuna [4], Koala [15], and Dolly [7]. Meanwhile, The tool learning has been brought to the fore by LLM as the rapid rise of the usage of external modules and information to enhance the foundation models.</p>
<p>Domain Language Models. Large language models become the foundation model to address the issues in many other domains. In life science field, Med-PaLm [43], MedGPT [22], BioGPT [28], and Bio-Megatron [42] The large language model is useful and reliable in the biomedicine field [48]. In natural science field, Geographic-BERT [25], MGeo [11], PK-Chat [9], ERNIE-GeoL [20] and GeoBERT [10] are typical cases in geography and geology [32], while MatSciBERT [17] is the one in material science. In academic scenario, SciBERT [2] and Galactica [45] are two examples.
Parameter-Efficient Tuning on LLMs. Conventional fine-tuning needs to update all the parameters in LLMs, leading to inefficient and leaving a large carbon footprint as the models grow along with the scaling law [21]. Soft Prompt tuning [23] frozen language models to perform specific downstream tasks. Prefix-tuning [24] draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were "virtual tokens". In addition, Adapter [18] make the parameters of the original network remain fixed, yielding a high degree of parameter sharing, and LoRA [19] views the update of the weights as the result of two tunable low-rank matrices multiplication.</p>
<h2>3 DATA COLLECTION AND CURATION</h2>
<p>To train K2, we collect geoscience text corpus and geoscienceoriented data from various resources. Then, we re-structure the data into signals and build up the instruction tuning dataset GeoSignal. This valuable information can serve for learning knowledge for geoscience tasks and instruct models for aligning with humans and experts. Moreover, we develop GeoBench to compare language models focusing on geoscience.</p>
<h3>3.1 Pre-training Data</h3>
<p>In this work, our text corpus for further pre-training on LLaMA-7B consists of 5.5 billion tokens, including geoscience-related Wikipedia pages, geoscience paper's abstracts, and open-access geoscience papers published in selected high-quality journals in earth science and mainly collected by GAKG [8].</p>
<h3>3.1.1 Geoscience Text Corpus Collection.</h3>
<p>Geoscience Open Access Literatures. With the support from DDE (Deep-Time Digital Earth Big Science Program), we can have</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<table>
<thead>
<tr>
<th>Simulation is an established practice in the imaging radar</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>community that serves many purposes, including inSAR aco-</td>
<td></td>
<td></td>
</tr>
<tr>
<td>recy assessment [1], [2], analysis of new algorithms [3], [4]</td>
<td></td>
<td></td>
</tr>
<tr>
<td>and new sensor designs [5]-[7], and improvement of image</td>
<td></td>
<td></td>
</tr>
<tr>
<td>interpretation algorithms [8]-[10].</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Figure 2: Tokenization processed text. A. shows an example of a figure marker, we only choose to preserve the captions; B. shows an example of a table marker, we transfer the tables into the form of Markdown; C. shows the tokenization of the citations, we replace the reference numbers into reference papers' title to preserve the readability of the text corpus; D. shows an example of the special tokens for formulas.</p>
<table>
<thead>
<tr>
<th>Data source</th>
<th>Document</th>
<th>Tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>Geoscience papers</td>
<td>$1,122,094$</td>
<td>3.9 B</td>
</tr>
<tr>
<td>Geoscience papers Metadata</td>
<td>$4,274,716$</td>
<td>0.1 B</td>
</tr>
<tr>
<td>Wikipedia pages</td>
<td>767,341</td>
<td>1.5 B</td>
</tr>
<tr>
<td>Total</td>
<td>$\mathbf{6 , 1 6 4 , 1 5 1}$</td>
<td>$\mathbf{5 . 5 B}$</td>
</tr>
</tbody>
</table>
<p>Table 1: The details of the text corpus used to train K2. the resources and chances to access materials and data strongly related to geoscience, including 531 journals and 4,274,716 papers' metadata. We use 1,122,094 open-access papers' PDFs organized by GAKG $^{2}$ to build the text corpus.</p>
<p>Wikipedia pages about Earth science. Wikipedia is an import resource we take into account for text corpus collection, and the root node of the Wikipedia category of geoscience we take into consideration is "https://en.wikipedia.org/wiki/Earth_science". We mine all the child and related topics connected to it and finally gain 767,341 Wikipedia pages.</p>
<p>In brief, the statistics of the collection of geoscience text corpus are shown in Table 1.</p>
<h3>3.1.2 Text Corpus Preprocessing.</h3>
<p>PDF Parsing. We build an automatic PDF parsing toolkit based on the GROBID library [1]. We use Markdown as the format for all papers in the corpus to preserve readability and consistency. Finally, we use regular expressions and rule-based scripts to clean the data, removing the text obstructing reading, garbled, and impurity data. The script will be released after the final draft, and currently, it is in use by DeepShovel [56].</p>
<p>Tokenization. Tokenization is an essential part of text corpus design. To make the language model understand the academic papers, we utilize specialized tokens for different modalities as follows, and the examples are shown in Figure 2.</p>
<ul>
<li>Illustrations: we use special tokens [START_FIGURE] and [END_FIGURE] to annotate the captions of the illustrations in the papers.</li>
</ul>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>|  SFT Data | Prompts | Data Type  |
| --- | --- | --- |
|  Alpaca-GPT4 | 52,002 | Self-instruct  |
|  Dolly-15K | 15,011 | Task-specific  |
|  Natural Instruction | 2,446 | Task-specific  |
|  AI2 Reasoning Challenge | 7,787 | Task-specific  |
|  GeoTool | 10,645 | Tools  |
|  GeoSignal | 39,749 | Knowledge Intensive  |</p>
<p>Table 2: Datasets used to train K2 during the instruction tuning process.</p>
<ul>
<li>Tables: Two special tokens [START_TABLE] and [END_TABLE] are used to locate the position of the table in the passage. In this process, we transform the tables in the PDFs into the format of Markdown.</li>
<li>Citations: We use special tokens [START_REF] and [END_REF] to annotate the citations.</li>
<li>Formulas: For mathematical content or formulas, we filter and clean the irregular formulas parsed from PDFs through regular expressions and rule-based methods. Further we use special tokens [START_FORMULA] and [END_FORMULA] to capture them.</li>
</ul>
<h3>3.2 Instruction Tuning Data: GeoSignal</h3>
<p>Next, we curate the instruction tuning data that will be used to align the pre-trained model with user intentions. Specifically, we first collect well-organized, general instruction tuning data, such as natural instruction [33], AI2 Reasoning Challenge [6], stanfordalpaca [44], and Dolly-15k [7]. Then, we utilize a semi-manual pipeline to build up a geoscience expert-alignment dataset called GeoSignal. Moreover, we create a tool training dataset based on ToolBench [38] to enable K2 to use tools. These instruction tuning data statistics are shown in Table 2. We detail the data curation process next. 3.2.1 Align-to-Human. In this part, we collect and gather several well-construct supervised datasets, including self-instruct, humanannotated, and tools-related data.</p>
<ul>
<li>Alpaca-GPT4: Alpaca-GPT4 ${ }^{3}$ is an instruction-following dataset generated by the techniques named Self-Instruct [50], and all the</li>
</ul>
<p>samples are in the form of <instruction, input, output>, which we choose to follow.</p>
<ul>
<li>Dolly-15k: databricks-dolly-15k [7] is an open-source dataset of instruction-following records generated by thousands of Databricks employees, including brainstorming, classification, closed QA, generation, information extraction, open QA, and summarization. We organize them all into <instruction, input, output> format.</li>
<li>Natural Instruction: Natural Instruction [33] maintains many tasks and their natural language definitions/instructions. Its v1.x dataset consists of 61 tasks. The v2.x dataset contains over 1.5 k tasks. We select objective tasks elaborately from the v2.x dataset and organize them into <instruction, input, output> format.</li>
<li>AI2 Reasoning Challenge: AI2 Reasoning Challenge (ARC) [6] is a dataset of 7,787 genuine grade-school level, multiple-choice science questions. As it is well-formed, we sample randomly and organize it into <instruction, input, output> format.</li>
<li>Tool Instruction Data: Refer to ToolBench [38], We manually curated a collection of 2 k instruction data for training K 2 to learn to use geoscience academic search engine (we name this tool as GeoSearch). Then, we combine 8k tool instruction data (arxiv, bing search, database, weather, and wolfram-alpha) from ToolBench to prepare for the tool learning. We call this collection of tool instruction data GeoTool, and it has 10k samples in total.
3.2.2 Align-to-Expert. An expert is a human who specializes in a given domain, and more than learning to follow human instruction is needed for the specialized domain, we set up to train the model with knowledge-intensive data. Referring to re-structured pre-training [5, 54], signals are the data we can use to train models and usually exist in databases and websites. Many data sources and materials have different types of geoscience signals in geoscience, as illustrated in Figure 3.</li>
</ul>
<p>These signals could be re-structured into <input, output> pairs as instruction tuning samples. For example, with a paper's abstract and title information, we can re-structure such signals into a title generation task given the abstract. In addition, and most importantly, with the support of several applications and products of DDE, we collect a large quantity of geoscience expertise data and re-structure it with prompts into a unified sequence-to-sequence format, namely GeoSignal. The databases and websites we use are as follows:</p>
<ul>
<li>GAKG: GAKG [8] is a multimodal Geoscience Academic Knowledge Graph organizing geoscience papers' illustrations, text, and bibliometric data. Each paper has several geor:mention_knowledge axioms to connect different knowledge points.</li>
<li>DDE Scholar: DDE Scholar ( https://ddescholar.acemap.info/), a geoscience academic literature search engine, contains more than 3 million papers and 4 million scholars' information in the field of earth sciences.</li>
<li>DataExpo: DataExpo [27] is a one-stop dataset service and has indexed over 960,000 datasets from more than 27,000 repositories in the context of Deep-time Digital Earth Program.</li>
<li>GSO: GSO (https://gso.acemap.info/) is a large-scale ontology of research areas that was automatically generated using the hierarchical topic modeling, which consists of more than 120 thousand research interests in the field of geoscience.</li>
<li>Geoscience QA: We crawler 4 question and answer platform, and 6 geoscience-related databases, using OpenAI [35] for template generation and with the help of the human expert, we finally have a clean and correct geoscience Q\&amp;A dataset. The distribution of each part is shown in Table 3.</li>
<li>Self-instruct: Refer to Alpaca-GPT4, we use GPT4 to generate 18,000 questions and their answers from 18 subfields of geoscience based on domain materials and verify most of the Q\&amp;A pairs by geoscientists. ${ }^{4}$
For a better understanding of geoscience signals, we list the main signals we consider in bellowing and illustrate the detail of re-structure.</li>
<li>G1: Paper content: The title, abstract, full-text of geoscience literature. This signal naturally exists on DDE Scholar, GAKG, and DataExpo and can be used in summarization tasks.</li>
<li>G2: Category: The category of a geoscience paper or term. This signal typically exists on DDE Scholar, GAKG, and Wikipedia. It can be used for the text classification task.</li>
<li>G3: Reference Paper: This signal exists in the reference lists and introduction of papers and is useful for text comprehension and summarization.</li>
<li>G4: The captions of paper table and illustration: Tables and figures in geoscience papers provide captions and content mentioned in the passage, which can be used for question-answering tasks.</li>
<li>G5: Entity mentions: The entities within a given text. This signal can be found in GAKG and Wikipedia and can be useful for named entity recognition tasks.</li>
<li>G6: Relations: The relationships between different geoscience entities. This information exists in human-annotated datasets such as GAKG and GSO. This signal is useful for finding synonyms and hyponymy terms in geoscience.</li>
<li>G7: Word description: The definition of a word. Various geoscience resources contain this signal, such as Geoscience Dictionary, WordNet, Wikipedia, and GSO. This signal is useful for the task of explanation.</li>
<li>G8: Synonyms \&amp; Taxonomy: The Synonyms and hyponymy relation between terms in geoscience. Geoscience Dictionary and GSO contain this signal, useful for finding synonyms and hyponymy terms in geoscience.</li>
<li>G9: Text Comprehension: This signal typically exists in geoscience academic platforms and other text material containing question and answer pairs and is useful for question answering.</li>
<li>G10: Factual knowledge: Geoscience facts, e.g., Dolomite is a carbonate rock. This signal typically exists in some geosciencerelated QA platforms and is useful for question-answering and fact verification.
Aiming to make good use of these signals, we re-structure the data into <input, output> pairs for tuning on tasks of Explanation, Named Entity Recognition, Reasoning, Fact Verification, Summarization, Text Classification, Word Semantics, and Question Answering. For better understanding, all the scripts will be open-sourced after the final draft, and the details are illustrated as follows:</li>
</ul>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The components of GeoSignal.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Resource</th>
<th style="text-align: center;">Count</th>
<th style="text-align: left;">Link</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">NGDB</td>
<td style="text-align: center;">148,212</td>
<td style="text-align: left;">https://mrdata.usgs.gov/</td>
</tr>
<tr>
<td style="text-align: left;">RRUFF</td>
<td style="text-align: center;">32,778</td>
<td style="text-align: left;">https://rruff.info/</td>
</tr>
<tr>
<td style="text-align: left;">Fossil</td>
<td style="text-align: center;">4,959</td>
<td style="text-align: left;">http://fossil-ontology.com/</td>
</tr>
<tr>
<td style="text-align: left;">MinDat</td>
<td style="text-align: center;">51,291</td>
<td style="text-align: left;">https://zh.mindat.org/</td>
</tr>
<tr>
<td style="text-align: left;">Dinosaur</td>
<td style="text-align: center;">11,348</td>
<td style="text-align: left;">https://dinoanimals.com/dinosaurdatabase/</td>
</tr>
<tr>
<td style="text-align: left;">Earthquake</td>
<td style="text-align: center;">37,284</td>
<td style="text-align: left;">https://public.opendatasoft.com/</td>
</tr>
<tr>
<td style="text-align: left;">SaveMyExam</td>
<td style="text-align: center;">1,107</td>
<td style="text-align: left;">https://www.savemyexams.com/</td>
</tr>
<tr>
<td style="text-align: left;">ResearchGate</td>
<td style="text-align: center;">3,680</td>
<td style="text-align: left;">https://www.researchgate.net/</td>
</tr>
<tr>
<td style="text-align: left;">Quizlet</td>
<td style="text-align: center;">301</td>
<td style="text-align: left;">https://quizlet.com/</td>
</tr>
<tr>
<td style="text-align: left;">Study</td>
<td style="text-align: center;">1,294</td>
<td style="text-align: left;">https://study.com/</td>
</tr>
<tr>
<td style="text-align: left;">Total</td>
<td style="text-align: center;">$\mathbf{2 9 2 , 2 5 4}$</td>
<td style="text-align: left;">(We clean out and sample 8,000 of them)</td>
</tr>
</tbody>
</table>
<p>Table 3: The statistics of the Geoscience QA data collection.</p>
<p>Explanation. To construct the data used for training the skills of word explanation, we digitize the geoscience dictionaries, taking all the words and their explanations inside the dictionaries (Signal G7). Moreover, we also include the related entries of geography in Wikipedia to construct the dataset for the explanation tasks.</p>
<p>Named Entity Recognition. Refer to Signal G5, GAKG preserves a connection between papers and knowledge entities. These entities are extracted from abstracts. Meanwhile, Wikipedia utilizes hyperlinks between Wikipedia pages. Consequently, we use the paragraphs (abstracts of papers and pages in Wikipedia) as inputs and the mentions (key entities in papers and mentions in Wikipedia pages) as outputs and re-structure the data to input-output pairs.</p>
<p>Reasoning. According to Signal G6, GSO contains many relations between geoscience knowledge entities (or called concepts), one of which is the co-occurrence relation, i.e., two geoscience knowledge entities co-occur in the same paragraph according to [53]. We take such co-occurrence entities along with their corresponding paragraphs as input and the relation-existence as the output to model how an idea comes up. Specifically, we curate samples of co-occurrence concepts in geoscience themes, which originate from highly-cited geoscience papers using Signal G2, G3, and G7. We invite geoscientists to verify the relation-existence for each pair of co-occurrence concepts. In this way, we can endow the model with the ability to reason for ideas generation.</p>
<p>Fact Verification. As for Signal G8, we collect the data from Wikipedia and papers in geoscience, and we re-structure the explicit declarative sentences. If we take a sentence with an opposite meaning as input, the output would be "False", while we handle the original sentence as input, the output would be "True".</p>
<p>Summarization. We re-structure the Signal G1 and G9 for summarization task. In academic papers, a paper's abstract is the summary of the full text of the article, and the title is a further summary of the abstract. Meanwhile, the one-sentence summary of papers often exists in the related works in papers. These pieces of information provide us with supervised pairs. Moreover, Signal G3 provides us with illustrative references for papers, a kind of supervised data for summary. In this way, we can get the text summary of the training data.</p>
<p>Text Classification. Based on Signal G2, we have 18 disciplines for each knowledge point in the DDE platform, and geoscience dictionaries have eight fields to classify each term. Consequently, we can construct the dataset for text classification.</p>
<p>Word Semantics. As mentioned in Signal G8, GSO monitor the semantic relationship between geoscience-related knowledge points. In this task, we will train the model by asking about geoscience-related entities' synonyms and hyponymy relations and answering with related entities.</p>
<p>Question Answering. As mentioned in G10, factual knowledge is the most essential part of model training. We re-structure four kinds of supervised data as follows:</p>
<ul>
<li>We clean the question-answer pairs inside the Geoscience QA ensemble.</li>
<li>We combine key-value pairs on the structured geoscience domain websites (the first six records in Table 3). Take the page of R070007 ${ }^{5}$ in RRUFF as an example, which is about the information of a sample of Abelsonite. We use the key of this table (shown in Figure 4) as the question and the value as the answer to re-structure QA question-answer pairs.</li>
<li>We make ChatGPT ask and answer questions to itself through Self-instruct methods and further invite experts in geoscience to validate them, ensuring the correctness of the data.</li>
<li>Signal G4 provides us with illustrative references towards illustration and table in paper, and we combine the special token to form a simple question dataset to train the model learning and further explain the academic content.</li>
</ul>
<p>It is worth mentioning that when we re-structure the GeoSignal data involving citations, graphs, and tables, we add the special token used in the pre-training to distinguish the unique data distribution. So we can maintain a strict format specification and consistency with further pre-trained data.</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: An example for GeoSignal re-structure when processing the geoscience website RRUFF.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Tasks</th>
<th style="text-align: center;">Records</th>
<th style="text-align: right;">Total (Cleaned)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Named Entity Recognition</td>
<td style="text-align: center;">$6,252,268$</td>
<td style="text-align: right;">2,400</td>
</tr>
<tr>
<td style="text-align: left;">Reasoning</td>
<td style="text-align: center;">1,200</td>
<td style="text-align: right;">600</td>
</tr>
<tr>
<td style="text-align: left;">Fact Verification</td>
<td style="text-align: center;">168,424</td>
<td style="text-align: right;">8,000</td>
</tr>
<tr>
<td style="text-align: left;">Summarization</td>
<td style="text-align: center;">$3,279,336$</td>
<td style="text-align: right;">800</td>
</tr>
<tr>
<td style="text-align: left;">Text Classification</td>
<td style="text-align: center;">8,313</td>
<td style="text-align: right;">2,000</td>
</tr>
<tr>
<td style="text-align: left;">Word Semantics</td>
<td style="text-align: center;">826,194</td>
<td style="text-align: right;">6,400</td>
</tr>
<tr>
<td style="text-align: left;">Explanation</td>
<td style="text-align: center;">731,374</td>
<td style="text-align: right;">4,200</td>
</tr>
<tr>
<td style="text-align: left;">Question Answering</td>
<td style="text-align: center;">$11,360,163$</td>
<td style="text-align: right;">15,349</td>
</tr>
<tr>
<td style="text-align: left;">Entire GeoSignal</td>
<td style="text-align: center;">$\mathbf{2 2 , 6 2 7 , 2 7 2}$</td>
<td style="text-align: right;">$\mathbf{3 9 , 7 4 9}$</td>
</tr>
</tbody>
</table>
<p>Table 4: The statistics of GeoSignal are categorized by tasks.</p>
<p>After the procedure above, we obtain a large number of supervised data, and we sample and clean the data to build the instruction tuning data GeoSignal since the data quality is much more important than quantity. We will open-source the pre-processing scripts. The statistics of GeoSignal are listed as Table 4.</p>
<h3>3.3 Evaluation on Expertise in Geoscience: GeoBench</h3>
<p>Lastly, in order to evaluate the language models for solving geoscience questions and the capacity to understand and utilize the geoscience knowledge, we extract the data from various Questionanswer websites, crawl several open-source test websites, and finally construct a benchmark, named GeoBench.</p>
<p>NPEE. First, we collected National Postgraduate Entrance Exam questions on geology and geography in the past five years. We chose the text-only questions and translated them into English since the base model is LLaMA. We invited a professional translator who specialized in geoscience-related works to translate the questions and corresponding answers and finally obtain 182 multiple-choice questions, 150 fill-in-the-blank questions, 454 word-explanation tasks, and 335 essay questions. Since the fill-in-the-blank questions, word-explanation tasks, and essay questions are hard to evaluate, we make them subjective tasks, while the multiple-choice questions are objective.</p>
<p>APTest. we also collect AP (Advanced Placement) examinations, which are exams offered in the US by the College Board and are taken each May by students. We collect and clean 1,395 multiplechoice questions about geology, geography, and environmental science.</p>
<p>To sum up, There are 183 multiple-choice questions in NPEE and 1,395 in total in the AP Test, constituting the objective task set. Meanwhile, we gather all 939 subjective questions in NPEE</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Tasks</th>
<th style="text-align: center;">Question Sample (with prompt)</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Objective Question</td>
<td style="text-align: center;">Question <br> Answer</td>
<td style="text-align: center;">The interface between crust and mantle is called: <br> Choose from: <br> A. Gutenberg <br> B. Conrad <br> C. Moho <br> The answer is: <br> C. Moho</td>
</tr>
<tr>
<td style="text-align: center;">Subjective Question</td>
<td style="text-align: center;">Question <br> Answer</td>
<td style="text-align: center;">What is Translational fault in geoscience? <br> The two walls are relatively staggered along the strike direction of the fault plane.</td>
</tr>
</tbody>
</table>
<p>Table 5: Ground truth samples in GeoBench.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Training recipe for domain large language models.
to be the subjective tasks set and use 50 to measure the baselines with human evaluation. In the experiment sessions, we further discuss the evaluation metrics on these tasks and give the example of GeoBench in Table 5.</p>
<h2>4 TRAINING THE K2</h2>
<p>In this section, we establish a recipe for tuning a large language model on a specific domain and share the settings we adopt to train the K2.</p>
<h3>4.1 Geoscience Domain Adaptation Recipe</h3>
<p>Since geoscience is a relatively secondary or arcane field of study, there are few language models for such scenarios. However, advanced natural language models and tools can help geoscientists with data mining and knowledge discovery in their research fields. Therefore, learning a language model for knowledge understanding, summary, and QA is necessary. Meanwhile, geoscience has a rich knowledge accumulation, such as academic papers and scientific reports, which has established a data foundation for training large-scale language models. Consequently, Based on the data in the field of geosciences, we explored a recipe for scientific domain adaptation and finally obtained K2.</p>
<p>As shown in Figure 5, scientific domain adaptation has three main steps. First, we use domain-specific text corpus to further pre-train the base model. In this paper, we use LLaMA as the base model. Second, since instruction tuning can make the language models generate content following human instructions, we can first do instruction tuning with general instruction-tuning data, such as Alpaca, and natural instruction. Lastly, after learning the paradigm to follow the instructions, the model can learn more information from the restructured domain knowledge, which we call expertiseinstruction tuning. In the ablation experiments, we will further verify the correctness of this recipe.</p>
<h3>4.2 Further Pre-training</h3>
<p>During the stage of further pre-training on geoscience text corpus, We initialize the LLaMA-7B [47] checkpoints and train it on 5.5B tokens geoscience corpus.</p>
<p>The entire parameters of LLaMA-7B ( 6.7 B trainable parameters) are further pre-trained for one epoch on 4 NVIDIA A100-SXM40GB GPUs, and the training takes 214 hours. In this stage, we set a learning rate of $1 \mathrm{e}-5$, with a global batch size of 128 and a microbatch size of 2 . The incremental steps of the train are 30,140 steps ( 1,000 for warm-up). Finally, we call the model obtained after the further pre-train GeoLLaMA for better distinction.</p>
<h3>4.3. Instruction tuning</h3>
<p>After further pre-training on geoscientific text data, we obtained a model that experienced domain shift. However, at this stage, the model could only accomplish the next token generation task that adhered to geoscientific knowledge distribution. In order to make the model compliant with human instructions, we employed multitask training. Human instructions took various forms during this process, as our recipe describes, including general task instructions like the Alpaca and knowledge-intensive instructions like GeoSignal. Through experimentation, we discovered that first conducting general instruction learning followed by knowledge-intensive instruction learning helped enhance the performance of our model, far surpassing the results obtained from mixed training.</p>
<p>During the instruction learning phase, we introduced parameterefficient fine-tuning (PEFT) to help us achieve the mission of training in a low-resource setting. As mentioned in [19], the weight updates during the fine-tuning process also have a low "intrinsic rank" during adaptation. Therefore, according to LoRA, a hidden layer $h=W_{0} x, W_{0} \in R^{d * k}$, the modified forward pass yields:</p>
<p>$$
h=W_{0} x+\Delta W x=W_{0} x+B A x
$$</p>
<p>where $B \in R^{d * r}, A \in R^{r * k}$ and $r&lt;&lt;\min (d, k)$ are two low "intrinsic rank" matrix containing trainable parameters. Moreover, after further pre-training the LLaMA, the adaptation to the field of geoscience is more comprehensive. During the instruction tuning stage, the target is to train the model to align with humans and experts. We use Low-Rank Adaption to tune the model.</p>
<p>In instruction tuning, we set a learning rate $1 \mathrm{e}-4$ with a global batch size of 128. As for the LoRA setup, we set lora_r as eight while lora_alpha as 16 . We set the lora_target_modules as k_proj, q_proj, and v_proj, based on our experimental observation. The instruction tuning via LoRA only trains 6 M parameters on one single NVIDIA GeForce RTX 3090 for 23 hours. In order to make the model perform better and inject part of the geoscience knowledge in the SFT stage, we first use alpaca instruction tuning data to train GeoLLaMA, which we recognize as Human-alignment. Then, we resume from the checkpoint obtained and continue fine-tuning the model using GeoSignal for further training. Our experimental observation shows that the performance does not improve if we mix these training data.</p>
<p>In addition, to fully exploit the capabilities of K2, we adapt tool learning for scientific utilization in geoscience. Just as we choose LoRA when using GeoSignal to fine-tune GeoLLaMA to align with humans and experts, we also use it when training K2 to use tools. In this process, we refer to the [38] and use our tool dataset learning to search geoscience-related knowledge and literature. This makes K2 an applicable foundation model that can call external API autonomously. The tool learning via LoRA only trains 4 M parameters
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Accuracy scores at selected training steps of K2 on the Objective tasks in GeoBench.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Baselines</th>
<th style="text-align: center;">NPEE</th>
<th style="text-align: center;">APTest</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Gal-6.7B</td>
<td style="text-align: center;">25.7</td>
<td style="text-align: center;">$\mathbf{2 9 . 9}$</td>
</tr>
<tr>
<td style="text-align: left;">LLaMA-7B</td>
<td style="text-align: center;">21.6</td>
<td style="text-align: center;">27.6</td>
</tr>
<tr>
<td style="text-align: left;">MPT-7B</td>
<td style="text-align: center;">28.4</td>
<td style="text-align: center;">26.0</td>
</tr>
<tr>
<td style="text-align: left;">Vicuna-7B</td>
<td style="text-align: center;">26.4</td>
<td style="text-align: center;">16.8</td>
</tr>
<tr>
<td style="text-align: left;">Alpaca-7B</td>
<td style="text-align: center;">$\underline{31.1}$</td>
<td style="text-align: center;">29.1</td>
</tr>
<tr>
<td style="text-align: left;">K2-7B (Ours)</td>
<td style="text-align: center;">$\mathbf{3 9 . 9}$</td>
<td style="text-align: center;">$\underline{29.3}$</td>
</tr>
</tbody>
</table>
<p>Table 6: comparison among baselines on Objective tasks in GeoBench. The best number is bolded, while the second best is underlined.
on eight NVIDIA GeForce RTX 3090 for 21 hours, with a learning rate of $1 \mathrm{e}-5$ and a max content length of 2,048 .</p>
<h2>5. EVALUATION AND RESULTS</h2>
<p>This section illustrates the evaluation methods and results of K2 and related baselines. GeoBench consists of two kinds of tasks: one is subjective, and one is objective. In this part, we choose four baseline models: Galactica-6.7b [45], MPT-7B [46], Vicuna-7B [4], LLaMA-7B [47] and Alpaca-7b [44].</p>
<h3>5.1. Objective tasks in GeoBench</h3>
<p>For objective tasks like multiple-choice tasks (GeoBench-AP and multiple-choice in GeoBench-NPEE), we prompt appropriately, ending with the phrase "The answer is", calculate the Softmax of the probability of next token among the choice label (e.g., $A, B, C, D$, sometimes $E$ ), and finally gain the score of the Accuracy based on these test ground truth.</p>
<p>First, we evaluate all the saved checkpoints, as shown in Figure 6. We can find that as the tokens seen by the model gradually scale up, the model's performance on our benchmark is improving. This result indicates that the model learns geoscience knowledge in further pre-train.</p>
<p>Moreover, compared with the baselines, shown in Table 6, we can see that K2 outperforms the model with a similar size over the NPEE dataset. However, in the AP Test, K2 is similar to the Galactica model since geoscience learned in high school is human geography and environmental science, including in the training corpus of Galactica.</p>
<h3>5.2. Subjective tasks in GeoBench</h3>
<p>For subjective tasks (mainly in GeoBench-NPEE), we use automatic methods, GPTScore [13] and perplexity to evaluate the quality of the output.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Baselines</th>
<th style="text-align: center;">Automatic Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Human Evaluation</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Perplexity $\downarrow$</td>
<td style="text-align: center;">GPTScore $\uparrow$</td>
<td style="text-align: center;">rationality</td>
<td style="text-align: center;">correctness</td>
<td style="text-align: center;">consistency</td>
</tr>
<tr>
<td style="text-align: center;">Gal-6.7B</td>
<td style="text-align: center;">34.57</td>
<td style="text-align: center;">$-2.3598$</td>
<td style="text-align: center;">1.96</td>
<td style="text-align: center;">1.74</td>
<td style="text-align: center;">1.79</td>
</tr>
<tr>
<td style="text-align: center;">LLaMA-7B</td>
<td style="text-align: center;">40.07</td>
<td style="text-align: center;">$-1.9531$</td>
<td style="text-align: center;">2.24</td>
<td style="text-align: center;">2.04</td>
<td style="text-align: center;">2.01</td>
</tr>
<tr>
<td style="text-align: center;">GeoLLaMA-7B</td>
<td style="text-align: center;">32.32</td>
<td style="text-align: center;">$-1.9457$</td>
<td style="text-align: center;">2.15</td>
<td style="text-align: center;">1.89</td>
<td style="text-align: center;">2.03</td>
</tr>
<tr>
<td style="text-align: center;">Alpaca-7B</td>
<td style="text-align: center;">40.07</td>
<td style="text-align: center;">$-1.9536$</td>
<td style="text-align: center;">2.09</td>
<td style="text-align: center;">1.93</td>
<td style="text-align: center;">2.34</td>
</tr>
<tr>
<td style="text-align: center;">K2-7B (Ours)</td>
<td style="text-align: center;">32.32</td>
<td style="text-align: center;">1.9487</td>
<td style="text-align: center;">2.38</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">2.14</td>
</tr>
</tbody>
</table>
<p>Table 7: Comparsion on subjective tasks in GeoBench. The best number is bolded, while the second best is underlined.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">NPEE</th>
<th style="text-align: center;">APTest</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">GeoLLaMA $\rightarrow$ Dolly</td>
<td style="text-align: center;">27.0</td>
<td style="text-align: center;">26.3</td>
</tr>
<tr>
<td style="text-align: left;">GeoLLaMA $\rightarrow$ Alpaca-GPT4</td>
<td style="text-align: center;">34.4</td>
<td style="text-align: center;">26.5</td>
</tr>
<tr>
<td style="text-align: left;">GeoLLaMA $\rightarrow$ GeoSignal</td>
<td style="text-align: center;">$\underline{37.2}$</td>
<td style="text-align: center;">$\underline{27.4}$</td>
</tr>
<tr>
<td style="text-align: left;">GeoLLaMA $\rightarrow$ GeoSignal mix Alpaca-GPT4</td>
<td style="text-align: center;">33.8</td>
<td style="text-align: center;">23.4</td>
</tr>
<tr>
<td style="text-align: left;">GeoLLaMA $\rightarrow$ Alpaca-GPT4 $\rightarrow$ GeoSignal (K2)</td>
<td style="text-align: center;">$\mathbf{3 9 . 9}$</td>
<td style="text-align: center;">$\mathbf{2 9 . 8}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Results when using different SFT data. The best number is bolded, while the second best is underlined.</p>
<p>GPTScore utilizes generative pre-trained models' emergent abilities (e.g., zero-shot instruction) to score generated texts. According to [13], we calculate the vanilla score, which is a negative loss, with an evaluator of GPT-2 [39]. In addition, perplexity is computed with GPT-2 on the generated text and measures the fluency of the generations.</p>
<p>Furthermore, referring to geoscientists, we collect 50 open geoscience questions and gather ten geoscience research practitioners to evaluate the output of baseline models. We evaluate the models on three metrics: 1) rationality, whether the generated content of the model is technical rationality or not; 2) correctness, whether the content generated by the model is reliable or not; 3) consistency, whether the generated content always stays in the topic. All the scores scale from 1 (poor) to 3 (good), with 2 indicating acceptable content. The complete results of the subjective tasks are in Table 7.</p>
<p>As we can see, K2 performs better on rationality and correctness. At the same time, consistency stays competitive. The results indicate that our model better understands geoscience and can utilize scientific knowledge.</p>
<h3>5.3 Ablation on Expert-Alignment</h3>
<p>To better understand the recipe for aligning the model with humans and experts, we deploy the ablation experiments to explore the detail. We treat the data constructed by self-instruct or humanannotated in the general domain or for dialogue generation as human-alignment data. At the same time, view the data annotated by experts in specific domains as expert-alignment data. As shown in Table 8, using task-special data, such as dolly-15k, fails to achieve a good performance, while using self-instruct data, such as AlpacaGPT4, is still not as effective as using knowledge-intensive data. Surprisingly, we have discovered that the results are unsatisfactory if we mix the knowledge-intensive data GeoSignal with humanalignment data Alpaca. It is better to use Alpaca to align the model to follow human instruction and then use the GeoSignal to align with the experts. Moreover, LoRA is deployed only on attention layers.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">rationality</th>
<th style="text-align: center;">correctness</th>
<th style="text-align: center;">consistency</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Alpaca-7B $\rightarrow$ GeoTool</td>
<td style="text-align: center;">1.23</td>
<td style="text-align: center;">2.45</td>
<td style="text-align: center;">$\mathbf{2 . 4 1}$</td>
</tr>
<tr>
<td style="text-align: left;">K2-7B $\rightarrow$ GeoTool</td>
<td style="text-align: center;">$\mathbf{2 . 6 7}$</td>
<td style="text-align: center;">$\mathbf{2 . 5 6}$</td>
<td style="text-align: center;">2.24</td>
</tr>
</tbody>
</table>
<p>Table 9: Results of evaluating the generated thought quality from foundation models while doing tool learning (scores are rated by prompting ChatGPT).
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: An example of tool augmented K2 for geoscience Q\&amp;A.</p>
<h3>5.4 Exploration on Tool Learning</h3>
<p>According to the ablation experiment of Expert-Alignment, we found that the knowledge injection of large language models mainly occurs in the further pre-train stage to a certain extent, while learning to express the knowledge and generating patterns occur in the SFT stage. For different types of SFT data, the model has a learning recipe. Similar to human beings, we learn to understand words before we can read and learn to communicate before we can discuss a specific field. So, we hypothesize that there is a similar phenomenon in learning to use tools. Thus, we conducted ablation experiments to evaluate different foundation models trained with the same tools.</p>
<p>In this experiment, we perform tool instruction tuning with the GeoTool dataset on both Alpaca-7B (i.e., LLaMA model tuned on the Alpaca-GPT4 dataset) and K2-7B, which undergoes both further pre-training and instruction tuning. This comparison is similar to a student educated in a general subject and a student specializing in geosciences using a geoscience literature search tool for knowledge queries and question answering. In this work, we need to compare the generative thoughts (illustrated in Figure 7). We take ChatGPT as a referee to decide which model generates better thoughts and ask ChatGPT to give a score based on the meaning of these three factors via simple prompts, following previous studies [16]. During this process, we use the 50 open geoscience questions mentioned in the subjective tasks above.</p>
<p>Finally, according to the results shown in Table 9, K2 may have better thoughts on how to write search queries since K2 knows more geoscience knowledge than Alpaca-GPT4 leading to a better understanding towards geoscience questions.</p>
<h2>6 APPLICATION</h2>
<p>In this section, will discuss the potential application of K2 in brief. We will show the use cases of K2 for research assistance and knowledge reasoning.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Concepts</th>
<th style="text-align: center;">yingcheng formation, kamchatka</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Prompt</td>
<td style="text-align: center;">Green two academic concepts: <br> Concept 1: yingcheng formation\nConcept 2: kamchatka Assuming you are a researcher: <br> What is the meaning of these two concepts? <br> What new knowledge can they merge to generate? <br> Please write a paper abstract to combine (fuse) these two concepts:</td>
</tr>
<tr>
<td style="text-align: center;">Output</td>
<td style="text-align: center;">Abstract: The Kamchatka Peninsula is located in the northwestern Pacific Ocean, between the Bering Sea and the Sea of Okhotsk. The region is characterized by a complex tectonic setting, with the Pacific Plate sub -ducting beneath the North American Plates.\nYingcheng formation, which is a Late Cretaceous to Early Paleogene sedimentary sequence. It is compos -ed of sandstone, siltstone and mudstone. These sediments were deposited in a shallow marine environment.\nThe Yincheng sedimeants were formed in an outer shelf environment, where the water depth was less than 500 meters. During the Late Paleocene, a large-scale volcanic eruption occurred in this region, resulting in volcano-sedimentation. A large amount of volcaniclastic sediments, such as tuff, tuftle and volcanic breccia, were interbedded with sedimentary rocks.The volcaniclatics were derived from the volcanic eruptive centres in eastern Kamchatsky peninsula:</td>
</tr>
</tbody>
</table>
<p>Table 10: An example for K2 used for knowledge reasoning and idea generation.</p>
<h3>6.1 Case Study 1: K2 for research assistance</h3>
<p>As a language model, K2 can understand geoscience materials and modify the statement about geoscience with suitable prompts. Since K2 is a generative language model, it can generate paragraphs and statements on word description and answer generation based on the given questions. In this way, K2 acts like a knowledge base and provides the geoscientist with a professional assistant.</p>
<p>Moreover, with the tools Augment K2, we can use external information and functions to generate reliable and promising results. Take GeoSearch as an example. After giving a question to K2, K2 will give out a thought to call the API of searching on the GAKG, then update the action and generate a suitable prompt based on observation. Finally, the model can generate a thought of generating appropriate answers. Figure 7 shows an example.</p>
<h3>6.2 Case Study 2: K2 for knowledge reasoning</h3>
<p>K2 shows the potential of adapting language models to a scientific field with domain barriers. Based on the observation of the performance on subjective tasks, in this paper, we adopt K2 to do the task of idea generation and knowledge reasoning in the field of geoscience to find the potential scientific relations, which will be brought to the fore. Consequently, K2 can generate new ideas in the form of abstracts according to the co-occurrence concepts mentioned above. Table 10 shows an example.</p>
<h2>7 CONCLUSION</h2>
<p>In this paper, we introduce K2, the first-ever large language model and foundation model in the geoscience field. K2 can answer geoscience questions and follow geoscientists' instructions with its geoscience professionalism. We construct the first geoscience-supervised instruction data, GeoSignal. Meanwhile, we build GeoBench, the first NLP benchmark in geoscience to evaluate the capability on geoscience knowledge understanding and utilization. On the geoscience benchmarks collected, K2 shows its professionalism and effectiveness compared with other similar-size language models. Moreover, we share the K2 potential applications including knowledge reasoning and research assistant. Finally, we open-source all the code, data and K2 model weights at https://github.com/davendw49/k2.</p>
<h2>REFERENCES</h2>
<p>[1] 2008-2023. GROBID: A machine learning software for extracting information from scholarly documents. https://github.com/kermitf2/geobid
[2] Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A Pretrained Language Model for Scientific Text. In Conference on Empirical Methods in Natural Language Processing.
[3] Marion E. Bickford. 2013. The Impact of the Geological Sciences on Society. Geological Society of America. https://doi.org/10.1130/SPE501
[4] Wei-Lin Chiang, Zhuohan Li, Zi-Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \%$ ChatGPT Quality. https://vicuna.lmeys.org
[5] Hyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Debghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Shie Petrov, Ed Huai bsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Finetuned Language Models. ArXiv abs/2210.11416 (2022).
[6] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. ArXiv abs/1803.05457 (2018).
[7] Databricks. 2023. Hello Dolly: Democratizing the magic of ChatGPT with open models. https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizin g-magic-chatgpt-open-models.html
[8] Cheng Deng, Yuting Jia, Hui Xu, Chong Zhang, Jingyao Tang, Luoyi Fu, Weinan Zhang, Haisong Zhang, Xinbing Wang, and Cheng Zhou. 2021. GAKG: A Multimodal Geoscience Academic Knowledge Graph. Proceedings of the 30th ACM International Conference on Information \&amp; Knowledge Management (2021).
[9] Cheng Deng, Bo Tong, Luoyi Fu, Jiaxin Ding, Dexing Cao, Xinbing Wang, and Chenghu Zhou. 2023. PK-Chat: Pointer Network Guided Knowledge Driven Generative Dialogue Model. arXiv preprint arXiv:2304.00592 (2023).
[10] Huseyin Denli, HassanJaved Chughtai, Brian Hughes, Robert Giatti, and Peng Xu. 2021. Geoscience Language Processing for Exploration. Day 3 Wed, November 17, 2021 (2021).
[11] Ruixue Ding, Boli Chen, Pengjun Xie, Fei Huang, Xin Li, Qiang-Wei Zhang, and Yao Xu. 2023. A Multi-Modal Geographic Pre-Training Method. ArXiv abs/2301.04283 (2023).
[12] Majigouren Enkhsajikhan, Wei Liu, Eun-Jung Holden, and Paul Duuring. 2021. Auto-labelling entities in low-resource text: a geological case study. Knowledge and Information Systems 63 (2021), 695 - 715.
[13] Jinlan Fu, See-Kiong Ng, Zhenghao Jiang, and Pengfei Liu. 2023. GPTScore: Evaluate as You Desire. ArXiv abs/2302.04166 (2023).
[14] Leo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. ArXiv abs/2101.00027 (2020).
[15] Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey Levine, and Dawn Song. 2023. Koala: A Dialogue Model for Academic Research. Blog post. https://hair.berkeley.edu/blog/2023/04/03/koala/
[16] Fabrizio Gilardi, Meysam Alizadeh, and Miel Kubli. 2023. ChatGPT outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences of the United States of America 120 (2023). https://api.semanticscholar. org/CorpusID:257766307
[17] Tanishq Gupta, Mohd Zaki, N. Krishnan, and Mausam. 2021. MatSciBERT: A materials domain language model for text mining and information extraction. npj Computational Materials 8 (2021), 1-11.
[18] Neil Houlsby, Andrei Giurgiu, Stanisław Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. In International Conference on Machine Learning.
[19] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. LoRA: Low-Rank Adaptation of Large Language Models. ArXiv abs/2106.09685 (2021).
[20] Jizhou Huang, Haifeng Wang, Yibe Sun, Yunsheng Shi, Zhengjie Huang, An Zhuo, and Shikun Feng. 2022. ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps. Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (2022).
[21] Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Chik, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. 2020. Scaling Laws for Neural Language Models. ArXiv abs/2001.08361 (2020).
[22] Zeljko Kraljevic, Anthony Shek, Daniel M Bean, Rebecca Bendayan, James T. H. Tro, and Richard J. B. Dobson. 2021. MedGPT: Medical Concept Prediction from Clinical Narratives. ArXiv abs/2107.03134 (2021).
[23] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. ArXiv abs/2104.08691 (2021).</p>
<p>[24] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers) abs/2101.00190 (2021).
[25] Xiao Liu, Juan Hu, Qi Shen, and Huan Chen. 2021. Geo-BERT Pre-training Model for Query Rewriting in POI Search. In Conference on Empirical Methods in Natural Language Processing.
[26] S. Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le, Barret Zoph, Jason Wei, and Adam Roberts. 2023. The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. ArXiv abs/2301.13688 (2023).
[27] Bin Lu, Lyuwen Wu, Lina Yang, Chenxing Sun, Wei Liu, Xiaoying Gan, Shiyu Liang, Luoyi Fu, Xinbing Wang, and Cheng Zhou. 2023. DataExpo: A One-Stop Dataset Service for Open Science Research. Companion Proceedings of the ACM Web Conference 2023 (2023).
[28] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining. Briefings in bioinformatics (2022).
[29] Kai Ma, Miao Tian, Yongjian Tan, Xuejing Xie, and Qinjun Qiu. 2021. What is this article about? Generative summarization with the BERT model in the geosciences domain. Earth Science Informatics 15 (2021), 21 - 36.
[30] Xiaogang Ma, Chao Ma, and Chengbin Wang. 2020. A new structure for representing and tracking version information in a deep time knowledge graph. Comput. Geosci. 145 (2020), 104620.
[31] Gengchen Mai, Weiming Huang, Jin Sun, Suhang Song, Deepak Mishra, Ninghao Liu, Song Gao, Tianming Liu, G. Cong, Yingjie Hu, Chris Cundy, Ziyuan Li, Rui Zhu, and Ni Lao. 2023. On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence. ArXiv abs/2304.06798 (2023).
[32] Gengchen Mai, Krzysztof Janowicz, Yingjie Hu, Song Gao, Bo Yan, Rui Zhu, Ling Cai, and Ni Lao. 2021. A review of location encoding for GeoAI: methods and applications. International Journal of Geographical Information Science 36 (2021), $639-673$. https://api.semanticscholar.org/CorpusID:243847917
[33] Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Natural Instructions: Benchmarking Generalization to New Tasks from Natural Language Instructions. arXiv preprint arXiv:2104.08773 (2021).
[34] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Haquan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis.
[35] OpenAI. 2022. Introducing ChatGPT. (2022). https://openai.com/blog/chatgpt
[36] OpenAI. 2023. GPT-4 Technical Report. ArXiv abs/2303.08774 (2023).
[37] José Padarian and Ignacio Fuentes. 2019. Word embeddings for application in geosciences: development, evaluation, and examples of soil-related concepts. SOIL (2019).
[38] Yujia Qin, Shi Liang, Yining Ye, Kunlun Zhu, Lan Yan, Ya-Ting Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Marc H. Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. ArXiv abs/2307.16789 (2023). https://api.semanticscholar.org/CorpusID:260334759
[39] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners. (2019).
[40] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. ArXiv abs/1910.10683 (2019).
[41] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablami, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Févry, Jason Alan Fries, Ryan Trehan, Stella Rose Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask Prompted Training Enables Zero-Shot Task Generalization. ArXiv abs/2110.08207 (2021).
[42] Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, and Raghav Mani. 2020. Bio-Megatron: Larger Biomedical Domain Language Model. ArXiv abs/2010.06060 (2020).
[43] K. Singhal, Shekoofeh Azizi, Tao Tu, Said Mahdavi, Jason Lee Kai Wei, Hyung Won Chung, Nathan Scales, Ajay Kumar Tanwani, Heather J. Cole-Lewis, Stephen J. Pfohl, P. A Payne, Martin G. Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, P. A. Mansfield, Blaise Agüera y Arcas, Dale R. Webster, Greg S. Corrado, Y. Matías, Katherine Hui-Ling Chou, Juraj Gottweis, Nenad Tomavsev, Yan Liu, Alvin Rajkomar, Joëlle K. Barral, Christopher Sennurs, Alan Karthikesalingam, and Vivek Natarajan. 2022. Large Language Models Encode Clinical Knowledge. ArXiv abs/2212.13138 (2022).
[44] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alp
aca.
[45] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony S. Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A Large Language Model for Science. ArXiv abs/2211.09085 (2022).
[46] MosaicML NLP Team. 2023. Introducing MPT-7B: A New Standard for OpenSource, ly Usable LLMs. (2023). www.mosaicml.com/blog/mpt-7b
[47] Hugo Touvron, Thibaut Laveil, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aur'elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. ArXiv abs/2302.13971 (2023).
[48] Benyou Wang, Qianqian Xie, Jiahuan Pei, Prayag Tiwari, Zhao Li, and Jie Fu. 2021. Pre-trained Language Models in Biomedical Domain: A Systematic Survey. ArXiv abs/2110.05006 (2021).
[49] Chengshan Wang, Robert M. Hazen, Qiuming Cheng, Michael H. Stephenson, Chenghu Zhou, Peter A. Fox, Shu-zhong Shen, Roland Oberhänslï, Zeng-qian Hou, Xiaogang Ma, Zhiqiang Feng, Junxuan Fan, Chao Ma, Xiumian Hu, Bin Luo, Juanle Wang, and Craig M. Schiffries. 2021. The Deep-Time Digital Earth program: data-driven discovery in geosciences. National Science Review 8 (2021).
[50] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct: Aligning Language Model with Self Generated Instructions. ArXiv abs/2212.10560 (2022).
[51] Jason Wei, Xuezhi Wang, Dale Schnurmans, Maarten Booma, Ed Huai hsin Chi, F. Xia, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning in Large Language Models. ArXiv abs/2201.11903 (2022). https: //api.semanticscholar.org/CorpusID:246411621
[52] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2023. Baize: An OpenSource Chat Model with Parameter-Efficient Tuning on Self-Chat Data. ArXiv abs/2304.01196 (2023).
[53] Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, and Chenghu Zhou. 2023. Exploring and Verbalizing Academic Ideas by Concept Co-occurrence. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, Toronto, Canada, 13001-13027. https://doi.org/10.18653/v1/2023.acl-long. 727
[54] Weizhe Yuan and Pengfei Liu. 2022. reStructured Pre-training. ArXiv abs/2206.11147 (2022).
[55] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tum, Zizuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, P. Zhang, Yuxiao Dong, and Jie Tang. 2022. GLM-130B: An Open Bilingual Pre-trained Model. ArXiv abs/2210.02414 (2022).
[56] Shao Zhang, Yuting Jia, Hui Xu, Ying Wen, Dakuo Wang, and Xinbing Wang. 2022. DeepShovel: An Online Collaborative Platform for Data Extraction in Geoscience Literature with AI Assistance. ArXiv abs/2202.10163 (2022). https: //api.semanticscholar.org/CorpusID:247011979</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ https://rruff.info/all/display=default/R070007&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
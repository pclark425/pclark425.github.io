<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1044 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1044</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1044</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-221640879</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2009.05429v1.pdf" target="_blank">Embodied Visual Navigation With Automatic Curriculum Learning in Real Environments</a></p>
                <p><strong>Paper Abstract:</strong> We present NavACL, a method of automatic curriculum learning tailored to the navigation task. NavACL is simple to train and efficiently selects relevant tasks using geometric features. In our experiments, deep reinforcement learning agents trained using NavACL significantly outperform state-of-the-art agents trained with uniform sampling – the current standard. Furthermore, our agents can navigate through unknown cluttered indoor environments to semantically-specified targets using only RGB images. Obstacle-avoiding policies and frozen feature networks support transfer to unseen real-world environments, without any modification or retraining requirements. We evaluate our policies in simulation, and in the real world on a ground robot and a quadrotor drone. Videos of real-world results are available in the supplementary material.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1044.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1044.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NavACL-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Collision-free embodied visual navigation agent trained with NavACL (Automatic Curriculum Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embodied visual navigation agent trained with an adaptive automatic curriculum (NavACL) using on-policy PPO, frozen pretrained visual feature encoders, and reward shaping to learn collision-free semantic object navigation in photorealistic indoor environments; evaluated in simulation and transferred zero-shot to physical robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NavACL agent (PPO policy with frozen feature encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Reinforcement learning agent trained with Proximal Policy Optimization (PPO) using an automatic curriculum (NavACL-Adaptive or NavACL-GOID), frozen spatial autoencoder features pretrained on real images, a Mask R-CNN semantic target feature, intrinsic exploration reward and LSTM memory; trained to navigate to semantic object labels while avoiding collisions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent; transferred to physical robots (Turtlebot3 AGV, DJI Tello UAV)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat/Gibson photorealistic indoor environments (Cooperstown, Avonia, Hometown)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Photorealistic, cluttered indoor scenes from the Gibson dataset rendered in Habitat: varied room layouts, furniture and obstacles, narrow corridors and open spaces, semantic object placements; tasks are semantic object-driven navigation from a start pose to a goal object class using RGB observations and discrete motion primitives, with episodes terminating on success, collision, or timeout (150 timesteps).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Geometric task features (Tab.I): geodesic (shortest-path) distance between start and goal, path complexity (ratio Euclidean/geodesic), turn angle (start orientation vs start->goal vector), agent and goal clearance (distance to nearest obstacles), agent/goal island radius (traversable area around start/goal); also task limits (max start-goal distance 10 m) and scene area (Cooperstown 40 m^2, Avonia 60 m^2, Hometown 61 m^2) are used to characterize complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Medium-to-high (tasks up to 10 m start-goal; scenes range ~40–61 m^2; narrow corridors and low clearance present in some tasks; Hometown reported as most challenging for the agent).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation characterized by number of held-out environment instances (three unseen test scenes), semantic variation (zero-shot target-class change during test), randomized object placements and scene clutter, and sampling of many random start-goal tasks; NavACL also measures task difficulty distribution (f_pi) across random tasks to adapt curricula.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Medium (3 unseen environments used for testing) to High (zero-shot tests on unseen semantic classes/objects and randomized tasks during training).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate (fraction of episodes reaching target) and SPL (Success weighted by inverse Path Length, as defined in eq.3).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>NavACL (all test envs): Success rate = 0.42 ± 0.19; SPL = 0.24 ± 0.09. Per-scene (NavACL): Cooperstown Success = 0.63 ± 0.14, SPL = 0.31 ± 0.09; Avonia Success = 0.50 ± 0.06, SPL = 0.21 ± 0.04; Hometown Success = 0.12 ± 0.08, SPL = 0.09 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>The paper explicitly connects environment complexity and variation to learning difficulty and generalization: collision-free constraints increase reward sparsity and make training harder; NavACL adaptively increases task complexity as policy competence grows (e.g., larger start-goal distances, narrower corridors), producing an implicit trade-off where higher complexity (longer distances, low clearance) reduces success and SPL, particularly under environment variation (unseen scenes or unseen semantic classes). The authors report markedly worse performance in larger/unseen environments (Hometown) and only moderate decreases for zero-shot semantic variation, concluding that adaptive curricula and frozen feature encoders help but do not fully close performance gaps under combined high complexity and variation.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Cooperstown (smaller area, unseen): Success = 0.63 ± 0.14; SPL = 0.31 ± 0.09 (interpreted as relatively high performance on a less complex but novel environment).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Hometown (larger/more complex, unseen): Success = 0.12 ± 0.08; SPL = 0.09 ± 0.06.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Automatic curriculum learning (NavACL-Adaptive and NavACL-GOID) producing a mixture of easy, frontier, and random tasks; on-policy PPO optimization; frozen pretrained spatial autoencoders; intrinsic exploration reward and collision-penalizing reward shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Generalization was explicitly tested: (1) environment generalization — policies evaluated on three unseen test environments (Cooperstown, Avonia, Hometown) showing reduced performance compared to human baseline and variation across scenes (best on Cooperstown, worst on Hometown); (2) semantic zero-shot generalization — NavACL Zero-Shot (train on ball, test on vase) shows lower but nonzero performance (All: Success = 0.36 ± 0.21; SPL = 0.17 ± 0.10), indicating capability to find unseen object classes without retraining but with performance degradation.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Main model trained for 20 million timesteps (took ~1 week on a GPU machine); ablation/variants used 5 million timesteps; authors note policies continued improving past 60 million timesteps, indicating substantial sample requirements but improved sample efficiency relative to uniform sampling baselines when using NavACL.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>NavACL (adaptive automatic curriculum) significantly improves learning in sparse, collision-free semantic navigation compared to uniform sampling and GoalGAN; frozen feature encoders reduce simulator overfitting and aid Sim2Real transfer; collision-free policies increase training difficulty but are crucial for real-world transfer; adaptive curricula progressively increase environment complexity (start-goal distance, clearance difficulty), and agent performance degrades under combined high complexity and high variation (e.g., Hometown unseen scene).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Visual Navigation With Automatic Curriculum Learning in Real Environments', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1044.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1044.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sim2Real-AGV-UAV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim2Real transfer of NavACL-trained policy to Turtlebot3 (AGV) and DJI Tello (UAV)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The NavACL-trained navigation policy was transferred without modification to two different physical robots (a Turtlebot3 wheeled AGV and a DJI Tello quadrotor UAV) and evaluated in real indoor environments, demonstrating collision-free behavior and unexpected cross-platform generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NavACL policy deployed on Turtlebot3 (AGV) and DJI Tello (UAV)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same learned PPO policy (with frozen feature encoders and collision-avoiding reward shaping) applied zero-shot to physical robots; AGV used wheel encoders for closed-loop execution of motion primitives, UAV used IR/IMU with noisy position estimates and same motion primitives abstractions.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>physical robot (ground AGV and quadrotor UAV)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Real-world indoor environments (multiple houses/offices; three test environments for AGV; additional rooms/hallways for UAV)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Cluttered indoor spaces with varied floor surfaces (hardwood, carpet, rugs), furniture, obstacles, moving people, narrow passages (e.g., between chair legs), and previously-unseen object instances and one unseen semantic class; AGV tasks: 7 tasks across 3 environments and 3 objects; UAV environment included hallways, desks, tripods and moving people.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of test tasks (AGV: 7 tasks), number of objects (3), real traveled distance (AGV: 29 m during tests), sensor noise (IMU/odometry), presence of dynamic obstacles; collision-free requirement increases effective task sparsity/complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Medium-to-high (real-world sensor noise, dynamic obstacles, narrow passages and varied terrains increase complexity relative to simulation).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Semantic variation (one unseen semantic class among test objects), terrain variation (hardwood, carpet, rugs), different physical environments and moving obstacles; mobility-type variation (same policy used on different robot embodiments).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>High (unseen object instances/classes, multiple environment instances, dynamic obstacles, and cross-platform embodiment variation).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Collision counts (qualitative safety), SPL for AGV (measured via wheel odometry), qualitative success (UAV trajectories and absence of collisions).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>AGV: zero collisions over 29 m traveled across the tests; SPL computed from wheel odometry was used but exact SPL numeric values are not provided in the paper text. UAV: qualitative success — flew through narrow gaps, through doorways, and around moving people without reported collisions; exact numeric SPLs or success counts not reported.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Authors attribute the successful Sim2Real transfer to the combination of collision-free policies (which reduce undesirable simulator artifacts like wall-sliding) and frozen feature encoders trained on real images; they note a trade-off: enforcing collision-free behavior makes training harder (increases sparsity) but produces policies that generalize better to real-world variation and different embodiment types (AGV and UAV).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Qualitative: UAV navigated complex, cluttered real-world scenes with moving people and narrow passages without collisions in many trials (no quantitative success/SPL numbers reported).</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Zero-shot transfer of NavACL-trained policy (automatic curriculum in simulation) with frozen visual feature encoders; no additional real-world training or fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>The policy generalized zero-shot to physical robots and to unseen real environments and objects: AGV executed seven tasks across three environments with no collisions over 29 m; UAV successfully navigated through constrained spaces and around moving people. This demonstrates cross-embodiment generalization and real-world robustness imparted by collision-free training and frozen encoders.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Transfer evaluation used a single policy trained in simulation (20 million timesteps); no further real-world training samples were used for transfer, so real-world sample cost = 0 (zero-shot).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Collision-avoiding policies trained with NavACL and frozen feature encoders can transfer zero-shot to different robot embodiments in real indoor scenes, achieving collision-free behavior in tested runs; collision-free constraints make simulation training harder but improve real-world robustness and cross-platform generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Visual Navigation With Automatic Curriculum Learning in Real Environments', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1044.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1044.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Random-Agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-action baseline agent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline agent that selects actions uniformly at random, used to provide a lower bound on embodied navigation performance in the same simulated tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Random policy</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Non-learning baseline that selects motion primitives uniformly at random; evaluated in the same Habitat/Gibson test episodes as learning agents to quantify baseline success and SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Habitat/Gibson unseen test environments (Cooperstown, Avonia, Hometown)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same photorealistic indoor scenes and task setup as the learned agents (semantic target navigation with collision termination); random actions operate under same motion primitives and episode limits.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same geometric features and task difficulty measures as for learned agents (start-goal distance up to 10 m, clearance, path complexity); no learning means complexity impacts success strongly.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Medium-to-high (same as learned agents).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Tested across the same three unseen environments and randomized tasks; variation as per dataset splits.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>Medium (three unseen environments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success rate and SPL.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Random (all test envs): Success rate = 0.03 ± 0.03; SPL = 0.02 ± 0.03. Per-scene: Cooperstown Success = 0.07 ± 0.00, SPL = 0.06 ± 0.00; Avonia Success = 0.01 ± 0.00, SPL = 0.01 ± 0.00; Hometown Success = 0.00 ± 0.00, SPL = 0.00 ± 0.00.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Random policy performs poorly across all complexity/variation conditions; the paper uses these baseline numbers to show learned agents substantially outperform chance, but does not analyze trade-offs for the random agent specifically.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>Cooperstown (smaller/unseen): Success = 0.07 ± 0.00; SPL = 0.06 ± 0.00 (still very low compared to learned agents).</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>Hometown (larger/more complex, unseen): Success = 0.00 ± 0.00; SPL = 0.00 ± 0.00.</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>None (non-learning baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>As a non-learning baseline, the random policy was evaluated on unseen environments and performed at near-zero success and SPL, providing a lower bound for comparison with learning agents.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Not applicable (no training).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Random baseline confirms the navigation tasks are non-trivial: learned NavACL agents achieve substantially higher success and SPL than chance even under environment variation and collision-free constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Embodied Visual Navigation With Automatic Curriculum Learning in Real Environments', 'publication_date_yy_mm': '2021-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Automatic goal generation for reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Intrinsic motivation and automatic curricula via asymmetric self-play <em>(Rating: 2)</em></li>
                <li>Habitat: A platform for embodied ai research <em>(Rating: 2)</em></li>
                <li>Gibson env: Real-world perception for embodied agents <em>(Rating: 2)</em></li>
                <li>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames <em>(Rating: 2)</em></li>
                <li>Deep spatial autoencoders for visuomotor learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1044",
    "paper_id": "paper-221640879",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "NavACL-Agent",
            "name_full": "Collision-free embodied visual navigation agent trained with NavACL (Automatic Curriculum Learning)",
            "brief_description": "An embodied visual navigation agent trained with an adaptive automatic curriculum (NavACL) using on-policy PPO, frozen pretrained visual feature encoders, and reward shaping to learn collision-free semantic object navigation in photorealistic indoor environments; evaluated in simulation and transferred zero-shot to physical robots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NavACL agent (PPO policy with frozen feature encoders)",
            "agent_description": "Reinforcement learning agent trained with Proximal Policy Optimization (PPO) using an automatic curriculum (NavACL-Adaptive or NavACL-GOID), frozen spatial autoencoder features pretrained on real images, a Mask R-CNN semantic target feature, intrinsic exploration reward and LSTM memory; trained to navigate to semantic object labels while avoiding collisions.",
            "agent_type": "simulated agent; transferred to physical robots (Turtlebot3 AGV, DJI Tello UAV)",
            "environment_name": "Habitat/Gibson photorealistic indoor environments (Cooperstown, Avonia, Hometown)",
            "environment_description": "Photorealistic, cluttered indoor scenes from the Gibson dataset rendered in Habitat: varied room layouts, furniture and obstacles, narrow corridors and open spaces, semantic object placements; tasks are semantic object-driven navigation from a start pose to a goal object class using RGB observations and discrete motion primitives, with episodes terminating on success, collision, or timeout (150 timesteps).",
            "complexity_measure": "Geometric task features (Tab.I): geodesic (shortest-path) distance between start and goal, path complexity (ratio Euclidean/geodesic), turn angle (start orientation vs start-&gt;goal vector), agent and goal clearance (distance to nearest obstacles), agent/goal island radius (traversable area around start/goal); also task limits (max start-goal distance 10 m) and scene area (Cooperstown 40 m^2, Avonia 60 m^2, Hometown 61 m^2) are used to characterize complexity.",
            "complexity_level": "Medium-to-high (tasks up to 10 m start-goal; scenes range ~40–61 m^2; narrow corridors and low clearance present in some tasks; Hometown reported as most challenging for the agent).",
            "variation_measure": "Variation characterized by number of held-out environment instances (three unseen test scenes), semantic variation (zero-shot target-class change during test), randomized object placements and scene clutter, and sampling of many random start-goal tasks; NavACL also measures task difficulty distribution (f_pi) across random tasks to adapt curricula.",
            "variation_level": "Medium (3 unseen environments used for testing) to High (zero-shot tests on unseen semantic classes/objects and randomized tasks during training).",
            "performance_metric": "Success rate (fraction of episodes reaching target) and SPL (Success weighted by inverse Path Length, as defined in eq.3).",
            "performance_value": "NavACL (all test envs): Success rate = 0.42 ± 0.19; SPL = 0.24 ± 0.09. Per-scene (NavACL): Cooperstown Success = 0.63 ± 0.14, SPL = 0.31 ± 0.09; Avonia Success = 0.50 ± 0.06, SPL = 0.21 ± 0.04; Hometown Success = 0.12 ± 0.08, SPL = 0.09 ± 0.06.",
            "complexity_variation_relationship": "The paper explicitly connects environment complexity and variation to learning difficulty and generalization: collision-free constraints increase reward sparsity and make training harder; NavACL adaptively increases task complexity as policy competence grows (e.g., larger start-goal distances, narrower corridors), producing an implicit trade-off where higher complexity (longer distances, low clearance) reduces success and SPL, particularly under environment variation (unseen scenes or unseen semantic classes). The authors report markedly worse performance in larger/unseen environments (Hometown) and only moderate decreases for zero-shot semantic variation, concluding that adaptive curricula and frozen feature encoders help but do not fully close performance gaps under combined high complexity and variation.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": "Cooperstown (smaller area, unseen): Success = 0.63 ± 0.14; SPL = 0.31 ± 0.09 (interpreted as relatively high performance on a less complex but novel environment).",
            "high_complexity_high_variation_performance": "Hometown (larger/more complex, unseen): Success = 0.12 ± 0.08; SPL = 0.09 ± 0.06.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Automatic curriculum learning (NavACL-Adaptive and NavACL-GOID) producing a mixture of easy, frontier, and random tasks; on-policy PPO optimization; frozen pretrained spatial autoencoders; intrinsic exploration reward and collision-penalizing reward shaping.",
            "generalization_tested": true,
            "generalization_results": "Generalization was explicitly tested: (1) environment generalization — policies evaluated on three unseen test environments (Cooperstown, Avonia, Hometown) showing reduced performance compared to human baseline and variation across scenes (best on Cooperstown, worst on Hometown); (2) semantic zero-shot generalization — NavACL Zero-Shot (train on ball, test on vase) shows lower but nonzero performance (All: Success = 0.36 ± 0.21; SPL = 0.17 ± 0.10), indicating capability to find unseen object classes without retraining but with performance degradation.",
            "sample_efficiency": "Main model trained for 20 million timesteps (took ~1 week on a GPU machine); ablation/variants used 5 million timesteps; authors note policies continued improving past 60 million timesteps, indicating substantial sample requirements but improved sample efficiency relative to uniform sampling baselines when using NavACL.",
            "key_findings": "NavACL (adaptive automatic curriculum) significantly improves learning in sparse, collision-free semantic navigation compared to uniform sampling and GoalGAN; frozen feature encoders reduce simulator overfitting and aid Sim2Real transfer; collision-free policies increase training difficulty but are crucial for real-world transfer; adaptive curricula progressively increase environment complexity (start-goal distance, clearance difficulty), and agent performance degrades under combined high complexity and high variation (e.g., Hometown unseen scene).",
            "uuid": "e1044.0",
            "source_info": {
                "paper_title": "Embodied Visual Navigation With Automatic Curriculum Learning in Real Environments",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Sim2Real-AGV-UAV",
            "name_full": "Sim2Real transfer of NavACL-trained policy to Turtlebot3 (AGV) and DJI Tello (UAV)",
            "brief_description": "The NavACL-trained navigation policy was transferred without modification to two different physical robots (a Turtlebot3 wheeled AGV and a DJI Tello quadrotor UAV) and evaluated in real indoor environments, demonstrating collision-free behavior and unexpected cross-platform generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "NavACL policy deployed on Turtlebot3 (AGV) and DJI Tello (UAV)",
            "agent_description": "Same learned PPO policy (with frozen feature encoders and collision-avoiding reward shaping) applied zero-shot to physical robots; AGV used wheel encoders for closed-loop execution of motion primitives, UAV used IR/IMU with noisy position estimates and same motion primitives abstractions.",
            "agent_type": "physical robot (ground AGV and quadrotor UAV)",
            "environment_name": "Real-world indoor environments (multiple houses/offices; three test environments for AGV; additional rooms/hallways for UAV)",
            "environment_description": "Cluttered indoor spaces with varied floor surfaces (hardwood, carpet, rugs), furniture, obstacles, moving people, narrow passages (e.g., between chair legs), and previously-unseen object instances and one unseen semantic class; AGV tasks: 7 tasks across 3 environments and 3 objects; UAV environment included hallways, desks, tripods and moving people.",
            "complexity_measure": "Number of test tasks (AGV: 7 tasks), number of objects (3), real traveled distance (AGV: 29 m during tests), sensor noise (IMU/odometry), presence of dynamic obstacles; collision-free requirement increases effective task sparsity/complexity.",
            "complexity_level": "Medium-to-high (real-world sensor noise, dynamic obstacles, narrow passages and varied terrains increase complexity relative to simulation).",
            "variation_measure": "Semantic variation (one unseen semantic class among test objects), terrain variation (hardwood, carpet, rugs), different physical environments and moving obstacles; mobility-type variation (same policy used on different robot embodiments).",
            "variation_level": "High (unseen object instances/classes, multiple environment instances, dynamic obstacles, and cross-platform embodiment variation).",
            "performance_metric": "Collision counts (qualitative safety), SPL for AGV (measured via wheel odometry), qualitative success (UAV trajectories and absence of collisions).",
            "performance_value": "AGV: zero collisions over 29 m traveled across the tests; SPL computed from wheel odometry was used but exact SPL numeric values are not provided in the paper text. UAV: qualitative success — flew through narrow gaps, through doorways, and around moving people without reported collisions; exact numeric SPLs or success counts not reported.",
            "complexity_variation_relationship": "Authors attribute the successful Sim2Real transfer to the combination of collision-free policies (which reduce undesirable simulator artifacts like wall-sliding) and frozen feature encoders trained on real images; they note a trade-off: enforcing collision-free behavior makes training harder (increases sparsity) but produces policies that generalize better to real-world variation and different embodiment types (AGV and UAV).",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": "Qualitative: UAV navigated complex, cluttered real-world scenes with moving people and narrow passages without collisions in many trials (no quantitative success/SPL numbers reported).",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Zero-shot transfer of NavACL-trained policy (automatic curriculum in simulation) with frozen visual feature encoders; no additional real-world training or fine-tuning.",
            "generalization_tested": true,
            "generalization_results": "The policy generalized zero-shot to physical robots and to unseen real environments and objects: AGV executed seven tasks across three environments with no collisions over 29 m; UAV successfully navigated through constrained spaces and around moving people. This demonstrates cross-embodiment generalization and real-world robustness imparted by collision-free training and frozen encoders.",
            "sample_efficiency": "Transfer evaluation used a single policy trained in simulation (20 million timesteps); no further real-world training samples were used for transfer, so real-world sample cost = 0 (zero-shot).",
            "key_findings": "Collision-avoiding policies trained with NavACL and frozen feature encoders can transfer zero-shot to different robot embodiments in real indoor scenes, achieving collision-free behavior in tested runs; collision-free constraints make simulation training harder but improve real-world robustness and cross-platform generalization.",
            "uuid": "e1044.1",
            "source_info": {
                "paper_title": "Embodied Visual Navigation With Automatic Curriculum Learning in Real Environments",
                "publication_date_yy_mm": "2021-04"
            }
        },
        {
            "name_short": "Random-Agent",
            "name_full": "Random-action baseline agent",
            "brief_description": "A baseline agent that selects actions uniformly at random, used to provide a lower bound on embodied navigation performance in the same simulated tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Random policy",
            "agent_description": "Non-learning baseline that selects motion primitives uniformly at random; evaluated in the same Habitat/Gibson test episodes as learning agents to quantify baseline success and SPL.",
            "agent_type": "simulated agent (baseline)",
            "environment_name": "Habitat/Gibson unseen test environments (Cooperstown, Avonia, Hometown)",
            "environment_description": "Same photorealistic indoor scenes and task setup as the learned agents (semantic target navigation with collision termination); random actions operate under same motion primitives and episode limits.",
            "complexity_measure": "Same geometric features and task difficulty measures as for learned agents (start-goal distance up to 10 m, clearance, path complexity); no learning means complexity impacts success strongly.",
            "complexity_level": "Medium-to-high (same as learned agents).",
            "variation_measure": "Tested across the same three unseen environments and randomized tasks; variation as per dataset splits.",
            "variation_level": "Medium (three unseen environments).",
            "performance_metric": "Success rate and SPL.",
            "performance_value": "Random (all test envs): Success rate = 0.03 ± 0.03; SPL = 0.02 ± 0.03. Per-scene: Cooperstown Success = 0.07 ± 0.00, SPL = 0.06 ± 0.00; Avonia Success = 0.01 ± 0.00, SPL = 0.01 ± 0.00; Hometown Success = 0.00 ± 0.00, SPL = 0.00 ± 0.00.",
            "complexity_variation_relationship": "Random policy performs poorly across all complexity/variation conditions; the paper uses these baseline numbers to show learned agents substantially outperform chance, but does not analyze trade-offs for the random agent specifically.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": "Cooperstown (smaller/unseen): Success = 0.07 ± 0.00; SPL = 0.06 ± 0.00 (still very low compared to learned agents).",
            "high_complexity_high_variation_performance": "Hometown (larger/more complex, unseen): Success = 0.00 ± 0.00; SPL = 0.00 ± 0.00.",
            "low_complexity_low_variation_performance": null,
            "training_strategy": "None (non-learning baseline).",
            "generalization_tested": true,
            "generalization_results": "As a non-learning baseline, the random policy was evaluated on unseen environments and performed at near-zero success and SPL, providing a lower bound for comparison with learning agents.",
            "sample_efficiency": "Not applicable (no training).",
            "key_findings": "Random baseline confirms the navigation tasks are non-trivial: learned NavACL agents achieve substantially higher success and SPL than chance even under environment variation and collision-free constraints.",
            "uuid": "e1044.2",
            "source_info": {
                "paper_title": "Embodied Visual Navigation With Automatic Curriculum Learning in Real Environments",
                "publication_date_yy_mm": "2021-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Automatic goal generation for reinforcement learning agents",
            "rating": 2,
            "sanitized_title": "automatic_goal_generation_for_reinforcement_learning_agents"
        },
        {
            "paper_title": "Intrinsic motivation and automatic curricula via asymmetric self-play",
            "rating": 2,
            "sanitized_title": "intrinsic_motivation_and_automatic_curricula_via_asymmetric_selfplay"
        },
        {
            "paper_title": "Habitat: A platform for embodied ai research",
            "rating": 2,
            "sanitized_title": "habitat_a_platform_for_embodied_ai_research"
        },
        {
            "paper_title": "Gibson env: Real-world perception for embodied agents",
            "rating": 2,
            "sanitized_title": "gibson_env_realworld_perception_for_embodied_agents"
        },
        {
            "paper_title": "Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames",
            "rating": 2,
            "sanitized_title": "ddppo_learning_nearperfect_pointgoal_navigators_from_25_billion_frames"
        },
        {
            "paper_title": "Deep spatial autoencoders for visuomotor learning",
            "rating": 1,
            "sanitized_title": "deep_spatial_autoencoders_for_visuomotor_learning"
        }
    ],
    "cost": 0.01710275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Embodied Visual Navigation with Automatic Curriculum Learning in Real Environments</p>
<p>Steven D Morad 
Roberto Mecca 
Rudra P K Poudel 
Stephan Liwicki 
Roberto Cipolla 
Embodied Visual Navigation with Automatic Curriculum Learning in Real Environments
270B1BA468C43ECDED17FC5010AE5346Visual-based navigationreinforcement learningautonomous agents
We present NavACL, a method of automatic curriculum learning tailored to the navigation task.NavACL is simple to train and efficiently selects relevant tasks using geometric features.In our experiments, deep reinforcement learning agents trained using NavACL in collision-free environments significantly outperform state-of-the-art agents trained with uniform sampling -the current standard.Furthermore, our agents are able to navigate through unknown cluttered indoor environments to semantically-specified targets using only RGB images.Collision avoidance policies and frozen feature networks support transfer to unseen real-world environments, without any modification or retraining requirements.We evaluate our policies in simulation, and in the real world on a ground robot and a quadrotor drone.Videos of real-world results are available in the supplementary material. 1</p>
<p>I. INTRODUCTION</p>
<p>Navigation forms a core challenge in embodied artificial intelligence (embodied AI) [1], [2].Typical tasks involve point [3]- [5], object [3], [6], [7] or area-driven [8], [9] navigation in synthetic and real environments [10]- [14].In our work, we focus on semantic object-driven navigation in unknown indoor scenes.Since semantic object-driven navigation uses objectclass labels alone (not specific instances), agents can directly be deployed to novel environments and duties; suitable for post disaster recovery robots or embodied assistance technology with a wide range of task scenarios.</p>
<p>Before the embodied AI renaissance, approaches such as active vision [15] and active visual simultaneous localization and mapping (active VSLAM) [16] were popular methods for building autonomous agents.They combined classical motion planning [17], [18] with non-learned exploration policies such as frontier expansion [19] to direct the agent.Active VSLAM and active vision work well in ideal circumstances, but are brittle and lack generalization ability in real world situations.</p>
<p>Deep reinforcement learning (DRL) gained traction in the landmark paper [20], where DRL agents outperformed humans -all be it in relatively simple arcade games.Since then, the scope of DRL has expanded to real-world applications.In robotics and navigation, DRL shows promise as an alternative to classical models due to its surprising stevenmorad@gmail.com,{roberto.mecca,rudra.poudel,stephan.liwicki}@crl.toshiba.co.uk § Faculty of the Department of Engineering, University of Cambridge, Trumpington St, Cambridge CB2 1PZ, UK rc10001@cam.ac.uk 1 Also available at https://www.youtube.com/playlist?list=PLkG_dDkoI9pjPdOGyTec-sSu20pB7iayC robustness and ability to generalize to real-world uncertainties.[6] trained visual navigation agents in a video-game maze, showing that over time, DRL agents can memorize the layout of a maze from vision alone.Since then, there has been an explosion of DRL-based visual navigation, fuelled by the abundance of indoor photo-realistic simulators and datasets [10]- [14].Evidence suggests DRL outperforms traditional methods in such cluttered, realistic indoor environments [5].</p>
<p>Recent pushes in indoor visual navigation with DRL have focused on point-driven navigation (e.g.[3]- [5]) -finding the most efficient path given visual observations, noise-free agent coordinates, and the goal coordinates.In 2019, [21] produced near-perfect visual point navigation agents by training on a large set of experience over six GPU-months.However, the requirements of point-driven navigation are less practical in real-world settings, as prior setup of an indoor localization system is required.Nevertheless, point navigation is an important stepping stone towards more natural and difficult forms of indoor navigation.Specifically, [3] reformulates a point navigation agent to find an object using visual cues alone.While promising, retraining is required for new target objects.In alternative approaches an image of the goal position is provided for targeting, which opens the door to generalizing to multiple and novel object types [8], [22].In particular, these methods store representations of the scene in memory, and match target observations to memorized representations.Therein lies a caveat -scene memorization requires retraining a portion of the policy on novel scenes before it can operate in them.Instead, we choose to use semantic object-driven navigation which generalizes to never-before-seen scenes and target object categories.</p>
<p>A. Contributions</p>
<p>In this work, we focus on indoor object-driven navigation using embodied AI and semantic targeting.We are interested in generalization to new environments and targets, including simulator-to-real-world (Sim2Real) generalization, without any retraining.</p>
<p>Navigating to targets specified by semantic label can generalize across multiple targets and to unknown environments without data refinement or retraining.We call this generalization ability about instances and classes zero-shot semantic navigation.We emphasize, by leveraging large segmentation datasets like COCO [23], we can use zeroshot semantic navigation to navigate to object instances, and even object classes, never seen before in training simulations.[3] does not learn an effective policy when the collision-free property is enforced (episode termination and negative reward upon collision).Plots represent mean and standard deviation of a single train/test scene over five trials.Collision-free agents are important for Sim2Real transfer, but are much harder to train (SPL defined in eq. 3 -larger is better).(1b top) Top-down view of fully-trained navigation agents exploiting collision mechanics to slide along walls.(1b bottom) Our agents with collision avoidance and automatic curriculum learning.Agent starts at the pink point and move towards the green goal, leaving a blue path.Red dots indicate collisions.and time again in simulation, performance rarely transfers to the real world.One challenge in Sim2Real transfer is overfitting to simulator-rendered images [24].Using frozen feature encoders trained on real images, [3] shows compelling generalization ability across multiple simulators.In our work, we demonstrate frozen feature networks and collision avoidance help bridge the Sim2Real gap by showcasing our policies on real robots in real environments.Another issue present in almost every navigation simulator is collision modeling exploitation [1].Agents drive into a wall at an angle and slide along it, covering the perimeter of a simulated building (Fig. 1b).[25] demonstrates collision-avoidance policies trained entirely in simulation can transfer to the real world, but stop short of investigating longer-term navigation policies.</p>
<p>Enforcing collision-free paths for agents results in increased reward sparsity, making training more difficult with state-ofthe-art navigation tools (Fig. 1a).We mitigate this elevated sparsity using automatic curriculum learning.The essence of curriculum learning is selecting and ordering training data in a way that produces desirable characteristics in the learner, such as generality, accuracy, and sample efficiency.Automatic curriculum learning (ACL) is the process of generating this curriculum without human in the loop.Curriculum for neural networks was proposed by [26], and [27] affords a thorough overview of ACL applied to DRL.For navigation, tasks can be represented using low-dimensional Cartesian start and goal states.Some ACL methods that produce tasks of this form are asymmetric self play [28] and GoalGAN [29].Asymmetric self play requires collecting distinct episodes for two separate policies, which is computationally expensive using 3D simulators.GoalGAN trades performance for generality.It can generate tasks for arbitrary problems, but uses a generative adversarial network (GAN) which is notoriously unstable and difficult to train.Instead, we trade generality for efficiency and propose a simple classification-based ACL method termed NavACL specifically for navigation.</p>
<p>In summary, we cast the visual navigation problem setup as follows:</p>
<p>i the agent's observations consist of RGB images from an agent-mounted camera and the semantic label of the target (e.g."football", "vase"), ii the agent's actions consist of discrete, position-based motion primitives (i.e.move forward, turn left or right), without explicit loop closure outside of said primitives, iii upon reaching the target, collision with the environment, or exceeding a preset time limit, the episode ends and contribute: i a simple and efficient method to automatically generate curriculum for visual navigation agents, ii zero-shot semantic navigation -finding objects and object classes never seen during training, iii a collision-free navigation policy for complex, unseen environments that bridges the Sim2Real gap without any sort of retraining II.APPROACH</p>
<p>A. NavACL</p>
<p>Motivated by evidence that intermediate difficulty tasks provide more learning signal for policy improvement than random tasks [27], [29] and that replaying easy tasks alleviates catastrophic forgetting [30]- [32], we formualte our ACL method, termed NavACL.NavACL filters down uniform random tasks to those that provide the most learning signal to the agent using predicted task success, described below.</p>
<p>Since our navigation problem has well-defined termination scenarios (agent reached the goal or not), we use binary task success as the signal metric.Let task h = (s 0 , s g ), with agent start position s 0 and goal position s g .f * π (h) denotes the probability of navigation policy π solving task h, zero Fig. 2. The agent training pipeline, and how our contributions fit within it.Observations from the environment are compressed into latent features before being passed to the policy network.NavACL trains on navigation episodes and serves tasks back to the simulator.</p>
<p>TABLE I NAVACL GEOMETRIC PROPERTIES Geodesic Distance</p>
<p>The shortest-path distance from s 0 to sg Path Complexity</p>
<p>The ratio of euclidean distance to geodesic distance of s 0 , sg Turn Angle</p>
<p>The angle between the starting orientation and − − → s 0 sg, represented as sine and cosine components Agent/Goal Clearance Distance from s 0 and sg respectively to the nearest obstacle Agent/Goal Island Radius of the traversable area at s 0 and sg respectively</p>
<p>for certain failure and one for certain success.We estimate task success probability f * π using a fully-connected deep neural network we call f π .Before each forward pass, f π preprocesses h into geometric properties (Tab.I), allowing f π to generalize across scenes.f π is updated alongside π in the training loop (Alg.1,2).We define task difficulty as the complement of the estimated success probability, 1 − f π (h).In contrast to [29] that formulates scenario generation with GANs for general frameworks, we optimize NavACL to generate scenarios efficiently for the navigation task using simple log loss.</p>
<p>Adaptive filtering Now that we can estimate the difficulty of tasks, which tasks should we feed the agent?In one implementation, we produce goals of intermediate difficulty (GOID) [29], selecting tasks bounded between two success probabilities.This ensures we never select tasks that are too easy or too hard.However, GOID does not explicitly deal with catastrophic forgetting of easy tasks.Furthermore, the bounds do not change as the agent improves and task distribution shifts (Fig. 3).Instead, we provide a mixture of task types, where certain tasks adapt to the learner.Easy tasks provide adequate learning signal early in the training process and prevent catastrophic forgetting.Frontier tasks teach the agent to solve new tasks at its current ability.Uniformly sampled random tasks inject entropy and prevent the learner from overfitting to specific task types.Initially, easy and frontier tasks form the majority of the task mixture.The mixture decays into random sampling as the learning agent learns to generalize.</p>
<p>We draw many random tasks and estimate their difficulty using f π , producing a difficulty estimate across the task space.We fit a normal distribution µ f , σ f to this distribution (Alg.1).µ f , σ f form an adaptive boundary in task space, partitioning it into easy and hard regions, predicated on policy π.In particular, task h is considered an
easy task if f π (h) &gt; µ f +βσ f and a frontier task if µ f −γσ f &lt; f π (h) &lt; µ f +γσ f ,
where β, γ are hyperparameters.In other words, task difficulty is relative to the current ability of the agent -if we expect π to do better on task h than an average task, it is easy.If h is near the difficulty of the average task, straddling the adaptive boundary, we call it a frontier task (Alg.3).Intuitively, this should provide a more conservative mixture of tasks than pure random sampling, promoting stable learning in difficult environments.The full algorithm is detailed in Alg.1-3.Fig. 2 presents a flowchart of our contributions, which includes NavACL, the reward function for collision-avoidance, and the frozen feature networks.We discuss each piece in the following subsections.</p>
<p>B. Reward Shaping</p>
<p>Our reward function provides negative rewards to discourage collision and intrinsic rewards for exploration and to encourage movement.We define it as:
r(s) = 1 succ + δ(−1 coll + 1 expl ) + 0.01d.(1)case frontier do if µ f − γσ f &lt; fπ(h) &lt; µ f + γσ f then return h; case random do return h;
The binary indicator 1 succ is true upon reaching the target, and false otherwise.1 coll is true upon collision and false otherwise.The hyperparameter 0 &lt; δ &lt; 1 controls the agent's affinity for learning exploration and motor skills compared to target-seeking behavior.1 expl is an intrinsic reward for exploration.We keep a buffer of past agent positions over an episode, and provide a reward if the current position of the agent is some distance from all previous positions.We find that without the intrinsic exploration term, the agent falls into a local maxima of spinning in place to avoid the negative reward from collisions, which is difficult to escape.d is the distance traveled in the current step, expressing the prior that the agent should be trying to cover as large a search area as possible.</p>
<p>C. Frozen Feature Networks</p>
<p>Traditional visual DRL agents use an autoencoder to transform input RGB images into a latent representation, where the autoencoder is trained end-to-end with the policy network [33], [34].End-to-end training can overfit the policy network to simulation artifacts, and hurt real-world transfer [24].We use spatial autoencoders pretrained on real images [3] and freeze their weights to prevent overfitting to simulation renders during training.High-polygon meshes scanned by [10] and photorealistic renderers provided by [5] produce detailed enough visualizations to work with encoders trained on realworld datasets.Freezing also speeds up policy convergence, as the gradient backpropagates through fewer layers.</p>
<p>D. Semantic Target Network</p>
<p>The semantic target feature is produced using a Mask R-CNN with an FPN backbone trained on the COCO dataset [23], [35].We introduce a small postprocessing layer that enables swapping target classes without retraining.Given an image, the Mask R-CNN predicts a binary mask M for each object class, along with the prediction confidence.We extract the mask with target label l and do scalar multiplication of the binary mask with the prediction confidence to get output O.</p>
<p>O(x, y) = P (M (x, y) label=l ).</p>
<p>(
)2
We can change l at runtime to search for different target classes.Pixels of O still contain shape information on the target object (e.g. a ball mask will be round but a box mask will be square).To prevent the downstream policy from overfitting to one specific object shape, as well as reduce latent size, we apply a max-pool operation to O which is then stacked along the other features into a latent representation, which is fed to the policy network.</p>
<p>III. MODEL DESCRIPTION</p>
<p>Our learner model consists of an actor-critic model with policy π(s) and value function V π (s) optimized using proximal policy optimization (PPO) with clipping (Tab.II) [36], [37].The policy network and value function take latent representations from the feature encoders as input, and produce an action and value estimate as output.The policy networks consists of feature-compression and memory sections.The feature-compression section compresses spatiallycoherent latent features into a more compact representation using convolutional layers.Receiving features instead of full RGB images reduces time to train and the likelihood of overfitting to the simulator.</p>
<p>To keep the navigation problem Markovian, the state must contain information on where the agent has been, and if it has previously seen the target.The purpose of the memory section is to store this information.The memory section uses long short-term memory (LSTM) [38] cells to represent state in the partially-observable Markov decision process (POMDP) [39].With this, we aim to reduce the likelihood of revisiting previously explored areas and to remember the target location if it leaves the view.Note, obstacle circumvention may lead to significant turns, losing sight of the target.</p>
<p>IV. EXPERIMENTS</p>
<p>We present three experiments: an ablation study of NavACL, a simulation benchmark of our model on unseen environments and target objects, and a benchmark of our agent operating in the real world.</p>
<p>A. Evaluating NavACL</p>
<p>Our first experiment compares the impact of NavACL on visual navigation to GoalGAN as well as the current standard of uniform task sampling.We evaluate NavACL with GOID (NavACL-GOID) and with adaptive filtering (NavACL-Adaptive).We hold all policy parameters the same, and run three navigation trials of five million samples on the Cooperstown environment from the Gibson dataset [10].Uniform sampling uses Habitat's built-in task generator to generate tasks [5].GoalGAN uses an intermediate difficulty value between 0.1 and 0.9, used in their MazeAnt navigation experiment.For NavACL-GOID, we filter uniformly random tasks using our f π framework, and target tasks with an intermediate difficulty value of 0.4 ≤ f π (h) ≤ 0.6.NavACL-Adaptive uses hyperparameters β = 1, γ = 0.1.Both NavACL variants significantly outperform uniform sampling as well as GoalGAN, with NavACL-Adaptive showing an improvement over NavACL-GOID (Fig. 4).Therefore, we use NavACL-Adaptive in remaining evaluations.</p>
<p>B. Evaluating Model Performance</p>
<p>Using our methodology, we train a policy over twenty million timesteps using the Habitat 2019 challenge split of the Gibson dataset.Policies are evaluated over ten trials of thirty episodes spread across three unseen test environments, held out from the Habitat split (Fig. 6).The test tasks are generated randomly using the same uniform sampling as the Habitat challenge datasets [5].The target object is an 11cm radius football (soccer ball).All policies are limited to 150 timesteps, and all tasks have a maximum start to goal distance of 10m.We find increasing the number of timesteps beyond 150 results in little improvement.The action space consists of a rotation of ±30 • and forward translation of 0.2m.</p>
<p>The random policy selects random actions to provide a lower bound on performance.The NavACL policy is trained using depth, reshading (de-texturing and re-lighting), and semantic features, along with NavACL and intrinsic rewards.NavACL Zero-Shot is identical to NavACL, but during testing we change the target from the ball to a large vase to evaluate zero-shot semantic generalization to unseen targets of different shapes and sizes.We recruit ten volunteers from varying backgrounds in order to establish an upper-bound on performance.The human policies are trained and tested just like the agent policies.The volunteers played the training set until they were comfortable with the controls, receiving the same RGB observations and action space as the agents.Once comfortable, the volunteers played through the same test set as the agents.We use the SPL metric defined by [40] 1
N N i=1 1 succ,i l i max(l i , p i ) (3)
where l is the length of the agent's path, p is the length of the shortest path from start to goal, and N is the number of episodes.Results are presented in Fig. 5 and Tab.III.</p>
<p>Our agents are able to find semantically specified targets in simulation, performing drastically better than random.Agents perform slightly worse on unseen semantic classes, but still exhibit zero-shot semantic generalization capability.On average, humans are able to outperform the agents on unseen environments.However, humans can memorize test scenes during the first few episodes, giving them an advantage over agents during later episodes.On Cooperstown, agents are within one standard deviation of human-level performance  in success rate, suggesting they outperform some humans in some cases.We found agents had trouble navigating to new spaces in larger, unseen environments.Agents did not have as much trouble when navigating in large, previously-seen environments.Memory for embodied visual agents is an active area of research [8], [41]- [43], and we expect leveraging these memory modules will improve performance in larger environments.Another limitation was model throughput -it took roughly a week to train twenty million timesteps on a GPU machine, and previous experiments were still showing policy improvement at sixty million timesteps.With memory improvements and distributed computing, we believe our models could approach human performance.</p>
<p>C. Sim2Real Transfer</p>
<p>We transfer our policy without modification to a Turtlebot3 wheeled robot (AGV) and a DJI Tello quadrotor (UAV).The AGV uses wheel encoders for closed-loop control for motion primitives (single actions), but does not estimate odometry across actions.We tested the AGV on seven tasks spanning three environments and three objects, one being an unseen object and one being from an unseen semantic class.We use wheel odometry to measure SPL for the AGV (Tab.IV).The AGV did not experience a single collision over the 29m it traveled during tests and was robust to actuator noise as well as wheel slip caused by terrain (hardwood, carpet, and rugs).</p>
<p>The UAV uses IR sensors to determine height and an IMU to obtain very noisy position estimates for motion primitives and hovering stability.We did not train a separate model for the UAV.We used the model trained with the AGV height  Policies trained for the AGV seemed surprisingly effective on the UAV, suggesting greater model generalization than we anticipated.The UAV was able to fly in-between legs of a camera tripod, through doorways, and even around moving people on many occasions without collision.This surprising generalization performance implies it may be possible to train a single navigation model for use on diverse types of robots that implement similar motion primitives.We provide video results of both the AGV and the UAV in the supplementary material,2 and illustrations in Fig. 7.</p>
<p>V. CONCLUSIONS We introduce NavACL and present two variants (NavACL-GOID) and (NavACL-Adaptive) for task generation in navigation.Both methods significantly improve upon uniform sampling (the current standard approach) as well as GoalGAN in sparse-reward settings.Combining NavACL with frozen feature networks and collision-free policies produces agents capable of zero-shot semantic navigation in both simulation and the real world.</p>
<p>A. Future Work</p>
<p>We found LSTMs had issues with generalization to new environments with the compute power available to us.Future work will focus on integrating more structured and efficient memory modules [8], [41]- [43] into our learning pipeline.</p>
<p>The unexpected real-world generalization ability between mobility types warrants further investigation.Training an agent with an actuator abstraction layer allows transfer to disparate, never-before-seen robots.It may be prudent to invest computational resources in training a single model with abstract actuation that can be applied to drones, wheeled robots, walking robots, blimps, etc., rather than training each model individually.</p>
<p>We evaluated NavACL using on-policy reinforcement learning, but NavACL may be useful for selecting which episodes to replay when using off-policy methods.It may also prove useful in selecting and ordering training episodes for imitation learning.</p>
<p>Furthermore, we try to bridge the Sim2Real generalization gap.Note, while DRL visual navigation has proven itself time arXiv:2009.05429v1[cs.RO] 11 Sep 2020 (a) Sax et al.</p>
<p>Fig. 1 .
1
Fig. 1. (1a) The 2019 CVPR Habitat RGB Navigation challenge winner [3] does not learn an effective policy when the collision-free property is enforced (episode termination and negative reward upon collision).Plots represent mean and standard deviation of a single train/test scene over five trials.Collision-free agents are important for Sim2Real transfer, but are much harder to train (SPL defined in eq. 3 -larger is better).(1b top) Top-down view of fully-trained navigation agents exploiting collision mechanics to slide along walls.(1b bottom) Our agents with collision avoidance and automatic curriculum learning.Agent starts at the pink point and move towards the green goal, leaving a blue path.Red dots indicate collisions.</p>
<p>Fig. 3 .
3
Fig. 3. Visualization of fπ(h) estimation of task space across two geometric properties, at various training epochs E.Over time, the task distribution shifts.Adaptive NavACL accounts for this shift.</p>
<p>Algorithm 3 :
3
GetDynamicTask input : Training timestep t; fπ; µ f ; σ f ; Hyperparameters β, γ output : Task h taskT ype ← GetTaskType(t); while true do h ← RandomTask() ; switch taskT ype do case easy do if fπ(h) &gt; µ f + βσ f then return h;</p>
<p>Fig. 4 .
4
Fig. 4. (4a) Validation episode success rate over three trials on a single test-train environment.(4b) As the policy improves over time, NavACL increases the distance from start to goal -ratcheting up the task difficulty.</p>
<p>Fig. 5 .
5
Fig. 5. (5a) Training and validation results of our model (5b) As the policy improves, NavACL produces harder tasks.At the beginning, shorter paths (geodesic distance) and narrow corridors (agent clearance) help guide the agent.As the policy improves, the agent navigates without corridor guidance, and even reaches goals near obstacles (goal clearance).</p>
<p>Fig. 6 .
6
Fig. 6.Simulation test scenes, left to right: Cooperstown (40m 2 ), Avonia (60m 2 ), Hometown (61m 2 )</p>
<p>Fig. 7 .
7
Fig. 7. (7a) The AGV navigating to the vase target in the house scene, with several never-before-seen obstacles littering the path to the target.Previous semantic targets (football and pink star ball) are present to emphasize zero-shot semantic navigation capability.The agent turned 360 • (1) to evaluate its options, then took the path between the desk and tent (2), adjusted the trajectory towards the wide-open area in front of the blue tent (3), and rotated 360 • (4).The agent explored the areas surrounding the bike, bookshelves, and the blue tent (5,6,7).Target detection occurred at (8), and the AGV made a beeline for the target (9,10).(7b) While flying down a hallway (1), the UAV notices an empty cubicle (2).It threads the needle, flying between the chair wheels and seat (3).After exploring the cubicle (4,5), it leaves and heads into the adjacent open office without collision (6).</p>
<p>Algorithm 1 :
1
Training loop with f π update input : ∅ output : π π, fπ, µ f , σ f ← Init(); for i ← 0 to numEpochs do tasks, successes, states, actions, rewards ← Rollouts(π, fπ, µ f , σ f ); π ← PPO(π, states, actions, rewards); fπ ← Train(fπ, tasks, successes); randomT asks ← GetRandomTasks(); µ f , σ f ← FitNormal(fπ, randomT asks);
return π;Algorithm 2: Rollouts
input : π, fπ; µ f ; σ f ; output : rollouts for i ← 0 to batchSize do task ← GetDynamicTask(fπ, µ f , σ f ); rollouts[i] ←RunEpisode(π, task); return rollouts;</p>
<p>TABLE II
IIPPO PARAMETERS# of Minibatches1 Learning Rate0.005Clipping Range ( )0.10 Discount Factor (γ)0.99Value Function Coef. (c 1 )0.5 Entropy Coef. (β or c 2 )0.01Timesteps per Update4000 Rollout Workers12Inner-Loop Epochs4 GAE λ0.95</p>
<p>TABLE III SIMULATION
III
RESULTS OF COLLISION-FREE AGENTS OVER TEN
TRIALS ON UNSEEN ENVIRONMENTSPolicySceneSuccess RateSPLµσµσRandomAll0.030.030.02 0.03Cooperstown 0.070.000.06 0.00Avonia0.010.000.01 0.00Hometown0.000.000.00 0.00NavACL Zero-ShotAll0.360.210.17 0.10Cooperstown 0.550.110.25 0.08Avonia0.430.040.20 0.02Hometown0.090.060.06 0.03NavACLAll0.420.190.24 0.09Cooperstown 0.630.140.31 0.09Avonia0.500.060.21 0.04Hometown0.120.080.09 0.06HumanAll0.790.200.56 0.15Cooperstown 0.760.220.54 0.17Avonia0.890.130.63 0.13Hometown0.720.190.51 0.13
Also available at https://www.youtube.com/playlist? list=PLkG_dDkoI9pjPdOGyTec-sSu20pB7iayC</p>
<p>Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation. A Kadian, J Truong, A Gokaslan, A Clegg, E Wijmans, S Lee, M Savva, S Chernova, D Batra, arXiv:1912.063212019ArXiv preprint</p>
<p>Interactive gibson benchmark: A benchmark for interactive navigation in cluttered environments. F Xia, W B Shen, C Li, P Kasimbeg, M E Tchapmi, A Toshev, R Martín-Martín, S Savarese, IEEE Robotics and Automation Letters. 522020</p>
<p>Mid-level visual representations improve generalization and sample efficiency for learning visuomotor policies. A Sax, B Emi, A R Zamir, L J Guibas, S Savarese, J Malik, 2018</p>
<p>Learning to explore using active neural slam. D S Chaplot, D Gandhi, S Gupta, A Gupta, R Salakhutdinov, arXiv:2004.051552020ArXiv preprint</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2019</p>
<p>Learning to navigate in complex environments. P Mirowski, R Pascanu, F Viola, H Soyer, A J Ballard, A Banino, M Denil, R Goroshin, L Sifre, K Kavukcuoglu, arXiv:1611.036732016ArXiv preprint</p>
<p>Deep reinforcement learning with successor features for navigation across similar environments. J Zhang, J T Springenberg, J Boedecker, W Burgard, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2017</p>
<p>Learning to visually navigate in photorealistic environments without any supervision. L Mezghani, S Sukhbaatar, A Szlam, A Joulin, P Bojanowski, arXiv:2004.049542020ArXiv preprint</p>
<p>Bayesian relational memory for semantic visual navigation. Y Wu, Y Wu, A Tamar, S Russell, G Gkioxari, Y Tian, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2019</p>
<p>Gibson env: Real-world perception for embodied agents. F Xia, A R Zamir, Z.-Y He, A Sax, J Malik, S Savarese, Computer Vision and Pattern Recognition (CVPR). IEEE2018. 2018</p>
<p>J Straub, T Whelan, L Ma, Y Chen, E Wijmans, S Green, J J Engel, R Mur-Artal, C Ren, S Verma, arXiv:1906.05797The replica dataset: A digital replica of indoor spaces. 2019ArXiv preprint</p>
<p>Matterport3d: Learning from rgb-d data in indoor environments. A Chang, A Dai, T A Funkhouser, M Halber, M Niebner, M Savva, S Song, A Zeng, Y Zhang, 7th IEEE International Conference on 3D Vision. 3DV Institute of Electrical and Electronics Engineers Inc2018</p>
<p>The adobeindoornav dataset: Towards deep reinforcement learning based real-world indoor robot visual navigation. K Mo, H Li, Z Lin, J.-Y Lee, arXiv:1802.088242018ArXiv preprint</p>
<p>Minos: Multimodal indoor simulator for navigation in complex environments. M Savva, A X Chang, A Dosovitskiy, T Funkhouser, V Koltun, arXiv:1712.039312017ArXiv preprint</p>
<p>Active vision. J Aloimonos, I Weiss, A Bandyopadhyay, International journal of computer vision. 141988</p>
<p>Active slam in structured environments. C Leung, S Huang, G Dissanayake, 2008 IEEE International conference on Robotics and Automation. IEEE2008</p>
<p>Probabilistic roadmaps for path planning in highdimensional configuration spaces. L E Kavraki, P Svestka, J.-C Latombe, M H , IEEE transactions on Robotics and Automation. 1241996Overmars</p>
<p>Rapidly-exploring random trees: A new tool for path planning. S M Lavalle, 1998</p>
<p>A frontier-based approach for autonomous exploration. B Yamauchi, Proceedings 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97.'Towards New Computational Principles for Robotics and Automation. 1997 IEEE International Symposium on Computational Intelligence in Robotics and Automation CIRA'97.'Towards New Computational Principles for Robotics and AutomationIEEE1997</p>
<p>Playing atari with deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, NIPS Deep Learning Workshop. 2013</p>
<p>Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. E Wijmans, A Kadian, A Morcos, S Lee, I Essa, D Parikh, M Savva, D Batra, ICLR2020</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Y Zhu, R Mottaghi, E Kolve, J J Lim, A Gupta, L Fei-Fei, A Farhadi, 2017 IEEE international conference on robotics and automation (ICRA). IEEE2017</p>
<p>Microsoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Dollár, C L Zitnick, European conference on computer vision. Springer2014</p>
<p>Splitnet: Sim2sim and task2task transfer for embodied visual navigation. D Gordon, A Kadian, D Parikh, J Hoffman, D Batra, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision2019</p>
<p>Cad2rl: Real single-image flight without a single real image. F Sadeghi, S Levine, arXiv:1611.042012016ArXiv preprint</p>
<p>Curriculum learning. Y Bengio, J Louradour, R Collobert, J Weston, Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learning2009</p>
<p>Automatic curriculum learning for deep rl: A short survey. R Portelas, C Colas, L Weng, K Hofmann, P.-Y Oudeyer, arXiv:2003.046642020ArXiv preprint</p>
<p>Intrinsic motivation and automatic curricula via asymmetric self-play. S Sukhbaatar, Z Lin, I Kostrikov, G Synnaeve, A Szlam, R Fergus, ICLR 2018 Through 03-05-2018English (US), 6th International Conference on Learning Representations. Jan. 2018</p>
<p>Automatic goal generation for reinforcement learning agents. C Florensa, D Held, X Geng, P Abbeel, International Conference on Machine Learning. 2018</p>
<p>Selective experience replay for lifelong learning. D Isele, A Cosgun, Thirty-second AAAI conference on artificial intelligence. 2018</p>
<p>Experience replay for continual learning. D Rolnick, A Ahuja, J Schwarz, T Lillicrap, G Wayne, Advances in Neural Information Processing Systems. 2019</p>
<p>Memory trace replay: The shaping of memory consolidation by neuromodulation. L A Atherton, D Dupret, J R Mellor, Trends in neurosciences. 3892015</p>
<p>World models. D Ha, J Schmidhuber, arXiv:1803.101222018ArXiv preprint</p>
<p>Deep spatial autoencoders for visuomotor learning. C Finn, Xin Yu Tan, Yan Duan, T Darrell, S Levine, P Abbeel, 2016 IEEE International Conference on Robotics and Automation (ICRA). 2016</p>
<p>. Y Wu, A Kirillov, F Massa, W.-Y Lo, R Girshick, 20192</p>
<p>Proximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.063472017ArXiv preprint</p>
<p>A Hill, A Raffin, M Ernestus, A Gleave, A Kanervisto, R Traore, P Dhariwal, C Hesse, O Klimov, A Nichol, M Plappert, A Radford, J Schulman, S Sidor, Y Wu, Stable baselines. 2018</p>
<p>Long short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 981997</p>
<p>Reinforcement learning: An introduction. R S Sutton, A G Barto, 2018MIT press</p>
<p>On evaluation of embodied navigation agents. P Anderson, A Chang, D S Chaplot, A Dosovitskiy, S Gupta, V Koltun, J Kosecka, J Malik, R Mottaghi, M Savva, arXiv:1807.067572018ArXiv preprint</p>
<p>Cognitive mapping and planning for visual navigation. S Gupta, J Davidson, S Levine, R Sukthankar, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition2017</p>
<p>Unifying map and landmark based representations for visual navigation. S Gupta, D Fouhey, S Levine, J Malik, arXiv:1712.081252017ArXiv preprint</p>
<p>Semiparametric topological memory for navigation. N Savinov, A Dosovitskiy, V Koltun, arXiv:1803.006532018ArXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6231 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6231</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6231</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-7ad0e4803dcf7232e33ea20842062aba99a5ddfb</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7ad0e4803dcf7232e33ea20842062aba99a5ddfb" target="_blank">SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> This work introduces SEAHORSE, a dataset for multilingual, multifaceted summarization evaluation, and shows that metrics trained with SEAHorSE achieve strong performance on the out-of-domain meta-evaluation benchmarks TRUE (Honovich et al., 2022) and mFACE (Aharoni et al, 2022).</p>
                <p><strong>Paper Abstract:</strong> Reliable automatic evaluation of summarization systems is challenging due to the multifaceted and subjective nature of the task. This is especially the case for languages other than English, where human evaluations are scarce. In this work, we introduce SEAHORSE, a dataset for multilingual, multifaceted summarization evaluation. SEAHORSE consists of 96K summaries with human ratings along 6 dimensions of text quality: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness, covering 6 languages, 9 systems and 4 datasets. As a result of its size and scope, SEAHORSE can serve both as a benchmark to evaluate learnt metrics, as well as a large-scale resource for training such metrics. We show that metrics trained with SEAHORSE achieve strong performance on the out-of-domain meta-evaluation benchmarks TRUE (Honovich et al., 2022) and mFACE (Aharoni et al., 2022). We make the SEAHORSE dataset and metrics publicly available for future research on multilingual and multifaceted summarization evaluation.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6231.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6231.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mt5_SEAHORSE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mT5_xxl finetuned on SEAHORSE (per-dimension evaluation metrics)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of learned, reference-free evaluation metrics created by fine-tuning mT5_xxl to predict human Yes/No ratings on six summarization quality dimensions (Q1–Q6). Each metric outputs a binary prediction ('0'/'1') for a given article+summary pair and was evaluated against human annotations using Pearson correlation and ROC AUC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multilingual summarization evaluation (six quality facets: comprehensibility, repetition, grammar, attribution, main ideas, conciseness)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>mT5_xxl fine-tuned on SEAHORSE (referred to as mt5_SEAHORSE; separate model per quality dimension)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Paid, full-time trained annotators (bilingual for non-English languages) rated summaries on six binary questions (Yes/No/Unsure) after training on a 109-example gold set; most items had one annotation, a subset (8,920) had additional duplicate annotations yielding ~82% pairwise agreement; Krippendorff's alpha reported per question (Q1–Q6).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pearson correlation (ρ) between metric score and human ratings, and ROC AUC for binary predictions; additional analyses include percent-Yes rates per model/question, pairwise annotator agreement, and Krippendorff's α.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>mt5_SEAHORSE substantially outperforms baselines (majority class, ROUGE-L) at predicting human ratings across SEAHORSE (example: Q1 ρ=0.52, AUC=0.90; Q2 ρ=0.86, AUC=0.98; Q4 ρ=0.59, AUC=0.85). It generalizes zero-shot to out-of-domain multilingual benchmarks (mFACE) and to other tasks in the TRUE benchmark (especially summarization datasets), often matching or exceeding baselines. Qualitatively, mt5_SEAHORSE matches human judgments in examples where lexical overlap metrics (ROUGE-L) fail.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Metrics trained from mT5 are: (1) primarily trained on summarization data (SEAHORSE) so performance can degrade on distributions very different from training data; (2) limited to six languages included in SEAHORSE (language coverage limitation); (3) trained/tested on binary Yes/No labels (loss of fine-grained score information); (4) not extensively optimized (no thorough architecture/hyperparameter search reported); (5) dependent on noisy/subjective human labels—lower agreement on Q4–Q6 reduces the signal quality for those dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>The paper highlights examples where ROUGE-L assigns misleadingly high scores while humans and mt5_SEAHORSE mark the summary as poor (e.g., a summary with unsupported first sentence and repetition: human 0, mt5_SEAHORSE 0.15, ROUGE-L 33.59). Conversely, NLI-trained models (t5_NLI) outperform mt5_SEAHORSE on tasks whose training data they include (e.g., FEVER, VitaminC, PAWS), showing domain-specific failure to generalize when training distributions differ. Also, lower annotator agreement (and low Krippendorff's α for some questions, notably grammar/Q3) indicates instability in the human signal that can lead to metric/human divergence.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Recommendations and methods used in the paper: (1) train metrics on large, diverse human-annotated datasets (SEAHORSE) across languages and quality dimensions to improve generalization; (2) keep train/dev/test splits disjoint from datasets' test sets (train on validation-split examples) to avoid test-set contamination/overfitting; (3) use task-specific training data for dimensions where applicable (e.g., NLI data for attribution tasks); (4) train separate metrics per quality dimension; (5) rigorous annotator training with gold examples and qualification procedures to improve label quality; (6) future suggestions: expand language coverage (low-resource languages), perform thorough model/architecture/hyperparameter searches, and combine multiple supervision sources (summarization annotations + NLI) to improve robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6231.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6231.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>t5_NLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>T5_xxl finetuned on a mixture of NLI datasets (NLI-based attribution baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An English NLI-trained T5_xxl model used as a baseline attribution/faithfulness metric; trained on SNLI, MNLI, Fever, SciTail, PAWS, VitaminC and other NLI-style datasets and applied to detect attribution/faithfulness in generated text.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>TRUE: Re-evaluating factual consistency evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Attribution / factual consistency evaluation across summarization, dialogue, and fact verification datasets</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>T5_xxl fine-tuned on a mixture of NLI datasets (referred to as t5_NLI)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared to human annotations in the TRUE benchmark tasks (multiple datasets spanning summarization, dialogue, fact verification); human annotation protocols vary per source dataset but serve as ground truth for attribution labels.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>ROC AUC primarily (TRUE benchmark) and task-wise comparisons showing where NLI supervision helps; compared against mt5_SEAHORSE and ROUGE-L.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>t5_NLI achieves especially strong performance on datasets included in its training mixture (FEVER, VitaminC, PAWS — marked with *), often outperforming mt5_SEAHORSE on those specific tasks; however, mt5_SEAHORSE outperforms t5_NLI on many summarization datasets in TRUE, demonstrating complementary strengths depending on training data.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Being trained primarily on English NLI datasets limits multilingual applicability; performance is heavily dependent on overlap between target task distribution and the NLI training corpora, so it may fail to generalize to summarization or dialogue distributions not covered by its training data.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>t5_NLI performs worse than summarization-trained mt5_SEAHORSE on several summarization datasets in TRUE and on out-of-domain summarization settings; the paper does not list a specific sentence-level failure example for t5_NLI but reports the pattern of dataset-specific performance differences.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Use multilingual NLI variants (e.g., mt5_XNLI) to increase cross-lingual coverage; combine NLI supervision with task-specific summarization annotations (train on both) to cover both faithfulness and summarization idiosyncrasies; use SEAHORSE-style diverse multilingual training data for broader generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6231.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6231.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>mt5_XNLI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>mT5_xxl finetuned on XNLI (multilingual NLI baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilingual NLI finetuned metric (mT5_xxl on XNLI) used as an attribution baseline to test multilingual NLI-based approaches for faithfulness evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Multilingual attribution / factual consistency evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>mT5_xxl fine-tuned on XNLI (referred to as mt5_XNLI)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Evaluated against SEAHORSE attribution labels (Q4) and on mFACE/TRUE benchmarks where applicable; human annotation setup is that of the target benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>ROC AUC and Pearson correlation versus human labels; compared to mt5_SEAHORSE and t5_NLI baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>mt5_XNLI provides decent attribution performance in multilingual settings but generally underperforms mt5_SEAHORSE trained directly on SEAHORSE annotations for attribution (e.g., on SEAHORSE Q4 mt5_XNLI roc ≈0.78 vs mt5_SEAHORSE roc ≈0.85).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Although multilingual, NLI-finetuned models may miss summarization-specific signals and still underperform a metric trained directly on human summarization attribution annotations; XNLI's domain may not cover summarization nuances.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>No single example highlighted, but aggregate results show mt5_XNLI trailing SEAHORSE-trained metrics on attribution tasks in the paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Augment multilingual NLI training with summarization-specific human annotations (e.g., combine XNLI with SEAHORSE), or fine-tune further on target-language/target-task data to close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6231.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6231.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ROUGE-L baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ROUGE-L (lexical-overlap baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A traditional n-gram / longest-common-subsequence overlap metric used as a non-LLM baseline; calculated between summary and article (SentencePiece tokenization).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Summarization evaluation (reference-free variant used here between article and summary)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>not an LLM (lexical-overlap metric)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Compared against human labels from SEAHORSE and other benchmarks; used as a baseline to show where neural metrics improve over lexical metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pearson correlation and ROC AUC vs human labels; percent-Yes rates contrasted.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>ROUGE-L shows low or near-zero correlation with human judgments in many cases and often fails to penalize unsupported or unfaithful summaries (examples provided where ROUGE-L gives relatively high/low scores inconsistent with human judgment and mt5_SEAHORSE).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Not an LLM, but limitation is lexical-only signal: cannot capture paraphrase, attribution/faithfulness beyond overlap, or many summarization quality facets.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Examples in the paper (Figure 4) show ROUGE-L assigning high values to degraded summaries (e.g., repetition or unsupported content) while human raters and mt5_SEAHORSE correctly judge them as bad; ROUGE also rates some concise, correct rephrasings low.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Replace or supplement ROUGE with learned, reference-free metrics trained on human annotations (e.g., mt5_SEAHORSE) and task-specific supervisory signals (NLI for attribution).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>TRUE: Re-evaluating factual consistency evaluation <em>(Rating: 2)</em></li>
                <li>Multilingual summarization with factual consistency evaluation <em>(Rating: 2)</em></li>
                <li>G-Eval: NLG evaluation using GPT-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>TrueTeacher: Learning factual consistency evaluation with large language models <em>(Rating: 1)</em></li>
                <li>FRANK: A benchmark for factuality metrics <em>(Rating: 1)</em></li>
                <li>SummEval: Re-evaluating summarization evaluation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6231",
    "paper_id": "paper-7ad0e4803dcf7232e33ea20842062aba99a5ddfb",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "mt5_SEAHORSE",
            "name_full": "mT5_xxl finetuned on SEAHORSE (per-dimension evaluation metrics)",
            "brief_description": "A set of learned, reference-free evaluation metrics created by fine-tuning mT5_xxl to predict human Yes/No ratings on six summarization quality dimensions (Q1–Q6). Each metric outputs a binary prediction ('0'/'1') for a given article+summary pair and was evaluated against human annotations using Pearson correlation and ROC AUC.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Multilingual summarization evaluation (six quality facets: comprehensibility, repetition, grammar, attribution, main ideas, conciseness)",
            "llm_judge_model": "mT5_xxl fine-tuned on SEAHORSE (referred to as mt5_SEAHORSE; separate model per quality dimension)",
            "human_evaluation_setup": "Paid, full-time trained annotators (bilingual for non-English languages) rated summaries on six binary questions (Yes/No/Unsure) after training on a 109-example gold set; most items had one annotation, a subset (8,920) had additional duplicate annotations yielding ~82% pairwise agreement; Krippendorff's alpha reported per question (Q1–Q6).",
            "metrics_compared": "Pearson correlation (ρ) between metric score and human ratings, and ROC AUC for binary predictions; additional analyses include percent-Yes rates per model/question, pairwise annotator agreement, and Krippendorff's α.",
            "reported_differences": "mt5_SEAHORSE substantially outperforms baselines (majority class, ROUGE-L) at predicting human ratings across SEAHORSE (example: Q1 ρ=0.52, AUC=0.90; Q2 ρ=0.86, AUC=0.98; Q4 ρ=0.59, AUC=0.85). It generalizes zero-shot to out-of-domain multilingual benchmarks (mFACE) and to other tasks in the TRUE benchmark (especially summarization datasets), often matching or exceeding baselines. Qualitatively, mt5_SEAHORSE matches human judgments in examples where lexical overlap metrics (ROUGE-L) fail.",
            "llm_specific_limitations": "Metrics trained from mT5 are: (1) primarily trained on summarization data (SEAHORSE) so performance can degrade on distributions very different from training data; (2) limited to six languages included in SEAHORSE (language coverage limitation); (3) trained/tested on binary Yes/No labels (loss of fine-grained score information); (4) not extensively optimized (no thorough architecture/hyperparameter search reported); (5) dependent on noisy/subjective human labels—lower agreement on Q4–Q6 reduces the signal quality for those dimensions.",
            "notable_failure_cases": "The paper highlights examples where ROUGE-L assigns misleadingly high scores while humans and mt5_SEAHORSE mark the summary as poor (e.g., a summary with unsupported first sentence and repetition: human 0, mt5_SEAHORSE 0.15, ROUGE-L 33.59). Conversely, NLI-trained models (t5_NLI) outperform mt5_SEAHORSE on tasks whose training data they include (e.g., FEVER, VitaminC, PAWS), showing domain-specific failure to generalize when training distributions differ. Also, lower annotator agreement (and low Krippendorff's α for some questions, notably grammar/Q3) indicates instability in the human signal that can lead to metric/human divergence.",
            "mitigation_strategies": "Recommendations and methods used in the paper: (1) train metrics on large, diverse human-annotated datasets (SEAHORSE) across languages and quality dimensions to improve generalization; (2) keep train/dev/test splits disjoint from datasets' test sets (train on validation-split examples) to avoid test-set contamination/overfitting; (3) use task-specific training data for dimensions where applicable (e.g., NLI data for attribution tasks); (4) train separate metrics per quality dimension; (5) rigorous annotator training with gold examples and qualification procedures to improve label quality; (6) future suggestions: expand language coverage (low-resource languages), perform thorough model/architecture/hyperparameter searches, and combine multiple supervision sources (summarization annotations + NLI) to improve robustness.",
            "uuid": "e6231.0",
            "source_info": {
                "paper_title": "SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "t5_NLI",
            "name_full": "T5_xxl finetuned on a mixture of NLI datasets (NLI-based attribution baseline)",
            "brief_description": "An English NLI-trained T5_xxl model used as a baseline attribution/faithfulness metric; trained on SNLI, MNLI, Fever, SciTail, PAWS, VitaminC and other NLI-style datasets and applied to detect attribution/faithfulness in generated text.",
            "citation_title": "TRUE: Re-evaluating factual consistency evaluation",
            "mention_or_use": "use",
            "task_domain": "Attribution / factual consistency evaluation across summarization, dialogue, and fact verification datasets",
            "llm_judge_model": "T5_xxl fine-tuned on a mixture of NLI datasets (referred to as t5_NLI)",
            "human_evaluation_setup": "Compared to human annotations in the TRUE benchmark tasks (multiple datasets spanning summarization, dialogue, fact verification); human annotation protocols vary per source dataset but serve as ground truth for attribution labels.",
            "metrics_compared": "ROC AUC primarily (TRUE benchmark) and task-wise comparisons showing where NLI supervision helps; compared against mt5_SEAHORSE and ROUGE-L.",
            "reported_differences": "t5_NLI achieves especially strong performance on datasets included in its training mixture (FEVER, VitaminC, PAWS — marked with *), often outperforming mt5_SEAHORSE on those specific tasks; however, mt5_SEAHORSE outperforms t5_NLI on many summarization datasets in TRUE, demonstrating complementary strengths depending on training data.",
            "llm_specific_limitations": "Being trained primarily on English NLI datasets limits multilingual applicability; performance is heavily dependent on overlap between target task distribution and the NLI training corpora, so it may fail to generalize to summarization or dialogue distributions not covered by its training data.",
            "notable_failure_cases": "t5_NLI performs worse than summarization-trained mt5_SEAHORSE on several summarization datasets in TRUE and on out-of-domain summarization settings; the paper does not list a specific sentence-level failure example for t5_NLI but reports the pattern of dataset-specific performance differences.",
            "mitigation_strategies": "Use multilingual NLI variants (e.g., mt5_XNLI) to increase cross-lingual coverage; combine NLI supervision with task-specific summarization annotations (train on both) to cover both faithfulness and summarization idiosyncrasies; use SEAHORSE-style diverse multilingual training data for broader generalization.",
            "uuid": "e6231.1",
            "source_info": {
                "paper_title": "SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "mt5_XNLI",
            "name_full": "mT5_xxl finetuned on XNLI (multilingual NLI baseline)",
            "brief_description": "A multilingual NLI finetuned metric (mT5_xxl on XNLI) used as an attribution baseline to test multilingual NLI-based approaches for faithfulness evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Multilingual attribution / factual consistency evaluation",
            "llm_judge_model": "mT5_xxl fine-tuned on XNLI (referred to as mt5_XNLI)",
            "human_evaluation_setup": "Evaluated against SEAHORSE attribution labels (Q4) and on mFACE/TRUE benchmarks where applicable; human annotation setup is that of the target benchmarks.",
            "metrics_compared": "ROC AUC and Pearson correlation versus human labels; compared to mt5_SEAHORSE and t5_NLI baselines.",
            "reported_differences": "mt5_XNLI provides decent attribution performance in multilingual settings but generally underperforms mt5_SEAHORSE trained directly on SEAHORSE annotations for attribution (e.g., on SEAHORSE Q4 mt5_XNLI roc ≈0.78 vs mt5_SEAHORSE roc ≈0.85).",
            "llm_specific_limitations": "Although multilingual, NLI-finetuned models may miss summarization-specific signals and still underperform a metric trained directly on human summarization attribution annotations; XNLI's domain may not cover summarization nuances.",
            "notable_failure_cases": "No single example highlighted, but aggregate results show mt5_XNLI trailing SEAHORSE-trained metrics on attribution tasks in the paper's evaluations.",
            "mitigation_strategies": "Augment multilingual NLI training with summarization-specific human annotations (e.g., combine XNLI with SEAHORSE), or fine-tune further on target-language/target-task data to close the gap.",
            "uuid": "e6231.2",
            "source_info": {
                "paper_title": "SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "ROUGE-L baseline",
            "name_full": "ROUGE-L (lexical-overlap baseline)",
            "brief_description": "A traditional n-gram / longest-common-subsequence overlap metric used as a non-LLM baseline; calculated between summary and article (SentencePiece tokenization).",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Summarization evaluation (reference-free variant used here between article and summary)",
            "llm_judge_model": "not an LLM (lexical-overlap metric)",
            "human_evaluation_setup": "Compared against human labels from SEAHORSE and other benchmarks; used as a baseline to show where neural metrics improve over lexical metrics.",
            "metrics_compared": "Pearson correlation and ROC AUC vs human labels; percent-Yes rates contrasted.",
            "reported_differences": "ROUGE-L shows low or near-zero correlation with human judgments in many cases and often fails to penalize unsupported or unfaithful summaries (examples provided where ROUGE-L gives relatively high/low scores inconsistent with human judgment and mt5_SEAHORSE).",
            "llm_specific_limitations": "Not an LLM, but limitation is lexical-only signal: cannot capture paraphrase, attribution/faithfulness beyond overlap, or many summarization quality facets.",
            "notable_failure_cases": "Examples in the paper (Figure 4) show ROUGE-L assigning high values to degraded summaries (e.g., repetition or unsupported content) while human raters and mt5_SEAHORSE correctly judge them as bad; ROUGE also rates some concise, correct rephrasings low.",
            "mitigation_strategies": "Replace or supplement ROUGE with learned, reference-free metrics trained on human annotations (e.g., mt5_SEAHORSE) and task-specific supervisory signals (NLI for attribution).",
            "uuid": "e6231.3",
            "source_info": {
                "paper_title": "SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "TRUE: Re-evaluating factual consistency evaluation",
            "rating": 2
        },
        {
            "paper_title": "Multilingual summarization with factual consistency evaluation",
            "rating": 2
        },
        {
            "paper_title": "G-Eval: NLG evaluation using GPT-4 with better human alignment",
            "rating": 2
        },
        {
            "paper_title": "TrueTeacher: Learning factual consistency evaluation with large language models",
            "rating": 1
        },
        {
            "paper_title": "FRANK: A benchmark for factuality metrics",
            "rating": 1
        },
        {
            "paper_title": "SummEval: Re-evaluating summarization evaluation",
            "rating": 1
        }
    ],
    "cost": 0.0166785,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>SEAHORSE: A Multilingual, Multifaceted Dataset for Summarization Evaluation</h1>
<p>Elizabeth Clark ${ }^{1}$ Shruti Rijhwani ${ }^{1}$ Sebastian Gehrmann ${ }^{2}$ Joshua Maynez ${ }^{1}$<br>Roee Aharoni ${ }^{2}$ Vitaly Nikolaev ${ }^{1}$ Thibault Sellam ${ }^{1}$ Aditya Siddhant ${ }^{1}$<br>Dipanjan Das ${ }^{1}$ Ankur P. Parikh ${ }^{1}$<br>${ }^{1}$ Google DeepMind ${ }^{2}$ Google Research<br>Contact: eaclark@google.com</p>
<h4>Abstract</h4>
<p>Reliable automatic evaluation of summarization systems is challenging due to the multifaceted and subjective nature of the task. This is especially the case for languages other than English, where human evaluations are scarce. In this work, we introduce SEAHORSE, a dataset for multilingual, multifaceted summarization evaluation. SEAHORSE consists of 96 K summaries with human ratings along 6 dimensions of text quality: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness. SEAHORSE covers 6 languages, 9 systems (including the reference text), and 4 summarization datasets. As a result of its size and scope, SEAHORSE can serve both as a benchmark to evaluate learnt metrics, as well as a large-scale resource for training such metrics. We show that metrics trained with SEAHORSE achieve strong performance on two out-ofdomain meta-evaluation benchmarks: TRUE (Honovich et al., 2022) and mFACE (Aharoni et al., 2023). We make the SEAHORSE dataset and metrics publicly available for future research on multilingual and multifaceted summarization evaluation. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Evaluating the quality of generated text is an increasingly difficult problem as large language models produce text of rapidly improving quality (Radford et al., 2019; Ouyang et al., 2022; Chowdhery et al., 2022). In spite of the improvements, such models often generate text that includes hallucinations and other subtle errors (Wiseman et al., 2017; Maynez et al., 2020; Parikh et al., 2020; Ji et al., 2023; Borji, 2023), making reliable evaluation essential for driving progress.</p>
<p>Common n-gram metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) are often not well correlated with human judgments</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Two summaries from the SEAHORSE dataset paired with human ratings for 6 dimensions of quality. In the second summary, the word in bold has a grammatical error in Russian; it uses the wrong aspect. The rater has noted this error, along with several others.
for many natural language generation (NLG) tasks such as machine translation (Kocmi et al., 2021; Freitag et al., 2021a), summarization (Kryscinski et al., 2020), and dialogue (Dziri et al., 2022). Consequently, human evaluation is often necessary to reliably evaluate NLG systems. However, designing human annotation pipelines and obtaining annotations is resource-intensive, time-consuming, and not easily reproducible. Developing more reliable automatic evaluation metrics would make model development faster and more efficient. With this in mind, much recent work has focused on learnt metrics, i.e., neural classification or regression models that aim to directly predict scores that evaluate the quality of generated text (Zhang* et al., 2020; Sellam et al., 2020; Rei et al., 2020; Liu et al., 2023), often trained with human ratings.</p>
<p>As a result, large-scale collections of human evaluations serve two critical roles in NLG metric development: (1) a source of training data for learnt metrics and (2) a meta-evaluation benchmark for the performance of these learnt metrics. The</p>
<p>large potential of such datasets is exemplified by the WMT metrics shared task, ${ }^{2}$ which has enabled rapid development of learnt metrics for machine translation that exhibit considerably higher correlation to human judgment than BLEU (Bojar et al., 2016; Freitag et al., 2021b).</p>
<p>However, outside of machine translation, the existence of such collections of human judgments is limited. Human annotations collected in NLG evaluations are rarely released (Gehrmann et al., 2022), and even when they are, they tend to cover a single language (typically English) and are from a single dataset or task, limiting the robustness of models and metrics trained on these annotations. Moreover, such annotations are often based on the test split of existing datasets (e.g., Fabbri et al., 2021; Aharoni et al., 2023), which can be problematic for training learnt metrics. This is because the primary advantage of reliable automatic evaluation is to help model development, e.g., hyperparameter selection on the validation set; therefore a neural metric trained on test set annotations would, in general, lead to overfitting.</p>
<p>In this work, we propose SEAHORSE, ${ }^{3}$ a largescale dataset for multilingual summarization evaluation. Our dataset consists of 96 K summaries with ratings along 6 quality dimensions: comprehensibility, repetition, grammar, attribution, main ideas, and conciseness, in 6 languages, for 9 systems ( 8 models plus the human-authored reference summaries) across 4 summarization datasets (see examples in Figure 1). The training and validation splits of the dataset come from the validation sets of the original summarization corpora to prevent test set contamination when training metrics. This permits us to train a learnt metric for each quality dimension that can be used for offline model evaluation.</p>
<p>We evaluate the metrics learned from SEAHORSE on the SEAHORSE test set, as well as other existing meta-evaluation benchmarks, such as mFACE (Aharoni et al., 2023) and TRUE (Honovich et al., 2022). Our experiments show that the metrics generalize across datasets, tasks, and languages. For example, we demonstrate that although SEAHORSE includes data in 6 languages, the resulting learnt metrics achieve strong performance on the mFACE benchmark, which consists of 45 languages, exhibiting their zero-shot multi-</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>lingual generalization potential. To summarize, the contributions of this paper are:</p>
<ul>
<li>We conduct a comprehensive, large-scale human evaluation for summarization across six languages, six quality facets, nine systems and four datasets, resulting in over 96 K humanrated summaries. To the best of our knowledge, this is the largest multilingual, multifaceted summarization evaluation resource.</li>
<li>We train a learnt metric for each of the evaluated quality facets, and show that the metrics outperform strong baselines across our in-domain test set and previously published out-of-domain benchmarks, highlighting the quality of the human annotations we collect and the broad utility of our learnt metrics.</li>
<li>We release our dataset and metrics to foster future work on multilingual, multifaceted summarization.</li>
</ul>
<h2>2 The SEAHORSE dataset</h2>
<p>The SEAHORSE dataset consists of 96,645 summaries annotated with human ratings along 6 quality dimensions. In this section, we describe the SEAHORSE dataset, how we generated the summaries, and how we collected the annotations.</p>
<h3>2.1 The summaries</h3>
<p>The examples in SEAHORSE are in 6 languages: German (de), English (en), Spanish (es), Russian (ru), Turkish (tr), and Vietnamese (vi). We chose these languages by considering geographic and typological diversity and the availability of summarization datasets in those languages.</p>
<p>The summaries are based on articles from 4 different datasets in the GEM benchmark (Gehrmann et al., 2021):</p>
<ul>
<li>XSum (Narayan et al., 2018): An English dataset where the task is to generate a onesentence summary of a BBC News article.</li>
<li>XL-Sum (Hasan et al., 2021): Similar to XSum, the goal of this dataset is to generate a single-sentence summary of a BBC news article, but it covers 44 languages excluding English.</li>
<li>MLSum (Scialom et al., 2020): A summarization dataset obtained from online newspapers in 5 languages.</li>
</ul>
<table>
<thead>
<tr>
<th>language</th>
<th>dataset</th>
<th>articles</th>
<th>annotations</th>
</tr>
</thead>
<tbody>
<tr>
<td>de</td>
<td>mlsum</td>
<td>3359</td>
<td>7506</td>
</tr>
<tr>
<td></td>
<td>wikilingua</td>
<td>2999</td>
<td>7085</td>
</tr>
<tr>
<td>en</td>
<td>xsum</td>
<td>894</td>
<td>6651</td>
</tr>
<tr>
<td></td>
<td>xlsum</td>
<td>2433</td>
<td>7884</td>
</tr>
<tr>
<td></td>
<td>wikilingua</td>
<td>2383</td>
<td>7804</td>
</tr>
<tr>
<td>es</td>
<td>xlsum</td>
<td>2231</td>
<td>4890</td>
</tr>
<tr>
<td></td>
<td>mlsum</td>
<td>2235</td>
<td>4857</td>
</tr>
<tr>
<td></td>
<td>wikilingua</td>
<td>2183</td>
<td>5002</td>
</tr>
<tr>
<td>ru</td>
<td>xlsum</td>
<td>3298</td>
<td>7254</td>
</tr>
<tr>
<td></td>
<td>wikilingua</td>
<td>2948</td>
<td>7288</td>
</tr>
<tr>
<td>tr</td>
<td>xlsum</td>
<td>2186</td>
<td>10627</td>
</tr>
<tr>
<td></td>
<td>wikilingua</td>
<td>770</td>
<td>4791</td>
</tr>
<tr>
<td>vi</td>
<td>xlsum</td>
<td>2497</td>
<td>7522</td>
</tr>
<tr>
<td></td>
<td>wikilingua</td>
<td>1951</td>
<td>7484</td>
</tr>
</tbody>
</table>
<p>Table 1: The number of unique articles and the number of annotated summaries collected from each dataset to create SEAHORSE. Each article is summarized by several different summarization systems, which were evaluated by human annotators.</p>
<ul>
<li>WikiLingua <em>Ladhak et al. (2020)</em>: A dataset in 18 languages where the goal is to summarize how-to guides from WikiHow.</li>
</ul>
<p>A breakdown of SEAHORSE across languages and datasets is in Table 1.</p>
<p>For each dataset, we randomly selected articles from their validation splits to comprise the SEAHORSE training and validation sets, and articles from the test splits to make up the SEAHORSE test set. This distinction is important when using the dataset for training evaluation metrics (discussed in §4), because learnt metrics are typically used for model development, and hyperparameter selection is done on the validation set. Using a metric that was trained on test data would lead to overfitting. Our dataset construction ensures that a learnt metric can be trained on SEAHORSE data without concerns of test set leakage.</p>
<p>Next, we generate summaries for each article in the dataset. The summaries come from a subset of 9 different systems, which we will denote as follows:</p>
<ul>
<li>reference: The human-authored summaries associated with each article from the original datasets.</li>
<li>t5_base: The 220M-parameter version of the T5 model <em>Raffel et al. (2020)</em>. (This model is English-only, so we only use it to generate summaries with our en datasets.)</li>
<li>t5_base_250: The t5_base model with an under-trained checkpoint, trained for only 250 steps (en only).</li>
<li>t5_xxl: The 11B-parameter version of T5 (en only).</li>
<li>mt5_small: The 300M-parameter version of mT5 <em>Xue et al. (2021)</em>.</li>
<li>mt5_small_250: The same mt5_small model but using the checkpoint after training 250 steps.</li>
<li>mt5_xxl: The 13B-parameter mT5 model.</li>
<li>palm_1shot: 540B-parameter PaLM model <em>Chowdhery et al. (2022)</em> prompted with one in-domain example.</li>
<li>palm_finetuned: 540B-parameter PaLM model <em>Chowdhery et al. (2022)</em> finetuned on training data for the respective dataset.</li>
</ul>
<p>Our choice of systems covers a range of expected system performances in order to capture a large diversity of system outputs and model error types. For instance, an under-trained small model (mt5_small_250) would likely have different errors than a 1-shot large language model (palm_1shot). Details about how the summaries are generated from these models are in Appendix A.</p>
<h3>2.2 Annotation methodology</h3>
<p>For each summary, we collect annotations along 6 dimensions, also referred to as Q1–6:</p>
<ul>
<li>Q1 comprehensible: The summary can be read and understood by the rater. (If "No," the rest of the questions will be skipped.)</li>
<li>Q2 repetition: The summary is free of unnecessarily repeated information.</li>
<li>Q3 grammar: The summary is grammatically correct.</li>
<li>Q4 attribution: All the information in the summary is fully attributable to the source article, as defined in <em>Rashkin et al. (2021)</em>.</li>
</ul>
<p>Q5 main ideas: The summary captures the main idea(s) of the source article.</p>
<p>Q6 conciseness: The summary concisely represents the information in the source article.</p>
<p>For the first 3 questions, annotators see only the summary. The article is revealed when the raters are answering questions 4-6. They can answer "Yes," "No," or "Unsure" to each question and have the option to leave comments or flag any issues they see in the article. The annotation interface is shown in Figure 2.</p>
<p>Note that our annotation process is referenceless, i.e., the annotator is never comparing a modelgenerated summary with the reference summary. They evaluate each summary on its own. Given the subjectivity of summarization, we believe this approach allows us to adequately reward models that generate relevant summaries that may be different than the reference. Moreover, this enables us to train reference-less metrics in $\S 4$, which have an added benefit of being able to be used at inference time for re-ranking.</p>
<p>The raters are paid, full-time annotators who were trained for this specific task and worked under the supervision of a project manager. For the non-English languages, the raters are bilingual, proficient in both the annotation language and English. They received a detailed set of instructions in English describing the 6 dimensions of quality and positive and negative examples of each in the target language. We created a set of 109 summaries with gold ratings, which we used to train the raters. Each annotator rated 20-30 summaries from this gold set. If the rater performed well on this subset, they were qualified to move forward with the annotation task. Otherwise, the annotator received feedback and were asked to complete another 10-20 ratings. This training process was repeated as needed.</p>
<p>A small number of approved annotators were removed during the annotation process, due to issues flagged by the annotation team and the authors. The ratings from the removed annotators are not included in the dataset.</p>
<h2>3 Dataset analysis</h2>
<p>We first analyze the dataset's composition and the quality of the collected annotations. Table 2 contains the median length of summaries produced by each model, along with two measures of the overlap between the summaries and the source articles.</p>
<table>
<thead>
<tr>
<th>model</th>
<th>length</th>
<th>rouge</th>
<th>$20 \%$ copy</th>
</tr>
</thead>
<tbody>
<tr>
<td>reference</td>
<td>227</td>
<td>20.26</td>
<td>0.00</td>
</tr>
<tr>
<td>t5_base_250</td>
<td>92</td>
<td>20.95</td>
<td>0.00</td>
</tr>
<tr>
<td>t5_base</td>
<td>101</td>
<td>22.02</td>
<td>0.02</td>
</tr>
<tr>
<td>t5_xxl</td>
<td>115</td>
<td>21.65</td>
<td>0.01</td>
</tr>
<tr>
<td>mt5_small_250</td>
<td>128</td>
<td>21.33</td>
<td>0.02</td>
</tr>
<tr>
<td>mt5_small</td>
<td>171</td>
<td>21.81</td>
<td>0.04</td>
</tr>
<tr>
<td>mt5_xxl</td>
<td>194</td>
<td>20.77</td>
<td>0.01</td>
</tr>
<tr>
<td>palm_1shot</td>
<td>254</td>
<td>27.34</td>
<td>0.14</td>
</tr>
<tr>
<td>palm_finetuned</td>
<td>194</td>
<td>20.97</td>
<td>0.01</td>
</tr>
</tbody>
</table>
<p>Table 2: The median number of characters (length), ROUGE-L between the summary and article (rouge), and the proportion of summaries where the first $20 \%$ of the summary exactly matches the beginning of the source article ( $20 \%$ copy) for all the summaries generated by each model.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Q1</th>
<th>Q2</th>
<th>Q3</th>
<th>Q4</th>
<th>Q5</th>
<th>Q6</th>
</tr>
</thead>
<tbody>
<tr>
<td>reference</td>
<td>0.97</td>
<td>0.97</td>
<td>0.91</td>
<td>0.54</td>
<td>0.68</td>
<td>0.43</td>
</tr>
<tr>
<td>t5_base_250</td>
<td>0.97</td>
<td>0.79</td>
<td>0.91</td>
<td>0.41</td>
<td>0.42</td>
<td>0.25</td>
</tr>
<tr>
<td>t5_base</td>
<td>0.98</td>
<td>0.92</td>
<td>0.93</td>
<td>0.59</td>
<td>0.59</td>
<td>0.43</td>
</tr>
<tr>
<td>t5_xxl</td>
<td>0.99</td>
<td>0.97</td>
<td>0.95</td>
<td>0.65</td>
<td>0.67</td>
<td>0.51</td>
</tr>
<tr>
<td>mt5_small_250</td>
<td>0.71</td>
<td>0.43</td>
<td>0.59</td>
<td>0.27</td>
<td>0.19</td>
<td>0.1</td>
</tr>
<tr>
<td>mt5_small</td>
<td>0.86</td>
<td>0.57</td>
<td>0.73</td>
<td>0.36</td>
<td>0.35</td>
<td>0.19</td>
</tr>
<tr>
<td>mt5_xxl</td>
<td>0.96</td>
<td>0.94</td>
<td>0.88</td>
<td>0.55</td>
<td>0.65</td>
<td>0.43</td>
</tr>
<tr>
<td>palm_1shot</td>
<td>0.88</td>
<td>0.85</td>
<td>0.79</td>
<td>0.71</td>
<td>0.57</td>
<td>0.47</td>
</tr>
<tr>
<td>palm_finetuned</td>
<td>0.98</td>
<td>0.98</td>
<td>0.9</td>
<td>0.69</td>
<td>0.71</td>
<td>0.56</td>
</tr>
</tbody>
</table>
<p>Table 3: The percent of "Yes" responses, broken down by model and question.</p>
<p>The 1-shot PaLM model is particularly likely to copy from the article as its output, obtaining the highest ROUGE-L ${ }^{4}$ (Lin, 2004) scores between the summary and the article. In $14 \%$ of cases, the beginning of the 1-shot summaries (the first $20 \%$ of the summary) exactly matched the beginning of the reference article.</p>
<p>Table 3 shows the percent of summaries from each summarization system that received a positive (i.e., "Yes") rating from annotators. While there is variation across models and datasets, most summaries are rated positively for questions 1-3 (comprehensibility, repetition, and grammar). The rate of positive responses drops for questions 4-6 (attribution, main ideas, and conciseness), indicating that these areas remain a challenge for summarization models. A more detailed break down of the positive response rates is in Appendix B.</p>
<p>Note that the reference summaries do not always receive the highest rate of positive responses. The</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup> <sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>: ${ }^{4}$ All ROUGE scores in this paper are calculated with SentencePiece tokens: https://github.com/google/ sentencepiece</p>
<table>
<thead>
<tr>
<th>Context</th>
<th>@xIsum_english-validation-3465</th>
<th>Evaluation Rate the summary</th>
</tr>
</thead>
<tbody>
<tr>
<td>SUMMARY</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: The Pearson correlation between responses for questions 2-6.</p>
<table>
<thead>
<tr>
<th>Lang</th>
<th>Avg</th>
<th>Q1</th>
<th>Q2</th>
<th>Q3</th>
<th>Q4</th>
<th>Q5</th>
<th>Q6</th>
</tr>
</thead>
<tbody>
<tr>
<td>de</td>
<td>0.84</td>
<td>0.97</td>
<td>0.98</td>
<td>0.95</td>
<td>0.81</td>
<td>0.67</td>
<td>0.66</td>
</tr>
<tr>
<td>es</td>
<td>0.82</td>
<td>0.92</td>
<td>0.97</td>
<td>0.83</td>
<td>0.74</td>
<td>0.7</td>
<td>0.74</td>
</tr>
<tr>
<td>en</td>
<td>0.81</td>
<td>0.97</td>
<td>0.94</td>
<td>0.95</td>
<td>0.69</td>
<td>0.61</td>
<td>0.69</td>
</tr>
<tr>
<td>ru</td>
<td>0.82</td>
<td>0.86</td>
<td>0.97</td>
<td>0.88</td>
<td>0.71</td>
<td>0.73</td>
<td>0.76</td>
</tr>
<tr>
<td>tr</td>
<td>0.82</td>
<td>0.93</td>
<td>0.96</td>
<td>0.86</td>
<td>0.74</td>
<td>0.7</td>
<td>0.74</td>
</tr>
<tr>
<td>vi</td>
<td>0.81</td>
<td>0.95</td>
<td>0.98</td>
<td>0.88</td>
<td>0.68</td>
<td>0.66</td>
<td>0.69</td>
</tr>
<tr>
<td>avg</td>
<td>0.82</td>
<td>0.93</td>
<td>0.97</td>
<td>0.89</td>
<td>0.73</td>
<td>0.68</td>
<td>0.72</td>
</tr>
</tbody>
</table>
<p>Table 4: The average pairwise agreement, broken down by language and question.</p>
<p>summary cannot be considered a "concise representation of the information in the article" if it has information that is not in the article (i.e., a "No" response for Q4) or if does not represent the main points in the article (i.e., a "No" response for Q5), which was a detail pointed out to evaluators in the task instructions. Therefore, we expect Q6 to be positively correlated with both of these dimensions if the annotators understood the task and the relationship between the dimensions of quality.</p>
<p>In &gt; 99% of cases when the annotator says a summary is not attributable (Q4) or they say it lacks the main ideas from the article (Q5), they also say it is not concise (Q6). This is also reflected in Figure 3, which shows that the strongest correlation between questions is between questions 4&amp;6 and questions 5&amp;6. These results show the pattern we expect to see in the data given the task definition and instructions, and it demonstrates the annotators' ability to understand and execute the annotation task.</p>
<table>
<thead>
<tr>
<th>Q1</th>
<th>Q2</th>
<th>Q3</th>
<th>Q4</th>
<th>Q5</th>
<th>Q6</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.49</td>
<td>0.87</td>
<td>0.35</td>
<td>0.47</td>
<td>0.4</td>
<td>0.41</td>
</tr>
</tbody>
</table>
<p>Table 5: Krippendorff's α by question.</p>
<p>Annotator agreement While most items in the dataset were annotated once, we collected 2 additional ratings for a subset of the data to compare annotators' scores. Out of 8,920 duplicated annotations, the overall pairwise agreement between raters was 82%. Table 4 breaks down the pairwise accuracy across all languages and questions. Questions 1–3 have higher agreement, while questions 4–6 (which depend on more context and have a higher degree of subjectivity) have lower agreement. A similar trend is reflected in the Krippendorff's α values (Krippendorff, 1980, shown in Table 5), which correct for the probability of random agreement, except grammar (Q3) scores lowest.</p>
<p>These patterns in the annotators' responses are positive indicators about the overall quality of the SEAHORSE ratings. However, the more important test of the dataset's quality is its usefulness for developing evaluation metrics, which we discuss in the next section.</p>
<h2>4 Learning and evaluating metrics with SEAHORSE</h2>
<p>The SEAHORSE dataset is meant to serve both as a source of training data for learnt metrics as well as a meta-evaluation benchmark for these metrics. In this section, we evaluate SEAHORSE on these aspects by looking at how well metrics finetuned with our collected annotations can predict human ratings of generated summaries, both from the SEAHORSE test set and other existing datasets. When training metrics, we use a filtered version of the dataset that removes all duplicates and non-Yes or No ratings (88,280 total items). We divide the annotations into train/dev/test splits, where the summaries in the train and dev sets are based on articles from the original datasets' validation sets. The test set of SEAHORSE contains summaries of the articles in the original datasets' test sets.</p>
<h3>4.1 Metrics</h3>
<p>One way to train a metric using SEAHORSE is to finetune a text-to-text generation model, where the model is trained to take an article and summary as its input and to output the string '0' or '1' as a prediction of the human rating. We finetune mT5_xxl (Xue et al., 2021) with the SEAHORSE training set to do this task, finetuning a separate metric for each dimension of quality. We call this model</p>
<p>$\mathrm{mt} 5_{\text {SEAHORSE }}{ }^{5}$. More details are in Appendix A. Note that our goal is not to train a state-of-the-art metric but rather to evaluate the utility of SEAHORSE as a resource to train and evaluate such metrics.</p>
<p>We compare the performance of mt5 $5_{\text {SEAHORSE }}$ to several baselines:</p>
<ul>
<li>majority_class A majority class baseline (i.e., picking the most frequent class).</li>
<li>ROUGE-L The ROUGE-L score between the article and the summary.</li>
</ul>
<p>Specifically for the attribution (Q4) task, we consider a third baseline approach; attribution is closely related to natural language inference (NLI) (Fyodorov et al., 2000; Dagan et al., 2006), and Honovich et al. (2022) show that models finetuned on NLI data perform well as faithfulness metrics. Therefore we consider two variants of an NLI-based baseline:</p>
<ul>
<li>t5 $5_{\text {NLI }}$ : An English NLI model proposed by Honovich et al. (2022). ${ }^{6}$ T5_xxl is finetuned on the following datasets: SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), Fever (Thorne et al., 2018), Scitail (Khot et al., 2018), PAWS (Zhang et al., 2019), and VitaminC (Schuster et al., 2021).</li>
<li>mt5 $5_{\text {XNLI }}$ : A multilingual version, where mT5_xxl is finetuned on XNLI (Conneau et al., 2018).</li>
</ul>
<p>We note that since we are operating in the reference-free setting, other learnt metrics such as BLEURT (Sellam et al., 2020) or BERTScore (Zhang* et al., 2020) are not applicable since they measure the similarity between the prediction and reference.</p>
<p>We evaluate the SEAHORSE and baseline metrics in two ways: the area under the ROC curve and the correlation (Pearson's $\rho$ ) between the metric and human scores. These measures are not sensitive to a thresholding value and are also used in the work we compare with (Honovich et al., 2022; Aharoni et al., 2023).</p>
<h3>4.2 Evaluation on the SEAHORSE test set</h3>
<p>We first evaluate mt5 $5_{\text {SEAHORSE }}$ on the SEAHORSE test set to confirm that a model is able to learn</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup>to predict the different dimensions of quality in SEAHORSE. The results are shown in Table 6. As expected, we see that the mt5 $5_{\text {SEAHORSE }}$ model is able to predict SEAHORSE ratings better than the baselines according to both our metrics. The repetition (Q2) metric performs the best out of the 6 dimensions, which is also the dimension with the highest pairwise annotator agreement. Examples of summaries paired with human, SEAHORSE, and ROUGE-L ratings can be found in Appendix C.</p>
<p>Reducing the size of the base mT5 model from XXL (13B parameters) to Large (1.2B) drops the performance of the metric, but shows similar trends and still outperforms all baseline approaches. More mt5_L $5_{\text {SEAHORSE }}$ results can be found in Appendix D.</p>
<h3>4.3 Evaluation on the mFACE dataset</h3>
<p>In addition to achieving good performance on the SEAHORSE test set, we would like to evaluate how well models trained on SEAHORSE generalize to other multilingual summarization human evaluation datasets without any further tuning. This would give evidence that improving on SEAHORSE would lead to better evaluation metrics in general.</p>
<p>For this purpose, we choose the mFACE dataset $^{7}$ (Aharoni et al., 2023). mFACE contains human evaluations of the XL-Sum test set, which consists of 45 languages on 3 dimensions: quality, attribution, and informativeness. While their definition of attribution is the same as ours (i.e., following AIS (Rashkin et al., 2021)), their definitions of quality (Is the summary comprehensible?) and informativeness (Is the summary a good summary of the article?) do not line up exactly with a single one of our questions, a misalignment that we expect to occur in practice given the lack of standardization of summarization human evaluation.</p>
<p>As a result, for each mFACE dimension, we use the SEAHORSE metric for the question that is most similar; attribution clearly aligns with Q4, and for quality and informativeness, we consider Q1 and Q6 to be the closest fit, respectively.</p>
<p>We evaluate on both the full mFACE dataset (all languages), as well as the 5-language subset that is common to both mFACE and SEAHORSE (en, es, ru, tr, vi). In addition to our baseline models, we also compare to an "upper-bound" mT5_xxl model that has been directly trained on mFACE data $\left(\mathrm{mt}^{5} \mathrm{MFACE}\right)$.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<table>
<thead>
<tr>
<th></th>
<th>Q1</th>
<th>Q2</th>
<th>Q3</th>
<th>Q4</th>
<th>Q5</th>
<th>Q6</th>
</tr>
</thead>
<tbody>
<tr>
<td>Metric</td>
<td>$\rho$</td>
<td>roc</td>
<td>$\rho$</td>
<td>roc</td>
<td>$\rho$</td>
<td>roc</td>
</tr>
<tr>
<td>majority_class</td>
<td>-</td>
<td>0.5</td>
<td>-</td>
<td>0.5</td>
<td>-</td>
<td>0.5</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>0.04</td>
<td>0.54</td>
<td>0.06</td>
<td>0.54</td>
<td>-0.03</td>
<td>0.43</td>
</tr>
<tr>
<td>mt5 ${ }_{\text {XNLI }}$</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>mt5_L $_{\text {SEAHORSE }}$</td>
<td>0.44</td>
<td>0.88</td>
<td>0.74</td>
<td>0.97</td>
<td>0.37</td>
<td>0.81</td>
</tr>
<tr>
<td>mt5 $_{\text {SEAHORSE }}$</td>
<td>$\mathbf{0 . 5 2}$</td>
<td>$\mathbf{0 . 9 0}$</td>
<td>$\mathbf{0 . 8 6}$</td>
<td>$\mathbf{0 . 9 8}$</td>
<td>$\mathbf{0 . 4 5}$</td>
<td>$\mathbf{0 . 8 4}$</td>
</tr>
</tbody>
</table>
<p>Table 6: Metrics' ability to predict SEAHORSE ratings, measured with Pearson's coefficient ( $\rho$ ) and the area under the ROC curve (roc). mt5_L $_{\text {SEAHORSE }}$ is a finetuned version of mT5_large; the other mt5 metrics finetune mT5_xxl.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">mFACE - 5 languages</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">mFACE - all languages</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Quality</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Attribution</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Informativeness</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Quality</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Attribution</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">Informativeness</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Metric</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">roc</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">roc</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">roc</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">roc</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">roc</td>
<td style="text-align: center;">$\rho$</td>
<td style="text-align: center;">roc</td>
</tr>
<tr>
<td style="text-align: center;">Not trained on mFACE</td>
<td style="text-align: center;">majority_class</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.5</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">ROUGE-L</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">0.14</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mt5 $_{\text {XNLI }}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">mt5 $_{\text {SEAHORSE }}$</td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.50</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.15</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.74</td>
</tr>
<tr>
<td style="text-align: center;">Trained on mFACE</td>
<td style="text-align: center;">mt5 $_{\text {MFACE }}$</td>
<td style="text-align: center;">0.25*</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">0.51*</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.47</td>
<td style="text-align: center;">0.79</td>
<td style="text-align: center;">0.35*</td>
<td style="text-align: center;">0.61</td>
<td style="text-align: center;">0.52*</td>
<td style="text-align: center;">0.82*</td>
<td style="text-align: center;">0.47*</td>
<td style="text-align: center;">0.80*</td>
</tr>
</tbody>
</table>
<p>Table 7: Metrics' ability to predict mFACE ratings, measured with Pearson's coefficient ( $\rho$ ) and the area under the ROC curve (roc). The asterisk indicates that the associated model was trained on the training portion of the mFACE dataset.</p>
<p>Results are shown in Table 7. In all but one column, mt5 $<em _SEAHORSE="{SEAHORSE" _text="\text">{\text {SEAHORSE }}$ outperforms the other methods that were not trained on the mFACE data and also performs well on the languages it was not finetuned on. mt5 $</em>$ even performs comparably to mt5 $}<em _MFACE="{MFACE" _text="\text">{\text {MFACE }}$ on the 5 language subset on all dimensions, and the attribution dimension on the all-language set. mt5 $</em>$ is applied in a zero-shot setting.}}$ performs better on quality and informativeness on the all-language set, as one would expect, since it has seen supervised data from those languages and dimensions whereas mt5 $_{\text {SEAHORSE }</p>
<h3>4.4 Evaluation on the TRUE Benchmark</h3>
<p>Finally, we focus on the attribution dimension of quality, since issues of faithfulness in generated text are increasingly important (Wiseman et al., 2017; Tian et al., 2019; Zhou et al., 2021; Dziri et al., 2022; Ji et al., 2023). The TRUE benchmark (Honovich et al., 2022) consists of several English datasets across summarization, dialogue, verification, and paraphrasing: FRANK (Pagnoni et al., 2021), SummEval (Fabbri et al., 2021), MNBM (Maynez et al., 2020), QAGS (Wang et al., 2020), BEGIN (Dziri et al., 2022), $Q^{2}$ (Honovich et al., 2021), DialFact (Gupta et al., 2022), FEVER (Thorne et al., 2018), VitaminC (Schuster et al., 2021), and PAWS (Zhang et al., 2019).</p>
<p>As in the prior section, we apply mt5 $<em _NLI="{NLI" _text="\text">{\text {SEAHORSE }}$ without any further finetuning to these datasets to assess its ability to evaluate attribution to other datasets and tasks beyond summarization. In ad-
dition to comparing to the majority class and ROUGE-L baselines, we also compare with t5 ${ }</em>$.}</p>
<p>Results are shown in Table 8. mt5 $<em _SEAHORSE="{SEAHORSE" _text="\text">{\text {SEAHORSE }}$ achieves the best results across the summarization datasets, which is expected as many of these datasets consist of XSum and CNN/DailyMail (Hermann et al., 2015), the first of which is also a source of the SEAHORSE summaries and the second is a different news summarization dataset. Interestingly, despite only being trained on summarization data, mt5 $</em>}}$ performs competitively to t5 ${ <em _NLI="{NLI" _text="\text">{\text {NLI }}$ on the dialogue datasets (BEGIN, $Q^{2}$, and DialFact), indicating its suitability for evaluating tasks outside of summarization. t5 ${ }</em>$ model was trained on these datasets.}}$ performs best on the Fever, VitaminC, and PAWS tasks, which is expected given that the t5 $_{\text {NLI }</p>
<h2>5 Related work</h2>
<p>We briefly review other large-scale datasets of human evaluations of summaries that have been released and compare them to SEAHORSE, but note that most focus on annotating the test data, which would lead to test data contamination when training metrics.</p>
<p>SummEval (Fabbri et al., 2021) and RealSumm (Bhandari et al., 2020) are summarization meta-evaluation benchmarks with 12,800 and 7,742 annotations respectively. These benchmarks focus on a single language and single dataset: the CNN/DailyMail English summarization dataset. The RoSE benchmark (Liu et al., 2022) contains</p>
<table>
<thead>
<tr>
<th></th>
<th>FRANK</th>
<th>SummEval</th>
<th>MNBN</th>
<th>QAGS-C</th>
<th>QAGS-X</th>
<th>BEGIN</th>
<th>$Q^{2}$</th>
<th>DialFact</th>
<th>Fever</th>
<th>VitaminC</th>
<th>PAWS</th>
</tr>
</thead>
<tbody>
<tr>
<td>majority_class</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
<td>0.5</td>
</tr>
<tr>
<td>ROUGE-L</td>
<td>0.55</td>
<td>0.57</td>
<td>0.53</td>
<td>0.44</td>
<td>0.55</td>
<td>0.63</td>
<td>0.54</td>
<td>0.49</td>
<td>0.48</td>
<td>0.50</td>
<td>0.60</td>
</tr>
<tr>
<td>mT5SEAHORSE</td>
<td>$\mathbf{0 . 9 4}$</td>
<td>$\mathbf{0 . 8 7}$</td>
<td>$\mathbf{0 . 8 3}$</td>
<td>$\mathbf{0 . 9 1}$</td>
<td>$\mathbf{0 . 8 7}$</td>
<td>0.84</td>
<td>0.82</td>
<td>0.87</td>
<td>$\mathbf{0 . 9 1}$</td>
<td>$\mathbf{0 . 7 8}$</td>
<td>$\mathbf{0 . 8 2}$</td>
</tr>
<tr>
<td>T5 $5_{\text {NLI }}$</td>
<td>0.90</td>
<td>0.79</td>
<td>0.76</td>
<td>0.77</td>
<td>0.85</td>
<td>$\mathbf{0 . 8 5}$</td>
<td>$\mathbf{0 . 8 3}$</td>
<td>$\mathbf{0 . 9 2}$</td>
<td>$\mathbf{0 . 9 5 *}$</td>
<td>$\mathbf{0 . 9 8 *}$</td>
<td>$\mathbf{0 . 9 9 *}$</td>
</tr>
</tbody>
</table>
<p>Table 8: Metrics' performance on the TRUE benchmark, measured with area under the ROC curve. $\mathrm{t} 5_{\text {NLI }}$ is a T5-xxl model trained on a mixture of NLI datasets that includes the FEVER, VitaminC, and PAWS training sets (and thus those numbers are indicated with an asterisk).</p>
<p>22 K summary-level annotations across 3 summarization datasets, including a subset from the CNN/DailyMail validation set, and Stiennon et al. (2020) released 65K summary comparisons on the TL;DR dataset (Völske et al., 2017); however, both only consider English summarization tasks. Rashkin et al. (2021) focus on attribution, releasing $\sim 4.5 \mathrm{~K}$ annotations from English summarization, table-to-text, and dialogue datasets; Gekhman et al. (2023) also release attribution annotations for 1.4 M summaries, but the labels are machine-generated rather than human-annotated. GENIE (Khashabi et al., 2022) released 17 K human evaluations across 5 tasks that includes one English summarization task (XSum).</p>
<p>The only other multilingual summarization evaluation dataset, to the best of our knowledge, is mFACE (Aharoni et al., 2023), which has annotations for 31,500 summaries covering a broader set of languages ( 45 languages). mFACE focuses on one dataset (XL-Sum) and a smaller set of models than SEAHORSE. In $\S 4$ we use mFACE as a comprehensive out-of-domain evaluation set, and view it as complementary to SEAHORSE, which aims to provide large-scale and diverse training data for metrics.</p>
<h2>6 Conclusion</h2>
<p>In this work, we present SEAHORSE, a large-scale multilingual, multifaceted dataset for summarization consisting of 96 K human annotations of summaries. Due to its size and scope, SEAHORSE enables the training and evaluation of learnt metrics across several quality dimensions. Our results show that SEAHORSE-trained metrics not only achieve strong performance on our own test set but also generalize to other external and out-of-domain benchmarks: mFACE and TRUE. In the future, we are interested in exploring how SEAHORSE can be used more directly to improve the quality of summarization models and metrics, and hope this paper and the public release of SEAHORSE enables further research on these topics.</p>
<h2>Limitations</h2>
<p>The summaries in this work are in 6 languages, and the selection of these languages was based on the number of datasets and articles available for each language. We would like future work to explore the incorporation of low-resource languages, perhaps with the use of crosslingual and fewshot summarization systems. While the raters we worked with in this project went through several rounds of instructions and training, there is a degree of subjectivity inherent in the 6 text quality evaluation tasks and human ratings are noisy, as each individual rater may interpret and rate qualities slightly differently. Finally, the mT5-based metrics presented in this work primarily serve as a demonstration of the potential of the SEAHORSE data for developing summarization metrics; they have not optimized via thorough hyperparameter search, comparing different modeling architectures or approaches, etc. We hope the dataset and experimental results will provide a starting point for this type of exploration in the future.</p>
<h2>Ethics Statement</h2>
<p>This work relies on the efforts of human evaluators, who were compensated for their work. The summaries in this work are machine-generated and should not be treated as truth; they may contain misleading or incorrect information. None of the human ratings capture this dimension of the text, as our quality dimensions focus on the relationship between the summary and the source article, not a broader set of information or perspectives. For example, if an article contains a factual error, a summary that contains the same error should be rated as "Yes" for Q4 (attribution) because it is consistent with the article. We used summarization models of varying quality in this work, but all are imperfect and their output should be treated with caution.</p>
<h2>Acknowledgements</h2>
<p>We would like to thank Ashwin Kakarla and his team for help with the annotations, as well as Slav Petrov, Hannah Rashkin, and our EMNLP reviewers for their feedback on the paper.</p>
<h2>References</h2>
<p>Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, and Mirella Lapata. 2023. Multilingual summarization with factual consistency evaluation. In Findings of the Association for Computational Linguistics: ACL 2023, pages 3562-3591, Toronto, Canada. Association for Computational Linguistics.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9347-9359, Online. Association for Computational Linguistics.</p>
<p>Ondřej Bojar, Yvette Graham, Amir Kamran, and Miloš Stanojević. 2016. Results of the WMT16 metrics shared task. In Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers, pages 199-231, Berlin, Germany. Association for Computational Linguistics.</p>
<p>Ali Borji. 2023. A categorical archive of chatGPT failures. arXiv preprint arXiv:2302.03494.</p>
<p>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632-642, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason</p>
<p>Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.</p>
<p>Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018. XNLI: Evaluating crosslingual sentence representations. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2475-2485, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, pages 177-190. Springer.</p>
<p>Nouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022. Evaluating attribution in dialogue systems: The BEGIN benchmark. Transactions of the Association for Computational Linguistics, 10:10661083.</p>
<p>Alexander R. Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391-409.</p>
<p>Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a. Experts, errors, and context: A large-scale study of human evaluation for machine translation. Transactions of the Association for Computational Linguistics, 9:1460-1474.</p>
<p>Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b. Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation, pages 733-774, Online. Association for Computational Linguistics.</p>
<p>Yaroslav Fyodorov, Yoad Winter, and Nissim Francez. 2000. A natural logic inference system. In Proceedings of the 2nd Workshop on Inference in Computational Semantics (ICoS-2).</p>
<p>Sebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ondřej Dušek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv</p>
<p>Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, João Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96-120, Online. Association for Computational Linguistics.</p>
<p>Sebastian Gehrmann, Elizabeth Clark, and Thibault Sellam. 2022. Repairing the cracked foundation: A survey of obstacles in evaluation practices for generated text. arXiv preprint arXiv:2202.06935.</p>
<p>Zorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen Elkind, and Idan Szpektor. 2023. TrueTeacher: Learning factual consistency evaluation with large language models. arXiv preprint arXiv:2305.11171.</p>
<p>Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. DialFact: A benchmark for fact-checking in dialogue. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3785-3801, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. XLsum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693-4703, Online. Association for Computational Linguistics.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. Advances in Neural Information Processing Systems, 28.</p>
<p>Or Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. TRUE: Re-evaluating factual consistency evaluation. In Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 161175, Dublin, Ireland. Association for Computational Linguistics.</p>
<p>Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021.
$q^{2}$ : Evaluating factual consistency in knowledgegrounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856-7870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.</p>
<p>Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language generation. ACM Computing Surveys, 55(12):1-38.</p>
<p>Daniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui Qin, Kyle Richardson, Sean Welleck, Hannaneh Hajishirzi, Tushar Khot, Ashish Sabharwal, Sameer Singh, and Yejin Choi. 2022. Prompt waywardness: The curious case of discretized interpretation of continuous prompts. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3631-3643, Seattle, United States. Association for Computational Linguistics.</p>
<p>Tushar Khot, Ashish Sabharwal, and Peter Clark. 2018. ScitaiL: A textual entailment dataset from science question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.</p>
<p>Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478-494, Online. Association for Computational Linguistics.</p>
<p>Klaus Krippendorff. 1980. Content analysis: An introduction to its methodology.</p>
<p>Wojciech Kryscinski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332-9346, Online. Association for Computational Linguistics.</p>
<p>Faisal Ladhak, Esin Durmus, Claire Cardie, and Kathleen McKeown. 2020. WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4034-4048, Online. Association for Computational Linguistics.</p>
<p>Chin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74-81, Barcelona, Spain. Association for Computational Linguistics.</p>
<p>Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-Eval: NLG evaluation using GPT-4 with better human alignment. arXiv preprint arXiv:2303.16634.</p>
<p>Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, and Dragomir Radev. 2022. Revisiting the gold standard: Grounding summarization evaluation with robust human evaluation. arXiv preprint arXiv:2212.07981.</p>
<p>Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906-1919, Online. Association for Computational Linguistics.</p>
<p>Shashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797-1807, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.</p>
<p>Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812-4829, Online. Association for Computational Linguistics.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.</p>
<p>Ankur Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-to-text generation dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1173-1186, Online. Association for Computational Linguistics.</p>
<p>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research, 21(140):1-67.</p>
<p>Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2021. Measuring attribution in natural language generation models. arXiv preprint arXiv:2112.12870.</p>
<p>Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. COMET: A neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685-2702, Online. Association for Computational Linguistics.</p>
<p>Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, et al. 2022. Scaling up models and data with t5x and seqio. arXiv preprint arXiv:2203.17189.</p>
<p>Tal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 624-643, Online. Association for Computational Linguistics.</p>
<p>Thomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2020. MLSUM: The multilingual summarization corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8051-8067, Online. Association for Computational Linguistics.</p>
<p>Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881-7892, Online. Association for Computational Linguistics.</p>
<p>Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. 2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems, volume 33, pages 3008-3021. Curran Associates, Inc.</p>
<p>James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809-819, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Ran Tian, Shashi Narayan, Thibault Sellam, and Ankur P Parikh. 2019. Sticking to the facts: Confident decoding for faithful data-to-text generation. arXiv preprint arXiv:1910.08684.</p>
<p>Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. 2017. TL;DR: Mining Reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization, pages 59-63, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Alex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008-5020, Online. Association for Computational Linguistics.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Sam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253-2263, Copenhagen, Denmark. Association for Computational Linguistics.</p>
<p>Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483-498, Online. Association for Computational Linguistics.</p>
<p>Tianyi Zhang<em>, Varsha Kishore</em>, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating text generation with bert. In International Conference on Learning Representations.</p>
<p>Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298-1308, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab, Francisco Guzmán, Luke Zettlemoyer, and Marjan Ghazvininejad. 2021. Detecting hallucinated content in conditional neural sequence generation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393-1404, Online. Association for Computational Linguistics.</p>
<h2>A Training details</h2>
<p>The summarization models were trained on the training split of each summarization dataset, with the exception of palm_1shot, which generated a summary given a single example from the dataset and the input article. The checkpoint for each model was selected using performance on the validation set of each respective dataset, except for t5_base_250 and mt5_small_250, which were only trained for 250 steps. The input length for the T5 and mT5 models was set to 1024, and 2048 for PaLM. The target length was 512.</p>
<p>The SEAHORSE metrics were trained on the SEAHORSE training split, and the best checkpoint was selected based on performance on the validation set. A separate metric was trained for each of the 6 dimensions of quality. We used only "Yes" and "No" ratings for training and testing the SEAHORSE metrics. The input length for the learnt metrics model is 2048. The article and summary are separated with "premise:" and "hypothesis:" tags, respectively, to be consistent with Honovich et al. (2022).</p>
<p>All training and inference was done with the t5x framework (Roberts et al., 2022) and run with TPU accelerators.</p>
<h2>B Rate of positive responses</h2>
<p>Table 9 shows a detailed breakdown of the proportion of responses that were positive (i.e., "Yes"), divided by language, dataset, model, and question. Summaries in languages other than English and produced by smaller models tend to have lower scores, indicating good directions for improving our summarization systems.</p>
<p>While most articles in the dataset were assigned to a subset of the summarization models, some articles were summarized by all 9 summarization systems (or 6 systems for the non-en languages that did not use the T5 models). Specifically in the test set, there were 543 articles that were summarized by all summarization systems. Table 10 shows the positive response rate across those summaries.</p>
<h2>C SEAHORSE example summaries and scores</h2>
<p>Figure 4 shows 3 summaries from the SEAHORSE dataset, along with ratings for the attribution (Q4) dimension from the human raters, mt5 ${ }_{\text {SEAHORSE }}$. and ROUGE-L.</p>
<h1>D Comparison between mT5_large and mT5_xxl</h1>
<p>Table 11 compares the results of two versions of mT5 finetuned on SEAHORSE data, mT5_large and mT5_xxl, on the SEAHORSE and mFACE test sets. Scores are generally close between the two models, but mT5_xxl outperforms the large metric in all cases except one.</p>
<p>Article: Take deep, slow breaths through your nose if you feel yourself getting emotional. This will help you calm down and give you something concrete to focus on. [...] Your facial muscles become tense when you cry, and it's natural for you to frown beforehand. Try to relax your frown and release all the tension from your face. You don't have to smile-you're at a funeral, after all-but relaxing your face will help keep you from crying. If you feel your facial muscles tensing up, take a couple deep breaths and relax your shoulders. Relaxing other parts of your body will help you relax your face as well. [...]</p>
<p>Summary: Make your face feel emotional. Relax your face. Relax your face.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Rater: 0</th>
<th style="text-align: left;">mt5 $_{\text {SEAHORSE: }} 0.15$</th>
<th style="text-align: left;">ROUGE-L: 33.59</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Comments: While the second (and repeated third) sentence in this summary is supported by the article, the first sentence is not, and both the rater and $\mathrm{mt}_{5}$ sEAHORSE rate it accordingly. ROUGE incorrectly rates it highly (in the 80th percentile of its scores).</p>
<p>Article: UN human rights chief backs Apple in FBI encryption row - The FBI has ordered the tech giant to assist it with unlocking an iPhone used by San Bernadino gunman Syed Farook. Prince Al Hussein said the law enforcement agency "deserves everyone's full support" in its investigation. However, encryption was essential in the interests of freedom, he added. "There are many ways to investigate whether or not these killers had accomplices besides forcing Apple to create software to undermine the security features of their own phones," he said in a statement. "It is potentially a gift to authoritarian regimes, as well as to criminal hackers. "Encryption and anonymity are needed as enablers of both freedom of expression and opinion, and the right to privacy. Without encryption tools, lives may be endangered." [...]</p>
<p>Summary: The UN human rights chief has backed Apple in its row with the FBI over encryption.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Rater: 1</th>
<th style="text-align: left;">$\mathbf{m t S}_{\text {SEAHORSE: }} 0.98$</th>
<th style="text-align: left;">ROUGE-L: 15.66</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Comments: This summary is a rewording of the first line of the article, so it is attributable to the article, which the rater and $\mathrm{mt}_{5}$ sEAHORSE agree with. ROUGE rates it low, however (in the 10th percentile).</p>
<p>Article: Wyoming diplodocus skeleton bought for Denmark museum - [...] Mystery had surrounded the buyer, but the Denmark museum confirmed on Tuesday it had acquired the skeleton. The museum bought the female dinosaur, nicknamed Misty, for $£ 400,000$ ( $\$ 652,000$ ), following a donation from the Obel Family Foundation. [...] Obel Family Foundation chairman Christen Obel said: "I think it's quite obvious and right that the Natural History Museum of Denmark should own a dinosaur. "So when we suddenly had the opportunity to give the museum this early Christmas present, we jumped at the chance. "Misty is an iconic object that fascinates us, and the dinosaur will certainly create value for the museum for many generations to come."</p>
<p>Summary: A diplodocus skeleton has been bought by the Natural History Museum of Denmark.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Rater: 1</th>
<th style="text-align: left;">$\mathbf{m t S}_{\text {SEAHORSE: }} 0.85$</th>
<th style="text-align: left;">ROUGE-L: 15.82</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Comments: Though the first line of the article seems to contradict the summary (for the museum vs. by the museum), the article later clarifies that it was in fact the museum that bought the dinosaur, so the rater and $\mathrm{mt}_{5}$ sEAHORSE are correct. Only the ROUGE metric rates it low (in the 10th percentile).</p>
<p>Figure 4: Example summaries and ratings from the human raters, $\mathrm{mt}_{5}$ SEAHORSE, and ROUGE-L for attribution (Q4).</p>
<p>| DE "YES" RATE |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
|  Dataset | Model | Q1 | Q2 | Q3 | Q4 | Q5 | Q6 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  mhum | reference | 0.99 | 0.99 | 0.98 | 0.82 | 0.64 | 0.55 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small_250 | 0.83 | 0.58 | 0.59 | 0.68 | 0.41 | 0.29 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small | 0.93 | 0.85 | 0.87 | 0.68 | 0.47 | 0.38 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_xxl | 0.98 | 0.97 | 0.95 | 0.8 | 0.59 | 0.5 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_1shot | 0.93 | 0.93 | 0.9 | 0.83 | 0.73 | 0.66 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_funetuned | 0.99 | 0.99 | 0.99 | 0.88 | 0.82 | 0.73 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | total | 0.94 | 0.89 | 0.88 | 0.79 | 0.62 | 0.53 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  wikilingua | reference | 0.97 | 0.96 | 0.94 | 0.65 | 0.63 | 0.49 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small_250 | 0.82 | 0.75 | 0.75 | 0.08 | 0.07 | 0.03 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small | 0.91 | 0.35 | 0.84 | 0.4 | 0.26 | 0.16 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_xxl | 0.97 | 0.91 | 0.93 | 0.69 | 0.62 | 0.49 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_1shot | 0.76 | 0.72 | 0.73 | 0.63 | 0.53 | 0.42 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_funetuned | 0.98 | 0.97 | 0.95 | 0.74 | 0.79 | 0.65 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | total | 0.9 | 0.78 | 0.85 | 0.53 | 0.48 | 0.37 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  total |  | 0.92 | 0.84 | 0.87 | 0.66 | 0.55 | 0.45 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   |  | EN "YES" RATE |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  Dataset | Model | Q1 | Q2 | Q3 | Q4 | Q5 | Q6 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  xsum | reference | 1.0 | 1.0 | 0.96 | 0.54 | 0.68 | 0.47 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_base_250 | 0.96 | 0.88 | 0.89 | 0.32 | 0.43 | 0.24 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_base | 0.96 | 0.91 | 0.91 | 0.42 | 0.5 | 0.32 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_xxl | 0.99 | 0.98 | 0.97 | 0.58 | 0.64 | 0.47 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small_250 | 0.7 | 0.47 | 0.57 | 0.17 | 0.2 | 0.09 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small | 0.84 | 0.68 | 0.75 | 0.17 | 0.24 | 0.12 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_xxl | 0.97 | 0.95 | 0.93 | 0.46 | 0.58 | 0.37 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_1shot | 0.97 | 0.96 | 0.91 | 0.48 | 0.55 | 0.39 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_funetuned | 0.99 | 0.99 | 0.99 | 0.6 | 0.65 | 0.51 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | total | 0.93 | 0.87 | 0.87 | 0.42 | 0.5 | 0.33 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  xsum | reference | 1.0 | 1.0 | 0.97 | 0.6 | 0.74 | 0.51 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_base_250 | 0.98 | 0.93 | 0.92 | 0.59 | 0.59 | 0.43 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_base | 0.99 | 0.96 | 0.96 | 0.65 | 0.68 | 0.52 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_xxl | 1.0 | 0.99 | 0.97 | 0.68 | 0.72 | 0.54 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small_250 | 0.74 | 0.53 | 0.59 | 0.29 | 0.24 | 0.15 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small | 0.89 | 0.78 | 0.79 | 0.4 | 0.44 | 0.29 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_xxl | 0.99 | 0.98 | 0.94 | 0.62 | 0.73 | 0.52 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_1shot | 0.95 | 0.95 | 0.92 | 0.73 | 0.68 | 0.58 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_funetuned | 1.0 | 1.0 | 1.0 | 0.62 | 0.59 | 0.45 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | total | 0.95 | 0.9 | 0.9 | 0.57 | 0.6 | 0.44 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  wikilingua | reference | 0.99 | 0.99 | 0.93 | 0.55 | 0.59 | 0.42 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_base_250 | 0.98 | 0.59 | 0.93 | 0.31 | 0.26 | 0.09 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_base | 0.98 | 0.89 | 0.93 | 0.67 | 0.57 | 0.45 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | t5_xxl | 0.98 | 0.95 | 0.92 | 0.68 | 0.63 | 0.51 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small_250 | 0.96 | 0.27 | 0.91 | 0.45 | 0.09 | 0.02 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small | 0.95 | 0.65 | 0.88 | 0.52 | 0.37 | 0.19 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_xxl | 1.0 | 0.96 | 0.92 | 0.62 | 0.64 | 0.49 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_1shot | 0.98 | 0.98 | 0.94 | 0.8 | 0.58 | 0.49 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_funetuned | 0.99 | 0.98 | 0.95 | 0.6 | 0.63 | 0.54 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | total | 0.98 | 0.8 | 0.92 | 0.58 | 0.48 | 0.35 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  total |  | 0.95 | 0.86 | 0.9 | 0.53 | 0.53 | 0.38 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   |  | ES "YES" RATE |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  Dataset | Model | Q1 | Q2 | Q3 | Q4 | Q5 | Q6 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  mhum | reference | 0.99 | 0.99 | 0.88 | 0.69 | 0.49 | 0.33 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small_250 | 0.78 | 0.69 | 0.63 | 0.38 | 0.2 | 0.11 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small | 0.94 | 0.88 | 0.8 | 0.61 | 0.38 | 0.25 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_xxl | 0.98 | 0.97 | 0.86 | 0.76 | 0.53 | 0.39 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_1shot | 0.73 | 0.72 | 0.27 | 0.41 | 0.45 | 0.32 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_funetuned | 0.99 | 0.99 | 0.02 | 0.92 | 0.78 | 0.75 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | total | 0.9 | 0.87 | 0.57 | 0.63 | 0.47 | 0.36 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  xsum | reference | 0.99 | 0.99 | 0.96 | 0.31 | 0.49 | 0.21 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small_250 | 0.64 | 0.44 | 0.55 | 0.17 | 0.16 | 0.07 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small | 0.8 | 0.63 | 0.71 | 0.23 | 0.28 | 0.12 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_xxl | 0.98 | 0.96 | 0.94 | 0.39 | 0.43 | 0.23 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_1shot | 0.9 | 0.89 | 0.85 | 0.76 | 0.7 | 0.64 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_funetuned | 0.99 | 0.99 | 0.98 | 0.5 | 0.66 | 0.41 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | total | 0.88 | 0.81 | 0.83 | 0.39 | 0.45 | 0.28 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  wikilingua | reference | 0.99 | 0.97 | 0.96 | 0.5 | 0.62 | 0.35 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small_250 | 0.75 | 0.61 | 0.73 | 0.16 | 0.08 | 0.03 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_small | 0.95 | 0.37 | 0.92 | 0.42 | 0.28 | 0.11 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | m85_xxl | 0.98 | 0.93 | 0.95 | 0.57 | 0.64 | 0.4 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_1shot | 0.96 | 0.91 | 0.93 | 0.85 | 0.62 | 0.46 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | palm_funetuned | 0.99 | 0.97 | 0.94 | 0.84 | 0.84 | 0.74 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|   | total | 0.93 | 0.79 | 0.9 | 0.55 | 0.51 | 0.34 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |
|  total |  | 0.91 | 0.83 | 0.77 | 0.52 | 0.48 | 0.33 |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |   |</p>
<p>Table 9: The percent of "Yes" responses, broken down by language, dataset, model, and question number.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: left;">Q1</th>
<th style="text-align: left;">Q2</th>
<th style="text-align: left;">Q3</th>
<th style="text-align: left;">Q4</th>
<th style="text-align: left;">Q5</th>
<th style="text-align: left;">Q6</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">reference</td>
<td style="text-align: left;">0.97</td>
<td style="text-align: left;">0.96</td>
<td style="text-align: left;">0.91</td>
<td style="text-align: left;">0.46</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.39</td>
</tr>
<tr>
<td style="text-align: left;">t5_base_250</td>
<td style="text-align: left;">0.96</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">0.9</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">0.48</td>
<td style="text-align: left;">0.31</td>
</tr>
<tr>
<td style="text-align: left;">t5_base</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">0.51</td>
<td style="text-align: left;">0.58</td>
<td style="text-align: left;">0.38</td>
</tr>
<tr>
<td style="text-align: left;">t5_xxl</td>
<td style="text-align: left;">0.99</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.72</td>
<td style="text-align: left;">0.58</td>
</tr>
<tr>
<td style="text-align: left;">mt5_small_250</td>
<td style="text-align: left;">0.64</td>
<td style="text-align: left;">0.45</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0.26</td>
<td style="text-align: left;">0.24</td>
<td style="text-align: left;">0.12</td>
</tr>
<tr>
<td style="text-align: left;">mt5_small</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.67</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">0.34</td>
<td style="text-align: left;">0.37</td>
<td style="text-align: left;">0.2</td>
</tr>
<tr>
<td style="text-align: left;">mt5_xxl</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">0.94</td>
<td style="text-align: left;">0.89</td>
<td style="text-align: left;">0.5</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.37</td>
</tr>
<tr>
<td style="text-align: left;">palm_1shot</td>
<td style="text-align: left;">0.93</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;">0.75</td>
<td style="text-align: left;">0.56</td>
<td style="text-align: left;">0.44</td>
</tr>
<tr>
<td style="text-align: left;">palm_finetuned</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">0.97</td>
<td style="text-align: left;">0.91</td>
<td style="text-align: left;">0.66</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">0.56</td>
</tr>
</tbody>
</table>
<p>Table 10: The percent of "Yes" responses for the set of articles that have summaries generated by all systems, broken down by model and question.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Q1</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Q2</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Q3</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Q4</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Q5</th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Q6</th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Dataset</td>
<td style="text-align: left;">Metric</td>
<td style="text-align: left;">$\rho$</td>
<td style="text-align: left;">roc</td>
<td style="text-align: left;">$\rho$</td>
<td style="text-align: left;">roc</td>
<td style="text-align: left;">$\rho$</td>
<td style="text-align: left;">roc</td>
<td style="text-align: left;">$\rho$</td>
<td style="text-align: left;">roc</td>
<td style="text-align: left;">$\rho$</td>
<td style="text-align: left;">roc</td>
<td style="text-align: left;">$\rho$</td>
<td style="text-align: left;">roc</td>
</tr>
<tr>
<td style="text-align: left;">SEAHORSE</td>
<td style="text-align: left;">mt5_L</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">0.74</td>
<td style="text-align: left;">0.97</td>
<td style="text-align: left;">0.37</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">0.55</td>
<td style="text-align: left;">0.82</td>
<td style="text-align: left;">0.46</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">0.45</td>
<td style="text-align: left;">0.77</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">mt5_XXL</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">0.90</td>
<td style="text-align: left;">0.86</td>
<td style="text-align: left;">0.98</td>
<td style="text-align: left;">0.45</td>
<td style="text-align: left;">0.84</td>
<td style="text-align: left;">0.59</td>
<td style="text-align: left;">0.85</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.80</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">mFACE -</td>
<td style="text-align: left;">mt5_L</td>
<td style="text-align: left;">0.14</td>
<td style="text-align: left;">0.77</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.48</td>
<td style="text-align: left;">0.78</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.32</td>
<td style="text-align: left;">0.70</td>
</tr>
<tr>
<td style="text-align: left;">5 langs</td>
<td style="text-align: left;">mt5_XXL</td>
<td style="text-align: left;">0.09</td>
<td style="text-align: left;">0.73</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.79</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.50</td>
<td style="text-align: left;">0.81</td>
</tr>
<tr>
<td style="text-align: left;">mFACE -</td>
<td style="text-align: left;">mt5_L</td>
<td style="text-align: left;">0.13</td>
<td style="text-align: left;">0.68</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.46</td>
<td style="text-align: left;">0.77</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.36</td>
<td style="text-align: left;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">all langs</td>
<td style="text-align: left;">mt5_XXL</td>
<td style="text-align: left;">0.15</td>
<td style="text-align: left;">0.70</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.52</td>
<td style="text-align: left;">0.81</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">-</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">0.74</td>
</tr>
</tbody>
</table>
<p>Table 11: Metrics' ability to predict SEAHORSE and mFACE ratings, measured with Pearson's coefficient ( $\rho$ ) and the area under the ROC curve (roc). Q1 maps to "Quality" in the mFACE dataset, Q4 to "Attribution," and Q6 to "Informativeness." mt5_L is a SEAHORSE-finetuned version of mT5_large; mt5_XXL is a SEAHORSE-finetuned version of mT5_xxl.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{5}$ There are actually 6 different models, one for each question, but we use the notation mt5 $5_{\text {SEAHORSE }}$ for simplicity.
${ }^{6}$ https://huggingface.co/google/t5_xxl_true_ nli_mixture&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{7}$ We obtained the dataset by contacting the authors.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
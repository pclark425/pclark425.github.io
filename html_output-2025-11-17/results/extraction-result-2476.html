<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2476 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2476</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2476</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-234280544</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2106.15412v1.pdf" target="_blank">An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble</a></p>
                <p><strong>Paper Abstract:</strong> Bayesian optimization is a promising methodology for analog circuit synthesis. However, the sequential nature of the Bayesian optimization framework significantly limits its ability to fully utilize real-world computational resources. In this paper, we propose an efficient parallelizable Bayesian optimization algorithm via Multi-objective ACquisition function Ensemble (MACE) to further accelerate the optimization procedure. By sampling query points from the Pareto front of the probability of improvement (PI), expected improvement (EI) and lower confidence bound (LCB), we combine the benefits of state-of-the-art acquisition functions to achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Based on this batch design, we further adjust the algorithm for the constrained optimization problem. By dividing the optimization procedure into two stages and first focusing on finding an initial feasible point, we manage to gain more information about the valid region and can better avoid sampling around the infeasible area. After achieving the first feasible point, we favor the feasible region by adopting a specially designed penalization term to the acquisition function ensemble. The experimental results quantitatively demonstrate that our proposed algorithm can reduce the overall simulation time by up to 74 times compared to differential evolution (DE) for the unconstrained optimization problem when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15 times compared to the weighted expected improvement based Bayesian optimization (WEIBO) approach, when the batch size is 15.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2476.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2476.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MACE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-objective ACquisition function Ensemble</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch-parallel Bayesian optimization algorithm that samples query points from the Pareto front of multiple acquisition functions (PI, EI, LCB) to jointly maximize information gain per observation while preserving batch diversity and balancing exploration vs exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MACE (Multi-objective ACquisition function Ensemble)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a multi-objective acquisition ensemble combining Lower Confidence Bound (LCB), Probability of Improvement (PI) and Expected Improvement (EI) as separate objectives, obtains a Pareto-optimal set of candidate points via a multi-objective optimizer (DEMO), and then randomly samples a batch of B points from that Pareto front to evaluate in parallel. For constrained problems MACE employs a two-stage procedure (seek first feasible point, then optimize with penalties/pruning) and augments the acquisition objectives with probability of feasibility (PF) and adaptive constraint-violation measures.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Automated analog circuit device sizing / black-box expensive optimization (analog/RF circuit synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate a fixed evaluation budget into iterations with batch size B; at each iteration allocate B parallel evaluations by sampling B points from the Pareto front of multiple acquisition objectives (LCB, -PI, -EI) to maximize information gain per evaluation while ensuring batch diversity. For constrained tasks, initially prioritize feasibility objectives (PF and scaled violation) until a feasible point is found, then prune candidates likely to violate constraints using an adaptive threshold ρ and include constraint-penalized acquisition objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of expensive circuit simulations (Avg. # Sim) and total simulation time; experimental comparisons reported as equivalent simulation time and speedup factors (e.g., X× faster).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Heuristic acquisition measures: Expected Improvement (EI), Probability of Improvement (PI), and Lower Confidence Bound (LCB); probability of feasibility (PF) and adaptive scaled constraint-violation (µ/σ) are used for constraints. The system uses these acquisition utilities as proxies for expected utility / information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Balances exploration and exploitation by treating LCB (exploration via µ - βσ), PI (exploitation), and EI (utility-weighted improvement) as separate objectives in a multi-objective optimization; points on the Pareto front represent different exploration/exploitation trade-offs and are sampled to form a batch that jointly spans these trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity achieved implicitly by sampling multiple Pareto-optimal solutions across the acquisition-objective frontier rather than selecting greedily or sequentially with hand-crafted penalization; no explicit local penalization is required for batch diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of expensive simulations (evaluation budget) and user-specified batch size B</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Divides total allowed simulations into iterations of batch size B, uses initial dataset size N_init, and constructs batches from Pareto set until budget exhausted; for constrained problems, a two-stage stage (seek feasible then optimize) focuses budget early on finding feasibility to avoid wasteful sampling of infeasible regions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Improved Figure-of-Merit (FOM) values for circuit objectives and success in finding feasible designs that satisfy constraints (binary success counts); higher objective improvements (best/mean/median) are used to identify important discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported metrics include best/worst/mean/std of FOM or objective, Avg. # Sim (equivalent simulation time), and # Success (runs that find feasible designs). Key reported results: up to 74× reduction in overall simulation time vs Differential Evolution (DE) for unconstrained problems at batch size 15, and up to 15× speedup vs WEIBO for constrained problems at batch size 15; consistent improvements in mean/best FOM across multiple analog circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Differential Evolution (DE), Expected Improvement (EI), Lower Confidence Bound (LCB), PI-EI / EI-LCB / PI-LCB ensembles, pBO, pHCBO, LP-EI, LP-LCB, BLCB, GPUCB-PE, WEIBO, GASPAD, MSP, PSO, SA</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MACE consistently outperformed single acquisition methods (EI, LCB) and other batch policies across circuits and batch sizes. Examples: up to 74× faster than DE (unconstrained, B=15) and up to 15× faster than WEIBO (constrained, B=15); produced better mean/best FOMs and smaller variance in many experiments reported in tables.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Reported up to 74× reduction in simulation time vs DE (unconstrained, B=15); up to 15× speedup vs WEIBO (constrained, B=15); other circuit-specific speedups reported (e.g., 42×, 11×, 49× reductions against various baselines across different circuits).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper analyzes tradeoffs qualitatively and empirically: ensembles of acquisition functions combine complementary behaviors (PI tends toward exploitation, LCB toward exploration, EI in-between), so sampling the Pareto front achieves a better exploration/exploitation balance and higher information gain per evaluation; larger batch sizes can degrade performance for some acquisition strategies but MACE remains robust; two-stage constrained strategy reduces wasted budget on infeasible regions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Main recommendations: (1) Form batches by sampling from the Pareto front of multiple acquisition functions to maximize per-evaluation information and inherent diversity rather than sequential penalization; (2) For constrained problems, use a staged approach by first locating a feasible point (using PF and adaptive violation measures) then switching to constraint-aware ensemble selection with pruning (threshold ρ) to focus remaining budget on feasible high-value regions; (3) adaptive scaling of constraint violation by µ/σ prioritizes constraints with higher confidence and yields more cost-efficient allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2476.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2476.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Constrained-MACE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Constrained Batch Multi-objective ACquisition function Ensemble (two-stage MACE)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An extension of MACE for optimization with unknown constraints that divides search into two stages: first prioritize feasibility (seek first feasible point) using PF and scaled-violation objectives, then search for constrained optimum using acquisition ensemble plus pruning based on predicted constraint violation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Constrained MACE (two-stage)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Stage 1: while no feasible observation exists, construct a Pareto front over objectives −PF(x), sum max(0, µ_i(x)), and sum max(0, µ_i(x)/σ_i(x)) to prioritize feasible and informative points; Stage 2: after a feasible point is found, include objective acquisition ensemble (LCB, -PI, -EI) plus constraint objectives and apply a pruning rule sum max(0, µ_i(x)/σ_i(x)) ≤ ρ to remove candidates likely to violate constraints, then sample batches from resulting Pareto set.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Constrained black-box optimization for analog/RF circuit synthesis (finding feasible designs that satisfy performance constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocates early budget to feasibility discovery (PF and adaptive violation objectives) to collect informative data about the feasible region, then reallocates budget to constrained optimization using augmented acquisition ensemble and pruning to avoid wasting evaluations on infeasible points.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of circuit simulations and equivalent simulation time (Avg. # Sim); budget is expressed as maximum number of simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition utilities (EI, PI, LCB) and probability-of-feasibility (PF); adaptive scaled violation (µ/σ) is used to prioritize constraints with higher predictive confidence — these serve as proxies for expected information regarding feasibility and objective improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Stage 1 emphasizes exploitation of feasibility via PF and targeted reduction of high-confidence violations; Stage 2 balances exploration/exploitation across objective and constraint-aware acquisition ensemble (LCB for exploration, PI/EI for exploitation), while pruning restricts search to promising feasible regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Maintains diversity by sampling across the multi-objective Pareto front that includes both objective and feasibility-related acquisition components; pruning removes high-risk infeasible candidates but does not force sequential penalization.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed simulation budget (max simulation count) and batch size B</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Front-loads budget to find a feasible sample quickly (reducing wasted evaluations on infeasible regions), then focuses remaining budget on constrained objective optimization; uses pruning threshold ρ to avoid spending budget on high-probability infeasible candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Ability to find feasible designs meeting constraints (binary success counts) and improved constrained FOM values (best/mean/median); reductions in Avg. # Sim to reach feasible solutions are treated as breakthroughs in efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported mean/median/best/worst objective values, Avg. # Sim, and # Success. Example: MACE reduced simulation time by up to 15× vs WEIBO (B=15) and achieved higher success rates and better FOMs across multiple constrained circuit benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>WEIBO (weighted EI with PF), GASPAD, MSP, DE, PSO, SA, and a one-stage variant (oMACE)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Constrained MACE outperformed WEIBO and other baselines in success rate, average FOM, and simulation efficiency. The two-stage design (MACE) consistently outperformed the one-stage variant (oMACE) across batch sizes, showing fewer wasted evaluations and higher robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Examples include up to 15× speedup vs WEIBO (charge pump and other circuits), up to 185× vs DE on some constrained tasks, and substantial reductions in Avg. # Sim reported in tables for specific circuits.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper shows the two-stage approach trades initial exploration of feasibility for later focused exploitation of feasible regions; adaptive µ/σ scaling shifts effort toward constraints where further confidence is most beneficial. Pruning parameter ρ trades risk (missing boundary-feasible improvements) vs wasted cost on infeasible evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Allocate early budget to discovering feasibility using PF and adaptive violation scaling; once a feasible point exists, restrict candidate pool via pruning based on predicted violation confidence and continue with a constrained acquisition ensemble to optimize objective — this reduces wasted evaluations and improves constrained optimization efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2476.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2476.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Local Penalization batch design</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch Bayesian optimization method that selects batch points sequentially by penalizing regions near previously selected points using a local repulsion based on an estimated Lipschitz constant.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Local Penalization (LP)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sequential batch construction: after selecting a point in the batch, introduce a local penalization term (based on an estimated Lipschitz constant) to reduce acquisition scores near that point, then select the next batch member; repeat until batch complete.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>General batch Bayesian optimization for expensive black-box functions (used here as a baseline for analog circuit synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequentially constructs a batch by greedily selecting points that maximize a penalized acquisition function, thereby allocating batch evaluations to spatially spread locations.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of simulations / evaluations (as used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Acquisition functions (commonly EI or LCB) modified by local penalization; information gain is proxied by the penalized acquisition values.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Original acquisition (EI/LCB) balances exploration/exploitation; the local penalizer increases spatial diversity (more exploration) by reducing acquisition near already-selected points.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Explicit local repulsive penalization using Lipschitz-based exclusion zones around selected batch points.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget / batch size</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Constructs batches one-by-one until budget exhausted; penalty reduces redundant sampling and aims to maximize marginal utility per evaluation under fixed budget.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in experiments as mean/worst/best FOM and Avg. # Sim; LP-EI/LP-LCB variants included in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MACE, pBO, pHCBO, BLCB, GPUCB-PE, DE, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>LP variants sometimes underperformed MACE; the paper notes sensitivity to Lipschitz constant estimation can hinder efficiency and may introduce bias.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not reported as superior in this paper; LP methods were generally outperformed by MACE in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper critiques LP for introducing human-biased penalization (Lipschitz approximation) which can misallocate budget if poorly estimated; penalization increases exploration at cost of possibly neglecting high-utility regions.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Manual penalization can help avoid batch clustering but requires careful Lipschitz estimation; ensemble/Pareto-based sampling avoids this manual bias.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2476.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2476.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BLCB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Batched Lower Confidence Bound with hallucinated observations</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A batch BO method that encourages diversity by 'hallucinating' observations at previously selected batch points (assuming their predicted means) and updating the surrogate to pick subsequent batch members.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BLCB (Batched LCB with hallucinated observations)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Sequential batch selection: after choosing a point, the method 'hallucinates' an observation (typically the GP predictive mean) at that point to update the GP posterior (uncertainty reduction) and then selects the next point minimizing LCB under that hallucinated update, aiming to diversify batch selections.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch Bayesian optimization for expensive black-box evaluations (baseline for circuit synthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequentially allocate batch evaluations by updating surrogate with imagined outcomes to avoid redundant sampling and to maximize marginal utility under the GP model.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of simulations</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>LCB values and GP posterior updates via hallucinated observations (heuristic for marginal information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>LCB encourages exploration via uncertainty term; hallucinated updates aim to spread batch across informative regions.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Implicit via sequential hallucinated posterior updates which lower acquisition near prior selections.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget / batch size</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Greedy sequential construction of batch under hallucinated updates until batch filled.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Included as a baseline in experiments; reported mean/best/worst FOM and Avg. # Sim.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against MACE, LP variants, GPUCB-PE, pBO, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BLCB produced competitive but generally worse or similar results to MACE; paper notes single-acquisition reliance can limit performance compared to ensembles.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not highlighted as best in experiments; MACE outperformed BLCB in many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes hallucinated-observation strategies reduce redundant sampling but can be suboptimal when the GP predictive mean is a poor surrogate for true responses.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Hallucination-based batch construction is effective to encourage diversity but is limited by the fidelity of the surrogate's predictive mean.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2476.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2476.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPUCB-PE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Gaussian Process UCB with Pure Exploration</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel batch design that combines UCB-style optimistic selection with pure exploration queries in the same batch to improve information gain per evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPUCB-PE</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a batch that mixes UCB (or LCB) queries focusing on optimism/exploitation with dedicated pure-exploration queries aimed at uncertainty reduction to increase batch-level information gain.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Parallel Bayesian optimization / expensive black-box functions</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate some batch slots to UCB-driven picks and others to pure-exploration picks to diversify information acquisition across exploitation and uncertainty reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of simulations</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>UCB utility and reduction in predictive uncertainty (pure exploration targets high-uncertainty points).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit partitioning of batch between exploratory (uncertainty-reducing) and optimistic (UCB) queries to balance exploration and exploitation within each batch.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Diversity arises from explicit inclusion of pure-exploration points alongside UCB selections.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed budget and batch size</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Designates batch slots for exploration vs exploitation according to a predetermined policy.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Included in comparisons; sometimes underperformed MACE depending on circuit and batch size.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with MACE, LP, BLCB, pBO, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>GPUCB-PE showed variable performance; paper notes pure-exploration-heavy schemes may not be efficient for problems requiring more exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not the top performer; MACE often outperformed GPUCB-PE in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>GPUCB-PE's fixed partitioning toward exploration can waste resources when problem requires exploitation; MACE's Pareto ensemble better adapts the tradeoff per batch.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Combining exploration and exploitation in a batch helps, but adaptive multi-objective ensemble sampling better matches diverse problem requirements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2476.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2476.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pBO / pHCBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Parallel Bayesian Optimization / parallel High Coverage BO</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>pBO selects batches by varying weighting parameters to achieve different exploration/exploitation trade-offs; pHCBO adds a penalization scheme to avoid cluster sampling and encourage high coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>pBO and pHCBO</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>pBO constructs batches by sampling acquisition functions using different weightings between exploration and exploitation to get diverse behavior; pHCBO further adds penalization to discourage clustered selections within a batch and improve coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Batch Bayesian optimization for expensive black-box tasks</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate batch members by sweeping weighting parameters (e.g., between mean and variance or different acquisition components) to create diversified selection strategies across the batch.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of evaluations/simulations</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Weighted acquisition function values reflecting different exploration/exploitation emphasis.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Diversity across weightings yields mixture of exploratory and exploitative picks; pHCBO penalizes clusters to boost exploration coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Multiple weighting strategies across batch and an added penalization term in pHCBO to discourage cluster sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed evaluation budget / batch size</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Batch construction by choosing several weighting parameter settings and selecting points accordingly until budget for batch is filled.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Included in experimental baselines; performance varies with batch size and problem.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to MACE, LP, BLCB, GPUCB-PE, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>pBO/pHCBO sometimes underperformed MACE; pHCBO's penalization reduced clustering but could still be less efficient than Pareto-ensemble sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Not highlighted as superior in reported experiments; MACE typically achieved better or more stable results.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper discusses that manually designed penalization/weighting may introduce bias and can be less adaptive than Pareto-front sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Using multiple weighting strategies helps diversity, but Pareto-based multi-objective sampling provides a less biased and more flexible diversity mechanism.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2476.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2476.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WEIBO</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weighted Expected Improvement Bayesian Optimization (for constraints)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequential constrained Bayesian optimization method that weights the expected improvement (EI) by the probability of feasibility (PF) to favor points likely to satisfy constraints while offering expected improvement on the objective.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WEIBO (weighted EI with PF)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute wEI(x)=EI(x)*PF(x) to rank candidate points; selects points that are expected to improve the objective while also being likely feasible under GP models of constraints, designed for sequential decision-making.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Constrained black-box optimization (e.g., analog circuit sizing where constraints must be satisfied)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Sequentially allocate evaluations to points maximizing weighted expected improvement, thus allocating budget preferentially to candidates with high expected objective gain and high feasibility probability.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of simulations / sequential evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement modulated by Probability of Feasibility (EI × PF) — a heuristic combining utility and feasibility likelihood.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>EI provides exploitation (and some exploration via uncertainty), PF modulates focus on feasible region; overall tends to exploit where both are high.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>None explicit for parallel batches (designed for sequential use); paper notes WEIBO is sequential and limits parallel efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed simulation budget (sequential)</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Sequential selection until budget exhausted; does not natively parallelize without modification.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used as constrained baseline in experiments; paper reports MACE achieving up to 15× speedup vs WEIBO in some constrained tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>MACE compared against WEIBO across constrained circuit benchmarks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MACE outperformed WEIBO in both efficiency (Avg. # Sim) and success/quality on constrained problems, particularly when batch parallelism is available.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Example: up to 15× faster than WEIBO (charged in constrained experiments with B=15).</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper notes WEIBO's sequential nature limits parallelism and thus resource utilization; weighting EI by PF can over-penalize exploration early when feasible region is unknown.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>For constrained problems, front-loading feasibility search (as in Constrained MACE) and then using parallel constrained acquisition/pooling is more budget-efficient than purely sequential wEI selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble', 'publication_date_yy_mm': '2021-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Batch bayesian optimization via local penalization <em>(Rating: 2)</em></li>
                <li>Parallel gaussian process optimization with upper confidence bound and pure exploration <em>(Rating: 2)</em></li>
                <li>Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization <em>(Rating: 2)</em></li>
                <li>Bayesian optimization with unknown constraints <em>(Rating: 2)</em></li>
                <li>Portfolio allocation for bayesian optimization <em>(Rating: 1)</em></li>
                <li>An entropy search portfolio for bayesian optimization <em>(Rating: 1)</em></li>
                <li>Batch bayesian optimization via multi-objective acquisition ensemble for automated analog circuit design <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2476",
    "paper_id": "paper-234280544",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "MACE",
            "name_full": "Multi-objective ACquisition function Ensemble",
            "brief_description": "A batch-parallel Bayesian optimization algorithm that samples query points from the Pareto front of multiple acquisition functions (PI, EI, LCB) to jointly maximize information gain per observation while preserving batch diversity and balancing exploration vs exploitation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "MACE (Multi-objective ACquisition function Ensemble)",
            "system_description": "Constructs a multi-objective acquisition ensemble combining Lower Confidence Bound (LCB), Probability of Improvement (PI) and Expected Improvement (EI) as separate objectives, obtains a Pareto-optimal set of candidate points via a multi-objective optimizer (DEMO), and then randomly samples a batch of B points from that Pareto front to evaluate in parallel. For constrained problems MACE employs a two-stage procedure (seek first feasible point, then optimize with penalties/pruning) and augments the acquisition objectives with probability of feasibility (PF) and adaptive constraint-violation measures.",
            "application_domain": "Automated analog circuit device sizing / black-box expensive optimization (analog/RF circuit synthesis)",
            "resource_allocation_strategy": "Allocate a fixed evaluation budget into iterations with batch size B; at each iteration allocate B parallel evaluations by sampling B points from the Pareto front of multiple acquisition objectives (LCB, -PI, -EI) to maximize information gain per evaluation while ensuring batch diversity. For constrained tasks, initially prioritize feasibility objectives (PF and scaled violation) until a feasible point is found, then prune candidates likely to violate constraints using an adaptive threshold ρ and include constraint-penalized acquisition objectives.",
            "computational_cost_metric": "Number of expensive circuit simulations (Avg. # Sim) and total simulation time; experimental comparisons reported as equivalent simulation time and speedup factors (e.g., X× faster).",
            "information_gain_metric": "Heuristic acquisition measures: Expected Improvement (EI), Probability of Improvement (PI), and Lower Confidence Bound (LCB); probability of feasibility (PF) and adaptive scaled constraint-violation (µ/σ) are used for constraints. The system uses these acquisition utilities as proxies for expected utility / information gain.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Balances exploration and exploitation by treating LCB (exploration via µ - βσ), PI (exploitation), and EI (utility-weighted improvement) as separate objectives in a multi-objective optimization; points on the Pareto front represent different exploration/exploitation trade-offs and are sampled to form a batch that jointly spans these trade-offs.",
            "diversity_mechanism": "Diversity achieved implicitly by sampling multiple Pareto-optimal solutions across the acquisition-objective frontier rather than selecting greedily or sequentially with hand-crafted penalization; no explicit local penalization is required for batch diversity.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of expensive simulations (evaluation budget) and user-specified batch size B",
            "budget_constraint_handling": "Divides total allowed simulations into iterations of batch size B, uses initial dataset size N_init, and constructs batches from Pareto set until budget exhausted; for constrained problems, a two-stage stage (seek feasible then optimize) focuses budget early on finding feasibility to avoid wasteful sampling of infeasible regions.",
            "breakthrough_discovery_metric": "Improved Figure-of-Merit (FOM) values for circuit objectives and success in finding feasible designs that satisfy constraints (binary success counts); higher objective improvements (best/mean/median) are used to identify important discoveries.",
            "performance_metrics": "Reported metrics include best/worst/mean/std of FOM or objective, Avg. # Sim (equivalent simulation time), and # Success (runs that find feasible designs). Key reported results: up to 74× reduction in overall simulation time vs Differential Evolution (DE) for unconstrained problems at batch size 15, and up to 15× speedup vs WEIBO for constrained problems at batch size 15; consistent improvements in mean/best FOM across multiple analog circuits.",
            "comparison_baseline": "Differential Evolution (DE), Expected Improvement (EI), Lower Confidence Bound (LCB), PI-EI / EI-LCB / PI-LCB ensembles, pBO, pHCBO, LP-EI, LP-LCB, BLCB, GPUCB-PE, WEIBO, GASPAD, MSP, PSO, SA",
            "performance_vs_baseline": "MACE consistently outperformed single acquisition methods (EI, LCB) and other batch policies across circuits and batch sizes. Examples: up to 74× faster than DE (unconstrained, B=15) and up to 15× faster than WEIBO (constrained, B=15); produced better mean/best FOMs and smaller variance in many experiments reported in tables.",
            "efficiency_gain": "Reported up to 74× reduction in simulation time vs DE (unconstrained, B=15); up to 15× speedup vs WEIBO (constrained, B=15); other circuit-specific speedups reported (e.g., 42×, 11×, 49× reductions against various baselines across different circuits).",
            "tradeoff_analysis": "Paper analyzes tradeoffs qualitatively and empirically: ensembles of acquisition functions combine complementary behaviors (PI tends toward exploitation, LCB toward exploration, EI in-between), so sampling the Pareto front achieves a better exploration/exploitation balance and higher information gain per evaluation; larger batch sizes can degrade performance for some acquisition strategies but MACE remains robust; two-stage constrained strategy reduces wasted budget on infeasible regions.",
            "optimal_allocation_findings": "Main recommendations: (1) Form batches by sampling from the Pareto front of multiple acquisition functions to maximize per-evaluation information and inherent diversity rather than sequential penalization; (2) For constrained problems, use a staged approach by first locating a feasible point (using PF and adaptive violation measures) then switching to constraint-aware ensemble selection with pruning (threshold ρ) to focus remaining budget on feasible high-value regions; (3) adaptive scaling of constraint violation by µ/σ prioritizes constraints with higher confidence and yields more cost-efficient allocation.",
            "uuid": "e2476.0",
            "source_info": {
                "paper_title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "Constrained-MACE",
            "name_full": "Constrained Batch Multi-objective ACquisition function Ensemble (two-stage MACE)",
            "brief_description": "An extension of MACE for optimization with unknown constraints that divides search into two stages: first prioritize feasibility (seek first feasible point) using PF and scaled-violation objectives, then search for constrained optimum using acquisition ensemble plus pruning based on predicted constraint violation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Constrained MACE (two-stage)",
            "system_description": "Stage 1: while no feasible observation exists, construct a Pareto front over objectives −PF(x), sum max(0, µ_i(x)), and sum max(0, µ_i(x)/σ_i(x)) to prioritize feasible and informative points; Stage 2: after a feasible point is found, include objective acquisition ensemble (LCB, -PI, -EI) plus constraint objectives and apply a pruning rule sum max(0, µ_i(x)/σ_i(x)) ≤ ρ to remove candidates likely to violate constraints, then sample batches from resulting Pareto set.",
            "application_domain": "Constrained black-box optimization for analog/RF circuit synthesis (finding feasible designs that satisfy performance constraints)",
            "resource_allocation_strategy": "Allocates early budget to feasibility discovery (PF and adaptive violation objectives) to collect informative data about the feasible region, then reallocates budget to constrained optimization using augmented acquisition ensemble and pruning to avoid wasting evaluations on infeasible points.",
            "computational_cost_metric": "Number of circuit simulations and equivalent simulation time (Avg. # Sim); budget is expressed as maximum number of simulations.",
            "information_gain_metric": "Acquisition utilities (EI, PI, LCB) and probability-of-feasibility (PF); adaptive scaled violation (µ/σ) is used to prioritize constraints with higher predictive confidence — these serve as proxies for expected information regarding feasibility and objective improvement.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Stage 1 emphasizes exploitation of feasibility via PF and targeted reduction of high-confidence violations; Stage 2 balances exploration/exploitation across objective and constraint-aware acquisition ensemble (LCB for exploration, PI/EI for exploitation), while pruning restricts search to promising feasible regions.",
            "diversity_mechanism": "Maintains diversity by sampling across the multi-objective Pareto front that includes both objective and feasibility-related acquisition components; pruning removes high-risk infeasible candidates but does not force sequential penalization.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed simulation budget (max simulation count) and batch size B",
            "budget_constraint_handling": "Front-loads budget to find a feasible sample quickly (reducing wasted evaluations on infeasible regions), then focuses remaining budget on constrained objective optimization; uses pruning threshold ρ to avoid spending budget on high-probability infeasible candidates.",
            "breakthrough_discovery_metric": "Ability to find feasible designs meeting constraints (binary success counts) and improved constrained FOM values (best/mean/median); reductions in Avg. # Sim to reach feasible solutions are treated as breakthroughs in efficiency.",
            "performance_metrics": "Reported mean/median/best/worst objective values, Avg. # Sim, and # Success. Example: MACE reduced simulation time by up to 15× vs WEIBO (B=15) and achieved higher success rates and better FOMs across multiple constrained circuit benchmarks.",
            "comparison_baseline": "WEIBO (weighted EI with PF), GASPAD, MSP, DE, PSO, SA, and a one-stage variant (oMACE)",
            "performance_vs_baseline": "Constrained MACE outperformed WEIBO and other baselines in success rate, average FOM, and simulation efficiency. The two-stage design (MACE) consistently outperformed the one-stage variant (oMACE) across batch sizes, showing fewer wasted evaluations and higher robustness.",
            "efficiency_gain": "Examples include up to 15× speedup vs WEIBO (charge pump and other circuits), up to 185× vs DE on some constrained tasks, and substantial reductions in Avg. # Sim reported in tables for specific circuits.",
            "tradeoff_analysis": "Paper shows the two-stage approach trades initial exploration of feasibility for later focused exploitation of feasible regions; adaptive µ/σ scaling shifts effort toward constraints where further confidence is most beneficial. Pruning parameter ρ trades risk (missing boundary-feasible improvements) vs wasted cost on infeasible evaluations.",
            "optimal_allocation_findings": "Allocate early budget to discovering feasibility using PF and adaptive violation scaling; once a feasible point exists, restrict candidate pool via pruning based on predicted violation confidence and continue with a constrained acquisition ensemble to optimize objective — this reduces wasted evaluations and improves constrained optimization efficiency.",
            "uuid": "e2476.1",
            "source_info": {
                "paper_title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "LP",
            "name_full": "Local Penalization batch design",
            "brief_description": "A batch Bayesian optimization method that selects batch points sequentially by penalizing regions near previously selected points using a local repulsion based on an estimated Lipschitz constant.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Local Penalization (LP)",
            "system_description": "Sequential batch construction: after selecting a point in the batch, introduce a local penalization term (based on an estimated Lipschitz constant) to reduce acquisition scores near that point, then select the next batch member; repeat until batch complete.",
            "application_domain": "General batch Bayesian optimization for expensive black-box functions (used here as a baseline for analog circuit synthesis)",
            "resource_allocation_strategy": "Sequentially constructs a batch by greedily selecting points that maximize a penalized acquisition function, thereby allocating batch evaluations to spatially spread locations.",
            "computational_cost_metric": "Number of simulations / evaluations (as used in experiments)",
            "information_gain_metric": "Acquisition functions (commonly EI or LCB) modified by local penalization; information gain is proxied by the penalized acquisition values.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Original acquisition (EI/LCB) balances exploration/exploitation; the local penalizer increases spatial diversity (more exploration) by reducing acquisition near already-selected points.",
            "diversity_mechanism": "Explicit local repulsive penalization using Lipschitz-based exclusion zones around selected batch points.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed evaluation budget / batch size",
            "budget_constraint_handling": "Constructs batches one-by-one until budget exhausted; penalty reduces redundant sampling and aims to maximize marginal utility per evaluation under fixed budget.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Reported in experiments as mean/worst/best FOM and Avg. # Sim; LP-EI/LP-LCB variants included in comparisons.",
            "comparison_baseline": "Compared against MACE, pBO, pHCBO, BLCB, GPUCB-PE, DE, etc.",
            "performance_vs_baseline": "LP variants sometimes underperformed MACE; the paper notes sensitivity to Lipschitz constant estimation can hinder efficiency and may introduce bias.",
            "efficiency_gain": "Not reported as superior in this paper; LP methods were generally outperformed by MACE in experiments.",
            "tradeoff_analysis": "Paper critiques LP for introducing human-biased penalization (Lipschitz approximation) which can misallocate budget if poorly estimated; penalization increases exploration at cost of possibly neglecting high-utility regions.",
            "optimal_allocation_findings": "Manual penalization can help avoid batch clustering but requires careful Lipschitz estimation; ensemble/Pareto-based sampling avoids this manual bias.",
            "uuid": "e2476.2",
            "source_info": {
                "paper_title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "BLCB",
            "name_full": "Batched Lower Confidence Bound with hallucinated observations",
            "brief_description": "A batch BO method that encourages diversity by 'hallucinating' observations at previously selected batch points (assuming their predicted means) and updating the surrogate to pick subsequent batch members.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "BLCB (Batched LCB with hallucinated observations)",
            "system_description": "Sequential batch selection: after choosing a point, the method 'hallucinates' an observation (typically the GP predictive mean) at that point to update the GP posterior (uncertainty reduction) and then selects the next point minimizing LCB under that hallucinated update, aiming to diversify batch selections.",
            "application_domain": "Batch Bayesian optimization for expensive black-box evaluations (baseline for circuit synthesis)",
            "resource_allocation_strategy": "Sequentially allocate batch evaluations by updating surrogate with imagined outcomes to avoid redundant sampling and to maximize marginal utility under the GP model.",
            "computational_cost_metric": "Number of simulations",
            "information_gain_metric": "LCB values and GP posterior updates via hallucinated observations (heuristic for marginal information gain).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "LCB encourages exploration via uncertainty term; hallucinated updates aim to spread batch across informative regions.",
            "diversity_mechanism": "Implicit via sequential hallucinated posterior updates which lower acquisition near prior selections.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed evaluation budget / batch size",
            "budget_constraint_handling": "Greedy sequential construction of batch under hallucinated updates until batch filled.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Included as a baseline in experiments; reported mean/best/worst FOM and Avg. # Sim.",
            "comparison_baseline": "Compared against MACE, LP variants, GPUCB-PE, pBO, etc.",
            "performance_vs_baseline": "BLCB produced competitive but generally worse or similar results to MACE; paper notes single-acquisition reliance can limit performance compared to ensembles.",
            "efficiency_gain": "Not highlighted as best in experiments; MACE outperformed BLCB in many settings.",
            "tradeoff_analysis": "Paper notes hallucinated-observation strategies reduce redundant sampling but can be suboptimal when the GP predictive mean is a poor surrogate for true responses.",
            "optimal_allocation_findings": "Hallucination-based batch construction is effective to encourage diversity but is limited by the fidelity of the surrogate's predictive mean.",
            "uuid": "e2476.3",
            "source_info": {
                "paper_title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "GPUCB-PE",
            "name_full": "Parallel Gaussian Process UCB with Pure Exploration",
            "brief_description": "A parallel batch design that combines UCB-style optimistic selection with pure exploration queries in the same batch to improve information gain per evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPUCB-PE",
            "system_description": "Constructs a batch that mixes UCB (or LCB) queries focusing on optimism/exploitation with dedicated pure-exploration queries aimed at uncertainty reduction to increase batch-level information gain.",
            "application_domain": "Parallel Bayesian optimization / expensive black-box functions",
            "resource_allocation_strategy": "Allocate some batch slots to UCB-driven picks and others to pure-exploration picks to diversify information acquisition across exploitation and uncertainty reduction.",
            "computational_cost_metric": "Number of simulations",
            "information_gain_metric": "UCB utility and reduction in predictive uncertainty (pure exploration targets high-uncertainty points).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit partitioning of batch between exploratory (uncertainty-reducing) and optimistic (UCB) queries to balance exploration and exploitation within each batch.",
            "diversity_mechanism": "Diversity arises from explicit inclusion of pure-exploration points alongside UCB selections.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed budget and batch size",
            "budget_constraint_handling": "Designates batch slots for exploration vs exploitation according to a predetermined policy.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Included in comparisons; sometimes underperformed MACE depending on circuit and batch size.",
            "comparison_baseline": "Compared with MACE, LP, BLCB, pBO, etc.",
            "performance_vs_baseline": "GPUCB-PE showed variable performance; paper notes pure-exploration-heavy schemes may not be efficient for problems requiring more exploitation.",
            "efficiency_gain": "Not the top performer; MACE often outperformed GPUCB-PE in reported experiments.",
            "tradeoff_analysis": "GPUCB-PE's fixed partitioning toward exploration can waste resources when problem requires exploitation; MACE's Pareto ensemble better adapts the tradeoff per batch.",
            "optimal_allocation_findings": "Combining exploration and exploitation in a batch helps, but adaptive multi-objective ensemble sampling better matches diverse problem requirements.",
            "uuid": "e2476.4",
            "source_info": {
                "paper_title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "pBO / pHCBO",
            "name_full": "Parallel Bayesian Optimization / parallel High Coverage BO",
            "brief_description": "pBO selects batches by varying weighting parameters to achieve different exploration/exploitation trade-offs; pHCBO adds a penalization scheme to avoid cluster sampling and encourage high coverage.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "pBO and pHCBO",
            "system_description": "pBO constructs batches by sampling acquisition functions using different weightings between exploration and exploitation to get diverse behavior; pHCBO further adds penalization to discourage clustered selections within a batch and improve coverage.",
            "application_domain": "Batch Bayesian optimization for expensive black-box tasks",
            "resource_allocation_strategy": "Allocate batch members by sweeping weighting parameters (e.g., between mean and variance or different acquisition components) to create diversified selection strategies across the batch.",
            "computational_cost_metric": "Number of evaluations/simulations",
            "information_gain_metric": "Weighted acquisition function values reflecting different exploration/exploitation emphasis.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Diversity across weightings yields mixture of exploratory and exploitative picks; pHCBO penalizes clusters to boost exploration coverage.",
            "diversity_mechanism": "Multiple weighting strategies across batch and an added penalization term in pHCBO to discourage cluster sampling.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed evaluation budget / batch size",
            "budget_constraint_handling": "Batch construction by choosing several weighting parameter settings and selecting points accordingly until budget for batch is filled.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Included in experimental baselines; performance varies with batch size and problem.",
            "comparison_baseline": "Compared to MACE, LP, BLCB, GPUCB-PE, etc.",
            "performance_vs_baseline": "pBO/pHCBO sometimes underperformed MACE; pHCBO's penalization reduced clustering but could still be less efficient than Pareto-ensemble sampling.",
            "efficiency_gain": "Not highlighted as superior in reported experiments; MACE typically achieved better or more stable results.",
            "tradeoff_analysis": "Paper discusses that manually designed penalization/weighting may introduce bias and can be less adaptive than Pareto-front sampling.",
            "optimal_allocation_findings": "Using multiple weighting strategies helps diversity, but Pareto-based multi-objective sampling provides a less biased and more flexible diversity mechanism.",
            "uuid": "e2476.5",
            "source_info": {
                "paper_title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble",
                "publication_date_yy_mm": "2021-06"
            }
        },
        {
            "name_short": "WEIBO",
            "name_full": "Weighted Expected Improvement Bayesian Optimization (for constraints)",
            "brief_description": "A sequential constrained Bayesian optimization method that weights the expected improvement (EI) by the probability of feasibility (PF) to favor points likely to satisfy constraints while offering expected improvement on the objective.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "WEIBO (weighted EI with PF)",
            "system_description": "Compute wEI(x)=EI(x)*PF(x) to rank candidate points; selects points that are expected to improve the objective while also being likely feasible under GP models of constraints, designed for sequential decision-making.",
            "application_domain": "Constrained black-box optimization (e.g., analog circuit sizing where constraints must be satisfied)",
            "resource_allocation_strategy": "Sequentially allocate evaluations to points maximizing weighted expected improvement, thus allocating budget preferentially to candidates with high expected objective gain and high feasibility probability.",
            "computational_cost_metric": "Number of simulations / sequential evaluations",
            "information_gain_metric": "Expected Improvement modulated by Probability of Feasibility (EI × PF) — a heuristic combining utility and feasibility likelihood.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "EI provides exploitation (and some exploration via uncertainty), PF modulates focus on feasible region; overall tends to exploit where both are high.",
            "diversity_mechanism": "None explicit for parallel batches (designed for sequential use); paper notes WEIBO is sequential and limits parallel efficiency.",
            "uses_diversity_promotion": false,
            "budget_constraint_type": "Fixed simulation budget (sequential)",
            "budget_constraint_handling": "Sequential selection until budget exhausted; does not natively parallelize without modification.",
            "breakthrough_discovery_metric": null,
            "performance_metrics": "Used as constrained baseline in experiments; paper reports MACE achieving up to 15× speedup vs WEIBO in some constrained tasks.",
            "comparison_baseline": "MACE compared against WEIBO across constrained circuit benchmarks",
            "performance_vs_baseline": "MACE outperformed WEIBO in both efficiency (Avg. # Sim) and success/quality on constrained problems, particularly when batch parallelism is available.",
            "efficiency_gain": "Example: up to 15× faster than WEIBO (charged in constrained experiments with B=15).",
            "tradeoff_analysis": "Paper notes WEIBO's sequential nature limits parallelism and thus resource utilization; weighting EI by PF can over-penalize exploration early when feasible region is unknown.",
            "optimal_allocation_findings": "For constrained problems, front-loading feasibility search (as in Constrained MACE) and then using parallel constrained acquisition/pooling is more budget-efficient than purely sequential wEI selection.",
            "uuid": "e2476.6",
            "source_info": {
                "paper_title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble",
                "publication_date_yy_mm": "2021-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Batch bayesian optimization via local penalization",
            "rating": 2,
            "sanitized_title": "batch_bayesian_optimization_via_local_penalization"
        },
        {
            "paper_title": "Parallel gaussian process optimization with upper confidence bound and pure exploration",
            "rating": 2,
            "sanitized_title": "parallel_gaussian_process_optimization_with_upper_confidence_bound_and_pure_exploration"
        },
        {
            "paper_title": "Parallelizing exploration-exploitation tradeoffs in gaussian process bandit optimization",
            "rating": 2,
            "sanitized_title": "parallelizing_explorationexploitation_tradeoffs_in_gaussian_process_bandit_optimization"
        },
        {
            "paper_title": "Bayesian optimization with unknown constraints",
            "rating": 2,
            "sanitized_title": "bayesian_optimization_with_unknown_constraints"
        },
        {
            "paper_title": "Portfolio allocation for bayesian optimization",
            "rating": 1,
            "sanitized_title": "portfolio_allocation_for_bayesian_optimization"
        },
        {
            "paper_title": "An entropy search portfolio for bayesian optimization",
            "rating": 1,
            "sanitized_title": "an_entropy_search_portfolio_for_bayesian_optimization"
        },
        {
            "paper_title": "Batch bayesian optimization via multi-objective acquisition ensemble for automated analog circuit design",
            "rating": 2,
            "sanitized_title": "batch_bayesian_optimization_via_multiobjective_acquisition_ensemble_for_automated_analog_circuit_design"
        }
    ],
    "cost": 0.020708749999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble</p>
<p>Shuhan Zhang 
Fan Yang 
Changhao Yan 
Dian Zhou 
Xuan Zeng 
An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble
1Index Terms-Analog circuit synthesisbatch Bayesian opti- mizationacquisition functionconstrained optimization problem
Bayesian optimization is a promising methodology for analog circuit synthesis. However, the sequential nature of the Bayesian optimization framework significantly limits its ability to fully utilize real-world computational resources. In this paper, we propose an efficient parallelizable Bayesian optimization algorithm via Multi-objective ACquisition function Ensemble (MACE) to further accelerate the optimization procedure. By sampling query points from the Pareto front of the probability of improvement (PI), expected improvement (EI) and lower confidence bound (LCB), we combine the benefits of state-of-theart acquisition functions to achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Based on this batch design, we further adjust the algorithm for the constrained optimization problem. By dividing the optimization procedure into two stages and first focusing on finding an initial feasible point, we manage to gain more information about the valid region and can better avoid sampling around the infeasible area. After achieving the first feasible point, we favor the feasible region by adopting a specially designed penalization term to the acquisition function ensemble. The experimental results quantitatively demonstrate that our proposed algorithm can reduce the overall simulation time by up to 74× compared to differential evolution (DE) for the unconstrained optimization problem when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15× compared to the weighted expected improvement based Bayesian optimization (WEIBO) approach, when the batch size is 15.</p>
<p>I. INTRODUCTION</p>
<p>With the scaling integrated circuit (IC) technology, circuit devices are becoming more complex and the parasite effect can no longer be ignored, which in turn complicates circuit design. Although digital circuit design has been automated for a long time, analog circuits are still designed manually. Due to the growing demands for high-performance, low-power and shortto-market circuits, designing analog circuits manually has become increasingly intractable. Therefore, automated analog circuit design tools are in urgent need.</p>
<p>Analog circuit design consists of two steps: topology selection and device sizing. In this paper, we focus on the device sizing problem, which can be formulated into both unconstrained and constrained optimization problem ( §II). Since the The  computational cost of circuit simulations can be prohibitively expensive and sometimes intractable, the evaluation budget to search for the optimum circuit design should be kept at a minimum level. What's worse, the derivatives and the convexity property of the circuit sizing problem are always inaccessible. Thus, the corresponding optimization algorithm should be able to handle the problems that are costly, noisy and multi-modal.</p>
<p>Decades of scientific efforts have been devoted to developing efficient optimization algorithms, which generally fall into two categories: model-based and simulation-based methods. The model-based approaches try to speed up the optimization process by constructing a cheap-to-evaluate substitute for the circuit simulation. Designers prescribe their prior knowledge about the analog circuit and manually derive the analytical expression to approximate the circuit performance. The automated regression algorithms will also be taken into consideration when the analytical expression can not be derived or not available [1]. One influential model-based approach is the geometric programming algorithm [2]- [4], which models the circuit performance with posynomial approximation. Other modeling strategies also exist, including artificial neural network (ANN) [5]- [7], support vector machine (SVM) [8], and Gaussian process regression (GPR) [9]- [14]. The disadvantage that prevents model-based methods from being widely used is that an accurate performance model is always hard to derive manually or requires a large set of simulation data to approximate. Besides, the generated model is not guaranteed to be accurate all over the design space. Considering that the constructed model may deviate from the real circuit performance, the optimization results can also divert from the real optimum.</p>
<p>The simulation-based approaches, on the other hand, view the circuit performance as a black-box function and optimize it on the fly. By leveraging the previously observed dataset, the simulation-based approaches guide the search by proposing the potential locations for evaluation. Since the computational cost of circuit simulations can be prohibitively expensive, the required number of evaluations should be kept at a minimum level to accelerate the optimization process. Embodiments of simulation-based approaches include the simulated annealing (SA) [15], the evolutionary algorithm [16]- [18] [19], the multiple start points (MSP) algorithm [20] [21], and the particle swarm optimization (PSO) algorithm [22]- [24]. All of these proposed algorithms try to mimic the physical or biological process to fully explore the state space and avoid stuck in the local optimum. The biggest limitation that hinders the simulation-based approaches from being widely used is their relatively low convergence rate.</p>
<p>Inspired by the above, the Bayesian optimization framework has been proposed to fully accelerate the optimization pro-cess by combining the model-based and simulation-based approaches [9]- [13], [25]- [31]. It is quite suitable for problems that don't have a closed-form expression for the objective function and can only be observed through sampled values [25]. The Bayesian optimization algorithm is especially efficient in situations when the sampled values are noisy, evaluations are incredibly expensive, or the convexity properties are unknown. Generally, there are two key elements in the Bayesian optimization framework: the probabilistic surrogate model and the acquisition function ( §III). The surrogate model incorporates our prior belief and provides a posterior distribution with the observed dataset. The prescribed prior belief is the modeling space of the possible latent function. The posterior distribution means the surrogate model provides not only the predictive means but also the corresponding uncertainty estimations. In other words, the surrogate model works as a cheap-to-evaluate substitute for the expensive latent function. The acquisition function instead describes the data generation mechanism. By leveraging the provided posterior distribution, it works as a utility-based selection criterion that helps to direct the sampling process. Instead of solely searching for the global optimum over the predictive mean, the acquisition function takes both exploration and exploitation into consideration. This means the acquisition function favors the potential region with high uncertainty estimations (exploration) and the area that is predicted to be optimal with high probability (exploitation). In this way, the acquisition function can better explore the state space and help to select better candidate points. As opposed to the model-based approaches that explore the design space offline, the Bayesian optimization algorithm updates the observed dataset incrementally and refines the surrogate model to provide a more informative posterior distribution at each iteration. It also gives us a theoretically-guaranteed global optimum after a certain number of observations. In summary, the Bayesian optimization algorithm opens a more fast and efficient lane for global optimization. Thanks to the Bayesian optimization framework, we can bypass the traffic jams of the model-based methods that greatly depend on the accuracy of the constructed model and the simulation-based approaches that have relatively low convergence rate.</p>
<p>Although the Bayesian optimization algorithm has been widely used to search the state space [9]- [12] [26] [28] [31], the sequential decision-making nature of the acquisition function prevents it from being parallelized. Without parallelism, the computational resources are not fully utilized and the optimization efficiency is greatly limited, especially in situations when the multi-core workstations are available and the circuit simulations are computationally intensive. To further tap the potential of the Bayesian optimization framework, great efforts have been made to make parallelism possible, which means the algorithm can propose several data points at each iteration. The simulation process can be distributed to different workers on the workstation. However, there are two challenges in parallelizing the Bayesian optimization algorithm. The first one is to avoid sampling redundantly around the same region, since maximizing the existing state-of-the-art acquisition functions naturally selects around the same region. The second one is to maximize the information gain of each query point in the batch. To solve these problems, most of the state-of-the-art batch Bayesian optimization algorithms design a penalization scheme that can penalize around the previously selected query points and select the candidate points in a batch one by one, like local penalization (LP) strategy [32], batched Bayesian optimization algorithm based on the lower confidence bound (BLCB) [33], parallelizable Gaussian process optimization with upper confidence bound and pure exploration (GPUCB-PE) [34], and parallelizable Bayesian optimization algorithm with high coverage consideration (pHCBO) [29]. Another limitation of the existing batch Bayesian optimization approaches is that they solely rely on a single acquisition function. Although there exist batch Bayesian optimization algorithms that can use arbitrary acquisition functions [32] [35] to facilitate the query points selection in a batch, most of the state-of-theart batch Bayesian optimization algorithms including BLCB [33] and GPUCB-PE [34] depend solely on one acquisition function, which can greatly limit their performance.</p>
<p>Although a large body of scientific literature has been published on developing a well-designed batch Bayesian optimization algorithm [27] [33]- [35], almost all these works solely focus on the unconstrained optimization problem -the constrained optimization problem is rarely considered. In this paper, we propose an efficient batch Bayesian optimization algorithm that can handle both unconstrained and constrained optimization problems. By randomly sampling data points from the Pareto front of the state-of-the-art acquisition functions, our proposed algorithm naturally maintain diversity in a batch while maximizing the information gain per observation for the unconstrained optimization problem. For the constrained optimization problem, we divide the optimization process into two stages to better explore the design space: (1) seeking the first feasible point and (2) searching for the global optimum that satisfies the constraints. In this way, we can focus on finding the first valid point before taking both constraints and objective into consideration. A preliminary version of this paper was published in [27]. We test our algorithm on four real-world analog circuits to quantitatively demonstrate the efficiency and effectiveness of our proposed algorithm. For the unconstrained optimization problem, our proposed algorithm can reduce the simulation time by up to 74× compared to DE [17] when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15× compared to WEIBO [10] when the batch size is 15.</p>
<p>The remainder of this paper is organized as follows. Section §II gives the problem formulation of the analog circuit device sizing problem. Section §III introduces the background knowledge of our proposed algorithm. Section §IV presents the fundamental challenges of parallelizing Bayesian optimization framework and the MACE algorithm for the unconstrained optimization problem. Section §IV explicitly presents our improved batch constrained Bayesian optimization algorithm and the spirit behind it. In section §VI, we report the experimental results and analytically compare the performances of our proposed algorithm with the state-of-the-art optimization algorithms. We conclude the paper in section §VII.</p>
<p>II. PROBLEM FORMULATION</p>
<p>In this section, we formulate the analog circuit device sizing problem into the unconstrained optimization problem ( §II-A) and the constrained optimization problem ( §II-B).</p>
<p>A. Unconstrained Optimization Problem</p>
<p>For a given circuit topology, the analog circuit optimization problem can be formulated as an unconstrained optimization problem by combining several circuit performances with weighting parameters:
minimize FOM = M i=1 α i f i (x),(1)
where x ∈ R d represents the input vector constructed by d design variables, f i (·) stands for the i-th performance matric of M circuit performances, α i is the i-th weighting parameter, and FOM denotes our interested Figure of Merit. For simplicity of denotation, we only consider the minimization problem in this paper.</p>
<p>B. Constrained Optimization Problem</p>
<p>The analog circuit device sizing problem can also be formulated as a constrained optimization problem:
minimize FOM s.t. c i (x) &lt; 0 ∀i ∈ 1 . . . N c ,(2)
where N c denotes the number of constraints, and c i (·) is the i-th constraint function. The target of the circuit design is to search for a circuit design that minimizes the FOM while satisfying constraints.</p>
<p>III. REVIEW OF BAYESIAN OPTIMIZATION In this section, we introduce the background knowledge of our proposed algorithm and give a brief overview of the Bayesian optimization framework based on the Gaussian process regression model in §III-A. We also outline several stateof-the-art acquisition functions and highlight their selection principles in §III-B.</p>
<p>A. Bayesian Optimization</p>
<p>Algorithm 1 Bayesian Optimization Framework</p>
<p>Require: The size of the initial dataset N init , and the maximum number of iteration N iter 1: Randomly sample a initial dataset D 0 = {X, y} 2: for t = 0 → N iter do 3:</p>
<p>Construct a Gaussian process regression model with D t</p>
<p>4:</p>
<p>x t ← argmax x α(x; D t ) 5:
y t = f (x t ) 6: D t+1 ← {D t ,
{x t , y t }} 7: end for 8: return Best y recorded after optimization Bayesian optimization framework has demonstrated significant potential in approximating the global optimum with a relatively small number of evaluations [10]- [13], [27]- [29], [33]. It gains efficiency by leveraging both the surrogate model and the acquisition function [25]. The surrogate model works as a simplified representation of the costly simulation process by taking the whole history of optimization into considerations. The informative posterior distribution provided by the surrogate model includes the predictive mean and wellcalibrated uncertainty estimation. The acquisition function prioritizes data points in the candidate pool and guides the search by proposing a sequence of promising data points. A comprehensive review of the Bayesian optimization framework can be found in [25], and the corresponding framework is presented in Algorithm 1.</p>
<p>The Gaussian process regression model is one of the most commonly used surrogate models in the Bayesian optimization framework [36]. Given a d-dimensional input design variable x, we assume the unknown objective function as y = f (x) + , where denotes the observation noise N (0, σ 2 n ). Let us assume the accumulated observations as D = {X, y}, where X represents a set of design variables X = {x 1 , x 2 , · · · , x N }, and y denotes the corresponding N observations y = {y 1 , y 2 , · · · , y N }. By capturing our prior belief about the performances of the unknown objective function with predefined mean function m(x) and kernel function k(x i , x j ), the Gaussian process regression model can provide posterior distribution for an arbitrary location x * as follow [36]:
µ(x * ) = k(x * , X)[K + σ 2 n I] −1 y σ 2 (x * ) = k(x * , x * ) − k(x * , X)[K + σ 2 n I] −1 k(X, x * ),(3)
where µ(x * ) is the predictive mean, σ(x * ) denotes the uncertainty estimation, k(x * , X) = k T (X, x * ), and K = k(X, X) is the corresponding covariance matrix. In this paper, we set m(x) = 0 and the kernel function as the squared exponential (SE) covariance function:
k SE (x i , x j ) = σ 2 f exp(− 1 2 (x i − x j ) T Λ −1 (x i − x j )). (4)
In equation (4), σ f denotes the variance, Λ = diag(l 1 , · · · , l d ) is a d × d diagonal matrix, and l i represents the length scale of the i-th dimension. The hyperparameter vector θ = (σ n , σ f , l 1 , · · · , l d ) can be determined during the model training process. For more detailed discussion about Gaussian processes, we refer readers to [36].</p>
<p>B. Acquisition Function</p>
<p>The acquisition function in the Bayesian optimization algorithm works as a cheap-to-evaluate utility function to guide the sampling decisions. Instead of exploring the design space only with the predictive mean, the acquisition function leverages the uncertainty estimation to explore the unknown area, until they are confidently ruled out as suboptimal. In this way, the acquisition function favors not only the current promising area with high confidence but also the unknown region with large uncertainty estimation. In other words, the acquisition function trades off between the exploration and exploitation based on the posterior beliefs provided by the surrogate model. There are three most widely used acquisition functions.</p>
<p>The Probability of Improvement (PI). Given the current minimum objective function value τ in the dataset, the probability of improvement function tries to measure the probability an arbitrary x exceeds the current best. The corresponding formulation is as follow [37]:
PI(x) = Φ(λ),(5)
where Φ(·) is the cumulative distribution function (CDF) of standard normal distribution. Following the suggestion of [38], we introduce a small positive jitter ξ to encourage exploration and set λ = (τ − ξ − µ(x))/σ(x). In this paper, we fix ξ as 0.001. The Expected Improvement (EI). Compared with PI that only measures the probability of improvement and treats the improvement equally, the expected improvement function tries to measure the amount of improvement upon the current best τ . By maximizing the expected improvement function, we can expect that the observation x will not only exceed the current best but also exceed the current best value at the highest magnitude. The corresponding formulation can be expressed as [39]:
EI(x) = σ(x)(λΦ(λ) + φ(λ)),(6)
where φ(·) is the probability density function (PDF) of standard normal distribution. The Lower Confidence Bound (LCB). Compared with the improvement-based strategies like PI and EI, the lower confidence bound function tries to guide the search from an optimistic perspective. With the carefully designed coefficient β, the cumulative regret is theoretically bounded [40] [41]. Thus, the convergence of the Bayesian optimization algorithm is guaranteed. The corresponding formulation is:
LCB(x) = µ(x) − βσ(x).(7)
In this paper, we follow the suggestion of [38] and set β = 2νlog(t d/2+2 π 2 /3δ), where t denotes the number of iterations, ν and δ are two user-defined parameters. In this paper, we fix ν = 0.5 and δ = 0.05. By minimizing the LCB function, the global optimum can be achieved within a limited number of observations. Apart from the above mentioned acquisition functions, there are also some other types of acquisition functions, including entropy search (ES) [42], Thompson sampling (TS) [43], predictive entropy search (PES) [44], max-value entropy search (MES) [45] and knowledge gradient (KG) [46], [47]. It is also possible to explore the state space with a portfolio of acquisition functions [48], [49].</p>
<p>IV. BATCH BAYESIAN OPTIMIZATION</p>
<p>In this section, we first identify the fundamental challenges in parallelizing Bayesian optimization algorithm ( §IV-A). We then explicitly describe our batch algorithm design and the spirit behind it ( §IV-B). Finally, we briefly review the multiobjective algorithm that we use to facilitate the optimization procedure ( §IV-C).</p>
<p>A. Challenges for Batch Bayesian Optimization</p>
<p>Since the traditional state-of-the-art acquisition functions generally work as a utility function and help to explore the design space by prioritizing the candidate data points, they naturally work in sequential mode and tend to select the same location repetitively. Due to the information gap between decisions and observations, classical works on batch Bayesian optimization generally address this problem by penalizing around the previous selections and sampling query points in a batch one by one. Compared to sequential Bayesian optimization, there are two fundamental challenges for designing an efficient parallelizable Bayesian optimization algorithm:</p>
<p>• C1: How to maximize the information gain of each data point?</p>
<p>• C2: How to maintain high diversity within each batch?</p>
<p>To tackle C1, most of the existing batch Bayesian optimization algorithms simply guide the search with traditional acquisition functions like EI, PI and LCB. However, considering that no single acquisition function can always outperform others [48], searching the design space by relying on solely one acquisition function can greatly limit the efficiency of the optimization procedure.</p>
<p>To address C2, most of the existing batch Bayesian optimization algorithms introduce a carefully designed penalization scheme to reduce sampling around the same region redundantly. For example, LP [32] proposes to penalize around the previous decisions in the batch with a manually designed local penalization strategy by introducing the Lipschitz constant as local repulsion. BLCB [33] instead heuristically encourages diversity in a batch with fake observations. Since the uncertainty estimations for arbitrary locations do not depend on the objective values, it penalizes around the previous decisions in a batch by taking advantages of the modeling property of the Gaussian process regression model. GPUCB-PE [34] is an exploratory batch design that combines the benefits of the upper confidence bound (UCB) policy and the pure exploration strategy to improve the selection efficiency from a theoretical perspective. pBO [29] proposes to select the batch with different weighting parameters to balance between the exploration and exploitation and encourage diversity in a batch. Based on pBO, pHCBO [29] further introduces a specially designed penalization scheme to prevent cluster sampling by the same weighting parameter.</p>
<p>In other words, the solution to C2 adopted by major batch designs is to propose locations for evaluation by mimicking the sequential process. Specifically, to maximize the payoff of each observation, the state-of-the-art batch policies reduce redundantly sampling over the same region and marginalize around the previous elements in a batch by introducing a carefully designed local repulsive term. Nonetheless, this solution is not good enough. The problem is that a manually designed penalization scheme always introduces human biased presumptions into the batch selection procedure, thus, tends to be overconfident about the decision-making process. For LP [32], the Lipschitz constant itself requires lots of effort to approximate and the corresponding approximation can deviate from its real value, thus, hinders the efficiency of the selection procedure. For BLCB [33], the batch policy generally assumes the objective value of each previous selection equals to the predictive mean, which deteriorates the performance of BLCB for problems that have a quick changing response surface. For GPUCB-PE [34], the specially designed pure exploration procedure focuses on reducing the systematic uncertainty and is not efficient enough for searching the global optimum. The penalization scheme proposed by pHCBO [29] is not efficient enough, since the refined surrogate model itself will provide a reduced uncertainty estimation for the new batch design and will naturally penalize around the previous batch.</p>
<p>We next describe how to truly overcome C1 and C2 by detailing our batch algorithm design. </p>
<p>B. Batch Bayesian Optimization via Acquisition Function Ensemble</p>
<p>The behavior of space exploration for a Bayesian optimization algorithm is largely determined by its acquisition function. Different acquisition functions are designed under the guidance of different selection principles, thus, tend to propose different candidate points for evaluation. Each acquisition function has its strengths and weaknesses. For example, PI selects data points that maximize the probability of improvement and is naturally biased towards exploitation. For problems with multiple local optima, PI tends to be too conservative and is not efficient enough for exploring the design space. EI instead takes the magnitude of improvement into consideration and guides the search in a greedy manner. By pickling data points that are compatible with the current best, EI tries to obtain improvement in each step. However, due to its greedy-choice property, EI function is not guaranteed to guide the search towards the right direction and convergence to the global optimum for a given number of iterations. LCB guides the search by minimizing the cumulative regret bound and theoretically guarantees the convergence after a relative number of iterations. But the exact value of the hyperparameter β is hard to define, since the requirement for exploration and exploitation could differ for different problems. For a given evaluation budget, LCB is also not guaranteed to achieve the best optimization results. In other words, no acquisition function works best in every scenario and no free lunch in optimization [48], [50]. Therefore, searching the design space by solely relying on a single acquisition function can greatly limit the efficiency and effectiveness of the optimization procedure.</p>
<p>To fill this gap, we propose a parallelizable Bayesian optimization algorithm based on the Multi-objective ACquisition function Ensemble (MACE). By sampling data points simultaneously from the Pareto front of PI, EI and LCB, we combine the benefits of the state-of-the-art acquisition functions and capture the best tradeoff between exploration and exploitation (C1). In this way, we can maximize the information gain of each selected data point without introducing human biased knowledge, while naturally encouraging diversity within the batch even without penalization scheme (C2). Other acquisition functions like KG and ES can also be incorporated into MACE framework to facilitate the space exploration procedure. The corresponding formulation is as follow:
minimize LCB(x), −PI(x), −EI(x).(8)
In Figure 1, we give an illustration of the proposed Paretooptimal set of the multi-objective acquisition function ensemble. The overall framework of the proposed parallelizable Bayesian optimization algorithm via multi-objective acquisition function ensemble is presented in Algorithm 2. Compared with most of batch Bayesian optimization algorithms that select data points one by one, our parallelizable Bayesian optimization framework naturally maintains diversity within each batch and simultaneously selects candidate points as a whole.</p>
<p>Algorithm 2 MACE algorithm</p>
<p>Require: The size of the initial dataset N init , the maximum number of iteration N iter , and the batch size B. Generate a Pareto-optimal dataset P t with equation (8) 5:
Randomly sample B data points X t = {x t,1 , x t,2 , · · · , x t,B } from P t 6: Evaluate the sampled data points y t = {f (x t,1 ), f (x t,2 ), · · · , f (x t,B ))} 7: D t ← {D t−1 , {X t ,
y t }} 8: end for 9: return Best y recorded after optimization</p>
<p>C. Multi-objective Optimization</p>
<p>To obtain the Pareto-optimal set of the acquisition function ensemble, we introduce the multi-objective optimization algorithm to facilitate the optimization procedure. The multiobjective optimization algorithms aim to optimize several objective functions at the same time, which can be expressed as:
minimize f 1 (x), f 2 (x), · · · , f m (x).(9)
Unlike the single-objective optimization, there is usually no single data point that has the best performances for all objective functions and the objective functions can contradict with each other. Instead of getting a single global optimum, we can only generate a Pareto-optimal set that no data point dominates any other. For arbitrary data points a and b, we call a dominates b if:
∀i ∈ {1, · · · , m}f i (a) ≥ f i (b) ∧ ∃i ∈ {1, · · · , m}f i (a) &gt; f i (b).(10)
We refer the entire non-dominated design space as the Paretooptimal front. A large body of literature has been published on multiobjective optimization algorithms, including non-dominated sorting genetic algorithm II (NSGA-II) [51], efficient global optimization algorithm with Gaussian processes model (ParEGO) [52], the multi-objective evolutionary algorithm based on decomposition (MOEA/D) [53], the improved strength Pareto evolutionary algorithm (SPEA2) [54], the specially designed multi-objective particle swarm optimization (OMOPSO) algorithm [55], the speed-constrained multiobjective particle swarm optimization algorithm (SMPSO) [56]. In this paper, we use differential evolution for multiobjective optimization (DEMO) algorithm [57] to obtain the Pareto-optimal set of the proposed acquisition function and facilitate the optimization procedure. In this paper, we fix the population size as 100 and the number of evaluations as 2000 during the acquisition function optimization procedure.</p>
<p>V. CONSTRAINED BATCH BAYESIAN OPTIMIZATION</p>
<p>Due to the limitations in real-world circuit design, some circuit performances should be kept below a certain level. Therefore, we propose a refined MACE algorithm to handle this constrained optimization problem. To fully maximize the information gain for both objective and constraints, we divide the optimization procedure into two stages: (1) seeking the first feasible point ( §V-A), (2) searching for the global optimum that satisfies constraints ( §V-B). §V-C summarizes our proposed batch constrained Bayesian optimization algorithm.</p>
<p>A. Seek the First Feasible Point</p>
<p>For the constrained problem, we decide to first focus on finding the first feasible point when there is no feasible point in the dataset. In this way, we can not only obtain the first feasible point more quickly but also provide more information about the feasible region for further optimization. One of the most widely used acquisition functions to favor the feasible region is the probability of feasibility (PF):
PF(x) = Nc i=1 Φ(− µ i (x) σ i (x)
).</p>
<p>By maximizing the probability of feasibility, we favor the region that is more likely to satisfy the constraints and reduce sampling around the invalid area. However, the sequential decision-making nature of the PF function prevents it from being parallelized. Also, the value of the PF function is easy to be zero even if there is only one constraint that violates the design specification, which significantly reduces its ability to prioritize candidate data points in practice. It is noteworthy that this problem deteriorates when the number of constraints increases.</p>
<p>To address this problem, we introduce two additional penalization terms to better prioritize data points and parallelize the optimization procedure. Intuitively, to reduce the amount of constraint violation, we can simply minimize the constraint that has a predictive mean higher than zero, i.e.,
minimize Nc i=1 max(0, µ i (x)).(12)
However, this fitness measurement relies solely on the predictive mean and tends to be overconfident. For the region with high uncertainty estimation, the provided predictive mean is less reliable. For the region with low predictive uncertainty, the surrogate model has much higher confidence in its prediction and the corresponding predictive mean is more trustworthy. This means that treating constraints equally is not cost-efficient and we should pay more attention to the constraint that has a higher confidence measurement. In other words, the amount of effort we should spend on making an arbitrary constraint feasible is negatively correlated with the uncertainty estimation. Therefore, we refine equation (12) and scale the predictive mean of each constraint with its estimated confidence measurement. The corresponding formulation is as follow:
minimize Nc i=1 max(0, µ i (x) σ i (x) ).(13)
With this adaptive measurement of the constraint violation, we focus on optimizing the constraint that violates the design specification and greatly speed up the process of searching for the first feasible point. However, for the region with predictive mean below zero, the adaptive constraint violation measurement can no longer handle the constraints.</p>
<p>To compensate disadvantages of the PF function and the adaptive constraint violation estimation, we select both PF and equation (13) to guide the search. In this way, the adaptive constraint violation measurement can help to prioritize data points when the PF function value is zero. The PF function can help to prioritize data points when the predictive means of all constraints satisfy design specifications and the expected constraint violation value is zero. To achieve a more extensive coverage over the Pareto front of exploration and exploitation, we also introduce the naive constraint violation measurement in equation (12) to encourage exploration. In other words, we select query points by sampling B data points from the Pareto front of the following equation:
minimize − PF(x), Nc i=1 max(0, µ i (x)), Nc i=1 max(0, µ i (x) σ i (x)
).</p>
<p>(14) Thus, we can not only increase the information gain about the feasible region for each selected data but also parallelize the optimization procedure.</p>
<p>B. Search for the Global Optimum</p>
<p>After finding out the first feasible point, we will try to search the state space by penalizing around the region with a high probability of violating the constraints and minimize the objective function by balancing between the exploration and exploitation.</p>
<p>The state-of-the-art constrained Bayesian optimization algorithm WEIBO [10] handles the design specifications by weighting the expected improvement function with the probability of feasibility. By penalizing around the region that has a much higher probability to violate the constraints, the weighted expected improvement (wEI) [58], [59] function favors the points that maximize the objective function while still satisfying the constraints.</p>
<p>However, WEIBO is designed to work in sequential mode, which greatly limits its efficiency. To cope with batch constrained Bayesian optimization, we build upon the spirit of wEI by combining our acquisition function ensemble in equation (8) with the previously designed penalization strategies in equation (14) to reduce sampling around the infeasible region. The corresponding formulation is as follow:
minimize LCB(x), −PI(x), −EI(x), −PF(x), Nc i=1 max(0, µ i (x)), Nc i=1 max(0, µ i (x) σ i (x) ).(15)
Also, to reduce sampling around the region that is more likely to violate the constraints, we propose a recommendation pruning strategy and select query points that satisfy:
Nc i=1 max(0, µ i (x) σ i (x) ) &lt;= ρ.(16)
where ρ is a user-defined parameter and we fixed it as 0.05 in this paper. With this specially designed recommendation pruning strategy, we favor the region with high probability to be valid while achieving a better tradeoff between exploration and exploitation. In this way, we can prevent the acquisition function from spending too much effort around the region that is likely to violate the constraints.</p>
<p>C. Summary</p>
<p>The overall framework of our proposed constrained batch Bayesian optimization algorithm is presented in Algorithm 3. By dividing the optimization procedure into two stages, we first focus on searching for the first feasible point before taking both constraints and objective into consideration. After obtaining the first feasible point, the updated surrogate model provides more informative posterior distribution about whether an arbitrary design satisfies constraints. Therefore, the overall time consumption of the optimization process can be significantly reduced. By adopting a new penalization scheme, we guide the decision-making process in parallel by favoring the potential area with better objective function value and penalizing around the region that is likely to violate the constraints. Thanks to the above, the global optimum that satisfies constraints can be reached after a limited number of iterations.</p>
<p>VI. EXPERIMENTAL RESULTS In this section, we experimentally evaluate the performances of MACE on four real-world analog circuits to show the benefit of our batch design. We examine the efficiency and effectiveness of MACE on both constrained and unconstrained optimization problems and quantitatively compare its performances with the state-of-the-art optimization algorithms. Our proposed MACE algorithm is implemented in Python with GPy [60] and Platypus 1 libraries. All circuit performances are generated with commercial HSPICE circuit simulator. All experiments are conducted on a Linux workstation with two Intel Xeon CPUs and 128GB memory.</p>
<p>For the unconstrained optimization problem, we first run the experiments in sequential mode to demonstrate that our acquisition function design can achieve a better tradeoff between exploration and exploitation. We also evaluate the performances of MACE in three different batch sizes (B=5, 10,15) to explore the impact of batch size and shows the effectiveness of our proposed batch design. To ensure a fair comparison, we run each Bayesian optimization algorithms 1 https://github.com/Project-Platypus/Platypus Algorithm 3 Constrained MACE algorithm Require: The size of the initial dataset N init , the maximum number of iteration N iter , and the batch size B. 1: Randomly sample an initial dataset D 0 = {X, y} 2: for t = 0 → N iter do 3: Construct Gaussian process regression model with training dataset D t</p>
<p>4:</p>
<p>if The first feasible point has been achieved then Generate a Pareto-optimal dataset with equation (15), and pruning the candidate pool with equation (16) to get P t</p>
<p>5:</p>
<p>else Generate a Pareto-optimal dataset P t with equation (14) 6: end if 7: Randomly sample B data points X t = {x t,1 , · · · , x t,B } from P t 8:</p>
<p>Evaluate the sampled data points y t = {f (x t,1 ), · · · , f (x t,B )} 9: D t+1 = {D t , {X t , y t }} 10: end for 11: return The best optimization result that satisifies the constraints.</p>
<p>with the same simulation budget, regardless of the batch size.</p>
<p>To give a quantitative measurement of the cost-effectiveness and stability of each algorithm, we run each algorithm 20 times to reduce the random fluctuations and present the optimization results in terms of the best-case, worst-case, mean, and standard deviation. To differentiate between the sequential and batch mode, we label different methods using batch policy type followed by the batch size.</p>
<p>In sequential mode, we compare the performances of MACE with 3 state-of-the-art optimization algorithms: (1) DE [17], which is an optimization strategy based on differential evolution. (2) EI [39], which is an improvement-based acquisition function for Bayesian optimization framework that guides the search by maximizing the expected improvement. (3) LCB [40], which is an optimistic strategy that traverses the design space with theoretical cumulative regret bound.</p>
<p>In batch mode, MACE is compared with 6 state-of-the-art batch policies based on the Bayesian optimization framework: (1) pBO [29], which selects the batch by introducing weighting parameter to balance between the exploration and exploitation.</p>
<p>(2) pHCBO [29], which encourages diversity in a batch by introducing a well-designed penalization scheme. (3) LP-EI [32], which explores the design space with EI and maintains diversity in a batch with local penalization term. (4) LP-LCB [32], which guides the search with LCB and introduces a local repulsive term to penalize around the early decisions in a batch. (5) BLCB [33], which reduces redundantly sampling around busy locations with hallucinated observations. (6) GPUCB-PE [34], which combines the benefits of UCB policies with pure exploration queries in the same batch to improve information gain per observation.</p>
<p>To further investigate the relative merits of different acquisition function ensembles, we also compare MACE with 3 additional batch alternatives in both sequential and batch mode: (1) PI-EI, which selects the batch from the Pareto front of PI and EI. (2) EI-LCB, which explores the design space with EI and LCB ensemble. (3) PI-LCB, which combines the benefits of PI and LCB to guide the search. For the constrained optimization problem, we compare the performances of MACE with 6 well-designed optimization algorithms: (1) WEIBO [10], is a Bayesian optimization algorithm based on the weighted expected improvement function.</p>
<p>(2) GASPAD [26], is a surrogate mode-aware evolutionary search algorithm that favors the valid region with the selectionbased constraint handling method. (3) MSP [20], is a selfadaptive multiple starting point approach that tries to approximate the global optimum by learning from the previous local search. (4) DE [17], is an optimization algorithm based on the differential evolutionary methodology. (5) PSO [61], is the particle swarm optimization methodology that tries to mimic the biological process to obtain the global optimum. (6) SA [19], is the simulated annealing algorithm that guides the search by simulating the physical process. To further investigate the impact of the two-stage approach, we also compare the performance of MACE with its one-stage counterpart oMACE, which only selects the batch with the second stage design of MACE framework.</p>
<p>To ensure a fair comparison, we respectively run each algorithm 12 times to average the random fluctuations and present the number of runs that satisfy the constraints. To quantitatively evaluate the performances of each algorithm, we present the optimization results in terms of the mean, median, best-case and worst-case results. For simplicity of comparison, we record the equivalent simulation time consumption with the equivalent number of circuit simulations on average to achieve the final circuit design (Avg. # Sim). We also record the number of runs that successfully find feasible designs for each algorithm (# Success). For each algorithm, we also present the constraint function values of the best design in all runs.</p>
<p>A. Two-Stage Operational Amplifier</p>
<p>The schematic of the two-stage operational amplifier circuit is presented in Figure 2. As proposed in [20], this circuit is implemented in a SMIC 180nm process and has a total of 10 design variables, including lengths and widths of the transistors, the resistance of the resistors, and the capacitance of the capacitors. In this circuit, we seek to maximize the open-loop gain (GAIN), the unity gain frequency (UGF), and the phase margin (PM) at the same time. We evaluate the performances of MACE on both unconstrained and constrained optimization problems.</p>
<p>Unconstrained optimization. For the unconstrained optimization problem, our formulated design specification is as To fully tap the potential of MACE, we conduct experiments in both sequential and batch mode. In batch mode, we run each algorithm with 3 different batch sizes (B=5, 10, 15) to demonstrate the impact of different batch sizes. For algorithms in Bayesian optimization literature, we set the size of the initial dataset as 20 and limit the maximum number of simulations as 270, regardless of the batch size. For DE, the simulation budget is set to be 20000. To ensure a fair comparison, we repetitively run each algorithm 20 times to eliminate the random fluctuations. The unconstrained optimization results are presented in Table I.</p>
<p>We start with examining the experimental results in sequential mode, which are presented in the top block of Table I. Clearly, MACE not only outperforms the state-of-the-art optimization algorithms on average, but also have a much smaller deviation. Compared with DE, our proposed algorithm reduces the simulation time by 74× while obtaining better optimization results. Compared with EI, LCB, PI-EI, EI-LCB and PI-LCB, MACE demonstrates a much higher convergence rate and more stable performance. This clearly shows that MACE can better guide the search by simultaneously maximizing the immediate improvement value and the cumulative regret of each candidate point in sequential mode. Another interesting observation is that the acquisition function ensembles (PI-EI, EI-LCB, PI-LCB and MACE) always obtain better optimization results compared to EI and LCB, which rely solely on a single acquisition function to search the state space. This reveals that the acquisition function ensemble can combine the benefits of several acquisition functions and increase the information gain per data point. Now we move on to analyze the performances of MACE in batch mode. As expected, MACE consistently outperforms the state-of-the-art batch policies for the same batch size, which demonstrates both the efficiency and effectiveness of our batch design. Compared with pBO, pHCBO, LP-EI, LP-LCB, BLCB and GPUCB-PE that select data points iteratively and greedily for each batch, MACE simply constructs the batch at once by randomly selecting query points from the Pareto front of PI, EI and LCB. In this way, MACE combines the strengths of several state-of-the-art acquisition functions. Instead of designing the penalization scheme manually and penalizing around the early observations in a batch with human biased presumptions, MACE naturally maintains diversity within the same batch. The optimization results of pBO and pHCBO, when the batch size is 5, further reveal that manually design penalization scheme sometimes can even hinder the optimization process. Compared with PI-EI, EI-LCB and PI-LCB that guide the search with only two acquisition functions, MACE consistently achieves better optimization results across different batch sizes. This indicates that different acquisition functions have different characteristics in selecting query points, and the efficiency of the acquisition function ensemble improves with the increasing number of selected acquisition functions. The comparably small deviations across different batch sizes also demonstrate the stability and robustness of MACE.</p>
<p>To further analyze the impact of batch size, we compare the optimization results across different batch sizes. From an information gain perspective, the performances in sequential mode can always be seen as a baseline for the batch policies with the same acquisition function for decision selections. The optimization results of EI, LCB, PI-EI, EI-LCB and PI-LCB in sequential mode are consistently better compared to its batch counterparts. This is because the two-stage operational amplifier circuit requires more exploitation than exploration to optimization. Compared with all the other batch policies the optimization results of which deteriorate quickly with the increase of the batch size, MACE demonstrates strong stability and robustness in terms of both the average results and the standard deviations. For BLCB, despite getting similar results for different batch sizes, its consistently outperformed optimization results of BLCB reveal that searching the design space by solely relying on a single utility function can greatly limit the efficiency of decision making. The experimental results of the unconstrained optimization problem provide quantitative evidence for the efficiency and robustness of our batch design. Another noteworthy phenomenon is that with a fixed number of selected acquisition functions, different acquisition function combinations have different behavior patterns. With the increase of the batch size, the performance of PI-LCB deteriorates much slower than PI-EI and EI-LCB. This is due to the fact that PI tends to do more exploitation, LCB tends to do more exploration and EI stays relatively in the middle. Thus, PI-LCB naturally has better coverage over the exploitation-exploration Pareto front than PI-EI and EI-LCB. With the increase of the batch size, PI-EI and EI-LCB become too conservative and PI-LCB starts to outperform them.</p>
<p>Constrained optimization. For the constrained optimization problem, our formulated design specification is as follow:
maximize GAIN s.t. U GF &gt; 40M Hz, P M &gt; 60 o .(18)
In this experiment, we compare the performances of MACE and oMACE with the state-of-the-art constrained optimization algorithms, including WEIBO, GASPAD, MSP, DE, PSO and SA. To fully explore the potential of our constrained batch design, we test both MACE and oMACE in 3 different batch sizes (B=5, 10, 15) and fix the maximum number of simulations as 620, regardless of the batch size. For MACE and oMACE, the size of the initial dataset is 20. For WEIBO, we randomly sample 20 initial data point and set the simulation budget as 200. For GASPAD, we limit the maximum number of simulations as 500. As for the rest of the algorithms, we limit the simulation budget as 10000. To reduce the random fluctuations and fairly compare the experimental results, we repetitively run each algorithm 12 times. The corresponding optimization results are presented in Table II.</p>
<p>It is worth notice that the optimization results of both PSO and SA fail to meet the constraints, which means the design specification is hard to satisfy. Compared with DE, MSP, GAS-PAD and WEIBO, MACE respectively reduces the simulation time by up to 185×, 164×, 12× and 5×, while obtaining relatively better optimization results. The experimental results quantitatively demonstrate that the optimization process can be significantly sped up by proposing a batch of data points at each iteration and assigning the circuit simulation to different workers. Also, the fact that MACE consistently outperforms its one-stage counterpart regardless of the batch size shows that the two-stage policy can greatly reduce sampling around the infeasible region, thus, accelerate the optimization procedure. We further investigate the impact of different batch sizes. Although the performance of MACE deteriorates slightly with the increase of the batch size, MACE still obtains much better optimization results in terms of mean, median, best-case and worst-case results when the batch size 15. This clearly demonstrates the robustness and stability of our proposed algorithm.  Fig. 3. The schematic of the class-E power amplifier circuit, which is reproduced from [27].</p>
<p>B. Class-E Power Amplifier</p>
<p>The second circuit we evaluate MACE with is the class-E power amplifier circuit, which is shown in Figure 3. Implemented in a 180nm TSMC process, this circuit has a total of 12 design variables. The corresponding circuit performances are generated via the commercial HSPICE circuit simulator. For this circuit, our target of design is to maximize the power-added efficiency (PAE) and the output power (Pout) simultaneously.</p>
<p>Unconstrained optimization. Our formulated design specification for the unconstrained optimization problem is:
maximize 3 × P AE + P out.(19)
To ensure a fair comparison, we run each algorithm 20 times to average the random fluctuations. For DE, we set the maximum number of simulations as 15000. For the rest of the Bayesian optimization algorithms, we compare the performances with a fixed simulation budget across different batch sizes to fully explore the potential of our proposed algorithm. Specifically, we set the size of the initial dataset as 20 and limit the maximum number of simulations as 470. The corresponding unconstrained optimization results are presented in Table III.</p>
<p>We start with analyzing the performances of MACE in sequential mode presented in the top block of Table III. Compared with EI and LCB, the acquisition function ensembles achieve much better optimization results with the same simulation budget. This observation again shows that the sampling efficiency of the acquisition function ensembles is much higher than the state-of-the-art acquisition functions. By searching the design space with several acquisition functions, the acquisition function ensembles can combine the benefits of the state-of-the-art utility functions and better guide the search. The fact that MACE consistently outperforms PI-EI, EI-LCB and PI-LCB further confirms that the performance of the acquisition function ensemble improves with the increasing number of acquisition functions. Compared with DE, MACE reduces the simulation time by up to 42× while obtaining more competitive optimization results. The superior performance of MACE in terms of the best-case result further shows that MACE has great potential in selecting the query point. Our results demonstrate that MACE can lead the search more efficiently and effectively.</p>
<p>We now move on to investigate the performances of MACE with the state-of-the-art batch policies in the batch mode. To fully exploit the latent capacity of MACE and demonstrate Overall, MACE consistently outperforms the state-of-the-art batch policies with respect to the same batch size. For pBO, pHCBO, LP-EI and LP-LCB, the corresponding optimization results for different batch sizes are comparably the same and sometimes even better than their sequential counterparts. This reveals that batch policy doesn't necessarily lead to penalized information gain per data point. The fact that the performances of BLCB improve with the increase of the batch size further demonstrates that a relative magnitude of batch size encourages exploration. Besides, the class-E power amplifier circuit actually requires more exploration than exploitation to fully search the design space. For GPUCB-PE, the optimization results indicate that GPUCB-PE naturally encourages more exploitation than exploration. As for MACE, the consistently competitive performances, regardless of the batch size, demonstrate that MACE can achieve a better tradeoff between exploration and exploitation. Instead of relying on a single acquisition function to guide the search, MACE selects query points from the Pareto front of the acquisition function ensemble. Therefore, MACE can always select more informative data point to better facilitate the optimization process.</p>
<p>The experimental results presented in Table III demonstrate the efficiency and effectiveness of MACE for both sequential and batch mode. Constrained optimization. Our formulated design specifi- cation for the constrained optimization problem is:
maximize P AE s.t. P out &gt; 2.0dBm,(20)
In this experiment, we test both MACE and oMACE with a fixed simulation budget for different batch sizes to explore the impact of batch size. For MACE, oMACE and WEIBO, we randomly sample 20 initial data points and set the maximum number of simulations as 920. For GASPAD, the simulation budget is limited to 600. For the rest of the algorithms, the maximum number of simulations is 5000. To average the random fluctuations, we run each algorithm 12 times and present the mean, median, best-case and worst-case results. The corresponding constrained optimization results are presented in Table IV. As expected, MACE achieves much better optimization results than the state-of-the-art optimization algorithms. The fact that SA and oMACE fail to successfully find a feasible design in all runs shows that the design specification is hard to satisfy. Considering that MACE successfully obtains valid designs in all runs and consistently outperforms oMACE, the two-stage approach can greatly help to relieve the burden of searching for the feasible region. Compared with WEIBO, GASPAD, MSP, DE and PSO, MACE reduces the simulation time by up to 11×, 9×, 49×, 70× and 69× respectively. Considering that GASPAD achieves better optimization results than WEIBO while requiring less simulation time on average, the response surface of the design specification can be well approximated with a relatively small number of data points. This also reveals that the constrained optimization of the class-E power amplifier circuit requires more exploitation than exploration. Another noteworthy phenomenon is that MACE exhibits much higher sampling efficiency compared to WEIBO when the batch size is 15. Considering that both MACE and WEIBO are assigned with the same amount of simulation budget, this suggests that our proposed batch policy can greatly accelerate the optimization process without penalization of the information gain per data points. </p>
<p>C. Low-Power Three-Stage Amplifier</p>
<p>The third circuit for testing is the low-power three-stage amplifier circuit. The corresponding schematic is presented in Figure 4, which is proposed in [62]. In this circuit, there are a total of 24 design variables, including the lengths and widths of the transistors, the capacitance and resistance and the bias current. For this circuit, we only run experiments on the constrained optimization problem.</p>
<p>Constrained optimization. Our formulated constrained design specification is: </p>
<p>where Iq is the static current, GAIN denotes the DC gain, UGF represents the unit gain frequency, PM stands for the phrase margin, GM means gain margin, SRR and SRF refers to the rising and falling slew rate.  Fig. 5. The schematic of the charge pump circuit, which is reproduced from [20].</p>
<p>In this experiment, we randomly sample 20 initial data points and set the maximum number of simulations as 920 for both MACE and oMACE, regardless of the batch size. For WEIBO, we initially sample 20 data points and limit the simulation budget as 720. For GASPAD, the simulation budget is limited as 1000. For the rest of the algorithms, we fix the maximum number of simulations as 3000. To ensure a fair comparison, we run each algorithm 12 times to reduce the random fluctuations. The constrained optimization results of the low-power three-stage amplifier circuit are presented in Table V.</p>
<p>In this experiment, both SA and MSP fail to meet the design specification in all runs. Compared with WEIBO, GASPAD, DE and PSO, MACE reduce the simulation time consumption by up to 6×, 13×, 42× and 42× respectively, while achieving higher sampling efficiency. The fact that both MACE and oMACE consistently outperform the state-of-the-art optimization algorithms again confirms the efficiency and effectiveness of our proposed batch policy. Another interesting observation is that the performance of oMACE deteriorates quickly with the increase of the batch size, while MACE achieves relatively the same optimization results across different batch sizes. This clearly demonstrates the robustness and effectiveness of the two-stage approach.</p>
<p>D. Charge Pump</p>
<p>The last circuit we evaluate MACE with is the charge pump circuit, the schematic of which is presented in Figure 5. Implemented in a SMIC 40nm process, there are a total of 36 design variables. In this circuit, our target of design is to let the current difference between M1 and M2 stay under 40µA. For this circuit, we only run experiments on the constrained optimization problem.</p>
<p>Constrained optimization. The formulated design specification for the constrained optimization problem is as follow:
minimize F OM s.t. diff 1 &lt; 20µA, diff 2 &lt; 20µA, diff 3 &lt; 5µA, diff 4 &lt; 5µA, deviation &lt; 5µA,(22)
where (23) In this experiment, we run both MACE and oMACE in 3 different batch sizes and compare experimental results with WEIBO, GASPAD, DE, PSO, and SA. For MACE and oMACE, we set the size of the initial dataset as 20 and fix the simulation budget as 830, regardless of the batch size. For WEIBO, we initially sample 120 data points and limit the overall simulation budget as 1000. For the rest of the algorithms, we fix the maximum number of simulations as 2500. Again, to ensure a fair comparison, we run each algorithm 12 times to average the random fluctuations. The corresponding mean, median, best-case and worst-case optimization results are presented in Table VI.
                                 diff 1 = max ∀P V
In this experiment, PSO and SA fail to find feasible designs in all 12 runs. Compared with WEIBO, GASPAD, MSP and DE, MACE again achieves much better optimization results while reducing the simulation time by 15×, 46×, 31× and 30×. This shows that MACE has much higher convergence rate and explore the design space more efficiently than the state-of-the-art optimization algorithms. One interesting observation is that GASPAD achieves worse optimization results compared to WEIBO while requiring much more simulation time on average. This indicates that the response surface of the corresponding design specification is multi-modal and requires a large set of data points to approximate. The fact that MACE consistently outperforms oMACE across different batch sizes again suggests that the two-stage policy helps to provide more information about the feasible region and can greatly accelerate the optimization procedure.</p>
<p>VII. CONCLUSION</p>
<p>In this paper, we propose a batch Bayesian optimization algorithm based on the acquisition function ensemble. Our algorithm can handle both unconstrained and constrained optimization problems. By sampling data points from the Pareto front of PI, EI and LCB, we combine the benefits of state-ofthe-art acquisition functions and achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Fueled with this explicitly designed batch policy, we further refine the algorithm to handle the constrained optimization problem by dividing the optimization procedure into two stages. By first focusing on finding the feasible designs, we manage to collect more information about the feasible region. We further reduce sampling around the invalid region while exploring the potential area by adopting a carefully designed penalization term. The experimental results demonstrate the robustness and cost-effectiveness of our proposed algorithm.</p>
<p>Fig. 1 .
1An illustration of the posterior distribution provided by the Gaussian process regression model and the corresponding Pareto-optimal set of the multi-objective acquisition function ensemble.</p>
<p>1 :
1Randomly sample an initial dataset D 0 = {X, y} 2: for t = 0 → N iter do</p>
<p>Fig. 2 .
2The schematic of the two-stage operational amplifier circuit, which is reproduced from[20].</p>
<p>Fig. 4 .
4The schematic of the low-power three-stage amplifier circuit, which is reproduced from[10].</p>
<p>T (I M 1,max − I M 1,avg ), diff 2 = max ∀P V T (I M 1,avg − I M 1,min ), diff 3 = max ∀P V T (I M 2,max − I M 2,avg ), diff 4 = max ∀P V T (I M 2,avg − I M 2,min ), diff = 4 i=1 diff i , deviation = max ∀P V T (|I M 1,avg − 40µA|) + max ∀P V T (|I M 2,avg − 40µA|), F OM = 0.3 × diff + 0.5 × deviation.</p>
<p>preliminary version has been presented at International Conference on Machine Learning (ICML) in 2018. This research is supported partly by National Key R&amp;D Program of China 2020YFA0711900, 2020YFA0711901, National Natural Science Foundation of China (NSFC) research projects 61822402, 61774045, 61974032, 61929102 and 62011530132. *Corresponding authors: Fan Yang; Xuan Zeng. Email: {yangfan, xzeng}@fudan.edu.cn S. Zhang, F. Yang, C. Yan and X. Zeng are with State Key Lab of ASIC &amp; System, Microelectronics Department, Fudan University, China. D. Zhou is with the Department of Electrical Engineering, The University of Texas at Dallas, USA.</p>
<p>TABLE I THE
IUNCONSTRAINED OPTIMIZATION RESULTS OF THE TWO-STAGE OPERATIONAL AMPLIFIER CIRCUIT.Algo 
Best 
Worst 
Mean 
Std 
DE 
685.44 680.65 682.19 
1.56 
LCB 
690.35 685.02 688.68 
1.51 
EI 
690.29 670.49 688.07 
4.98 
PI-EI 
690.35 690.27 690.34 
0.03 
EI-LCB 
690.35 690.27 690.34 
0.03 
PI-LCB 
690.35 688.09 690.23 
0.49 
MACE 
690.36 690.27 690.34 
0.03 
pBO-5 
690.36 688.00 689.59 
1.01 
pHCBO-5 
690.36 615.27 678.47 20.82 
LP-EI-5 
690.15 664.68 685.15 
8.34 
LP-LCB-5 
690.18 660.42 685.25 
8.87 
BLCB-5 
690.33 685.70 688.69 
1.28 
GPUCB-PE-5 
690.35 618.41 679.33 23.52 
PI-EI-5 
690.35 653.74 687.72 
9.42 
EI-LCB-5 
690.35 680.24 689.35 
2.56 
PI-LCB-5 
690.35 689.85 690.28 
0.13 
MACE-5 
690.36 690.27 
690.33 
0.03 
pBO-10 
690.36 535.80 676.45 39.49 
pHCBO-10 
690.36 641.83 685.67 12.24 
LP-EI-10 
689.93 633.32 677.67 17.25 
LP-LCB-10 
690.12 663.58 685.37 
6.51 
BLCB-10 
690.36 685.09 688.77 
1.47 
GPUCB-PE-10 690.28 575.75 658.14 37.18 
PI-EI-10 
690.35 655.14 684.33 10.91 
EI-LCB-10 
690.27 673.54 686.31 
4.98 
PI-LCB-10 
690.35 533.55 682.30 34.13 
MACE-10 
690.36 685.45 
690.00 
1.26 
pBO-15 
690.35 558.18 673.52 34.15 
pHCBO-15 
690.36 608.29 681.25 22.71 
LP-EI-15 
689.70 609.82 677.53 20.54 
LP-LCB-15 
687.88 641.51 668.71 15.77 
BLCB-15 
690.36 685.78 688.73 
1.36 
GPUCB-PE-15 689.35 612.63 672.49 22.15 
PI-EI-15 
690.32 500.57 659.80 56.18 
EI-LCB-15 
690.07 654.13 678.93 11.55 
PI-LCB-15 
690.35 522.36 681.03 36.45 
MACE-15 
690.35 682.97 
688.96 
2.28 </p>
<p>follow: </p>
<p>maximize 1.2 × GAIN + 10.0 × U GF + 1.6 × P M. (17) </p>
<p>TABLE II THE
IICONSTRAINED OPTIMIZATION RESULTS OF THE TWO-STAGE OPERATIONAL AMPLIFIER CIRCUIT.Algo 
oMACE-5 MACE-5 oMACE-10 MACE-10 oMACE-15 MACE-15 WEIBO GASPAD 
MSP 
DE 
PSO 
SA 
UGF/MHz 
40.00 
40.01 
40.00 
40.00 
40.02 
40.03 
40.03 
40.20 
40.01 
40.10 
fail 
fail 
PM/ o 
61.03 
61.03 
60.98 
60.99 
60.94 
60.93 
60.87 
60.83 
61.43 
60.95 
fail 
fail 
GAIN(mean) 
90.05 
90.06 
89.93 
89.93 
89.75 
89.94 
89.61 
89.24 
89.42 
89.42 
fail 
fail 
GAIN(median) 
90.05 
90.11 
89.96 
90.00 
89.96 
90.07 
89.73 
89.24 
89.35 
89.38 
fail 
fail 
GAIN(best) 
90.18 
90.18 
90.14 
90.15 
90.16 
90.18 
90.15 
89.95 
89.95 
89.67 
fail 
fail 
GAIN(worst) 
89.89 
89.84 
89.55 
89.57 
88.61 
89.81 
87.71 
88.50 
89.00 
89.09 
fail 
fail 
Avg. # Sim 
103 
74 
53 
39 
36 
32 
170 
385 
5263 
5931 
fail 
fail </p>
<h1>Success</h1>
<p>12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 12/12 
0/12 
0/12 </p>
<p>TABLE III THE
IIIUNCONSTRAINED OPTIMIZATION RESULTS OF THE CLASS-E POWER AMPLIFIER CIRCUIT.Algo 
Best Worst Mean 
Std 
DE 
4.56 
4.33 
4.43 
0.08 
LCB 
4.10 
3.59 
3.89 
0.14 
EI 
4.13 
3.52 
3.85 
0.19 
PI-EI 
5.80 
4.20 
4.63 
0.34 
EI-LCB 
5.11 
4.09 
4.45 
0.25 
PI-LCB 
4.83 
3.99 
4.49 
0.26 
MACE 
6.12 
3.85 
4.79 
0.57 
pBO-5 
4.61 
3.76 
4.17 
0.19 
pHCBO-5 
4.42 
3.66 
4.16 
0.16 
LP-EI-5 
4.17 
3.51 
3.87 
0.19 
LP-LCB-5 
4.26 
3.43 
3.82 
0.26 
BLCB-5 
4.73 
4.07 
4.23 
0.14 
GPUCB-PE-5 
4.51 
3.62 
4.17 
0.19 
PI-EI-5 
4.60 
3.55 
4.22 
0.27 
EI-LCB-5 
5.88 
3.79 
4.43 
0.51 
PI-LCB-5 
4.98 
3.65 
4.49 
0.31 
MACE-5 
4.83 
3.76 
4.51 
0.31 
pBO-10 
4.34 
3.80 
4.11 
0.16 
pHCBO-10 
4.82 
3.79 
4.17 
0.23 
LP-EI-10 
4.60 
3.49 
3.86 
0.28 
LP-LCB-10 
4.26 
3.59 
3.88 
0.20 
BLCB-10 
4.46 
3.94 
4.26 
0.13 
GPUCB-PE-10 
4.40 
3.66 
4.04 
0.21 
PI-EI-10 
4.82 
2.85 
4.13 
0.52 
EI-LCB-10 
4.97 
3.39 
4.29 
0.38 
PI-LCB-10 
5.65 
3.61 
4.46 
0.45 
MACE-10 
5.54 
3.73 
4.64 
0.46 
pBO-15 
4.61 
3.87 
4.17 
0.19 
pHCBO-15 
4.31 
3.67 
4.10 
0.16 
LP-EI-15 
4.33 
3.50 
3.87 
0.23 
LP-LCB-15 
4.28 
3.58 
3.85 
0.17 
BLCB-15 
4.70 
4.10 
4.30 
0.13 
GPUCB-PE-15 
4.20 
3.54 
3.93 
0.17 
PI-EI-15 
4.78 
3.06 
4.14 
0.43 
EI-LCB-15 
4.96 
3.67 
4.20 
0.30 
PI-LCB-15 
4.77 
3.31 
4.28 
0.37 
MACE-15 
5.41 
3.39 
4.32 
0.42 </p>
<p>the impact of different batch sizes, we run each algorithm 
in 3 different batch sizes (B=5, 10, 15) and compare the 
experimental results with the state-of-the-art batch policies. </p>
<p>TABLE IV THE
IVCONSTRAINED OPTIMIZATION RESULTS OF THE CLASS-E POWER AMPLIFIER CIRCUIT. Algo oMACE-5 MACE-5 oMACE-10 MACE-10 oMACE-15 MACE-15 WEIBO GASPADTABLE V THE CONSTRAINED OPTIMIZATION RESULTS OF THE LOW-POWER THREE-STAGE AMPLIFIER CIRCUIT, THE RESULTS OF MSP, DE, PSO AND SA COME FROM[10].MSP 
DE 
PSO 
SA 
Pout/dBm 
2.11 
3.36 
2.23 
2.73 
3.04 
2.94 
2.04 
2.21 
2.31 
2.21 
2.33 
2.01 
PAE(mean) 
0.83 
0.85 
0.83 
0.84 
0.80 
0.83 
0.74 
0.78 
0.73 
0.73 
0.71 
0.66 
PAE(median) 
0.80 
0.81 
0.79 
0.80 
0.79 
0.80 
0.75 
0.75 
0.71 
0.72 
0.70 
0.67 
PAE(best) 
0.97 
1.06 
0.97 
1.10 
0.92 
1.03 
0.76 
0.95 
0.98 
0.77 
0.86 
0.72 
PAE(worst) 
0.75 
0.76 
0.73 
0.70 
0.75 
0.73 
0.73 
0.72 
0.66 
0.69 
0.66 
0.60 
Avg. # Sim 
156 
124 
63 
70 
51 
52 
583 
516 
2580 
3657 
3610 
2986 </p>
<h1>Success</h1>
<p>12/12 
12/12 
11/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 12/12 12/12 
7/12 </p>
<p>Algo 
oMACE-5 MACE-5 oMACE-10 MACE-10 oMACE-15 MACE-15 WEIBO GASPAD 
MSP 
DE 
PSO 
SA 
GAIN/dB 
101.36 
102.37 
101.25 
102.65 
102.58 
101.08 
100.67 
100.82 
100.81 102.73 102.39 102.49 
UGF/MHz 
0.92 
0.92 
0.92 
0.92 
0.93 
0.92 
0.93 
0.94 
0.98 
0.96 
0.96 
1.05 
PM/ o 
52.52 
52.50 
52.51 
52.70 
53.50 
52.51 
53.10 
52.66 
53.22 
54.62 
54.25 
56.70 
GM/dB 
19.80 
19.53 
19.86 
19.50 
19.55 
19.50 
19.58 
19.83 
22.30 
20.62 
21.32 
20.92 
SR+(V/µs) 
0.21 
0.20 
0.20 
0.20 
0.24 
0.20 
0.19 
0.21 
0.23 
0.21 
0.23 
0.25 
SR-(V/µs) 
0.41 
0.37 
0.51 
0.43 
0.37 
0.48 
0.41 
0.54 
0.51 
0.54 
0.51 
0.49 
Iq(mean) 
31.67 
30.61 
33.01 
30.67 
33.10 
29.91 
37.78 
35.08 
49.60 
41.26 
44.22 
59.78 
Iq(median) 
30.87 
29.83 
32.41 
29.75 
33.38 
30.24 
34.90 
35.64 
48.29 
40.70 
43.16 
53.56 
Iq(best) 
29.34 
27.27 
28.26 
27.18 
29.24 
26.54 
29.28 
29.26 
32.06 
37.32 
37.18 
46.34 
Iq(worst) 
36.74 
35.65 
38.22 
34.98 
36.59 
32.80 
49.89 
38.77 
75.32 
46.09 
59.56 
83.48 
Avg. # Sim 
166 
172 
85 
80 
58 
57 
396 
743 
2163 
2400 
2417 
620 </p>
<h1>Success</h1>
<p>12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
9/12 
12/12 
12/12 
5/12 </p>
<p>TABLE VI THE
VICONSTRAINED OPTIMIZATION RESULTS OF THE CHARGE PUMP CIRCUIT, THE RESULTS OF WEIBO, GASPAD, MSP, DE, PSO AND SA COME FROM [10]. MACE-5 oMACE-10 MACE-10 oMACE-15 MACE-15 WEIBO GASPADAlgo 
oMACE-5 MSP 
DE 
PSO 
SA 
diff1 
5.43 
5.59 
5.56 
5.50 
5.98 
5.86 
6.58 
6.83 
17.81 
17.97 
fail 
fail 
diff2 
4.75 
4.44 
4.50 
4.48 
5.10 
4.45 
5.30 
5.28 
16.82 
15.49 
fail 
fail 
diff3 
0.06 
0.15 
0.12 
0.06 
0.10 
0.14 
0.24 
0.29 
1.51 
1.84 
fail 
fail 
diff4 
0.06 
0.22 
0.24 
0.07 
0.12 
0.18 
0.37 
0.40 
2.57 
3.56 
fail 
fail 
deviation 
0.39 
0.20 
0.27 
0.37 
0.22 
0.18 
0.41 
0.33 
0.38 
0.39 
fail 
fail 
FOM(mean) 
3.47 
3.43 
3.52 
3.49 
3.75 
3.65 
3.95 
4.00 
11.80 
11.85 
fail 
fail 
FOM(median) 
3.44 
3.44 
3.53 
3.40 
3.70 
3.55 
3.97 
4.99 
11.67 
12.31 
fail 
fail 
FOM(best) 
3.29 
3.22 
3.26 
3.28 
3.50 
3.28 
3.48 
3.74 
8.26 
9.29 
fail 
fail 
FOM(worst) 
3.77 
3.61 
3.95 
3.88 
4.16 
4.30 
4.48 
4.43 
14.03 
13.40 
fail 
fail 
Avg. # Sim 
131 
139 
73 
72 
44 
50 
790 
2328 
1599 
1538 
fail 
fail </p>
<h1>Success</h1>
<p>12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 
12/12 12/12 
0/12 
0/12 </p>
<p>dnb </p>
<p>i5u </p>
<p>M2 </p>
<p>pd </p>
<p>cpout </p>
<p>M1 </p>
<p>QUENCH 
Vref 
Vin 
Vout 
upb </p>
<p>VDD </p>
<p>pdb </p>
<p>i10u </p>
<p>up 
up </p>
<p>dn </p>
<p>upb </p>
<p>dnb 
dn </p>
<p>Vin </p>
<p>Vref 
Vout </p>
<p>VDD </p>
<p>Hierarchical modeling, optimization, and synthesis for system-level analog and rf designs. R A Rutenbar, G G Gielen, J Roychowdhury, Proceedings of the IEEE. 953R. A. Rutenbar, G. G. Gielen, and J. Roychowdhury, "Hierarchical modeling, optimization, and synthesis for system-level analog and rf designs," Proceedings of the IEEE, vol. 95, no. 3, pp. 640-669, 2007.</p>
<p>Geometric programming for circuit optimization. S P Boyd, S J Kim, Proceedings of the 2005 international symposium on Physical design. the 2005 international symposium on Physical designS. P. Boyd and S. J. Kim, "Geometric programming for circuit optimiza- tion," in Proceedings of the 2005 international symposium on Physical design, 2005, pp. 44-46.</p>
<p>A tutorial on geometric programming. S Boyd, S.-J Kim, L Vandenberghe, A Hassibi, Optimization and engineering. 8167S. Boyd, S.-J. Kim, L. Vandenberghe, and A. Hassibi, "A tutorial on geometric programming," Optimization and engineering, vol. 8, no. 1, p. 67, 2007.</p>
<p>Ensemble methods for convex regression with applications to geometric programming based circuit design. L Hannah, D Dunson, arXiv:1206.4645arXiv preprintL. Hannah and D. Dunson, "Ensemble methods for convex regression with applications to geometric programming based circuit design," arXiv preprint arXiv:1206.4645, 2012.</p>
<p>Design optimization of analog integrated circuits by using artificial neural networks. A Jafari, S Sadri, M Zekri, " in 2010 international conference of soft computing and pattern recognition. IEEEA. Jafari, S. Sadri, and M. Zekri, "Design optimization of analog inte- grated circuits by using artificial neural networks," in 2010 international conference of soft computing and pattern recognition. IEEE, 2010, pp. 385-388.</p>
<p>An artificial neural network assisted optimization system for analog design space exploration. Y Li, Y Wang, Y Li, R Zhou, Z Lin, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. Y. Li, Y. Wang, Y. Li, R. Zhou, and Z. Lin, "An artificial neural network assisted optimization system for analog design space exploration," IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2019.</p>
<p>Using Artificial Neural Networks for Analog Integrated Circuit Design Automation. J P Rosa, D J Guerra, N C Horta, R M Martins, N C Lourenço, Springer NatureJ. P. Rosa, D. J. Guerra, N. C. Horta, R. M. Martins, and N. C. Lourenço, Using Artificial Neural Networks for Analog Integrated Circuit Design Automation. Springer Nature, 2019.</p>
<p>Nonlinear soft fault diagnosis of analog circuits based on rcca-svm. Y Li, R Zhang, Y Guo, P Huan, M Zhang, IEEE Access. 8Y. Li, R. Zhang, Y. Guo, P. Huan, and M. Zhang, "Nonlinear soft fault diagnosis of analog circuits based on rcca-svm," IEEE Access, vol. 8, pp. 60 951-60 963, 2020.</p>
<p>An efficient bayesian optimization approach for analog circuit synthesis via sparse gaussian process modeling. B He, S Zhang, F Yang, C Yan, D Zhou, X Zeng, 2020 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). IEEEB. He, S. Zhang, F. Yang, C. Yan, D. Zhou, and X. Zeng, "An efficient bayesian optimization approach for analog circuit synthesis via sparse gaussian process modeling," in 2020 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). IEEE, 2020, pp. 67-72.</p>
<p>An efficient bayesian optimization approach for automated optimization of analog circuits. W Lyu, P Xue, F Yang, C Yan, Z Hong, X Zeng, D Zhou, IEEE Transactions on Circuits and Systems I: Regular Papers. 656W. Lyu, P. Xue, F. Yang, C. Yan, Z. Hong, X. Zeng, and D. Zhou, "An efficient bayesian optimization approach for automated optimization of analog circuits," IEEE Transactions on Circuits and Systems I: Regular Papers, vol. 65, no. 6, pp. 1954-1967, 2017.</p>
<p>Bayesian optimization approach for analog circuit synthesis using neural network. S Zhang, W Lyu, F Yang, C Yan, D Zhou, X Zeng, 2019 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). IEEES. Zhang, W. Lyu, F. Yang, C. Yan, D. Zhou, and X. Zeng, "Bayesian optimization approach for analog circuit synthesis using neural network," in 2019 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE). IEEE, 2019, pp. 1463-1468.</p>
<p>An efficient multi-fidelity bayesian optimization approach for analog circuit synthesis. S Zhang, W Lyu, F Yang, C Yan, D Zhou, X Zeng, X Hu, Proceedings of the 56th Annual Design Automation Conference. the 56th Annual Design Automation ConferenceACM64S. Zhang, W. Lyu, F. Yang, C. Yan, D. Zhou, X. Zeng, and X. Hu, "An efficient multi-fidelity bayesian optimization approach for analog circuit synthesis," in Proceedings of the 56th Annual Design Automation Conference 2019. ACM, 2019, p. 64.</p>
<p>Bayesian methods for the yield optimization of analog and sram circuits. S Zhang, F Yang, D Zhou, X Zeng, 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEES. Zhang, F. Yang, D. Zhou, and X. Zeng, "Bayesian methods for the yield optimization of analog and sram circuits," in 2020 25th Asia and South Pacific Design Automation Conference (ASP-DAC). IEEE, 2020, pp. 440-445.</p>
<p>Analog/rf postsilicon tuning via bayesian optimization. R Pan, J Tao, Y Su, D Zhou, X Zeng, X Li, ACM Transactions on Design Automation of Electronic Systems (TODAES). 251R. Pan, J. Tao, Y. Su, D. Zhou, X. Zeng, and X. Li, "Analog/rf post- silicon tuning via bayesian optimization," ACM Transactions on Design Automation of Electronic Systems (TODAES), vol. 25, no. 1, pp. 1-17, 2019.</p>
<p>Analog circuit design optimization based on symbolic simulation and simulated annealing. G Gielen, H Walscharts, W Sansen, IEEE Journal of Solid-state Circuits. 253G. Gielen, H. Walscharts, and W. Sansen, "Analog circuit design optimization based on symbolic simulation and simulated annealing," IEEE Journal of Solid-state Circuits, vol. 25, no. 3, pp. 707-713, 1990.</p>
<p>An evolutionary approach to automatic synthesis of high-performance analog integrated circuits. G Alpaydin, S Balkir, G Dundar, IEEE Transactions on Evolutionary Computation. 73G. Alpaydin, S. Balkir, and G. Dundar, "An evolutionary approach to automatic synthesis of high-performance analog integrated circuits," IEEE Transactions on Evolutionary Computation, vol. 7, no. 3, pp. 240- 252, 2003.</p>
<p>Analog circuit optimization system based on hybrid evolutionary algorithms. B Liu, Y Wang, Z Yu, L Liu, M Li, Z Wang, J Lu, F V Fernandez, Integration. 422B. Liu, Y. Wang, Z. Yu, L. Liu, M. Li, Z. Wang, J. Lu, and F. V. Fernan- dez, "Analog circuit optimization system based on hybrid evolutionary algorithms," Integration, vol. 42, no. 2, pp. 137-148, 2009.</p>
<p>Efficient and accurate statistical analog yield optimization and variation-aware circuit sizing based on computational intelligence techniques. B Liu, F V Fernandez, G Gielen, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 306B. Liu, F. V. Fernandez, and G. Gielen, "Efficient and accurate sta- tistical analog yield optimization and variation-aware circuit sizing based on computational intelligence techniques," IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 30, no. 6, pp. 793-805, 2011.</p>
<p>Anaconda: simulation-based synthesis of analog circuits via stochastic pattern search. R Phelps, M Krasnicki, R A Rutenbar, L R Carley, J R Hellums, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 196R. Phelps, M. Krasnicki, R. A. Rutenbar, L. R. Carley, and J. R. Hellums, "Anaconda: simulation-based synthesis of analog circuits via stochastic pattern search," IEEE Transactions on Computer-Aided Design of Inte- grated Circuits and Systems, vol. 19, no. 6, pp. 703-717, 2000.</p>
<p>Smartmsp: A self-adaptive multiple starting point optimization approach for analog circuit synthesis. Y Yang, H Zhu, Z Bi, C Yan, D Zhou, Y Su, X Zeng, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 373Y. Yang, H. Zhu, Z. Bi, C. Yan, D. Zhou, Y. Su, and X. Zeng, "Smart- msp: A self-adaptive multiple starting point optimization approach for analog circuit synthesis," IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 37, no. 3, pp. 531-544, 2018.</p>
<p>Efficient multiple starting point optimization for automated analog circuit optimization via recycling simulation data. B Peng, F Yang, C Yan, X Zeng, D Zhou, Proceedings of the 2016 Conference on Design, Automation &amp; Test in Europe. EDA Consortium. the 2016 Conference on Design, Automation &amp; Test in Europe. EDA ConsortiumB. Peng, F. Yang, C. Yan, X. Zeng, and D. Zhou, "Efficient multiple starting point optimization for automated analog circuit optimization via recycling simulation data," in Proceedings of the 2016 Conference on Design, Automation &amp; Test in Europe. EDA Consortium, 2016, pp. 1417-1422.</p>
<p>Analog circuit design optimization through the particle swarm optimization technique. M Fakhfakh, Y Cooren, A Sallem, M Loulou, P Siarry, Analog Integrated Circuits and Signal Processing. 631M. Fakhfakh, Y. Cooren, A. Sallem, M. Loulou, and P. Siarry, "Analog circuit design optimization through the particle swarm optimization technique," Analog Integrated Circuits and Signal Processing, vol. 63, no. 1, pp. 71-82, 2010.</p>
<p>A particle swarm optimization approach for components placement inspection on printed circuit boards. C Wu, D Wang, A W H Ip, D Wang, C Chan, H Wang, Journal of Intelligent Manufacturing. 205C. Wu, D. Wang, A. W. H. Ip, D. Wang, C. Chan, and H. Wang, "A par- ticle swarm optimization approach for components placement inspection on printed circuit boards," Journal of Intelligent Manufacturing, vol. 20, no. 5, pp. 535-549, 2009.</p>
<p>Use of particle swarm optimization to design combinational logic circuits. C A C Coello, E H Luna, A H Aguirre, international conference on evolvable systemsC. A. C. Coello, E. H. Luna, and A. H. Aguirre, "Use of particle swarm optimization to design combinational logic circuits," international con- ference on evolvable systems, pp. 398-409, 2003.</p>
<p>Taking the human out of the loop: A review of bayesian optimization. B Shahriari, K Swersky, Z Wang, R P Adams, N. De Freitas, Proceedings of the IEEE. 1041B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. De Freitas, "Taking the human out of the loop: A review of bayesian optimization," Proceedings of the IEEE, vol. 104, no. 1, pp. 148-175, 2015.</p>
<p>Gaspad: A general and efficient mm-wave integrated circuit synthesis method based on surrogate model assisted evolutionary algorithm. B Liu, D Zhao, P Reynaert, G Gielen, IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. 332B. Liu, D. Zhao, P. Reynaert, and G. Gielen, "Gaspad: A general and efficient mm-wave integrated circuit synthesis method based on surrogate model assisted evolutionary algorithm," IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 33, no. 2, pp. 169-182, 2014.</p>
<p>Batch bayesian optimization via multi-objective acquisition ensemble for automated analog circuit design. W Lyu, F Yang, C Yan, D Zhou, X Zeng, International Conference on Machine Learning. W. Lyu, F. Yang, C. Yan, D. Zhou, and X. Zeng, "Batch bayesian opti- mization via multi-objective acquisition ensemble for automated analog circuit design," in International Conference on Machine Learning, 2018, pp. 3312-3320.</p>
<p>Multi-objective bayesian optimization for analog/rf circuit synthesis. W Lyu, F Yang, C Yan, D Zhou, X Zeng, Proceedings of the 55th Annual Design Automation Conference. the 55th Annual Design Automation ConferenceACMW. Lyu, F. Yang, C. Yan, D. Zhou, and X. Zeng, "Multi-objective bayesian optimization for analog/rf circuit synthesis," in Proceedings of the 55th Annual Design Automation Conference. ACM, 2018, pp. 1-6.</p>
<p>Parallelizable bayesian optimization for analog and mixed-signal rare failure detection with high coverage. H Hu, P Li, J Z Huang, Proceedings of the International Conference on Computer-Aided Design. the International Conference on Computer-Aided DesignH. Hu, P. Li, and J. Z. Huang, "Parallelizable bayesian optimization for analog and mixed-signal rare failure detection with high coverage," in Proceedings of the International Conference on Computer-Aided Design, 2018, pp. 1-8.</p>
<p>Multi-fidelity surrogate based optimization for electromagnetic simulation acceleration. Y Wang, P Franzon, D Smart, B Swahn, ACM Transactions on Design Automation of Electronic Systems. TODAESY. Wang, P. Franzon, D. Smart, and B. Swahn, "Multi-fidelity surrogate based optimization for electromagnetic simulation acceleration," ACM Transactions on Design Automation of Electronic Systems (TODAES).</p>
<p>A multi-fidelity surrogate-modelassisted evolutionary algorithm for computationally expensive optimization problems. B Liu, S Koziel, Q Zhang, Journal of computational science. 12B. Liu, S. Koziel, and Q. Zhang, "A multi-fidelity surrogate-model- assisted evolutionary algorithm for computationally expensive optimiza- tion problems," Journal of computational science, vol. 12, pp. 28-37, 2016.</p>
<p>Batch bayesian optimization via local penalization. J González, Z Dai, P Hennig, N Lawrence, Artificial intelligence and statistics. J. González, Z. Dai, P. Hennig, and N. Lawrence, "Batch bayesian opti- mization via local penalization," in Artificial intelligence and statistics, 2016, pp. 648-657.</p>
<p>Parallelizing explorationexploitation tradeoffs in gaussian process bandit optimization. T Desautels, A Krause, J W Burdick, The Journal of Machine Learning Research. 151T. Desautels, A. Krause, and J. W. Burdick, "Parallelizing exploration- exploitation tradeoffs in gaussian process bandit optimization," The Journal of Machine Learning Research, vol. 15, no. 1, pp. 3873-3923, 2014.</p>
<p>Parallel gaussian process optimization with upper confidence bound and pure exploration. E Contal, D Buffoni, A Robicquet, N Vayatis, Computer Science. 8188E. Contal, D. Buffoni, A. Robicquet, and N. Vayatis, "Parallel gaussian process optimization with upper confidence bound and pure exploration," Computer Science, vol. 8188, pp. 225-240, 2013.</p>
<p>Batch bayesian optimization via simulation matching. J Azimi, A Fern, X Z Fern, Advances in Neural Information Processing Systems. J. Azimi, A. Fern, and X. Z. Fern, "Batch bayesian optimization via simulation matching," in Advances in Neural Information Processing Systems, 2010, pp. 109-117.</p>
<p>Gaussian processes in machine learning. C E Rasmussen, Lecture Notes in Computer Science. C. E. Rasmussen, "Gaussian processes in machine learning," Lecture Notes in Computer Science, pp. 63-71, 2003.</p>
<p>A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise. H J Kushner, H. J. Kushner, "A new method of locating the maximum point of an arbitrary multipeak curve in the presence of noise," 1964.</p>
<p>A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. E Brochu, V M Cora, N. De Freitas, arXiv:1012.2599arXiv preprintE. Brochu, V. M. Cora, and N. De Freitas, "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning," arXiv preprint arXiv:1012.2599, 2010.</p>
<p>The application of bayesian methods for seeking the extremum. J Mockus, V Tiesis, A Zilinskas, Towards global optimization. 22J. Mockus, V. Tiesis, and A. Zilinskas, "The application of bayesian methods for seeking the extremum," Towards global optimization, vol. 2, no. 117-129, p. 2, 1978.</p>
<p>Gaussian process optimization in the bandit setting: No regret and experimental design. N Srinivas, A Krause, S M Kakade, M Seeger, arXiv:0912.3995arXiv preprintN. Srinivas, A. Krause, S. M. Kakade, and M. Seeger, "Gaussian process optimization in the bandit setting: No regret and experimental design," arXiv preprint arXiv:0912.3995, 2009.</p>
<p>Informationtheoretic regret bounds for gaussian process optimization in the bandit setting. N Srinivas, A Krause, S M Kakade, M W Seeger, IEEE Transactions on Information Theory. 585N. Srinivas, A. Krause, S. M. Kakade, and M. W. Seeger, "Information- theoretic regret bounds for gaussian process optimization in the bandit setting," IEEE Transactions on Information Theory, vol. 58, no. 5, pp. 3250-3265, 2012.</p>
<p>Entropy search for information-efficient global optimization. P Hennig, C J Schuler, The Journal of Machine Learning Research. 131P. Hennig and C. J. Schuler, "Entropy search for information-efficient global optimization," The Journal of Machine Learning Research, vol. 13, no. 1, pp. 1809-1837, 2012.</p>
<p>On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. W R Thompson, Biometrika. 253/4W. R. Thompson, "On the likelihood that one unknown probability exceeds another in view of the evidence of two samples," Biometrika, vol. 25, no. 3/4, pp. 285-294, 1933.</p>
<p>Predictive entropy search for efficient global optimization of black-box functions. J M Hernández-Lobato, M W Hoffman, Z Ghahramani, Advances in neural information processing systems. J. M. Hernández-Lobato, M. W. Hoffman, and Z. Ghahramani, "Pre- dictive entropy search for efficient global optimization of black-box functions," in Advances in neural information processing systems, 2014, pp. 918-926.</p>
<p>Max-value entropy search for efficient bayesian optimization. Z Wang, S Jegelka, arXiv:1703.01968arXiv preprintZ. Wang and S. Jegelka, "Max-value entropy search for efficient bayesian optimization," arXiv preprint arXiv:1703.01968, 2017.</p>
<p>The correlated knowledge gradient for simulation optimization of continuous parameters using gaussian process regression. W Scott, P Frazier, W Powell, SIAM Journal on Optimization. 213W. Scott, P. Frazier, and W. Powell, "The correlated knowledge gradient for simulation optimization of continuous parameters using gaussian process regression," SIAM Journal on Optimization, vol. 21, no. 3, pp. 996-1026, 2011.</p>
<p>The knowledge-gradient policy for correlated normal beliefs. P Frazier, W Powell, S Dayanik, INFORMS journal on Computing. 214P. Frazier, W. Powell, and S. Dayanik, "The knowledge-gradient policy for correlated normal beliefs," INFORMS journal on Computing, vol. 21, no. 4, pp. 599-613, 2009.</p>
<p>Portfolio allocation for bayesian optimization. M D Hoffman, E Brochu, N De Freitas, UAI. Citeseer. M. D. Hoffman, E. Brochu, and N. de Freitas, "Portfolio allocation for bayesian optimization." in UAI. Citeseer, 2011, pp. 327-336.</p>
<p>An entropy search portfolio for bayesian optimization. B Shahriari, Z Wang, M W Hoffman, A Bouchard-Côté, N De Freitas, arXiv:1406.4625arXiv preprintB. Shahriari, Z. Wang, M. W. Hoffman, A. Bouchard-Côté, and N. de Freitas, "An entropy search portfolio for bayesian optimization," arXiv preprint arXiv:1406.4625, 2014.</p>
<p>No free lunch theorems for optimization. D H Wolpert, W G Macready, IEEE transactions on evolutionary computation. 11D. H. Wolpert and W. G. Macready, "No free lunch theorems for optimization," IEEE transactions on evolutionary computation, vol. 1, no. 1, pp. 67-82, 1997.</p>
<p>A fast and elitist multiobjective genetic algorithm: Nsga-ii. K Deb, A Pratap, S Agarwal, T Meyarivan, IEEE transactions on evolutionary computation. 62K. Deb, A. Pratap, S. Agarwal, and T. Meyarivan, "A fast and elitist multiobjective genetic algorithm: Nsga-ii," IEEE transactions on evolu- tionary computation, vol. 6, no. 2, pp. 182-197, 2002.</p>
<p>Parego: a hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems. J Knowles, IEEE Transactions on Evolutionary Computation. 101J. Knowles, "Parego: a hybrid algorithm with on-line landscape ap- proximation for expensive multiobjective optimization problems," IEEE Transactions on Evolutionary Computation, vol. 10, no. 1, pp. 50-66, 2006.</p>
<p>Moea/d: A multiobjective evolutionary algorithm based on decomposition. Q Zhang, H Li, IEEE Transactions on evolutionary computation. 116Q. Zhang and H. Li, "Moea/d: A multiobjective evolutionary algorithm based on decomposition," IEEE Transactions on evolutionary computa- tion, vol. 11, no. 6, pp. 712-731, 2007.</p>
<p>Spea2: Improving the strength pareto evolutionary algorithm. E Zitzler, M Laumanns, L Thiele, TIK-report. 103E. Zitzler, M. Laumanns, and L. Thiele, "Spea2: Improving the strength pareto evolutionary algorithm," TIK-report, vol. 103, 2001.</p>
<p>Improving pso-based multiobjective optimization using crowding, mutation and -dominance. M R Sierra, C A C Coello, International conference on evolutionary multi-criterion optimization. SpringerM. R. Sierra and C. A. C. Coello, "Improving pso-based multi- objective optimization using crowding, mutation and -dominance," in International conference on evolutionary multi-criterion optimization. Springer, 2005, pp. 505-519.</p>
<p>Smpso: A new pso-based metaheuristic for multi-objective optimization. A J Nebro, J J Durillo, J Garcia-Nieto, C C Coello, F Luna, E Alba, 2009 IEEE Symposium on computational intelligence in multi-criteria decision-making (MCDM. IEEEA. J. Nebro, J. J. Durillo, J. Garcia-Nieto, C. C. Coello, F. Luna, and E. Alba, "Smpso: A new pso-based metaheuristic for multi-objective optimization," in 2009 IEEE Symposium on computational intelligence in multi-criteria decision-making (MCDM). IEEE, 2009, pp. 66-73.</p>
<p>Differential evolution for multiobjective optimization. T Robič, B Filipič, International Conference on Evolutionary Multi-Criterion Optimization. SpringerT. Robič and B. Filipič, "Differential evolution for multiobjective opti- mization," in International Conference on Evolutionary Multi-Criterion Optimization. Springer, 2005, pp. 520-533.</p>
<p>Bayesian optimization with unknown constraints. M A Gelbart, J Snoek, R P Adams, arXiv:1403.5607arXiv preprintM. A. Gelbart, J. Snoek, and R. P. Adams, "Bayesian optimization with unknown constraints," arXiv preprint arXiv:1403.5607, 2014.</p>
<p>Bayesian optimization and semiparametric models with applications to assistive technology. J R Snoek, CiteseerPh.D. dissertationJ. R. Snoek, "Bayesian optimization and semiparametric models with applications to assistive technology," Ph.D. dissertation, Citeseer, 2013.</p>
<p>GPy: A gaussian process framework in python. Gpy, GPy, "GPy: A gaussian process framework in python," http://github. com/SheffieldML/GPy, since 2012.</p>
<p>Analog circuit sizing via swarm intelligence. R A Vural, T Yildirim, Aeu-international Journal of Electronics and Communications. 669R. A. Vural and T. Yildirim, "Analog circuit sizing via swarm intelli- gence," Aeu-international Journal of Electronics and Communications, vol. 66, no. 9, pp. 732-740, 2012.</p>
<p>A 0.016-mm 2 144-µ w three-stage amplifier capable of driving 1-to-15 nf capacitive load with &gt;0.95-mhz gbw. Z Yan, P Mak, M Law, R P Martins, IEEE Journal of Solid-State Circuits. 482Z. Yan, P. Mak, M. Law, and R. P. Martins, "A 0.016-mm 2 144-µ w three-stage amplifier capable of driving 1-to-15 nf capacitive load with &gt;0.95-mhz gbw," IEEE Journal of Solid-State Circuits, vol. 48, no. 2, pp. 527-540, 2013.</p>
<p>She is currently pursuing the Ph.D. degree with State Key Lab. of Application Specific Integrated Circuits and System. Shanghai, China; Shanghai, ChinaShuhan Zhang received the B.S. degree in microelectronics from Fudan University ; Microelectronics Department, Fudan UniversityHer current research interests include analog circuit design automation and optimization, few-shot learningShuhan Zhang received the B.S. degree in micro- electronics from Fudan University, Shanghai, China in 2016. She is currently pursuing the Ph.D. degree with State Key Lab. of Application Specific Integrated Circuits and System, Microelectronics Department, Fudan University, Shanghai, China. Her current re- search interests include analog circuit design au- tomation and optimization, few-shot learning, gen- erative adversarial networks and style transfer.</p>
<p>he was an Assistant Professor with Fudan University. He is currently an Associate Professor with the Microelectronics Department, Fudan University. His research interests include model order reduction, circuit simulation, high-level synthesis. M'08) received the B.S. degree from Xi'an Jiaotong University in 2003, and the Ph.D. degree from Fudan University. yield analysis, and design for manufacturabilityFan Yang (M'08) received the B.S. degree from Xi'an Jiaotong University in 2003, and the Ph.D. de- gree from Fudan University in 2008. From 2008 to 2011, he was an Assistant Professor with Fudan University. He is currently an Associate Professor with the Microelectronics Department, Fudan Uni- versity. His research interests include model order reduction, circuit simulation, high-level synthesis, yield analysis, and design for manufacturability.</p>
<p>His current research interests include parasitic parameter extraction of interconnects, parallel algorithms for large scale computation, design for manufacturability. 2006, and received the B.E. and M.E. degrees from the Huazhong University of Science and Technology. Beijing, China; Wuhan, China; Shanghai, ChinaChanghao Yan received the Ph.D. degree in computer science from the Tsinghua University ; Fudan UniversityHe is currently an Associate Prof. of School of Microelectronics. robust analysis of circuits, and AI and machine learning in medicineChanghao Yan received the Ph.D. degree in com- puter science from the Tsinghua University, Beijing, China, in 2006, and received the B.E. and M.E. degrees from the Huazhong University of Science and Technology, Wuhan, China, in 1996 and 2002, respectively. He is currently an Associate Prof. of School of Microelectronics, Fudan University, Shanghai, China. His current research interests in- clude parasitic parameter extraction of interconnects, parallel algorithms for large scale computation, de- sign for manufacturability, robust analysis of cir- cuits, and AI and machine learning in medicine.</p>
<p>S. degree in electrical engineering from Fudan University, China, in 1982 and 1985, respectively, and the Ph.D. degree in electrical and computer engineering from the University of Illinois in 1990. He joined the University of North Carolina at Charlotte as an Assistant Professor in 1990, where he became an Associate Professor in 1995. He joined The University of Texas at Dallas as a Full Professor in 1999. His research interests include high-speed VLSI systems. Dian Zhou, M'89-SM'07) received the B.S. degree in physics and the M. CAD tools, mixed-signal ICs, and algorithmsDian Zhou (M'89-SM'07) received the B.S. degree in physics and the M.S. degree in electrical engineer- ing from Fudan University, China, in 1982 and 1985, respectively, and the Ph.D. degree in electrical and computer engineering from the University of Illinois in 1990. He joined the University of North Carolina at Charlotte as an Assistant Professor in 1990, where he became an Associate Professor in 1995. He joined The University of Texas at Dallas as a Full Professor in 1999. His research interests include high-speed VLSI systems, CAD tools, mixed-signal ICs, and algorithms.</p>            </div>
        </div>

    </div>
</body>
</html>
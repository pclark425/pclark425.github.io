<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6212 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6212</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6212</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-7cfb88ee7babd5cd717c525f8b642776c47c59cd</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/7cfb88ee7babd5cd717c525f8b642776c47c59cd" target="_blank">Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech</a></p>
                <p><strong>Paper Venue:</strong> The Web Conference</p>
                <p><strong>Paper TL;DR:</strong> This work examines whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection, and design and conduct user studies to evaluate their qualities by comparison with human-written NLEs.</p>
                <p><strong>Paper Abstract:</strong> Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.</p>
                <p><strong>Cost:</strong> 0.007</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6212.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6212.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT-as-judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (Jan 9 version, 2023) used as a judgment-and-explanation system</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>ChatGPT was used to perform binary implicit-hate classification (Yes/No/Uncertain) and to generate one-sentence natural language explanations (NLEs) for each decision; three independent ChatGPT responses were collected per tweet and aggregated into a numeric 'ChatGPT score'. The paper compares these LLM judgments and NLEs against original human labels and human re-annotations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Implicit hate speech detection and natural-language explanation (NLE) generation for tweets</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>ChatGPT (Jan 9 version, 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>For disagreement cases between ChatGPT and the LatentHatred labels, Amazon Mechanical Turk (MTurk) Masters (approval rate >98%, >5000 HITs, US-based) provided three independent annotations per instance under three separate conditions: (1) post only, (2) post + human-written NLE, (3) post + ChatGPT-generated NLE. For NLE quality, a random sample of 100 tweets was rated by three MTurk workers each on Informativeness and Clarity (7-point Likert); research assistants adjudicated disputes. Overall dataset: 795 sampled tweets (12.5% of LatentHatred test set); three ChatGPT responses per tweet collected in fresh sessions.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Agreement rate with original labels (percent), proportion of instances labeled Hateful/Non-Hateful/Uncertain by ChatGPT, average "hatefulness score" from human annotators (mean of +1/0/-1), Informativeness (7-point Likert), Clarity (7-point Likert), and statistical significance via t-tests.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Quantitative: ChatGPT labeled 636/795 (80%) as implicitly hateful, 146 (18.4%) as not hateful, 13 (1.6%) uncertain, yielding ~80% agreement with the LatentHatred original labels. In human re-evaluation of disagreement cases, lay annotators' average hatefulness scores were: post-only mean -0.41 (lean non-hateful), post + ChatGPT NLE mean -0.52 (significantly more non-hateful; t(316)=-13.75, p<0.001), post + human NLE mean +0.29 (leans hateful). For NLE quality (n=100): ChatGPT NLEs had higher Clarity (mean 5.39) than human NLEs (mean 4.68; t(197)=-3.73, p<0.001); Informativeness showed no significant difference (t(198)=-0.69, p=0.49). Qualitative: ChatGPT-generated NLEs are clearer and can strongly influence non-expert judgments, sometimes more than human NLEs.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>ChatGPT can produce inconsistent judgments across independent runs (yes/no/unclear for the same input), explicitly express uncertainty (e.g., "It's unclear"), and produce plausible but potentially incorrect rationales that can shift human judgment. The model's persuasive NLEs can mislead lay annotators into accepting incorrect labels; the paper flags this as a risk when using ChatGPT as an annotation tool. The model also struggles with extremely implicit/subtle instances and gray-area cases.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Example provided: a tweet about 'identitarianism' produced three differing ChatGPT responses (unclear; yes identifying as hateful; no not hateful), demonstrating intra-model inconsistency. Broadly, ~20% of instances disagreed with original labels; in some of these, ChatGPT labeled non-hateful while dataset labeled hateful, and ChatGPT NLEs then reduced lay annotators' tendency to label those tweets hateful (i.e., reinforcing a potentially incorrect non-hateful classification). Cases where ChatGPT responded 'Unclear' were labeled Uncertain and could not be straightforwardly mapped to binary labels.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors recommend human oversight and caution when using ChatGPT for annotation: aggregate multiple LLM responses (they averaged three), re-evaluate disagreement cases with human annotators, provide controlled prompt design (they tested prompts and used a concise prompt), create fresh chat sessions per instance to avoid context carryover, and further study prompt variants. More general recommendations include using ChatGPT as an assistive tool rather than sole annotator and conducting human rechecks (as done via MTurk) for contentious cases.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech', 'publication_date_yy_mm': '2023-02'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection <em>(Rating: 2)</em></li>
                <li>ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports <em>(Rating: 2)</em></li>
                <li>How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment <em>(Rating: 2)</em></li>
                <li>How would Stance Detection Techniques Evolve after the Launch of ChatGPT? <em>(Rating: 2)</em></li>
                <li>An Analysis of the Automatic Bug Fixing Performance of ChatGPT <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6212",
    "paper_id": "paper-7cfb88ee7babd5cd717c525f8b642776c47c59cd",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "ChatGPT-as-judge",
            "name_full": "ChatGPT (Jan 9 version, 2023) used as a judgment-and-explanation system",
            "brief_description": "ChatGPT was used to perform binary implicit-hate classification (Yes/No/Uncertain) and to generate one-sentence natural language explanations (NLEs) for each decision; three independent ChatGPT responses were collected per tweet and aggregated into a numeric 'ChatGPT score'. The paper compares these LLM judgments and NLEs against original human labels and human re-annotations.",
            "citation_title": "Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech",
            "mention_or_use": "use",
            "task_domain": "Implicit hate speech detection and natural-language explanation (NLE) generation for tweets",
            "llm_judge_model": "ChatGPT (Jan 9 version, 2023)",
            "human_evaluation_setup": "For disagreement cases between ChatGPT and the LatentHatred labels, Amazon Mechanical Turk (MTurk) Masters (approval rate &gt;98%, &gt;5000 HITs, US-based) provided three independent annotations per instance under three separate conditions: (1) post only, (2) post + human-written NLE, (3) post + ChatGPT-generated NLE. For NLE quality, a random sample of 100 tweets was rated by three MTurk workers each on Informativeness and Clarity (7-point Likert); research assistants adjudicated disputes. Overall dataset: 795 sampled tweets (12.5% of LatentHatred test set); three ChatGPT responses per tweet collected in fresh sessions.",
            "metrics_compared": "Agreement rate with original labels (percent), proportion of instances labeled Hateful/Non-Hateful/Uncertain by ChatGPT, average \"hatefulness score\" from human annotators (mean of +1/0/-1), Informativeness (7-point Likert), Clarity (7-point Likert), and statistical significance via t-tests.",
            "reported_differences": "Quantitative: ChatGPT labeled 636/795 (80%) as implicitly hateful, 146 (18.4%) as not hateful, 13 (1.6%) uncertain, yielding ~80% agreement with the LatentHatred original labels. In human re-evaluation of disagreement cases, lay annotators' average hatefulness scores were: post-only mean -0.41 (lean non-hateful), post + ChatGPT NLE mean -0.52 (significantly more non-hateful; t(316)=-13.75, p&lt;0.001), post + human NLE mean +0.29 (leans hateful). For NLE quality (n=100): ChatGPT NLEs had higher Clarity (mean 5.39) than human NLEs (mean 4.68; t(197)=-3.73, p&lt;0.001); Informativeness showed no significant difference (t(198)=-0.69, p=0.49). Qualitative: ChatGPT-generated NLEs are clearer and can strongly influence non-expert judgments, sometimes more than human NLEs.",
            "llm_specific_limitations": "ChatGPT can produce inconsistent judgments across independent runs (yes/no/unclear for the same input), explicitly express uncertainty (e.g., \"It's unclear\"), and produce plausible but potentially incorrect rationales that can shift human judgment. The model's persuasive NLEs can mislead lay annotators into accepting incorrect labels; the paper flags this as a risk when using ChatGPT as an annotation tool. The model also struggles with extremely implicit/subtle instances and gray-area cases.",
            "notable_failure_cases": "Example provided: a tweet about 'identitarianism' produced three differing ChatGPT responses (unclear; yes identifying as hateful; no not hateful), demonstrating intra-model inconsistency. Broadly, ~20% of instances disagreed with original labels; in some of these, ChatGPT labeled non-hateful while dataset labeled hateful, and ChatGPT NLEs then reduced lay annotators' tendency to label those tweets hateful (i.e., reinforcing a potentially incorrect non-hateful classification). Cases where ChatGPT responded 'Unclear' were labeled Uncertain and could not be straightforwardly mapped to binary labels.",
            "mitigation_strategies": "Authors recommend human oversight and caution when using ChatGPT for annotation: aggregate multiple LLM responses (they averaged three), re-evaluate disagreement cases with human annotators, provide controlled prompt design (they tested prompts and used a concise prompt), create fresh chat sessions per instance to avoid context carryover, and further study prompt variants. More general recommendations include using ChatGPT as an assistive tool rather than sole annotator and conducting human rechecks (as done via MTurk) for contentious cases.",
            "uuid": "e6212.0",
            "source_info": {
                "paper_title": "Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech",
                "publication_date_yy_mm": "2023-02"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
            "rating": 2
        },
        {
            "paper_title": "ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports",
            "rating": 2
        },
        {
            "paper_title": "How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
            "rating": 2
        },
        {
            "paper_title": "How would Stance Detection Techniques Evolve after the Launch of ChatGPT?",
            "rating": 2
        },
        {
            "paper_title": "An Analysis of the Automatic Bug Fixing Performance of ChatGPT",
            "rating": 1
        }
    ],
    "cost": 0.0074175000000000005,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech</h1>
<p>Fan Huang<br>huangfan@acm.org<br>Indiana University Bloomington Bloomington, IN, United States</p>
<p>Haewoon Kwak<br>haewoon@acm.org<br>Indiana University Bloomington Bloomington, IN, United States</p>
<p>Jisun An<br>jisun.an@acm.org<br>Indiana University Bloomington Bloomington, IN, United States</p>
<h2>ABSTRACT</h2>
<p>Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.</p>
<h2>CCS CONCEPTS</h2>
<ul>
<li>Computing methodologies $\rightarrow$ Natural language generation.</li>
</ul>
<h2>KEYWORDS</h2>
<p>Hate Speech, Toxicity Detection, Natural Language Explanation, ChatGPT, Large Language Models, Human Annotation</p>
<h2>ACM Reference Format:</h2>
<p>Fan Huang, Haewoon Kwak, and Jisun An. 2023. Is ChatGPT better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech. In Companion Proceedings of the ACM Web Conference 2023 (WWW '23 Companion), April 30-May 4, 2023, Austin, TX, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3543873.3587368</p>
<h2>1 INTRODUCTION</h2>
<p>Warning: This paper contains offensive content and may be upsetting.
In November 2022, OpenAI launched a new chatbot model, ChatGPT [15]. Just two months after its debut, ChatGPT acquired 100 million monthly active users in January 2023, reportedly making it the fastest-growing AI tool in history [8]. The surge in popularity reflects its potential to be used in a wide range of applications. As the most powerful text generation model, not surprisingly, researchers and practitioners have been evaluating its capability for various tasks, including question-and-answer in financial, medical, legal, and psychological areas [10], medical report simplification [13], bug fixing in computer programs [18], and stance detection from texts [20]. While ChatGPT has shown impressive performances for those 'objective' tasks, it is underexplored how ChatGPT would</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>respond to more 'subjective' tasks, which are crucial elements in understanding social phenomena and require social judgment and decision-making. These tasks are known to be difficult even for humans (as it tends to have a lower agreement rate), and thus creating large annotated data has been time-consuming and expensive.</p>
<p>In this work, we study how ChatGPT performs for subjective tasks that require knowledge of social norms and cultural context. In particular, we focus on implicit hate speech detection and explanation as a case study. Online hate speech is known to be one of the most significant societal issues with many real-world negative consequences [1, 11]. Recent studies have discovered that many online hate speeches are implicit, and even advanced machine learning models cannot achieve high accuracy in their detection [7]. Moreover, it has been a challenging problem to provide the explainability of those models by using natural language explanations (NLEs) $[4,7]$.</p>
<p>To fill this gap, our study empirically evaluates how good ChatGPT is in 1) classifying implicit hate speech and 2) generating explanations for implicit hate speech. In particular, our two research questions are as follows:</p>
<ul>
<li>RQ1: Can ChatGPT detect implicit hateful tweets well?</li>
<li>RQ2: Does ChatGPT generate quality NLEs?</li>
</ul>
<p>Using the LatentHatred dataset [7], which is one of the most widely used datasets in the domain, we compare the responses of ChatGPT with the human-written data via human evaluations, highlighting the potentials and limitations of ChatGPT responses.</p>
<h2>2 RELATED WORK</h2>
<p>Hate Speech Explanation Generation To hinder the spread of toxic speech among online social platforms, researchers developed machine learning models in recent years [2, 5, 16]. Explicit hate speech is easy to understand and detect, while implicit hate speech can be hard to capture due to its nuanced nature [7]. Human-written NLEs to explain why a given text is hateful can be useful in both AI-assisted systems and ML model fine-tuning pipelines [12]. Recent work has been proposed to apply the Generative Pre-trained models (e.g., GPT-2, BART, OPT, and T5 [7, 12, 19]), to create NLEs describing why a given text is considered to be biased [17] or hateful $[7,17]$.
ChatGPT and its Evaluation ChatGPT is built based on the Reinforcement Learning from Human Feedback (RLHF) approach, which takes advantage of the extensive human annotations [15]. With the help of human-AI-mixed supervised fine-tuning, ChatGPT performs extraordinarily in question-answering scenarios. Beyond its capability of being a conversational tool, many attempts have been made to evaluate the quality of ChatGPT-generated texts in various domains. Guo et al. [10] construct a Human ChatGPT Comparison</p>
<p>Corpus (HC3), which compiles a handful of question-and-answer datasets ranging from financial, medical, to psychological areas, and study the characteristics of ChatGPT's responses compared with that of humans. Jeblick et al. [13] use ChatGPT to generate a simplified version of a radiology report and assess its quality by radiologists. Sobania et al. [18] evaluate ChatGPT on the standard bug fixing benchmark set, QuixBugs. They find that ChatGPT's bug-fixing performance is competitive with the other existing approaches. Gilson et al. [9] examine the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams and show that the ChatGPT is comparable to a third-year medical student. Zhang et al. [20] examine how ChatGPT performs on stance detection task (i.e., inferring the standpoint (Favor, Against, or Neither) towards a target in a given text). The above works similarly find out that, in most cases, ChatGPT's answers are comparable to existing human annotations.</p>
<p>In this work, we apply ChatGPT to a new problem that provides a concise NLE for implicit hateful speech, which is a nuanced and context-dependent task. The inherent nature of unclear boundaries of implicit hateful speech and online toxicity makes the problem more challenging.</p>
<h2>3 CHATGPT-BASED EXPLANATIONS</h2>
<h3>3.1 Case Study: LatentHatred Dataset</h3>
<p>As a case study, we conduct our analysis by using the LatentHatred dataset [7], including 6,358 implicit hateful tweets with their corresponding human annotated target group (i.e., the specific group as the target of hatefulness) and implied statement (i.e., the implications of the expressed hatefulness). For our experiment, we randomly select $12.5 \%$ of the whole data (i.e., 795 instances), which is the equal number of the test instances for the implicit hate explanation generation task in [7]). We call these implied statements the human-written NLEs as explained in [7].</p>
<h3>3.2 ChatGPT Response Collection</h3>
<p>Based on the 795 implicit hateful tweets, we use ChatGPT (Jan 9 version (2023)) to generate classification answers (whether a given tweet is hateful or not) and explanations for the reasoning. After testing several variations, we finalize our prompt as follows:</p>
<p>Given Post: 'An Implicit Hateful Tweet'. Answer yes or no for whether the post is implicitly hateful. Explain why in one sentence.
Specifically, "Given Post: 'An Implicit Hateful Tweet'" is to point out what the given tweet is, "Answer yes or no for whether the post is implicitly hateful" is to ensure that ChatGPT provides a clear answer for implicit hate classification, and "Explain why in one sentence" is to generate a concise explanation for its decision.</p>
<p>Since ChatGPT does not provide official APIs as of January 2023, we use one of the unofficial APIs ${ }^{1}$ to collect the ChatGPT responses. For each implicit hate tweet post, we collect three responses from ChatGPT. As the responses by ChatGPT can be influenced by history, we create a new chat session for every implicit hateful tweet to ensure no context is provided to ChatGPT. Table 1 shows an</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>| Prompt | Given Post: 'do you consider identitarianism more of a viable future option than the alt right label identity evropa seem to be going that way'. Answer yes or no for whether the post is implicitly hateful. Explain why in one sentence. |
| :--: | :--: |
| Humanwritten NLEs [7] | non-white people are enemies |
| ChatGPT 1 | It's unclear. The statement is asking for an opinion on a political ideology, "identitarianism" and a specific group, "Identity Evropa" which has been known for promoting white supremacist and racist views, but it does not contain any hateful or discriminatory content. |
| ChatGPT 2 | Yes. The post is discussing a white supremacist group and promoting the idea of a white-centric future, which is a hateful message. |
| ChatGPT 3 | No. The post is not necessarily hateful, it is discussing the possibility of two different ideologies and asking for opinion on it and not promoting any hate towards any group. |</p>
<p>Table 1: Example of our prompt and ChatGPT's responses
example of an input and responses by ChatGPT. We regard the Yes/No response as the binary classification result and the following explanations as the ChatGPT-generated NLE. We notice that, even though we try to limit the answer to Yes or No (i.e., "Answer yes or no for whether the post is implicitly hateful"), ChatGPT sometimes expresses its confusion (e.g., 'It's unclear') when a given tweet falls in a gray area.</p>
<p>For each tweet post, we aggregate the three responses from ChatGPT in the following way. We assign a numeric score of +1 to Yes (i.e., ChatGPT classifies a given post as implicitly hateful), -1 to No (not hateful), and 0 to other responses. We then get the average of the three scores and call it ChatGPT score of a given tweet. We label a tweet post 'Hateful' if the ChatGPT score is greater than 0 , 'Non-Hateful' if it is less than 0 , and 'Uncertain' if it is 0 .</p>
<h3>3.3 Human Evaluation</h3>
<p>We conduct human evaluations to measure the quality of ChatGPT on the classification task and the generated NLEs.
Quality of ChatGPT-based Classification. While all our test instances are from the implicit hate tweet dataset, we find that ChatGPT disagrees (i.e., responding non-hateful or uncertain) for a sizable number of tweet posts. Our manual inspection shows that those posts are potentially debatable or extremely implicit/subtle to be hateful. Thus, we turn to re-evaluating those disagreeing cases to examine to what extent human agrees with ChatGPT.</p>
<p>We annotate the data using Amazon Mechanical Turk (Mturk). For each disagreeing instance, we ask Mturk workers to classify whether a given tweet is hateful, not hateful, or uncertain. We further design three types of evaluations by providing different</p>
<p>contexts. We give a respondent 1) a post only, 2) a post with its human-written NLE, and 3) a post with its ChatGPT-generated NLE. For 3), since we collect three responses from ChatGPT, we choose one NLE from the corresponding label. For example, for a post labeled as Non-Hateful by ChatGPT, if it has two NLEs with the answer No and one NLE with the answer Yes, we randomly select one NLE with the answer No. When a post is labeled Uncertain, we provide the NLE with a score of 0 . We collect the three responses from three different Mturk workers for each instance in each evaluation experiment.</p>
<p>To ensure the quality of the collected human annotations, we recruit Mturk Masters who: i) have an approval rate greater than $98 \%$ ii) have more than 5000 HITs approved; and iii) are located in the United States. We also note that no demographic information is collected during this process. All the respondents are monetarily compensated at a rate above the minimum hourly wage in the United States. Since our experiments contain hateful content, we provided extra mental wellness consultant information to prevent the participants from being exposed to unexpected pressures and getting hurt.</p>
<p>Quality of ChatGPT-generated Explanation. To evaluate and compare the quality of human-written NLEs and ChatGPT-generated ones, we use Informativeness [6] and Clarity [3], which are commonly used metrics that reflect the human perception of generated texts. In our context, Informativeness captures the relevance of the NLE in explaining why a tweet would be considered hateful (7-point Likert scale ranging from 1 (Completely Not Informative) to 7 (Very Informative)). Clarity measures how clear the NLE is (7-point Likert scale ranging from 1 (Completely Unclear) to 7 (Very Clear)).</p>
<p>In this evaluation, we randomly sample 100 implicit hateful tweets. For each of those 100 cases, we provide a Mturk worker with an original tweet and its corresponding NLE and ask to rate the Informativeness and Clarity of the given NLE. We conduct two experiments, one with human-written NLEs and another with ChatGPT-generated NLEs. Since we aim to inspect the general quality of the ChatGPT-generated NLEs, we randomly select one NLE from the three responses of the ChatGPT.</p>
<p>We follow the same procedure as the above Mturk experiment. We also collect three annotations for each tweet-NLE pair. To ensure the quality of responses, we hire experienced research assistants to review them and resolve any disputing cases.</p>
<h2>4 RESULT</h2>
<h3>4.1 RQ1: Does ChatGPT detect implicit hateful tweets well?</h3>
<p>Our first research question is whether ChatGPT can detect implicit hateful tweets well. It is a prerequisite for generating correct and quality NLEs. Among the 795 instances from the LatentHatred dataset [7], ChatGPT recognizes 636 instances as 'Implicitly Hateful (80\%) .' In other words, ChatGPT recognizes 146 instances as 'Not Implicitly Hateful (18.4\%)' and 13 instances as 'Uncertain (1.6\%)', which conflicts with the original labels in [7]. To examine which is correct, we conduct a user study explained in $\S 3.3$. We ask MTurk workers whether a given post is implicitly hateful but with 1) a
post only, 2) a post and human-written NLE, and 3) a post and ChatGPT-generated NLE. Considering the possibility that it is not straightforward to decide whether a given post is hateful or not, we provide the option of 'Not Sure' in addition to 'Yes' and 'No.' We collect three responses for each instance.</p>
<p>Similar to the computation of the ChatGPT score, we assign +1 when a Mturk worker answers Yes (hateful), -1 when the worker answers No (non-hateful), and 0 otherwise. We compute the mean of the three scores for each instance and call the average hatefulness score.
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Human evaluation for disagreement between [7] and ChatGPT. We give MTurk workers 1) a post only, 2) a post and human-written NLE ("post+Human"), and 3) a post and ChatGPT-generated NLE ("post+ChatGPT").</p>
<p>There are several interesting observations from the result in Figure 1. First, when only a post is given, we find the average hatefulness score is negative (mean: -0.41), which means lay people (i.e., Mturk workers) are less likely to consider a tweet hateful when ChatGPT could not find hate in it. Second, the average hatefulness score is negative (mean: -0.52) when a post is given with the ChatGPT-generated NLE. Interestingly, lay people are more likely to find a given tweet non-hateful when ChatGPT explains why the given tweet is non-hateful (mean: -0.52) compared to when only a post is given (mean: -0.41). The difference is statistically significant, confirmed by the $t$-test $(t(316)=-13.75, p&lt;0.001)$. Third, the average hatefulness score is positive (mean: 0.29 ) when a post is given with the human-written NLE. This shows that lay people's decisions can be drifted by the additional explanation; however, the lower absolute score indicates that providing human-written NLE is not as convincing as the other two cases.</p>
<p>Considering the diversity and nuanced nature of the implicit hate speech [7] and overall toxic behavior online [14], it is not surprising that lay people's decisions can be affected by additional context. Nevertheless, what we observe from the above experiment clearly demonstrates the potential of ChatGPT for nuanced, subjective tasks. Lay people tend to agree with ChatGPT that those tweets are not likely to be hateful. However, on the other hand, ChatGPTgenerated NLEs reinforce the human perception. In a case when ChatGPT's decision is wrong, such capability might be a danger to misleading lay people and potentially constructing wrong labels.</p>
<p>In summary, our answer to RQ1 is yes. ChatGPT shows $80 \%$ agreement with [7]. Furthermore, for the 20\% disagreement cases,</p>
<p>lay people are highly likely to lean toward ChatGPT's classification results.</p>
<h3>4.2 RQ2: Does ChatGPT generate quality NLEs?</h3>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Quality evaluation of human-written NLEs and ChatGPT-generated NLEs</p>
<p>For the randomly selected 100 instances from our dataset, we collect the Informativeness and Clarity scores as we explained in §3.3. In Figure 2, ChatGPT-generated NLEs show significantly higher Clarity scores (mean: 5.39) than human-written NLEs (mean: 4.68). As confirmed by the t-test, the difference is statistically significant $(t(197)=-3.73, p&lt;0.001)$. However, there is no statistically significant difference in Informativeness $(t(198)=-0.69, p=0.49)$.</p>
<p>Our findings demonstrate that, for end users, both human-written NLEs and ChatGPT-generated NLEs provide information to help users identify implicit hatefulness. However, ChatGPT provides more comprehensive illustrations so users can easily confirm implicit hatefulness from a given tweet.</p>
<p>In summary, our answer to RQ2 is also yes. ChatGPT generates quality NLEs comparable to human annotators for implicit hate speech.</p>
<h2>5 DISCUSSIONS AND CONCLUSION</h2>
<p>In this paper, we examine ChatGPT-generated NLEs for the implicit hateful speech from the perspectives of their accuracy and quality.</p>
<p>ChatGPT correctly identifies $80 \%$ of the implicit hateful tweets in our experiment setting. The results demonstrate the great potential of ChatGPT as a data annotation tool using a simple prompt design. Further studies can be done to investigate the effect of different prompt designs. Furthermore, for those $20 \%$ disagreement cases, our experiments show that ChatGPT's results are more likely to align with lay people's perceptions. Moreover, we also find that the ChatGPT-generated NLEs can reinforce human perception, and they tend to be perceived as clearer than human-written NLEs. These results pose an important question. Since ChatGPT can be convincing, its capability would become a risk of misleading lay people if its decision is wrong. This highlights that it would require extra caution when using ChatGPT as a tool for assisting data annotation.</p>
<h2>6 ETHICAL CONSIDERATIONS</h2>
<p>Our data collection design via Amazon Mechanical Turk (Mturk) has been approved by Singapore Management University (Approval No.: IRB-22-076-A043(622)). To prevent human annotators from feeling offended or having mental discomfort, we highlight the possible hatefulness in the headline of the Mturk tasks. We also suggest annotators stop labeling anytime they felt uncomfortable and provide the consultant hotlines in our task instructions.</p>
<h2>REFERENCES</h2>
<p>[1] Jisun An, Haewoon Kwak, Claire Seungeun Lee, Bogang Jun, and Yong-Yeol Ahn. 2021. Predicting Anti-Asian Hateful Users on Twitter during COVID-19. In EMNLP Findings.
[2] Valerio Basile, Cristina Bosco, Elisabetta Fersini, Debora Nozza, Viviana Patti, Francisco Manuel Rangel Pardo, Paolo Rosso, and Manuela Sanguinetti. 2019. SemEval-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter. In 15th International Workshop on Semantic Evaluation.
[3] Anja Bolz and Eric Kow. 2009. System Building Cost vs. Output Quality in Data-to-Text Generation. In Proceedings of European Workshop on Natural Language Generation.
[4] Oana-Maria Camburu, Tim Rocktäschel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-SNLI: Natural language inference with natural language explanations. In NeurIPS.
[5] Ona de Gibert, Naiara Perez, Aitor Garcia-Pablos, and Montse Cuadros. 2018. Hate Speech Dataset from a White Supremacy Forum. In Proceedings of the 2nd Workshop on Abusive Language Online.
[6] Ondřej Dušek, Jekaterina Novikova, and Verena Rieser. 2020. Evaluating the state-of-the-art of end-to-end natural language generation: The E2E NLG challenge. Computer Speech \&amp; Language (2020).
[7] Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. 2021. Latent Hatred: A Benchmark for Understanding Implicit Hate Speech. In EMNLP.
[8] Engadget. 2023. ChatGPT reportedly reached 100 million users in January. https: //www.engadget.com/chatgpt-100-million-users-january-130619073.html.
[9] Aidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, and David Chartash. 2022. How Well Does ChatGPT Do When Taking the Medical Licensing Exams? The Implications of Large Language Models for Medical Education and Knowledge Assessment. medRxiv (2022), 2022-12.
[10] Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection. arXiv:2301.07597 (2023).
[11] Gabriel Emile Hine, Jeremiah Onaslapo, Emiliano De Cristofaro, Nicolas Kourtellis, Iliao Leontiadis, Riginos Samaras, Gianluca Stringhini, and Jeremy Blackburn. 2017. Kek, cucks, and god emperor trump: A measurement study of 4chan's politically incorrect forum and its effects on the web. In ICWSM.
[12] Fan Huang, Haewoon Kwak, and Jisun An. 2023. Chain of Explanation: New Prompting Method to Generate Higher Quality Natural Language Explanation for Implicit Hate Speech. In Companion Proceedings of the ACM Web Conference.
[13] Katharina Jeblick, Balthasar Schachtner, Jakob Dexl, Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp Wesp, Bastian Sabel, Jens Rickx, et al. 2022. ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports. arXiv:2212.14882 (2022).
[14] Haewoon Kwak, Jeremy Blackburn, and Seungyeop Han. 2015. Exploring cyberbullying and other toxic behavior in team competition online games. In Proceedings of the 33rd annual ACM conference on human factors in computing systems.
[15] OpenAI. 2022. ChatGPT: Optimizing Language Model for Dialogue. OpenAI blog (2022).
[16] Joni Salminen, Hind Almerekhi, Milica Milenković, Soon-gyo Jung, Jisun An, Haewoon Kwak, and Bernard J Jansen. 2018. Anatomy of online hate: developing a taxonomy and machine learning models for identifying and classifying hate in online news media. In ICWSM.
[17] Maarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A. Smith, and Yejin Choi. 2020. Social Bias Frames: Reasoning about Social and Power Implications of Language. In ACL.
[18] Dominik Sobania, Martin Briesch, Carol Hanna, and Justyna Petke. 2023. An Analysis of the Automatic Bug Fixing Performance of ChatGPT. arXiv:2301.08653 (2023).
[19] Rohit Sridhar and Diyi Yang. 2022. Explaining Toxic Text via Knowledge Enhanced Text Generation. In NAACL.
[20] Bowen Zhang, Daijun Ding, and Liwen Jing. 2022. How would Stance Detection Techniques Evolve after the Launch of ChatGPT? arXiv:2212.14548 (2022).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://github.com/acheong08/ChatGPT&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
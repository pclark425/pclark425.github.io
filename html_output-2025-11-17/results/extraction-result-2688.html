<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2688 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2688</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2688</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-278788464</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.16477v1.pdf" target="_blank">Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery</a></p>
                <p><strong>Paper Abstract:</strong> With recent Nobel Prizes recognising AI contributions to science, Large Language Models (LLMs) are transforming scientific research by enhancing productivity and reshaping the scientific method. LLMs are now involved in experimental design, data analysis, and workflows, particularly in chemistry and biology. However, challenges such as hallucinations and reliability persist. In this contribution, we review how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery. We conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics. The transition to AI-driven science raises ethical questions about creativity, oversight, and responsibility. With careful guidance, LLMs could evolve into creative engines, driving transformative breakthroughs across scientific disciplines responsibly and effectively. However, the scientific community must also decide how much it leaves to LLMs to drive science, even when associations with 'reasoning', mostly currently undeserved, are made in exchange for the potential to explore hypothesis and solution regions that might otherwise remain unexplored by human exploration alone.</p>
                <p><strong>Cost:</strong> 0.032</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2688.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2688.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM outputs with retrieved external context by indexing and retrieving relevant documents, then conditioning generation on that context to reduce hallucination and provide up-to-date information.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Architecture combines a retrieval component (semantic/text embeddings + vector index) with an LLM; at inference the retriever returns relevant passages given a query which are provided as additional context to the LLM prompt to ground generation. Typically uses embedding models, a vector DB, and an LLM decoder that concatenates retrieved context into prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented / LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (applied across biology, chemistry, literature review, and other domains)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Uses retrieved literature and factual context as grounding to prompt LLMs to propose hypotheses; the LLM composes new hypotheses conditioned on retrieved documents.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Mentioned use: compare proposed hypotheses against retrieved literature to identify novelty (iterative use of literature as negative examples); no explicit metric reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility improved by grounding generated statements in retrieved source passages (citation-backed assertions); explicit metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Described qualitatively: RAG increases plausibility by grounding while allowing novelty through generative LLM conditioned on retrieved context; no formal trade-off optimization described.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational validation via checking retrieved evidence and cross-referencing sources; suggested combination with formal systems or programmatic checks for further validation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Using retrieved primary sources as traceable provenance for claims (citation grounding) to aid reproducibility; no protocol specified.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Grounding via retrieved source context to reduce unsupported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Cross-checking output against retrieved passages; self-RAG (where the model generates and then verifies its own references) is mentioned as a variation to detect hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Implicitly via provenance / citation confidence; paper notes RAG can reduce hallucinations but does not provide a formal uncertainty metric.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Qualitative claim: RAG reduces hallucinations and improves factuality versus plain LLM generation; no numeric comparisons provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reliant on quality and coverage of retrieval corpus; retrieval errors or missing up-to-date sources still allow hallucinations; translation between modalities can lose information.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2688.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that asks LLMs to generate intermediate step-by-step reasoning (a 'chain of thought') before answering, which empirically improves performance on multi-step reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Thought (CoT) prompting</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Prompt engineering technique that instructs LLMs to produce internal reasoning traces (stepwise explanations) prior to final answers; can be applied as few-shot or zero-shot prompts to elicit decomposed reasoning paths.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>prompting / LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (used for planning, experiment design, and reasoning in multiple domains)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Enables LLMs to decompose complex prompts, which can assist generation of more structured hypotheses by producing intermediate rationales that lead to hypothesis text.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is improved indirectly by making intermediate reasoning explicit, allowing humans or automated verifiers to inspect steps for errors.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Allows downstream verification of intermediate steps; combined with verification agents or formal checks for validation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Produces reproducible intermediate reasoning traces that can be archived alongside hypotheses for inspection.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>When combined with verification (self-verification, RAG), CoT decomposition can reduce error propagation; by itself it may still propagate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Decomposed checkpoints permit targeted fact checks at each reasoning step (enables chain-of-verification approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Not inherent; CoT outputs can be used by downstream modules to estimate confidence per step, but no specific method is defined in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported in referenced literature to improve multi-step reasoning; no numeric values provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Qualitatively improves complex reasoning vs. single-shot prompting; specific benchmarks cited but not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>CoT can still propagate errors; long chains can accumulate hallucinations and error without external verification.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2688.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-powered autonomous agent (e.g., AutoGPT / BabyAGI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Autonomous systems that use an LLM as the core planner/decision-maker, able to observe environments, call external tools/APIs, plan multi-step workflows, and act with little human intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM agents (AutoGPT, BabyAGI, multi-agent frameworks)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Architecture uses an LLM as the central reasoning/planning module, orchestrating tool calls, web searches, code execution, and interaction with other agents or hardware; often incorporates memory, skill libraries, and a task decomposition/iteration loop.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based agent / tool-augmented</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general; examples in chemistry and biology (automated experiment planning and execution)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Agents can iteratively propose hypotheses as subtasks while exploring objectives and interacting with tools and literature; generation emerges from planner prompts combined with retrieved evidence and tool outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Iterative improvement using literature comparison, negative example mining, and multi-agent debates are discussed as ways to assess novelty; no explicit quantitative metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Use of external tools (search, databases), human-in-the-loop confirmations, and automated verification (e.g., running code or experiments) to assess plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Argued qualitatively that agent architectures enable exploratory (novel) search while tool grounding and human checks retain plausibility; no formal optimization described.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Validation via nested loops: automated computational checks, experiments (where possible), human oversight, and use of formal verifiers where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Skill libraries and saved programmatic steps (e.g., code, JSON actions) are suggested to archive agent actions for reproducibility; no standardized protocol given.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Integrate RAG, tools, human confirmation, and hard-coded safety pipelines to reduce unsupported actions.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Multi-agent debate, repeated runs with voting, self-verification routines, and checking low-confidence outputs are proposed detection strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>The paper advocates for 'algorithmic confidence' scores across foundation model + tool + workflow, but concrete methods are not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Reliability, hallucinations, and need for rigorous evaluation across workflows; current agents may adapt unexpectedly and require supervision for safety-critical science.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2688.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Boiko LLM planner</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based chemical experiment planner (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous system that uses an LLM as a planner to design and orchestrate chemical experiments, including searching literature, computing experimental parameters, generating code, and calling lab tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent autonomous scientific research capabilities of large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-based planner for chemical experimentation (Boiko et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses an LLM as a flexible planner to reason about experimental steps, conduct web searches, compute parameters via generated Python code, and interact with lab instruments or external tools; emphasizes flexibility over hard-coded planners.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based / tool-augmented agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>chemistry (automated reaction execution and experimental workflows)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Planner reasons over objectives and prior knowledge to propose experimental plans and potential mechanistic hypotheses as part of planning; not described as an explicit hypothesis engine but produces experimental proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plans are checked via computational calculations (generated code) and iteratively refined from observations and search results; human oversight used for safety.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Iterative feedback loop: search > compute > plan > execute (tool) > feed results back to planner for refinement; human confirmation and safety pipelines recommended.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Use of generated code, logged search results, and programmatic plan representations to record experiments for reproducibility; no standardized protocol specified in review.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Integration with literature search (RAG), human-in-the-loop for disambiguation (e.g., ORGANA example), and hard-coded safety modules.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Iterative verification through tool outputs and human checks; contradictions across repeated runs can reveal hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Described as capable of performing chemical syntheses (e.g., Suzuki and Sonogashira) in automated setups per cited work; the review cites these as demonstrations of flexible planning.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires careful human oversight for lab safety; reliability and correctness of generated plans remain concerns; translation errors from plan to instrument actions possible.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2688.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CRISPR-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-powered agent for designing CRISPR gene-editing experiments, automating experimental design in the biological domain.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CRISPR-GPT</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>LLM agent tuned or prompted to design gene-editing experiments; integrates literature/tools to propose experimental designs specific to CRISPR workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based agent / domain-specific application</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biology / genomics / gene editing</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates experimental designs and associated mechanistic hypotheses by conditioning on genomic/biological context and domain constraints provided to the LLM agent.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is improved via domain-specific conditioning, tool usage, and (implicitly) human review; explicit metrics not provided.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Designed experiments can be executed or further evaluated by domain tools and human experts; specific validation reported in cited work but not quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Domain-specific grounding and tool integration to reduce unsupported claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires domain-specific safeguards; the review notes general LLM limitations (hallucination, reasoning) apply.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2688.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioDiscoveryAgent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioDiscoveryAgent</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI agent designed to plan genetic perturbation experiments, integrating LLM reasoning with experiment design tools in genomics.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioDiscoveryAgent</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An LLM-driven agent that proposes and designs genetic perturbation experiments, likely using retrieval, domain tools, and programmatic experiment descriptions to automate design workflows.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based agent / domain-specific</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>genomics / molecular biology</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates hypotheses about perturbation effects by synthesizing literature and domain data via LLM prompting and tool calls.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Combines literature grounding and domain-specific tools; human review is suggested for experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Designs intended to be validated experimentally and via computational checks; specifics are in cited preprint.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>RAG and tool grounding suggested as mitigation strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>As a domain agent, subject to LLM hallucinations and reasoning limits; requires rigorous validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2688.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Virtual Lab</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>The Virtual Lab (AI agents designing nanobodies)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An AI agent pipeline where LLM-driven agents designed novel SARS-CoV-2 nanobody binders that were experimentally validated, demonstrating LLMs as hypothesis generators leading to real-world validation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Virtual Lab (LLM agents for molecular design)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline of LLM agents that propose molecular designs (nanobody binders), interface with design/simulation tools and experimentalists, and produce candidates that are experimentally tested.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based agent / multi-modality with domain tools</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>molecular biology / protein engineering</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>LLMs generate proposed molecular designs and binding hypotheses by combining literature-derived knowledge with in silico design tools, iterative refinement and selection.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty assessed by screening against existing literature and databases and by human/expert evaluation; paper reports experimental validation as ultimate novelty test.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility assessed via in silico filters, domain-specific simulation/design tools, and downstream experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Pipeline emphasizes generation of novel candidates followed by computational and experimental filters to ensure plausibility before lab testing.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Candidates underwent experimental validation in the lab (wet-lab assays) as reported by the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Combination of computational records, candidate sequences, and experimental protocols support reproducibility; specific protocols are in the cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Use of domain tools and simulation filters to screen LLM proposals; human-in-the-loop checks prior to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported successful experimental validation of designed nanobodies in cited work (no numeric metrics provided in this review).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Generated novel nanobody binders against SARS-CoV-2 variants that were experimentally validated per the cited study.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires strong domain filtering and wet-lab capacity; LLM proposals still need experimental confirmation and may contain unsafe or non-viable designs without screening.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e2688.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evo / Evo2</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Evo and Evo 2 (foundation models for genome-scale modeling and generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Foundation language-model-style systems pretrained on genomic sequences (prokaryotic, phage, and eukaryotic genomes) that can predict function and generate molecular constructs, with experimental validation of certain generated constructs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sequence modeling and design from molecular to genome scale with Evo.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Evo / Evo 2</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Large-scale sequence models trained on millions of genomes to generate and predict functions across DNA, RNA, and protein modalities; capable of multimodal generation and zero-shot function prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>foundation model / sequence LLM</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>genomics, molecular biology, synthetic biology</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generative modeling of sequence space produces candidate molecular constructs and functional hypotheses (e.g., CRISPR-Cas complexes) which are proposed as testable designs.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty inferred by generating sequences not present in training corpora and by downstream experimental validation; no explicit novelty metric given in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility assessed via predicted functional scores and experimental assays; Evo-generated constructs had experimental validation in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Model enables high-diversity generation (novelty) with functional prediction scores used to prioritize plausible candidates for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>In vitro experimental assays validated function of generated CRISPR-Cas molecular complexes and transposable systems as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Model-generated sequences and training data provenance used to reproduce experiments; detailed protocols in cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Domain-specific pretraining and predictive scoring reduce implausible outputs; still requires empirical validation.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Functional prediction scores used as implicit confidence; no formal uncertainty framework described.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported zero-shot function prediction capability across DNA/RNA/protein modalities and experimental validation of some generated constructs; no numeric rates provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Experimental validation of Evo-generated CRISPR-Cas molecular complexes and transposable systems highlighted as first examples of language-model-driven proteinnucleic-acid co-design.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Training-data biases, interpretability challenges, and need for experimental validation; possible safety and biosecurity concerns when generating functional sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e2688.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-SR (Scientific Equation Discovery via programming with LLMs)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that iteratively generates symbolic hypotheses (equations) using LLMs, evaluates them, and uses the evaluation signal to refine search for better hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scientific Equation Discovery via Programming with Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM-SR</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines LLM generation of candidate symbolic equations with programmatic evaluation (e.g., fitting, error computation) in an iterative loop; LLM proposes hypotheses, a validator evaluates fit to data, and feedback refines proposals.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural-symbolic / LLM + symbolic regression</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>physics / data-driven equation discovery / general scientific modeling</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>LLM generates symbolic expressions/equations from prompts; generation guided by observed input-output data and past evaluation feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility assessed via computational fit/error metrics when hypotheses are translated to executable code and evaluated against data.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Iterative evaluation refines candidates toward both explanatory power (fit) and simplicity as encoded in scoring; details left to cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Implied: fit/error metrics from programmatic evaluation (e.g., MSE or other regression loss) used to score hypotheses, but not specified numerically in review.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational validation by executing translated hypotheses (e.g., in Python) on data to accept/discard candidates prior to experimental testing.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Translation of hypotheses into code and programmatic evaluation aids reproducibility; specifics in original paper.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Programmatic evaluation filters out unsupported symbolic hypotheses that fail empirical tests.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Disproof via executing generated hypotheses on held-out data to detect incorrect conjectures.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Not explicitly described beyond using evaluation scores as confidence signals.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on LLM's ability to correctly translate symbolic ideas to executable code and on adequacy of data for evaluation; auto-translation errors can introduce mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e2688.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AlphaProof</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AlphaProof (automated theorem proving system)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A system trained via reinforcement learning and formal verification to produce proofs in a formal language (LEAN), achieving performance comparable to top human competitors on challenging mathematical problems.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deepmind, Alphaproof, AI achieves silver-medal standard solving International Mathematical Olympiad problems.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AlphaProof</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Combines LLM reasoning with formal proof environment (LEAN): LLM generates formal proofs, the LEAN compiler verifies them, and reinforcement learning uses verification signals as reward to improve the prover.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM + formal verifier (neural-symbolic / reinforcement learning)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics / automated theorem proving</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generates candidate proof steps or conjectures in formal language; uses iterative proof search guided by learned policy.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility equates to provability: LEAN verification accepts only formally valid proofs; unprovable candidate steps are discarded.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Focused on correctness (provability) via formal verification rather than creative novelty; RL promotes solutions that lead to verifiable proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Binary formal verification (provable / not provable) is used as the primary quality signal.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Formal verification by the LEAN compiler provides definitive validation of generated proofs or disproofs.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Formalization in LEAN ensures reproducibility and auditability of proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Formal verification eliminates hallucinated (invalid) proofs by requiring machine-checkable correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>LEAN rejects invalid proof steps; the compiler provides explicit failure signals.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Not described beyond using verification success as a deterministic correctness signal.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Large corpus of informal problems translated into LEAN (~1M problems used for training per review).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Achieved mathematical capabilities comparable to human competitors at IMO level as reported; no precise numeric metrics provided in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperformed prior automated theorem provers in cited reports; treated as state-of-the-art in the review.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires large-scale formalized datasets and domain-specific formal systems; generalizing this approach beyond formalizable domains is challenging.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e2688.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A verification method that decomposes generated content into multiple fact checkpoints and uses separate agents/verification steps to validate each checkpoint, reducing hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain-of-Verification Reduces Hallucination in Large Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Chain-of-Verification</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline that decomposes an LLM's output into atomic facts/checkpoints; separate verifier agents or retrieval mechanisms evaluate each checkpoint and the original answer is updated or filtered based on verification outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>verification pipeline / multi-agent</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (applicable to literature claims, hypotheses, and factual statements)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generator per se; used to verify or filter hypotheses produced by LLMs by checking constituent factual claims.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility assessed by verifying each atomic claim against evidence sources or verifier agents; aggregated verification informs plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automated fact-checking of decomposed claims, optionally using retrieval or verifier LLMs, to accept/reject hypotheses before experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Explicit checkpoints and source citations can be logged to reproduce verification steps.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Detects and filters hallucinated atomic claims during verification before aggregation into final answers.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Separate verifier agents or retrieval-backed checks identify unsupported or contradicted checkpoints.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Aggregating verification outcomes across checkpoints yields a measure of confidence; no formal aggregate metric defined in review.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Reported to reduce hallucinations compared to unverified LLM outputs in cited work; no numeric values provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Effectiveness depends on verifier quality and retrieval coverage; decomposition granularity and verifier agreement thresholds are design choices that affect performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e2688.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>self-RAG (self-retrieval augmented generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant of RAG where the LLM itself generates candidate references or contexts and then verifies outputs against those self-generated references to reduce hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>self-RAG</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure where an LLM generates candidate supporting contexts or citations for its outputs and then rechecks or reconditions its generation on those self-produced references, creating an internal verification loop.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented / self-verification</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (used for grounding and reducing hallucinations)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>LLM generates hypotheses and simultaneously proposes internal supporting contexts which are used to re-evaluate or refine hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>LLM-generated references serve as provisional grounding; further retrieval or external checks improve plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Self-generated reference contexts are re-used to check consistency and reduce unsupported claims; may be combined with external retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Encourages generation of explicit supporting contexts and cross-checks to catch hallucinated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Detects contradictions between original output and self-generated references or across multiple self-generated contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Self-generated references can themselves be hallucinated; needs external grounding for robust correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e2688.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>self-consistency</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-Consistency (sampling + voting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that samples multiple reasoning traces/answers from an LLM and selects the most frequent answer as a more robust prediction, which also provides an empirical measure of uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Self-Consistency Improves Chain of Thought Reasoning in Language Models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-Consistency (multiple-sample voting)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Run the LLM multiple times with stochastic decoding to produce diverse chain-of-thought traces/answers, then aggregate (majority vote or frequency) to choose the final answer and estimate confidence by agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based ensemble / uncertainty estimation via sampling</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (used to improve reasoning robustness and provide uncertainty cues)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a direct generator; used to stabilize and estimate confidence for generated hypotheses by aggregating multiple samples.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility approximated by consensus among multiple LLM samples; high agreement suggests higher plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Consensus voting among samples used as lightweight pre-experiment validation; can reveal inconsistent outputs that require further checking.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Sampling seeds and prompt variants can be logged to reproduce the ensemble behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Sampling and majority voting can reduce idiosyncratic hallucinated outputs by preferring commonly generated responses.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Low consensus / high variance across samples signals potential hallucination or uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Consensus frequency across samples used as empirical confidence; paper suggests this helps human estimation of uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Reported improvement in reasoning correctness relative to single-sample chain-of-thought in cited literature; no numeric values in this review.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Consensus does not guarantee correctness (models can be consistently wrong); computationally expensive due to multiple samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.13">
                <h3 class="extraction-instance">Extracted Data Instance 13 (e2688.13)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Verification / Fact-checking methods (generalized)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-verification and multi-agent verification methods (e.g., Chain-of-Verification, multi-agent debate, self-reflection)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of methods that use decomposition, independent verification agents, debate, or self-reflection loops to check LLM outputs and reduce hallucinations or improve answer quality.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Verification families: Chain-of-Verification, multi-agent debate, self-verification, Reflexion, ReAct</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Includes methods that combine reasoning traces (CoT), separate verifier agents, multi-agent debates, and reinforcement-like reflection loops to iteratively identify and fix errors in LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>multi-agent / verification / LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general; applied to literature claims, planning, and hypothesis filtering</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Verification methods are applied post-generation to assess or refine hypotheses rather than generate them from scratch.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is evaluated via independent agents/verifiers, retrieval checks, and iteration; aggregation of verifier outputs yields plausibility estimate.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Automated verifier outputs, multi-agent consensus, and iterated reflection used to validate claims prior to experimental investment.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Logging verifier steps and debate transcripts provides provenance for reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Explicit verification pipelines, multi-agent cross-checks, and iterative reflection are used to detect and reduce hallucinated claims.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Contradictions between agents, verifier failures on checkpoints, and low-confidence outputs trigger detection.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Consensus levels and verifier agreement provide empirical uncertainty signals; paper calls for more predictive trustworthiness quantification frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Verifier quality and coverage limit effectiveness; may be computationally expensive and can still be fooled by consistent model errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.14">
                <h3 class="extraction-instance">Extracted Data Instance 14 (e2688.14)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stitch (Time Saves Nine)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stitch: Detecting and Mitigating Hallucinations by Validating Low-Confidence Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that detects and mitigates hallucinations by validating model generations that the model itself reports as low confidence, focusing verification resources where needed.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Stitch, Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Stitch (low-confidence validation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach that identifies low-confidence model outputs and allocates validation (retrieval, external checks) preferentially to those outputs to efficiently reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>confidence-guided verification / LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (fact-checking and hallucination mitigation)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Targets plausibility checks to outputs with low model-reported confidence, thereby increasing the likelihood that validated outputs are plausible.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Selective external validation (e.g., retrieval, programmatic checks) applied to low-confidence outputs prior to accepting claims.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Validation focused on most uncertain outputs reduces false claims while saving validation resources.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Detection based on model's own confidence estimates identifying candidates for checking.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Uses model-reported confidence to triage validation effort; the paper cites methods for enabling LLMs to express confidence.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Depends on model's ability to accurately report low confidence; poorly calibrated confidence undermines effectiveness.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.15">
                <h3 class="extraction-instance">Extracted Data Instance 15 (e2688.15)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Formal verification (LEAN / Prover9)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Formal proof systems and programmatic validation (e.g., LEAN, Prover9, autoformalization to Python)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Translating generated hypotheses or proofs into formal languages (LEAN, Prover9) or executable code (Python) to enable deterministic validation or disproof before experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Don't Trust: Verify --Grounding LLM Quantitative Reasoning with Autoformalization.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Formal verification via autoformalization and programmatic checks (LEAN, Prover9, Python translation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Approach translates natural-language hypotheses or reasoning into formal logic/proof systems or executable programs; compilers/verifiers then accept or reject candidates, enabling definitive validation or falsification.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>neural-symbolic / formal verification</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mathematics, logic, and any domain where hypotheses can be formalized (e.g., quantitative reasoning tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>LLMs generate hypotheses which are then autoformalized into formal languages or code for deterministic verification.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Formal provability or successful execution on data indicates plausibility; inability to verify disqualifies candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Binary verification outcome (proved / not proved) and programmatic fit metrics (e.g., computational error) when translated to code.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Formal verification (e.g., LEAN compiler) or programmatic execution in Python to disprove incorrect hypotheses prior to experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Formal encodings and executable code provide rigorous reproducibility and audit trails.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>False claims are filtered out by mechanistic verification; prevents acceptance of unsupported hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Verification failures and counterexamples serve as detection signals.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Verification success is a deterministic signal; no probabilistic uncertainty measure described, though programmatic fit scores can be used.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Many scientific hypotheses are not easily formalizable; autoformalization itself can introduce translation errors and is an active area of research.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.16">
                <h3 class="extraction-instance">Extracted Data Instance 16 (e2688.16)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-explanation / probing methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM self-explanation and probing (Logit Lens, representation engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Methods to interpret LLM internal representations: ask models to explain their reasoning or probe hidden activations with techniques like Logit Lens or representation engineering to improve interpretability and detect unreliable reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>the Logit Lens - interpreting GPT: the logit lens -LessWrong.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-explanation and probing (Logit Lens, representation engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Techniques include asking LLMs to output intermediate explanations (self-explanation), probing hidden states with Logit Lens to infer token-level semantics, and representation engineering to detect/control behaviours (e.g., dishonesty, harmfulness) via hidden activations.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>interpretability / probing / LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general (understanding model behaviour, detecting inconsistencies and potential hallucinations)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not primarily a hypothesis generator; can be used to inspect generated hypotheses and the model's internal rationale.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Inspecting hidden states and explanations can help identify implausible internal reasoning paths, flagging questionable hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Use of internal activations and self-explanations to audit the model's chain-of-thought; not a deterministic validator but aids human interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Logging hidden-state probes and explanations provides data for reproducible analysis of model behaviour.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Probing may detect signals correlated with unreliable outputs; representation engineering can be used to suppress undesirable behaviours.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Comparing self-explanations to observed behaviour and using activation patterns (logit lens) to detect inconsistencies indicative of hallucination.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Not directly producing calibrated uncertainty; explanation inconsistency can serve as a heuristic for uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>LLM self-explanations can be unfaithful; probing methods may not reliably predict counterfactual behaviour and require careful interpretation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2688.17">
                <h3 class="extraction-instance">Extracted Data Instance 17 (e2688.17)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Self-consistent / multi-run voting (uncertainty)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Self-consistent sampling and majority-vote uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An empirical uncertainty estimation method where multiple independent model completions are sampled and aggregated to produce a consensus answer and an empirical confidence estimate based on agreement frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Self-consistent multi-sample voting (uncertainty estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Procedure: sample N independent outputs (chains-of-thought or final answers) under stochastic decoding, then select the most frequent answer; the frequency distribution across samples provides an empirical uncertainty measure.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based ensemble / uncertainty estimation</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Used to estimate confidence in generated hypotheses by sampling multiple proposals and measuring consensus.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Higher inter-sample agreement indicates higher plausibility; low agreement flags uncertain or potentially hallucinated hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Consensus-based triage before expensive validation; multi-run disagreement suggests further verification needed.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Sampling seeds and prompt versions can be recorded to reproduce the ensemble.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Aggregation helps filter single-run hallucinations but cannot eliminate systematic errors.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Disagreement across runs identifies candidate hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Simple empirical confidence = fraction of runs agreeing on outcome; paper cites this as useful though not a formal calibrated probability.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Consensus may be wrong if model is systematically biased; computational cost increases with samples.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery', 'publication_date_yy_mm': '2025-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks. <em>(Rating: 2)</em></li>
                <li>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Emergent autonomous scientific research capabilities of large language models <em>(Rating: 2)</em></li>
                <li>CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments. <em>(Rating: 2)</em></li>
                <li>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation. <em>(Rating: 2)</em></li>
                <li>Sequence modeling and design from molecular to genome scale with Evo. <em>(Rating: 2)</em></li>
                <li>Scientific Equation Discovery via Programming with Large Language Models. <em>(Rating: 2)</em></li>
                <li>Deepmind, Alphaproof, AI achieves silver-medal standard solving International Mathematical Olympiad problems. <em>(Rating: 2)</em></li>
                <li>Chain-of-Verification Reduces Hallucination in Large Language Models. <em>(Rating: 2)</em></li>
                <li>Stitch, Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. <em>(Rating: 2)</em></li>
                <li>Self-Consistency Improves Chain of Thought Reasoning in Language Models. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2688",
    "paper_id": "paper-278788464",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A method that augments LLM outputs with retrieved external context by indexing and retrieving relevant documents, then conditioning generation on that context to reduce hallucination and provide up-to-date information.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "mention_or_use": "mention",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "Architecture combines a retrieval component (semantic/text embeddings + vector index) with an LLM; at inference the retriever returns relevant passages given a query which are provided as additional context to the LLM prompt to ground generation. Typically uses embedding models, a vector DB, and an LLM decoder that concatenates retrieved context into prompts.",
            "system_type": "retrieval-augmented / LLM-based",
            "scientific_domain": "general (applied across biology, chemistry, literature review, and other domains)",
            "hypothesis_generation_method": "Uses retrieved literature and factual context as grounding to prompt LLMs to propose hypotheses; the LLM composes new hypotheses conditioned on retrieved documents.",
            "novelty_assessment_method": "Mentioned use: compare proposed hypotheses against retrieved literature to identify novelty (iterative use of literature as negative examples); no explicit metric reported in this paper.",
            "plausibility_assessment_method": "Plausibility improved by grounding generated statements in retrieved source passages (citation-backed assertions); explicit metrics not provided.",
            "novelty_plausibility_balance": "Described qualitatively: RAG increases plausibility by grounding while allowing novelty through generative LLM conditioned on retrieved context; no formal trade-off optimization described.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational validation via checking retrieved evidence and cross-referencing sources; suggested combination with formal systems or programmatic checks for further validation.",
            "reproducibility_measures": "Using retrieved primary sources as traceable provenance for claims (citation grounding) to aid reproducibility; no protocol specified.",
            "hallucination_prevention_method": "Grounding via retrieved source context to reduce unsupported claims.",
            "hallucination_detection_method": "Cross-checking output against retrieved passages; self-RAG (where the model generates and then verifies its own references) is mentioned as a variation to detect hallucination.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Implicitly via provenance / citation confidence; paper notes RAG can reduce hallucinations but does not provide a formal uncertainty metric.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": "Qualitative claim: RAG reduces hallucinations and improves factuality versus plain LLM generation; no numeric comparisons provided in this paper.",
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Reliant on quality and coverage of retrieval corpus; retrieval errors or missing up-to-date sources still allow hallucinations; translation between modalities can lose information.",
            "uuid": "e2688.0",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that asks LLMs to generate intermediate step-by-step reasoning (a 'chain of thought') before answering, which empirically improves performance on multi-step reasoning tasks.",
            "citation_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "mention_or_use": "mention",
            "system_name": "Chain-of-Thought (CoT) prompting",
            "system_description": "Prompt engineering technique that instructs LLMs to produce internal reasoning traces (stepwise explanations) prior to final answers; can be applied as few-shot or zero-shot prompts to elicit decomposed reasoning paths.",
            "system_type": "prompting / LLM-based",
            "scientific_domain": "general (used for planning, experiment design, and reasoning in multiple domains)",
            "hypothesis_generation_method": "Enables LLMs to decompose complex prompts, which can assist generation of more structured hypotheses by producing intermediate rationales that lead to hypothesis text.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is improved indirectly by making intermediate reasoning explicit, allowing humans or automated verifiers to inspect steps for errors.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Allows downstream verification of intermediate steps; combined with verification agents or formal checks for validation.",
            "reproducibility_measures": "Produces reproducible intermediate reasoning traces that can be archived alongside hypotheses for inspection.",
            "hallucination_prevention_method": "When combined with verification (self-verification, RAG), CoT decomposition can reduce error propagation; by itself it may still propagate hallucinations.",
            "hallucination_detection_method": "Decomposed checkpoints permit targeted fact checks at each reasoning step (enables chain-of-verification approaches).",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Not inherent; CoT outputs can be used by downstream modules to estimate confidence per step, but no specific method is defined in the paper.",
            "benchmark_dataset": null,
            "performance_metrics": "Reported in referenced literature to improve multi-step reasoning; no numeric values provided in this review.",
            "comparison_with_baseline": "Qualitatively improves complex reasoning vs. single-shot prompting; specific benchmarks cited but not quantified here.",
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "CoT can still propagate errors; long chains can accumulate hallucinations and error without external verification.",
            "uuid": "e2688.1",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM agent",
            "name_full": "LLM-powered autonomous agent (e.g., AutoGPT / BabyAGI)",
            "brief_description": "Autonomous systems that use an LLM as the core planner/decision-maker, able to observe environments, call external tools/APIs, plan multi-step workflows, and act with little human intervention.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "LLM agents (AutoGPT, BabyAGI, multi-agent frameworks)",
            "system_description": "Architecture uses an LLM as the central reasoning/planning module, orchestrating tool calls, web searches, code execution, and interaction with other agents or hardware; often incorporates memory, skill libraries, and a task decomposition/iteration loop.",
            "system_type": "LLM-based agent / tool-augmented",
            "scientific_domain": "general; examples in chemistry and biology (automated experiment planning and execution)",
            "hypothesis_generation_method": "Agents can iteratively propose hypotheses as subtasks while exploring objectives and interacting with tools and literature; generation emerges from planner prompts combined with retrieved evidence and tool outputs.",
            "novelty_assessment_method": "Iterative improvement using literature comparison, negative example mining, and multi-agent debates are discussed as ways to assess novelty; no explicit quantitative metric provided.",
            "plausibility_assessment_method": "Use of external tools (search, databases), human-in-the-loop confirmations, and automated verification (e.g., running code or experiments) to assess plausibility.",
            "novelty_plausibility_balance": "Argued qualitatively that agent architectures enable exploratory (novel) search while tool grounding and human checks retain plausibility; no formal optimization described.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Validation via nested loops: automated computational checks, experiments (where possible), human oversight, and use of formal verifiers where applicable.",
            "reproducibility_measures": "Skill libraries and saved programmatic steps (e.g., code, JSON actions) are suggested to archive agent actions for reproducibility; no standardized protocol given.",
            "hallucination_prevention_method": "Integrate RAG, tools, human confirmation, and hard-coded safety pipelines to reduce unsupported actions.",
            "hallucination_detection_method": "Multi-agent debate, repeated runs with voting, self-verification routines, and checking low-confidence outputs are proposed detection strategies.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "The paper advocates for 'algorithmic confidence' scores across foundation model + tool + workflow, but concrete methods are not specified.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Reliability, hallucinations, and need for rigorous evaluation across workflows; current agents may adapt unexpectedly and require supervision for safety-critical science.",
            "uuid": "e2688.2",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Boiko LLM planner",
            "name_full": "LLM-based chemical experiment planner (Boiko et al.)",
            "brief_description": "An autonomous system that uses an LLM as a planner to design and orchestrate chemical experiments, including searching literature, computing experimental parameters, generating code, and calling lab tools.",
            "citation_title": "Emergent autonomous scientific research capabilities of large language models",
            "mention_or_use": "mention",
            "system_name": "LLM-based planner for chemical experimentation (Boiko et al.)",
            "system_description": "Uses an LLM as a flexible planner to reason about experimental steps, conduct web searches, compute parameters via generated Python code, and interact with lab instruments or external tools; emphasizes flexibility over hard-coded planners.",
            "system_type": "LLM-based / tool-augmented agent",
            "scientific_domain": "chemistry (automated reaction execution and experimental workflows)",
            "hypothesis_generation_method": "Planner reasons over objectives and prior knowledge to propose experimental plans and potential mechanistic hypotheses as part of planning; not described as an explicit hypothesis engine but produces experimental proposals.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plans are checked via computational calculations (generated code) and iteratively refined from observations and search results; human oversight used for safety.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Iterative feedback loop: search &gt; compute &gt; plan &gt; execute (tool) &gt; feed results back to planner for refinement; human confirmation and safety pipelines recommended.",
            "reproducibility_measures": "Use of generated code, logged search results, and programmatic plan representations to record experiments for reproducibility; no standardized protocol specified in review.",
            "hallucination_prevention_method": "Integration with literature search (RAG), human-in-the-loop for disambiguation (e.g., ORGANA example), and hard-coded safety modules.",
            "hallucination_detection_method": "Iterative verification through tool outputs and human checks; contradictions across repeated runs can reveal hallucinations.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Described as capable of performing chemical syntheses (e.g., Suzuki and Sonogashira) in automated setups per cited work; the review cites these as demonstrations of flexible planning.",
            "limitations": "Requires careful human oversight for lab safety; reliability and correctness of generated plans remain concerns; translation errors from plan to instrument actions possible.",
            "uuid": "e2688.3",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "CRISPR-GPT",
            "name_full": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments",
            "brief_description": "An LLM-powered agent for designing CRISPR gene-editing experiments, automating experimental design in the biological domain.",
            "citation_title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.",
            "mention_or_use": "mention",
            "system_name": "CRISPR-GPT",
            "system_description": "LLM agent tuned or prompted to design gene-editing experiments; integrates literature/tools to propose experimental designs specific to CRISPR workflows.",
            "system_type": "LLM-based agent / domain-specific application",
            "scientific_domain": "biology / genomics / gene editing",
            "hypothesis_generation_method": "Generates experimental designs and associated mechanistic hypotheses by conditioning on genomic/biological context and domain constraints provided to the LLM agent.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is improved via domain-specific conditioning, tool usage, and (implicitly) human review; explicit metrics not provided.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Designed experiments can be executed or further evaluated by domain tools and human experts; specific validation reported in cited work but not quantified here.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Domain-specific grounding and tool integration to reduce unsupported claims.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Requires domain-specific safeguards; the review notes general LLM limitations (hallucination, reasoning) apply.",
            "uuid": "e2688.4",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "BioDiscoveryAgent",
            "name_full": "BioDiscoveryAgent",
            "brief_description": "An AI agent designed to plan genetic perturbation experiments, integrating LLM reasoning with experiment design tools in genomics.",
            "citation_title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments.",
            "mention_or_use": "mention",
            "system_name": "BioDiscoveryAgent",
            "system_description": "An LLM-driven agent that proposes and designs genetic perturbation experiments, likely using retrieval, domain tools, and programmatic experiment descriptions to automate design workflows.",
            "system_type": "LLM-based agent / domain-specific",
            "scientific_domain": "genomics / molecular biology",
            "hypothesis_generation_method": "Generates hypotheses about perturbation effects by synthesizing literature and domain data via LLM prompting and tool calls.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Combines literature grounding and domain-specific tools; human review is suggested for experimental validation.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Designs intended to be validated experimentally and via computational checks; specifics are in cited preprint.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "RAG and tool grounding suggested as mitigation strategies.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "As a domain agent, subject to LLM hallucinations and reasoning limits; requires rigorous validation.",
            "uuid": "e2688.5",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Virtual Lab",
            "name_full": "The Virtual Lab (AI agents designing nanobodies)",
            "brief_description": "An AI agent pipeline where LLM-driven agents designed novel SARS-CoV-2 nanobody binders that were experimentally validated, demonstrating LLMs as hypothesis generators leading to real-world validation.",
            "citation_title": "The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation.",
            "mention_or_use": "mention",
            "system_name": "Virtual Lab (LLM agents for molecular design)",
            "system_description": "Pipeline of LLM agents that propose molecular designs (nanobody binders), interface with design/simulation tools and experimentalists, and produce candidates that are experimentally tested.",
            "system_type": "LLM-based agent / multi-modality with domain tools",
            "scientific_domain": "molecular biology / protein engineering",
            "hypothesis_generation_method": "LLMs generate proposed molecular designs and binding hypotheses by combining literature-derived knowledge with in silico design tools, iterative refinement and selection.",
            "novelty_assessment_method": "Novelty assessed by screening against existing literature and databases and by human/expert evaluation; paper reports experimental validation as ultimate novelty test.",
            "plausibility_assessment_method": "Plausibility assessed via in silico filters, domain-specific simulation/design tools, and downstream experimental validation.",
            "novelty_plausibility_balance": "Pipeline emphasizes generation of novel candidates followed by computational and experimental filters to ensure plausibility before lab testing.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Candidates underwent experimental validation in the lab (wet-lab assays) as reported by the cited work.",
            "reproducibility_measures": "Combination of computational records, candidate sequences, and experimental protocols support reproducibility; specific protocols are in the cited paper.",
            "hallucination_prevention_method": "Use of domain tools and simulation filters to screen LLM proposals; human-in-the-loop checks prior to experiments.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": "Reported successful experimental validation of designed nanobodies in cited work (no numeric metrics provided in this review).",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Generated novel nanobody binders against SARS-CoV-2 variants that were experimentally validated per the cited study.",
            "limitations": "Requires strong domain filtering and wet-lab capacity; LLM proposals still need experimental confirmation and may contain unsafe or non-viable designs without screening.",
            "uuid": "e2688.6",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Evo / Evo2",
            "name_full": "Evo and Evo 2 (foundation models for genome-scale modeling and generation)",
            "brief_description": "Foundation language-model-style systems pretrained on genomic sequences (prokaryotic, phage, and eukaryotic genomes) that can predict function and generate molecular constructs, with experimental validation of certain generated constructs.",
            "citation_title": "Sequence modeling and design from molecular to genome scale with Evo.",
            "mention_or_use": "mention",
            "system_name": "Evo / Evo 2",
            "system_description": "Large-scale sequence models trained on millions of genomes to generate and predict functions across DNA, RNA, and protein modalities; capable of multimodal generation and zero-shot function prediction.",
            "system_type": "foundation model / sequence LLM",
            "scientific_domain": "genomics, molecular biology, synthetic biology",
            "hypothesis_generation_method": "Generative modeling of sequence space produces candidate molecular constructs and functional hypotheses (e.g., CRISPR-Cas complexes) which are proposed as testable designs.",
            "novelty_assessment_method": "Novelty inferred by generating sequences not present in training corpora and by downstream experimental validation; no explicit novelty metric given in this review.",
            "plausibility_assessment_method": "Plausibility assessed via predicted functional scores and experimental assays; Evo-generated constructs had experimental validation in cited work.",
            "novelty_plausibility_balance": "Model enables high-diversity generation (novelty) with functional prediction scores used to prioritize plausible candidates for experiments.",
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "In vitro experimental assays validated function of generated CRISPR-Cas molecular complexes and transposable systems as reported.",
            "reproducibility_measures": "Model-generated sequences and training data provenance used to reproduce experiments; detailed protocols in cited work.",
            "hallucination_prevention_method": "Domain-specific pretraining and predictive scoring reduce implausible outputs; still requires empirical validation.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Functional prediction scores used as implicit confidence; no formal uncertainty framework described.",
            "benchmark_dataset": null,
            "performance_metrics": "Reported zero-shot function prediction capability across DNA/RNA/protein modalities and experimental validation of some generated constructs; no numeric rates provided here.",
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": "Experimental validation of Evo-generated CRISPR-Cas molecular complexes and transposable systems highlighted as first examples of language-model-driven proteinnucleic-acid co-design.",
            "limitations": "Training-data biases, interpretability challenges, and need for experimental validation; possible safety and biosecurity concerns when generating functional sequences.",
            "uuid": "e2688.7",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "LLM-SR",
            "name_full": "LLM-SR (Scientific Equation Discovery via programming with LLMs)",
            "brief_description": "An approach that iteratively generates symbolic hypotheses (equations) using LLMs, evaluates them, and uses the evaluation signal to refine search for better hypotheses.",
            "citation_title": "Scientific Equation Discovery via Programming with Large Language Models.",
            "mention_or_use": "mention",
            "system_name": "LLM-SR",
            "system_description": "Combines LLM generation of candidate symbolic equations with programmatic evaluation (e.g., fitting, error computation) in an iterative loop; LLM proposes hypotheses, a validator evaluates fit to data, and feedback refines proposals.",
            "system_type": "neural-symbolic / LLM + symbolic regression",
            "scientific_domain": "physics / data-driven equation discovery / general scientific modeling",
            "hypothesis_generation_method": "LLM generates symbolic expressions/equations from prompts; generation guided by observed input-output data and past evaluation feedback.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility assessed via computational fit/error metrics when hypotheses are translated to executable code and evaluated against data.",
            "novelty_plausibility_balance": "Iterative evaluation refines candidates toward both explanatory power (fit) and simplicity as encoded in scoring; details left to cited work.",
            "hypothesis_quality_metrics": "Implied: fit/error metrics from programmatic evaluation (e.g., MSE or other regression loss) used to score hypotheses, but not specified numerically in review.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational validation by executing translated hypotheses (e.g., in Python) on data to accept/discard candidates prior to experimental testing.",
            "reproducibility_measures": "Translation of hypotheses into code and programmatic evaluation aids reproducibility; specifics in original paper.",
            "hallucination_prevention_method": "Programmatic evaluation filters out unsupported symbolic hypotheses that fail empirical tests.",
            "hallucination_detection_method": "Disproof via executing generated hypotheses on held-out data to detect incorrect conjectures.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Not explicitly described beyond using evaluation scores as confidence signals.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Relies on LLM's ability to correctly translate symbolic ideas to executable code and on adequacy of data for evaluation; auto-translation errors can introduce mistakes.",
            "uuid": "e2688.8",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "AlphaProof",
            "name_full": "AlphaProof (automated theorem proving system)",
            "brief_description": "A system trained via reinforcement learning and formal verification to produce proofs in a formal language (LEAN), achieving performance comparable to top human competitors on challenging mathematical problems.",
            "citation_title": "Deepmind, Alphaproof, AI achieves silver-medal standard solving International Mathematical Olympiad problems.",
            "mention_or_use": "mention",
            "system_name": "AlphaProof",
            "system_description": "Combines LLM reasoning with formal proof environment (LEAN): LLM generates formal proofs, the LEAN compiler verifies them, and reinforcement learning uses verification signals as reward to improve the prover.",
            "system_type": "LLM + formal verifier (neural-symbolic / reinforcement learning)",
            "scientific_domain": "mathematics / automated theorem proving",
            "hypothesis_generation_method": "Generates candidate proof steps or conjectures in formal language; uses iterative proof search guided by learned policy.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility equates to provability: LEAN verification accepts only formally valid proofs; unprovable candidate steps are discarded.",
            "novelty_plausibility_balance": "Focused on correctness (provability) via formal verification rather than creative novelty; RL promotes solutions that lead to verifiable proofs.",
            "hypothesis_quality_metrics": "Binary formal verification (provable / not provable) is used as the primary quality signal.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Formal verification by the LEAN compiler provides definitive validation of generated proofs or disproofs.",
            "reproducibility_measures": "Formalization in LEAN ensures reproducibility and auditability of proofs.",
            "hallucination_prevention_method": "Formal verification eliminates hallucinated (invalid) proofs by requiring machine-checkable correctness.",
            "hallucination_detection_method": "LEAN rejects invalid proof steps; the compiler provides explicit failure signals.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Not described beyond using verification success as a deterministic correctness signal.",
            "benchmark_dataset": "Large corpus of informal problems translated into LEAN (~1M problems used for training per review).",
            "performance_metrics": "Achieved mathematical capabilities comparable to human competitors at IMO level as reported; no precise numeric metrics provided in this review.",
            "comparison_with_baseline": "Outperformed prior automated theorem provers in cited reports; treated as state-of-the-art in the review.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Requires large-scale formalized datasets and domain-specific formal systems; generalizing this approach beyond formalizable domains is challenging.",
            "uuid": "e2688.9",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Chain-of-Verification",
            "name_full": "Chain-of-Verification",
            "brief_description": "A verification method that decomposes generated content into multiple fact checkpoints and uses separate agents/verification steps to validate each checkpoint, reducing hallucinations.",
            "citation_title": "Chain-of-Verification Reduces Hallucination in Large Language Models.",
            "mention_or_use": "mention",
            "system_name": "Chain-of-Verification",
            "system_description": "Pipeline that decomposes an LLM's output into atomic facts/checkpoints; separate verifier agents or retrieval mechanisms evaluate each checkpoint and the original answer is updated or filtered based on verification outcomes.",
            "system_type": "verification pipeline / multi-agent",
            "scientific_domain": "general (applicable to literature claims, hypotheses, and factual statements)",
            "hypothesis_generation_method": "Not a generator per se; used to verify or filter hypotheses produced by LLMs by checking constituent factual claims.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility assessed by verifying each atomic claim against evidence sources or verifier agents; aggregated verification informs plausibility.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automated fact-checking of decomposed claims, optionally using retrieval or verifier LLMs, to accept/reject hypotheses before experiments.",
            "reproducibility_measures": "Explicit checkpoints and source citations can be logged to reproduce verification steps.",
            "hallucination_prevention_method": "Detects and filters hallucinated atomic claims during verification before aggregation into final answers.",
            "hallucination_detection_method": "Separate verifier agents or retrieval-backed checks identify unsupported or contradicted checkpoints.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Aggregating verification outcomes across checkpoints yields a measure of confidence; no formal aggregate metric defined in review.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": "Reported to reduce hallucinations compared to unverified LLM outputs in cited work; no numeric values provided here.",
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Effectiveness depends on verifier quality and retrieval coverage; decomposition granularity and verifier agreement thresholds are design choices that affect performance.",
            "uuid": "e2688.10",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "self-RAG",
            "name_full": "self-RAG (self-retrieval augmented generation)",
            "brief_description": "A variant of RAG where the LLM itself generates candidate references or contexts and then verifies outputs against those self-generated references to reduce hallucination.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "self-RAG",
            "system_description": "Procedure where an LLM generates candidate supporting contexts or citations for its outputs and then rechecks or reconditions its generation on those self-produced references, creating an internal verification loop.",
            "system_type": "retrieval-augmented / self-verification",
            "scientific_domain": "general (used for grounding and reducing hallucinations)",
            "hypothesis_generation_method": "LLM generates hypotheses and simultaneously proposes internal supporting contexts which are used to re-evaluate or refine hypotheses.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "LLM-generated references serve as provisional grounding; further retrieval or external checks improve plausibility.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Self-generated reference contexts are re-used to check consistency and reduce unsupported claims; may be combined with external retrieval.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Encourages generation of explicit supporting contexts and cross-checks to catch hallucinated claims.",
            "hallucination_detection_method": "Detects contradictions between original output and self-generated references or across multiple self-generated contexts.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Self-generated references can themselves be hallucinated; needs external grounding for robust correctness.",
            "uuid": "e2688.11",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "self-consistency",
            "name_full": "Self-Consistency (sampling + voting)",
            "brief_description": "A method that samples multiple reasoning traces/answers from an LLM and selects the most frequent answer as a more robust prediction, which also provides an empirical measure of uncertainty.",
            "citation_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "mention_or_use": "mention",
            "system_name": "Self-Consistency (multiple-sample voting)",
            "system_description": "Run the LLM multiple times with stochastic decoding to produce diverse chain-of-thought traces/answers, then aggregate (majority vote or frequency) to choose the final answer and estimate confidence by agreement.",
            "system_type": "LLM-based ensemble / uncertainty estimation via sampling",
            "scientific_domain": "general (used to improve reasoning robustness and provide uncertainty cues)",
            "hypothesis_generation_method": "Not a direct generator; used to stabilize and estimate confidence for generated hypotheses by aggregating multiple samples.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility approximated by consensus among multiple LLM samples; high agreement suggests higher plausibility.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Consensus voting among samples used as lightweight pre-experiment validation; can reveal inconsistent outputs that require further checking.",
            "reproducibility_measures": "Sampling seeds and prompt variants can be logged to reproduce the ensemble behaviour.",
            "hallucination_prevention_method": "Sampling and majority voting can reduce idiosyncratic hallucinated outputs by preferring commonly generated responses.",
            "hallucination_detection_method": "Low consensus / high variance across samples signals potential hallucination or uncertainty.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Consensus frequency across samples used as empirical confidence; paper suggests this helps human estimation of uncertainty.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": "Reported improvement in reasoning correctness relative to single-sample chain-of-thought in cited literature; no numeric values in this review.",
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Consensus does not guarantee correctness (models can be consistently wrong); computationally expensive due to multiple samples.",
            "uuid": "e2688.12",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Chain-of-Verification / Fact-checking methods (generalized)",
            "name_full": "Self-verification and multi-agent verification methods (e.g., Chain-of-Verification, multi-agent debate, self-reflection)",
            "brief_description": "A family of methods that use decomposition, independent verification agents, debate, or self-reflection loops to check LLM outputs and reduce hallucinations or improve answer quality.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Verification families: Chain-of-Verification, multi-agent debate, self-verification, Reflexion, ReAct",
            "system_description": "Includes methods that combine reasoning traces (CoT), separate verifier agents, multi-agent debates, and reinforcement-like reflection loops to iteratively identify and fix errors in LLM outputs.",
            "system_type": "multi-agent / verification / LLM-based",
            "scientific_domain": "general; applied to literature claims, planning, and hypothesis filtering",
            "hypothesis_generation_method": "Verification methods are applied post-generation to assess or refine hypotheses rather than generate them from scratch.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is evaluated via independent agents/verifiers, retrieval checks, and iteration; aggregation of verifier outputs yields plausibility estimate.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Automated verifier outputs, multi-agent consensus, and iterated reflection used to validate claims prior to experimental investment.",
            "reproducibility_measures": "Logging verifier steps and debate transcripts provides provenance for reproducibility.",
            "hallucination_prevention_method": "Explicit verification pipelines, multi-agent cross-checks, and iterative reflection are used to detect and reduce hallucinated claims.",
            "hallucination_detection_method": "Contradictions between agents, verifier failures on checkpoints, and low-confidence outputs trigger detection.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Consensus levels and verifier agreement provide empirical uncertainty signals; paper calls for more predictive trustworthiness quantification frameworks.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Verifier quality and coverage limit effectiveness; may be computationally expensive and can still be fooled by consistent model errors.",
            "uuid": "e2688.13",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Stitch (Time Saves Nine)",
            "name_full": "Stitch: Detecting and Mitigating Hallucinations by Validating Low-Confidence Generation",
            "brief_description": "A method that detects and mitigates hallucinations by validating model generations that the model itself reports as low confidence, focusing verification resources where needed.",
            "citation_title": "Stitch, Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation.",
            "mention_or_use": "mention",
            "system_name": "Stitch (low-confidence validation)",
            "system_description": "Approach that identifies low-confidence model outputs and allocates validation (retrieval, external checks) preferentially to those outputs to efficiently reduce hallucinations.",
            "system_type": "confidence-guided verification / LLM-based",
            "scientific_domain": "general (fact-checking and hallucination mitigation)",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Targets plausibility checks to outputs with low model-reported confidence, thereby increasing the likelihood that validated outputs are plausible.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Selective external validation (e.g., retrieval, programmatic checks) applied to low-confidence outputs prior to accepting claims.",
            "reproducibility_measures": null,
            "hallucination_prevention_method": "Validation focused on most uncertain outputs reduces false claims while saving validation resources.",
            "hallucination_detection_method": "Detection based on model's own confidence estimates identifying candidates for checking.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Uses model-reported confidence to triage validation effort; the paper cites methods for enabling LLMs to express confidence.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Depends on model's ability to accurately report low confidence; poorly calibrated confidence undermines effectiveness.",
            "uuid": "e2688.14",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Formal verification (LEAN / Prover9)",
            "name_full": "Formal proof systems and programmatic validation (e.g., LEAN, Prover9, autoformalization to Python)",
            "brief_description": "Translating generated hypotheses or proofs into formal languages (LEAN, Prover9) or executable code (Python) to enable deterministic validation or disproof before experiments.",
            "citation_title": "Don't Trust: Verify --Grounding LLM Quantitative Reasoning with Autoformalization.",
            "mention_or_use": "mention",
            "system_name": "Formal verification via autoformalization and programmatic checks (LEAN, Prover9, Python translation)",
            "system_description": "Approach translates natural-language hypotheses or reasoning into formal logic/proof systems or executable programs; compilers/verifiers then accept or reject candidates, enabling definitive validation or falsification.",
            "system_type": "neural-symbolic / formal verification",
            "scientific_domain": "mathematics, logic, and any domain where hypotheses can be formalized (e.g., quantitative reasoning tasks)",
            "hypothesis_generation_method": "LLMs generate hypotheses which are then autoformalized into formal languages or code for deterministic verification.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Formal provability or successful execution on data indicates plausibility; inability to verify disqualifies candidates.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Binary verification outcome (proved / not proved) and programmatic fit metrics (e.g., computational error) when translated to code.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Formal verification (e.g., LEAN compiler) or programmatic execution in Python to disprove incorrect hypotheses prior to experiments.",
            "reproducibility_measures": "Formal encodings and executable code provide rigorous reproducibility and audit trails.",
            "hallucination_prevention_method": "False claims are filtered out by mechanistic verification; prevents acceptance of unsupported hypotheses.",
            "hallucination_detection_method": "Verification failures and counterexamples serve as detection signals.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Verification success is a deterministic signal; no probabilistic uncertainty measure described, though programmatic fit scores can be used.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Many scientific hypotheses are not easily formalizable; autoformalization itself can introduce translation errors and is an active area of research.",
            "uuid": "e2688.15",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-explanation / probing methods",
            "name_full": "LLM self-explanation and probing (Logit Lens, representation engineering)",
            "brief_description": "Methods to interpret LLM internal representations: ask models to explain their reasoning or probe hidden activations with techniques like Logit Lens or representation engineering to improve interpretability and detect unreliable reasoning.",
            "citation_title": "the Logit Lens - interpreting GPT: the logit lens -LessWrong.",
            "mention_or_use": "mention",
            "system_name": "Self-explanation and probing (Logit Lens, representation engineering)",
            "system_description": "Techniques include asking LLMs to output intermediate explanations (self-explanation), probing hidden states with Logit Lens to infer token-level semantics, and representation engineering to detect/control behaviours (e.g., dishonesty, harmfulness) via hidden activations.",
            "system_type": "interpretability / probing / LLM-based",
            "scientific_domain": "general (understanding model behaviour, detecting inconsistencies and potential hallucinations)",
            "hypothesis_generation_method": "Not primarily a hypothesis generator; can be used to inspect generated hypotheses and the model's internal rationale.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Inspecting hidden states and explanations can help identify implausible internal reasoning paths, flagging questionable hypotheses.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Use of internal activations and self-explanations to audit the model's chain-of-thought; not a deterministic validator but aids human interpretation.",
            "reproducibility_measures": "Logging hidden-state probes and explanations provides data for reproducible analysis of model behaviour.",
            "hallucination_prevention_method": "Probing may detect signals correlated with unreliable outputs; representation engineering can be used to suppress undesirable behaviours.",
            "hallucination_detection_method": "Comparing self-explanations to observed behaviour and using activation patterns (logit lens) to detect inconsistencies indicative of hallucination.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Not directly producing calibrated uncertainty; explanation inconsistency can serve as a heuristic for uncertainty.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "LLM self-explanations can be unfaithful; probing methods may not reliably predict counterfactual behaviour and require careful interpretation.",
            "uuid": "e2688.16",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        },
        {
            "name_short": "Self-consistent / multi-run voting (uncertainty)",
            "name_full": "Self-consistent sampling and majority-vote uncertainty estimation",
            "brief_description": "An empirical uncertainty estimation method where multiple independent model completions are sampled and aggregated to produce a consensus answer and an empirical confidence estimate based on agreement frequency.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Self-consistent multi-sample voting (uncertainty estimation)",
            "system_description": "Procedure: sample N independent outputs (chains-of-thought or final answers) under stochastic decoding, then select the most frequent answer; the frequency distribution across samples provides an empirical uncertainty measure.",
            "system_type": "LLM-based ensemble / uncertainty estimation",
            "scientific_domain": "general",
            "hypothesis_generation_method": "Used to estimate confidence in generated hypotheses by sampling multiple proposals and measuring consensus.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Higher inter-sample agreement indicates higher plausibility; low agreement flags uncertain or potentially hallucinated hypotheses.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Consensus-based triage before expensive validation; multi-run disagreement suggests further verification needed.",
            "reproducibility_measures": "Sampling seeds and prompt versions can be recorded to reproduce the ensemble.",
            "hallucination_prevention_method": "Aggregation helps filter single-run hallucinations but cannot eliminate systematic errors.",
            "hallucination_detection_method": "Disagreement across runs identifies candidate hallucinations.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Simple empirical confidence = fraction of runs agreeing on outcome; paper cites this as useful though not a formal calibrated probability.",
            "benchmark_dataset": null,
            "performance_metrics": null,
            "comparison_with_baseline": null,
            "validated_on_real_science": null,
            "novel_discoveries": null,
            "limitations": "Consensus may be wrong if model is systematically biased; computational cost increases with samples.",
            "uuid": "e2688.17",
            "source_info": {
                "paper_title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
                "publication_date_yy_mm": "2025-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks.",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.",
            "rating": 2
        },
        {
            "paper_title": "Emergent autonomous scientific research capabilities of large language models",
            "rating": 2
        },
        {
            "paper_title": "CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments.",
            "rating": 2
        },
        {
            "paper_title": "The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation.",
            "rating": 2
        },
        {
            "paper_title": "Sequence modeling and design from molecular to genome scale with Evo.",
            "rating": 2
        },
        {
            "paper_title": "Scientific Equation Discovery via Programming with Large Language Models.",
            "rating": 2
        },
        {
            "paper_title": "Deepmind, Alphaproof, AI achieves silver-medal standard solving International Mathematical Olympiad problems.",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-Verification Reduces Hallucination in Large Language Models.",
            "rating": 2
        },
        {
            "paper_title": "Stitch, Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation.",
            "rating": 2
        },
        {
            "paper_title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models.",
            "rating": 2
        }
    ],
    "cost": 0.032064249999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery</p>
<p>Yanbo Zhang 
Allen Discovery Center at Tufts University
MedfordMAUSA</p>
<p>Sumeer A Khan 
Kingdom of Saudi Arabia 3. Intelligent Infrastructure Team
Living Systems Lab
KAUST
Thuwal</p>
<p>Network Rail
UK</p>
<p>SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence
23952ThuwalSaudi Arabia</p>
<p>Adnan Mahmud 
Department for AI in Society, Science, and Technology
Zuse Institute Berlin
Germany</p>
<p>Huck Yang 
NVIDIA Research 6. Biological and Environmental Science and Engineering Division
King Abdullah University of Science and Technology
ThuwalSaudi Arabia</p>
<p>Alexander Lavin 
Pasteur Labs
BrooklynNYUSA</p>
<p>Michael Levin 
Allen Discovery Center at Tufts University
MedfordMAUSA</p>
<p>Wyss Institute for Biologically Inspired Engineering
Harvard University
BostonMAUSA</p>
<p>Jeremy Frey 
Department of Chemistry
University of Southampton
University RoadSouthamptonHampshireUK</p>
<p>Jared Dunnmon 
Department of Biomedical Data Science
Stanford University
StanfordCAUSA</p>
<p>James Evans 
Department of Sociology
Knowledge Lab
University of Chicago
ILUSA</p>
<p>Santa Fe Institute
NMUSA</p>
<p>School of Informatics
The University of Edinburgh
UK</p>
<p>Department of Knowledge Technologies
Joef Stefan Institute
LjubljanaSlovenia</p>
<ol>
<li>Biological and Environmental Science and Engineering Division
King Abdullah University of Science and Technology (KAUST)
23955-6900ThuwalSaudi Arabia</li>
</ol>
<p>Department of Medicine
Center for Molecular Medicine
Unit of Computational Medicine
Karolinska Institutet</p>
<p>Karolinska University Hospital
L8:05, SE-171 76StockholmSweden</p>
<ol>
<li>Computer, Electrical and Mathematical Sciences and Engineering Division
Science for Life Laboratory
King Abdullah University of Science and Technology (KAUST)
Saudi Arabia. 19, Tomtebodavagen 23A23955-6900, SE-17165Thuwal, SolnaSweden</li>
</ol>
<p>School of Biomedical Engineering and Imaging Sciences
Algorithmic Dynamics Lab, Research Departments of Biomedical Computing and Digital Twins
King's College London
UK</p>
<p>Institute for Artificial Intelligence
LondonUK</p>
<p>The Alan Turing Institute
LondonUK</p>
<p>Oxford Immune Algorithmics
Oxford University Innovation and London Institute for Healthcare Engineering
LondonUK</p>
<p>Cancer Interest Group
Francis Crick Institute
LondonUK</p>
<p>Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery
D21A9C51027B9862C00BF620469151CD
With recent Nobel Prizes recognising AI contributions to science, Large Language Models (LLMs) are transforming scientific research by enhancing productivity and reshaping the scientific method.LLMs are now involved in experimental design, data analysis, and workflows, particularly in chemistry and biology.However, challenges such as hallucinations and reliability persist.In this contribution, we review how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery.We conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics.The transition to AI-driven science raises ethical questions about creativity, oversight, and responsibility.With careful guidance, LLMs could evolve into creative engines, driving transformative breakthroughs across scientific disciplines responsibly and effectively.However, the scientific community must also decide how much it leaves to LLMs to drive science, even when associations with 'reasoning', mostly currently undeserved, are made in exchange for the potential to explore hypothesis and solution regions that might otherwise remain unexplored by human exploration alone.</p>
<p>Introduction</p>
<p>Recent advances in artificial intelligence (AI) have transformed multiple areas of society, the world economy, and academic and scientific practice.Generative AI and Large Language Models (LLMs) present unprecedented opportunities to transform scientific practice, advance Science, and accelerate technological innovation.Nobel Prizes in Physics and Chemistry were awarded to several AI leaders for their contributions to AI and frontier models, such as Large Language Models (LLMs).This promises to transform or contribute to scientific research by enhancing productivity and supporting various stages of the scientific method.The use of AI in science is booming across numerous scientific areas and is impacting different parts of the scientific method.</p>
<p>Despite the potential of LLMs for hypothesis generation and data synthesis, AI and LLMs face challenges in fundamental science and scientific discovery.Hence, our premise in our perspective is that AI, in general, has so far been limited in its impact on fundamental science, which is defined here as the discovery of new principles or new scientific laws.</p>
<p>Here, we review how LLMs are currently used -as a technological tool -to augment the scientific process in practice and how they may be used in the future as they become more powerful tools and develop into powerful scientific assistants.Combining data-driven techniques with symbolic systems, such a system could fuse into hybrid engines that may lead to novel research directions.We aim to describe the gap between LLMs as technical tools and "creative engines" that could enable new high-quality scientific discoveries and pose novel questions and hypotheses to human scientists.We first review the current use of LLMs in Science, aiming to identify limitations that need to be addressed when moving toward creative engines.</p>
<p>There is solid recognition and excitement for the transformative potential of AI in Science.For example, leading machine learning conferences (NeurIPS, ICML) have recently (2021-2023) arranged targeted workshops on AI4Science.Some recent reviews and papers include  . Thisdemonstrates the energy and potential of using automated (i.e., AI tools) for Science.This "dream" can be traced back to the times of Turing and the emergence of Artificial Intelligence in the 1950s 39 .With recent advancements in computational techniques, vastly increased production of scientific data, and the rapid evolution of machine learning, this long-held vision can be transformed into reality.Yet, most current reviews and original papers focus on specifically designed machine learning architectures targeting particular application domains or problems.</p>
<p>For example, recent reviews have explored how to use variants of Deep Learning, Geometric Deep Learning, or Generative AI in its generality (including different architectures such as CNNs, GNNs, GANs, diffusion models, VAEs, and Transformers) as a tool for assisting Science 3,11,13,15,19,22 .For example, Wang et al. 1 , reviews breakthroughs in how specific techniques such as geometric deep learning, self-supervised learning, neural operators, and language modelling have augmented Science in protein folding, nuclear fusion, and drug discovery.An essential thread in their review is the vital notion of representation, pointing out that different AI architectures can support valuable representations of scientific data and thereby augment Science.Recent papers demonstrate the appeal and the potential of using AI-driven and augmented tools for automating science 1,4,13,40 .Traditional scientific advancements have been primarily driven by hypothesis-led experimentation and theoretical development, often limited by human cognitive capacities and manual data processing.For example, the formulation of Newtonian mechanics required meticulous observation and mathematical formalization over extended periods.Here, the rise of AI4Science represents a paradigmatic revolution that could reach beyond human cognitive limitations.AI-driven advancements promise to enable rapid processing and analysis of massive data sets, revealing complex patterns that surpass human analytical capabilities.For example, DeepMind's AlphaFold dramatically transformed protein structure prediction, a longstanding scientific challenge, using deep learning to predict protein folding accurately.Furthermore, AI4Science could reverse the slowdown in scientific productivity in recent years, where literature search and peer-review evaluation [41][42][43] are bottlenecks.</p>
<p>In contrast to previous reviews, here we first address the use of LLMs, regardless of the specific underlying architecture, and their use as a tool for the scientific process.We assess how different areas of science use LLMs in their respective scientific process.This analysis sets the stage for asking how LLMs can synthesize information, generate new ideas and hypotheses, guide the scientific method, and augment fundamental scientific discoveries.Here, we ask to what extent AI can be described as a "general method of invention," which could open up new paradigms and directions of scientific investigations.</p>
<p>Hence, complementary to a purely representational and architectural viewpoint of AI4Science, we find it constructive to ask and assess to what extent the nature of the scientific process, both its inductive and deductive components, can and should be transformed by AI techniques.</p>
<p>Current use of LLMs -From Specialised Scientific Copilots to LLMassisted Scientific Discoveries</p>
<p>The ability of Large Language Models (LLMs) to process and generate human-like text, handle vast amounts of data, and analyse complex patterns with potentially some reasoning capabilities has increasingly set the stage for them to be used in scientific research across various disciplines.Their applications range from simple tasks, such as acting as copilots to assist scientists, to complex tasks, such as autonomously performing experiments and proposing novel hypotheses.We will first introduce the fundamental concepts of LLMs and then review their various applications in scientific discovery.</p>
<p>Prompting LLMs: From Chatbot to Prompt Engineering</p>
<p>Current mainstream LLMs are primarily conditional generative models, where the input, such as the beginning of a sentence or instructions, serves as a condition, and the output is the generated text, such as a reply.This text is typically sampled auto-regressively: the next token (considered the building block of words) is sampled from a predicted distribution.See Figure 1A.Given LLMs' capabilities in computation and emerging potential for reasoning, which we define as the ability to solve tasks that require reasoning, they can be considered programming languages that use human language as the code that instructs them to perform desired tasks.This code takes the form of "prompts."For instruct-tuned LLMs, the prompt often consists of three parts: the system prompt and the user prompt, with an LLM's reply considered the assistant prompt.Hence, a chat is frequently composed of <system><user><assistant><user><assistant>, see Figure 1B.The system prompt typically includes general instructions for the LLMs, such as behaviour, meta-information, format, etc.The user prompt usually contains detailed instructions and questions.Using these prompts, the LLMs generate replies under the role of "assistant.</p>
<p>Since LLMs do not have background knowledge about the user, and prompts are their major input, designing a good prompt is often critical to achieving the desired output and superior performance.Researchers have shown that specific prompts, including accuracy, creativity, and reasoning, can significantly improve output performance.Specifically, the chain-of-thought (CoT) method 44 can instruct LLMs to think step-by-step, leading to better results.Beyond these, the Retrieval-augmented Generation (RAG) method 45 can incorporate a large amount of context by indexing the contents and retrieving relevant materials, then combining the retrieved information with prompts to generate the output.Due to the importance of prompts and LLM agents, designing prompts is now often called "prompt engineering," and many techniques and tricks have been developed in this area 46,47 , such as asking JSON format outputs, formulating clear instructions, setting temperatures, etc 47,48 .</p>
<p>While carefully designed prompts can accomplish many tasks, they are not robust and reliable enough for complex tasks requiring multiple steps or non-language computations, nor can they explore autonomously.LLM agents are developed for these requirements, especially for complex tasks.LLM agents are autonomous systems powered by LLMs, which can actively seek to observe environments, make decisions, and perform actions using external tools 49 .In many cases, we need to ensure reliability, achieve highperformance levels, enable automation, or process large amounts of context.These tasks cannot be accomplished solely with LLMs and require integrating LLMs into agent systems.Early examples include AutoGPT 50 and BabyAGI 51 , where LLMs are treated as essential tools within the agent system (Figure 1C).In scientific discovery, LLM agents become even more critical due to the complexity of science and its high-performance requirements.Many tools have also been developed to provide easy access to these prompting and agent methods, such as LangChain 52 and LlamaIndex 53 .Automated prompt design methods, such as DSPy 54 and TextGrad 55 , are also being developed to design prompts and LLM agents in a data-driven way.</p>
<p>LLMs as Practical Scientific Copilots</p>
<p>The ability of LLMs to work with a large body of text is being exploited in the practice of science.For example, LLMs assist in proposing novel ideas, writing scientific papers and generating computer code, thereby improving productivity; they also adapt texts for diverse audiences ranging from experts to broader audiences, thus supporting communication in science.</p>
<p>Furthermore, LLMs can sift through vast bodies of scientific literature to identify relevant papers, findings, and trends.Such reviewing of the relevant literature helps investigators quickly digest and identify gaps in enormous bodies of scientific knowledge.</p>
<p>These capabilities can also mitigate discursive barriers across different scientific fields, supporting interdisciplinary scientific collaborations and knowledge sharing.Recently, chatbots have emerged in several disciplines as virtual assistants answering scientific queries posed by scientists.Such tools exploit the power of LLMs to extract and detect patterns, data, and knowledge.These techniques may also serve as important tools in science education and communication.</p>
<p>These examples demonstrate the rise of LLMs in extracting and sharing information and the exciting open research frontier of the potential of reasoning that they represent in different scientific domains [56][57][58] .For instance, Caufield et al. proposed the SPIRES method 59 , which uses LLMs to extract structured data from the literature.Beyond data extraction, LLMs have also shown evidence of outperforming annotation tasks 60,61 , enabling scientists to scale data annotation.Some domain-specific models also show superior performance in classification, annotation, and prediction tasks [62][63][64] .With the help of RAG methods 45 , LLMs can directly apply their information extraction and distillation capabilities to large amounts of text data.With the combination of diverse capabilities of LLMs interconnected through LLM-agents, the recent "AI co-scientist 65 " demonstrates impressive ability in generating novel research ideas by leveraging existing literature, engaging in internal LLM-agent debates, and refining its outputs.This process leads to constructive progress when applied to real scientific tasks.</p>
<p>Moreover, LLMs are currently used to automate the experimental design and the execution of experiments.For example, Boiko, et al. 66 propose an autonomous LLM capable of performing chemical experiments.This work employs an LLM planner to manage the experimental process, such as drawing patterns on plates or conducting more complex chemical syntheses.Compared to hard-coded planners, the LLM-based planner is more flexible and can handle unexpected situations.Similar kinds of loop and tool usage are also shown in 67 , which includes literature tools, consulting with humans, experimental tools, and safety tools.</p>
<p>In the biological domain, for instance, the CRISPR-GPT 68</p>
<p>Foundation Models for Science</p>
<p>A key observation when using LLMs as clever text engines or exploiting the underlying machine learning (neural) architecture for solving specific scientific problems was the importance of scale.Larger models trained on larger amounts of data, or spending larger amounts of computation during inference time yielded an increase in performance 56,70,71 .</p>
<p>The discovery of such scaling laws 72 demonstrated that LLMs' performance improves as the number of parameters increases.Thus, we can expect the above trends to grow in importance as these systems are trained on ever larger amounts of data.Emergent behaviours, such as reasoning were suggested when models increased in scale 73 .</p>
<p>Concurrent with the appreciation of scaling laws came the realisation that instead of using LLMs for specialised problems or as text engines, one could potentially train them on large amounts of data, not necessarily text, but different modalities of scientific data.This is the idea of a foundation model.These are large-scale pre-trained models that, when trained with a sufficient amount of data of different types, such models "learn" or "encapsulate" knowledge of a large scientific domain, thus reaching beyond a specific scientific problem.</p>
<p>When fine-tuned to particular tasks, such models can solve a wide range of downstream tasks.The notion of foundation models refers to their generality in that they can be adapted to many different applications, unlike task-specific engineered models solving a specialised task such as protein folding.Notably, the famous transformer architecture that fuels LLMs has become the architecture of choice when constructing the foundational models in different domains of science.These self-supervised models are usually pretrained on extensive and diverse datasets.This enables them to learn from massive unlabelled data since masking parts of the data and then requiring the model to predict the occluded parts provides foundation models with their learning objective.This technique is used when training LLMs on large amounts of text.The idea is thus exploited in scientific domains where multi-modal data is used to train self-supervised foundation models.Once trained, the model can be fine-tuned for various downstream tasks without requiring additional training.Consequently, the same model can be applied to a wide range of downstream tasks.The foundation model encapsulates a large body of scientific "knowledge" inherent in the training data.</p>
<p>Leveraging these ideas, there has been a rise in the number of foundation models of science.For example, the Evo and Evo 2 models enable prediction and generation tasks from the molecular to the genome scale 74 .While Evo is trained on millions of prokaryotic and phage genomes, Evo 2 75 includes massive eukaryotic genomes, and both demonstrate zero-shot function prediction across DNA, RNA, and protein modalities.It excels at multimodal generation tasks, as shown by generating synthetic CRISPR-Cas molecular complexes and transposable systems.The functional activity of Evo-generated CRISPR-Cas molecular complexes and IS200 and IS605 transposable systems was experimentally validated, representing the first examples of protein-RNA and protein-DNA co-design using a language model.Similarly, scGPT is for learning single cell transcriptional data 76 , ChemBERT encodes molecular structures as strings, which then can be used for different downstream tasks such as drug discovery and material science 77 .Similarly, OmniJet- is the first cross-task foundation model in particle physics, enhancing performance with reduced training needs 78 .Additionally, multiple physics pretraining (MPP) introduces a task-agnostic approach to modelling multiple physical systems, improving predictions across various physics applications without extensive fine-tuning 79 .The LLM-SR 80 implements similar symbolic regression methods iteratively, generating and evaluating hypotheses, using the evaluation signal to refine and search for more hypotheses.</p>
<p>Incorporating diverse scientific data modalities, which represent different "languages" to interact with observations beyond natural language, is crucial.There are two major approaches emerging: 1) End-to-end training on domain-specific modalities: Models like ChemBERT 77 (using chemical SMILES strings) and scGPT 76 (using singlecell data), as mentioned above, are directly trained on these specialized data types.2)</p>
<p>Separate training with compositional capabilities: This involves training separate encoders for new modalities or enabling LLM agents to utilize tools that interact with these modalities.For instance, models like BiomedCLIP 81 connect biological images with natural language, while PaperCLIP 82 and AstroCLIP 83 link astronomical images and spectral data to textual descriptions.Furthermore, frameworks like ChemCrow 84 leverage the tool-using abilities of LLMs to connect with non-natural-language modalities, such as chemical analysis tools.</p>
<p>Yet, as with text-based LLMs, several challenges remain.These include potential biases in datasets, which can bias the performance and output of these models.Since science is mainly about understanding systems, the scale, and opaqueness of these models make interpretation a particularly challenging problem.Also, several observations, such as their capability for generalisation, multi-modality, and apparent emergent capabilities, have led to intense discussions at the research frontier on the extent to which these foundation models can reason within and beyond their training regimes.The text-based LLMs (or models incorporated with text modality) discussed above are constructed using these techniques.Examples include GPT-4 (OpenAI) 85 , BERT (Bidirectional Encoder Representation from Transformers) 86 , CLIP (Contrastive Language-Image Pre-training, OpenAI) 87 , and DALL-E from OpenAI 88 .</p>
<p>These foundation models have the potential to achieve professional human-level performance or even surpass human capabilities when trained using reinforcement learning, particularly with feedback from reliable formal systems.For example, AlphaProof 89 has become state-of-the-art in automated theorem-proving systems, achieving mathematical capabilities comparable to human competitors at IMO 2024.</p>
<p>Approximately one million informal mathematical problems were translated into the formal language LEAN, a mathematical proof verification language, enabling the LLM to be trained through reinforcement learning.Solutions generated by the LLM in LEAN are either proved or disproved by the LEAN compiler, with the resulting correct or incorrect solutions serving as feedback to refine the LLM.While this approach has been explicitly applied within the mathematical domain, it demonstrates significant potential for training LLMs to surpass human performance in highly complex and deductive reasoning.</p>
<p>Although developing formal systems for general tasks remains challenging, reinforcement learning methods are employed to build foundation models with enhanced deductive capabilities, leading to the rise of reasoning models such as OpenAI o1/o3 70 , Deepseek R1 56 , and others.In scientific domains such as physics, external and reliable feedback mechanisms are already used to improve answer quality 90 , highlighting the potential for creating domain-specific foundation models.</p>
<p>In conclusion, the rise of foundation models will continue to affect and disrupt science due to their powerful nature, scaling properties, and ability to handle very different data modalities.However, for our purposes, the question remains of what extent foundation models could be a proper gateway to making fundamental scientific discoveries.To what extent can foundation models be creative and reason outside their training domains?</p>
<p>Toward Large Language Models as Creative Engines for Fundamental Science</p>
<p>Here, we ask how AI can impact fundamental Science?That is, what is required for an AI to be able to discover new principles of scientific laws from observations, available conjectures, and data analysis?Broadly, can generative AI develop to become a "creative engine" that can make fundamental scientific discoveries and pose new questions and hypotheses?Einstein famously stated, "If I had an hour to solve a problem, I'd spend 55 minutes thinking about the problem and 5 minutes thinking about solutions".This underscores the importance of carefully considering the question or problem itself, as posing hypotheses effectively can be the most intellectually demanding part of Science.As a first approximation, the ability to pose novel hypotheses is -at least for us humanswhat appears to be essential for making novel discoveries.Thus, what is required for an AI to advance beyond a valuable tool for text generation and engineered systems for solving a particular problem?Or could foundation models provide a possible path forward?</p>
<p>In our view, if LLMs are to contribute to fundamental Science, it is necessary to assess what putative roles LLMs can play in the core of the scientific process.To this end, we discuss below how LLMs can augment the scientific method.This includes how LLMs could support observations, automate experimentation, and generate novel hypotheses.We will also explore how human scientists can collaborate with LLMs.</p>
<p>Augmenting the Scientific Method</p>
<p>As a first approximation, scientific discovery can be described as a reward-searching process, where scientists propose hypothetical ideas and verify or falsify them through experiments 91 .Under this Popperian formulation, LLMs can assist scientific discovery in two ways (Figure 2): On the one hand, LLMs could assist in the hypothesis-proposing stage, helping scientists find novel, valuable, or potentially high-reward directions or even propose hypotheses that human scientists might have difficulty generating.On the other hand, LLMs have the potential to make experiments more efficient, accelerate the search process, and reduce experimental costs.</p>
<p>At the stage of proposing hypotheses, scientists choose unknown areas to explore, which requires a deep command of domain knowledge, incorporating observational data, and manipulating existing knowledge in novel ways 45,92 .Their expertise and creativity could carry the potential for proposing novel research hypotheses.</p>
<p>Then, at the verification stage, experiments are conducted to obtain relevant information and test hypotheses.This requires the ability to plan and design experiments effectively.Given LLMs' planning capabilities and potential understanding of causality 93- 95 , they can help scientists design experiments.By incorporating tool-using abilities 96 , LLMs can directly implement experiments.LLM agents can perform complex workflows and undertake repetitive explorations that are time-consuming for human scientists.This allows us to search for novel results efficiently, which is key to scientific discovery 97,98 .This process often involves a trial-and-error loop for a research topic or question.Thus, scientific discovery requires the following steps: observation, hypothesis proposal, experimentation, and automating the loop.</p>
<p>Expanding or Narrowing the Observation Process</p>
<p>Scientists rely upon observational results for guidance in proposing hypotheses, designing and refining experiments, evaluating experimental results, and validating their hypotheses.</p>
<p>In general, observations act as dimension reduction methods 99 , which include annotating, classification, and information extraction.</p>
<p>General purpose LLMs, such as GPT-4, Llama, can be good observers for language and image data for general purposes.Their in-context and zero-shot learning capabilities can be used as universal classifiers to extract specific information from these data, such as annotation and evaluation.In domains like NLP and Social Science, annotating and evaluating language data at scale is a fundamental task for downstream experiments.</p>
<p>Trained humans or crowd-workers have often done such jobs.However, LLMs, such as ChatGPT, can perform higher or comparable performance levels relative to crowd-workers on annotation tasks, especially on more challenging tasks 60,61 .</p>
<p>Besides language processing, scientists must also describe complex behaviours at scale qualitatively.LLMs show potential in describing such complex black-box systems, where we observe only their inputs and outputs without knowledge of their underlying mechanisms.Although deciphering such systems can often become a stand-alone research question, having a qualitative description can still be helpful when faced with large-scale data.With LLMs, black-box systems, such as language input-output, mathematical inputoutput pairs, fMRI data 100 , or observational data, can be described using natural language 100 .Beyond text and text-represented systems, different data modalities represent different "languages" to interact with observations, and domain-specific modalities are extremely important for scientific discovery.Scientific research often involves other data types, including image, video, audio, table 101,102 , or even general files 103 , as well as domainspecific modalities like genomic sequences, chemical graphs, or spectra 76,77,82,83 .Multimodality LLMs can play the observer role vis-a-vis these data.However, most multimodality LLMs are still struggling to handle some domain-specific data formats, such as genomic data or chemical compounds, which may require converting and where information may be lost during the conversion process.</p>
<p>For highly specialised domains, domain-specific LLMs trained on specialised data can achieve superior performance within their respective fields (representing the end-to-end approach discussed earlier).For example, with the same number of parameters, BioGPT 64 outperforms GPT-2 medium 104 when trained on domain-specific data.Even with fewer parameters, models like PubMedBERT 62 can perform at a level comparable to GPT-2 medium.In the chemical domain, LLMs have been pre-trained on chemical SMILES data 105 , enabling them to infer molecular properties and biological activities 51 .LLMinspired models are also useful for case-specific tasks.In 106 , transformers are trained on cellular automata to study the relationship between complexity and intelligence.This highlights the importance of exploring domain-specific and case-specific LLM and the opportunities for further exploration in this area.</p>
<p>Experimentation and Automation</p>
<p>The experiment is a critical part of all research steps, including making observations and validating the hypothesis.Both humans and LLMs need external tools to implement experiments.Specifically, this involves calling external functions or directly generating and running code.LLMs that have been fine-tuned for tool usage 85,96 can generate structured entities (often in JSON) that contain the function name and inputs to be implemented by external functions.These functions are versatile and can include simple calculations, laboratory control functions, external memory, requests for assistance from human scientists, etc. LLMs can also direct programming by generating and running code for complex experiments requiring fine-grained control or enhancing the calculation abilities of LLMs 107,108 .Beyond this, generated programs can also call other functions or be saved into a function library, enabling the combinatory development of complex actions 109 .</p>
<p>For complex experiments, planning becomes important, which involves setting up an objective and decomposing it into practical steps.This is critical to solving complex tasks while sustaining coherent behaviour.While the planning capabilities of LLMs are questioned in many studies, certain tools and methods still demonstrate valuable assistance.</p>
<p>The chain-of-thought (CoT) 44 method significantly improves various tasks by decomposing a question into steps.In complex tasks with more steps, where LLMs seek long-term objectives and interact with environments, they can generate plans in natural language based on given objectives 110 .It is also important to adapt to observations and unexpected results.For this reason, methods like Reflexion 111 , ReAct 112 combine the CoT and planning, dynamically update its plans, manage exceptions, and utilizes external information.And it also overcomes hallucination and error propagation in the chain-ofthought.</p>
<p>Automation is a significant aspect of LLM-assisted research, serving as a key contributor to accelerating scientific discovery.Automation involves repetition and feedback loops 113 .LLMs can be seen as a function --prompt in, reply out -with human users as the driving force behind making LLMs produce output.To automate such a system, the key is to replace the human user.For instance, an LLM-powered chemical reaction system can perform Suzuki and Sonogashira reactions by incorporating an LLMbased planner, which replaces the human user.The planner reasons through the given task and determines the next steps, including searching the internet for information on both reactions, writing Python code to calculate experimental parameters, and finally calling external tools to conduct the experiments.At each step, the results, i.e., the search outcomes and calculation results, are fed back to the LLM-based planner to automate the system 66 .</p>
<p>Another approach is to replace the human user with multiple LLMs and allow them to communicate with each other 114 .Since such automation is not fully hard-coded and the core of this automation is also an LLM, they can exhibit some emergent behaviour 66,114 , adapting unexpected situations, which is vital for exploring new knowledge.Specifically, automated LLMs can help in three dimensions of scientific discovery: scaling, enhancing, and validation.</p>
<p>Scaling: Automated LLM agents can scale previously challenging experiments for large-scale studies.Examples include inferring underlying functions from input-output pairs 115 .The LLMs perform multiple rounds of trial and error to find the correct function.</p>
<p>This approach can extend to neuron interpretation of GPT-2 using GPT-4, which has billions of parameters 116 .This method involves two layers of loops: the trial-and-error process and the application to all the billions of neurons 117,118 .Both layers are timeconsuming for human scientists, and LLMs make such studies feasible.Another example is when LLMs are used to infer the functionality of human brain voxels from fMRI activation data, their proposed functions are first validated by calculating the probability of observing the activation data given a specific functional hypothesis.Subsequently, the hypotheses with the high probability are selected to aid in generating new hypotheses and improving overall performance 100 .Lab experiments can also be parallelized with the help of LLMs, which further accelerate the experiment speed and increase the potential for scaling the scope of experiments 110,119 .</p>
<p>Enhancing:</p>
<p>The aforementioned scientific methods, such as hypothesis generation, experiments, and observations, can all be enhanced by automation.One direct application is using LLMs as optimisers: by iteratively providing historical solutions and scores, LLMs can propose new solutions and ultimately achieve superior performance 120 .In both the hypothesis-validation loop and in experimental trials, failed cases constitute valuable feedback.When evaluators and reflection are incorporated into the workflow, LLMs can improve their decisions, showing significant performance improvements compared to simply using LLMs 111 .Iteration can also enhance the hypothesis generation stage.By comparing hypotheses with existing literature on related topics, LLMs can iteratively improve novelty by using this literature as a source of negative examples 121 .Another enhancement comes from accumulating knowledge, which is critical to research success.</p>
<p>Many exploration tasks require accumulating knowledge and developing new strategies based on this knowledge 122 .For example, Voyager 109 uses GPT-4 to examine the space of the Minecraft game.This study consists of three main parts: an automatic curriculum to propose exploration objectives, an iterative prompting mechanism to write code to control the game, and a skill library to accumulate the knowledge and skills gained during the exploration, which is then reused in future explorations.Equipped with all these components, this LLM-assisted explorer can explore the game more efficiently.While game environments in silico are a non-trivial departure from real worlds in situ, they are not too dissimilar from the biochemical simulation engines 123 that scientists rely on today.</p>
<p>However, the current "physics" engines in-game systems are still inconsistent with the physical sciences, and new simulation software technologies are needed to allow for any AI-based exploration of multi-physics environments 113 .From a macroscopic viewpoint, scientific discovery can also be considered a quality-diversity search process 124,125  This loop is essential given the probabilistic nature of LLMs 126 and the hallucination problem of LLMs 127,128 .Experiments show that repeatedly verifying the results from LLMs' observations and proposed hypotheses increases the likelihood of obtaining reliable results 129,130 .A promising direction is leveraging formal systems to validate results and hypotheses by translating generated hypotheses and answers into formal languages, such as LEAN or Prover9 131,132 .For instance, in 131 , LLMs first generate multiple answers.</p>
<p>These answers are then translated into the LEAN language and verified using the LEAN compiler to choose the correct responses.With these filtered answers, LLMs can aggregate toward a final answer.Another example involves using Python code to aid validation.</p>
<p>While general programming languages are often not considered formal systems, they can still disprove certain hypotheses.In 133 , LLMs were prompted to solve the Abstraction and Reasoning Corpus (ARC) tasks 134 , which involve identifying underlying laws and making predictions based on new initial states.LLMs initially propose hypotheses, which are then translated into Python code.This Python code is used to disprove incorrect hypotheses.</p>
<p>Although these non-formal systems cannot fully validate hypotheses, they partially perform validation and improve predictive accuracy.While humans could also conduct such translation and validation processes, the high speed of hypothesis generation by LLMs makes automated approaches more suitable.A limitation, however, is the reliance on LLMs to translate hypotheses into formal languages, which may introduce errors in the process.This suggests the need for caution when interpreting results, even if they have been tested using formal systems.</p>
<p>Expanding the Literature Review and the Hypothesis Horizon</p>
<p>In brief, advancing beyond current knowledge includes using LLMs to explore unknown territories in knowledge space, encompassing human discoverable, human-machine discoverable, non-human-machine discoverable, and the entirety of the knowledge space, as illustrated in Evidence indicates that LLMs can propose novel ideas, such as drug combinations 135 , with designed prompting, thus underscoring the importance of prompting, as discussed previously.An example is the use of LLMs for drug discovery: In 135 LLMs are prompted to propose novel combinations of drugs for treating MCF7 breast cancer cells while incorporating additional constraints such as avoiding harm to healthy cells and prioritizing FDA-approved and readily accessible drugs.The experiment results demonstrate that LLMs can effectively propose hypothetical drug combinations.More advanced techniques can also improve novelty, such as asking LLMs to role-play as scientists 136 or iteratively provide feedback on existing similar ideas 66 .This is further exemplified by the Virtual Lab project 137 , where AI agents, powered by LLMs, were used to design novel nanobody binders against SARS-CoV-2 variants.LLMs effectively functioned as hypothesis generators, facilitating rapid and innovative scientific discovery that translates to validated experimental results in real-world applications.Although some human evaluations show that LLM-generated ideas have lower novelty 138 , the fast speed at which LLMs propose ideas can still be valuable.With proper instruction and background knowledge, LLMs can act as zero-shot hypothesis generators 139 .LLMs can also generate hypotheses semantically or numerically based on observations about the underlying mechanisms for language processing and mathematical tasks 140,141 .With neuron activation heatmaps, GPT-4 can propose potential explanations for neuron behaviour 116 .</p>
<p>Besides directly proposing hypotheses, a significant part of creativity combines existing knowledge, making literature research critical.With their vast stored compressed knowledge 142,143 , LLMs can be viewed as databases queried using natural language 123 .This not only accelerates the search but also breaks down barriers of domain terminology, making it easier to access interdisciplinary knowledge.For accessing more up-to-date and domain-specific information, LLMs can help scientists by using the RAG method and accessing internet information, see Figure 2. Generally, text embedding is used for semantically searching vector databases 45,92 .For example, STORM 144  This case also highlights the importance of the hypothesis-experiment-observation loop, where each step is critical: hypotheses rely on observations, experiments require hypotheses and planning, and observations depend on experiments.Such a self-dependent loop is typical in scientific discovery and can be initiated either by starting with a tangible step in the hypothesis-experiment-observation process or by allowing human intervention.</p>
<p>Human Scientists in the Loop</p>
<p>While we showcase the capabilities of LLMs in assisting scientific discovery, human scientists remain indispensable.During the literature review stage, with the help of LLM agents, humans can contribute by providing deeper perspectives or guiding the focus toward the needs of human scientists 144 .In the reasoning processes, by identifying uncertain reasoning and thoughts, humans can correct LLMs.This significantly improves the accuracy of the chain-of-thought method, making the LLMs more reliable 147 .Human scientists can be involved in further improving safety and reliability.For example, ORGANA 110 , an LLM-powered chemical task robot, uses LLMs to interact with humans via natural language and actively engages with users for disambiguation and troubleshooting.Beyond this, humans can assist LLMs to enhance performance with a reduced workload.For example, by involving humans in the hypothesis-proposing stage to select generated hypotheses, LLMs can perform similarly to humans 133 .At the experiment stage, many lab experiments still require human implementation and correction of invalid experimental plans 67 , and LLMs can request human help on these experiments 66 .</p>
<p>While the methods described above focus on LLMs as drivers of scientific inquiry, we must clarify that human-in-the-loop is more aptly cast as LLM-in-the-loop, emphasising "assistance" or augmentation as the practical value-added dimension of LLMs.The opportunities described in this paper show potential to shift this mode of scientific practice to be more reliant on AI-driven approaches, but not without significant advances in AIScience approaches with respect to physics-infused ML and causal reasoning and in rigorous testing systems for LLMs interacting with the natural world.Hypotheses -Literature review: using an LLM's own trained knowledge [ 142,143,148 ], or using the RAG method to access up-to-date information [ 45,92,121 ].</p>
<p>-Novelty: hallucinations of LLMs can sometimes benefit novelty [ 149 ]; using the role-play method, LLMs can increase their novelty [ 136 ]; LLMs can also propose novel ideas iteratively [ 121 ].</p>
<p>-Observation-based Hypotheses: LLMs can propose hypotheses based on [ 100,115,116,139,141</p>
<p>Challenges and Opportunities</p>
<p>While LLMs have shown signs of delivering promising results and of having positive impacts on scientific discovery, investigators have recognised their limitations, such as hallucinations, limited reasoning capabilities, and lack of transparency.Compared to everyday usage, when applied to scientific domains, these limitations require careful consideration, as scientific processes and discoveries require high standards of truthfulness, complex reasoning, and interpretability.The scientific community's increasing recognition and communication of these limitations of LLMs is essential to enabling solutions while also limiting expectations.Such rigour is a cornerstone of science and engineering, and a requirement if LLMs are to play a practical role.</p>
<p>Beyond all this, LLMs also affect scientific research at the scientific community level.While many papers and reviews involve LLMs' assistance, LLMs still face challenges in producing qualified reviews.</p>
<p>Hallucinations as Putative Sources of Novel Hypotheses</p>
<p>Hallucinations produced by LLMs, also called confabulation or delusion, refer to artificially intelligent systems generating responses that contain false or misleading information presented as fact.This is analogous to hallucination in human psychology, though, for LLMs, it manifests as unjustified responses or beliefs rather than perceptual hallucinations 151 .Hallucinating LLMs can make unsupported claims, thus failing to meet a prior set of standards.While some "incorrect" LLM responses may reflect nuances in the training data not apparent to human reviewers 152 , this argument has been challenged as not robust to real-world use 153 .</p>
<p>In scientific discovery, hallucination becomes a critical hurdle when applying</p>
<p>LLMs to literature review, data processing, or reasoning.Various methods have been developed to mitigate hallucinations 154 .Using the RAG method, LLMs can reference accurate source contexts and up-to-date information, which can reduce hallucinations 155 .</p>
<p>Knowledge graphs can also provide reliable information to reduce hallucinations 64 .</p>
<p>A self-RAG method can also reduce hallucinations, where the LLMs generate and verify the reference contexts, and outputs are also verified by the LLMs themselves 151 . 156proposes an even simpler solution: create answers for the same query multiple times and vote for the final answer.This method can significantly improve the accuracy of outputs.</p>
<p>Repetition from prompt variation and reiteration can also detect hallucinations-by finding contradictions 157 .By repeatedly generating the same context, LLMs may sometimes generate contradictory content, which can be fixed by the LLMs themselves iteratively.</p>
<p>Another method to mitigate hallucinations is through self-verification.This often involves decomposing the generated content into multiple fact checkpoints.For example, the Chain-of-Verification method uses separate LLM agents to verify them individually and update the original answer 158 .Such a verification process can also adopt RAG methods for greater reliability 159 .</p>
<p>An important origin of hallucinations is the auto-regressive generation process of mainstream LLMs, where errors may accumulate during generation 146 .Hence, as discussed above, a general way to mitigate hallucinations is to decompose the end-to-end generation process using chain-of-thoughts, the RAG method, multiple agents, feedback, and iteration loops.</p>
<p>While significant research efforts target the challenge of how to control or limit hallucinations, we may ask to what extent hallucinations are a bug or a feature.For example, could hallucination provide a gateway to creativity in that it could represent a steady stream of novel conjectures?An LLM could then be used to filter such a string of hallucinated hypotheses and rank them to recommend which ones to test.This remains unexplored territory, as far as we can tell.</p>
<p>Another approach to treating hallucinations is to move beyond a binary perspective of trust versus distrust.Instead, similar to statistical confidence, we may quantify the extent to which research conducted by LLM agents can be trusted.Current studies primarily focus on confidence measurements at the foundation model level [160][161][162] and the output level 163 .Some research has also proposed multidimensional assessments of LLM trustworthiness 164 .Additionally, efforts have been made to enable LLMs to express their confidence levels 160,165 .However, confidence measurements at the LLM agent level are primarily limited to success rates rather than trustworthiness, particularly when dealing with open-ended tasks.Moreover, existing measurements predominantly rely on post-hoc quantifications, which restrict their applicability in scientific research 166 .Therefore, predictive trustworthiness quantification frameworks for LLM agents that collectively consider foundation models, tasks, tools usage, workflow, and external feedback are needed.</p>
<p>The Value of Reasoning and Interpretation in LLM-led Science</p>
<p>While LLMs have been suggested to perform reasoning on some tasks, they exhibit severe defects in logical reasoning and serious limitations with respect to common sense reasoning.Notably, while LLMs can correctly answer "X is the capital of Y", they struggle to accurately determine that "Y's capital is X."This is known as the "reversal curse" 167 .</p>
<p>Another example is shuffling the order of conditions in a query, which may reduce the performance of LLMs.When the conditions are provided logically, the LLMs can often perform correct inferences but may fail when the conditions do not follow a specific order 168 .LLMs can also fail at simple puzzles, such as determining the odd-numbered dates on a list of famous people's birthdays 169 , or in simple questions like "Alice has N brothers, and she also has M sisters.How many sisters does Alice's brother have?"Many LLMs, while achieving high performance on other benchmarks, have shown a lower success rate on this task 170 .When faced with unseen tasks, which are common for human scientists in research, LLMs exhibit a significant drop in accuracy even on simple questions, such as performing 9-base number addition or writing Python code with indexing starting at 1 instead of 0. This suggests that LLMs may rely more on pattern matching than on reasoning, contrary to what many assume [171][172][173] .Consequently, caution is advised when applying LLMs to novel reasoning tasks, and incorporating human oversight into the process is recommended 173,174 .Another crucial aspect of reasoning is planning capability.</p>
<p>As discussed earlier, planning is essential for implementing experiments.While techniques such as ReAct and Reflection demonstrate some planning capabilities in LLMs, their effectiveness remains questionable.Current state-of-the-art LLMs often fail at simple planning tasks 175 , such as Blocksword, and are unable to verify the correctness of their plans 175 .In contrast, humans generally excel at creating effective plans for such tasks.</p>
<p>However, studies also indicate that integrating LLMs with traditional methods or solvers can enhance planning success rates, reduce research time 175 , and provide more flexible ways to interact when developing plans 176 .Some reasoning improvement methods, such as self-correction, can also fail.When</p>
<p>LLMs receive feedback without new information or correct labels, self-correction can often lead to compromised performance 177 .Such self-correction prompts may even bias the LLMs, causing them to turn correct answers into incorrect ones.To mitigate this problem, directly including all requirements and criteria in the query prompt is suggested instead of providing them as feedback.This result also indicates that to make corrections, effective feedback needs to include external information, such as experimental results and trustworthy sources 177 .</p>
<p>While some progress has been made, more advanced methods are needed to address these reasoning-related challenges.One crucial aspect to consider is consistency -when different LLM agents generate different responses to the same query, the result is considered inconsistent.Notably, the self-consistent method 178 uses LLMs to answer the same question multiple times and chooses the most frequent answer.The answers also help people estimate uncertainty 178 , given that LLMs often behave too confidently 179 .Similar methods have also been proposed in 156 .Other methods use different LLM agents to suggest different ideas and then conduct a multi-round debate to arrive at a final answer 180 .As illustrated in Figure 2, these LLM-agent methods can benefit all steps in the hypothesisexperiment-observation loop.</p>
<p>A straightforward but challenging route to scientific discovery is to fine-tune or directly train a model.In 181 , the authors propose an innovative solution to the Reversal Curse through "reverse training," which involves training models with both the original and reversed versions of factual statements.Considering the requirements for rigor and prudence in scientific research, attention must be given to the limitations of reasoning tasks.This is particularly important given that LLMs often exhibit reduced performance in reasoning correctness when encountering novel tasks-a frequent occurrence in scientific research, where the focus is on exploring unknown knowledge.</p>
<p>The Challenge to Understand LLMs, and the Opportunity to Understand by using LLMs A comprehensive scientific interpretation stimulates discussion and further discoveries among scientists.This is especially important for LLM-assisted scientific discovery-given that current LLMs are mostly black boxes, it becomes difficult to trust LLM outputs.To understand LLMs' behaviour, LLMs' language capabilities can be leveraged.There are two types of methods.First, LLMs' hidden states can be used in what are known as probing methods.The Logit Lens method 182 applies the unembedding layer to hidden states or transformed hidden states, enabling semantic understanding of LLMs' hidden states.Representation engineering methods 183 can further detect and control emotion, dishonesty, harmfulness, etc., at the token level, allowing people to read their hidden activities.Besides these, dictionary learning methods can also be used to understand LLMs' hidden states and activations, leading to a fine-grained understanding of LLMs.</p>
<p>The second method is to ask LLMs to explain their reasoning.For instance, the CoT method or reasoning models 56 can explain the thought process before generating results or ask LLMs to explain their reasoning after generating results.However, the selfexplanation of LLMs is also questionable.Their explanations are often inconsistent with their behaviours, and we cannot use their explanations to predict their behaviours in counterfactual scenarios 184 .This suggests that LLMs' self-explanation may not accurate and not generalizable.Beyond this, LLMs may also hallucinate in their self-explanations, including content that is not factually grounded 184 , making their self-explanations even less trustworthy.</p>
<p>Despite the many difficulties in understanding LLMs, they present a significant opportunity for understanding other systems -they can be used to understand data, interpret other systems, and then prompt humans.By directly showing input and output pairs to LLMs, including language input-output pairs 115 , mathematical function inputoutput pairs, or experimental data, LLMs can be made to explain these black-box systems, including, fMRI data, complex systems like GPT-2, or, potentially, papers written by human scientists that are becoming increasingly difficult to reproduce because of various forces at play (e.g., an increasing number of publications and dubious incentives).This indicates the potential of applying LLMs to explain data and other systems, even though understanding them may still be challenging 116 .This capacity to interpret systems is not limited to human language.Foundation models in specific scientific domains offer domain-grounded interpretability, distinct from the pitfalls of LLM self-explanation.Pre-trained on vast, specialized data, these models learn the "language" inherent in that data.For instance, scGPT in single-cell biology demonstrates this: its learned representations align with established biological knowledge, and it utilizes attention-map visualizations to enhance transparency, elucidating gene interactions that are subsequently validated by domain-specific evidence 76 .Although the faithfulness of attention weight interpretability has been questioned in various cases 185,186 , it remains widely used in many AI-for-science applications 187 .Models, including LLMs that use transformer architectures, often inherently benefit from such emergent interpretability for both feature attribution and interaction highlighting.These approaches support viewing complex domain representations, such as Gene Regulatory Networks (GRNs), as a form of decipherable "domain language."Understanding these intrinsic languages through models whose interpretations are rooted in verifiable domain semantics, rather than potentially unreliable self-explanations, provides a robust method for advancing scientific discovery in specialized fields.</p>
<p>The Impact on Scientific Practice and the Community</p>
<p>An open question is how much human science and scientists will be willing to let AI, through technology like LLMs, drive scientific discovery.Would scientists be satisfied letting LLMs set the agenda and conduct experiments with little to no supervision, or do we expect to supervise AI always, driven by the fear of multiple levels of misalignment?This is a misalignment between human scientific interests and the actual practice of science, possibly forcing AI and LLMs to produce data as humans do, with its advantages or disadvantages, including constraining the search space and, therefore, the solution space.</p>
<p>Beyond the individual deployment of the scientific method, scientific discovery also happens at the community level, where scientists publish their work, share ideas, and collaborate.We can consider the scientists and even entire scientific community as an agent that learns from experiments and research publications in a manner similar to reinforcement learning processes 188 .However, learning from failed research (or negative results) is just as important as learning from successful studies 189,190 , yet it is currently undervalued 191 .This may be because failed research is far more common than successful research.However, with the massive text-processing capabilities of LLMs, we now have the opportunity to systematically share and learn from failures.Therefore, we advocate for journals and conference to encourage the publication of failed studies and negative results.</p>
<p>This learning process also depend on human values emphasising communication, mutual understanding, and peer review.Evidence shows a significant adoption of LLMassisted paper writing and peer review in recent years.Estimates indicate that 1% to 10% of papers are written with LLM assistance 192 .In computer science, up to 17.5% of research papers are estimated to be assisted by LLMs, a figure that mainly reflects the output of researchers with strict time constraints 193 .Beyond papers, estimates also show that around 7%  15% of reviews are written with LLM assistance 194,195 .</p>
<p>While LLMs can provide feedback that shows a high degree of overlap with human reviewers, they are not proficient at assessing the quality and novelty of research 196 .This limitation is especially significant for high-quality research 197 .Beyond this, LLM-assisted reviews tend to assign higher scores to papers than human reviewers evaluating the same papers 194 .Upon closer examination, LLMs also exhibit a homogenisation problem -they tend to provide similar critiques for different papers 196,198 .Despite LLMs displaying limitations at tasks such as peer reviewing and raising ethical concerns in directly generating academic content, they may still benefit scientific communication.For example, most researchers today are non-native English speakers, so they can benefit from LLMs' language capabilities that fit their diverse demands for proofreading 198 , helping alleviate the current bias towards Western Science.On another application, LLMs' code explanation capabilities may help scientists understand poorly documented code, making existing knowledge and work more accessible to a broader range of scientists 198 .With significant growth in using LLMs for writing papers, such impacts will become increasingly important 193 .[ 192,194,195 ] -LLMs are widely used in paper and review writing [192][193][194][195] LLMs can mitigate the disadvantage of non-native English speakers [ 198,[205][206][207] ] LLMs can help interdisciplinary research [ 198 ] Table 2: Challenges and opportunities in applying LLM in scientific discovery.There is much room for AI4Science as a field to fill in gaps under the listed themes, especially as LLM research matures over the next several years; consistent with all innovations in scientific practices and engineering standards, the demonstration-through-validation of said innovations requires time and resources several fold greater than are available in "safer" domains (where LLMs are currently embedded).</p>
<p>Challenges</p>
<p>Conclusions</p>
<p>In this perspective paper, we reviewed the rapid development and integration of large language models (LLMs) in scientific research, highlighting the profound implications of these models for the scientific process.LLMs have evolved from tools of convenienceperforming tasks like summarising literature, generating code, and analysing datasets-to emerging as pivotal aids in hypothesis generation, experimental design, and even process automation.As AI advances, foundation models have emerged, representing adaptable, scalable models with the potential to apply across diverse scientific domains, reinforcing the collaborative synergy between humans and machines.</p>
<p>LLMs have reshaped how researchers approach the vast amounts of scientific information available today.By efficiently summarising literature and detecting knowledge gaps, scientists can speed up literature review and idea generation.Despite the promise, current limitations pose significant hurdles to fully realising LLMs as independent scientific agents.Among these are reasoning limitations, interpretability issues, and challenges like "hallucinations"-where LLMs generate plausible-sounding but inaccurate information.While helpful in generating hypotheses, these models require careful oversight to prevent misleading or unverified information from influencing scientific processes.</p>
<p>The challenges of reasoning and hallucinations pose serious concerns regarding the use of LLMs in scientific discovery.Instead of treating LLMs as simply trustworthy or untrustworthy in a binary manner, we suggest an analogy to statistical confidence, using a continuous value-it may term as algorithmic confidence-to quantify the trustworthiness of an LLM agent system in scientific research.We further suggest that all LLM-assisted research should either be verified by humans or undergo algorithmic confidence testing.</p>
<p>The interpretability of LLMs also remains a complex issue.Their black-box nature can obstruct transparency, limiting trust in outputs that affect high-stakes scientific decision-making.Consequently, researchers continue to explore methods such as probing, logit lens techniques, and visualisation of neuron activations to demystify the decisionmaking processes within these models.Increased interpretability will be critical as we strive for ethically responsible and scientifically sound applications.On the other hand, it is essential to recognize that LLMs are showcasing their potential to explain other blackbox systems through their language and reasoning capabilities.</p>
<p>Integrating LLMs into scientific workflows brings ethical considerations, particularly regarding transparency and fairness.For instance, LLMs hold the potential for democratising access to scientific information, aiding researchers from non-English speaking backgrounds in publication and collaborative research.However, they also risk perpetuating biases present in training data, thereby influencing scientific outputs and potentially reinforcing existing disparities in research.</p>
<p>Another concern involves the over-reliance on AI in scientific processes.As we incorporate LLMs deeper into workflows, human oversight becomes essential to maintaining scientific rigor and addressing potential misalignments between AI-generated outputs and human-defined research goals.The question of how much autonomy AI should have in guiding scientific inquiries raises ongoing debate about accountability and the evolving role of human oversight.</p>
<p>To harness LLMs as creativity engines, moving beyond task-oriented applications to generate new scientific hypotheses and theories is paramount.For LLMs to contribute meaningfully to fundamental scientific discoveries, they must be equipped to recognize patterns and autonomously generate novel, insightful questions-a hallmark of scientific creativity.This would require advancements in prompt engineering, automated experimentation, iterative reasoning, and building an AI that evolves its approach based on experimental feedback.However, a significant gap in general reasoning capabilities separates current models from domain-specific superhuman systems like AlphaGo/AlphaZero 209,210 .AlphaGo leveraged a critical symmetry where an "answer" (a move) inherently generates a new "question" (the next board state challenge)-a dynamic largely absent in today's reasoning models, yet key for mastering novel tasks.For scientific discovery, developing this symmetry is crucial, as the ability to ask questions is as important as answering them; though some preliminary work has explored this 211 , it remains an unsolved and highly challenging problem.</p>
<p>The evolution of LLMs and foundation models signals a transformative era for science.While current applications largely support scientists in managing data and expediting workflows, the future may see these models as integral components of the scientific process.By addressing challenges in accuracy, interpretability, and ethical concerns, we can enhance their reliability and pave the way for responsible AI in scientific contexts.</p>
<p>Looking ahead, the collaboration between AI and human scientists will likely define the next generation of discovery.As we refine foundation models to become more adaptable and creative, they may transition from merely assisting to potentially leading explorations into uncharted scientific domains.The challenge lies in responsibly developing these models to ensure they complement and elevate human expertise without compromising scientific integrity.Ultimately, LLMs and foundation models may come to represent a synthesis of human and artificial intelligence, each amplifying the strengths of the other.With continued research and ethical vigilance, LLMs have the potential to accelerate and deepen scientific discovery, heralding a new era where AI not only supports but inspires new frontiers in science 212 .This includes embracing and learning from scientific failures and leveraging them to drive comprehensive exploration.As LLMs evolve, they may reshape scientific methodologies, impacting how science values discovery and reproducibility and may ultimately redefine the purpose of scientific inquiry.</p>
<p>Figure 1 :
1
Figure 1: (A) LLMs generate sentences in an auto-regressive manner, sampling tokens from a predicted distribution at each step.(B) A typical prompt for LLMs consists of a system prompt and a user prompt.The LLM will then respond as an assistant.A multiround dialogue will repeat the user and assistant contents.(C) LLM agents are systems that use a large language model as its core reasoning and decision-making engine, enabling it to interpret instructions, plan actions, and autonomously interact with external tools, environments, or other LLM agents to fulfil a given goal.</p>
<p>Figure 2 :
2
Figure 2: Illustration of the scientific discovery process: Scientific research can be formulated as a search for rewards in an abstract knowledge space.By synthesizing existing knowledge -represented by blue disks (human-discovered) and stars (humanmachine discovered)) -in novel ways, new knowledge (indicated by red stars) can be explored.For specific research, scientists or LLMs need to traverse the hypothesesexperiment-observation loop, where hypotheses are proposed based on existing knowledge (including LLM knowledge, and additional literature provided via RAG methods), observation, and the creativity of LLMs.Then, with aid of external tools such as programming languages, formal validations, and other methodologies, experiments are conducted to test the hypotheses or gather data for further analysis.The experimental results can be observed and described through the observation process, facilitated by domain-specific models and the multi-modality capabilities of language models.All these parts-observation, proposing hypotheses, conducting experiments, and automation-can be assisted by LLMs and LLM-agents, considering the non-trivial implementations of scientific environments in silico.</p>
<p>, and thisVoyager study has shown how LLMs can assist diversity search in a new way by proposing objectives, iteratively solving problems, and contributing to and utilising literature (skill library).Validation: Automated LLM agents are critical for validating hypothesis.Beyond scaling and enhancing performance, research often involves multiple rounds of the hypothesis experiment loop to meet scientific discovery's rigor and safety requirements.</p>
<p>Figure 2 .
2
Namely, to perform hypothesis generation and develop predictive models of more complex systems.Hence, can LLMs do open-ended Exploration of the Hypothesis Space?Can LLMs also explore complex environments in an open-ended way?These are open-ended challenges addressing the (unknown) limits of the capabilities of LLMs and Generative AI.Proposing hypotheses is a crucial step in scientific discovery, perhaps the most important since it often involves significant creativity and innovation.Scientists propose hypotheses to explore unknown topics or address research questions.This step often involves novel ideas, recombining existing literature, and key insights.Experiment design and subsequent verification are based on these hypotheses.Thus, hypothesis proposing is a central step that connects observation and experiments.</p>
<p>proposes an LLMpowered literature review agent that, for a given topic, actively searches literature on the internet from different perspectives and automatically generates follow-up questions to improve depth and thoroughness.Another important example is Deep Research 145,146 , which integrates internet browsing and reasoning to deliver more in-depth and relevant literature review results.LLMs can propose novel hypotheses by retrieving related literature as inspiration, finding semantically similar content, connecting concepts, and utilising citations based on research topics and motivation 121 .Alternatively, it may require additional ingredients or experiments to extrapolate and search outside current knowledge domains.</p>
<p>-</p>
<p>Replacing human evaluation and annotations [60,61 ] -Simplifying observed data by providing qualitative descriptions [100,115 ] -Domain-specific LLMs can perform better on classification and prediction tasks [[62][63][64]</p>
<p>Furthermore,</p>
<p>LLMs facilitate interdisciplinary research, bridging the knowledge divide by summarising complex ideas across fields, thereby fostering collaborations previously limited by domain-specific language and methods.Beyond these benefits, the massive textprocessing capabilities of LLMs create new opportunities for utilizing failed research failed research, which has received limited attention.Therefore, we encourage the scientific community to promote the publication of negative results and failed research.The utility of LLMs in designing experiments is another notable advancement.Models like CRISPR-GPT in biology exemplify this by automating gene-editing experiment designs, significantly accelerating genomics research.Moreover, LLMpowered autonomous systems like BioDiscoveryAgent indicate a shift towards AI-driven experimental processes that can augment researchers' efficiency and, more importantly, enable scientific exploration previously constrained by resource limitations.So, Large Language Models (LLMs) present two contrasting roles in scientific discovery: accuracy in experimental phases and creativity in hypothesis generation.On the one hand, scientific research requires LLMs to be reliable, accurate, and capable of logical reasoning, particularly for experimental validation.On the other, there is value in promoting creative "hallucinations" or speculative ideas at the hypothesis stage, which mirrors human intuition and expands research boundaries 208 .Besides the general foundation models like GPT-4, Claude and Deepseek, domain specific foundation models have shown special potential for applying LLMs in scientific research Notable examples such as Evo and ChemBERT showcase the success of domainspecific adaptations in genomics and chemistry, where they excel in predicting gene interactions and molecular properties.These foundational models also highlight a promising approach by treating genomic, chemical, and other scientific data as new modalities for LLMs, similar to how images, videos, and audio are considered as modalities.Integrating these modalities often follows two main strategies: end-to-end training, where models like ChemBERT develop deep, intrinsic capabilities on specialized data, potentially exceeding human performance on specific tasks; and compositional approaches, which offer greater flexibility by leveraging intermediate modalities common to human scientists (like vision and text) or specialized tools.While end-to-end methods provide depth, compositional flexibility is crucial for adapting to diverse and rapidly changing scientific demands.Consequently, combining and scaling these scientific modalities, particularly when models can be seamlessly inserted into various scientific workflows, has the potential to profoundly transform scientific research.</p>
<p>-</p>
<p>]. Implement experiment: LLMs can use external tools [ 85,96 ], API calls [ 66 ], or directly write code [ 150 ] to implement experiments.Experiment planning: chain-of-thought [ 44 ], ReAct [ 112 ].Hardcoded pipeline for safety [ 67 ]; Human confirmation [ 50 ]
AutomationExperiment
--Safety:</p>
<p>-</p>
<p>LLM agent: LLM-based planner [66], multi-LLM agent [ 109 ] Scaling: complex tasks [ 116-118 ]; knowledge accumulation [ 109 ] Iteratively optimising proposed hypotheses [ 121 ], and experiments [ 112 ].
-Human-in-the-loop
--Enhance:</p>
<p>Table 1 :
1
How LLMs can assist scientific methods at different stages.</p>
<p>Scientific discovery in the age of artificial intelligence. H Wang, Nature. 6202023</p>
<p>Can Scientific Discovery Be Automated? The Atlanctic. A Alkhateeb, Aeon, 2017</p>
<p>GFlowNets for AI-driven scientific discovery. M Jain, Digital Discovery. 22023</p>
<p>Nobel Turing Challenge: creating the engine for scientific discovery. H Kitano, NPJ Syst Biol Appl. 7292021</p>
<p>Combining data and theory for derivable scientific discovery with AI-Descartes. C Cornelio, Nature Communications. 1412023. 2023</p>
<p>Integration of Neural Network-Based Symbolic Regression in Deep Learning for Scientific Discovery. S Kim, IEEE Trans Neural Netw Learn Syst. 322021</p>
<p>Amplify scientific discovery with artificial intelligence: Many human activities are a bottleneck in progress. Y Gil, M Greaves, J Hendler, H Hirsh, Science. 3461979. 2014</p>
<p>Artificial Intelligence to Win the Nobel Prize and Beyond: Creating the Engine for Scientific Discovery. H Kitano, AI Mag. 372016</p>
<p>P Berens, K Cranmer, N D Lawrence, U Von Luxburg, J Montgomery, AI for Science: An Emerging Agenda. 2023</p>
<p>. Z Li, J Ji, Y From Zhang, Kepler To Newton, Explainable AI for Science Discovery. 2022</p>
<p>N Baker, 10.2172/1478744Workshop Report on Basic Research Needs for Scientific Machine Learning: Core Technologies for Artificial Intelligence. 2019</p>
<p>GFlowNets for Causal Discovery: an Overview. C D Manta, E Hu, Y Bengio, OpenReview. 2023</p>
<p>The transformative potential of machine learning for experiments in fluid mechanics. R Vinuesa, S L Brunton, B J Mckeon, Nature Reviews Physics. 52023</p>
<p>Synthesizing domain science with machine learning. Z Del Rosario, M Del Rosario, Nat Comput Sci. 22022</p>
<p>M Krenn, J Landgraf, T Foesel, F Marquardt, Artificial Intelligence and Machine Learning for Quantum Technologies. 2022</p>
<p>How artificial intelligence and machine learning can help healthcare systems respond to COVID-19. M Van Der Schaar, Mach Learn. 1102021</p>
<p>AI for Global Climate Cooperation: Modeling Global Climate Negotiations, Agreements, and Long-Term Cooperation. T Zhang, RICE-N.2022</p>
<p>Learning from learning machines: a new generation of AI technology to meet the needs of science. L Pion-Tonachini, 2021</p>
<p>Combining Machine Learning and Computational Chemistry for Predictive Insights into Chemical Systems. J A Keith, 10.1021/acs.chemrev.1c00107Chemical Reviews. 1212021</p>
<p>Science in the age of large language models. A Birhane, A Kasirzadeh, D Leslie, S Wachter, Nature Reviews Physics. 52023</p>
<p>How machines could teach physicists new scientific concepts. I Georgescu, Nature Reviews Physics. 42022</p>
<p>Machine Learning for Molecular Simulation. F No, A Tkatchenko, K.-R Mller, C Clementi, 10.1146/annurev-physchem-042018Annu Rev Phys Chem. 2020</p>
<p>Foundation models for generalist medical artificial intelligence. M Moor, Nature. 6162023</p>
<p>AI in health and medicine. P Rajpurkar, E Chen, O Banerjee, E J Topol, Nat Med. 282022</p>
<p>Multimodal biomedical AI. J N Acosta, G J Falcone, P Rajpurkar, E J Topol, 10.1038/s41591-022-01981-2Nat Med. 2022</p>
<p>Welcoming new guidelines for AI clinical research. E J Topol, Nat Med. 262020</p>
<p>. E Topol, Deep-Medicine, 2019</p>
<p>Self-supervised learning in medicine and healthcare. R Krishnan, P Rajpurkar, E J Topol, 10.1038/s41551-022-00914-1Nat Biomed Eng. 2022</p>
<p>The imperative of interpretable machines. J Stoyanovich, J J Bavel, T V Van &amp; West, Nat Mach Intell. 22020</p>
<p>The imperative for regulatory oversight of large language models (or generative AI) in healthcare. B Mesk, E J Topol, NPJ Digit Med. 62023</p>
<p>The imperative of physics-based modeling and inverse theory in computational science. K E Willcox, O Ghattas, P Heimbach, Nat Comput Sci. 12021</p>
<p>Six ways large language models are changing healthcare. P Webster, Nat Med. 292023</p>
<p>Use of GPT-4 to Diagnose Complex Clinical Cases. A V Eriksen, S Mller, J Ryg, NEJM AI. 12023</p>
<p>From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities. F Ishmam, M Sakib, H Shovon, M F Mridha, N Dey, 2023Preprint at</p>
<p>Multimodal Foundation Models: From Specialists to General-Purpose Assistants. C Li, 2023</p>
<p>Large language models in medicine: the potentials and pitfalls. J A Omiye, H Gui, S J Rezaei, J Zou, R Daneshjou, 2023Preprint at</p>
<p>Maithra Raghu, Eric Schmidt, arXiv:2003.11755A Survey of Deep Learning for Scientific Discovery. 2020arXiv preprint</p>
<p>DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. S Song, NeurIPS 2023 AI for Science Workshop. 2023</p>
<p>. J Mccarthy, M M , R N , S C E , Dartmouth Summer Research Project on Artificial Intelligence. in Dartmouth Summer Research Project on Artificial Intelligence. 1956</p>
<p>A Review of Large Language Models and Autonomous Agents in Chemistry. M C Ramos, C J Collison, A D White, 2024</p>
<p>Artificial intelligence in scientific discovery: Challenges and opportunities. H Zenil, R King, Science: Challenges, Opportunities and the Future of Research. ParisOECD Publishing2023</p>
<p>A framework for evaluating the AI-driven automation of science. H Zenil, R King, Science: Challenges, Opportunities and the Future of Research. ParisOECD Publishing2023</p>
<p>The Far Future of AI in Scientific Discovery. H Zenil, R King, Choudhary, F. and T. H.)2023World Scientific</p>
<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. J Wei, Adv Neural Inf Process Syst. 352022</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, Adv Neural Inf Process Syst. 332020</p>
<p>A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT. J White, 2023</p>
<p>The Prompt Report: A Systematic Survey of Prompting Techniques. S Schulhoff, 2024</p>
<p>ChatGPT Prompt Engineering for Developers. I Fulford, A Ng, 2024</p>
<p>Exploring large language model based intelligent agents: Definitions, methods, and prospects. Y Cheng, arXiv:2401.034282024arXiv preprint</p>
<p>Auto-GPT for Online Decision Making: Benchmarks and Additional Opinions. H Yang, S Yue, Y He, 2023</p>
<p>. Y Nakajima, Babyagi, 2024Preprint at</p>
<p>. H Chase, Langchain, 2022</p>
<p>. J Liu, Llamaindex, 2022</p>
<p>DSPy: Compiling Declarative Language Model Calls into Self-Improving Pipelines. O Khattab, ICLR 202412th International Conference on Learning Representations. 2023</p>
<p>TextGrad: Automatic 'Differentiation' via Text. M Yuksekgonul, 2024</p>
<p>DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning. -Ai Deepseek, 2025</p>
<p>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models. Z Shao, 2024</p>
<p>From System 1 to System 2: A Survey of Reasoning Large Language Models. Z.-Z Li, 2025</p>
<p>Structured prompt interrogation and recursive extraction of semantics (SPIRES): A method for populating knowledge bases using zero-shot learning. J H Caufield, Bioinformatics. 40e1042024</p>
<p>ChatGPT outperforms crowd workers for text-annotation tasks. F Gilardi, M Alizadeh, M Kubli, Proceedings of the National Academy of Sciences. 120e23050161202023</p>
<p>Y Liu, 10.18653/V1/2023.EMNLP-MAIN.153NLG Evaluation using Gpt-4 with Better Human Alignment. EMNLP 2023 -2023 Conference on Empirical Methods in Natural Language Processing. 2511-2522 (2023</p>
<p>Domain-specific language model pretraining for biomedical natural language processing. Y Gu, ACM Transactions on Computing for Healthcare (HEALTH). 32021</p>
<p>Chemformer: a pre-trained transformer for computational chemistry. R Irwin, S Dimitriadis, J He, E J Bjerrum, Mach Learn Sci Technol. 3150222022</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. R Luo, Brief Bioinform. 234092022</p>
<p>Towards an AI co-scientist. J Gottweis, 2025</p>
<p>Emergent autonomous scientific research capabilities of large language models. D A Boiko, R Macknight, G Gomes, arXiv:2304.053322023arXiv preprint</p>
<p>Augmenting large language models with chemistry tools. M Bran, A , Nat Mach Intell. 2024</p>
<p>CRISPR-GPT: An LLM Agent for Automated Design of Gene-Editing Experiments. Y Qu, 10.1101/2024.04.25.591003v2doi:10.1101/2024.04.25.5910032024</p>
<p>BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments. Y Roohani, arXiv:2405.176312024arXiv preprint</p>
<p>Openai, arXiv:2305.14947v2OpenAI o1 System Card. 2024</p>
<p>Learning to Reason with LLMs. Openai, 2024</p>
<p>Scaling laws for neural language models. J Kaplan, arXiv:2001.083612020arXiv preprint</p>
<p>J Wei, arXiv:2206.07682Emergent abilities of large language models. 2022arXiv preprint</p>
<p>Sequence modeling and design from molecular to genome scale with Evo. E Nguyen, Science. 3861979. 2024</p>
<p>Genome modeling and design across all domains of life with Evo 2. bioRxiv 2025.02. G Brixi, 10.1101/2025.02.18.638918202518</p>
<p>scGPT: toward building a foundation model for single-cell multiomics using generative AI. H Cui, Nature Methods. 212024. 2024</p>
<p>Large-Scale Self-Supervised Pretraining for Molecular Property Prediction. S Chithrananda, G Grand, B Ramsundar, Chemberta, 2020</p>
<p>OmniJet-$\alpha$: The first cross-task foundation model for particle physics. J Birk, A Hallin, G Kasieczka, Mach Learn Sci Technol. 52024</p>
<p>Multiple Physics Pretraining for Physical Surrogate Models. M Mccabe, 2023</p>
<p>Scientific Equation Discovery via Programming with Large Language Models. P Shojaee, K Meidani, S Gupta, A B Farimani, C K Reddy, Llm-Sr, ArXiv abs/2404.184002024</p>
<p>BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. S Zhang, 2023</p>
<p>Associating Astronomical Observations and Natural Language with Multi-Modal Models. S Mishra-Sharma, Y Song, J Thaler, Paperclip, 2024</p>
<p>AstroCLIP: a cross-modal foundation model for galaxies. L Parker, Mon Not R Astron Soc. 5312024</p>
<p>Augmenting large language models with chemistry tools. M Bran, A , Nature Machine Intelligence. 65 62024. 2024</p>
<p>. Openai, 2023GPT-4 Technical Report</p>
<p>Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M W Chang, K Lee, K Toutanova, Bert, NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Conference. 20181</p>
<p>Learning Transferable Visual Models From Natural Language Supervision. A Radford, Proc Mach Learn Res. 1392021</p>
<p>Zero-Shot Text-to-Image Generation. A Ramesh, Proc Mach Learn Res. 1392021</p>
<p>Deepmind, Alphaproof, AI achieves silver-medal standard solving International Mathematical Olympiad problems. 2024</p>
<p>. E Zelikman, Y Wu, J Mu, N D Goodman, Star, 2022Bootstrapping Reasoning With Reasoning</p>
<p>K Karl Popper, Popper, The Logic of Scientific Discovery. 1959</p>
<p>A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models. W Fan, 10.1145/3637528.3671470Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining2024</p>
<p>Prompting Large Language Models for Counterfactual Generation: An Empirical Study. Y Li, M Xu, X Miao, S Zhou, T Qian, Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC-COLING 2024 -Main Conference Proceedings. 2024. 2023</p>
<p>E Kcman, R Ness, A Sharma, C Tan, arXiv:2305.00050Causal reasoning and large language models: Opening a new frontier for causality. 2023arXiv preprint</p>
<p>Efficient Causal Graph Discovery Using Large Language Models. T Jiralerspong, X Chen, Y More, V Shah, Y Bengio, arXiv:2402.012072024arXiv preprint</p>
<p>Toolformer: Language Models Can Teach Themselves to Use Tools. T Schick, Adv Neural Inf Process Syst. 362023</p>
<p>The Philosopher's Stone for Science--The Catalyst Change of AI for Scientific Creativity. Pin and Wang, Dashun, The Philosopher's Stone for Science--The Catalyst Change of AI for Scientific Creativity. Q Chen, Y.-J I Ho, P Sun, D Wang, March 5, 2024. 2024</p>
<p>An Algorithmic Information Calculus for Causal Discovery and Reprogramming Systems. H Zenil, 201919</p>
<p>Evolving self-reference: matter, symbols, and semantic closure. Laws, Language and Life: Howard Pattee's classic papers on the physics of symbols with contemporary commentary. H H Pattee, J Rczaszek-Leonardi, H H Pattee, 2012</p>
<p>Explaining patterns in data with language models via interpretable autoprompting. C Singh, J X Morris, J Aneja, A M Rush, J Gao, arXiv:2210.018482022arXiv preprint</p>
<p>Table-gpt: Table-tuned gpt for diverse table tasks. P Li, arXiv:2310.092632023arXiv preprint</p>
<p>Anygpt: Unified multimodal llm with discrete sequence modeling. J Zhan, arXiv:2402.122262024arXiv preprint</p>
<p>Beyond Language Models: Byte Models are. S Wu, Digital World Simulators. 2024</p>
<p>Language models are unsupervised multitask learners. A Radford, OpenAI blog. 192019</p>
<p>SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. D Weininger, J Chem Inf Comput Sci. 281988</p>
<p>S Zhang, Edge of Chaos. 2024</p>
<p>Large Language Models as Code Executors: An Exploratory Study. C Lyu, 2024</p>
<p>A Survey on Large Language Models for Code Generation. J Jiang, F Wang, J Shen, S Kim, S Kim, 2024</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, arXiv:2305.162912023arXiv preprint</p>
<p>ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and Characterization. K Darvish, 2024</p>
<p>Reflexion: Language Agents with Verbal Reinforcement Learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Adv Neural Inf Process Syst. 362023</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, arXiv:2210.036292022arXiv preprint</p>
<p>A Lavin, ArXiv abs/2112.03235Simulation Intelligence: Towards a New Generation of Scientific Methods. 2021</p>
<p>Generative Agents: Interactive Simulacra of Human Behavior. J S Park, 10.1145/3586183.3606763UIST 2023 -Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023</p>
<p>Explaining black box text modules in natural language with language models. C Singh, arXiv:2305.098632023arXiv preprint</p>
<p>Language models can explain neurons in language models. S Bills, 14. 05. 20232023</p>
<p>Quantifying the dynamics of failure across science, startups and security. Y Yin, Y Wang, J A Evans, D Wang, Nature. 5752019</p>
<p>. S A Kauffman, Investigations, 2000Oxford University Press</p>
<p>A mobile robotic chemist. B Burger, Nature. 5832020. 2020</p>
<p>Large language models as optimizers. C Yang, arXiv:2309.034092023arXiv preprint</p>
<p>Q Wang, D Downey, H Ji, T Hope, Scimon, arXiv:2305.14259Scientific Inspiration Machines Optimized for Novelty. 2023arXiv preprint</p>
<p>DreamCoder: growing generalizable, interpretable knowledge with wake-sleep Bayesian program learning. K Ellis, Philosophical Transactions of the Royal Society A. 3812020</p>
<p>Exploring The Behavior of Bioelectric Circuits using Evolution Heuristic Search. H Hazan, M Levin, Bioelectricity. 42022</p>
<p>Recombinant uncertainty in technological search. L Fleming, Manage Sci. 472001</p>
<p>Optimal Search for the Best Alternative. M Weitzman, 1978Department of Energy78</p>
<p>ChatGPT: five priorities for research. E A M Van Dis, J Bollen, W Zuidema, R Van Rooij, C L Bockting, Nature. 6142023</p>
<p>Why ChatGPT and Bing Chat are so good at making things up. B Edwards, Ars Technica. 2023</p>
<p>Shaking the foundations: delusions in sequence models for interaction and control. Deepmind, 2023</p>
<p>R Wang, arXiv:2309.05660Hypothesis search: Inductive reasoning with language models. 2023arXiv preprint</p>
<p>Technology readiness levels for machine learning systems. A Lavin, Nat Commun. 132020</p>
<p>Don't Trust: Verify --Grounding LLM Quantitative Reasoning with Autoformalization. J P Zhou, ICLR 202412th International Conference on Learning Representations. 2024</p>
<p>LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers. T X Olausson, 10.18653/V1/2023.EMNLP-MAIN.313EMNLP 2023 -2023 Conference on Empirical Methods in Natural Language Processing. 202351535176</p>
<p>Hypothesis Search: Inductive Reasoning with Language Models. R Wang, ICLR 202412th International Conference on Learning Representations. 2023</p>
<p>On the Measure of Intelligence. F Chollet, 2019</p>
<p>Scientific Hypothesis Generation by a Large Language Model: Laboratory Validation in Breast Cancer Treatment. A Abdel-Rehim, Royal Society Interface. 22202406742025</p>
<p>Assessing and Understanding Creativity in Large Language Models. Y Zhao, arXiv:2401.12491[cs.CL]2024</p>
<p>The Virtual Lab: AI Agents Design New SARS-CoV-2 Nanobodies with Experimental Validation. K Swanson, W Wu, N L Bulaong, J E Pak, J Zou, 10.1101/2024.11.11.6230042024</p>
<p>Ideas are dimes a dozen: Large language models for idea generation in innovation. K Girotra, L Meincke, C Terwiesch, K T Ulrich, Available at SSRN. 45260712023</p>
<p>Large Language Models are Zero Shot Hypothesis Proposers. B Qi, arXiv:2311.059652023arXiv preprint</p>
<p>Explaining black box text modules in natural language with language models. C Singh, 2023</p>
<p>LLMs learn governing principles of dynamical systems. T J B Liu, N Boull, R Sarfati, C J Earls, arXiv:2402.007952024arXiv preprintrevealing an</p>
<p>Language Modeling Is Compression. G Deltang, ICLR 202412th International Conference on Learning Representations. 2023</p>
<p>. Y Huang, J Zhang, Z Shan, He, J. Compression Represents Intelligence Linearly. 2024</p>
<p>Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models. Y Shao, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024 1. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024 12024</p>
<p>Introducing Perplexity Deep Research. A I Perplexity, 2025</p>
<p>Human-in-the-Loop through Chain-of-Thought. Z Cai, B Chang, W Han, 2023</p>
<p>F Petroni, 10.18653/v1/d19-1250Language Models as Knowledge Bases? EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference 2463-2473. 2019</p>
<p>A mathematical investigation of hallucination and creativity in gpt models. M Lee, Mathematics. 1123202023</p>
<p>Evaluating Large Language Models Trained on Code. M Chen, 2021</p>
<p>Survey of Hallucination in Natural Language Generation. Z Ji, ACM Comput Surv. 552022</p>
<p>Artificial Intelligence May Not 'Hallucinate' After All. L Matsakis, 2019</p>
<p>A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Adversarial Example Researchers Need to Expand What is Meant by 'Robustness. J Gilmer, D Hendrycks, 20194</p>
<p>A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models. S M T I Tonmoy, 2024</p>
<p>N Varshney, W Yao, H Zhang, J Chen, D Yu, Stitch, Time Saves Nine: Detecting and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation. 2023</p>
<p>More Agents Is All You Need. J Li, Q Zhang, Y Yu, Q Fu, D Ye, 2024</p>
<p>Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. N Mndler, J He, S Jenko, M Vechev, 2023</p>
<p>Chain-of-Verification Reduces Hallucination in Large Language Models. S Dhuliawala, 10.18653/V1/2024.FINDINGS-ACL.212Findings of the Association for Computational Linguistics ACL. 20242024</p>
<p>RARR: Researching and Revising What Language Models Say, Using Language Models. L Gao, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2023</p>
<p>On the Calibration of Large Language Models and Alignment. C Zhu, B Xu, Q Wang, Y Zhang, Z Mao, 2022</p>
<p>HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. J Li, X Cheng, W X Zhao, J Y Nie, J R Wen, 10.18653/V1/2023.EMNLP-MAIN.397EMNLP 2023 -2023 Conference on Empirical Methods in Natural Language Processing. 20236449</p>
<p>Measuring How Models Mimic Human Falsehoods. S Lin, J Hilton, O Evans, Truthfulqa, Proceedings of the Annual Meeting of the Association for Computational Linguistics. the Annual Meeting of the Association for Computational Linguistics2021</p>
<p>FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. S Min, 10.18653/v1/2023.emnlp-main.741EMNLP 2023 -2023 Conference on Empirical Methods in Natural Language Processing, Proceedings 12076-12100. 2023</p>
<p>Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. Y Liu, 2023</p>
<p>Think Twice Before Trusting: Self-Detection for Large Language Models through Comprehensive Answer Reflection. M Li, 2024</p>
<p>How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench. Q Ye, H Y Fu, X Ren, R Jia, 10.18653/v1/2023.findings-emnlp.503EMNLP. 20232023Findings of the Association for Computational Linguistics</p>
<p>The Reversal Curse: LLMs trained on 'A is B' fail to learn. L Berglund, arXiv:2309.122882023B is A'. arXiv preprint</p>
<p>Premise Order Matters in Reasoning with Large Language Models. X Chen, R A Chi, X Wang, D Zhou, Proc Mach Learn Res. 2352024</p>
<p>Physics of Language Models: Part 3.2, Knowledge Manipulation. Z Allen-Zhu, Yuanzhi Labs, F Li Yuanzhili, 2023</p>
<p>Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models. M Nezhurina, L Cipolina-Kun, M Cherti, J Jitsev, 2024</p>
<p>Faith and Fate: Limits of Transformers on Compositionality. N Dziri, Adv Neural Inf Process Syst. 362023</p>
<p>G Deltang, ICLR 2023Neural Networks and the Chomsky Hierarchy. 11th International Conference on Learning Representations. 2022</p>
<p>Embers of autoregression show how large language models are shaped by the problem they are trained to solve. R T Mccoy, S Yao, D Friedman, M D Hardy, T L Griffiths, Proc Natl Acad Sci U S A. 121e23224201212024</p>
<p>Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. Z Wu, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024 1. the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024 12024</p>
<p>LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks. S Kambhampati, 2024</p>
<p>On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS). V Pallagani, Proceedings of the International Conference on Automated Planning and Scheduling. the International Conference on Automated Planning and Scheduling202434</p>
<p>Large Language Models Cannot Self-Correct Reasoning Yet. J Huang, ICLR 202412th International Conference on Learning Representations. 2023</p>
<p>Self-Consistency Improves Chain of Thought Reasoning in Language Models. X Wang, ICLR 202311th International Conference on Learning Representations. 2022</p>
<p>Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty. K Zhou, J D Hwang, X Ren, M Sap, arXiv:2401.067302024arXiv preprint</p>
<p>Improving Factuality and Reasoning in Language Models through Multiagent Debate. Y Du, S Li, A Torralba, J B Tenenbaum, I Mordatch, Proc Mach Learn Res. 2352023</p>
<p>. O Golovneva, Z Allen-Zhu, J Weston, S Sukhbaatar, arXiv:2403.137992024arXiv preprintReverse Training to Nurse the Reversal Curse</p>
<p>interpreting GPT: the logit lens -LessWrong. </p>
<p>Representation engineering: A top-down approach to ai transparency. A Zou, arXiv:2310.014052023arXiv preprint</p>
<p>Do models explain themselves? Counterfactual simulatability of natural language explanations. Y Chen, arXiv:2307.086782023arXiv preprint</p>
<p>Attention is not not Explanation. S Wiegreffe, Y Pinter, 10.18653/V1/D19-1002EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference. 201911</p>
<p>Attention is not Explanation. S Jain, B C Wallace, 10.18653/V1/N19-1357Proceedings of the 2019 Conference of the North. the 2019 Conference of the North2019</p>
<p>Attention is all you need: utilizing attention in AI-enabled drug discovery. Y Zhang, Brief Bioinform. 252023</p>
<p>Aviary: training language agents on challenging scientific tasks. S Narayanan, 2024</p>
<p>Learning from negative findings. M I Taragin, Isr J Health Policy Res. 82019</p>
<p>Publishing negative results is good for science. E M Bik, Access Microbiol. 67922024</p>
<p>Researcher's Perceptions on Publishing "Negative" Results and Open Access. L Echevarri, A Malerba, V Arechavala-Gomeza, Nucleic Acid Ther. 311852021</p>
<p>ChatGPT 'contamination': estimating the prevalence of LLMs in the scholarly literature. A Gray, 10.1002/leap.15782024</p>
<p>Mapping the Increasing Use of LLMs in Scientific Papers. W Liang, 2024</p>
<p>G R Latona, M H Ribeiro, T R Davidson, V Veselovsky, R West, The AI Review Lottery: Widespread AI-Assisted Peer Reviews Boost Paper Scores and Acceptance Rates. 2024</p>
<p>Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews. W Liang, 2024</p>
<p>Can Large Language Models Provide Useful Feedback on Research Papers? A Large-Scale Empirical Analysis. W Liang, NEJM AI. 12024</p>
<p>Can ChatGPT evaluate research quality. M Thelwall, Journal of Data and Information Science. 92024</p>
<p>ChatGPT and large language models in academia: opportunities and challenges. J G Meyer, BioData Min. 16202023</p>
<p>Check your facts and try again: Improving large language models with external knowledge and automated feedback. B Peng, arXiv:2302.128132023arXiv preprint</p>
<p>Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning. L Luo, Y.-F Li, G Haffari, S Pan, 2023</p>
<p>Towards Mitigating LLM Hallucination via Self Reflection. Z Ji, 10.18653/V1/2023.FINDINGS-EMNLP.1232023</p>
<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models. S Yao, Adv Neural Inf Process Syst. 362023</p>
<p>The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning. X Ye, G Durrett, Adv Neural Inf Process Syst. 352022</p>
<p>Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models. A Ghandeharioun, A Caciularu, A Pearce, L Dixon, M Geva, Proc Mach Learn Res. 2352024</p>
<p>Academic publishing and the myth of linguistic injustice. K Hyland, J Second Lang Writ. 312016</p>
<p>Awkward wording. Rephrase"': linguistic injustice in ecological journals. M Clavero, 2010</p>
<p>Shakespeare and the English poets: The influence of native speaking English reviewers on the acceptance of journal articles. P Strauss, Publications. 7202019</p>
<p>Night science. I Yanai, M Lercher, Genome Biol. 202019</p>
<p>Mastering the game of Go with deep neural networks and tree search. D Silver, Nature. 5292016. 2016</p>
<p>A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, Science. 3621979. 2018</p>
<p>Absolute Zero: Reinforced Self-play Reasoning with Zero Data. A Zhao, 2025</p>
<p>Computational disease modeling -Fact or fiction?. J N Tegnr, BMC Syst Biol. 32009</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2661 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2661</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2661</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-0b063955bb5cbf6c2e89630206a921de82fafa05</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/0b063955bb5cbf6c2e89630206a921de82fafa05" target="_blank">PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> It is shown that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses, and is shown that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses.</p>
                <p><strong>Paper Abstract:</strong> PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art AI techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2661.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2661.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubTator 3.0</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PubTator 3.0</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large biomedical literature resource and NLP pipeline that provides precomputed named-entity annotations and 12 types of relation extractions across ~36M PubMed abstracts and ~6M PMC-OA full-text articles, and that supports semantic, relation, and retrieval-augmented interactions with LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PubTator 3.0</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An integrated literature curation and search system built on a weekly NLP pipeline: AIONER for named-entity recognition, per-entity normalization modules (GNorm2, tmVar3, NLM-Chem, TaggerOne), the BioREx unified deep-learning relation extractor, MongoDB for storage, and Apache Solr for search. PubTator precomputes entity mentions and 12 relation types, indexes them for semantic and relation-based queries, exposes REST APIs (Find Entity ID, Find Related Entities, Export Relevant Search Results), and supports retrieval-augmented generation by exposing these APIs to LLMs. The system replaced PubMedBERT with LinkBERT in its relation extraction encoder to improve performance.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented knowledgebase / NLP pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedicine / biomedical literature mining</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not an automated hypothesis-generator per se; enables hypothesis formulation by providing high-recall semantic and relation search over precomputed entity–relation annotations and by serving as a high-quality retrieval backend for LLMs (retrieval-augmented generation) that can propose hypotheses grounded in extracted relations and supporting articles.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is supported indirectly via relation extraction (BioREx) and by surfacing article evidence (PubMed IDs and highlighted snippets) so users or LLMs can assess plausibility; no explicit automated plausibility scoring beyond relation confidence/prioritization is described.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Provides article-level evidence (PMIDs) and extracted relations as verifiable provenance for claims; used in human evaluation (manual review of top-20 search results) and in retrieval-augmented GPT-4 experiments where cited articles were manually verified to assess citation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Open-source components and code releases (AIONER, BioREx, tmVar3, GNorm2, TaggerOne, and example code for ChatGPT integration) and public APIs and bulk downloads (FTP) to enable reproducibility of annotations and retrieval-augmented experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Used as a grounded retrieval source for LLMs (retrieval-augmented generation) — LLMs were instructed to call PubTator APIs to find entity IDs, related entities, and exporting relevant PubMed article IDs that contain textual evidence; prompts and a function-calling interface force LLM answers to cite verifiable PubTator-provided evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Manual verification of cited PMIDs for whether the cited article supports the stated relation; comparison of citation accuracy across unaugmented GPT-4, PubMed-augmented GPT-4, and PubTator-augmented GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Evaluated using BioRED (relation extraction) and BioCreative V CDR (chemical-disease relation) among other standard corpora referenced in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Contains >1.6 billion entity annotations and 33 million relations. Relation extraction: BioREx achieved F1 = 79.6% on the BioRED test set; when PubTator replaced PubMedBERT with LinkBERT the relation-extraction F1 increased to 82.0%. Entity normalization/NER improvements versus prior PubTator Central reported (macro-average F1 for NER+Norm = 83.80% previously vs PubTator3.0 micro-average 83.80% and macro-average 86.71% in Supplementary Table 3 per entity type). Retrieval case studies: average precision in top-20 = 90.0% across 12 entity-pair queries (216/240).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperforms PubMed and Google Scholar in the provided entity-pair retrieval case studies (higher recall and higher top-20 precision). Relation extraction outperformed SemRep, CD-REST, and previous state-of-the-art systems on BioCreative V CDR and BioRED benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Full-text relation extraction currently limited to abstracts for computational reasons; only 12 relation types are extracted (though common); automated annotations remain imperfect despite improved performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2661.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2661.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BioREx</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BioREx</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A unified deep-learning relation-extraction model that reconciles heterogeneous relation-annotation datasets to extract 12 relation types across multiple biomedical entity-pairs, improving cross-dataset generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BioREx</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A deep-learning unified relation extraction framework that aggregates and reconciles disparate, task-specific relation datasets into a single training resource using a data-centric approach. It uses a transformer-based encoder (originally PubMedBERT, later replaced with LinkBERT in PubTator 3.0) and a relation classification head to extract 12 relation types across eight entity pairs (chemical-chemical, chemical-disease, chemical-gene, chemical-variant, disease-gene, disease-variant, gene-gene, variant-variant).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>deep-learning relation extraction (transformer-based)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical relation extraction / text mining</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Relation-level classification and confidence (implicit in model outputs) provide plausibility signals for candidate relations extracted from literature; no explicit novelty scoring reported.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluated on manually annotated relation extraction datasets (BioRED test set, BioCreative V CDR) and compared against rule-based and prior state-of-the-art systems; performance measured by F1.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Source code available (GitHub link referenced); trained/evaluated on public corpora (BioRED, BioCreative V CDR) enabling reproducibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Model output confidence (implicit) via classification scores; no explicit Bayesian or calibrated uncertainty framework described.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>BioRED (relation extraction), BioCreative V Chemical-Disease Relation (CDR) corpus</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>BioREx raised BioRED test-set F1 from 74.4% (prior) to 79.6%; replacing PubMedBERT with LinkBERT increased F1 to 82.0%. On BioCreative V CDR, PubTator 3.0 (using BioREx) achieved substantially higher F-score than SemRep, CD-REST, and previous best systems (exact prior numbers not all enumerated in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Improved F1 relative to transfer-learning, multi-task learning, isolated-dataset models, and rule-based SemRep/C D-REST baselines on benchmark corpora.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires substantial and harmonized training data; training-data fragmentation across many datasets necessitated data reconciliation; current relation scope limited to 12 relation types; relation extraction on full text is not yet performed at scale in production (abstracts only).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2661.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2661.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (unaugmented)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Generative Pre-trained Transformer 4 (GPT-4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large multimodal pre-trained language model by OpenAI used here as a baseline LLM for answering biomedical queries; strong language abilities but prone to producing unsupported citations (hallucinations) when not grounded in retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4 (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A general-purpose large language model used via the OpenAI ChatCompletion API (Azure OpenAI Services) with deterministic decoding (temperature=0) in experiments; in these experiments GPT-4 generated natural-language answers and citations when prompted, without guaranteed grounding to a verified retrieval backend.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>LLM-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>general / biomedical QA</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generative completion from prompt; can propose candidate answers/hypotheses based on learned distribution but without grounded retrieval, producing hallucinated or fabricated citations.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>None intrinsic; in the paper, outputs were manually checked for citation validity as part of the evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Experiments used Azure OpenAI Services, model version and date specified, and deterministic temperature setting (temperature=0), supporting reproducibility of output given identical prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Manual verification of cited PMIDs revealed frequent fabricated citations; used as baseline in citation-accuracy experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td>High hallucination observed in this study: citation-accuracy across 8 test questions = 2/36 (≈5.6% correct), implying ≈94.4% of cited items were incorrect/unsupported in this small evaluation sample.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Citation-accuracy (manual) on 8 questions: 2 correct citations out of 36 total citations (5.6% accuracy) in this paper's qualitative evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Used as baseline; substantially outperformed by PubTator-augmented GPT-4 in citation accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prone to hallucinations / fabricated citations when not retrieval-anchored; lacks built-in provenance for factual claims in the biomedical domain.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2661.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2661.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubTator-augmented GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 augmented with PubTator 3.0 via function-calling retrieval-augmented generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented generation (RAG) setup where GPT-4 is prompted to call PubTator APIs to obtain entity identifiers, related entities (relations), and PubMed article identifiers with textual evidence, and then synthesize answers with cited evidence — used to dramatically reduce hallucinated citations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PubTator-augmented GPT-4 (RAG via function calls)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integration implemented via OpenAI ChatCompletion API function-calling: GPT-4 is given descriptions of PubTator APIs (Find Entity ID, Find Related Entities, Export Relevant Search Results), instructed to decompose queries into API calls, execute them, and synthesize results starting with 'Summary:' and including citations. The system used deterministic decoding (temperature=0) and automated decomposition and evidence retrieval to anchor answers in PubTator-extracted relations and articles.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented LLM</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical QA / literature-based validation</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>LLM generation grounded via retrieved relations and articles from PubTator; GPT-4 composes candidate answers/hypotheses from retrieved evidence rather than relying solely on parametric memory.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility enforced by requiring explicit retrieved article evidence for each claimed relation; manual verification of cited PMIDs served as plausibility check in evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Measured via manual citation-accuracy: number of cited articles that actually support the claimed relation (per question: numerator/denominator counts).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Validation by returning explicit PubMed IDs and textual snippets from articles identified by PubTator relations; manual review of cited PMIDs to verify that each cited article supports the stated relation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Code and examples released (github link for pubtator-gpt); OpenAI API versions and model versions specified; deterministic decoding settings documented.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Anchoring answers in PubTator-extracted relations + forcing function calls for evidence retrieval; instructing LLM to include only evidence-backed claims and cite PMIDs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Manual verification of each cited PMID against the claimed relation; the experimental setup measured citation-accuracy as a proxy for hallucination frequency.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td>In the paper's 8-question qualitative evaluation, PubTator-augmented GPT-4 produced 279 correct citations out of 292 citations (≈95.55% citation-accuracy), implying an empirical hallucination rate of ≈4.45% in that sample.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>No explicit probabilistic uncertainty quantification reported; reliance on retrieved evidence and deterministic generation (temperature=0) to reduce spurious outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>8 hand-selected user-grounded biomedical questions drawn from PubMed query logs (detailed in Supplementary Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Citation-accuracy across 8 questions: 279/292 (≈95.6% correct citations). Individual question-level results are reported in Supplementary Table 5 (many entries 100% or near 100%).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Substantially outperformed unaugmented GPT-4 (2/36 = 5.6% citation-accuracy) and PubMed-augmented GPT-4 (22/34 = 64.7% citation-accuracy) in the paper's manual citation checks.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Evaluation was qualitative and limited in scale (8 questions); the approach depends on the coverage and accuracy of PubTator's entity and relation extraction (which is high but imperfect); full-text relation extraction at scale is currently limited (abstract-only relations in production).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2661.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2661.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PubMed-augmented GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 augmented with PubMed (E-utilities) retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A variant retrieval-augmented GPT-4 that uses PubMed E-utilities to find articles and then synthesize answers; improves citation accuracy over baseline GPT-4 but performs worse than PubTator-augmented GPT-4 for relation-specific queries because PubMed search is keyword/Boolean based and not relation-aware.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>PubMed-augmented GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integration of GPT-4 with PubMed database search via NCBI E-utilities API; GPT-4 uses retrieved article lists from PubMed (keyword/Boolean search) to attempt to cite supporting articles and synthesize answers. Implemented in the same experimental framework as PubTator-augmented GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>retrieval-augmented LLM (keyword search backend)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical QA / literature retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>LLM generation using articles retrieved by PubMed search; lacks relation-specific retrieval so hypothesis/citation selection relies on keyword matches and LLM judgment.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Measured via manual citation-accuracy: 22 correct citations out of 34 total (≈64.7%) across the evaluation questions.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Returned PubMed article IDs which were manually checked to see whether they supported the claimed relation; however, because PubMed search is not relation-aware, some cited articles did not actually support the relation.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Use of NCBI E-utilities; experimental details (OpenAI/Azure versions, deterministic settings) documented.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Limited; relies on PubMed search to provide candidate evidence but no relation-aware filtering, so hallucination reduction is partial.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td>Manual verification of cited PMIDs in the experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td>In the study: 22/34 correct citations → citation-accuracy ≈64.7%, implying an empirical hallucination rate ≈35.3% for cited items in this small sample.</td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same 8 question set used in the paper's RAG evaluation (Supplementary Table 5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Citation-accuracy 22/34 (≈64.7%) across the tested questions; outperforms unaugmented GPT-4 but underperforms PubTator-augmented GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Better than GPT-4 alone (5.6% accuracy) but worse than PubTator-augmented GPT-4 (95.6% accuracy) in the manual citation-accuracy evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>PubMed's keyword/Boolean search returns articles based on surface lexical matches rather than explicit relations, so evidence returned may not support specific relation claims; leads to lower citation-accuracy relative to relation-aware retrieval (PubTator).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2661.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2661.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SemRep</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>SemRep</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A widely used rule-based biomedical relation extraction system used as a baseline comparison for PubTator's relation extraction performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Broad-coverage biomedical relation extraction with SemRep.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SemRep</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A rule-based, broad-coverage system for extracting biomedical relations from text (lexico-syntactic and knowledge-based rules); cited in this paper as a comparative baseline when evaluating BioREx and PubTator relation extraction performance.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>rule-based relation extraction</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>biomedical relation extraction / text mining</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Evaluation on benchmark relation corpora (used as baseline for comparisons in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Benchmarked on corpora like BioCreative V CDR in comparative evaluations in the literature (used as a baseline here).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Presented as a baseline that PubTator/BioREx outperformed on the BioCreative V CDR and BioRED evaluations (exact SemRep numbers not enumerated in the main text of this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Cited as a baseline; PubTator 3.0 (BioREx) reported substantially higher F-score than SemRep on benchmark datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Rule-based approaches can be less flexible and lower-performing than modern deep-learning unified models on heterogeneous relation corpora, per the paper's comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AIONER: all-in-one schemebased biomedical named entity recognition using deep learning. <em>(Rating: 2)</em></li>
                <li>BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets. <em>(Rating: 2)</em></li>
                <li>Domain-specific language model pretraining for biomedical natural language processing. <em>(Rating: 1)</em></li>
                <li>LinkBERT: Pretraining Language Models with Document Links. <em>(Rating: 1)</em></li>
                <li>Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature? <em>(Rating: 2)</em></li>
                <li>Opportunities and challenges for ChatGPT and large language models in biomedicine and health. <em>(Rating: 2)</em></li>
                <li>Broad-coverage biomedical relation extraction with SemRep. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2661",
    "paper_id": "paper-0b063955bb5cbf6c2e89630206a921de82fafa05",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "PubTator 3.0",
            "name_full": "PubTator 3.0",
            "brief_description": "A large biomedical literature resource and NLP pipeline that provides precomputed named-entity annotations and 12 types of relation extractions across ~36M PubMed abstracts and ~6M PMC-OA full-text articles, and that supports semantic, relation, and retrieval-augmented interactions with LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PubTator 3.0",
            "system_description": "An integrated literature curation and search system built on a weekly NLP pipeline: AIONER for named-entity recognition, per-entity normalization modules (GNorm2, tmVar3, NLM-Chem, TaggerOne), the BioREx unified deep-learning relation extractor, MongoDB for storage, and Apache Solr for search. PubTator precomputes entity mentions and 12 relation types, indexes them for semantic and relation-based queries, exposes REST APIs (Find Entity ID, Find Related Entities, Export Relevant Search Results), and supports retrieval-augmented generation by exposing these APIs to LLMs. The system replaced PubMedBERT with LinkBERT in its relation extraction encoder to improve performance.",
            "system_type": "retrieval-augmented knowledgebase / NLP pipeline",
            "scientific_domain": "biomedicine / biomedical literature mining",
            "hypothesis_generation_method": "Not an automated hypothesis-generator per se; enables hypothesis formulation by providing high-recall semantic and relation search over precomputed entity–relation annotations and by serving as a high-quality retrieval backend for LLMs (retrieval-augmented generation) that can propose hypotheses grounded in extracted relations and supporting articles.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility is supported indirectly via relation extraction (BioREx) and by surfacing article evidence (PubMed IDs and highlighted snippets) so users or LLMs can assess plausibility; no explicit automated plausibility scoring beyond relation confidence/prioritization is described.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Provides article-level evidence (PMIDs) and extracted relations as verifiable provenance for claims; used in human evaluation (manual review of top-20 search results) and in retrieval-augmented GPT-4 experiments where cited articles were manually verified to assess citation accuracy.",
            "reproducibility_measures": "Open-source components and code releases (AIONER, BioREx, tmVar3, GNorm2, TaggerOne, and example code for ChatGPT integration) and public APIs and bulk downloads (FTP) to enable reproducibility of annotations and retrieval-augmented experiments.",
            "hallucination_prevention_method": "Used as a grounded retrieval source for LLMs (retrieval-augmented generation) — LLMs were instructed to call PubTator APIs to find entity IDs, related entities, and exporting relevant PubMed article IDs that contain textual evidence; prompts and a function-calling interface force LLM answers to cite verifiable PubTator-provided evidence.",
            "hallucination_detection_method": "Manual verification of cited PMIDs for whether the cited article supports the stated relation; comparison of citation accuracy across unaugmented GPT-4, PubMed-augmented GPT-4, and PubTator-augmented GPT-4.",
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Evaluated using BioRED (relation extraction) and BioCreative V CDR (chemical-disease relation) among other standard corpora referenced in the paper.",
            "performance_metrics": "Contains &gt;1.6 billion entity annotations and 33 million relations. Relation extraction: BioREx achieved F1 = 79.6% on the BioRED test set; when PubTator replaced PubMedBERT with LinkBERT the relation-extraction F1 increased to 82.0%. Entity normalization/NER improvements versus prior PubTator Central reported (macro-average F1 for NER+Norm = 83.80% previously vs PubTator3.0 micro-average 83.80% and macro-average 86.71% in Supplementary Table 3 per entity type). Retrieval case studies: average precision in top-20 = 90.0% across 12 entity-pair queries (216/240).",
            "comparison_with_baseline": "Outperforms PubMed and Google Scholar in the provided entity-pair retrieval case studies (higher recall and higher top-20 precision). Relation extraction outperformed SemRep, CD-REST, and previous state-of-the-art systems on BioCreative V CDR and BioRED benchmarks.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Full-text relation extraction currently limited to abstracts for computational reasons; only 12 relation types are extracted (though common); automated annotations remain imperfect despite improved performance.",
            "uuid": "e2661.0",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "BioREx",
            "name_full": "BioREx",
            "brief_description": "A unified deep-learning relation-extraction model that reconciles heterogeneous relation-annotation datasets to extract 12 relation types across multiple biomedical entity-pairs, improving cross-dataset generalization.",
            "citation_title": "BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets.",
            "mention_or_use": "use",
            "system_name": "BioREx",
            "system_description": "A deep-learning unified relation extraction framework that aggregates and reconciles disparate, task-specific relation datasets into a single training resource using a data-centric approach. It uses a transformer-based encoder (originally PubMedBERT, later replaced with LinkBERT in PubTator 3.0) and a relation classification head to extract 12 relation types across eight entity pairs (chemical-chemical, chemical-disease, chemical-gene, chemical-variant, disease-gene, disease-variant, gene-gene, variant-variant).",
            "system_type": "deep-learning relation extraction (transformer-based)",
            "scientific_domain": "biomedical relation extraction / text mining",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Relation-level classification and confidence (implicit in model outputs) provide plausibility signals for candidate relations extracted from literature; no explicit novelty scoring reported.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Evaluated on manually annotated relation extraction datasets (BioRED test set, BioCreative V CDR) and compared against rule-based and prior state-of-the-art systems; performance measured by F1.",
            "reproducibility_measures": "Source code available (GitHub link referenced); trained/evaluated on public corpora (BioRED, BioCreative V CDR) enabling reproducibility.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Model output confidence (implicit) via classification scores; no explicit Bayesian or calibrated uncertainty framework described.",
            "benchmark_dataset": "BioRED (relation extraction), BioCreative V Chemical-Disease Relation (CDR) corpus",
            "performance_metrics": "BioREx raised BioRED test-set F1 from 74.4% (prior) to 79.6%; replacing PubMedBERT with LinkBERT increased F1 to 82.0%. On BioCreative V CDR, PubTator 3.0 (using BioREx) achieved substantially higher F-score than SemRep, CD-REST, and previous best systems (exact prior numbers not all enumerated in the paper).",
            "comparison_with_baseline": "Improved F1 relative to transfer-learning, multi-task learning, isolated-dataset models, and rule-based SemRep/C D-REST baselines on benchmark corpora.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Requires substantial and harmonized training data; training-data fragmentation across many datasets necessitated data reconciliation; current relation scope limited to 12 relation types; relation extraction on full text is not yet performed at scale in production (abstracts only).",
            "uuid": "e2661.1",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "GPT-4 (unaugmented)",
            "name_full": "Generative Pre-trained Transformer 4 (GPT-4)",
            "brief_description": "A large multimodal pre-trained language model by OpenAI used here as a baseline LLM for answering biomedical queries; strong language abilities but prone to producing unsupported citations (hallucinations) when not grounded in retrieval.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4 (baseline)",
            "system_description": "A general-purpose large language model used via the OpenAI ChatCompletion API (Azure OpenAI Services) with deterministic decoding (temperature=0) in experiments; in these experiments GPT-4 generated natural-language answers and citations when prompted, without guaranteed grounding to a verified retrieval backend.",
            "system_type": "LLM-based",
            "scientific_domain": "general / biomedical QA",
            "hypothesis_generation_method": "Generative completion from prompt; can propose candidate answers/hypotheses based on learned distribution but without grounded retrieval, producing hallucinated or fabricated citations.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "None intrinsic; in the paper, outputs were manually checked for citation validity as part of the evaluation.",
            "reproducibility_measures": "Experiments used Azure OpenAI Services, model version and date specified, and deterministic temperature setting (temperature=0), supporting reproducibility of output given identical prompts.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": "Manual verification of cited PMIDs revealed frequent fabricated citations; used as baseline in citation-accuracy experiments.",
            "hallucination_rate": "High hallucination observed in this study: citation-accuracy across 8 test questions = 2/36 (≈5.6% correct), implying ≈94.4% of cited items were incorrect/unsupported in this small evaluation sample.",
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": null,
            "performance_metrics": "Citation-accuracy (manual) on 8 questions: 2 correct citations out of 36 total citations (5.6% accuracy) in this paper's qualitative evaluation.",
            "comparison_with_baseline": "Used as baseline; substantially outperformed by PubTator-augmented GPT-4 in citation accuracy.",
            "validated_on_real_science": false,
            "novel_discoveries": null,
            "limitations": "Prone to hallucinations / fabricated citations when not retrieval-anchored; lacks built-in provenance for factual claims in the biomedical domain.",
            "uuid": "e2661.2",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PubTator-augmented GPT-4",
            "name_full": "GPT-4 augmented with PubTator 3.0 via function-calling retrieval-augmented generation",
            "brief_description": "A retrieval-augmented generation (RAG) setup where GPT-4 is prompted to call PubTator APIs to obtain entity identifiers, related entities (relations), and PubMed article identifiers with textual evidence, and then synthesize answers with cited evidence — used to dramatically reduce hallucinated citations.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "PubTator-augmented GPT-4 (RAG via function calls)",
            "system_description": "Integration implemented via OpenAI ChatCompletion API function-calling: GPT-4 is given descriptions of PubTator APIs (Find Entity ID, Find Related Entities, Export Relevant Search Results), instructed to decompose queries into API calls, execute them, and synthesize results starting with 'Summary:' and including citations. The system used deterministic decoding (temperature=0) and automated decomposition and evidence retrieval to anchor answers in PubTator-extracted relations and articles.",
            "system_type": "retrieval-augmented LLM",
            "scientific_domain": "biomedical QA / literature-based validation",
            "hypothesis_generation_method": "LLM generation grounded via retrieved relations and articles from PubTator; GPT-4 composes candidate answers/hypotheses from retrieved evidence rather than relying solely on parametric memory.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": "Plausibility enforced by requiring explicit retrieved article evidence for each claimed relation; manual verification of cited PMIDs served as plausibility check in evaluation.",
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Measured via manual citation-accuracy: number of cited articles that actually support the claimed relation (per question: numerator/denominator counts).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Validation by returning explicit PubMed IDs and textual snippets from articles identified by PubTator relations; manual review of cited PMIDs to verify that each cited article supports the stated relation.",
            "reproducibility_measures": "Code and examples released (github link for pubtator-gpt); OpenAI API versions and model versions specified; deterministic decoding settings documented.",
            "hallucination_prevention_method": "Anchoring answers in PubTator-extracted relations + forcing function calls for evidence retrieval; instructing LLM to include only evidence-backed claims and cite PMIDs.",
            "hallucination_detection_method": "Manual verification of each cited PMID against the claimed relation; the experimental setup measured citation-accuracy as a proxy for hallucination frequency.",
            "hallucination_rate": "In the paper's 8-question qualitative evaluation, PubTator-augmented GPT-4 produced 279 correct citations out of 292 citations (≈95.55% citation-accuracy), implying an empirical hallucination rate of ≈4.45% in that sample.",
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "No explicit probabilistic uncertainty quantification reported; reliance on retrieved evidence and deterministic generation (temperature=0) to reduce spurious outputs.",
            "benchmark_dataset": "8 hand-selected user-grounded biomedical questions drawn from PubMed query logs (detailed in Supplementary Table 5).",
            "performance_metrics": "Citation-accuracy across 8 questions: 279/292 (≈95.6% correct citations). Individual question-level results are reported in Supplementary Table 5 (many entries 100% or near 100%).",
            "comparison_with_baseline": "Substantially outperformed unaugmented GPT-4 (2/36 = 5.6% citation-accuracy) and PubMed-augmented GPT-4 (22/34 = 64.7% citation-accuracy) in the paper's manual citation checks.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Evaluation was qualitative and limited in scale (8 questions); the approach depends on the coverage and accuracy of PubTator's entity and relation extraction (which is high but imperfect); full-text relation extraction at scale is currently limited (abstract-only relations in production).",
            "uuid": "e2661.3",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "PubMed-augmented GPT-4",
            "name_full": "GPT-4 augmented with PubMed (E-utilities) retrieval",
            "brief_description": "A variant retrieval-augmented GPT-4 that uses PubMed E-utilities to find articles and then synthesize answers; improves citation accuracy over baseline GPT-4 but performs worse than PubTator-augmented GPT-4 for relation-specific queries because PubMed search is keyword/Boolean based and not relation-aware.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "PubMed-augmented GPT-4",
            "system_description": "Integration of GPT-4 with PubMed database search via NCBI E-utilities API; GPT-4 uses retrieved article lists from PubMed (keyword/Boolean search) to attempt to cite supporting articles and synthesize answers. Implemented in the same experimental framework as PubTator-augmented GPT-4.",
            "system_type": "retrieval-augmented LLM (keyword search backend)",
            "scientific_domain": "biomedical QA / literature retrieval",
            "hypothesis_generation_method": "LLM generation using articles retrieved by PubMed search; lacks relation-specific retrieval so hypothesis/citation selection relies on keyword matches and LLM judgment.",
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": "Measured via manual citation-accuracy: 22 correct citations out of 34 total (≈64.7%) across the evaluation questions.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Returned PubMed article IDs which were manually checked to see whether they supported the claimed relation; however, because PubMed search is not relation-aware, some cited articles did not actually support the relation.",
            "reproducibility_measures": "Use of NCBI E-utilities; experimental details (OpenAI/Azure versions, deterministic settings) documented.",
            "hallucination_prevention_method": "Limited; relies on PubMed search to provide candidate evidence but no relation-aware filtering, so hallucination reduction is partial.",
            "hallucination_detection_method": "Manual verification of cited PMIDs in the experiment.",
            "hallucination_rate": "In the study: 22/34 correct citations → citation-accuracy ≈64.7%, implying an empirical hallucination rate ≈35.3% for cited items in this small sample.",
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same 8 question set used in the paper's RAG evaluation (Supplementary Table 5).",
            "performance_metrics": "Citation-accuracy 22/34 (≈64.7%) across the tested questions; outperforms unaugmented GPT-4 but underperforms PubTator-augmented GPT-4.",
            "comparison_with_baseline": "Better than GPT-4 alone (5.6% accuracy) but worse than PubTator-augmented GPT-4 (95.6% accuracy) in the manual citation-accuracy evaluation.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "PubMed's keyword/Boolean search returns articles based on surface lexical matches rather than explicit relations, so evidence returned may not support specific relation claims; leads to lower citation-accuracy relative to relation-aware retrieval (PubTator).",
            "uuid": "e2661.4",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SemRep",
            "name_full": "SemRep",
            "brief_description": "A widely used rule-based biomedical relation extraction system used as a baseline comparison for PubTator's relation extraction performance.",
            "citation_title": "Broad-coverage biomedical relation extraction with SemRep.",
            "mention_or_use": "mention",
            "system_name": "SemRep",
            "system_description": "A rule-based, broad-coverage system for extracting biomedical relations from text (lexico-syntactic and knowledge-based rules); cited in this paper as a comparative baseline when evaluating BioREx and PubTator relation extraction performance.",
            "system_type": "rule-based relation extraction",
            "scientific_domain": "biomedical relation extraction / text mining",
            "hypothesis_generation_method": null,
            "novelty_assessment_method": null,
            "plausibility_assessment_method": null,
            "novelty_plausibility_balance": null,
            "hypothesis_quality_metrics": null,
            "pre_experiment_evaluation": null,
            "validation_mechanism": "Evaluation on benchmark relation corpora (used as baseline for comparisons in the paper).",
            "reproducibility_measures": null,
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Benchmarked on corpora like BioCreative V CDR in comparative evaluations in the literature (used as a baseline here).",
            "performance_metrics": "Presented as a baseline that PubTator/BioREx outperformed on the BioCreative V CDR and BioRED evaluations (exact SemRep numbers not enumerated in the main text of this paper).",
            "comparison_with_baseline": "Cited as a baseline; PubTator 3.0 (BioREx) reported substantially higher F-score than SemRep on benchmark datasets.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Rule-based approaches can be less flexible and lower-performing than modern deep-learning unified models on heterogeneous relation corpora, per the paper's comparisons.",
            "uuid": "e2661.5",
            "source_info": {
                "paper_title": "PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AIONER: all-in-one schemebased biomedical named entity recognition using deep learning.",
            "rating": 2
        },
        {
            "paper_title": "BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets.",
            "rating": 2
        },
        {
            "paper_title": "Domain-specific language model pretraining for biomedical natural language processing.",
            "rating": 1
        },
        {
            "paper_title": "LinkBERT: Pretraining Language Models with Document Links.",
            "rating": 1
        },
        {
            "paper_title": "Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature?",
            "rating": 2
        },
        {
            "paper_title": "Opportunities and challenges for ChatGPT and large language models in biomedicine and health.",
            "rating": 2
        },
        {
            "paper_title": "Broad-coverage biomedical relation extraction with SemRep.",
            "rating": 1
        }
    ],
    "cost": 0.01984925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge</h1>
<p>Chih-Hsuan Wei ${ }^{1,1}$, Alexis Allot ${ }^{1,2}$, Po-Ting Lai ${ }^{1}$, Robert Leaman ${ }^{1}$, Shubo Tian ${ }^{1}$, Ling Luo ${ }^{1}$, Qiao Jin ${ }^{1}$, Zhizheng Wang ${ }^{1}$, Qingyu Chen ${ }^{1}$ and Zhiyong Lu ${ }^{1, <em>}$<br>${ }^{1}$ National Center for Biotechnology Information (NCBI), National Library of Medicine (NLM), National Institutes of Health (NIH), MD, 20894, Bethesda, USA<br></em> To whom correspondence should be addressed. Tel: +1 301594 7089; Email: zhiyong.lu@nih.gov<br>Present Address: Alexis Allot, The Neuro (Montreal Neurological Institute-Hospital), McGill University, Montreal, Quebec H3A 2B4, Canada<br>Present Address: Ling Luo, School of Computer Science and Technology, Dalian University of Technology, 116024, Dalian, China<br>Present Address: Qingyu Chen, Biomedical Informatics and Data Science, Yale School of Medicine, CT, 06510, New Haven, USA</p>
<h4>Abstract</h4>
<p>PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a biomedical literature resource using state-of-the-art Al techniques to offer semantic and relation searches for key concepts like proteins, genetic variants, diseases, and chemicals. It currently provides over one billion entity and relation annotations across approximately 36 million PubMed abstracts and 6 million full-text articles from the PMC open access subset, updated weekly. PubTator 3.0's online interface and API utilize these precomputed entity relations and synonyms to provide advanced search capabilities and enable large-scale analyses, streamlining many complex information needs. We showcase the retrieval quality of PubTator 3.0 using a series of entity pair queries, demonstrating that PubTator 3.0 retrieves a greater number of articles than either PubMed or Google Scholar, with higher precision in the top 20 results. We further show that integrating ChatGPT (GPT-4) with PubTator APIs dramatically improves the factuality and verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive set of features and tools that allow</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery.</p>
<h1>INTRODUCTION</h1>
<p>The biomedical literature is a primary resource to address information needs across the biological and clinical sciences (1), however the requirements for literature search vary widely. Activities such as formulating a research hypothesis require an exploratory approach, whereas tasks like interpreting the clinical significance of genetic variants are more focused.</p>
<p>Traditional keyword-based search methods have long formed the foundation of biomedical literature search (2). While generally effective for basic search, these methods also have significant limitations, such as missing relevant articles due to differing terminology or including irrelevant articles because surface-level term matches cannot adequately represent the required association between query terms. These limitations cost time and risk information needs remaining unmet.</p>
<p>Natural language processing (NLP) methods provide substantial value for creating bioinformatics resources (3-5), and may improve literature search by enabling semantic and relation search. In semantic search, users indicate specific concepts of interest (entities) for which the system has precomputed matches regardless of the terminology used. Relation search increases precision by allowing users to specify the type of relationship desired between entities, such as whether a chemical enhances or reduces expression of a gene. In this regard, we present PubTator 3.0, a novel resource engineered to support semantic and relation search in the biomedical literature. Its search capabilities allow users to explore automated entity annotations for six key biomedical entities: genes, diseases, chemicals, genetic variants, species, and cell lines. PubTator 3.0 also identifies and makes searchable 12 common types of relations between entities, enhancing its utility for both targeted and exploratory searches. Focusing on relations and entity types of interest across the biomedical sciences allows PubTator 3.0 to retrieve information precisely while providing broad utility (see detailed comparisons with its predecessor in Supplementary Table 1).</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />
ure 1. PubTator 3.0 system overview and search results page: 1. Query auto-complete enhances search accuracy and synonym matching. 2. Natural language processing (NLP)-enhanced relevance: Search results are prioritized according to the depth of the relationship between the entities queried. 3. Users can further refine results with facet filters-section, journal, and type. 4. Search results include highlighted entity snippets explaining relevance. 5. Histogram visualizes number of results by publication year. 6. Entity highlighting can be switched on or off according to user preference.</p>
<h1>SYSTEM OVERVIEW</h1>
<p>The PubTator 3.0 online interface, illustrated in Fig. 1 and Supplementary Fig. 1, is designed for interactive literature exploration, supporting semantic, relation, keyword, and Boolean queries. An auto-complete function provides semantic search suggestions to assist users with query</p>
<p>formulation. For example, it automatically suggests replacing either "COVID-19" or "SARS-CoV-2 infection" with the semantic term "@DISEASE_COVID_19". Relation queries - new to PubTator 3.0 - provide increased precision, allowing users to target articles which discuss specific relationships between entities.</p>
<p>PubTator 3.0 offers unified search results, simultaneously searching approximately 36 million PubMed abstracts and over 6 million full-text articles from the PMC Open Access Subset (PMCOA), improving access to the substantial amount of relevant information present in the article full text (6). Search results are prioritized based on the depth of the relationship between the query terms: articles containing identifiable relations between semantic terms receive the highest priority, while articles where semantic or keyword terms co-occur nearby (e.g., within the same sentence) receive secondary priority. Search results are also prioritized based on the article section where the match appears (e.g., matches within the title receive higher priority). Users can further refine results by employing filters, narrowing articles returned to specific publication types, journals, or article sections.</p>
<p>PubTator 3.0 is supported by an NLP pipeline, depicted in Fig. 2A. This pipeline, run weekly, first identifies articles newly added to PubMed and PMC-OA. Articles are then processed through three major steps: 1. named entity recognition, provided by the recently developed deeplearning transformer model AIONER (7), 2. identifier mapping, and 3. relation extraction, performed by BioREx (8) of 12 common types of relations (described in Supplementary Table 2).</p>
<p>In total, PubTator 3.0 contains over 1.6 billion entity annotations ( 4.6 million unique identifiers) and 33 million relations ( 8.8 million unique pairs). It provides enhanced entity recognition and normalization performance over its previous version, PubTator 2 (9), also known as PubTator Central (Fig. 2B and Supplementary Table 3). We show the relation extraction performance of PubTator 3.0 in Fig. 2C and its comparison results to the previous state-of-the-art systems (1012) on the BioCreative V Chemical-Disease Relation (13) corpus, finding that PubTator 3.0 provided substantially higher accuracy. Moreover, when evaluating a randomized sample of entity pair queries compared to PubMed and Google Scholar, PubTator 3.0 consistently returns a</p>
<p>greater number of articles with higher precision in the top 20 results (Fig. 2D and Supplementary Table 4).
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2. A. The PubTator 3.0 processing pipeline: AlONER (7) identifies six types of entities in PubMed abstracts and PMC-OA full-text articles. Entity annotations are associated with database identifiers by specialized mappers and BioREx (8) identifies relations between entities. Extracted data is stored in MongoDB and made searchable using Solr. B. Entity recognition performance for each entity type compared with PubTator2 (also known as PubTatorCentral) (13) on the BioRED corpus (14). C. Relation</p>
<p>extraction performance compared with SemRep (10) and notable previous best systems $(11,12)$ on the BioCreative V Chemical-Disease Relation (13) corpus. D. Comparison of information retrieval for PubTator 3.0, PubMed, and Google Scholar for entity pair queries, with respect to total article count and top-20 article precision.</p>
<h1>MATERIAL AND METHODS</h1>
<h2>Data Sources and Article Processing</h2>
<p>PubTator 3.0 downloads new articles weekly from the BioC PubMed API (https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PubMed/) and the BioC PMC API (https://www.ncbi.nlm.nih.gov/research/bionlp/APIs/BioC-PMC/) in BioC-XML format (15). Local abbreviations are identified using Ab3P (16). Article text and extracted data are stored internally using MongoDB and indexed for search with Solr, ensuring robust and scalable accessibility unconstrained by external dependencies such as the NCBI eUtils API.</p>
<h2>Entity Recognition and Normalization / Linking</h2>
<p>PubTator 3.0 uses AIONER (7), a recently developed named entity recognition (NER) model, to recognize entities of six types: genes/proteins, chemicals, diseases, species, genetic variants, and cell lines. AIONER utilizes a flexible tagging scheme to integrate training data created separately into a single resource. These training datasets include NLM-Gene (17), NLM-Chem (18), NCBIDisease (19), BC5CDR (13), tmVar3 (20), Species-800 (21), BioID (22), and BioRED (14). This consolidation creates a larger training set, improving the model's ability to generalize to unseen data. Furthermore, it enables recognizing multiple entity types simultaneously, enhancing efficiency and simplifying the challenge of distinguishing boundaries between entities that reference others, such as the disorder "Alpha-1 antitrypsin deficiency" and the protein "Alpha-1 antitrypsin." We previously evaluated the performance of AIONER on 14 benchmark datasets (7), including the test sets for the aforementioned training sets. This evaluation demonstrated that AIONER's performance surpasses or matches previous state-of-the-art methods.</p>
<p>Entity mentions found by AIONER are normalized (linked) to a unique identifier in an appropriate entity database. Normalization is performed by a module designed for (or adapted to) each entity type, using the latest version. The recently-upgraded GNorm2 system (23) normalizes genes to NCBI Gene identifiers and species mentions to NCBI Taxonomy. tmVar3 (20), also recently upgraded, normalizes genetic variants; it uses dbSNP identifiers for variants listed in dbSNP and HGNV format otherwise. Chemicals are normalized by the NLM-Chem tagger (18) to MeSH identifiers (24). TaggerOne (25) normalizes diseases to MeSH and cell lines to Cellosaurus (26) using an improved normalization-only mode. These enhancements provide a significant overall improvement in entity normalization performance (Supplementary Table 2).</p>
<h1>Relation Extraction</h1>
<p>Relations for PubTator 3.0 are extracted by the unified relation extraction model BioREx (8), designed to simultaneously extract 12 types of relations across eight entity type pairs: chemicalchemical, chemical-disease, chemical-gene, chemical-variant, disease-gene, disease-variant, gene-gene, and variant-variant. Detailed definitions of these relation types and their corresponding entity pairs are presented in Supplementary Table 2. Deep-learning methods for relation extraction, such as BioREx, require ample training data. However, training data for relation extraction is fragmented into many datasets, often tailored to specific entity pairs. BioREx overcomes this limitation with a data-centric approach, reconciling discrepancies between disparate training datasets to construct a comprehensive, unified dataset.</p>
<p>We evaluated the relations extracted by BioREx using performance on manually annotated relation extraction datasets as well as a comparative analysis between BioREx and notable comparable systems. BioREx established a new performance benchmark on the BioRED corpus test set (14), elevating the performance from $74.4 \%$ (F-score) to $79.6 \%$, and demonstrating higher performance than alternative models such as transfer learning (TL), multi-task learning (MTL), and state-of-the-art models trained on isolated datasets (8). For PubTator 3.0, we replaced its deep learning module, PubMedBERT (27), with LinkBERT (28), further increasing the performance to $82.0 \%$. Furthermore, we conducted a comparative analysis between BioREx and SemRep (10), a widely used rule-based method for extracting diverse relations, the CD-REST (12)</p>
<p>system, and the previous state-of-the-art system (11), using the BioCreative V Chemical Disease Relation corpus test set (13). Our evaluation demonstrated that PubTator 3.0 provided substantially higher F-score than previous methods.</p>
<h1>Programmatic Access and Data Formats</h1>
<p>PubTator 3.0 offers programmatic access through its API and bulk download. The API (https://www.ncbi.nlm.nih.gov/research/pubtator3/) supports keyword, entity and relation search, and also supports exporting annotations in XML and JSON-based BioC (15) formats and tabdelimited free text. The PubTator 3.0 FTP site (https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3) provides bulk downloads of annotated articles and extraction summaries for entities and relations. Programmatic access supports more flexible query options; for example, the information need "what chemicals reduce expression of JAK1?" can be answered directly via API (e.g., https://www.ncbi.nlm.nih.gov/research/pubtator3-
api/relations?e1=@GENE_JAK1\&amp;type=negative_correlate\&amp;e2=Chemical) or by filtering the bulk relations file. Additionally, the PubTator 3.0 API supports annotation of user-defined free text.</p>
<h2>Case Study I: Entity Relation Queries</h2>
<p>We analyzed the retrieval quality of PubTator 3.0 by preparing a series of 12 entity pairs to serve as case studies for comparison between PubTator 3.0, PubMed, and Google Scholar. To provide an equal comparison, we filtered Google Scholar results for articles not in PubMed. To ensure that the number of results would remain low enough to allow filtering Google Scholar results for articles not in PubMed, we identified entity pairs first discussed together in the literature in 2022 or later. We then randomly selected two entity pairs of each of the following types: Disease/Gene, Chemical/Disease, Chemical/Gene, Chemical/Chemical, Gene/Gene and Disease/Variant. The comparison was performed with respect to a snapshot of the search results returned by all search engines on May 19, 2023. We manually evaluated the top 20 results for each system and each query; articles were judged to be relevant if they mentioned both entities in the query and supported a relationship between them.</p>
<p>Our analysis is summarized in Fig. 2D, and Supplementary Table 4 presents a detailed comparison of the quality of retrieved results between PubTator 3.0, PubMed, and Google Scholar. Our results demonstrate that PubTator 3.0 retrieves a greater number of articles than the comparison systems and its precision is higher for the top 20 results. For instance, PubTator 3.0 returned 346 articles for the query "GLPG0634 + Ulcerative Colitis," and manual review of the top 20 articles showed that all contained statements about an association between GLPG0634 and ulcerative colitis. In contrast, PubMed only returned a total of 18 articles, with only 12 mentioning an association. Moreover, when searching for "COVID19 + PON1," PubTator 3.0 returns 212 articles in PubMed, surpassing the 43 articles obtained from Google Scholar, only 29 of which are sourced from PubMed. These disparities can be attributed to several factors: 1. PubTator 3.0's search includes full texts available in PMC-OA, resulting in significantly broader coverage of articles, 2. Entity normalization improves recall, for example, by matching "paraoxonase 1" to "PON1," 3. PubTator 3.0 prioritizes articles containing relations between the query entities, 4. Pubtator 3.0 prioritizes articles where the entities appear nearby, rather than distant paragraphs. Across the 12 information retrieval case studies, PubTator 3.0 demonstrated an overall precision of $90.0 \%$ for the top 20 articles ( 216 out of 240), which is significantly higher than PubMed's precision of $81.6 \%$ ( 84 out of 103) and Google Scholar's precision of $48.5 \%$ ( 98 out of 202).</p>
<h1>Case Study II: Retrieval-Augmented Generation</h1>
<p>In the era of large language models (LLMs), PubTator 3.0 can also enhance their factual accuracy via retrieval augmented generation. Despite their strong language ability, LLMs are prone to generating incorrect assertions, sometimes known as hallucinations $(29,30)$. For example, when requested to cite sources for questions such as "which diseases can doxorubicin treat," GPT-4 frequently provides seemingly plausible but nonexistent references. Augmenting GPT-4 with PubTator 3.0 APIs can anchor the model's response to verifiable references via the extracted relations, significantly reducing hallucinations.</p>
<p>We assessed the citation accuracy of responses from three GPT-4 variations: PubTatoraugmented GPT-4, PubMed-augmented GPT-4 and standard GPT-4. We performed a qualitative</p>
<p>evaluation based on eight questions selected as follows. We identified entities mentioned in the PubMed query logs and randomly selected from entities searched both frequently and rarely. We then identified the common queries for each entity that request relational information and adapted one into a natural language question. Each question is therefore grounded on common information needs of real PubMed users. For example, the questions "What can be caused by tocilizumab?" and "What can be treated by doxorubicin?" are adapted from the user queries "tocilizumab side effects" and "doxorubicin treatment" respectively. Such questions typically require extracting information from multiple articles and an understanding of biomedical entities and relationship descriptions. Supplementary Table 5 lists the questions chosen.</p>
<p>We augmented the GPT-4 large language model (LLM) with PubTator 3.0 via the function calling mechanism of the OpenAI ChatCompletion API. This integration involved prompting GPT-4 with descriptions of three PubTator APIs: 1. Find Entity ID, which retrieves PubTator entity identifiers; 2. Find Related Entities, which identifies related entities based on an input entity and specified relations; and 3. Export Relevant Search Results, which returns PubMed article identifiers containing textual evidence for specific entity relationships. Our instructions prompted GPT-4 to decompose user questions into sub-questions addressable by these APIs, execute the function calls, and synthesize the responses into a coherent final answer. Our prompt promoted a summarized response by instructing GPT-4 to start its message with "Summary:" and requested the response include citations to the articles providing evidence. The PubMed augmentation experiments provided GPT-4 with access to PubMed database search via the National Center for Biotechnology Information (NCBI) E-utils APIs (31). We used Azure OpenAI Services (version 2023-07-01-preview) and GPT-4 (version 2023-06-13) and set the decoding temperature to zero to obtain deterministic outputs. The full prompts are provided in Supplementary Table 6.</p>
<p>PubTator-augmented GPT-4 generally processed the questions in three steps: 1. finding the standard entity identifiers, 2. finding its related entity identifiers, and 3. searching PubMed articles. For example, to answer "What drugs can treat breast cancer?", GPT-4 first found the PubTator entity identifier for breast cancer (@DISEASE_Breast_Cancer) using the Find Entity ID API. It then used the Find Related Entities API to identify entities related to</p>
<p>@DISEASE_Breast_Cancer through a "treat" relation. For demonstration purposes, we limited the maximum number of output entities to five. Finally, GPT-4 called the Export Relevant Search Results API for the PubMed article identifiers containing evidence for these relationships. The raw responses to each prompt for each method are provided in Supplementary Table 6.</p>
<p>We manually evaluated the accuracy of the citations in the responses by reviewing each PubMed article and verifying whether each PubMed article cited supported the stated relationship (e.g., Tamoxifen treating breast cancer). Supplementary Table 5 reports the proportion of the cited articles with valid supporting evidence for each method. GPT-4 frequently generated fabricated citations, widely known as the hallucination issue. While PubMed-augmented GPT-4 showed a higher proportion of accurate citations, some articles cited did not support the relation claims. This is likely because PubMed is based on keyword and Boolean search and does not support queries for specific relationships. Responses generated by PubTator-augmented GPT-4 demonstrated the highest level of citation accuracy, underscoring the potential of PubTator 3.0 as a high-quality knowledge source for addressing biomedical information needs through retrieval-augmented generation with LLMs such as GPT-4.</p>
<h1>DISCUSSION</h1>
<p>Previous versions of PubTator have fulfilled over one billion API requests since 2015, supporting a wide range of research applications. Numerous studies have harnessed PubTator annotations for disease-specific gene research, including efforts to prioritize candidate genes (32), determine gene-phenotype associations (33), and identify the genetic underpinnings of disease comorbidities (34). Several projects have used PubTator to create gene and genetic variant resources $(35,36)$ or to enrich disease knowledge graphs $(37,38)$. Moreover, PubTator has supported biocuration efforts $(39,40)$ and the creation of NLP benchmarks (41). With enhanced accuracy, PubTator 3.0 will better support these use cases.</p>
<p>Introducing relation annotations to PubTator 3.0 opens novel avenues for expanded use scenarios. With relations precomputed from the literature, complex research questions can often</p>
<p>be answered directly. Drug repurposing, for example, can be formulated as identifying chemicals which target specific genes. Conversely, determining the genetic targets of a chemical can be achieved by querying the same chemical/gene relations. Clinicians evaluating genetic variants, e.g. for rare diseases or personalized medicine, may explore the relationships between specific genetic variants and disease. Biologists, on the other hand, may utilize interactions between multiple genes to assemble complex molecular pathways.</p>
<p>There are several notable limitations for PubTator 3.0. Although it is capable of extracting relations from full-text articles, this feature is currently restricted to abstracts due to computational constraints. However, the system has been designed to support full-text relation extraction in a future enhancement. The current system only extracts 12 relation types, though these represent common uses. Finally, entity annotation and relation extraction are automated; though these systems exhibit high performance, their accuracy remains imperfect.</p>
<h1>CONCLUSION</h1>
<p>PubTator 3.0 offers a comprehensive set of features and tools that allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery. The PubTator 3.0 interface, API, and bulk file downloads are available at https://www.ncbi.nlm.nih.gov/research/pubtator3/.</p>
<h2>DATA AVAILABILITY</h2>
<p>Data is available through the online interface at https://www.ncbi.nlm.nih.gov/research/pubtator3/, through the API at https://www.ncbi.nlm.nih.gov/research/pubtator3/api or bulk FTP download at https://ftp.ncbi.nlm.nih.gov/pub/lu/PubTator3/.</p>
<p>The source code for each component of PubTator 3.0 is openly accessible. The AIONER named entity recognizer is available at https://github.com/ncbi/AIONER. GNorm2, for gene name normalization, is available at https://github.com/ncbi/GNorm2. The tmVar3 variant name normalizer is available at https://github.com/ncbi/tmVar3. The NLM-Chem Tagger, for chemical name normalization, is available at https://ftp.ncbi.nlm.nih.gov/pub/lu/NLMChem. The TaggerOne system, for disease and cell line normalization, is available at https://www.ncbi.nlm.nih.gov/research/bionlp/Tools/taggerone. The BioREx relation extraction system is available at https://github.com/ncbi/BioREx. The code for customizing ChatGPT with the PubTator 3.0 API is available at https://github.com/ncbi-nlp/pubtator-gpt.</p>
<h1>FUNDING</h1>
<p>This research was supported by the Intramural Research Program of the National Library of Medicine (NLM), National Institutes of Health. Funding for open access charge: National Institutes of Health.</p>
<h2>CONFLICT OF INTEREST</h2>
<p>None declared.</p>
<h2>REFERENCES</h2>
<ol>
<li>Lindberg, D.A. and Humphreys, B.L. (2008) Rising expectations: access to biomedical information. Yearb. Med. Inform., 3, 165-172.</li>
<li>Jin, Q., Leaman, R. and Lu, Z. (2023) PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence. arXiv.</li>
<li>Rzhetsky, A., Seringhaus, M. and Gerstein, M. (2008) Seeking a new biology through text mining. Cell, 134, 9-13.</li>
<li>Mayers, M., Li, T.S., Queralt-Rosinach, N. and Su, A.I. (2019) Time-resolved evaluation of compound repositioning predictions on a text-mined knowledge network. BMC Bioinf., 20, 653.</li>
<li>Zhao, S., Su, C., Lu, Z. and Wang, F. (2021) Recent advances in biomedical literature mining. Brief Bioinform, 22.</li>
<li>
<p>Westergaard, D., Staerfeldt, H.H., Tonsberg, C., Jensen, L.J. and Brunak, S. (2018) A comprehensive and quantitative comparison of text-mining in 15 million full-text articles versus their corresponding abstracts. PLoS Comput. Biol., 14, e1005962.</p>
</li>
<li>
<p>Luo, L., Wei, C.-H., Lai, P.-T., Leaman, R., Chen, Q. and Lu, Z. (2023) AIONER: all-in-one schemebased biomedical named entity recognition using deep learning. Bioinformatics, 39.</p>
</li>
<li>Lai, P.T., Wei, C.H., Luo, L., Chen, Q. and Lu, Z. (2023) BioREx: Improving biomedical relation extraction by leveraging heterogeneous datasets. J. Biomed. Inf., 146, 104487.</li>
<li>Wei, C.-H., Allot, A., Leaman, R. and Lu, Z. (2019) PubTator central: automated concept annotation for biomedical full text articles. Nucleic Acids Res., 47, W587-W593.</li>
<li>Kilicoglu, H., Rosemblat, G., Fiszman, M. and Shin, D. (2020) Broad-coverage biomedical relation extraction with SemRep. BMC Bioinf., 21, 188.</li>
<li>Peng, Y., Wei, C.-H. and Lu, Z. (2016) Improving chemical disease relation extraction with rich features and weakly labeled data. J. Cheminf., 8, 1-12.</li>
<li>Xu, J., Wu, Y., Zhang, Y., Wang, J., Lee, H.J. and Xu, H. (2016) CD-REST: a system for extracting chemical-induced disease relation in literature. Database, 2016.</li>
<li>Li, J., Sun, Y., Johnson, R.J., Sciaky, D., Wei, C.-H., Leaman, R., Davis, A.P., Mattingly, C.J., Wiegers, T.C. and Lu, Z. (2016) BioCreative V CDR task corpus: a resource for chemical disease relation extraction. Database, 2016.</li>
<li>Luo, L., Lai, P.-T., Wei, C.-H., Arighi, C.N. and Lu, Z. (2022) BioRED: A Rich Biomedical Relation Extraction Dataset. Briefings Bioinf., 23, bbac282.</li>
<li>Comeau, D.C., Islamaj Doğan, R., Ciccarese, P., Cohen, K.B., Krallinger, M., Leitner, F., Lu, Z., Peng, Y., Rinaldi, F. and Torii, M.J.D. (2013) BioC: a minimalist approach to interoperability for biomedical text processing. Database, 2013, bat064.</li>
<li>Sohn, S., Comeau, D.C., Kim, W. and Wilbur, W.J. (2008) Abbreviation definition identification based on automatic precision estimates. BMC Bioinf., 9, 1-10.</li>
<li>Islamaj, R., Wei, C.-H., Cissel, D., Miliaras, N., Printseva, O., Rodionov, O., Sekiya, K., Ward, J. and Lu, Z.J.J.o.b.i. (2021) NLM-Gene, a richly annotated gold standard dataset for gene entities that addresses ambiguity and multi-species gene recognition. J. Biomed. Inf., 118, 103779.</li>
<li>Islamaj, R., Leaman, R., Kim, S., Kwon, D., Wei, C.-H., Comeau, D.C., Peng, Y., Cissel, D., Coss, C. and Fisher, C. (2021) NLM-Chem, a new resource for chemical entity recognition in PubMed full text literature. Scientific Data, 8, 91.</li>
<li>Doğan, R.I., Leaman, R. and Lu, Z. (2014) NCBI disease corpus: a resource for disease name recognition and concept normalization. J. Biomed. Inf., 47, 1-10.</li>
<li>Wei, C.-H., Allot, A., Riehle, K., Milosavljevic, A. and Lu, Z. (2022) tmVar 3.0: an improved variant concept recognition and normalization tool. Bioinformatics, 38, 4449-4451.</li>
<li>Pafilis, E., Frankild, S.P., Fanini, L., Faulwetter, S., Pavloudi, C., Vasileiadou, A., Arvanitidis, C. and Jensen, L.J. (2013) The SPECIES and ORGANISMS resources for fast and accurate identification of taxonomic names in text. PLoS One, 8, e65390.</li>
<li>Arighi, C., Hirschman, L., Lemberger, T., Bayer, S., Liechti, R., Comeau, D. and Wu, C. (2017), Proc. BioCreative Workshop, Vol. 482, pp. 376.</li>
<li>Wei, C.H., Luo, L., Islamaj, R., Lai, P.T. and Lu, Z. (2023) GNorm2: an improved gene name recognition and normalization system. Bioinformatics, 39.</li>
<li>Lipscomb, C.E. (2000) Medical subject headings (MeSH). Bull. Med. Libr. Assoc., 88, 265.</li>
<li>Leaman, R. and Lu, Z. (2016) TaggerOne: joint named entity recognition and normalization with semi-Markov Models. Bioinformatics, 32, 2839-2846.</li>
<li>Bairoch, A. (2018) The Cellosaurus, a Cell-Line Knowledge Resource. J. Biomol Tech., 29, 25-38.</li>
<li>Gu, Y., Tinn, R., Cheng, H., Lucas, M., Usuyama, N., Liu, X., Naumann, T., Gao, J. and Poon, H. (2021) Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare, 3, 1-23.</li>
<li>
<p>Yasunaga, M., Leskovec, J. and Liang, P. (2022), Association for Computational Linguistics, pp. 8003-8016.</p>
</li>
<li>
<p>Jin, Q., Leaman, R. and Lu, Z. (2023) Retrieve, Summarize, and Verify: How Will ChatGPT Affect Information Seeking from the Medical Literature? J. Am. Soc. Nephrol., 34, 1302-1304.</p>
</li>
<li>Tian, S., Jin, Q., Yeganova, L., Lai, P.T., Zhu, Q., Chen, X., Yang, Y., Chen, Q., Kim, W., Comeau, D.C. et al. (2023) Opportunities and challenges for ChatGPT and large language models in biomedicine and health. Brief Bioinform, 25.</li>
<li>National Center for Biotechnology Information (US). (2010) Entrez Programming Utilities Help. National Center for Biotechnology Information (US), Bethesda (MD).</li>
<li>Lieberwirth, J.K., Buttner, B., Klockner, C., Platzer, K., Popp, B. and Abou Jamra, R. (2022) AutoCaSc: Prioritizing candidate genes for neurodevelopmental disorders. Hum. Mutat., 43, 1795-1807.</li>
<li>Buch, A.M., Vertes, P.E., Seidlitz, J., Kim, S.H., Grosenick, L. and Liston, C. (2023) Molecular and network-level mechanisms explaining individual differences in autism spectrum disorder. Nat. Neurosci., 26, 650-663.</li>
<li>Pinto, B.G.G., Oliveira, A.E.R., Singh, Y., Jimenez, L., Goncalves, A.N.A., Ogava, R.L.T., Creighton, R., Schatzmann Peron, J.P. and Nakaya, H.I. (2020) ACE2 Expression Is Increased in the Lungs of Patients With Comorbidities Associated With Severe COVID-19. J. Infect. Dis., 222, 556-563.</li>
<li>Mitsuhashi, N., Toyo-Oka, L., Katayama, T., Kawashima, M., Kawashima, S., Miyazaki, K. and Takagi, T. (2022) TogoVar: A comprehensive Japanese genetic variation database. Hum. Genome Var., 9, 44.</li>
<li>Jiang, J., Yuan, J., Hu, Z., Zhang, Y., Zhang, T., Xu, M., Long, M., Fan, Y., Tanyi, J.L., Montone, K.T. et al. (2022) Systematic illumination of druggable genes in cancer genomes. Cell Rep., 38, 110400 .</li>
<li>Pu, Y., Beck, D. and Verspoor, K. (2023) Graph embedding-based link prediction for literaturebased discovery in Alzheimer's Disease. J. Biomed. Inf., 145, 104464.</li>
<li>Chen, C., Ross, K.E., Gavali, S., Cowart, J.E. and Wu, C.H. (2021) COVID-19 Knowledge Graph from semantic integration of biomedical literature and databases. Bioinformatics, 37, 4597-4598.</li>
<li>Lou, P., Jimeno Yepes, A., Zhang, Z., Zheng, Q., Zhang, X. and Li, C. (2020) BioNorm: deep learning-based event normalization for the curation of reaction databases. Bioinformatics, 36, 611-620.</li>
<li>Percha, B. and Altman, R.B. (2018) A global network of biomedical relationships derived from text. Bioinformatics, 34, 2614-2624.</li>
<li>Legrand, J., Gogdemir, R., Bousquet, C., Dalleau, K., Devignes, M.-D., Digan, W., Lee, C.-J., Ndiaye, N.-C., Petitpain, N. and Ringot, P. (2020) PGxCorpus, a manually annotated corpus for pharmacogenomics. Scientific Data, 7, 3.</li>
<li>Luo, L., Lai, P.-T., Wei, C.-H., Arighi, C.N. and Lu, Z. (2022) BioRED: A Rich Biomedical Relation Extraction Dataset. Briefings in Bioinformatics.</li>
<li>Wei, C.-H., Kao, H.-Y. and Lu, Z. (2015) GNormPlus: an integrative approach for tagging genes, gene families, and protein domains. BioMed research international, 2015, 918710.</li>
<li>Wei, C.-H., Luo, L., Islamaj, R., Lai, P.-T. and Lu, Z. (2023) GNorm2: an improved gene name recognition and normalization system. Bioinformatics, in press.</li>
<li>Wei, C.-H., Kao, H.-Y. and Lu, Z. (2012) SR4GN: a species recognition software tool for gene normalization. PloS one, 7, e38460.</li>
<li>Wei, C.-H., Phan, L., Feltz, J., Maiti, R., Hefferon, T. and Lu, Z. (2018) tmVar 2.0: integrating genomic variant information from literature with dbSNP and ClinVar for precision medicine. Bioinformatics, 34, 80-87.</li>
</ol>
<p>Supplementary Table 1. Feature comparison between PubTator 3.0 and its previous version, PubTator 2 (also known as PubTator Central).</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Feature</th>
<th style="text-align: left;">PubTator 2</th>
<th style="text-align: left;">PubTator 3.0</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Entity annotations</td>
<td style="text-align: left;">Genes, diseases, chemicals, <br> genetic variants, species, <br> and cell lines in abstracts <br> and full text</td>
<td style="text-align: left;">Same types and scope; higher accuracy</td>
</tr>
<tr>
<td style="text-align: left;">Relation annotations</td>
<td style="text-align: left;">None</td>
<td style="text-align: left;">12 relation types across eight entity type pairs; <br> scope: abstracts only</td>
</tr>
<tr>
<td style="text-align: left;">Search scope</td>
<td style="text-align: left;">Abstracts only, via NCBI <br> eUtils</td>
<td style="text-align: left;">Unified search in abstracts \&amp; full text via <br> Apache Solr, no external dependencies</td>
</tr>
<tr>
<td style="text-align: left;">Query types</td>
<td style="text-align: left;">Keyword, Boolean</td>
<td style="text-align: left;">Also: semantic, relation</td>
</tr>
<tr>
<td style="text-align: left;">Search support</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Semantic autocomplete, facet filters (section, <br> journal, article type)</td>
</tr>
<tr>
<td style="text-align: left;">Retrieval relevance</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Results prioritized by entity relationships, co- <br> occurrence \&amp; matching sections; highlighted <br> snippets (explains relevance); temporal <br> visualization</td>
</tr>
<tr>
<td style="text-align: left;">Literature management</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">User-defined collections</td>
</tr>
<tr>
<td style="text-align: left;">API</td>
<td style="text-align: left;">Retrieve articles and <br> annotations by PMID</td>
<td style="text-align: left;">Also: query relevant articles (semantic, <br> relation, keyword, Boolean), and query related <br> entities</td>
</tr>
<tr>
<td style="text-align: left;">Advanced natural- <br> language search</td>
<td style="text-align: left;">N/A</td>
<td style="text-align: left;">Retrieval-augmented generation with GPT-4 <br> large language model</td>
</tr>
</tbody>
</table>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Supplementary Figure 1. PubTator 3.0 article display page. (1) List of entities and relations identified by PubTator 3.0, providing a quick content overview. (2) Extracted entities highlighted in article text. (3) Display highlighting for query entities or all entities; display article abstract or full text. (4) Add article to custom collection for convenient access later.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">PubTator 3.0 Relations</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Entity types</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">ASSOCIATE</td>
<td style="text-align: left;">Complex or unclear relationships</td>
<td style="text-align: left;">Chemical / Disease <br> Chemical / Gene <br> Chemical / Variant <br> Disease / Gene <br> Disease / Variant <br> Variant / Variant</td>
</tr>
<tr>
<td style="text-align: left;">CAUSE</td>
<td style="text-align: left;">Triggering a disease by a specific agent</td>
<td style="text-align: left;">Chemical / Disease <br> Variant / Disease</td>
</tr>
<tr>
<td style="text-align: left;">COMPARE</td>
<td style="text-align: left;">Comparing the effects of two chemicals or drugs</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">COTREAT</td>
<td style="text-align: left;">Simultaneous administration of multiple drugs</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">DRUG_INTERACT</td>
<td style="text-align: left;">Pharmacodynamic interactions between two <br> chemicals</td>
<td style="text-align: left;">Chemical / Chemical</td>
</tr>
<tr>
<td style="text-align: left;">INHIBIT</td>
<td style="text-align: left;">Reduction in amount or degree of one entity by <br> another</td>
<td style="text-align: left;">Chemical / Variant <br> Gene / Disease</td>
</tr>
<tr>
<td style="text-align: left;">INTERACT</td>
<td style="text-align: left;">Physical interactions, such as protein-binding</td>
<td style="text-align: left;">Chemical / Gene <br> Chemical / Variant <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">NEGATIVE_CORRELATE</td>
<td style="text-align: left;">Increases in the amount or degree of one entity <br> decreases the amount or degree of the other entity</td>
<td style="text-align: left;">Chemical / Gene <br> Chemical / Variant <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">POSITIVE_CORRELATE</td>
<td style="text-align: left;">The amount or degree of two entities increase or <br> decrease together</td>
<td style="text-align: left;">Chemical / Chemical <br> Chemical / Gene <br> Gene / Gene</td>
</tr>
<tr>
<td style="text-align: left;">PREVENT</td>
<td style="text-align: left;">Prevention of a disease by a genetic variant</td>
<td style="text-align: left;">Variant / Disease</td>
</tr>
<tr>
<td style="text-align: left;">STIMULATE</td>
<td style="text-align: left;">Increase in amount or degree of one entity by <br> another</td>
<td style="text-align: left;">Chemical / Variant <br> Gene / Disease</td>
</tr>
<tr>
<td style="text-align: left;">TREAT</td>
<td style="text-align: left;">Treatment of a disease using a chemical or drug</td>
<td style="text-align: left;">Chemical / Disease</td>
</tr>
</tbody>
</table>
<h1>Supplementary Table 3. Normalization performance enhancements from PubTator Central to PubTator 3.0. Measurements reflect document-level normalization performance on the BioRED test set (42).</h1>
<table>
<thead>
<tr>
<th></th>
<th>PubTator Central (2.0)</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th>PubTator 3.0</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>NER/Norm</td>
<td>Precision</td>
<td>Recall</td>
<td>F-score</td>
<td>NER</td>
<td>Norm</td>
<td>Precision</td>
<td>Recall</td>
<td>F-score</td>
</tr>
<tr>
<td>Gene</td>
<td>GNormPlus (43)</td>
<td>86.92%</td>
<td>73.00%</td>
<td>79.35%</td>
<td>AIONER (7)</td>
<td>GNorm2 (44)</td>
<td>90.60%</td>
<td>79.41%</td>
<td>84.63%</td>
</tr>
<tr>
<td>Disease</td>
<td>TaggerOne (25)</td>
<td>77.13%</td>
<td>76.45%</td>
<td>76.79%</td>
<td></td>
<td>TaggerOne (25)</td>
<td>75.33%</td>
<td>83.43%</td>
<td>79.17%</td>
</tr>
<tr>
<td>Chemical</td>
<td>TaggerOne (25)</td>
<td>73.42%</td>
<td>78.38%</td>
<td>75.82%</td>
<td></td>
<td>NLM-Chem (18)</td>
<td>83.26%</td>
<td>80.63%</td>
<td>81.92%</td>
</tr>
<tr>
<td>Species</td>
<td>SR4GN (45)</td>
<td>94.69%</td>
<td>94.69%</td>
<td>94.69%</td>
<td></td>
<td>GNorm2 (44)</td>
<td>93.97%</td>
<td>96.46%</td>
<td>95.20%</td>
</tr>
<tr>
<td>CellLine</td>
<td>TaggerOne (25)</td>
<td>42.42%</td>
<td>63.64%</td>
<td>50.91%</td>
<td></td>
<td>TaggerOne (25)</td>
<td>76.00%</td>
<td>86.36%</td>
<td>80.85%</td>
</tr>
<tr>
<td>Variant</td>
<td>tmVar2 (46)</td>
<td>94.92%</td>
<td>84.85%</td>
<td>89.60%</td>
<td></td>
<td>tmVar3 (20)</td>
<td>98.48%</td>
<td>98.48%</td>
<td>98.48%</td>
</tr>
<tr>
<td>Micro-average</td>
<td></td>
<td>77.30%</td>
<td>77.49%</td>
<td>77.40%</td>
<td></td>
<td></td>
<td>84.04%</td>
<td>83.55%</td>
<td>83.80%</td>
</tr>
<tr>
<td>Macro-average</td>
<td></td>
<td>78.25%</td>
<td>78.50%</td>
<td>77.86%</td>
<td></td>
<td></td>
<td>86.27%</td>
<td>87.46%</td>
<td>86.71%</td>
</tr>
</tbody>
</table>
<p>Supplementary Table 4. Comparison of PubTator 3.0, PubMed, and Google Scholar search results for various recently discussed relation pairs. D: Disease, G: Gene, C: Chemical, and V: Variant. The '#' column lists the number of results; for Google Scholar the number in parentheses indicates the number of articles that appear in PubMed. The 'Top 20' column indicates the number of articles in the top 20 results which discuss a relation between the specified entities; some queries return fewer than 20 articles.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Entity <br> pair</th>
<th style="text-align: center;">Entities</th>
<th style="text-align: center;">PubTator 3.0</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">PubMed</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Google Scholar</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Top20</td>
<td style="text-align: center;">#</td>
<td style="text-align: center;">Top20</td>
<td style="text-align: center;"># (in <br> PubMed)</td>
<td style="text-align: center;">Top20</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;$ D,G&gt;</td>
<td style="text-align: center;">COVID19 + PON1</td>
<td style="text-align: center;">212</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">$9 / 11$</td>
<td style="text-align: center;">43 (29)</td>
<td style="text-align: center;">$10 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coronary Artery <br> Disease + SESN2</td>
<td style="text-align: center;">61</td>
<td style="text-align: center;">$17 / 20$</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">$3 / 3$</td>
<td style="text-align: center;">151 (104)</td>
<td style="text-align: center;">$14 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{D}&gt;$</td>
<td style="text-align: center;">GLPG0634 + Ulcerative <br> Colitis</td>
<td style="text-align: center;">346</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">$12 / 18$</td>
<td style="text-align: center;">362 (281)</td>
<td style="text-align: center;">$8 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">brolucizumab <br> Glycogen <br> Storage <br> Disease Type II</td>
<td style="text-align: center;">96</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$0 / 0$</td>
<td style="text-align: center;">1 (1)</td>
<td style="text-align: center;">$0 / 0$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{G}&gt;$</td>
<td style="text-align: center;">Gallium-68 + FAP alpha</td>
<td style="text-align: center;">261</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$2 / 2$</td>
<td style="text-align: center;">11 (4)</td>
<td style="text-align: center;">$9 / 11$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Lipopolysaccharides + <br> PVT1</td>
<td style="text-align: center;">321</td>
<td style="text-align: center;">$19 / 20$</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">$18 / 20$</td>
<td style="text-align: center;">128 (95)</td>
<td style="text-align: center;">$6 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{C}, \mathrm{C}&gt;$</td>
<td style="text-align: center;">N -dimethylnitrosamine <br> + Metformin</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">$1 / 1$</td>
<td style="text-align: center;">11 (8)</td>
<td style="text-align: center;">$1 / 11$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2'-fucosyllactose <br> Volatile fatty acids</td>
<td style="text-align: center;">284</td>
<td style="text-align: center;">$17 / 20$</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">$3 / 6$</td>
<td style="text-align: center;">71 (40)</td>
<td style="text-align: center;">$6 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{G}, \mathrm{G}&gt;$</td>
<td style="text-align: center;">interleukin 17 + cell <br> division cycle 42</td>
<td style="text-align: center;">599</td>
<td style="text-align: center;">$12 / 20$</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">$1 / 2$</td>
<td style="text-align: center;">85 (39)</td>
<td style="text-align: center;">$1 / 20$</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">HAVCR2 + TOX</td>
<td style="text-align: center;">615</td>
<td style="text-align: center;">$11 / 20$</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">$1 / 4$</td>
<td style="text-align: center;">701 (479)</td>
<td style="text-align: center;">$3 / 20$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;\mathrm{D}, \mathrm{V}&gt;$</td>
<td style="text-align: center;">COVID19 + rs12329760</td>
<td style="text-align: center;">149</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">32</td>
<td style="text-align: center;">$19 / 20$</td>
<td style="text-align: center;">87 (63)</td>
<td style="text-align: center;">20/20</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">COVID19 + rs4646994</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">20/20</td>
<td style="text-align: center;">16</td>
<td style="text-align: center;">$15 / 16$</td>
<td style="text-align: center;">30 (24)</td>
<td style="text-align: center;">20/20</td>
</tr>
</tbody>
</table>
<p>Supplementary Table 5. Article citation precision for all 8 queries tested using GPT-4 only, GPT-4 augmented with PubMed, and GPT-4 augmented with PubTator 3.0. Results are summarized as "number of articles correctly referenced / total number of articles referenced." Full responses provided in Supplemental Data.</p>
<table>
<thead>
<tr>
<th>Questions</th>
<th>GPT4 <br> Only</th>
<th>GPT4 with PubMed <br> Augmentation</th>
<th>GPT4 with PubTator <br> Augmentation</th>
</tr>
</thead>
<tbody>
<tr>
<td>What can be caused by tocilizumab? For <br> each disease in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$0 / 1$</td>
<td>$1 / 5$</td>
<td>$49 / 50$</td>
</tr>
<tr>
<td>Can you tell me what the causes of memory <br> deficits are? For each cause in your answer, <br> please cite and summarized the article <br> PMIDs that contain the evidence.</td>
<td>$0 / 5$</td>
<td>$4 / 5$</td>
<td>$15 / 15$</td>
</tr>
<tr>
<td>In what situations can cocaine be used? For <br> each situation in your answer, please cite <br> and summarize the article PMIDs that <br> contain the evidence.</td>
<td>$0 / 3$</td>
<td>$1 / 3$</td>
<td>$20 / 25$</td>
</tr>
<tr>
<td>What can be treated by doxorubicin? For <br> each disease in your answer, please cite and <br> summarize the article PMIDs that contain <br> the evidence.</td>
<td>$0 / 7$</td>
<td>$3 / 5$</td>
<td>$45 / 45$</td>
</tr>
<tr>
<td>Are there any genes that interact with <br> cocaine? For each drug in your answer, <br> please cite the article PMIDs that contain <br> the evidence.</td>
<td>$0 / 5$</td>
<td>$5 / 5$</td>
<td>$16 / 17$</td>
</tr>
<tr>
<td>What drugs can treat breast cancer? For <br> each drug in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$0 / 6$</td>
<td>$3 / 4$</td>
<td>$45 / 45$</td>
</tr>
<tr>
<td>What drugs can treat Scleroderma? For each <br> drug in your answer, please cite the article <br> PMIDs that contain the evidence.</td>
<td>$0 / 6$</td>
<td>$1 / 2$</td>
<td>$50 / 50$</td>
</tr>
<tr>
<td>What can be treated by finasteride? For <br> each disease in your answer, please cite the <br> article PMIDs that contain the evidence.</td>
<td>$2 / 3$</td>
<td>$4 / 5$</td>
<td>$39 / 45$</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ Joint Authors&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3322 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3322</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3322</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-07955e96cbd778d0ae2a68f09d073b866dd84c2a</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/07955e96cbd778d0ae2a68f09d073b866dd84c2a" target="_blank">Decomposed Prompting: A Modular Approach for Solving Complex Tasks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> It is shown that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3 and to incorporate a symbolic information retrieval within the decomposition framework, leading to improved performance on both tasks.</p>
                <p><strong>Paper Abstract:</strong> Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA task, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can incorporate a symbolic information retrieval within our decomposition framework, leading to improved performance on both tasks. Datasets, Code and Prompts available at https://github.com/allenai/DecomP.</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3322.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3322.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECOMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposed Prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular few-shot prompting paradigm that decomposes a complex task into a program of simpler sub-queries dispatched to dedicated sub-task handlers (separate LLM prompts, recursive decompositions, or symbolic modules such as retrieval), enabling targeted teaching, debugging, replacement, recursion and integration of non-neural tools.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various LLMs (text-davinci-002, davinci-001, code-davinci-002, Flan-T5 family)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>The paper experiments with OpenAI GPT-3 variants (text-davinci-002, davinci-001), Codex (code-davinci-002) for long contexts, and Flan-T5 models (Large/XL/XXL) as sub-task handlers or for decomposition; a symbolic controller orchestrates calls between the decomposer and sub-task handlers and can call external retrieval (Elasticsearch) or symbolic functions.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['modular decomposition (DECOMP)', 'hierarchical decomposition', 'recursive decomposition', 'retrieval-augmented decomposition (Elasticsearch retrieve_odqa module)', 'post-processing answer extraction (CoT post-processing)', 'Chain-of-Thought (used as baseline or submodule)', 'Least-to-most / successive prompting (baselines / related)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DECOMP: a decomposer prompt generates a sequence of sub-queries and assigns each to a sub-task handler (handler can be another prompt, a further decomposed program, or a symbolic API like Elasticsearch). Hierarchical decomposition: break a difficult sub-task into simpler sub-tasks (e.g., str_pos -> split + arr_pos). Recursive decomposition: decomposer can call itself to handle inputs by dividing into smaller instances (e.g., reverse list via splitting). Retrieval-augmented: sub-task handler performs document retrieval + RC. Post-processing: use an extra handler to robustly extract answers from CoT output instead of brittle regex.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse set — DECOMP intentionally composes heterogeneous reasoning styles (specialized CoT prompts for sub-tasks, purely symbolic retrieval, recursive calls, and dedicated answer-extraction modules) rather than relying on a single unified chain-of-thought style; the paper shows this diversity is implemented by prompting different handlers or replacing handlers with symbolic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Multiple (k-th letter concatenation; list reversal; long-context multi-hop QA (CommaQA-E); open-domain multi-hop QA (2WikiMultihopQA, MuSiQue, HotpotQA); Math QA (GSM8K, MultiArith))</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A set of synthetic and real benchmarks covering symbolic string/list manipulation tasks (character extraction and concatenation, list reversal), long-context reading comprehension (CommaQA-E), open-domain multi-hop question answering (converted versions of 2WikiMultihopQA, MuSiQue, HotpotQA), and arithmetic/math word problems (GSM8K, MultiArith).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Aggregated summary as reported: DECOMP outperforms baseline CoT and least-to-most prompting across tasks. Example highlights: on k-th letter concatenation (k=3) DECOMP achieves nearly 100% EM and remains stable with increasing input length while CoT and least-to-most drop noticeably; recursive DECOMP enables strong length generalization on list reversal where CoT fails; on CommaQA-E DECOMP variants (coarse and fine-grained) produce higher EM than CoT (figure-level improvements reported); on open-domain multi-hop QA, Decomp-Ctxt (DECOMP + retrieval) significantly outperforms No-Ctxt (closed-book) and a strong retrieval baseline NoDecomp-Ctxt for most settings (Answer F1 improvements shown in paper figures), with one exception where Codex on HotpotQA was comparable; on Math QA, using DECOMP post-processing for CoT answer extraction yields large gains (GSM8K: 36 -> 50.6, +~14.6 points; MultiArith: 78 -> 95, +17 points).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>The paper directly compares DECOMP to Chain-of-Thought (CoT), a rolled-out CoT (single long CoT describing full decomposition), and least-to-most prompting. Across experiments DECOMP provides higher accuracy and better generalization (length/compositional generalization). DECOMP's separate sub-task prompts enable teaching sub-tasks more effectively than a single rolled-out CoT even when the CoT encodes the same decomposition steps. DECOMP also demonstrates that replacing sub-tasks with symbolic retrieval improves open-domain QA compared to retrieval baselines that do not decompose questions.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Modular, heterogeneous decomposition (DECOMP) yields higher accuracy and better generalization than uniform chain-of-thought style prompting on a variety of reasoning tasks. 2) Teaching sub-tasks with their own prompts (or symbolic handlers) is more effective than embedding all intermediate steps in one CoT prompt. 3) Recursive decomposition enables length generalization for operations like list reversal, allowing weaker models to handle longer inputs. 4) Integrating symbolic retrieval as a handler improves open-domain multi-hop QA relative to monolithic retrieval+QA baselines. 5) Targeted post-processing handlers can correct systematic CoT extraction errors, producing large gains on math datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Exceptions and limitations: on HotpotQA using Codex, Decomp-Ctxt is roughly comparable (not strictly better) to the strong retrieval baseline; overall gains diminish with weaker/smaller models (e.g., curie-001 performance drops toward zero for some tasks and DECOMP's advantage vanishes when base model capability is very low). Errors that remain in DECOMP are mainly due to sub-task execution failures (incorrect extraction/formatting), i.e., DECOMP shifts errors to sub-task handlers rather than eliminating them entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3322.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3322.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that elicits intermediate reasoning steps in the model's output (a sequence of 'thoughts') to improve multi-step reasoning performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (used as CoT baseline in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An InstructGPT3 variant (text-davinci-002) used to produce chain-of-thought explanations as few-shot examples; used both as stand-alone CoT and as part of decomposition baselines (rolled-out CoT).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought (standard)', 'CoT with rollout (rolled-out long chain describing full decomposition)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>CoT: few-shot examples include intermediate reasoning steps (thoughts) which the model reproduces to arrive at the answer. Rolled-out CoT: the entire decomposition procedure is described inline as a long chain-of-thought rather than modular sub-prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>single / similar style — CoT uses the same chain-of-thought style across the whole task rather than modularly combining different handler types.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Used as baseline across tasks (k-th letter concatenation, list reversal, CommaQA-E, open-domain QA baselines, Math QA)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>CoT applied as few-shot prompts explaining intermediate steps for multi-step symbolic and textual reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported or implied baseline performance: On k-th letter concatenation and longer inputs, CoT accuracy drops noticeably compared to DECOMP (figures show substantial gaps). On list reversal CoT fails to generalize to longer sequences; on CommaQA CoT achieves lower EM than DECOMP variants. On math QA reported CoT baseline scores (prior to DECOMP post-processing) were: MultiArith 78 EM and GSM8K 36 EM (paper shows these are improved by DECOMP post-processing to 95 and 50.6 respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>CoT is the primary baseline. DECOMP consistently outperforms CoT on symbolic tasks, QA, and generalization experiments. Rolled-out CoT that encodes the same decomposition as DECOMP still underperforms, indicating limits of teaching sub-tasks within a single monolithic CoT prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>CoT is effective for eliciting reasoning but struggles when component reasoning steps are themselves hard to learn from a few demonstrations embedded in complex examples; modular decomposition that exposes sub-tasks directly leads to better learning of those sub-tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Rolled-out CoT can become too long and convoluted and fails to generalize (e.g., length generalization for reversal). CoT's standard answer-extraction heuristics (regex 'answer is . *') are brittle and can be fixed by DECOMP post-processing; CoT makes more frequent sub-task errors compared to DECOMP.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3322.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3322.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Least-to-most</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Least-to-most prompting</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting strategy that decomposes a problem by generating and answering subproblems in order from easiest to hardest, using one LLM to generate subquestions and another to answer them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Least-to-most prompting enables complex reasoning in large language models.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002 (used as a baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Adapted least-to-most prompting applied as a baseline, including a 'rolled out' variant for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['least-to-most prompting (ordered subquestion generation and answering)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate subquestions starting from easiest, progressively working to harder ones; in the paper this baseline is adapted and compared to DECOMP and rolled-out CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>similar style — sequential single-model decomposition (less modular than DECOMP because it relies on one model to both generate and answer subquestions and enforces an 'easy-to-hard' ordering).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>k-th letter concatenation and other symbolic tasks (as baseline comparisons)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Applied as a baseline for symbolic string tasks and compared against DECOMP and CoT variants.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>DECOMP outperforms least-to-most prompting on the k-th letter concatenation task (figure-level results indicate a noticeable gap; exact numeric values not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>DECOMP's flexibility (nonlinear decompositions, modular handlers, recursion) outperforms least-to-most which requires one-shot identification of subproblems in an easy-to-hard ordering; least-to-most variants did not match DECOMP performance in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Least-to-most helps structured decomposition but is less flexible than DECOMP and underperforms in the studied symbolic tasks when compared directly to modular decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Least-to-most is by definition ordered and requires the model eventually answer the full question; DECOMP imposes no such restriction and can achieve better results by assigning different handlers per subtask.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3322.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3322.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Decomp-Ctxt (Retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Decomposed Prompting with Elasticsearch retrieval (Decomp-Ctxt)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A DECOMP configuration that assigns retrieval sub-tasks to an external Elasticsearch-based retrieve_odqa handler, which returns documents and single-hop answers; used to solve open-domain multi-hop QA by composing retrieval and RC modules under a decomposer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>code-davinci-002 (Codex) and Flan-T5 family (Large/XL/XXL)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Codex (code-davinci-002) used for decomposition in long-context settings; Flan-T5-Large (0.7B), Flan-T5-XL (3B), Flan-T5-XXL (11B) were evaluated to test scale effects; retrieval handled by Elasticsearch index and retrieved paragraphs passed to RC prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Flan-T5 sizes: Large: 0.7B, XL: 3B, XXL: 11B (as given in paper); Codex size not specified in paper</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['decomposed prompting + symbolic retrieval (Elasticsearch) + RC', 'multi-hop decomposition into single-hop retrievals']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Decomposer produces single-hop subquestions; retrieve_odqa handler queries Elasticsearch to return titles and passages and an RC answer; subsequent sub-questions can use retrieved answers and documents; final multihop_rcqa model answers using the aggregated context.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>diverse — combines LLM prompting for decomposition with a symbolic retrieval API and separate RC prompts, i.e., heterogeneous modules.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Open-domain multi-hop QA (2WikiMultihopQA, MuSiQue, HotpotQA converted to open-domain)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Multi-hop question answering where supporting evidence must be retrieved from a large paragraph corpus; evaluation on 300 dev questions per dataset in open-domain setting.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Decomp-Ctxt (DECOMP + retrieval) significantly outperforms No-Ctxt (closed-book) and NoDecomp-Ctxt (single-step retrieval + QA) across datasets and model choices (figures in paper report substantial Answer F1 gains). Exception: with Codex on HotpotQA the Decomp-Ctxt system is comparable (not a clear win). The paper also shows that Flan-T5-XXL with DECOMP can approach Codex performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Paper compares: (A) No-Ctxt (closed-book), (B) NoDecomp-Ctxt (retrieve K paragraphs for full question) and (C) Decomp-Ctxt (DECOMP + retrieval). Decomp-Ctxt beats both alternatives in most settings; K (number of paragraphs retrieved) was separately tuned for NoDecomp-Ctxt and Decomp-Ctxt.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Decomposition that delegates retrieval to a symbolic retriever and restricts retrieval per subquestion improves open-domain multi-hop QA performance over monolithic retrieval+QA baselines; even smaller LMs (Flan-T5-XXL) benefit substantially from DECOMP and can rival larger Codex results.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Codex on HotpotQA: Decomp-Ctxt is only comparable to strong baselines, not strictly better. Performance is sensitive to retrieval hyperparameters (number of paragraphs K) and the quality of retrieved passages; DECOMP does not eliminate retrieval errors.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3322.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3322.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>k-th-letter-exp</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>k-th letter concatenation experiment</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Symbolic string manipulation task: extract the k-th character from each word in an input string and concatenate them using a delimiter; used to probe sub-task learning and length generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>text-davinci-002</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>InstructGPT3 (text-davinci-002) used as the base LLM for decomposer and as baseline for CoT and least-to-most prompting in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['DECOMP with hierarchical decomposition (split, str_pos further decomposed, merge)', 'Chain-of-Thought (CoT)', 'CoT rolled-out (single prompt describing full decomposition)', 'Least-to-most (adapted baseline)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>DECOMP: decomposer delegates to split (word tokenization), str_pos (find character at position) which itself is decomposed into split characters + arr_pos, and merge; CoT variants attempt to encode the same steps inside a single chain-of-thought prompt; least-to-most variant adapted similarly.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>DECOMP uses diverse sub-task-specific handlers allowing specialized training examples per subtask; CoT and least-to-most use a single unified chain-style prompt (similar style).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>k-th letter concatenation (synthetic symbolic task; k tested at 3 and other positions; inputs of varying number of words)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given a string of N words, return the k-th character of each word concatenated with a delimiter; evaluated on generalization to unseen k, longer input lengths, and new delimiters.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>DECOMP: close to 100% EM across increasing input lengths (paper notes 'near 100% accuracy' and zero std in aggregated metrics). CoT and least-to-most: substantially lower and degrade as number of words increases (figures show clear drop but exact numeric values not tabulated in text).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Direct comparison shows DECOMP outperforms CoT and least-to-most even when CoT encodes the same decomposition (rolled-out CoT), indicating modular sub-task prompts are more effective at teaching difficult sub-steps (e.g., finding k-th character).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Factoring the task into sub-tasks and teaching each separately yields strong generalization and near-perfect performance; monolithic CoT struggles to learn some sub-steps from a few demonstrations embedded in complex examples.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>DECOMP errors observed are attributable to sub-task execution errors (e.g., incorrect character extraction or incorrect concatenation formatting), but these are rarer than CoT errors; CoT has higher frequency of similar mistakes.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3322.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3322.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DECOMP-postproc-math</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>DECOMP post-processing for Math QA answer extraction</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use DECOMP to add a targeted sub-task handler that extracts the final numeric answer from a chain-of-thought (CoT) output, replacing brittle regex extractors and correcting formatting or extraction errors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3 variants (davinci-family, e.g., text-davinci-002)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Paper uses GPT-3 (davinci variants) as the CoT generator and as the answer-extraction submodule (GPT3-based submodule) to re-parse CoT and return the final answer robustly.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['Chain-of-Thought generation + Decomposed answer-extraction handler']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Generate CoT with a standard CoT prompt; then send the CoT text to a separate GPT-3 sub-task handler prompted specifically to output the final answer (robust extraction), instead of using brittle regex patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>pipeline combination (CoT reasoning followed by a different extraction handler) — a small but meaningful increase in method diversity within the pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Math QA (GSM8K, MultiArith)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Arithmetic and math word-problem benchmarks requiring multi-step numerical reasoning and final numeric answer extraction.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Applying DECOMP post-processing: GSM8K EM improved from 36 to 50.6 (≈+14.6 points); MultiArith EM improved from 78 to 95 (+17 points), as reported in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Raw CoT (with regex extraction) vs CoT + DECOMP answer-extractor: the latter greatly reduces answer extraction and formatting errors, producing large improvements on both math datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Targeted modular post-processing can correct systematic CoT output extraction issues and yield very large gains on math benchmarks without changing the CoT reasoning itself.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>No negative result specifically reported for this post-processing strategy on these two math datasets; gains depend on the quality of the CoT generation — if the CoT is highly erroneous, extraction alone may not suffice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Decomposed Prompting: A Modular Approach for Solving Complex Tasks', 'publication_date_yy_mm': '2022-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Chain of thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Least-to-most prompting enables complex reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>Text modular networks: Learning to decompose tasks in the language of existing models. <em>(Rating: 2)</em></li>
                <li>Show your work: Scratchpads for intermediate computation with language models. <em>(Rating: 1)</em></li>
                <li>Language model cascades. <em>(Rating: 1)</em></li>
                <li>Internet-augmented language models through few-shot prompting for open-domain question answering. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3322",
    "paper_id": "paper-07955e96cbd778d0ae2a68f09d073b866dd84c2a",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "DECOMP",
            "name_full": "Decomposed Prompting",
            "brief_description": "A modular few-shot prompting paradigm that decomposes a complex task into a program of simpler sub-queries dispatched to dedicated sub-task handlers (separate LLM prompts, recursive decompositions, or symbolic modules such as retrieval), enabling targeted teaching, debugging, replacement, recursion and integration of non-neural tools.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various LLMs (text-davinci-002, davinci-001, code-davinci-002, Flan-T5 family)",
            "model_description": "The paper experiments with OpenAI GPT-3 variants (text-davinci-002, davinci-001), Codex (code-davinci-002) for long contexts, and Flan-T5 models (Large/XL/XXL) as sub-task handlers or for decomposition; a symbolic controller orchestrates calls between the decomposer and sub-task handlers and can call external retrieval (Elasticsearch) or symbolic functions.",
            "model_size": null,
            "reasoning_methods": [
                "modular decomposition (DECOMP)",
                "hierarchical decomposition",
                "recursive decomposition",
                "retrieval-augmented decomposition (Elasticsearch retrieve_odqa module)",
                "post-processing answer extraction (CoT post-processing)",
                "Chain-of-Thought (used as baseline or submodule)",
                "Least-to-most / successive prompting (baselines / related)"
            ],
            "reasoning_methods_description": "DECOMP: a decomposer prompt generates a sequence of sub-queries and assigns each to a sub-task handler (handler can be another prompt, a further decomposed program, or a symbolic API like Elasticsearch). Hierarchical decomposition: break a difficult sub-task into simpler sub-tasks (e.g., str_pos -&gt; split + arr_pos). Recursive decomposition: decomposer can call itself to handle inputs by dividing into smaller instances (e.g., reverse list via splitting). Retrieval-augmented: sub-task handler performs document retrieval + RC. Post-processing: use an extra handler to robustly extract answers from CoT output instead of brittle regex.",
            "diversity_of_methods": "diverse set — DECOMP intentionally composes heterogeneous reasoning styles (specialized CoT prompts for sub-tasks, purely symbolic retrieval, recursive calls, and dedicated answer-extraction modules) rather than relying on a single unified chain-of-thought style; the paper shows this diversity is implemented by prompting different handlers or replacing handlers with symbolic systems.",
            "reasoning_task_name": "Multiple (k-th letter concatenation; list reversal; long-context multi-hop QA (CommaQA-E); open-domain multi-hop QA (2WikiMultihopQA, MuSiQue, HotpotQA); Math QA (GSM8K, MultiArith))",
            "reasoning_task_description": "A set of synthetic and real benchmarks covering symbolic string/list manipulation tasks (character extraction and concatenation, list reversal), long-context reading comprehension (CommaQA-E), open-domain multi-hop question answering (converted versions of 2WikiMultihopQA, MuSiQue, HotpotQA), and arithmetic/math word problems (GSM8K, MultiArith).",
            "performance_by_method": "Aggregated summary as reported: DECOMP outperforms baseline CoT and least-to-most prompting across tasks. Example highlights: on k-th letter concatenation (k=3) DECOMP achieves nearly 100% EM and remains stable with increasing input length while CoT and least-to-most drop noticeably; recursive DECOMP enables strong length generalization on list reversal where CoT fails; on CommaQA-E DECOMP variants (coarse and fine-grained) produce higher EM than CoT (figure-level improvements reported); on open-domain multi-hop QA, Decomp-Ctxt (DECOMP + retrieval) significantly outperforms No-Ctxt (closed-book) and a strong retrieval baseline NoDecomp-Ctxt for most settings (Answer F1 improvements shown in paper figures), with one exception where Codex on HotpotQA was comparable; on Math QA, using DECOMP post-processing for CoT answer extraction yields large gains (GSM8K: 36 -&gt; 50.6, +~14.6 points; MultiArith: 78 -&gt; 95, +17 points).",
            "comparison_of_methods": "The paper directly compares DECOMP to Chain-of-Thought (CoT), a rolled-out CoT (single long CoT describing full decomposition), and least-to-most prompting. Across experiments DECOMP provides higher accuracy and better generalization (length/compositional generalization). DECOMP's separate sub-task prompts enable teaching sub-tasks more effectively than a single rolled-out CoT even when the CoT encodes the same decomposition steps. DECOMP also demonstrates that replacing sub-tasks with symbolic retrieval improves open-domain QA compared to retrieval baselines that do not decompose questions.",
            "key_findings": "1) Modular, heterogeneous decomposition (DECOMP) yields higher accuracy and better generalization than uniform chain-of-thought style prompting on a variety of reasoning tasks. 2) Teaching sub-tasks with their own prompts (or symbolic handlers) is more effective than embedding all intermediate steps in one CoT prompt. 3) Recursive decomposition enables length generalization for operations like list reversal, allowing weaker models to handle longer inputs. 4) Integrating symbolic retrieval as a handler improves open-domain multi-hop QA relative to monolithic retrieval+QA baselines. 5) Targeted post-processing handlers can correct systematic CoT extraction errors, producing large gains on math datasets.",
            "counter_examples_or_negative_results": "Exceptions and limitations: on HotpotQA using Codex, Decomp-Ctxt is roughly comparable (not strictly better) to the strong retrieval baseline; overall gains diminish with weaker/smaller models (e.g., curie-001 performance drops toward zero for some tasks and DECOMP's advantage vanishes when base model capability is very low). Errors that remain in DECOMP are mainly due to sub-task execution failures (incorrect extraction/formatting), i.e., DECOMP shifts errors to sub-task handlers rather than eliminating them entirely.",
            "uuid": "e3322.0",
            "source_info": {
                "paper_title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "CoT",
            "name_full": "Chain-of-Thought prompting",
            "brief_description": "A prompting technique that elicits intermediate reasoning steps in the model's output (a sequence of 'thoughts') to improve multi-step reasoning performance.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (used as CoT baseline in experiments)",
            "model_description": "An InstructGPT3 variant (text-davinci-002) used to produce chain-of-thought explanations as few-shot examples; used both as stand-alone CoT and as part of decomposition baselines (rolled-out CoT).",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought (standard)",
                "CoT with rollout (rolled-out long chain describing full decomposition)"
            ],
            "reasoning_methods_description": "CoT: few-shot examples include intermediate reasoning steps (thoughts) which the model reproduces to arrive at the answer. Rolled-out CoT: the entire decomposition procedure is described inline as a long chain-of-thought rather than modular sub-prompts.",
            "diversity_of_methods": "single / similar style — CoT uses the same chain-of-thought style across the whole task rather than modularly combining different handler types.",
            "reasoning_task_name": "Used as baseline across tasks (k-th letter concatenation, list reversal, CommaQA-E, open-domain QA baselines, Math QA)",
            "reasoning_task_description": "CoT applied as few-shot prompts explaining intermediate steps for multi-step symbolic and textual reasoning tasks.",
            "performance_by_method": "Reported or implied baseline performance: On k-th letter concatenation and longer inputs, CoT accuracy drops noticeably compared to DECOMP (figures show substantial gaps). On list reversal CoT fails to generalize to longer sequences; on CommaQA CoT achieves lower EM than DECOMP variants. On math QA reported CoT baseline scores (prior to DECOMP post-processing) were: MultiArith 78 EM and GSM8K 36 EM (paper shows these are improved by DECOMP post-processing to 95 and 50.6 respectively).",
            "comparison_of_methods": "CoT is the primary baseline. DECOMP consistently outperforms CoT on symbolic tasks, QA, and generalization experiments. Rolled-out CoT that encodes the same decomposition as DECOMP still underperforms, indicating limits of teaching sub-tasks within a single monolithic CoT prompt.",
            "key_findings": "CoT is effective for eliciting reasoning but struggles when component reasoning steps are themselves hard to learn from a few demonstrations embedded in complex examples; modular decomposition that exposes sub-tasks directly leads to better learning of those sub-tasks.",
            "counter_examples_or_negative_results": "Rolled-out CoT can become too long and convoluted and fails to generalize (e.g., length generalization for reversal). CoT's standard answer-extraction heuristics (regex 'answer is . *') are brittle and can be fixed by DECOMP post-processing; CoT makes more frequent sub-task errors compared to DECOMP.",
            "uuid": "e3322.1",
            "source_info": {
                "paper_title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Least-to-most",
            "name_full": "Least-to-most prompting",
            "brief_description": "A prompting strategy that decomposes a problem by generating and answering subproblems in order from easiest to hardest, using one LLM to generate subquestions and another to answer them.",
            "citation_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "mention_or_use": "use",
            "model_name": "text-davinci-002 (used as a baseline)",
            "model_description": "Adapted least-to-most prompting applied as a baseline, including a 'rolled out' variant for comparison.",
            "model_size": null,
            "reasoning_methods": [
                "least-to-most prompting (ordered subquestion generation and answering)"
            ],
            "reasoning_methods_description": "Generate subquestions starting from easiest, progressively working to harder ones; in the paper this baseline is adapted and compared to DECOMP and rolled-out CoT.",
            "diversity_of_methods": "similar style — sequential single-model decomposition (less modular than DECOMP because it relies on one model to both generate and answer subquestions and enforces an 'easy-to-hard' ordering).",
            "reasoning_task_name": "k-th letter concatenation and other symbolic tasks (as baseline comparisons)",
            "reasoning_task_description": "Applied as a baseline for symbolic string tasks and compared against DECOMP and CoT variants.",
            "performance_by_method": "DECOMP outperforms least-to-most prompting on the k-th letter concatenation task (figure-level results indicate a noticeable gap; exact numeric values not tabulated in text).",
            "comparison_of_methods": "DECOMP's flexibility (nonlinear decompositions, modular handlers, recursion) outperforms least-to-most which requires one-shot identification of subproblems in an easy-to-hard ordering; least-to-most variants did not match DECOMP performance in experiments.",
            "key_findings": "Least-to-most helps structured decomposition but is less flexible than DECOMP and underperforms in the studied symbolic tasks when compared directly to modular decomposition.",
            "counter_examples_or_negative_results": "Least-to-most is by definition ordered and requires the model eventually answer the full question; DECOMP imposes no such restriction and can achieve better results by assigning different handlers per subtask.",
            "uuid": "e3322.2",
            "source_info": {
                "paper_title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "Decomp-Ctxt (Retrieval)",
            "name_full": "Decomposed Prompting with Elasticsearch retrieval (Decomp-Ctxt)",
            "brief_description": "A DECOMP configuration that assigns retrieval sub-tasks to an external Elasticsearch-based retrieve_odqa handler, which returns documents and single-hop answers; used to solve open-domain multi-hop QA by composing retrieval and RC modules under a decomposer.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "code-davinci-002 (Codex) and Flan-T5 family (Large/XL/XXL)",
            "model_description": "Codex (code-davinci-002) used for decomposition in long-context settings; Flan-T5-Large (0.7B), Flan-T5-XL (3B), Flan-T5-XXL (11B) were evaluated to test scale effects; retrieval handled by Elasticsearch index and retrieved paragraphs passed to RC prompts.",
            "model_size": "Flan-T5 sizes: Large: 0.7B, XL: 3B, XXL: 11B (as given in paper); Codex size not specified in paper",
            "reasoning_methods": [
                "decomposed prompting + symbolic retrieval (Elasticsearch) + RC",
                "multi-hop decomposition into single-hop retrievals"
            ],
            "reasoning_methods_description": "Decomposer produces single-hop subquestions; retrieve_odqa handler queries Elasticsearch to return titles and passages and an RC answer; subsequent sub-questions can use retrieved answers and documents; final multihop_rcqa model answers using the aggregated context.",
            "diversity_of_methods": "diverse — combines LLM prompting for decomposition with a symbolic retrieval API and separate RC prompts, i.e., heterogeneous modules.",
            "reasoning_task_name": "Open-domain multi-hop QA (2WikiMultihopQA, MuSiQue, HotpotQA converted to open-domain)",
            "reasoning_task_description": "Multi-hop question answering where supporting evidence must be retrieved from a large paragraph corpus; evaluation on 300 dev questions per dataset in open-domain setting.",
            "performance_by_method": "Decomp-Ctxt (DECOMP + retrieval) significantly outperforms No-Ctxt (closed-book) and NoDecomp-Ctxt (single-step retrieval + QA) across datasets and model choices (figures in paper report substantial Answer F1 gains). Exception: with Codex on HotpotQA the Decomp-Ctxt system is comparable (not a clear win). The paper also shows that Flan-T5-XXL with DECOMP can approach Codex performance.",
            "comparison_of_methods": "Paper compares: (A) No-Ctxt (closed-book), (B) NoDecomp-Ctxt (retrieve K paragraphs for full question) and (C) Decomp-Ctxt (DECOMP + retrieval). Decomp-Ctxt beats both alternatives in most settings; K (number of paragraphs retrieved) was separately tuned for NoDecomp-Ctxt and Decomp-Ctxt.",
            "key_findings": "Decomposition that delegates retrieval to a symbolic retriever and restricts retrieval per subquestion improves open-domain multi-hop QA performance over monolithic retrieval+QA baselines; even smaller LMs (Flan-T5-XXL) benefit substantially from DECOMP and can rival larger Codex results.",
            "counter_examples_or_negative_results": "Codex on HotpotQA: Decomp-Ctxt is only comparable to strong baselines, not strictly better. Performance is sensitive to retrieval hyperparameters (number of paragraphs K) and the quality of retrieved passages; DECOMP does not eliminate retrieval errors.",
            "uuid": "e3322.3",
            "source_info": {
                "paper_title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "k-th-letter-exp",
            "name_full": "k-th letter concatenation experiment",
            "brief_description": "Symbolic string manipulation task: extract the k-th character from each word in an input string and concatenate them using a delimiter; used to probe sub-task learning and length generalization.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "text-davinci-002",
            "model_description": "InstructGPT3 (text-davinci-002) used as the base LLM for decomposer and as baseline for CoT and least-to-most prompting in these experiments.",
            "model_size": null,
            "reasoning_methods": [
                "DECOMP with hierarchical decomposition (split, str_pos further decomposed, merge)",
                "Chain-of-Thought (CoT)",
                "CoT rolled-out (single prompt describing full decomposition)",
                "Least-to-most (adapted baseline)"
            ],
            "reasoning_methods_description": "DECOMP: decomposer delegates to split (word tokenization), str_pos (find character at position) which itself is decomposed into split characters + arr_pos, and merge; CoT variants attempt to encode the same steps inside a single chain-of-thought prompt; least-to-most variant adapted similarly.",
            "diversity_of_methods": "DECOMP uses diverse sub-task-specific handlers allowing specialized training examples per subtask; CoT and least-to-most use a single unified chain-style prompt (similar style).",
            "reasoning_task_name": "k-th letter concatenation (synthetic symbolic task; k tested at 3 and other positions; inputs of varying number of words)",
            "reasoning_task_description": "Given a string of N words, return the k-th character of each word concatenated with a delimiter; evaluated on generalization to unseen k, longer input lengths, and new delimiters.",
            "performance_by_method": "DECOMP: close to 100% EM across increasing input lengths (paper notes 'near 100% accuracy' and zero std in aggregated metrics). CoT and least-to-most: substantially lower and degrade as number of words increases (figures show clear drop but exact numeric values not tabulated in text).",
            "comparison_of_methods": "Direct comparison shows DECOMP outperforms CoT and least-to-most even when CoT encodes the same decomposition (rolled-out CoT), indicating modular sub-task prompts are more effective at teaching difficult sub-steps (e.g., finding k-th character).",
            "key_findings": "Factoring the task into sub-tasks and teaching each separately yields strong generalization and near-perfect performance; monolithic CoT struggles to learn some sub-steps from a few demonstrations embedded in complex examples.",
            "counter_examples_or_negative_results": "DECOMP errors observed are attributable to sub-task execution errors (e.g., incorrect character extraction or incorrect concatenation formatting), but these are rarer than CoT errors; CoT has higher frequency of similar mistakes.",
            "uuid": "e3322.4",
            "source_info": {
                "paper_title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
                "publication_date_yy_mm": "2022-10"
            }
        },
        {
            "name_short": "DECOMP-postproc-math",
            "name_full": "DECOMP post-processing for Math QA answer extraction",
            "brief_description": "Use DECOMP to add a targeted sub-task handler that extracts the final numeric answer from a chain-of-thought (CoT) output, replacing brittle regex extractors and correcting formatting or extraction errors.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "GPT-3 variants (davinci-family, e.g., text-davinci-002)",
            "model_description": "Paper uses GPT-3 (davinci variants) as the CoT generator and as the answer-extraction submodule (GPT3-based submodule) to re-parse CoT and return the final answer robustly.",
            "model_size": null,
            "reasoning_methods": [
                "Chain-of-Thought generation + Decomposed answer-extraction handler"
            ],
            "reasoning_methods_description": "Generate CoT with a standard CoT prompt; then send the CoT text to a separate GPT-3 sub-task handler prompted specifically to output the final answer (robust extraction), instead of using brittle regex patterns.",
            "diversity_of_methods": "pipeline combination (CoT reasoning followed by a different extraction handler) — a small but meaningful increase in method diversity within the pipeline.",
            "reasoning_task_name": "Math QA (GSM8K, MultiArith)",
            "reasoning_task_description": "Arithmetic and math word-problem benchmarks requiring multi-step numerical reasoning and final numeric answer extraction.",
            "performance_by_method": "Applying DECOMP post-processing: GSM8K EM improved from 36 to 50.6 (≈+14.6 points); MultiArith EM improved from 78 to 95 (+17 points), as reported in the paper.",
            "comparison_of_methods": "Raw CoT (with regex extraction) vs CoT + DECOMP answer-extractor: the latter greatly reduces answer extraction and formatting errors, producing large improvements on both math datasets.",
            "key_findings": "Targeted modular post-processing can correct systematic CoT output extraction issues and yield very large gains on math benchmarks without changing the CoT reasoning itself.",
            "counter_examples_or_negative_results": "No negative result specifically reported for this post-processing strategy on these two math datasets; gains depend on the quality of the CoT generation — if the CoT is highly erroneous, extraction alone may not suffice.",
            "uuid": "e3322.5",
            "source_info": {
                "paper_title": "Decomposed Prompting: A Modular Approach for Solving Complex Tasks",
                "publication_date_yy_mm": "2022-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Least-to-most prompting enables complex reasoning in large language models.",
            "rating": 2
        },
        {
            "paper_title": "Text modular networks: Learning to decompose tasks in the language of existing models.",
            "rating": 2
        },
        {
            "paper_title": "Show your work: Scratchpads for intermediate computation with language models.",
            "rating": 1
        },
        {
            "paper_title": "Language model cascades.",
            "rating": 1
        },
        {
            "paper_title": "Internet-augmented language models through few-shot prompting for open-domain question answering.",
            "rating": 1
        }
    ],
    "cost": 0.022311499999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Decomposed Prompting: A Modular Approach for Solving Complex Tasks</h1>
<p>Tushar Khot ${ }^{\mathbf{A}}$, Harsh Trivedi ${ }^{\text { }}$, Matthew Finlayson ${ }^{\mathbf{A}}$, Yao Fu ${ }^{\mathbf{A}}$; Kyle Richardson ${ }^{\mathbf{A}}$, Peter Clark ${ }^{\mathbf{A}}$, Ashish Sabharwal ${ }^{\mathbf{A}}$<br>${ }^{\text {A }}$ Allen Institute for AI ${ }^{\text {® }}$ Stony Brook University ${ }^{\text {A }}$ University of Edinburgh<br>tushark@allenai.org, hjtrivedi@cs.stonybrook.edu, matthewf@allenai.org, yao.fu@ed.ac.uk, kyler@allenai.org, peterc@allenai.org, ashishs@allenai.org</p>
<h4>Abstract</h4>
<p>Few-shot prompting is a surprisingly powerful way to use Large Language Models (LLMs) to solve various tasks. However, this approach struggles as the task complexity increases or when the individual reasoning steps of the task themselves are hard to learn, especially when embedded in more complex tasks. To address this, we propose Decomposed Prompting, a new approach to solve complex tasks by decomposing them (via prompting) into simpler sub-tasks that can be delegated to a shared library of prompting-based LLMs dedicated to these sub-tasks. This modular structure allows each prompt to be optimized for its specific sub-task, further decomposed if necessary, and even easily replaced with more effective prompts, trained models, or symbolic functions if desired. We show that the flexibility and modularity of Decomposed Prompting allows it to outperform prior work on few-shot prompting using GPT-3. On symbolic reasoning tasks, we can further decompose sub-tasks that are hard for LLMs into even simpler solvable sub-tasks. When the complexity comes from the input length, we can recursively decompose the task into the same task but with smaller inputs. We also evaluate our approach on textual multi-step reasoning tasks: on long-context multi-hop QA, we can more effectively teach the sub-tasks via our separate sub-tasks prompts; and on open-domain multi-hop QA, we can easily incorporate a symbolic information retrieval module within our decomposition framework, leading to improved performance on both tasks. ${ }^{1}$</p>
<h2>1 INTRODUCTION</h2>
<p>Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020) have been shown to solve various tasks given only a few examples as prompts, also referred to as in-context learning. These models can even perform more complex reasoning tasks when shown the sequence of simple reasoning steps needed to perform the complex task as a prompt (Wei et al., 2022; Nye et al., 2021). In essence, the sequence of reasoning steps, such as in Chains-of-Thought (CoT) prompting (Wei et al., 2022), demonstrates how to decompose the complex task as well as how each reasoning step should be performed. However, as tasks become more complex, few demonstrations of the complex task aren't sufficient for current models to learn to perform all necessary reasoning steps. E.g., fewshot demonstrations of concatenating the $k^{\text {th }}$ letter of words in a string is insufficient for GPT-3 to learn to extract the $k^{\text {th }}$ letter, or learn to answer hard single-hop questions when only provided a few demonstrations of multi-hop questions. Additionally, it is unclear whether tasks such as document retrieval and integration, for knowledge-intensive tasks, can even be done by few-shot prompts.</p>
<p>To address these limitations, we propose Decomposed Prompting (DECOMP), a new approach to solve complex tasks by instead decomposing them into simpler sub-tasks and delegating these to sub-task specific LLMs, with both the decomposer and the sub-task LLMs (henceforth, sub-task handlers) having their own few-shot prompts. Fig 1 illustrates our approach. The decomposer</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: While standard approaches only provide labeled examples (shown as a grey input box with green label box), Chain-of-Thought prompting also describes the reasoning steps to arrive at the answer for every example in the prompt. Decomposed Prompting, on the other hand, uses the decomposer prompt to only describe the procedure to solve the complex tasks using certain subtasks. Each sub-task, indicated here with A, B and C is handled by sub-task specific handlers which can vary from a standard prompt (sub-task A), a further decomposed prompt (sub-task B) or a symbolic function such as retrieval (sub-task C)
prompt only describes a sequence of sub-tasks (A, B, and C) needed to solve the complex tasks, indicated with the dashed lines. Each sub-task is then delegated to the corresponding sub-task handler shown on the right.</p>
<p>Using a software engineering analogy, the decomposer defines the top-level program for the complex task using interfaces to simpler, sub-task functions. The sub-task handlers serve as modular, debuggable, and upgradable implementations of these simpler functions, akin to a software library. If a particular sub-task handler, say the one for identifying the $k^{\text {th }}$ letter or retrieving a document, is not performing well enough, we can debug this handler in isolation, explore alternative prompts or implementations, and seamlessly plug the improved module back into the overall system, as a systematic way to try to improve performance on the complex end-task.</p>
<p>This approach has several advantages over prior work (as also shown in the figure). The sub-task handlers can be shown a broader and richer set of examples (of the simpler task) than the specific ones needed for the complex task prompt (task A). If a sub-task is too complex, it can be further decomposed into simpler sub-tasks (task B). Similar to software libraries, these sub-task handlers can be shared across multiple tasks; e.g., here tasks A and C are reused in the model for task B. As noted above, a sub-task handler can be easily swapped with an improved implementation without any change to the rest of the system. Few-shot prompt based LLMs can be even replaced with a symbolic system for tasks more suited for non-neural methods; e.g., task C uses a symbolic retrieval system such as Elasticsearch that can handle very large-scale corpora. Lastly, we can even improve upon prior work by simply adding an error-correcting sub-task handler as a post-processing step.
To illustrate these advantages of DECOMP, we empirically evaluate it against prior work on eight challenging datasets using GPT3 models: (1) On a task of concatenating the $k^{\text {th }}$ letter, we show that our approach of factoring out each sub-task allows us to more effectively teach the sub-problem of extracting the $k^{\text {th }}$ letter(specifically, by decomposing it into even easier sub-tasks). (2) On a task of reversing a list, we show that DECOMP allows us to extend the capabilities of a weaker model and build a scale-invariant system by recursively decomposing the task into reversal of smaller and smaller lists. (3) On a task of long-context QA (Khot et al., 2022), our approach allows each subtask handler to accommodate more examples than feasible with CoT prompting leading to better QA performance. (4) On three multi-hop open-domain QA datasets (Yang et al., 2018; Ho et al., 2020; Trivedi et al., 2022), we can incorporate a symbolic retrieval (ElasticSearch) API as the handler for the retrieval sub-task leading to better results than CoT. (5) On two Math QA datasets (Cobbe et al., 2021; Roy \&amp; Roth, 2015), we can post-process CoT to easily fix frequent formatting errors, resulting in a surprisingly high improvement of 14-17 pts.</p>
<h1>2 Related Work</h1>
<p>Few-shot Prompts for Multi-Step Reasoning Large-scale Language models (LLMs) have been shown to learn various NLP tasks given just few examples as prompts (Brown et al., 2020). Recently, they have also been successfully applied to various multi-step reasoning tasks by providing the intermediate reasoning steps, i.e. Chain-of-Thought (Wei et al., 2022; Chowdhery et al., 2022), needed to arrive at the answer. An alternate approach has been to compose multiple LLMs or LLMs with symbolic functions to perform multi-step reasoning (Jung et al., 2022; Creswell et al., 2023;</p>
<p>Press et al., 2022; Parisi et al., 2022; Gao et al., 2022; Schick et al., 2023, inter alia). We view these prior works as specialized systems with a pre-defined decomposition structure.</p>
<p>The closest works to our approach are the ideas of least-to-most prompting (Zhou et al., 2023) and successive prompting (Dua et al., 2022) where one prompt/model is used to generate the subquestions needed to answer a complex question and a second prompt/model sequentially answers these sub-questions. In contrast, our approach allows for diverse decomposition structures including recursion and other non-linear decomposition structures. E.g., by definition, least-to-most asks questions from easiest to the hardest and requires an LLM to eventually answer the complete question ("most" in least-to-most) whereas we have no such restriction. Additionally, we iteratively generate new questions based on previous answers (similar to successive prompting) and can explicitly assign different prompts or symbolic systems to answer each sub-question.</p>
<p>Modular Approaches for Multi-Step Reasoning Our work follows a long literature in NLP on neural modular modeling architectures (Andreas et al., 2016; Talmor \&amp; Berant, 2018; Min et al., 2019; Jiang \&amp; Bansal, 2019; Gupta et al., 2020; Perez et al., 2020; Khot et al., 2021; Levine et al., 2022) for question-answering and other tasks. We take particular inspiration from the Text Modular Networks approach of Khot et al. (2021), whereby problem decomposition consists of a learned next question generator trained to generate questions in the language of a collection of textual and symbolic agents. Best-first search strategy was used to explore the space of possible decompositions during inference. In contrast to this work, which largely centered around supervised training of the next-question generator given existing agents, we leverage the power and recent successes of fewshot LLMs to build both the decomposer and the sub-task agents that best fit the ideal decomposition. This has the advantage of obviating the need for specialized supervised training data that may not always be available for all sub-tasks - a key bottleneck of this prior work.</p>
<h1>3 DeCOMPOSED PROMPTING</h1>
<p>As with conventional few-shot prompting, the goal is to teach an LLM to find an answer $A$ to a query $Q$ using a small set of in-context examples $D=\left{E_{1}, \ldots, E_{|D|}\right}$. The answer $A$ is obtained from the underlying distribution $p(A \mid Q, D, \theta)$ (Dohan et al., 2022). In the most basic few-shot setup, examples take the form $E_{j}=\left(Q_{j}, A_{j}\right)$. In the case of CoT-style prompting, the goal is to obtain answers by first generating a sequence or chain of intermediate reasoning steps or "thoughts" $T$, and then deriving the final answer based on $T$. To teach this ability, one uses more sophisticated in-context examples that take the form $E_{j}=\left(Q_{j},\left(T_{j, 1}, \ldots, T_{j, k}\right), A_{j}\right)$.</p>
<p>In DECOMP, the core is a decomposer LLM that tries to solve a complex task by generating a prompting program $P$ for it. Each step of $P$ directs a simpler sub-query to a function in an auxiliary set of sub-task functions $\mathcal{F}$ available to the system. Given a query $Q$ whose answer is $A$, the program $P$ is a sequence of the form $\left(\left(f_{1}, Q_{1}, A_{1}\right), \ldots,\left(f_{k}, Q_{k}, A_{k}\right)\right)$ where $A_{k}$ is the final answer predicted by $P$ and $Q_{i}$ is a sub-query directed to the sub-task function $f_{i} \in \mathcal{F} . P$ is executed by a high-level imperative controller, which passes the inputs and outputs between the decomposer and sub-task handler until a stopping condition in $P$ is met and the final output obtained.</p>
<p>To teach the decomposer LLM in a few-shot prompting manner, we use in-context examples that take the form $E_{j}=\left(\left(Q_{j},\left(f_{j, 1}, Q_{j, 1}, A_{j, 1}\right), \ldots,\left(f_{j, k_{j}}, Q_{j, k_{j}}, A_{j, k_{j}}\right)\right)\right)$ where $A_{j, k_{j}}=A_{j}$ is the final answer for $Q_{j}$ and $\left(Q_{j, 1}, \ldots, Q_{j, k_{j}}\right)$ is a decomposition of $Q_{j}$. Each sub-task function $f$, in turn, is operationalized via a sub-task handler as an in-context prompting LLM (e.g., a separate CoT-style prompt or a additional prompting program dedicated to that sub-task), or any other symbolic or learned function (e.g., a calculator or specialized supervised trained model).</p>
<h3>3.1 DeCOMPOSED PROMPTS</h3>
<p>To illustrate this with an example, consider a multi-step task such as "Concatenate the first letter of every word in $\operatorname{str}$ using a space". We can solve this task by decomposing it into a sequence of three simple sub-tasks: 1) Collect the list of words in the $\operatorname{str}$; 2) For each word, extract the third letter; 3) Concatenate the extracted letters using space as the separator. Fig. 2 shows an example decomposition prompt for this task. Much like a conventional structured program, the top-level decomp prompt provides an example program $E_{j}$ using three sub-task functions: $f_{1}$ :split that</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Prompts for the decomposer and the split and merge sub-tasks used by the decomposer. The decomposer specifies the sequence of questions and corresponding sub-tasks (within square braces). The sub-task prompts can be written independent of the complex task examples and can even capture generalizations, e.g., letters in word (split) and no delimiter (merge).
splits words in an input string, $f_{2}$ :str_pos that finds character positions in strings and $f_{3}$ :merge that concatenates characters. In this case, we operationalize each sub-task function as a separate in-context prompt (e.g., using a standard prompting approach for split and merge on the right side), each containing a set of in-context examples that are independent of the original complex task.</p>
<p>In addition to the three functions described above, additional control structure is included, such as the symbolic function foreach, which iterates over arrays and references to previous answers such as #1. We note that such a helper function is not strictly necessary (e.g., we could directly generate "Q2': What is the first letter of Jack?" and "Q3': What is the first letter of Ryan?" instead of Q2 in the figure) and is added to reduce the manual effort needed to specify the decomposition and also reduce potential errors during decomposition. In our experiments we use two of the compositional operators defined by Khot et al. (2022) (see appendix for details), although it is capable of using all their operators (which also capture the QDMR operators from Wolfson et al. (2020)).
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The inference procedure in DECOMP iteratively calls the decomposer prompt to generate the next question and sub-task at each step, given the current history of question and answers. The generated question is then routed to the assigned sub-task handler (with some handling of special operators, when needed). When the special end-of-questions [EOQ] marker is generated, the previous answer is returned as the final prediction.</p>
<h1>3.2 Prompt Execution and Inference</h1>
<p>Given a new question and a set of background in-context examples $D$, the inference (i.e., the program construction and execution) process is illustrated in Fig. 3. The new complex question is fed to the decomposer prompt to get the first sub-question to be asked to the split prompt. With the help of our symbolic controller, the answer generated from this prompt is then appended to the decomposer prompt to get the second sub-question, $Q 2$. Due to the foreach operator in the generated question, $Q 2$ results in two questions (one for each word in #1) to be fed to the str_pos prompt. The answers are combined into an array to get the answer $# 2$. The entire decomposition history is used to generate $Q 3$ and passed to the merge prompt to get the final answer. Since the task has been solved, the decomposition prompt produces the special end-of-sequence marker([EOQ]) and the last</p>
<p>answer is returned as the final answer. Formally, performing inference involves finding the best answer $A$ to a new query $Q$, which in the simplest form involves computing the MAP answer using the LLMs predictive distribution for $A$, i.e., $\hat{A}=\arg \max _{A} p(A \mid D, Q, \theta)$ (Dohan et al., 2022). For practicality, such computations are approximated using greedy search in our experiments.</p>
<h1>3.3 DECOMP CAPABILITIES</h1>
<p>Hierarchical Decomposition Certain sub-tasks, even when given many examples, are not solvable with few-shot prompting. E.g., we found identifying the $k^{\text {th }}$ letter of a string to be challenging for the GPT3 text-davinci-002 model. In such a scenario, we can decompose the sub-task prompt further, to first identify the letters and their position and then select the $k^{\text {th }}$ element of this array (see Fig. 4). We can also re-use existing sub-task prompts in our framework. E.g., the split prompt can be reused since it was developed for the general task of splitting strings. ${ }^{2}$
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Since identifying the $k^{t h}$ character is challenging for GPT3 davinci-002 model, we further decompose it into two simpler sub-tasks: split the word into its letters (using the shared sub-task split) and then return the $k^{t h}$ item of this list using the arr_pos prompt.</p>
<p>Recursive Decomposition Some problems can be naturally broken down into one or more smaller problems of the same form. Recursive algorithms such as merge sort use this idea to solve large problems efficiently, using a succinctly described method. We apply this same principle in DECOMP by allowing the decomposer prompt to recursively call itself, as shown in Fig. 5 for the task of list reversal. By using recursion, we are able to generalize any base prompting approach (CoT in this figure) to much longer lists by breaking the input into smaller and smaller lists till we reach a list length where the model is highly accurate. Such recursive approaches can not be described by current methods such as CoT and standard prompting. Least-to-most prompting (Zhou et al., 2023) also proposes a similar solution but differs in two key aspects (a) it has to identify all the subproblems in one-shot instead of our iterative top-down decomposition (b) it has to learn to identify the relevant answers from the previous solutions which we get for free from our decomposition.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Base Case (using CoT)</th>
<th style="text-align: center;">QC: Reverse the sequence "newspaper, glasses, laptop, bottle".</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">QC: Reverse the sequence "laptop, photo, clip",</td>
<td style="text-align: center;">QS: [list_split] Split the sequence "..."</td>
</tr>
<tr>
<td style="text-align: center;">QS: [extract] First is laptop. Second is photo. Third is</td>
<td style="text-align: center;">A: "newspaper, glasses" and "laptop, bottle"</td>
</tr>
<tr>
<td style="text-align: center;">clip. Now to reverse, change the order to: Third is</td>
<td style="text-align: center;">QS: [reverse] Reverse the sequence "newspaper, glasses"</td>
</tr>
<tr>
<td style="text-align: center;">clip. Second is photo. First is laptop. So the answer</td>
<td style="text-align: center;">A: "glasses, newspaper"</td>
</tr>
<tr>
<td style="text-align: center;">is "clip, photo, laptop",</td>
<td style="text-align: center;">QS: [reverse] Reverse the sequence "laptop, bottle"</td>
</tr>
<tr>
<td style="text-align: center;">A: "laptop, photo, clip"</td>
<td style="text-align: center;">A: "bottle, laptop"</td>
</tr>
<tr>
<td style="text-align: center;">QS: [EOQ]</td>
<td style="text-align: center;">QS: [merge] Concatenate "bottle, laptop" and "glasses, newspaper" using a comma A: "bottle, laptop, glasses, newspaper"</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">QS: [EOQ]</td>
</tr>
</tbody>
</table>
<p>Figure 5: Sample prompt for recursive decomposition for reversing lists. Each list is split into two halves and each half is reversed and concatenated in the reverse order. We can recursively split a list till we hit the base case (lists of length 3 here) where existing approaches such as CoT are accurate.</p>
<p>External API Calls In certain cases, the sub-tasks may not be feasible to solve using only a LLM. E.g., retrieving knowledge from a KB or large corpus. Such sub-tasks, however, can be easily solved using existing systems such as retrieving documents using an Elasticsearch index or webpages using Google search (Lazaridou et al., 2022). Fig. 6 shows how DECOMP can easily use such a system to retrieve the relevant documents and answer a single-hop open-domain question.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: A Decomposed Prompt to answer open-domain questions using Elasticsearch-based retrieval. Full usage of this prompt for open-domain multihop questions is given in Fig. 11.</p>
<h1>4 CASE STUDIES</h1>
<p>We showcase DECOMP's strengths through four tasks; two symbolic manipulation tasks similar to those investigated by Wei et al. (2022) and two existing textual multi-hop reasoning tasks. Unless specified, we use text-davinci-002 InstructGPT3 model (Ouyang et al., 2022) as the LLM and report the Exact Match (EM) numbers, following prior work. For order-independent list answers, we evaluate set equality as EM. We compare our approach to CoT rather than each specific decomposition structure used in prior work. See App. G for the complete prompts for all our tasks.</p>
<h2>4.1 $k^{\text {th }}$ LETTER CONCATENATION (HIERARCHICAL DECOMPOSITION)</h2>
<p>We compare DECOMP to CoT prompting for concatenating letters at the $k^{\text {th }}$ position. All prompts contain examples of concatenating letters in position 1, 4, and last position of strings with 3 words. We create three different prompts for all our baselines and present the average to account for variance due to the choice of examples following Perez et al. (2021). We use the decomp, split, str_pos (further decomposed as shown in Fig. 4), and merge prompts for decomposition prompting. We adapt the CoT for last letter concatenation from prior work (Wei et al., 2022) for this task as shown below. In addition, we consider a rolled out version of our decomposition prompts in terms of a CoT, i.e., we describe the entire decomposition process (identify words, split each word into letters, take $k^{\text {th }}$ letter and concatenate) as a single CoT. e.g, for the question "Take the letters at position 4 of the words in "Herbert Alexander Simon" and concatenate them using a space.", we use the CoT:</p>
<h2>Chain-Of-Thought</h2>
<p>The letter at position 4 of "Herbert" is "b". The letter at position 4 of "Alexander" is "x". The letter at position 4 of "Simon" is "o". Concatenating "b", "x", "o" using a space leads to "b x o". So, "Herbert Alexander Simon" outputs "b x o". ...</p>
<h2>Chain-Of-Thought (rolled out)</h2>
<p>The words in "Herbert Alexander Simon" are "Herbert", "Alexander", and "Simon". The letters and their positions in "Herbert" are " $[(H, 1),(e, 2),(r$, 3), (b, 4), (e, 5), (r, 6), (t, 7)]". The letter at position 4 in this sequence is "b". $\cdots$ outputs "b x o". ...</p>
<p>We similarly adapt the least-to-most prompt (Zhou et al., 2023) to include rollout. (see App. G). We compare these four prompting techniques on 4 datasets to evaluate generalization along 3 axes: (1) new letter position $k=3 ;{ }^{5}$ (2) longer inputs, #words=4 and 5; (3) new delimiter ";". The words in the test examples come from a list of most popular first and last names. ${ }^{4}$ All evaluation datasets have 100 examples. We present results on space as a delimiter averaged across three prompts in Fig. 7. ${ }^{5}$
DECOMP outperforms chain-of-thought and least-to-most prompting, even when the prompt uses the same reasoning procedure as the rolled out decomposition. This shows that the separate prompts are more effective at teaching hard sub-tasks than a single CoT prompt.
DECOMP generalizes perfectly to longer sequences. As the length of the input sequence increases, our approach continues to achieve close to $100 \%$ accuracy on this task. ${ }^{6}$ The CoT-based approaches drop noticeably in their scores with longer input lengths, widening the performance gap.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: EM Results on the $k^{t h}$ letter concatenation task $(\mathrm{k}=3$ ) using space as delimiter with different number of words in the input. DECOMP outperforms and generalizes better than CoT as well as Least-to-most prompting.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: EM results on reversing sequences. Incorporating CoT in DECOMP greatly increases the ability of the model to generalize to new sequence lengths.</p>
<h1>4.2 LIST REVERSAL (RECURSIVE DECOMPOSITION)</h1>
<p>We use the task of reversing lists of words ${ }^{7}$ to show how recursive DECOMP enables length generalization. We adapt the relevant CoT prompt from Wei et al. (2022), and integrate it in a decomposed prompt. As a control, we also compare to a CoT version w/ rollout of our decomposed prompt. All prompts contain the same 3 examples of reversing word sequences with 3-5 items. We evaluate all prompts for generalization to $4,6,8$, and 10 -item sequences. Here we use davinci-001 to show that DECOMP enables a weaker model approach davinci-002's performance (which does solve this task). We use the strategy from Fig. 5 and provide our prompts in App. G. Fig. 8 shows the results of the prompting strategies on different input lengths.
DECOMP improves the length generalization of few-shot prompting. While our base CoT prompt does not generalize at all to longer sequences, our approach can recursively decompose the problem and achieve better length generalization. Moreover, the CoT version of our decomposition strategy fails because the unrolled prompt becomes too long and convoluted without the ability to abstract away sub-modules.</p>
<h3>4.3 LONG-CONTEXT QUESTION ANSWERING</h3>
<p>We next evaluate on the CommaQA-E dataset (Khot et al., 2022) under the reading comprehension setting. The dataset consists of synthetically generated entities (e.g. Erowid award), facts ("Wetherality was an actor in the movie Dewbar.") and multi-hop questions (e.g., "What awards have the actors of the Erowid winning movies received?"). Due to the presence of many distractors and, as a result, longer context, this dataset has been shown to be hard for standard LMs even when fine-tuned.
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Sample prompts used for the CommaQA dataset. On the left, the coarse-grained decomposition defines a single QA sub-task with all single-hop questions being delegated to a single sub-task handler. On the right, the fine-grained decomposition assigns questions to three different sub-tasks (see App. G for their prompts) depending on the question type. This allows us to provide more examples for each question type allowing the model to learn the sub-task more effectively.</p>
<p>To fit these questions within GPT3's context limit (2049 tokens), we generate a smaller version of the CommaQA-E dataset and of the compositional generalization split such that we can fit at least</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>four examples in the context for CoT prompts. The CoT prompts describe the sequence of facts needed to arrive at the answer (see App. G for all the prompts).</p>
<p>For DECOMP, we can separate the task of decomposition (independent of the context) from the sub-tasks of single-hop question answering. As shown in Fig. 9, we provide examples of the context-independent decomposition in the decomposer prompt and use the separate sub-task prompts to teach the QA skill over the given context. Additionally, we can choose the granularity of decomposition to trade off human effort for increased accuracy. For example, we could have single QA prompt to handle all the questions or create QA prompts for different classes of questions. In our experiments, each sub-task prompt contains 8 QA examples (2 questions/para). We evaluate three different prompts and report the average results in Fig. 10.</p>
<p>We make three observations on CommaQA. DeCOMP is more accurate than CoT irrespective of the granularity of decomposition or the evaluation split. Finer grained decomposition can help improve task performance by providing more examples for each class of questions, which in turn increases single-hop QA accuracy. DECOMP generalizes to new compositions such as the compositional generalization split of CommaQA, which tests models on unseen compositions of relations observed in the training set. While CoT has a drop in score, both decomposition-based approaches actually get a small bump (the subset of relations used in this split are easier for our QA models).</p>
<h1>4.4 Open-Domain Question Answering</h1>
<p>Next, we demonstrate the ability of our approach to integrate external API calls on the task of open-domain multihop question answering. We evaluate our approach on three datasets: (1) 2WikiMultihopQA (Ho et al., 2020) (2) MuSiQue (Trivedi et al., 2022) (3) HotpotQA (Yang et al., 2018). We describe the open-domain versions of these datasets in more detail in App. A We use the Codex (code-davinci-002) model here since it can fit the much longer contexts needed. We also evaluate the impact of model scale on DECOMP by using models from the Flan-T5 family: Flan-T5-Large (0.7B), Flan-T5-XL (3B), and Flan-T5-XXL (11B). ${ }^{8}$</p>
<div class="codehilite"><pre><span></span><code><span class="n">QC</span><span class="o">:</span><span class="w"> </span><span class="n">In</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">country</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">Lost</span><span class="w"> </span><span class="n">Gravity</span><span class="w"> </span><span class="n">manufactured</span><span class="o">?</span>
<span class="n">QS</span><span class="o">:</span><span class="w"> </span><span class="n">retrieve_odqa</span><span class="o">)</span><span class="w"> </span><span class="n">Which</span><span class="w"> </span><span class="n">company</span><span class="w"> </span><span class="n">manufactured</span><span class="w"> </span><span class="n">Lost</span><span class="w"> </span><span class="n">Gravity</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">(</span><span class="s2">&quot;titles&quot;</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;Lost Gravity (roller coaster)&quot;</span><span class="w"> </span><span class="o">...</span><span class="w"> </span><span class="o">],</span><span class="w"> </span><span class="s2">&quot;answer&quot;</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;Mack Rides&quot;</span><span class="o">])</span>
<span class="n">QS</span><span class="o">:</span><span class="w"> </span><span class="n">retrieve_odqa</span><span class="o">)</span><span class="w"> </span><span class="n">The</span><span class="w"> </span><span class="n">company</span><span class="w"> </span><span class="n">Mack</span><span class="w"> </span><span class="n">Rides</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">from</span><span class="w"> </span><span class="n">which</span><span class="w"> </span><span class="n">country</span><span class="o">?</span>
<span class="n">A</span><span class="o">:</span><span class="w"> </span><span class="o">(</span><span class="s2">&quot;titles&quot;</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;Mack Rides&quot;</span><span class="o">,</span><span class="w"> </span><span class="o">...],</span><span class="w"> </span><span class="s2">&quot;answer&quot;</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;Germany&quot;</span><span class="o">])</span>
<span class="n">QS</span><span class="o">:</span><span class="w"> </span><span class="n">multihop_rcqa</span><span class="o">)</span><span class="w"> </span><span class="n">Titles</span><span class="o">:</span><span class="w"> </span><span class="o">[</span><span class="s2">&quot;Lost Gravity (roller coaster)&quot;</span><span class="w"> </span><span class="o">...,</span><span class="s2">&quot;Mack Rides&quot;</span><span class="o">...].</span>
<span class="n">Question</span><span class="o">:</span><span class="w"> </span><span class="n">In</span><span class="w"> </span><span class="n">what</span><span class="w"> </span><span class="n">country</span><span class="w"> </span><span class="n">was</span><span class="w"> </span><span class="n">Lost</span><span class="w"> </span><span class="n">Gravity</span><span class="w"> </span><span class="n">manufactured</span><span class="o">?</span>
</code></pre></div>

<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: EM results on the CommaQA-E datasets. DECOMP always outperforms CoT, with fine-grained marginally out-performing coarse-grained decomposition.</p>
<p>Wikipedia Title: Lost Gravity (roller ... ) Lost Gravity is a steel roller coaster Wikipedia Title: Mack Rides
Mack Rides GmbH \&amp; Co ...
Q: In what country was Lost Gravity manufactured?
A: "Germany" multihop_rcqa</p>
<p>Figure 11: The prompt used to answer open-domain multihop questions using Elasticsearch-based retrieval. The retrieve_odqa prompt is given in Fig. 6.</p>
<p>Fig. 11 shows the decomposition prompt we use. The decomposer generates (singlehop) subquestions and delegates them to retrieve_odqa (described in Fig. 6). As we showed earlier, this module retrieves relevant documents then uses an RC model to answer. retrieve_odqa returns both the answer and the documents, allowing subsequent sub-questions to use the answers (e.g. "Mack Rides") and the multihop_rcqa model to use the documents. The final multihop_rcqa model is prompted to produce the answer directly or using CoT given K paragraphs.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>We compare our approach against two baselines: A. No Context (No-Ctxt), A closed-book setting baseline where the model must rely only on its parametric knowledge. B. NoDecomp Context (NoDecomp-Ctxt), A simple retrieval baseline where we retrieve K paragraphs using the multi-hop question as the input and use that as context. For both NoDecomp-Ctxt and Decomp-Ctxt, K is selected by hyperparameter tuning (App. A). We manually annotate CoTs and decompositions for 20 training set questions, and sample 3 prompts of 15 questions each for all approaches. The detailed prompts are given in the Appendix G. We evaluate on 300 held-out dev questions in each dataset.</p>
<p><img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 12: Answer F1 on three open-domain QA datasets using two base LMs: Codex (left) and Flan-T5-XXL (right) with direct prompting. Decomp-Ctxt models (ours) significantly outperforms the No-Ctxt models (no retrieval) in all settings and also outperforms our strong retrieval baseline (NoDecomp-Ctxt QA), with the exception of Codex on HotpotQA where it is comparable. See App. A.3 for results on smaller Flan-T5 models and CoT prompting.</p>
<p>We present results on all three datasets with direct QA prompts in Fig. 12 with other results in App. A. The Decomp-Ctxt models performs significantly better than No-Ctxt models in all the settings showing that external knowledge can be leveraged to improve few-shot models on open-domain mulithop QA. Furthermore, we show that our Decomp-Ctxt models outperform the strong retrieval baseline (NoDecomp-Ctxt) in all settings except one (Codex with HotpotQA). Finally, we show that even with the much smaller Flan-T5-XXL model, Decomp-Ctxt outperforms all the baselines and can even achieve scores comparable to the Codex-only systems.</p>
<h1>4.5 ADDITIONAL RESULTS</h1>
<p>Post-processing CoT for error correction DECOMP also allows us to create a targeted sub-task handler to focus on the source of error in any system. For example, CoT for arithmetic reasoning often rely on patterns (answer is . *) to extract answers but the CoT does not always fit this pattern. Instead, we can assign the answer extraction to a better sub-task handler (GPT3) and reduce these types of errors. This results in a 17 pt improvement on MultiArith $(78 \rightarrow 95)$ and 14 pt improvement on GSM8K $(36 \rightarrow 50.6)$ compared to CoT prompting (details in App. B).</p>
<p>While DECOMP outperforms the baselines in aggregate, we also see the gains of DECOMP are consistent across prompt choices (see App. D) and decomposition schemes (see App. E).</p>
<h2>5 CONCLUSION</h2>
<p>We proposed a new approach, Decomposed Prompting, to solve complex tasks using few-shot prompts, by decomposing them into a prompting program built out of simpler sub-tasks. Drawing inspiration from software libraries, our decomposer and shared sub-tasks are designed in a modular fashion: they use their own few-shot prompts, allowing one to independently optimize each prompt, decompose a sub-task further if necessary, or even seamlessly replace it with a symbolic system. We show that Decomposed Prompting outperforms prior work on four different tasks and generalization settings, establishing it as an effective few-shot paradigm for solving complex tasks.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>ACKNOWLEDGEMENTS</h1>
<p>We thank members of the Aristo team at the Allen Institute for AI (AI2) for their constructive feedback and the reviewers for their invaluable suggestions. This work was supported in part by the National Science Foundation under grants IIS2007290.</p>
<h2>REFERENCES</h2>
<p>Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR, 2016.</p>
<p>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>
<p>Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models for interpretable logical reasoning. In ICLR, 2023. URL https:// openreview.net/forum?id=3Pf3Wg6o-A4.</p>
<p>David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes, Yuhuai Wu, Henryk Michalewski, Rif A Saurous, Jascha Sohl-dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.</p>
<p>Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting for decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1251-1265, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL https://aclanthology.org/ 2022.emnlp-main. 81.</p>
<p>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. PAL: Program-aided language models. ArXiv, abs/2211.10435, 2022.</p>
<p>Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. Neural module networks for reasoning over text. In ICLR, 2020.</p>
<p>Xanh Ho, A. Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps. In COLING, 2020.</p>
<p>Yichen Jiang and Mohit Bansal. Self-assembling modular networks for interpretable multi-hop reasoning. In EMNLP, 2019.</p>
<p>Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent reasoning with recursive explanations. In EMNLP, 2022.</p>
<p>Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. Text modular networks: Learning to decompose tasks in the language of existing models. In NAACL, 2021.</p>
<p>Tushar Khot, Kyle Richardson, Daniel Khashabi, and Ashish Sabharwal. Hey AI, can you solve complex tasks by talking to agents? In Findings of ACL, 2022.</p>
<p>Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internetaugmented language models through few-shot prompting for open-domain question answering. ArXiv, abs/2203.05115, 2022.</p>
<p>Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin, Opher Lieber, Barak Lenz, Shai Shalev-Shwartz, et al. Standing on the shoulders of giant frozen language models. arXiv preprint arXiv:2204.10019, 2022.</p>
<p>Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop reading comprehension through question decomposition and rescoring. In $A C L, 2019$.</p>
<p>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate computation with language models. ArXiv, abs/2112.00114, 2021.</p>
<p>Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022.</p>
<p>Aaron Parisi, Yao Zhao, and Noah Fiedel. TALM: Tool augmented language models. arXiv preprint arXiv:2205.12255, 2022.</p>
<p>Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. Unsupervised question decomposition for question answering. In EMNLP, 2020.</p>
<p>Ethan Perez, Douwe Kiela, and Kyunghyun Cho. True few-shot learning with language models. In NeurIPS, 2021.</p>
<p>Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350, 2022.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100, 000+ questions for machine comprehension of text. In EMNLP, 2016.</p>
<p>Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, 2015. doi: 10.18653/v1/D15-1202. URL https://aclanthology.org/D15-1202.</p>
<p>Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. ArXiv, abs/2302.04761, 2023.</p>
<p>Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions. In NAACL, 2018.</p>
<p>Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop questions via single-hop question composition. TACL, 2022.</p>
<p>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In $I C L R, 2023$.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In NeurIPS, 2022.</p>
<p>Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. Break it down: A question understanding benchmark. TACL, 2020.</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In EMNLP, 2018.</p>
<p>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. In $I C L R, 2023$.</p>
<h1>A OPEN DOMAIN QA DETAILS</h1>
<h2>A. 1 RETRIEVAL CORPUSES FOR OPEN DOMAIN QA</h2>
<p>We use HotpotQA in the fullwiki setting where it comes with the associated Wikipedia corpus for open-domain QA. 2WikiMultihopQA and MuSiQue, however, are originally reading comprehension datasets. Questions in 2WikiMultihopQA and MuSiQue are associated with 10 and 20 paragraphs respectively. To turn these datasets into open-domain QA datasets, we create a corpora for each dataset by combining all the paragraphs in the train, dev and test questions. As a result we get a corpus size of 430,225 paragraphs for 2WikiMultihopQA and 139,416 for MuSiQue.</p>
<h2>A. 2 HYPERPARAMETER TUNING FOR OPEN DOMAIN QA</h2>
<p>We treat the number of paragraphs to retrieve $(K)$ in NoDecomp-Ctxt and Decomp-Ctxt models as a hyperparameter. We select it based on a grid search on a set of values to maximize performance on a held out set of 100 questions for each dataset. For NoDecomp-Ctxt, we search $K \in{6,8,10}$ for GPT3 models and $K \in 2,4,6,8$ for Flan-T5-<em> models. For Decomp-Ctxt, we search $K \in{2,4,6}$ for GPT3 and Flan-T5-</em> models. Note that the ranges are different between GPT3 and Flan-T5-* as GPT3 can fit in more number of tokens. The ranges are different for NoDecomp-Ctxt and DecompCtxt as $K$ refers to number of paragraphs retrieved in each round of retrieval, and NoDecomp-Ctxt has only one step of retrieval whereas Decomp-Ctxt usually has multiple retrieval steps.</p>
<h2>A. 3 Additional ReSults</h2>
<p><img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 13: Results on MuSiQue dataset
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 14: Results on HotpotQA dataset</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 15: Results on 2WikiMultihopQA dataset</p>
<h1>A.3.1 MuSiQUE</h1>
<p>We present all the results on the MuSiQue dataset in Fig. 13. Across all settings, we can see that retrieval helps substantially (large gains over No-Ctxt QA) with further improvements achieved by our DecomP-based Decomp-Ctxt QA model.</p>
<h2>A.3.2 HotpotQA</h2>
<p>We present all the results on the HotpotQA dataset in Fig. 14. On this dataset too, we can see large gains by incorporating retrieval but the gains from using DecomP are mostly seen in the smaller models.</p>
<h2>A.3.3 2WIKIMULTIHOPQA</h2>
<p>We present all the results on the 2WikiMultihopQA dataset in Fig. 15. On this dataset, we can see large gains by incorporating retrieval and also observe substantial gains by incorporating DecomP (as compared to NoDecomp-Ctxt).</p>
<h2>B МАТН QA</h2>
<p>We apply Decomposed Prompting to two math QA datasets: GSM8K Cobbe et al. (2021) and MultiArith Roy \&amp; Roth (2015). For Chain-of-thought, we used the original prompts for math reasoning Wei et al. (2022). For example:</p>
<p>Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today? A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been $21-15=6$. The answer is 6 .</p>
<p>Most CoT systems Wei et al. (2022); Wang et al. (2023) rely on extracting the answer by finding the number following "answer is". However, this may not always be accurate. For example, the following CoT would be unanswerable by relying on simple patterns.</p>
<p>Parker chews 4 pieces of gum a day. There are 15 pieces of gum in a pack. So he will need $4 * 30$ / $15=8$ packs of gum to last him 30 days.</p>
<p>Rather than relying on patterns with limited generalization, we can use a language model to extract the answer more reliably. Specifically, we use Decomposed Prompting to decompose the task into first identifying the chain-of-thought reasoning and then using a second GPT3-based sub-module to extract the answer from the CoT. We show examples of our prompts here (full prompt in App. G):</p>
<h2>Example from the Decomposition Prompt</h2>
<p>QC: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
QS: [cot] There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been $21-15=6$ trees planted.
QS: [gpt_ans] There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been $21-15=6$ trees planted.
A: 6
QS: [EOQ]</p>
<h1>Example from the gpt_ans prompt</h1>
<p>Q: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been $21-15=6$ trees planted.
A: 6
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 16: Our simple decomposition results in 14-17 pts on two MathQA datasets: GSM8k and MultiArith.
<img alt="img-14.jpeg" src="img-14.jpeg" /></p>
<p>Figure 17: As the models become weaker (davinci-001) and smaller (curie-001), the performance of all the models drop. DECODP still outperforms CoT till the performance reaches close to zero with curie.</p>
<p>We present our results in Fig. 16. On the GSM8K data set ${ }^{10}$, we outperform CoT by 14 points. On the MultiArith dataset ${ }^{11}$, we achieve a 17 pt improvement compare to CoT. While this is a simple change, it illustrates the possibility of using DECOMP for other complex answer types, e.g. nonextractive answer generation from chain-of-thoughts.</p>
<h2>C Effect of Scale on CommaQA</h2>
<p>We evaluate text-curie-001, text-davinci-001 and text-davinci-002 on the CommAQA dataset. Since the curie-001 and davinci-001 have a smaller context window size, we further reduced our prompts to fit within their context windows (2048 tokens). As shown in Fig. 17, both CoT and DECOMP are effected by the model size.</p>
<h2>D ReSULTS ON ALL PROMPTS</h2>
<h2>D. 1 PER-Prompt Result on Letter Concatenation</h2>
<p>We present the results of the letter concatenation task (with space delimiter) for different values of N in Fig. 18. Our results are stable across the different prompts (P1, P2 and P3) and always outperform CoT and Least-to-Most prompting.</p>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-15.jpeg" src="img-15.jpeg" /></p>
<p>Figure 18: Across all values of N and different prompts (P1, P2 and P3), DECODP outperform chain-of-thought reasoning and even least-to-most prompting.</p>
<h1>D. 2 Per-Prompt Results on CommaQA</h1>
<p>We also present the results of all the prompts on the CommAQA dataset in Fig. 19. Here too, we can observe that DECODP outperforms CoT on each prompt set.
<img alt="img-16.jpeg" src="img-16.jpeg" /></p>
<p>Figure 19: Results of different prompts on the CommAQA dataset.</p>
<h2>E Effect of Decomposition Scheme</h2>
<p>To evaluate the effect of the decomposition scheme, we experiment with two other simple decomposition structures for the letter concatenation and reversal tasks.</p>
<p>Letter Concatenation For letter concatenation, we consider an alternate scheme where we use GPT3 to generate each question rather than loop over the answers, e.g.,</p>
<table>
<thead>
<tr>
<th style="text-align: left;">QC: Take the last letters of the words in "Augusta Ada King" and concatenate them using a space.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">QS: [split] What are the words in "Augusta Ada King"?</td>
</tr>
<tr>
<td style="text-align: left;">A: ["Augusta", "Ada", "King"]</td>
</tr>
<tr>
<td style="text-align: left;">QS: [str_position] What is the last letter in "Augusta"?</td>
</tr>
<tr>
<td style="text-align: left;">A: "a"</td>
</tr>
<tr>
<td style="text-align: left;">QS: [str_position] What is the last letter in "Ada"?</td>
</tr>
<tr>
<td style="text-align: left;">A: "a"</td>
</tr>
<tr>
<td style="text-align: left;">QS: [str_position] What is the last letter in "King"?</td>
</tr>
<tr>
<td style="text-align: left;">A: "g"</td>
</tr>
<tr>
<td style="text-align: left;">QS: [merge] Concatenate ["a", "a", "g"] using a space.</td>
</tr>
<tr>
<td style="text-align: left;">A: "a a g"</td>
</tr>
<tr>
<td style="text-align: left;">QS: [EOQ]</td>
</tr>
</tbody>
</table>
<p>By using the decomposer prompt model to generate the sub-questions, we can be more robust to formatting issues in the output answers, e.g., we can expect GPT3 to still generate the appropriate sub-questions even if the first answer is not a valid array. However, the generated sub-questions may not correctly use all the elements of the list (change in order, missed element, repeated elements, etc).</p>
<p>List Reversal For list reversal, instead of splitting into halves, we take the tail of the list, reverse it and then concatenate it to the head. i.e. reverse(list) $=$ reverse(list $[1:])+$ list $[0]$. This requires more GPT3 calls $(\mathrm{O}(\mathrm{n})$ ) compared to the original approach of splitting the list into halves $(\mathrm{O}(\log (\mathrm{n}))$ ).</p>
<p>In both these cases, we noticed that the performance did not drop as shown in Fig. 20 and Fig. 21. On the letter concatenation task, the results were exactly the same. The new reversal decomposition schema was actually stronger on longer inputs at the cost of more calls to GPT3 ( $\mathrm{O}(\ln (\mathrm{n})$ ) using binary splits vs $\mathrm{O}(\mathrm{n})$ one element at a time). Both these decomposition schemes are still better than CoT.
<img alt="img-17.jpeg" src="img-17.jpeg" /></p>
<p>Figure 20: Both decomposition schemes for the letter concatenation task have the same scores.
<img alt="img-18.jpeg" src="img-18.jpeg" /></p>
<p>Figure 21: Recursively reversing the tail of a list is more stable at longer lengths but comes at the cost of more calls to GPT3.</p>
<h1>F Error Analysis</h1>
<h2>F. 1 LETTER CONCATENATION</h2>
<h2>F.1.1 DECOMP</h2>
<p>We analyzed the errors in DECOMP on the letter concatenation task and only found errors in the sub-task execution.</p>
<p><img alt="img-19.jpeg" src="img-19.jpeg" /></p>
<p>Figure 22: EM Results on the $k^{t h}$ letter concatenation task $(\mathrm{k}=3$ ) using semi-colon as delimiter with different values for N , the number of words in the input. DECOMP always outperforms and generalizes better than CoT.</p>
<p>Q: Take the letters at position 3 of the words in "Nancy Samina Abbas Caudhari Bano" and concatenate them using a space.
A: n m b u n
Prediction: c m b u n
Error: Incorrect letter extraction (Sub-task)
What is at position 3 in " $[(N, 1),(a, 2),(n, 3),(c, 4),(y, 5)] " ? \Rightarrow$ " $c$ "
Q: Take the letters at position 3 of the words in "Orlando Stephen Cho Teixeira Pierre" and concatenate them using a space.
A: 1 e o i e
Prediction: leoie
Error: Incorrect concatenation (Sub-task)
Concatenate ["l", "e", "o", "i", "e"] using a space. $\Rightarrow$ "leoie"</p>
<h1>F.1.2 COT W/ ROLLOUT</h1>
<p>We analyzed the errors in CoT on the letter concatenation task and found similar errors during the generation of CoT. But the frequency of these errors was higher than DECOMP, as it is not possible to effectively teach each sub-task with CoT.</p>
<p>Q: Take the letters at position 3 of the words in "Sheila Nicolas Verma Sha Sousa" and concatenate them using a space.
A: e c r a u
Pred: i c r a u
Error: Incorrect letter extraction
...The letters and their positions in "Sheila" are "[ $(S, 1),(h, 2),(e, 3),(i, 4),(l, 5),(a, 6)]$ ". The letter at position 3 in this sequence is " $i$ "...</p>
<p>Q:Take the letters at position 3 of the words in "Shobha Kailash Nakamura Peter Benitez" and concatenate them using a space.
A: o i k t n
Pred: o 1 k t i
Error: Incorrect letter extraction
..."Benitez" are " $[(B, 1),(e, 2),(n, 3),(i, 4),(t, 5),(e, 6),(z, 7)]$ ". The letter at position 3 in this sequence is " $i$ "...</p>
<h1>F. 2 COMMAQA</h1>
<p>Similarly in CommaQA, the errors are mostly due to sub-task errors, which in this dataset correspond to answering single-hop questions. CoT also makes the same types of errors but they are more frequent since this QA sub-task can not be delegated to a specialized prompt in CoT. Since all errors are of this type, we show only one example here.</p>
<p>Q: What awards have movies written by people born in 1933 won?
A: ["Hydrallium", "Pompasole"]
Pred: ["Pompasole"]
Error: Incorrect sub-question answer
Sub-Q:What movies has Haldron written?
Sub-A: ["Polytetrafluoromethane", "Skia", "Warpstone"]
Pred: ["Skia", "Warpstone"]</p>
<h2>G TASK PROMPTS</h2>
<p>We have provided the task prompts for all the datasets for COT and our Decomposed Prompting approach.</p>
<p>CoT Since CoT methods also perform 2-step reasoning: first generate the chain-of-thought and second extract the answer from the CoT, we use the same decomposition-based framework for COT baselines too. For example, consider the following example in our COT prompt:</p>
<p>QC: Take the letters at position 1 of the words in "Alan Mathison Turing" and concatenate them using a space.
QS: [extract] The letter at position 1 of "Alan" is "A". The letter at position 1 of "Mathison" is "M". The letter at position 1 of "Turing" is "T". Concatenating "A", "M", "T" using a space leads to "A M T". So, "Alan Mathison Turing" outputs "A M T".
A: "A M T"
QS: [EOQ]</p>
<p>GPT3 generates the chain-of-thought during the "decomposition" step and a regex-based answer extractor extract (' . * outputs " (. *) "..' ) then takes this CoT and generates the answer. In some cases, the module name is skipped in the prompt (the CoT is sent to the extractor by default).</p>
<p>Operators In this work, we use the same operators as defined by Khot et al.. Their select operator is just the basic operator that replaces references to an answer index with its answer. When not specified, select is assumed to be the default operator. In addition, we consider two operators in our experiments: project_values and project_values_flat_unique.</p>
<ul>
<li>project_values: This operator takes a list answer $# i=X$ and iterates over it to generate new questions by replacing mentions of $# i$ i.e. $\mathrm{Q}=[\mathrm{q} . \operatorname{replace}(# \mathrm{i}, \mathrm{x})$ for $\mathrm{x} \in$ X]. The answer to each question is simply concatenated to get the final answer i.e. $\mathrm{A}=$ [model(q) for $\mathrm{q} \in \mathrm{Q}$ ]. We refer to this as foreach for simplicity in the main text.</li>
<li>project_values_flat_unique: This operator performs the same steps as project_values but then additionally flattens the list and only returns the unique entities in the flattened list. We refer to this as foreach_merge in the main text for simplicity.</li>
</ul>
<h2>G. 1 LETTER CONCATENTATION</h2>
<p>We show one of the prompts used for experiments here. The entire set of prompts is provided as supplemetary material.</p>
<h1>G.1.1 Decomposed Prompting</h1>
<h2>decomp</h2>
<p>QC: Take the last letters of the words in "Augusta Ada King" and concatenate them using a space.
QS: [split] What are the words in "Augusta Ada King"?
A: ["Augusta", "Ada", "King"]
QS: (project_values) [str_position] What is the last letter in "#1"?
A: ["a", "a", "g"]
QS: [merge] Concatenate #2 using a space.
A: "a a g"
QS: [EOQ]
QC: Take the letters at position 1 of the words in "Alan Mathison Turing" and concatenate them using a space.
QS: [split] What are the words in "Alan Mathison Turing"?
A: ["Alan", "Mathison", "Turing"]
QS: (project_values) [str_position] What is the letter at position 1 in "#1"?
A: ["A", "M", "T"]
QS: [merge] Concatenate #2 using a space.
A: "A M T"
QS: [EOQ]
QC: Take the letters at position 4 of the words in "Herbert Alexander Simon" and concatenate them using a space.
QS: [split] What are the words in "Herbert Alexander Simon"?
A: ["Herbert", "Alexander", "Simon"]
QS: (project_values) [str_position] What is the letter at position 4 in "#1"?
A: ["b", "x", "o"]
QS: [merge] Concatenate #2 using a space.
A: "b x o"
QS: [EOQ]
split
Q: What are the words in "Alan Mathison Turing"?
A: ["Alan", "Mathison", "Turing"]
Q: What are the letters in "Alan"?
A: ["A", "l", "a", "n"]
Q: What are the letters and their positions in "Mathison"?
A: "[(M, 1), (a, 2), (t, 3), (h, 4), (i, 5), (s, 6), (o, 7), (n, 8)]"
Q: What are the words and their positions in "Herbert Alexander Simon"?
A: "[(Herbert, 1), (Alexander, 2), (Simon, 3)]"</p>
<h2>str_position</h2>
<p>QC: What is the letter at position 1 of the word "Augusta"?
QS: (select) [split] What are the letters and their positions in "Augusta"?
A: "[(A, 1), (u, 2), (g, 3), (u, 4), (s, 5), (t, 6), (a, 7)]"
QS: (select) [arr_position] What is at position 1 in #1?
A: "A"
QS: [EOQ]
QC: What is the last letter of the word "Mathison"?
QS: (select) [split] What are the letters and their positions in "Mathison"?
A: "[(M, 1), (a, 2), (t, 3), (h, 4), (i, 5), (s, 6), (o, 7), (n, 8)]"
QS: (select) [arr_position] What is the last letter in #1?
A: "n"</p>
<p>QS: [EOQ]
QC: What is the word at the position 4 in "Colorless green ideas sleep furiously"?
QS: (select) [split] What are the words and their positions in "Colorless green ideas sleep furiously "?
A: "[(Colorless, 1), (green, 2), (ideas, 3), (sleep, 4), (furiously, 5)]"
QS: (select) [arr_position] What is at the position 4 in #1?
A: "sleep"
QS: [EOQ]
merge
Q: Concatenate ["A", "l", "a", "n"].
A: "Alan"
Q: Concatenate ["b", "x", "o"] using a space.
A: "b x o"
Q: Concatenate ["a", "a", "g"] using a comma.
A: "a,a,g"
Q: Concatenate ["Alan", "Mathison", "Turing"] using a space.
A: "Alan Mathison Turing"
Q: Concatenate ["Allen", "Institute"].
A: "AllenInstitute"
arr_position
Q: What is at position 4 in "[("Colorless", 1), ("green", 2), ("ideas", 3), ("sleep", 4), ("furiously", 5)]"?
A: "sleep"
Q: What is at position 1 in "[(M, 1), (a, 2), (t, 3), (h, 4), (i, 5), (s, 6), (o, 7), (n, 8)]"?
A: "M"
Q: What is at the last position in "[(A, 1), (u, 2), (g, 3), (u, 4), (s, 5), (t, 6), (a, 7)]"?
A: "a"
Q: What is at position 1 in "[(Herbert, 1), (Alexander, 2), (Simon, 3)]"?
A: "Herbert"
Q: What is at last position in "[(Allen, 1), (Institute, 2), (for, 3), (Artificial, 4), (Intelligence, 5)]"?
A: "Intelligence"
Q: What is at position 4 in "[(A, 1), (l, 2), (e, 3), (x, 4), (a, 5), (n, 6), (d, 7), (e, 8), (r, 9)]"?
A: "x"</p>
<h1>G.1.2 COT WITH ROLLOUT</h1>
<h2>COT w/ rollout</h2>
<p>QC: Take the last letters of the words in "Augusta Ada King" and concatenate them using a space. QS: The words in "Augusta Ada King" are "Augusta", "Ada" and "King". The letters and their positions in "Augusta" are "[(A, 1), (u, 2), (g, 3), (u, 4), (s, 5), (t, 6), (a, 7)]". The last letter in this sequence is "a". The letters and their positions in "Ada" are "[(A, 1), (d, 2), (a, 3)]". The last letter in this sequence is "a". The letters and their positions in "King" are "[(K, 1), (i, 2), ( n, 3), (g, 4)]". The last letter in this sequence is "g". Concatenating "a", "a", "g" using a space leads to "a a g". So, "Augusta Ada King" outputs "a a g".
A: "a a g"
QS: [EOQ]</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{10}$ We randomly sample 300 examples from the test set due to costs with API usage
${ }^{11}$ We randomly sample 200 examples from the test set due to costs with API usage&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
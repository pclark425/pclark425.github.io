<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3852 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3852</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3852</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-93.html">extraction-schema-93</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <p><strong>Paper ID:</strong> paper-270560577</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2406.11657v1.pdf" target="_blank">Can LLM be a Personalized Judge?</a></p>
                <p><strong>Paper Abstract:</strong> Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally. It is therefore encouraging to see the growing interest in LLM personalization within the research community. However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity. In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge, asking LLMs to judge user preferences based on personas. Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth. The personas typically used are often overly simplistic, resulting in low predictive power. To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments. This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks. Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples. Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3852.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3852.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Standard LLM-as-Personalized-Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard Large Language Model as a Personalized Judge (no uncertainty estimation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical evaluation showing standard LLM-as-a-judge applied to personalization tasks yields low and inconsistent agreement with human ground truth, varying strongly by dataset and model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>LLM conditioned on a persona selects between two responses (binary choice), compared to users' self-reported preferences (human ground truth); both with and without allowing a tie option.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Personalization evaluation across datasets (PR, PRISM, OpinionQA, Empathetic Conversation (EC))</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4, GPT-3.5, Command R+, Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Average reported accuracy for most powerful model ~72.5% (binary, no-tie); by-dataset for GPT-4: PR 94.6%, PRISM 72.8%, OpinionQA 63.5%, EC 59.1%; per-model averages reported: Llama3 70.2%, GPT-3.5 63.8%, GPT-4 72.5%, Command R+ 71.0%</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>LLM judges struggle when persona information is sparse or not task-relevant (persona sparsity); they perform well on datasets with explicit, entailed persona signals (PR) but miss nuanced, context-dependent cues (e.g., empathy in EC where responses are similar across personas); LLMs rarely choose 'tie' even when appropriate.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Low and inconsistent agreement with humans (drops below ~60% on EC and some OpinionQA tasks); sensitivity to persona sparsity; model-dependent calibration—weaker models (GPT-3.5) perform substantially worse; known evaluation biases (positional, self-enhancement, length, prompt sensitivity) remain concerns; tie option ineffective because models rarely select it.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>On datasets with explicit persona signals (PR) LLMs (GPT-4) reach very high agreement (≈94.6%), showing that LLM judges can succeed when persona information is directly predictive.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Do not assume off-the-shelf LLM-as-judge is reliable for personalization; prioritize datasets/samples with strong persona signals and use additional mechanisms (see certainty thresholding) to filter low-confidence cases; avoid relying on tie option alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLM be a Personalized Judge?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3852.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3852.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Certainty-enhanced Judge</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-as-a-Personalized-Judge with Verbal Uncertainty Estimation (certainty thresholding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Introducing verbal certainty (1-100) from the LLM and filtering predictions by a threshold substantially increases agreement with human ground truth on retained (high-certainty) samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Same binary preference inference but LLM is asked to provide a verbal certainty score (1-100); a manual threshold (80) is used to select high-confidence predictions for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Personalization evaluation (PR, PRISM, OpinionQA, EC) using self-reported persona data</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4, Command R+ (primary successes); comparisons also include GPT-3.5 and Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>High-confidence subset accuracy ≈80%+ for powerful models (GPT-4 and Command R+); paper uses a threshold of 80 to classify high-confidence; e.g., high-confidence GPT-4 samples reach approx 80% agreement.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Certainty enables the judge to abstain implicitly (by flagging low-confidence cases), preserving quality by focusing on samples where persona predictive power exists; this recovers performance comparable to prior LLM-as-judge claims but only on a subset of samples.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Filtering by certainty reduces coverage—many samples become unscored; verbal estimation is a manual/simple strategy (authors used a fixed threshold of 80) and may be brittle; weaker models often cannot reliably self-estimate uncertainty (GPT-3.5 fails to improve accuracy on high-confidence samples); choice of threshold and verbalization scheme affects results.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>On high-certainty samples, LLM judges (GPT-4, Command R+) reach >80% agreement, matching or exceeding previously reported LLM-judge performance and exceeding third-person human performance on the same high-certainty subset.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Incorporate verbal uncertainty estimation and apply thresholding to prioritize high-confidence samples; use powerful LLMs (e.g., GPT-4); collect richer persona features to increase coverage of high-confidence cases; consider tuning the certainty threshold to control coverage vs accuracy trade-off.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLM be a Personalized Judge?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3852.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3852.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM vs Third-person Humans (crowd)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of GPT-4 judge performance to third-person crowd annotators</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Crowdsourced third-person human annotations were collected and compared to GPT-4 judgments; overall accuracies are similar, but GPT-4 outperforms human annotators on the subset of high-certainty samples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Crowdsourcing (Prolific) of 300 OpinionQA instances with three annotators per instance; majority vote forms the human answer and annotators also report certainty; compared against GPT-4 judge (with verbal certainty).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>OpinionQA personalization (inferring persona responses)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4 (compared to human annotators)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Overall: GPT-4 62.3% vs human majority-vote 63.3%; on high-certainty samples: GPT-4 79.2% vs human 71.4%; inter-annotator mean agreement 0.597 (std 0.087) from bootstrap.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Human annotators show variability and some annotators perform worse than majority vote; GPT-4's certainty-filtered judgments produce a cleaner (higher-accuracy) subset than raw crowd annotations; humans may be inconsistent on difficult personalization inferences.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Overall accuracy for both LLM and third-person humans is modest—crowd annotators still struggle in many personalization cases; majority vote improves human results but remains imperfect compared to first-person ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>GPT-4 achieves comparable overall accuracy to third-person humans and exceeds human accuracy on high-certainty samples, supporting LLMs as a scalable proxy when first-person labels are unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>When first-person labels are not available, consider LLM judges with certainty thresholding as a scalable proxy; still aim to collect first-person annotations when possible as the gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLM be a Personalized Judge?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3852.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3852.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Persona Sparsity Effects</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Persona Sparsity and its impact on LLM-as-Judge performance</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical ablation shows that reducing the number or relevance of persona features drastically lowers LLM judges' certainty and the number of high-confidence (and accurate) predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Ablation experiments feeding LLM judges with full persona features vs only three important features vs only one (least important) feature; tracking change in certainty distribution and high-confidence counts.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>OpinionQA and PRISM personalization evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4, Command R+</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Not a single summary accuracy; reported effect: for OpinionQA GPT-4 high-confidence sample count dropped from 480 (16.0%) with all features to 10 (0.3%) with one feature; PRISM showed a smaller drop (e.g., GPT-4 high-confidence samples from 120 to 92) because some tasks have large inherent response differences.</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Availability and relevance of persona variables strongly determine whether the judge can make reliable, fine-grained inferences; when persona is sparse, LLMs appropriately lower certainty and abstain, but coverage suffers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Persona sparsity is a major cause of unreliability—many personalization instances lack informative persona signals, producing low coverage/high-abstention when using certainty filtering; datasets with entailed signals (PR) can mask this problem.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>LLMs tend to assign low confidence correctly when persona information is insufficient, which is a desirable failure mode (abstention instead of wrong confident predictions).</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Collect richer, task-relevant persona attributes to improve judge coverage; use certainty-aware filtering to avoid evaluating samples where persona is uninformative; interpret high performance on entailed datasets (like PR) cautiously as they may not reflect true personalization ability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLM be a Personalized Judge?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3852.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3852.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge evaluations and human evaluations, including agreement rates, qualitative differences, limitations, failure cases, and any aspects that are lost or degraded when using LLMs as judges instead of humans.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Model Variability & Calibration Issues</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inter-model variability, calibration, and failure modes in LLM judges</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different LLMs show large performance and calibration differences: GPT-4 and Command R+ can self-assess uncertainty usefully, while GPT-3.5 and some open models (Llama3-70B) show narrow or unreliable certainty distributions and poorer accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_setting</strong></td>
                            <td>Cross-model comparisons of accuracy and the distribution/utility of verbalized certainty scores across datasets; examination of calibration degradation after preference tuning referenced in background.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_domain</strong></td>
                            <td>Personalization evaluation across PR, PRISM, OpinionQA, EC</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_name</strong></td>
                            <td>GPT-4, Command R+, GPT-3.5, Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_rate</strong></td>
                            <td>Per-model averages: Llama3 ≈70.2%, GPT-3.5 ≈63.8%, GPT-4 ≈72.5%, Command R+ ≈71.0%; weaker models fail to improve accuracy on high-confidence subsets (GPT-3.5).</td>
                        </tr>
                        <tr>
                            <td><strong>qualitative_differences</strong></td>
                            <td>Weaker models produce concentrated/narrow certainty ranges (Llama3) or poor calibration (GPT-3.5), limiting the usefulness of their verbal uncertainty; stronger models exhibit a clearer monotonic relation between self-reported certainty and correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Calibration can degrade after preference tuning; some LLMs exhibit biases (positional, self-enhancement, length) and are sensitive to prompting; non-English performance and cultural gaps not evaluated and likely worse; models rarely select tie option, limiting that strategy.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexamples_or_strengths</strong></td>
                            <td>Powerful LLMs (GPT-4, Command R+) both perform better and provide more informative uncertainty estimates, enabling effective thresholding and higher-accuracy subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>recommendations_or_best_practices</strong></td>
                            <td>Prefer higher-capacity, well-calibrated models for judging; use verbal uncertainty instead of raw token-level probabilities; validate per-model calibration and test multilingual/cultural generalization before deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation</strong></td>
                            <td>Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Can LLM be a Personalized Judge?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Judging LLM-as-a-judge with MT-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback <em>(Rating: 2)</em></li>
                <li>Large language models are inconsistent and biased evaluators <em>(Rating: 2)</em></li>
                <li>Can language models recognize convincing arguments? <em>(Rating: 1)</em></li>
                <li>The PRISM alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3852",
    "paper_id": "paper-270560577",
    "extraction_schema_id": "extraction-schema-93",
    "extracted_data": [
        {
            "name_short": "Standard LLM-as-Personalized-Judge",
            "name_full": "Standard Large Language Model as a Personalized Judge (no uncertainty estimation)",
            "brief_description": "Empirical evaluation showing standard LLM-as-a-judge applied to personalization tasks yields low and inconsistent agreement with human ground truth, varying strongly by dataset and model capacity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "LLM conditioned on a persona selects between two responses (binary choice), compared to users' self-reported preferences (human ground truth); both with and without allowing a tie option.",
            "task_or_domain": "Personalization evaluation across datasets (PR, PRISM, OpinionQA, Empathetic Conversation (EC))",
            "llm_model_name": "GPT-4, GPT-3.5, Command R+, Llama3-70B",
            "agreement_rate": "Average reported accuracy for most powerful model ~72.5% (binary, no-tie); by-dataset for GPT-4: PR 94.6%, PRISM 72.8%, OpinionQA 63.5%, EC 59.1%; per-model averages reported: Llama3 70.2%, GPT-3.5 63.8%, GPT-4 72.5%, Command R+ 71.0%",
            "qualitative_differences": "LLM judges struggle when persona information is sparse or not task-relevant (persona sparsity); they perform well on datasets with explicit, entailed persona signals (PR) but miss nuanced, context-dependent cues (e.g., empathy in EC where responses are similar across personas); LLMs rarely choose 'tie' even when appropriate.",
            "limitations_or_failure_cases": "Low and inconsistent agreement with humans (drops below ~60% on EC and some OpinionQA tasks); sensitivity to persona sparsity; model-dependent calibration—weaker models (GPT-3.5) perform substantially worse; known evaluation biases (positional, self-enhancement, length, prompt sensitivity) remain concerns; tie option ineffective because models rarely select it.",
            "counterexamples_or_strengths": "On datasets with explicit persona signals (PR) LLMs (GPT-4) reach very high agreement (≈94.6%), showing that LLM judges can succeed when persona information is directly predictive.",
            "recommendations_or_best_practices": "Do not assume off-the-shelf LLM-as-judge is reliable for personalization; prioritize datasets/samples with strong persona signals and use additional mechanisms (see certainty thresholding) to filter low-confidence cases; avoid relying on tie option alone.",
            "citation": "Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)",
            "uuid": "e3852.0",
            "source_info": {
                "paper_title": "Can LLM be a Personalized Judge?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Certainty-enhanced Judge",
            "name_full": "LLM-as-a-Personalized-Judge with Verbal Uncertainty Estimation (certainty thresholding)",
            "brief_description": "Introducing verbal certainty (1-100) from the LLM and filtering predictions by a threshold substantially increases agreement with human ground truth on retained (high-certainty) samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Same binary preference inference but LLM is asked to provide a verbal certainty score (1-100); a manual threshold (80) is used to select high-confidence predictions for evaluation.",
            "task_or_domain": "Personalization evaluation (PR, PRISM, OpinionQA, EC) using self-reported persona data",
            "llm_model_name": "GPT-4, Command R+ (primary successes); comparisons also include GPT-3.5 and Llama3-70B",
            "agreement_rate": "High-confidence subset accuracy ≈80%+ for powerful models (GPT-4 and Command R+); paper uses a threshold of 80 to classify high-confidence; e.g., high-confidence GPT-4 samples reach approx 80% agreement.",
            "qualitative_differences": "Certainty enables the judge to abstain implicitly (by flagging low-confidence cases), preserving quality by focusing on samples where persona predictive power exists; this recovers performance comparable to prior LLM-as-judge claims but only on a subset of samples.",
            "limitations_or_failure_cases": "Filtering by certainty reduces coverage—many samples become unscored; verbal estimation is a manual/simple strategy (authors used a fixed threshold of 80) and may be brittle; weaker models often cannot reliably self-estimate uncertainty (GPT-3.5 fails to improve accuracy on high-confidence samples); choice of threshold and verbalization scheme affects results.",
            "counterexamples_or_strengths": "On high-certainty samples, LLM judges (GPT-4, Command R+) reach &gt;80% agreement, matching or exceeding previously reported LLM-judge performance and exceeding third-person human performance on the same high-certainty subset.",
            "recommendations_or_best_practices": "Incorporate verbal uncertainty estimation and apply thresholding to prioritize high-confidence samples; use powerful LLMs (e.g., GPT-4); collect richer persona features to increase coverage of high-confidence cases; consider tuning the certainty threshold to control coverage vs accuracy trade-off.",
            "citation": "Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)",
            "uuid": "e3852.1",
            "source_info": {
                "paper_title": "Can LLM be a Personalized Judge?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "LLM vs Third-person Humans (crowd)",
            "name_full": "Comparison of GPT-4 judge performance to third-person crowd annotators",
            "brief_description": "Crowdsourced third-person human annotations were collected and compared to GPT-4 judgments; overall accuracies are similar, but GPT-4 outperforms human annotators on the subset of high-certainty samples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Crowdsourcing (Prolific) of 300 OpinionQA instances with three annotators per instance; majority vote forms the human answer and annotators also report certainty; compared against GPT-4 judge (with verbal certainty).",
            "task_or_domain": "OpinionQA personalization (inferring persona responses)",
            "llm_model_name": "GPT-4 (compared to human annotators)",
            "agreement_rate": "Overall: GPT-4 62.3% vs human majority-vote 63.3%; on high-certainty samples: GPT-4 79.2% vs human 71.4%; inter-annotator mean agreement 0.597 (std 0.087) from bootstrap.",
            "qualitative_differences": "Human annotators show variability and some annotators perform worse than majority vote; GPT-4's certainty-filtered judgments produce a cleaner (higher-accuracy) subset than raw crowd annotations; humans may be inconsistent on difficult personalization inferences.",
            "limitations_or_failure_cases": "Overall accuracy for both LLM and third-person humans is modest—crowd annotators still struggle in many personalization cases; majority vote improves human results but remains imperfect compared to first-person ground truth.",
            "counterexamples_or_strengths": "GPT-4 achieves comparable overall accuracy to third-person humans and exceeds human accuracy on high-certainty samples, supporting LLMs as a scalable proxy when first-person labels are unavailable.",
            "recommendations_or_best_practices": "When first-person labels are not available, consider LLM judges with certainty thresholding as a scalable proxy; still aim to collect first-person annotations when possible as the gold standard.",
            "citation": "Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)",
            "uuid": "e3852.2",
            "source_info": {
                "paper_title": "Can LLM be a Personalized Judge?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Persona Sparsity Effects",
            "name_full": "Persona Sparsity and its impact on LLM-as-Judge performance",
            "brief_description": "Empirical ablation shows that reducing the number or relevance of persona features drastically lowers LLM judges' certainty and the number of high-confidence (and accurate) predictions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Ablation experiments feeding LLM judges with full persona features vs only three important features vs only one (least important) feature; tracking change in certainty distribution and high-confidence counts.",
            "task_or_domain": "OpinionQA and PRISM personalization evaluations",
            "llm_model_name": "GPT-4, Command R+",
            "agreement_rate": "Not a single summary accuracy; reported effect: for OpinionQA GPT-4 high-confidence sample count dropped from 480 (16.0%) with all features to 10 (0.3%) with one feature; PRISM showed a smaller drop (e.g., GPT-4 high-confidence samples from 120 to 92) because some tasks have large inherent response differences.",
            "qualitative_differences": "Availability and relevance of persona variables strongly determine whether the judge can make reliable, fine-grained inferences; when persona is sparse, LLMs appropriately lower certainty and abstain, but coverage suffers.",
            "limitations_or_failure_cases": "Persona sparsity is a major cause of unreliability—many personalization instances lack informative persona signals, producing low coverage/high-abstention when using certainty filtering; datasets with entailed signals (PR) can mask this problem.",
            "counterexamples_or_strengths": "LLMs tend to assign low confidence correctly when persona information is insufficient, which is a desirable failure mode (abstention instead of wrong confident predictions).",
            "recommendations_or_best_practices": "Collect richer, task-relevant persona attributes to improve judge coverage; use certainty-aware filtering to avoid evaluating samples where persona is uninformative; interpret high performance on entailed datasets (like PR) cautiously as they may not reflect true personalization ability.",
            "citation": "Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)",
            "uuid": "e3852.3",
            "source_info": {
                "paper_title": "Can LLM be a Personalized Judge?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Model Variability & Calibration Issues",
            "name_full": "Inter-model variability, calibration, and failure modes in LLM judges",
            "brief_description": "Different LLMs show large performance and calibration differences: GPT-4 and Command R+ can self-assess uncertainty usefully, while GPT-3.5 and some open models (Llama3-70B) show narrow or unreliable certainty distributions and poorer accuracy.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_setting": "Cross-model comparisons of accuracy and the distribution/utility of verbalized certainty scores across datasets; examination of calibration degradation after preference tuning referenced in background.",
            "task_or_domain": "Personalization evaluation across PR, PRISM, OpinionQA, EC",
            "llm_model_name": "GPT-4, Command R+, GPT-3.5, Llama3-70B",
            "agreement_rate": "Per-model averages: Llama3 ≈70.2%, GPT-3.5 ≈63.8%, GPT-4 ≈72.5%, Command R+ ≈71.0%; weaker models fail to improve accuracy on high-confidence subsets (GPT-3.5).",
            "qualitative_differences": "Weaker models produce concentrated/narrow certainty ranges (Llama3) or poor calibration (GPT-3.5), limiting the usefulness of their verbal uncertainty; stronger models exhibit a clearer monotonic relation between self-reported certainty and correctness.",
            "limitations_or_failure_cases": "Calibration can degrade after preference tuning; some LLMs exhibit biases (positional, self-enhancement, length) and are sensitive to prompting; non-English performance and cultural gaps not evaluated and likely worse; models rarely select tie option, limiting that strategy.",
            "counterexamples_or_strengths": "Powerful LLMs (GPT-4, Command R+) both perform better and provide more informative uncertainty estimates, enabling effective thresholding and higher-accuracy subsets.",
            "recommendations_or_best_practices": "Prefer higher-capacity, well-calibrated models for judging; use verbal uncertainty instead of raw token-level probabilities; validate per-model calibration and test multilingual/cultural generalization before deployment.",
            "citation": "Dong, Y.R., Hu, T., Collier, N., Can LLM be a Personalized Judge?, 2024 (arXiv:2406.11657)",
            "uuid": "e3852.4",
            "source_info": {
                "paper_title": "Can LLM be a Personalized Judge?",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback",
            "rating": 2,
            "sanitized_title": "just_ask_for_calibration_strategies_for_eliciting_calibrated_confidence_scores_from_language_models_finetuned_with_human_feedback"
        },
        {
            "paper_title": "Large language models are inconsistent and biased evaluators",
            "rating": 2,
            "sanitized_title": "large_language_models_are_inconsistent_and_biased_evaluators"
        },
        {
            "paper_title": "Can language models recognize convincing arguments?",
            "rating": 1,
            "sanitized_title": "can_language_models_recognize_convincing_arguments"
        },
        {
            "paper_title": "The PRISM alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models",
            "rating": 2,
            "sanitized_title": "the_prism_alignment_project_what_participatory_representative_and_individualised_human_feedback_reveals_about_the_subjective_and_multicultural_alignment_of_large_language_models"
        }
    ],
    "cost": 0.013320499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Can LLM be a Personalized Judge?
17 Jun 2024</p>
<p>Yijiang River Dong 
University of Cambridge</p>
<p>Tiancheng Hu 
University of Cambridge</p>
<p>Nigel Collier 
University of Cambridge</p>
<p>Can LLM be a Personalized Judge?
17 Jun 202404BDA1C2386CAF3DD5AE9367B7C05950arXiv:2406.11657v1[cs.CL]
Ensuring that large language models (LLMs) reflect diverse user values and preferences is crucial as their user bases expand globally.It is therefore encouraging to see the growing interest in LLM personalization within the research community.However, current works often rely on the LLM-as-a-Judge approach for evaluation without thoroughly examining its validity.In this paper, we investigate the reliability of LLM-as-a-Personalized-Judge-asking LLMs to judge user preferences based on personas.Our findings suggest that directly applying LLM-as-a-Personalized-Judge is less reliable than previously assumed, showing low and inconsistent agreement with human ground truth.The personas typically used are often overly simplistic, resulting in low predictive power.To address these issues, we introduce verbal uncertainty estimation into the LLM-as-a-Personalized-Judge pipeline, allowing the model to express low confidence on uncertain judgments.This adjustment leads to much higher agreement (above 80%) on high-certainty samples for binary tasks.Through human evaluation, we find that the LLM-as-a-Personalized-Judge achieves comparable performance to third-party humans evaluation and even surpasses human performance on high-certainty samples.Our work indicates that certainty-enhanced LLM-as-a-Personalized-Judge offers a promising direction for developing more reliable and scalable methods for evaluating LLM personalization.Our code is available at https://github.com/dong-river/Personalized-Judge.</p>
<p>Introduction</p>
<p>As large language models (LLMs) gain widespread adoption among global users with diverse backgrounds, it is imperative to ensure these models designed to reflect their values and preferences (Sorensen et al., 2024;Kirk et al., 2024).*</p>
<p>Equal contribution</p>
<p>However, the current alignment process often assumes a homogeneous set of human preferences and ignores individual perspectives, even in context-dependent, subjective tasks (Santurkar et al., 2023).Therefore, efforts have been made to fine-tune LLMs to encode individual preferences or enhance role-playing capabilities (Jang et al., 2023;Shao et al., 2023;Occhipinti et al., 2023;Li et al., 2024a;Andukuri et al., 2024) with "LLMas-a-Judge" as the main evaluation metric (Zheng et al., 2023), often without adequate validation.</p>
<p>Despite "LLM-as-a-Judge" showing high agreement with human annotators in many tasks, its effectiveness for personalization tasks remains largely unscrutinized.MT-Bench (Zheng et al., 2023) includes a role-playing component but only considered simplistic personas, such as "imagine you are a doctor," without addressing more complex personas that encompass demographics, user descriptions, and prior interactions-settings increasingly employed in recent research.Furthermore, a persona description may not always be contextually relevant.Knowing that someone is a doctor, for instance, provides little insight into their favorite types of beverages.We refer to this issue as the persona sparsity issue. 1n this paper, we examine the validity of LLMas-a-Judge for personalization, where the objective is to generate personalized outputs based on a given user persona (see Figure 1).We assess performance on tasks where ground truth data is available, including PRISM (Kirk et al., 2024), OpinionQA (Santurkar et al., 2023), Public Reddit (Staab et al., 2024), and Empathetic Conversation (Omitaomu et al., 2022).To address the issue of persona sparsity, we then propose a verbal uncertainty estima-Figure 1: Overall workflow of Personalized Judge.Given a subjective question and two distinct responses, we ask an LLM to infer the preference of a real user based on a user persona.We also ask the LLM to estimate its certainty level in this prediction.The inferred preference is then compared against the user's self-reported ground truth to evaluate the performance of the Judge.</p>
<p>tion component into the Judge pipeline.By articulating its own certainty levels, an LLM can assign lower certainty to samples for which it perceives insufficient predictive power.Additionally, we conduct a crowdsourcing experiment and compare the performance of LLM-as-a-Personalized-Judge to third-person human evaluation.</p>
<p>Our findings are as follows: (1) Contrary to previous assumptions, standard LLM-as-a-Judge is not sufficiently reliable for personalization tasks, showing only around 70% agreement with human judgments in binary choice scenarios, and dropping below 60% for certain tasks.(2) We identify persona sparsity as a major factor contributing to this unreliability.To address this, we introduce verbal uncertainty estimation into the LLM-as-Personalized-Judge process and achieve above 80% performance in high-certainty samples.(3) In a crowdsourcing experiment, we find that LLM-as-a-Personalized-Judge achieves performance comparable to thirdperson 2 human judgment and even surpasses human performance on high-certainty samples.While first-person human evaluation from diverse backgrounds remains the gold standard for personalization, in the absence of such annotations, LLM-asa-Personalized-Judge with certainty thresholding could serve as an effective and scalable alternative.</p>
<p>2 Here, first-person evaluation refers to judgments made by the individuals for whom the personalization is intended, reflecting their own preferences and values.Third-person evaluation involves external annotators who assess the personalization based on persona descriptions rather than personal preferences.</p>
<p>Background and Related Work</p>
<p>Personalization in machine learning refers to the process of tailoring a model's output to suit the unique preferences, needs, and behaviors of individual users (see Fan and Poole (2006) for an in-depth discussion).This concept is at the core of recommender systems (Sarwar et al., 2001), and been explored in various contexts in NLP, such as dialogue system (Li et al., 2016;Zhang et al., 2018), summarization (Díaz and Gervás, 2007;Yan et al., 2011), user profiling and computational sociolinguistics (Nguyen et al., 2016).These studies typically aim to understand the diverse linguistic patterns of users from varying backgrounds and contexts and to integrate persona information to enhance task performance.For surveys on these topics, see Flek (2020); Hovy and Yang (2021); Yang et al. (2024).</p>
<p>In the context of LLMs, personalization has become even more critical due to the vast, diverse, and ever-growing user base.The necessity to align LLMs to a pluralistic set of user needs is discussed in (Sorensen et al., 2024).However, the current alignment processes typically assume a single set of human preferences and researchers are just beginning to explore methods to address the varied preferences and values of different users, either at the collective level (Conitzer et al., 2024;Klingefjord et al., 2024) or at individual level (Salemi et al., 2023;Gao et al., 2024;Li et al., 2024b;Jang et al., 2023;Wang et al., 2023).Our study focuses on the evaluation aspect of personalized alignment approaches.</p>
<p>A challenging issue in this domain is the definition of personas.Not all variables are univer-sally applicable or useful (Hu and Collier, 2024).For instance, while knowing an individual's profession as a doctor may offer some insights about this individual, it does not necessarily inform us about their preferred types of beverages.Ideally, we would include demographic, behavioral, and contextual factors that are relevant to the specific task at hand.However, defining the relevant set of variables a priori is inherently difficult.Additionally, even if surveys are designed to gather this information, acquiring such detailed information on a large scale is often impractical and can frequently result in incomplete responses.We refer to this challenge as the persona sparsity issue.In practice, this means that in some cases, we cannot reasonably infer preferences based on the available persona information and should therefore deprioritize such samples.This motivates us to explore verbal uncertainty estimation as a method to filter out cases where persona information is insufficient for the LLM-Judge to make well-informed judgments.</p>
<p>Evaluation of LLMs Evaluating natural language generation (NLG) systems is challenging, but the evaluation of LLMs arguably presents even greater difficulties.This is due to the advanced capabilities and versatility of current-generation LLMs, as well as the diverse ways in which they are employed in practice.</p>
<p>Recently, "LLM-as-Judge" (Zheng et al., 2023) is introduced as a versatile and reference-free evaluation metric that shows high agreement with human annotators on various NLP tasks.Despite concerns over issue such as positional bias, self-enhancement bias, length bias, sensitivity to prompting, and cost (Zheng et al., 2023;Stureborg et al., 2024;Wu and Aji, 2023;Verga et al., 2024;Kim et al., 2024), it is becoming the new paradigm for LLM evaluation (Dubois et al., 2024;Shankar et al., 2024;Liu et al., 2024), and have been used in LLM personalization works such as Shao et al. (2023); Andukuri et al. (2024).However, there is little work in validating LLM-asa-Personalized-Judge.While MT-Bench (Zheng et al., 2023) included a role-playing component, it focused only on simplistic cases such as roleplaying specific professions and did not account for the complex personas typically used in LLM personalization works, encompassing diverse demographics, user descriptions, and prior interactions.Our work considers more realistic cases of LLM-as-a-Personalized-Judge and carefully examines its validity.</p>
<p>Calibration of LLMs Pretrained LLMs are wellcalibrated but preference tuning can degrade this calibration (Kadavath et al., 2022;Achiam et al., 2023).Recent studies have shown that verbalized confidence levels in LLMs are typically more reliable than token-level confidence scores (Tian et al., 2023).Additionally, LLMs possess some intrinsic capabilities to assess the answerability of questions (Kadavath et al., 2022;Yin et al., 2023).Building on these findings, our research introduces uncertainty quantification within the context of LLM-as-Personalized-Judge.</p>
<p>Methodology</p>
<p>In this work, we study LLM-as-a-Personalized-Judge (Figure 1), building on Zheng et al. (2023).We condition an LLM witha persona profile, which can include information ranging from demographic data and socio-behavioral indicators to free-form user descriptions, as well as any other pertinent details that could enrich the persona profile.Using this conditioned persona, we task the LLM with selecting the preferred response to a subjective question in a binary choice setting, aiming to reflect the preferences that the persona would likely have.As is done in Zheng et al. (2023), we also consider a setting where a "tie" option is allowed.</p>
<p>As highlighted in Section 2, persona sparsity can lead to instances where an LLM struggles to assess certain questions accurately.However, we hypothesize that the LLM possesses some notion of its uncertainty in these instances.Therefore, we instruct the LLM to estimate the certainty in its answer.The overall workflow is shown in Figure 1.The prompts used are detailed in Appendix A.7, while the experimental setups are described in Section 4.2.OpinionQA (Santurkar et al., 2023), built using Pew Research's American Trends Panel, contains ticipants' responses, and their demographics.Empathetic Conversation (EC) (Omitaomu et al., 2022) consists of 1000 essay responses (both empathy score and textual response) to a news article with their demographics and self-reported personality traits.It further includes dialog interactions between paired participants, enriched with various dialog annotations, such as other-reported empathy levels and turn-by-turn emotion ratings.Personal Reddit (PR) (Staab et al., 2024) consists of 500 samples of Reddit posts with their (anonymized) personal attributes, such as location, income, and sex.Unlike other datasets, it is specifically designed to test the ability of LLMs to infer explicit persona attributes.For example, if a post mentions "I remember watching Twin Peaks after coming home from school", given that Twin Peaks aired from 1990 to 1991, one could reason that the author of the post is now in the age group of 45-50.Other datasets require annotators to complete tasks and questionnaires, where persona variables may influence responses indirectly but do not explicitly reveal persona information.</p>
<p>Experimental Setup</p>
<p>LLM as Personalized Judge</p>
<p>As shown in Figure 1, given a persona, we instruct the LLM to infer the preferred response of the persona.We have three settings: (1) Standard LLMas-a-Personalized-Judge: In this setting, the model is directed to make a preference judgment based on the persona, similar to in Zheng et al. ( 2023).</p>
<p>(2) Standard LLM-as-a-Personalized-Judge with Verbal Uncertainty Estimation.In addition to (1), we add an instruction for the model to estimate its certainty in the task on a scale of 1 to 100.(3) Standard LLM-as-a-Personalized-Judge with a Tie Option.In this setting, we introduce a third option, allowing the model to indicate a tie.In this case, we do not permit the model to express verbal uncertainty.</p>
<p>We study the performance of GPT-4 (Achiam et al., 2023), GPT-3.5 (OpenAI, 2023), Command R+ (Cohere, 2024), andLLama3 70B (Meta, 2024).For generation with all models, we use nucleus sampling (Holtzman et al., 2020) with top-p of 0.95 and temperature of 0.7.For LLama3 70B, we load the model in 16 bit.In cases when the model reject to answer the question or fail to follow the formatting instruction, We ask the model regenerate at most 4 times until we can parse the results.For details on our experimental setups for each dataset, please refer to Appendix A.2.</p>
<p>Results</p>
<p>LLM-based Personalized Judge shows low agreement with human</p>
<p>In Table 1, we present the agreement between different LLM judges and the human ground truth.The results indicate that, for binary preference choice questions where random guessing would yield an accuracy of 50%, the average accuracy of the LLM-as-a-Personalized-Judge, even for the most powerful model, is only 72.5%.This accuracy is significantly lower than the 80+% agreement reported in Zheng et al. ( 2023), and it drops to around 60% for challenging tasks such as EC and OpinionQA.These findings suggest that LLM judges are less reliable for personalization tasks compared to simpler role-playing tasks.</p>
<p>Accuracy also varies substantially across different tasks and LLMs.PR is the easiest task, with all models performing best on this dataset; for instance, GPT-4 achieves an accuracy of 94.6%.This high performance is likely attributable to the dataset's design, where one response explicitly reflects certain persona characteristics while the other does not.Thus, PR may not represent genuine personalization.For example, if a persona includes a statement like "I enjoy outdoor activities," and one response is "I love hiking," while the other is "I prefer watching movies indoors," the distinction is clear.Hence, PR may not reflect personalization; rather, it can be reviewed as a task akin to instruction-following and textual entailment.</p>
<p>Conversely, EC appears to be the most difficult, with all models achieving less than 60% accuracy.This may be because the persona included lacks sufficient predictive power for the task.The articles in EC are chosen to elicit empathetic responses, which are generally very negative and lead to similar responses from different individuals.</p>
<p>Among different models, GPT-4 consistently performs the best across nearly all tasks, followed by Command R+ and Llama-3 70B.In contrast, GPT-3.5 shows substantially worse performance.</p>
<p>When models are allowed to choose a tie option, similar trends are observed.While model performance on both PRISM and EC declines, the drop is much more significant for EC.This is because models rarely choose the tie option even when it is available.Therefore, we suggest that incorporating a tie option in practical applications is not ideal.Conceptually, using tie options to filter samples is Table 2: Agreement for high and low confidence for different models."High" and "low" refers to the certainty level estimated by the model.The number of correct answers/total number of samples are provided below the accuracy.In our analysis, we use a certainty threshold of 80 to classify responses as high confidence.The italicized numbers indicate that very few samples are available for accuracy calculation.not as flexible as having the model express its confidence since we can choose different thresholds to control the number of samples being filtered.Additionally, for PR and OpinionQA, we do not add a tie option because we do not have ground truth data for ties.</p>
<p>Certainty estimation improves Personalized Judge</p>
<p>In Figure 2, we plot the accuracy of predictions across different certainty levels for various models and tasks.Some models, such as Llama3, exhibit a highly concentrated distribution of certainty levels within a narrow range, while others display a more Gaussian-like distribution, which is arguably more ideal.We observe a clear trend indicating that predictions from more powerful LLMs (e.g.GPT-4) with higher certainty scores are more likely to be correct.In contrast, less powerful LLMs, (e.g.GPT-3.5),often struggle to accurately quantify their uncertainty.This observation suggests that we can rely, at least to some degree, on a model's self-assessed confidence to evaluate whether the information in the persona is sufficient for making reliable predictions.</p>
<p>We manually assign a threshold of 80 for all models to classify a sample as high-confidence and we show in Table 2 the judge performance for each model under this certainty thresholding.High confidence samples from GPT-4 and Command R+ can achieve approximately 80% agreement with human ground-truth, on par with Zheng et al. (2023).</p>
<p>LLM-as-a-Personalized-Judge performance varies greatly across models We also observe substantial performance differences across models.As shown in Table 1, GPT-4 is the most powerful model followed by Command R+ and then LLama-3 70B.The performance of GPT-3.5 is significantly worse, with a 5%-10% performance gap on average.More importantly, GPT-3.5 and LLama-3 70B's capacity to self-estimate certainty    Crowd annotators assess the preferences of individuals based on specific profile descriptions, and these assessments are compared with the GPT-4 powered LLM-as-a-Personalized-Judge.For each sample, three annotators provide annotations, and the final human answer is determined by a simple majority vote.</p>
<p>is significantly worse.As shown in Table 2 and Figure 2, GPT-3.5 fails to achieve higher accuracy on high-confidence samples.LLama-3 70B has slightly better certainty estimation than GPT-3.5 but is still far from GPT-4 and Command R+ which achieve 80%+ accuracy on high-confidence samples.Given these results, we focus the rest of our experiment and discussion primarily on GPT-4 and Command R+.</p>
<p>Confidence distribution as a proxy of task and sample difficulty In Table 2, we observe significant variation in the number of samples categorized under high and low confidence across different tasks.We hypothesize that this variation corresponds to the difficulty of the tasks.For example, as shown in Table 1, PR is the most straightforward task based on high average accuracy for most models while EC poses significant challenges for all models.Thus, as shown in Table 2 and Figure 2, on the PR dataset, around 50% of the prediction by GPT-4 and nearly 100% predictions by Command R+ is considered high confidence, much higher than the PRISM and OpinionQA datasets, which has only around 10% -20% high confidence samples.On the contrary, only around 1% of the predictions on EC are considered high-confidence.This result illustrates that on more difficult tasks, LLMs are able to assign low confidence for a larger number of predictions, supporting our hypothesis that the an LLM's confidence judgment can be a reliable indicator of task difficulty and persona sparsity.We believe this is a crucial property to have for an LLM-Judge: in personalization tasks, end users may not always be aware of the difficulty level of a given task for all samples.They can therefore rely on the model's confidence as a surrogate measure.When evaluating a personalization task using an LLM-as-a-Personalized-Judge, users should prioritize high-confidence samples, as these are more likely to reflect accurate and reliable judgments.Implementing a confidence threshold can facilitate more meaningful comparisons between methods of personalization in future evaluations.</p>
<p>Certainty significantly drops when only very few persona features are given In real-world applications, the availability of persona variables can vary, and it is important to observe how the model's confidence changes with both the quantity and relevance of these variables.To explore this, we conduct an ablation study to further verify that LLMs would indeed assign low confidence to the predictions when the persona is insufficiently predictive.We provide different numbers of persona variables to the LLM-Judge.While the precise predictive power of a persona is hard to quantify, fewer features should lead to lower confidence in LLM predictions.Concretely, instead of using all features as before, we provide the LLM with only three important features (education, location, ethnicity) or one less important feature (religion) for OpinionQA and PRISM.</p>
<p>For OpinionQA, in Table 3, we find that GPT-4 assigns low confidence to much fewer samples when only three or one features are provided.Specifically, the number of high-confidence samples drop from 480 (16.0%) to 10 (0.3%) for the one-feature case.For Command R+, since the number of high-confidence samples is already very low,   it remains relatively unchanged.Figure 3 provides a more detailed analysis of the change in certainty distribution when providing a different number of persona variables.</p>
<p>For PRISM, as shown in Table 4, we observe a similar trend, albeit with fewer changes compared to OpinionQA.We attribute this to the significant inherent quality differences between the two response options provided for each question in the dataset.In many instances, these differences are so pronounced that the preferred choice remains evident regardless of the persona (as indicated in 8).Consequently, the number of high-confidence samples predicted by GPT-4 only slightly decreased (from 120 to 92) from full-feature to one-feature, while maintaining a high accuracy on these highconfidence samples.</p>
<p>LLM-as-a-Personalized Judge achieves comparable performance as third-person human judge In dialog system personalization, third-person human annotation is a widely adopted evaluation strategy.Typically, this involves human crowd annotators inferring the prefrences of personas of others rather than expressing their own opinions.Although this is considered a gold standard, its effectiveness scenarios remain underexplored.</p>
<p>For our evaluation, we use the OpinionQA dataset and collect crowd annotations via Prolific.We sample 300 instances, with each instance receiving annotations from three different human annotators, totaling 30 samples per annotator.Annotators infer how a persona would respond in specific scenarios and rate their certainty levels, using the same prompts as the LLMs.We establish the final human answer based on a simple majority vote, and average the certainty levels of the majority answers to establish ground truth certainty.The crowd-sourced results are presented in Table 5.The overall accuracy of GPT-4 was 62.3%, closely matching the human-level accuracy of 63.3%.On high-certainty samples, GPT-4 achieved an accuracy of 79.2%, surpassing the human performance level of 71.4%.These results corroborate findings by Rescala et al. (2024) which suggests that LLMs can match human performance in evaluating whether arguments are likely to resonate with individuals characterized by specific persona attributes.</p>
<p>To further validate the reliability of the crowd judgments, we conducted bootstrap sampling 1,000 times with 30 samples each, performing random draws without replacement of the annotations.The mean agreement between two annotators is 0.597, with a standard deviation of 0.087, indicating a reasonably high level of internal consistency in our results.Additionally, we provide the unaggregated annotation results in Table 7.Here, human performance was inferior to the majority vote, likely reflecting variations in annotators' skills.Our human annotation results also underscore the inherent challenges in personalization evaluation.While first-person annotations can be considered ground truth and are therefore always accurate, even thirdperson human judges often struggle to reach correct judgments in many cases.</p>
<p>Our crowd-sourcing exercise indicates that LLMs when used as personalized judges, can achieve accuracy levels comparable to those of human annotators.However, under the default setting, the overall accuracy remains low, likely due to persona sparsity issues.When certainty thresholding is applied, LLMs achieve better accuracy on highcertainty samples than human annotators.While we advocate for the collection of more first-person datasets-where individuals provide information about themselves and then answer questions-we also propose that LLMs, with certainty thresholding, represent a promising and scalable alternative for evaluating personalization tasks in the absence of first-person data.</p>
<p>Conclusion</p>
<p>In this paper, we formalized and examined the validity of LLM-as-a-Personalized-Judge.Contrary to previous assumptions, we demonstrated that the standard LLM-as-a-Judge setting is not sufficiently reliable for personalization tasks, showing low agreement with human ground truth.We identified persona sparsity as a major cause of this unreliability.We then introduced verbal certainty estimation and found that powerful LLMs (e.g.GPT-4) are capable of effectively assessing the certainty of their own responses.This led to the observation that high-certainty samples indeed exhibit high accuracy (80%).We additionally conducted a human annotation experiment and found that LLM-as-a-Personalized-Judge achieves comparable accuracy as third-person human judge and surpasses humans on high-certainty samples.While we advocate for the collection of more first-person personalization data, we also believe that a certainty-aware LLMas-a-Personalized-Judge is a promising proxy for evaluation, particularly in cases first-person pref-erence data are not available, provided that personas are as fine-grained as possible.We hope our work helps the community recognize the challenges in evaluating LLM personalization and ultimately leads to the development of LLMs that better serve each individual's preferences and needs.</p>
<p>Limitations</p>
<p>The availability of diverse and comprehensive datasets for evaluating LLM personalization remains limited, and such datasets are predominantly available in English.Consequently, we cannot make conclusive statements about the performance of LLMs as personalized judges in non-English languages.Furthermore, existing multilingual LLMs often exhibit cultural gaps (Liu et al., 2023), which suggests that their performance might be suboptimal in non-English contexts due to the complex cultural associations tied to persona variables.Future research should aim to compile and utilize more extensive datasets with richer and more varied persona attributes in a multilingual setting to better evaluate and improve LLM personalization.</p>
<p>Although numerous methods for quantifying uncertainty in LLMs have been proposed, we opted to use direct verbal estimation.This method is straightforward and has better performance compared to the model's conditional probability (Tian et al., 2023).Although a comprehensive evaluation of existing uncertainty estimation in LLMas-a-Personalized-Judge would make a valuable contribution for future work, it is beyond the scope of the present work, which is mostly focused on the integration of uncertainty estimation into LLMs-as-Personalized-Judge as a framework.</p>
<p>Ethical Considerations</p>
<p>The goal of the LLM-as-a-Personalized-Judge is to enhance personalization in LLMs to better serve a diverse global community.However, achieving this goal necessitates a rigorous adherence to ethical principles throughout the research and production phases.For example, personalization should always remain an opt-in choice for end users, ensuring user autonomy and consent without any adverse consequences for those who opt out.Additionally, LLMs have been shown to have various kinds of social biases (Liang et al., 2021;Hu et al., 2023;Liu et al., 2022, inter alia), some of which may exhibit itself during the LLM-as-a-Judge process.We need to be mindful of such biases not to reinforce the bias and stereotypes encoded in the LLMs.Privacy concerns become especially salient when personal information is utilized to fine-tune or condition models.It is crucial to manage such data responsibly by obtaining explicit user consent and adhering to data protection regulations, such as the General Data Protection Regulation.In our research, we have relied on existing publicly available datasets, which have undergone institutional review board approval and anonymization prior to release.</p>
<p>A Appendix</p>
<p>A.1 LLM Model Details</p>
<p>For GPT-4o, we use gpt-4o-2024-05-13.For GPT-3.5-turbo, we use gpt-3.5-turbo-0125.</p>
<p>A.2 Experiment Details</p>
<p>For experiments on PRISM, we run the experiments on the first 1,000 samples from the utterance subset of the dataset.We only consider the first turn in each conversation.As suggested by Kirk et al. (2024), we consider the two responses with scores smaller or equal to 10 to be a tie.For setting (1) and (2), we only consider samples that is not deemed a tie.For (3), we include those tie samples as well.To mitigate positional bias, we randomly shuffle the position of the two responses.</p>
<p>To mitigate self-enhancement bias (preferring text generated by itself) (Zheng et al., 2023), we filter out the responses that are generated by the same LLM as the Judge.We also filter out the responses that refuse to answer the question.This is because different LLMs have different safety constraints and different rejection ratios but humans typically find the LLM rejection undesirable and assign low scores to it.For experiments on OpinionQA, we randomly select one binary choice question from each of the 15 topics covered by OpinionQA.For each question, we randomly select 200 respondent's answers.</p>
<p>For experiments on EC, we only consider the essay response part of the dataset.We select two responses to a news article, and let the LLM to infer which is written by a user with a specific persona.We ran 500 samples in total.We consider two essay responses to be a tie if the difference in their empathy score or distress score is smaller than 2. Since most responses are similar in score and are considered as tie, we control the ratio of the tie cases in EC to be the same as the ratio in PRISM which is around 20% in setting 3).</p>
<p>For experiments on PR, since no user responds to the same question, we need to provide a questionresponse pair and let the LLM to infer which response is likely written by the target user.Concretely, for each persona-question-response triple, we select the most similar persona to the target user based on cosine similarity computed by the all-MiniLM-L6-v23 model from Sentence Transformer (Reimers and Gurevych, 2019), the backbone of which is a MiniLM model (Wang et al., 2020).Then take this user's response to another question to form another persona-questionresponse triple and let the LLM infer which response is written by the target user.</p>
<p>A.3 Persona Variables Used for Each Task</p>
<p>In Table 6, we show the persona variables we used for each task.</p>
<p>A.4 Crowdsourcing Details</p>
<p>We recruited 30 U.S. annotators via Prolific.For quality control purposes, each annotator was required to have completed a minimum of 50 prior crowd tasks with an approval rating of at least 99%.We applied the quota sample feature from Prolific to ensure that the gender and political affiliation distribution among annotators was balanced.We restricted the age of the participants to be between 18 and 75 years old.Annotators were compensated at the rate of $13.5 per hour.This study received approval from an institutional ethics review board.</p>
<ol>
<li>1
1
DatasetsPRISM(Kirk et al., 2024) is a participatory, representative, and individualized human feedback dataset.It encompasses feedback on over 8,000 conversations, gathered from 1,500 participants across 75 countries.Additionally, the dataset is enriched with detailed participant profiles.</li>
</ol>
<p>Figure 2 :
2
Figure 2: Distribution of LLM verbal certainty score and the corresponding accuracy.The plots show the certainty distribution and corresponding accuracy of correct (blue) and wrong (orange) answers for LLAMA3-70B, Command R+, and GPT-4 models on PR, PRISM, OpinionQA, and EC datasets.Each plot provides overall accuracy (ACC), high certainty accuracy (High Certainty ACC), and low certainty accuracy (Low Certainty ACC).The top of each bar shows the accuracy within that certainty bin.The certainty levels are truncated to be between 40 and 90 by adjusting values outside this range.</p>
<p>Table 1 :
1
Agreement between different LLM judges with the human ground truth from PRISM, OpinionQA, and EC.FollowingZheng et al. (2023), we report two cases for the judge: with tie and without tie.The agreement between two random judges under each setup is denoted as "R=".Average is calculated as the direct (non-weighted) average of accuracy across the datasets.Due to the unavailability of relevant data in the PR and OpinionQA datasets, we thus omit them for the with tie setting.
SetupNo Tie (R=50%)With Tie (R=33%)ModelLlama3 GPT-3.5 GPT-4 Command R+ Llama3 GPT-3.5 GPT-4 Command R+PR0.9490.7960.9460.964----PRISM0.7220.6560.7280.7200.6780.5370.7270.689OpinionQA 0.6290.5690.6350.616----EC0.5070.5290.5910.5410.3760.3840.4170.430Average0.7020.6380.7250.7100.5270.4610.5720.560ModelLlama 3GPT-3.5GPT-4Command R+ConfidenceHighLowHighLowHighLowHighLowPR0.9481.0000.7920.6670.9420.9500.9580.976(492/520)(5/5)(375/473)(34/51)(228/243)(266/280)(345/361)(160/164)PRISM0.7530.6250.6730.5990.9080.7030.8930.690(570/758)(150/240)(520/773)(135/227)(108/120)(612/871)(133/149)(587/852)OpinionQA0.7060.5660.5680.5780.8040.6021.0000.616(964/1366)(928/1640)(1641/2890)(58/102)(385/480)(1526/2535)(2/2)(1856/3013)EC0.5040.5480.5300.5171.0000.588-0.541(240/478)(16/31)(250/472)(14/29)(4/4)(295/502)(0/0)(276/510)</p>
<p>Table 3 :
3
Ablation study on using different features of the user persona to predict the user preference on Opin-ionQA.The italicized numbers indicate that too few samples are used to compute the accuracy.
ModelGPT-4Command R+ConfidenceHighLowHighLowAll Features0.9080.7030.8930.690(108/120)(612/871)(133/149)(160/164)Three Imp. Features0.9040.6800.7370.653(104/115)(594/873)(225/305)(454/696)Least Imp. Feature0.8800.710.7800.644(81/92)(642/903)(166/214)(506/787)</p>
<p>Table 4 :
4
Ablation study on using different features of the user persona to predict the user preference on PRISM.
MethodGPT-4Third Person Human JudgeConfidenceHighLowHighLowAll Features0.7920.5920.7140.620(38/48)(149/252)(30/42)(160/258)Overall Average0.623 (187/300)0.633 (190/300)</p>
<p>Table 5 :
5
Third-person human evaluation on OpinionQA:</p>
<p>Findings of the Association for Computational Linguistics: ACL 2023, pages 8653-8665, Toronto, Canada.Association for Computational Linguistics.Yinhe Zheng, Rongsheng Zhang, Minlie Huang, and Xiaoxi Mao.2020.A pre-training based personalized dialogue generation model with persona-sparse data.Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):9693-9700.
Saizheng Zhang, Emily Dinan, Jack Urbanek, ArthurSzlam, Douwe Kiela, and Jason Weston. 2018. Per-sonalizing dialogue agents: I have a dog, do youhave pets too? In Proceedings of the 56th AnnualMeeting of the Association for Computational Lin-guistics (Volume 1: Long Papers), pages 2204-2213,Melbourne, Australia. Association for ComputationalLinguistics.Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, SiyuanZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,Joseph E. Gonzalez, and Ion Stoica. 2023. JudgingLLM-as-a-judge with MT-bench and chatbot arena.In Thirty-seventh Conference on Neural InformationProcessing Systems Datasets and Benchmarks Track.
Our use of the term "persona sparsity" diverges from works like Zheng et al. (2020);Song et al. (2021). While they typically refer to the scarcity of naturalistic dialog data directly reflecting persona variables, we highlight a related but distinct problem: the available persona variables may not offer an informed prior about the person involved for a specific task.
https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
AcknowledgementsT.H is supported by Gates Cambridge Trust (grant OPP1144 from the Bill &amp; Melinda Gates Foundation).This work was partially performed using resources provided by the Cambridge Service for Data Driven Discovery (CSD3) operated by the University of Cambridge Research Computing Service (www.csd3.cam.ac.uk), provided by Dell EMC and Intel using Tier-2 funding from the Engineering and Physical Sciences Research Council (capital grant EP/T022159/1), and DiRAC funding from the Science and Technology Facilities Council (www.dirac.ac.uk).We are grateful for support received in the form of research access or credits from Cohere and OpenAI.We thank Yinhong Liu, Ivan Vulić, Songbo Hu, and Fabian David Schmidt for helpful feedback and discussions at various stages of the project.A.6 Number of Persona Variables Provided Influence Certainty DistributionIn Table3, we show the effect of using different number of persona variables on the certainty distribution.We observe that, on OpinionQA, GPT-4 and Command R+ show clear drop in confidence when fewer persona variables.On PRISM, since the quality difference is so large that the preference can be inferred regardless of the persona, only minimal change occurred to the certainty distribution.A.7 Prompts for LLM-as-a-Personalized-JudgeIn Figure5and Figure4, we include the prompts that we used for PRISM.For other datasets, minor modifications are made to the prompt to fit the dataset.Prompt for PRISM (without tie)Given the user profile provided below, select the response from AI assistant A or B that the user would most likely prefer.Declare your choice by using the format: "[[A]]" if you believe assistant A's response is more suitable, or "[[B]]" if assistant B's response is better suited.Additionally, assess your confidence in this decision by assigning a certainty level from 1 to 100.Use the following guidelines to assign the certainty level:1-20 (Uncertain): The user profile provides insufficient or minimal evidence.The decision is largely based on weak or indirect hints.For other datasets, including EC, PR, and OpinionQA, minor modifications are made to the prompt to adapt to the specific requirements of each dataset.21-40 (Moderately
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Chinmaya Andukuri, Jan-Philipp Fränken, Tobias Gerstenberg, Noah D Goodman, ArXiv:2403.19154STaR-GATE: Teaching Language Models to Ask Clarifying Questions. 2024arXiv preprint</p>
<p>Introducing command r+: A scalable llm built for business. Cohere, 2024</p>
<p>Social choice for ai alignment: Dealing with diverse human feedback. Vincent Conitzer, Rachel Freedman, Jobst Heitzig, Wesley H Holliday, Bob M Jacobs, Nathan Lambert, Milan Mossé, Eric Pacuit, Stuart Russell, Hailey Schoelkopf, arXiv:2404.102712024arXiv preprint</p>
<p>User-model based personalized summarization. Information Processing &amp; Management. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, Tatsunori B Hashimoto, 10.1016/j.ipm.2007.01.009Advances in Neural Information Processing Systems. 2024. 200736Alpacafarm: A simulation framework for methods that learn from human feedback. Text Summarization</p>
<p>What is personalization? perspectives on the design and implementation of personalization in information systems. Haiyan Fan, Marshall Scott Poole, 10.1080/10919392.2006.9681199Journal of Organizational Computing and Electronic Commerce. 163-42006</p>
<p>Returning the N to NLP: Towards contextually personalized classification models. Lucie Flek, 10.18653/v1/2020.acl-main.700Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational Linguistics2020</p>
<p>Aligning llm agents by learning latent preference from user edits. Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, Dipendra Misra, arXiv:2404.152692024arXiv preprint</p>
<p>The curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, International Conference on Learning Representations. 2020</p>
<p>The importance of modeling social factors of language: Theory and practice. Dirk Hovy, Diyi Yang, 10.18653/v1/2021.naacl-main.49Proceedings of the 2021 Conference of the North American Chapter. the 2021 Conference of the North American ChapterOnline. Association for Computational Linguistics2021</p>
<p>Quantifying the Persona Effect in LLM Simulations. Tiancheng Hu, Nigel Collier, ArXiv:2402.108112024arXiv preprint</p>
<p>Tiancheng Hu, Yara Kyrychenko, Steve Rathje, Nigel Collier, Jon Sander Van Der Linden, Roozenbeek, arXiv:2310.15819Generative language models exhibit social identity biases. 2023arXiv preprint</p>
<p>Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu, arXiv:2310.11564Personalized soups: Personalized large language model alignment via post-hoc parameter merging. 2023arXiv preprint</p>
<p>Language models (mostly) know what they know. Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova Dassarma, Eli Tran-Johnson, arXiv:2207.05221arXiv:2405.01535Prometheus 2: An open source language model specialized in evaluating other language models. Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, Minjoon Seo, 2022. 2024arXiv preprint</p>
<p>The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models. Rose Hannah, Alexander Kirk, Paul Whitefield, Andrew Röttger, Katerina Bean, Juan Margatina, Rafael Ciro, Max Mosquera, Adina Bartolo, He Williams, He, arXiv:2404.160192024arXiv preprint</p>
<p>Oliver Klingefjord, Ryan Lowe, Joe Edelman, arXiv:2404.10636What are human values, and how do we align ai to them?. 2024arXiv preprint</p>
<p>Cheng Li, Mengzhou Chen, Jindong Wang, arXiv:2402.10946Sunayana Sitaram, and Xing Xie. 2024a. Culturellm: Incorporating cultural differences into large language models. arXiv preprint</p>
<p>A persona-based neural conversation model. Jiwei Li, Michel Galley, Chris Brockett, Georgios Spithourakis, Jianfeng Gao, Bill Dolan, 10.18653/v1/P16-1094Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics20161</p>
<p>Personalized Language Modeling from Personalized Human Feedback. Xinyu Li, Zachary C Lipton, Liu Leqi, ArXiv:2402.051332024barXiv preprint</p>
<p>Towards understanding and mitigating social biases in language models. Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov, Proceedings of the 38th International Conference on Machine Learning. the 38th International Conference on Machine LearningPMLR2021139</p>
<p>Are multilingual llms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings. Cecilia Chen, Fajri Liu, Timothy Koto, Iryna Baldwin, Gurevych, arXiv:2309.085912023arXiv preprint</p>
<p>Quantifying and alleviating political bias in language models. Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Soroush Vosoughi, 10.1016/j.artint.2021.103654Artificial Intelligence. 3041036542022</p>
<p>Aligning with human judgement: The role of pairwise preference in large language model evaluators. Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, Nigel Collier, arXiv:2403.169502024Preprint</p>
<p>Introducing meta llama 3: The most capable openly available llm to date. Meta. 2024</p>
<p>Computational Sociolinguistics: A Survey. Dong Nguyen, A Seza, Carolyn P Dogruöz, Franciska Rosé, Jong De, 10.1162/COLI_a_00258Computational Linguistics. 4232016</p>
<p>Prodigy: a profile-based dialogue generation dataset. Daniela Occhipinti, Serra Sinem Tekiroglu, Marco Guerini, arXiv:2311.051952023arXiv preprint</p>
<p>Damilola Omitaomu, Shabnam Tafreshi, Tingting Liu, Sven Buechel, Chris Callison-Burch, Johannes Eichstaedt, Lyle Ungar, João Sedoc, arXiv:2205.12698Empathic conversations: A multi-level dataset of contextualized conversations. 2022arXiv preprint</p>
<p>Sentence-BERT: Sentence embeddings using Siamese BERTnetworks. 10.18653/v1/D19-1410Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational Linguistics2023. 2019OpenAIIntroducing ChatGPT. Nils Reimers and Iryna Gurevych</p>
<p>Can language models recognize convincing arguments?. Paula Rescala, Horta Manoel, Tiancheng Ribeiro, Robert West, arXiv:2404.007502024arXiv preprint</p>
<p>LaMP: When Large Language Models Meet Personalization. Alireza Salemi, Sheshera Mysore, Michael Bendersky, Hamed Zamani, ArXiv:2304.114062023arXiv preprint</p>
<p>Whose opinions do language models reflect?. Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, Tatsunori Hashimoto, International Conference on Machine Learning. PMLR2023</p>
<p>Item-based collaborative filtering recommendation algorithms. Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, 10.1145/371920.372071Proceedings of the 10th International Conference on World Wide Web, WWW '01. the 10th International Conference on World Wide Web, WWW '01New York, NY, USAAssociation for Computing Machinery2001</p>
<p>Who validates the validators? aligning llmassisted evaluation of llm outputs with human preferences. Shreya Shankar, Björn Zamfirescu-Pereira, Aditya G Hartmann, Ian Parameswaran, Arawjo, arXiv:2404.122722024arXiv preprint</p>
<p>Yunfan Shao, Linyang Li, Junqi Dai, Xipeng Qiu, ArXiv:2310.10158Character-LLM: A Trainable Agent for Role-Playing. 2023arXiv preprint</p>
<p>BoB: BERT over BERT for training persona-based dialogue models from limited personalized data. Haoyu Song, Yan Wang, Kaiyan Zhang, Wei-Nan Zhang, Ting Liu, 10.18653/v1/2021.acl-long.14Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language ProcessingOnline. Association for Computational Linguistics20211</p>
<p>Taylor Sorensen, Jared Moore, Jillian Fisher, Mitchell Gordon, Niloofar Mireshghallah, Christopher Michael Rytting, Andre Ye, Liwei Jiang, Ximing Lu, Nouha Dziri, Tim Althoff, Yejin Choi, ArXiv:2402.05070A Roadmap to Pluralistic Alignment. 2024arXiv preprint</p>
<p>Beyond memorization: Violating privacy via inference with large language models. Robin Staab, Mark Vero, 2024ICLRMislav Balunović, and Martin Vechev</p>
<p>Large language models are inconsistent and biased evaluators. Rickard Stureborg, Dimitris Alikaniotis, Yoshi Suhara, arXiv:2405.017242024arXiv preprint</p>
<p>Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. Katherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn, Christopher Manning, 10.18653/v1/2023.emnlp-main.330Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational Linguistics2023</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, arXiv:2404.187962024arXiv preprint</p>
<p>Learning Personalized Story Evaluation. Danqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng Yang, Andrew Cohen, Lei Li, Yuandong Tian, ArXiv:2310.033042023arXiv preprint</p>
<p>Minilm: Deep selfattention distillation for task-agnostic compression of pre-trained transformers. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou, Advances in Neural Information Processing Systems. Curran Associates, Inc202033</p>
<p>Style over substance: Evaluation biases for large language models. Minghao Wu, Alham Fikri, Aji , arXiv:2307.030252023arXiv preprint</p>
<p>Summarize what you are interested in: An optimization framework for interactive personalized summarization. Rui Yan, Jian-Yun Nie, Xiaoming Li, Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. the 2011 Conference on Empirical Methods in Natural Language ProcessingEdinburgh, Scotland, UKAssociation for Computational Linguistics2011</p>
<p>Diyi Yang, Dirk Hovy, David Jurgens, Barbara Plank, arXiv:2405.02411The call for socially aware language technologies. 2024arXiv preprint</p>
<p>Do large language models know what they don't know?. Zhangyue Yin, Qiushi Sun, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Xuanjing Huang, 10.18653/v1/2023.findings-acl.5512023</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-853 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-853</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-853</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-260681466</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2308.03427v2.pdf" target="_blank">TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage</a></p>
                <p><strong>Paper Abstract:</strong> With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications. Despite their prowess, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks which necessitate a combination of task planning and the usage of external tools. In this paper, we first propose a structured framework tailored for LLM-based AI Agents and discuss the crucial capabilities necessary for tackling intricate problems. Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process. Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications. Our study emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e853.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e853.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large dialogue-oriented transformer language model from OpenAI used via API; evaluated in this paper for planning, single-tool SQL/math generation, and end-to-end multi-tool usage within LLM-based agent frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based generative LLM accessed via API; used with prompting, chain-of-thought (CoT) and in-context examples inside agent frameworks (one-step and sequential).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>200B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Mathematical questions (paper-specific eval)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>90% accuracy (Table 10)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>TPTU end-to-end multi-tool evaluation (planning + tool use), plus planning/tool-subtask pair planning and SQL generation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; tool use; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>TPTU-OA end-to-end: 50% success (Table 11); TPTU-SA end-to-end: 55% (Table 11). Tool-subtask pair planning: 75% (Table 5). Simple SQL: 90% (Table 8). Complex nested SQL: 80% direct / 80% CoT (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Transformer LLM used with external tool interface (SQL & Python generators) inside agent architectures (One-step and Sequential); evaluated with CoT prompting and structured prompt formats.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only in experiments (in-context learning, CoT); model itself pre-trained and instruction fine-tuned externally (not by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy; agent design</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Prompt engineering: unified tool-subtask pair dictionary format; CoT vs direct guidance prompts for nested SQL; agent design: One-step (TPTU-OA) vs Sequential (TPTU-SA) agents to expose intermediate feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Unified tool-subtask pair format improved planning performance over separate tool+subtask generation (overall +52.9% reported across models). For ChatGPT specifically, tool-subtask pair planning 75% (Table 5) vs tool+subtask separate 55% (Table 4) (≈+20 pp), and sequential agent planning improved ChatGPT from 75% → 80% (Table 7) and end-to-end from 50% → 55% (Table 11) (+5 pp).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors attribute gap to: LLMs' generative training not directly teaching tool invocation and stateful multi-step execution; difficulties in formatting structured outputs; lack of grounding/execution feedback handling; inability to reliably track and incorporate intermediate tool results; and insufficient flexibility when asked to generate full global plans.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e853.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e853.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Claude (Anthropic)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large safety-focused LLM from Anthropic evaluated for planning and SQL/math/tool-subtask tasks in the agent framework; shows strong performance on planning and SQL generation in paper's evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Claude</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large transformer-based LLM prioritizing honesty/safety; used via API with prompting and in-context demonstrations inside agent frameworks.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>>52B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Mathematical questions (paper-specific eval)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>85% accuracy (Table 10)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool-subtask pair planning; SQL generation; sequential planning (TPTU-SA) evaluations</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; tool use; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Tool-subtask pair planning: 90% (Table 5). TPTU-SA planning: 100% (Table 7). Simple SQL: 100% (Table 8). Complex nested SQL: 100% both direct and CoT (Table 9). End-to-end multi-tool end-to-end evaluation not included (not API-based in that stage).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Transformer LLM used inside agent framework with tool descriptions; evaluated with CoT prompting and structured prompting.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only in experiments (in-context learning, CoT); model itself pre-trained and instruction-fine-tuned externally (not by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy; agent design</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Used CoT and direct guidance prompts for nested SQL; applied unified tool-subtask pair prompting and sequential agent design (TPTU-SA) to expose outcomes between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Claude shows near-perfect performance on SQL and planning tasks; sequential agent planning reached 100% (Table 7). The unified tool-subtask pairing improved performance in planning evaluations (overall +52.9% reported across models).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>While Claude performed strongly on planning and SQL, the paper suggests performance gaps generally arise from lack of grounding/execution feedback and difficulty producing strictly-formatted structured outputs in some models; these general causes apply across models.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e853.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e853.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>InternLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>InternLM (Shanghai AI Lab)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multilingual transformer LLM (120B) evaluated via API for planning, SQL, and mathematical code generation within the agent frameworks; shows mid-to-high performance on many tool tasks but trailing ChatGPT/Claude on end-to-end multi-tool use.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>InternLM</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large multilingual transformer LLM (120B) with multiround dialogue capability and long-context understanding, used via API with prompts and CoT where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>120B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Mathematical questions (paper-specific eval)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>95% accuracy (Table 10)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>TPTU end-to-end multi-tool evaluation; planning and tool-subtask pair planning; SQL generation</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; tool use; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>TPTU-OA end-to-end: 15% (Table 11); TPTU-SA end-to-end: 20% (Table 11). Tool-subtask pair planning: 55% (Table 5). TPTU-SA planning: 65% (Table 7). Simple SQL: 90% (Table 8). Complex nested SQL: 60% direct / 50% CoT (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Transformer LLM with long-context dialogue strengths; used with tool interface and CoT prompting in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only in experiments (in-context learning, CoT); model pre-trained previously (not by this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>agent design; prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Evaluated both one-step and sequential agent designs; tested CoT vs direct guidance for nested SQL; used unified tool-subtask pair prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Sequential agent (TPTU-SA) improved planning accuracy vs one-step (Tool-subtask planning 55% → 65% in Table 5 → Table 7); end-to-end multi-tool improved from 15% → 20% (Table 11). CoT prompting reduced performance for InternLM on nested SQL (60% direct → 50% CoT per Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper suggests InternLM's decrease under CoT may indicate difficulties managing dependencies/continuity in stepwise problem solving, and general issues (formatting, grounding, intermediate-result incorporation) explain gaps between QA-like tasks and interactive tool use.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e853.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e853.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ziya-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ziya (IDEA) 13B</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 13B bilingual LLaMA-derived model evaluated for planning and SQL/math tasks; shows mixed ability—reasonable on some single-tool tasks but weaker in planning and end-to-end tool usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Ziya-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-derived pre-trained transformer model (13B) with bilingual capabilities, used with prompting and in-context examples in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Mathematical questions (paper-specific eval)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>50% accuracy (Table 10)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool-subtask pair planning; SQL generation; end-to-end multi-tool evaluation (limited)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; tool use; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Tool-subtask pair planning: 20% (Table 5). TPTU-SA planning: 10% (Table 7). End-to-end multi-tool: 0% (Table 11). Simple SQL: 50% (Table 8). Complex nested SQL: 50% direct / 40% CoT (Table 9).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Transformer LLM (LLaMA-derived) used with prompt-based tool interface; no additional architectural modules introduced in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only in experiments (in-context learning, CoT); model pre-trained and fine-tuned externally.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy; agent design</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Evaluated unified tool-subtask pair prompts and sequential agent; CoT vs direct guidance for nested SQL.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Unified tool-subtask pairing modestly improved planning vs separate generation but overall performance remained low; CoT reduced nested SQL accuracy compared to direct for Ziya (50% → 40%). Sequential agent did not improve end-to-end performance (TPTU-SA planning 10%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors note the model struggles with sequential continuity and complex planning; hypothesized causes include poor handling of structured output formats, difficulty aligning tools and subtasks, and limited abilities to incorporate intermediate tool outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e853.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e853.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGLM-130B (Tsinghua University)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A bilingual dialogue-optimized LLM (GLM architecture) evaluated on SQL, math code generation, and planning tasks; shows selective strengths (better with CoT for SQL) but weaknesses in mathematical code and structured planning outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ChatGLM-130B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>GLM-architecture bilingual dialogue model (130B in table) designed for Chinese/English Q&A; evaluated with prompts and CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>130B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Mathematical questions (paper-specific eval)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>0% accuracy (Table 10) for mathematical code generation (noted severe weakness in math code generation)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>SQL generation and planning (tool-subtask planning), nested SQL with CoT</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Simple SQL: 30% (Table 8). Complex nested SQL: 60% direct / 70% CoT (Table 9) — model prefers CoT. Tool-subtask pair planning: 0% (Table 5). TPTU-SA planning: 0% (Table 7). End-to-end multi-tool evaluation not included.</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>GLM-based dialogue model used with prompt-based tool interfaces; benefits from CoT prompting for nested SQL.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only in experiments (in-context learning, CoT); model pre-trained/fine-tuned externally.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>CoT prompting for nested SQL improved performance compared to direct guidance; tested unified tool-subtask pair prompts (but model struggled to produce required format).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>CoT-based prompting increased complex SQL accuracy (60% → 70% for ChatGLM in Table 9); however, ChatGLM failed on mathematical code generation and on structured tool-subtask planning formats.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Paper suggests ChatGLM benefits from stepwise reasoning prompts (CoT) for SQL, but lacks reliable tool-subtask output formatting and struggles to ground generated code for execution; overall gap due to format production and grounding limitations.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e853.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e853.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chinese-Alpaca-Plus</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chinese-Alpaca-Plus (LLaMA-derived, 33B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 33B LLaMA-derived bilingual model fine-tuned with expanded Chinese tokens and instruction data; evaluated for SQL, math, and planning tasks and shows uneven strengths (some math ability but poor SQL and planning).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Chinese-Alpaca-Plus</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>LLaMA-derived LLM (33B) with expanded Chinese vocabulary and instruction fine-tuning; used via prompts and CoT in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>33B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td>Mathematical questions (paper-specific eval)</td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td>55% accuracy (Table 10)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>SQL generation; planning/tool-subtask pair planning; end-to-end multi-tool (included in some studies)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use; planning; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Simple SQL: 20% (Table 8). Complex nested SQL: 0% direct / 0% CoT (Table 9). Tool-subtask pair planning: 5% (Table 5). TPTU-SA planning: 0% (Table 7). End-to-end multi-tool: 0% (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>LLaMA-derived transformer with instruction fine-tuning and expanded Chinese tokens, used in prompts within agent framework.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>prompting only in experiments; model pre-trained and fine-tuned externally.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>prompting strategy; agent design</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Evaluated CoT vs direct guidance for nested SQL and unified tool-subtask pair prompts; sequential agent setup tested.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Performed moderately on math but poorly on SQL/planning; sequential agent did not improve planning performance (remained low), and end-to-end tool usage failed (0%).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors suggest limited ability to generate correct structured outputs and to ground intermediate execution results; specialized SQL generation and multi-step tool use remain failure modes for this model.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e853.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e853.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPTU-OA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TPTU One-step Agent (TPTU-OA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>One-step agent design proposed in this paper that instructs the LLM to produce a global plan (sequence of tools and subtasks) in a single pass; evaluated for tool-order planning, tool-subtask generation and multi-tool execution.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>TPTU-OA (One-step Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent architecture that prompts an LLM to produce a complete global decomposition of the task (tool sequence and subtask descriptions) in a single inference step, without iterative tool-execution feedback between steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool planning and multi-tool usage experiments (global planning approach)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>planning; tool use; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Tool order planning accuracy high for top models (e.g., ChatGPT/Claude 100% Table 3); tool+subtask separate generation lower (ChatGPT 55% Table 4); tool-subtask pair generation in one-step (Table 5) varied (ChatGPT 75%, Claude 90%, InternLM 55%). End-to-end multi-tool success: ChatGPT 50%, InternLM 15%, Ziya/Chinese-Alpaca 0% (Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Global planning (single-pass), requires producing fully-formed structured outputs (tool lists, subtask descriptions) without intermediate execution feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change; prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>One-step global decomposition vs sequential; multiple prompt formats tested (separate tool + subtask generation, unified tool-subtask pair dictionary format); evaluated robustness to unrelated tools in prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>One-step planning produced lower accuracy when tools and subtasks generated separately; unified tool-subtask pair format improved one-step planning substantially (paper reports +52.9% improvement overall compared to separate generation). However, one-step agents were less flexible and sometimes produced misaligned or brittle plans compared to sequential agents.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>One-shot global planning depends on strong global understanding early and is more brittle because errors require regeneration; lacks iterative feedback so cannot adapt to intermediate tool outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e853.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e853.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TPTU-SA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TPTU Sequential Agent (TPTU-SA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Sequential agent design proposed in this paper that iteratively solves one subtask at a time, requesting the next subtask after each tool execution; shown to improve planning and end-to-end tool usage for multiple LLMs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>TPTU-SA (Sequential Agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Agent architecture that decomposes tasks into a sequence of sub-tasks and iteratively invokes tools, using outputs from prior steps as context for subsequent planning and execution.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Tool-subtask pair generation and end-to-end multi-tool usage with iterative feedback</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>sequential decision-making; planning; tool use; multi-step reasoning</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Improved planning accuracy vs one-step for several models (e.g., ChatGPT tool-subtask planning 80% Table 7 vs 75% Table 5; Claude 100% vs 90%). End-to-end multi-tool success modestly improved (ChatGPT 50% → 55%; InternLM 15% → 20% Table 11).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>Iterative planning loop; uses intermediate outputs as context; supports updating plans and error correction after each step.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>architectural change (agent design); prompting strategy</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Sequential design allows the LLM to see the result of executed subtasks before planning the next step; used recursion to generate {tool: subtask} pairs one at a time and incorporate feedback; combined with structured prompts (tool-subtask dict-format).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Sequential agent generally outperformed one-step agent on planning and end-to-end use for high-performing LLMs (example improvements: ChatGPT planning 75% → 80%, end-to-end 50% → 55%; InternLM planning 55% → 65%, end-to-end 15% → 20%). Authors report reasons: stepwise feedback, error correction and richer context.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Sequential approach reduces brittleness by allowing feedback and correction; authors argue many LLM failures in interactive tasks stem from lack of iterative grounding and inability to process intermediate execution outputs.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Toolformer: Language models can teach themselves to use tools. <em>(Rating: 2)</em></li>
                <li>WebGPT: Browser-assisted question-answering with human feedback. <em>(Rating: 2)</em></li>
                <li>ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases. <em>(Rating: 2)</em></li>
                <li>API-Bank: A benchmark for tool-augmented LLMs. <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models. <em>(Rating: 2)</em></li>
                <li>ART: Automatic multi-step reasoning and tool-use for large language models. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-853",
    "paper_id": "paper-260681466",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (OpenAI)",
            "brief_description": "A large dialogue-oriented transformer language model from OpenAI used via API; evaluated in this paper for planning, single-tool SQL/math generation, and end-to-end multi-tool usage within LLM-based agent frameworks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ChatGPT",
            "model_description": "Transformer-based generative LLM accessed via API; used with prompting, chain-of-thought (CoT) and in-context examples inside agent frameworks (one-step and sequential).",
            "model_size": "200B",
            "qa_task_name": "Mathematical questions (paper-specific eval)",
            "qa_performance": "90% accuracy (Table 10)",
            "interactive_task_name": "TPTU end-to-end multi-tool evaluation (planning + tool use), plus planning/tool-subtask pair planning and SQL generation",
            "interactive_task_type": "planning; tool use; multi-step reasoning",
            "interactive_performance": "TPTU-OA end-to-end: 50% success (Table 11); TPTU-SA end-to-end: 55% (Table 11). Tool-subtask pair planning: 75% (Table 5). Simple SQL: 90% (Table 8). Complex nested SQL: 80% direct / 80% CoT (Table 9).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Transformer LLM used with external tool interface (SQL & Python generators) inside agent architectures (One-step and Sequential); evaluated with CoT prompting and structured prompt formats.",
            "training_method": "prompting only in experiments (in-context learning, CoT); model itself pre-trained and instruction fine-tuned externally (not by this paper).",
            "intervention_type": "prompting strategy; agent design",
            "intervention_description": "Prompt engineering: unified tool-subtask pair dictionary format; CoT vs direct guidance prompts for nested SQL; agent design: One-step (TPTU-OA) vs Sequential (TPTU-SA) agents to expose intermediate feedback.",
            "intervention_effect": "Unified tool-subtask pair format improved planning performance over separate tool+subtask generation (overall +52.9% reported across models). For ChatGPT specifically, tool-subtask pair planning 75% (Table 5) vs tool+subtask separate 55% (Table 4) (≈+20 pp), and sequential agent planning improved ChatGPT from 75% → 80% (Table 7) and end-to-end from 50% → 55% (Table 11) (+5 pp).",
            "hypothesized_cause_of_gap": "Authors attribute gap to: LLMs' generative training not directly teaching tool invocation and stateful multi-step execution; difficulties in formatting structured outputs; lack of grounding/execution feedback handling; inability to reliably track and incorporate intermediate tool results; and insufficient flexibility when asked to generate full global plans.",
            "uuid": "e853.0"
        },
        {
            "name_short": "Claude",
            "name_full": "Claude (Anthropic)",
            "brief_description": "A large safety-focused LLM from Anthropic evaluated for planning and SQL/math/tool-subtask tasks in the agent framework; shows strong performance on planning and SQL generation in paper's evaluations.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Claude",
            "model_description": "Large transformer-based LLM prioritizing honesty/safety; used via API with prompting and in-context demonstrations inside agent frameworks.",
            "model_size": "&gt;52B",
            "qa_task_name": "Mathematical questions (paper-specific eval)",
            "qa_performance": "85% accuracy (Table 10)",
            "interactive_task_name": "Tool-subtask pair planning; SQL generation; sequential planning (TPTU-SA) evaluations",
            "interactive_task_type": "planning; tool use; multi-step reasoning",
            "interactive_performance": "Tool-subtask pair planning: 90% (Table 5). TPTU-SA planning: 100% (Table 7). Simple SQL: 100% (Table 8). Complex nested SQL: 100% both direct and CoT (Table 9). End-to-end multi-tool end-to-end evaluation not included (not API-based in that stage).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Transformer LLM used inside agent framework with tool descriptions; evaluated with CoT prompting and structured prompting.",
            "training_method": "prompting only in experiments (in-context learning, CoT); model itself pre-trained and instruction-fine-tuned externally (not by this paper).",
            "intervention_type": "prompting strategy; agent design",
            "intervention_description": "Used CoT and direct guidance prompts for nested SQL; applied unified tool-subtask pair prompting and sequential agent design (TPTU-SA) to expose outcomes between steps.",
            "intervention_effect": "Claude shows near-perfect performance on SQL and planning tasks; sequential agent planning reached 100% (Table 7). The unified tool-subtask pairing improved performance in planning evaluations (overall +52.9% reported across models).",
            "hypothesized_cause_of_gap": "While Claude performed strongly on planning and SQL, the paper suggests performance gaps generally arise from lack of grounding/execution feedback and difficulty producing strictly-formatted structured outputs in some models; these general causes apply across models.",
            "uuid": "e853.1"
        },
        {
            "name_short": "InternLM",
            "name_full": "InternLM (Shanghai AI Lab)",
            "brief_description": "A multilingual transformer LLM (120B) evaluated via API for planning, SQL, and mathematical code generation within the agent frameworks; shows mid-to-high performance on many tool tasks but trailing ChatGPT/Claude on end-to-end multi-tool use.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "InternLM",
            "model_description": "Large multilingual transformer LLM (120B) with multiround dialogue capability and long-context understanding, used via API with prompts and CoT where applicable.",
            "model_size": "120B",
            "qa_task_name": "Mathematical questions (paper-specific eval)",
            "qa_performance": "95% accuracy (Table 10)",
            "interactive_task_name": "TPTU end-to-end multi-tool evaluation; planning and tool-subtask pair planning; SQL generation",
            "interactive_task_type": "planning; tool use; multi-step reasoning",
            "interactive_performance": "TPTU-OA end-to-end: 15% (Table 11); TPTU-SA end-to-end: 20% (Table 11). Tool-subtask pair planning: 55% (Table 5). TPTU-SA planning: 65% (Table 7). Simple SQL: 90% (Table 8). Complex nested SQL: 60% direct / 50% CoT (Table 9).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Transformer LLM with long-context dialogue strengths; used with tool interface and CoT prompting in experiments.",
            "training_method": "prompting only in experiments (in-context learning, CoT); model pre-trained previously (not by this paper).",
            "intervention_type": "agent design; prompting strategy",
            "intervention_description": "Evaluated both one-step and sequential agent designs; tested CoT vs direct guidance for nested SQL; used unified tool-subtask pair prompts.",
            "intervention_effect": "Sequential agent (TPTU-SA) improved planning accuracy vs one-step (Tool-subtask planning 55% → 65% in Table 5 → Table 7); end-to-end multi-tool improved from 15% → 20% (Table 11). CoT prompting reduced performance for InternLM on nested SQL (60% direct → 50% CoT per Table 9).",
            "hypothesized_cause_of_gap": "Paper suggests InternLM's decrease under CoT may indicate difficulties managing dependencies/continuity in stepwise problem solving, and general issues (formatting, grounding, intermediate-result incorporation) explain gaps between QA-like tasks and interactive tool use.",
            "uuid": "e853.2"
        },
        {
            "name_short": "Ziya-13B",
            "name_full": "Ziya (IDEA) 13B",
            "brief_description": "A 13B bilingual LLaMA-derived model evaluated for planning and SQL/math tasks; shows mixed ability—reasonable on some single-tool tasks but weaker in planning and end-to-end tool usage.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Ziya-13B",
            "model_description": "LLaMA-derived pre-trained transformer model (13B) with bilingual capabilities, used with prompting and in-context examples in experiments.",
            "model_size": "13B",
            "qa_task_name": "Mathematical questions (paper-specific eval)",
            "qa_performance": "50% accuracy (Table 10)",
            "interactive_task_name": "Tool-subtask pair planning; SQL generation; end-to-end multi-tool evaluation (limited)",
            "interactive_task_type": "planning; tool use; multi-step reasoning",
            "interactive_performance": "Tool-subtask pair planning: 20% (Table 5). TPTU-SA planning: 10% (Table 7). End-to-end multi-tool: 0% (Table 11). Simple SQL: 50% (Table 8). Complex nested SQL: 50% direct / 40% CoT (Table 9).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "Transformer LLM (LLaMA-derived) used with prompt-based tool interface; no additional architectural modules introduced in experiments.",
            "training_method": "prompting only in experiments (in-context learning, CoT); model pre-trained and fine-tuned externally.",
            "intervention_type": "prompting strategy; agent design",
            "intervention_description": "Evaluated unified tool-subtask pair prompts and sequential agent; CoT vs direct guidance for nested SQL.",
            "intervention_effect": "Unified tool-subtask pairing modestly improved planning vs separate generation but overall performance remained low; CoT reduced nested SQL accuracy compared to direct for Ziya (50% → 40%). Sequential agent did not improve end-to-end performance (TPTU-SA planning 10%).",
            "hypothesized_cause_of_gap": "Authors note the model struggles with sequential continuity and complex planning; hypothesized causes include poor handling of structured output formats, difficulty aligning tools and subtasks, and limited abilities to incorporate intermediate tool outputs.",
            "uuid": "e853.3"
        },
        {
            "name_short": "ChatGLM-130B",
            "name_full": "ChatGLM-130B (Tsinghua University)",
            "brief_description": "A bilingual dialogue-optimized LLM (GLM architecture) evaluated on SQL, math code generation, and planning tasks; shows selective strengths (better with CoT for SQL) but weaknesses in mathematical code and structured planning outputs.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "ChatGLM-130B",
            "model_description": "GLM-architecture bilingual dialogue model (130B in table) designed for Chinese/English Q&A; evaluated with prompts and CoT.",
            "model_size": "130B",
            "qa_task_name": "Mathematical questions (paper-specific eval)",
            "qa_performance": "0% accuracy (Table 10) for mathematical code generation (noted severe weakness in math code generation)",
            "interactive_task_name": "SQL generation and planning (tool-subtask planning), nested SQL with CoT",
            "interactive_task_type": "tool use; planning; multi-step reasoning",
            "interactive_performance": "Simple SQL: 30% (Table 8). Complex nested SQL: 60% direct / 70% CoT (Table 9) — model prefers CoT. Tool-subtask pair planning: 0% (Table 5). TPTU-SA planning: 0% (Table 7). End-to-end multi-tool evaluation not included.",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "GLM-based dialogue model used with prompt-based tool interfaces; benefits from CoT prompting for nested SQL.",
            "training_method": "prompting only in experiments (in-context learning, CoT); model pre-trained/fine-tuned externally.",
            "intervention_type": "prompting strategy",
            "intervention_description": "CoT prompting for nested SQL improved performance compared to direct guidance; tested unified tool-subtask pair prompts (but model struggled to produce required format).",
            "intervention_effect": "CoT-based prompting increased complex SQL accuracy (60% → 70% for ChatGLM in Table 9); however, ChatGLM failed on mathematical code generation and on structured tool-subtask planning formats.",
            "hypothesized_cause_of_gap": "Paper suggests ChatGLM benefits from stepwise reasoning prompts (CoT) for SQL, but lacks reliable tool-subtask output formatting and struggles to ground generated code for execution; overall gap due to format production and grounding limitations.",
            "uuid": "e853.4"
        },
        {
            "name_short": "Chinese-Alpaca-Plus",
            "name_full": "Chinese-Alpaca-Plus (LLaMA-derived, 33B)",
            "brief_description": "A 33B LLaMA-derived bilingual model fine-tuned with expanded Chinese tokens and instruction data; evaluated for SQL, math, and planning tasks and shows uneven strengths (some math ability but poor SQL and planning).",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Chinese-Alpaca-Plus",
            "model_description": "LLaMA-derived LLM (33B) with expanded Chinese vocabulary and instruction fine-tuning; used via prompts and CoT in experiments.",
            "model_size": "33B",
            "qa_task_name": "Mathematical questions (paper-specific eval)",
            "qa_performance": "55% accuracy (Table 10)",
            "interactive_task_name": "SQL generation; planning/tool-subtask pair planning; end-to-end multi-tool (included in some studies)",
            "interactive_task_type": "tool use; planning; multi-step reasoning",
            "interactive_performance": "Simple SQL: 20% (Table 8). Complex nested SQL: 0% direct / 0% CoT (Table 9). Tool-subtask pair planning: 5% (Table 5). TPTU-SA planning: 0% (Table 7). End-to-end multi-tool: 0% (Table 11).",
            "reports_both_qa_and_interactive": true,
            "performance_gap_observed": true,
            "architectural_features": "LLaMA-derived transformer with instruction fine-tuning and expanded Chinese tokens, used in prompts within agent framework.",
            "training_method": "prompting only in experiments; model pre-trained and fine-tuned externally.",
            "intervention_type": "prompting strategy; agent design",
            "intervention_description": "Evaluated CoT vs direct guidance for nested SQL and unified tool-subtask pair prompts; sequential agent setup tested.",
            "intervention_effect": "Performed moderately on math but poorly on SQL/planning; sequential agent did not improve planning performance (remained low), and end-to-end tool usage failed (0%).",
            "hypothesized_cause_of_gap": "Authors suggest limited ability to generate correct structured outputs and to ground intermediate execution results; specialized SQL generation and multi-step tool use remain failure modes for this model.",
            "uuid": "e853.5"
        },
        {
            "name_short": "TPTU-OA",
            "name_full": "TPTU One-step Agent (TPTU-OA)",
            "brief_description": "One-step agent design proposed in this paper that instructs the LLM to produce a global plan (sequence of tools and subtasks) in a single pass; evaluated for tool-order planning, tool-subtask generation and multi-tool execution.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "TPTU-OA (One-step Agent)",
            "model_description": "Agent architecture that prompts an LLM to produce a complete global decomposition of the task (tool sequence and subtask descriptions) in a single inference step, without iterative tool-execution feedback between steps.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool planning and multi-tool usage experiments (global planning approach)",
            "interactive_task_type": "planning; tool use; multi-step reasoning",
            "interactive_performance": "Tool order planning accuracy high for top models (e.g., ChatGPT/Claude 100% Table 3); tool+subtask separate generation lower (ChatGPT 55% Table 4); tool-subtask pair generation in one-step (Table 5) varied (ChatGPT 75%, Claude 90%, InternLM 55%). End-to-end multi-tool success: ChatGPT 50%, InternLM 15%, Ziya/Chinese-Alpaca 0% (Table 11).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Global planning (single-pass), requires producing fully-formed structured outputs (tool lists, subtask descriptions) without intermediate execution feedback.",
            "training_method": null,
            "intervention_type": "architectural change; prompting strategy",
            "intervention_description": "One-step global decomposition vs sequential; multiple prompt formats tested (separate tool + subtask generation, unified tool-subtask pair dictionary format); evaluated robustness to unrelated tools in prompt.",
            "intervention_effect": "One-step planning produced lower accuracy when tools and subtasks generated separately; unified tool-subtask pair format improved one-step planning substantially (paper reports +52.9% improvement overall compared to separate generation). However, one-step agents were less flexible and sometimes produced misaligned or brittle plans compared to sequential agents.",
            "hypothesized_cause_of_gap": "One-shot global planning depends on strong global understanding early and is more brittle because errors require regeneration; lacks iterative feedback so cannot adapt to intermediate tool outputs.",
            "uuid": "e853.6"
        },
        {
            "name_short": "TPTU-SA",
            "name_full": "TPTU Sequential Agent (TPTU-SA)",
            "brief_description": "Sequential agent design proposed in this paper that iteratively solves one subtask at a time, requesting the next subtask after each tool execution; shown to improve planning and end-to-end tool usage for multiple LLMs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "TPTU-SA (Sequential Agent)",
            "model_description": "Agent architecture that decomposes tasks into a sequence of sub-tasks and iteratively invokes tools, using outputs from prior steps as context for subsequent planning and execution.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Tool-subtask pair generation and end-to-end multi-tool usage with iterative feedback",
            "interactive_task_type": "sequential decision-making; planning; tool use; multi-step reasoning",
            "interactive_performance": "Improved planning accuracy vs one-step for several models (e.g., ChatGPT tool-subtask planning 80% Table 7 vs 75% Table 5; Claude 100% vs 90%). End-to-end multi-tool success modestly improved (ChatGPT 50% → 55%; InternLM 15% → 20% Table 11).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "Iterative planning loop; uses intermediate outputs as context; supports updating plans and error correction after each step.",
            "training_method": null,
            "intervention_type": "architectural change (agent design); prompting strategy",
            "intervention_description": "Sequential design allows the LLM to see the result of executed subtasks before planning the next step; used recursion to generate {tool: subtask} pairs one at a time and incorporate feedback; combined with structured prompts (tool-subtask dict-format).",
            "intervention_effect": "Sequential agent generally outperformed one-step agent on planning and end-to-end use for high-performing LLMs (example improvements: ChatGPT planning 75% → 80%, end-to-end 50% → 55%; InternLM planning 55% → 65%, end-to-end 15% → 20%). Authors report reasons: stepwise feedback, error correction and richer context.",
            "hypothesized_cause_of_gap": "Sequential approach reduces brittleness by allowing feedback and correction; authors argue many LLM failures in interactive tasks stem from lack of iterative grounding and inability to process intermediate execution outputs.",
            "uuid": "e853.7"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Toolformer: Language models can teach themselves to use tools.",
            "rating": 2,
            "sanitized_title": "toolformer_language_models_can_teach_themselves_to_use_tools"
        },
        {
            "paper_title": "WebGPT: Browser-assisted question-answering with human feedback.",
            "rating": 2,
            "sanitized_title": "webgpt_browserassisted_questionanswering_with_human_feedback"
        },
        {
            "paper_title": "ToolAlpaca: Generalized tool learning for language models with 3000 simulated cases.",
            "rating": 2,
            "sanitized_title": "toolalpaca_generalized_tool_learning_for_language_models_with_3000_simulated_cases"
        },
        {
            "paper_title": "API-Bank: A benchmark for tool-augmented LLMs.",
            "rating": 2,
            "sanitized_title": "apibank_a_benchmark_for_toolaugmented_llms"
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models.",
            "rating": 2,
            "sanitized_title": "chainofthought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "ART: Automatic multi-step reasoning and tool-use for large language models.",
            "rating": 1,
            "sanitized_title": "art_automatic_multistep_reasoning_and_tooluse_for_large_language_models"
        }
    ],
    "cost": 0.01901825,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage
22 Oct 2023</p>
<p>Jingqing Ruan ruanjingqing@sensetime.com 
Yihong Chen chenyihong@sensetime.com 
Tianpeng Bao baotianpeng@sensetime.com 
Hangyu Mao maohangyu@sensetime.com 
Ziyue Li 
Xingyu Zeng zengxingyu@sensetime.com 
Rui Zhao zhaorui@sensetime.com 
TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage
22 Oct 2023ED7684255DA08A6ACB6CB59F574F063CarXiv:2308.03427v2[cs.AI]
With recent advancements in natural language processing, Large Language Models (LLMs) have emerged as powerful tools for various real-world applications.Despite their powers, the intrinsic generative abilities of LLMs may prove insufficient for handling complex tasks, which necessitate a combination of task planning and the usage of external tools.In this paper, we first propose a structured framework tailored for LLM-based AI Agents and then discuss the crucial capabilities necessary for tackling intricate problems.Within this framework, we design two distinct types of agents (i.e., one-step agent and sequential agent) to execute the inference process.Subsequently, we instantiate the framework using various LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on typical tasks.By highlighting key findings and challenges, our goal is to provide a helpful resource for researchers and practitioners to leverage the power of LLMs in their AI applications.Our study emphasizes the substantial potential of these models while also identifying areas that need more investigation and improvement.The code and resources will be available on GitHub.</p>
<p>Introduction</p>
<p>Large Language Model (LLM) [1] is a recent breakthrough in natural language processing (NLP) research.These models are trained on massive amounts of text data and can solve a wide range of tasks, even those that were not included in their training dataset, known as "emerging" ability.This</p>
<p>ChatGLM InternLM</p>
<p>ChatGPT Claude + How?</p>
<p>Step 1</p>
<p>Step 2</p>
<p>Step 3</p>
<p>our agents ability is especially evident in the tasks of few-shot [2] and zero-shot [3] learning, where LLMs can perform well with minimal or even no fine-tuning to adapt to a new task.</p>
<p>… … based on different LLMs</p>
<p>However, the application of LLMs in real-world settings presents unique challenges.On the one hand, LLMs have proved to be incompetent in solving logic problems such as mathematics, and their training data is also out of date (e.g., the knowledge cutoff date for GPT-4 [4] is up to January 2022).Teaching LLMs to use tools such as calculators, calendar, or search engines can help prevent them from hallucinating [5].On the other hand, despite their impressive problem-solving abilities, the successful integration of these models into complex systems often requires more than just task understanding -it requires the capacity to manipulate various tools and interact effectively with users.This is exemplified in systems like AutoGPT1 , BabyAGI2 , and ChatGPT-plugins 3 , which leverage LLMs' capabilities beyond merely generating well-written texts and programs.In these systems, LLMs operate as the central controller, manipulating different tools and interacting with humans, thus taking on the role of Artificial Intelligence Agents (AI Agents).In addition to being central planners, LLMs are often used as intermediaries between macro plans and low-level tool calls or as specific tools.As such, LLMs are seen as a crucial approximation of the linguistic world model in real-world systems.</p>
<p>In this paper, we propose a structured framework for LLM-based AI Agents to evaluate the existing LLMs' planning and tool-using ability and discuss the necessary abilities of such LLM-based AI Agents.Furthermore, we instantiate the framework with different LLMs and evaluate their Task Planning and Tool Usage (TPTU) abilities on several tasks.As shown in Figure 1, we use the Doraemon as an analogy of our LLM-based agents: Doraemon's magic 4D pocket consists of millions of gadgets (the Tool Set), and Doraemon needs to pick the right tools and solve tasks in a right order.Our main contributions are summarized as follows:</p>
<p>Method</p>
<p>To the best of our knowledge, the study of "Agent", "Autonomous Agent", "AI Agent" and "Multi-Agent" has been a central part of AI research for decades [6][7][8][9][10][11], aimed at understanding and building intelligent and autonomous systems, but there is currently no standardized definition for AI Agents, particularly those that are based on LLMs.</p>
<p>In this paper, the Artificial Intelligence Agent (AI Agent) is defined as a program that employs AI techniques to perform tasks that typically require human-like intelligence.AI Agents can take many forms, from simple chatbots to complex autonomous systems that interact with their environment and make decisions in real-time.They can be trained using a variety of machine learning techniques, including supervised, unsupervised, and reinforcement learning, and can be programmed to perform specific tasks or learn from their experiences in order to improve their performance over time.</p>
<p>Agent Framework</p>
<p>We are particularly interested in the AI Agent that employs the LLM techniques (i.e., LLM-based AI Agent), due to its high efficiency and flexibility in various tasks and domains.Specifically, we design our AI Agent framework with six components as shown in Figure 2: 1. Task Instruction.This is the explicit input of the agent.In practical systems, the task instruction comes from human users of the systems.For example, in a human resources (HR) system, the user may give a task instruction: How much budget is required to provide a 100$ incentive for each colleague who has worked for five years?In contrast, in a criminal investigation system, the user may give a task instruction: Deploy surveillance on a group of suspects.</p>
<ol>
<li>
<p>Designed Prompt.This is an additional form of input for the agent, derived from tasks that the human users anticipate the AI Agent will complete.Humans can craft specific instructions or demonstrations to steer the LLM-based AI Agents toward generating suitable responses.These guiding inputs could encompass system instructions, tool descriptions, few-shot demonstrations, chat history, or even error output.</p>
</li>
<li>
<p>Tool Set.It is another input for the agent, which refers to the set of external resources, services, or subsystems that the AI Agent can utilize to aid in its tasks.This could include databases for information retrieval [12], APIs for interacting with external systems [5], other AI models specialized for tasks such as image recognition or sentiment analysis [13], or even non-AI tools and resources such as web scraping tools or data visualization libraries [14].The toolset expands the capabilities of the AI Agent, enabling it to access and process information beyond its internal knowledge, interact with other systems, or perform specialized tasks that it may not be capable of on its own.For example, an AI Agent might use a weather API to fetch current weather information, or a Python interpreter to solve the mathematical question.</p>
</li>
</ol>
<p>4.</p>
<p>LLM.This is the core component of the system that interprets the task instructions and prompts, interacts with the toolset, and generates the intermediate outputs and final answers.</p>
<p>In this context, we utilize publicly available large language models such as ChatGPT, GPT-4 [4], InterLM [15], and others.</p>
<ol>
<li>
<p>Intermediate Output.This represents the output generated by the LLM-based AI Agent after it processes the task instructions and prompts, and interacts with the toolset.There are three typical intermediate outputs: (1) the high-level plans to fulfill the original user instruction, (2) selected and created tools to fulfill each subtask in the plans, and (3) the results or errors produced after tool execution.The output can be reviewed and refined, either by the AI Agent itself or with human oversight, to ensure it is accurate and meets the requirements of the task instruction.</p>
</li>
<li>
<p>Final Answer.This is the output that the AI Agent summarizes and provides to the user after all processing (including task planning, tool usage, and possibly error feedback) has been completed.</p>
</li>
</ol>
<p>Agent Ability</p>
<p>To apply LLM-based AI Agents to augment or replace human decision-making in real-world applications, the agents typically require the following abilities:</p>
<ol>
<li>
<p>Perception Ability: AI Agents must be able to perceive the task instruction from human and system specifications.</p>
</li>
<li>
<p>Task Planing Ability: AI Agents should have the capacity to create a step-by-step plan for complex task composition based on the perceived instruction and specifications.This usually involves the generation of critical subtask sequences, and the ability to adjust the plan dynamically in response to changes in the task or environment.</p>
</li>
<li>
<p>Tool Usage Ability: On the one hand, AI Agents should possess the capacity to select a variety of existing tools or resources to execute complex tasks.On the other hand, AI Agents should create new tools by converting the task requirements.This ability enables the AI Agent to extend its capabilities beyond LLM itself and the existing tools by leveraging the vast resources available in the digital world.Finally, AI Agents should be able to execute the selected or created tools for truly grounding the human request based on the resources and constraints of systems.</p>
</li>
</ol>
<p>4.</p>
<p>Learning/Reflection/Memory (from Feedback): AI Agents should be capable of learning from feedback, including correct results and exception errors.They should incorporate memory, such as logging or chat history, and reflection to adapt their plans or decisions.This allows the agents to improve their performance and efficiency in task execution continuously.</p>
<ol>
<li>Summarization: After several rounds of interaction with humans, tools, and systems, AI agents can ultimately complete the original task provided by the users.At this point, AI agents should be able to summarize the interaction history and provide a final answer that is concise and easy to understand for the users.</li>
</ol>
<p>To endow AI Agents with the aforementioned abilities, some techniques that can be used include chain-of-thought (CoT) and vector databases, as shown in Table 1.</p>
<p>Agent Design</p>
<p>Task planning and tool usage represent the cornerstone of LLM's abilities.Others like perception, learning/reflection/memory (from feedback), and summarization are indeed critical, but they primarily serve to enhance and support these two core competencies.Therefore, concentrating on these two key competencies -Task Planning and Tool Usage (TPTU for short) -we have devised two distinct types of AI agents, as depicted in Figure 3:</p>
<p>• The first one, named as the One-step Agent (TPTU-OA), adopts a global perspective to interpret the original problem, effectively breaking it down into a sequence of sub-tasks in a single instance.This strategy fully harnesses the model's comprehensive understanding capabilities to map out the problem-solving steps for all sub-tasks at once.This method underscores the significance of a holistic understanding and planning of the overall task, albeit it might lack flexibility when dealing with individual sub-tasks.</p>
<p>• The second type, referred to as the Sequential Agent (TPTU-SA), emphasizes tackling the current sub-task at hand.Upon successfully resolving the ongoing sub-task, this agent requests the LLMs to provide the succeeding sub-task.This approach enables the model to maintain a clear and concentrated focus throughout the problem-solving journey, tackling issues incrementally.Such a methodology allows for continuous feedback and progress within the confines of addressing a broader problem.</p>
<p>These two distinct agent models represent two disparate problem-solving strategies -the one-step and sequential resolution 4 .In our subsequent experiments, we aim to understand their respective strengths and weaknesses and how they can be best utilized to leverage the capabilities of LLMs in real-world problem-solving scenarios.</p>
<p>Evaluation</p>
<p>We instantiate the proposed LLM-based AI Agent framework (TPTU-OA and TPTU-SA) with different LLMs and evaluate their performance on typical tasks.</p>
<p>Preparations</p>
<p>Before beginning our evaluation, we first outline the preparations.We will give detailed descriptions of the datasets, available tools, and popular large language models.</p>
<p>Datasets</p>
<p>We first clarify the motivations behind our choice of tools for evaluation.The selection was guided by two primary factors: the number of tools to be evaluated and the specific tools to be included.</p>
<p>Firstly, regarding the number of tools, it is important to state that our proposed evaluation framework is extensible.It can incorporate any number of tools as pluggable components to be managed by the LLM-based AI agents.Secondly, looking at the current work on tool-augmented LLMs, such as T-Bench [16] and ToolBench [17], we see that only a handful of tools are launched and executed in a single scenario.Meanwhile, API-Bank [18], in a single scenario, typically dispatches only one API tool and awaits its response.APIBench [19] and ToolAlpaca [20] do not even execute a tool response.Hence, for the sake of simplicity and focus in our evaluation, we have decided to primarily assess two tools (which can be called multiple times) within a single scenario.</p>
<p>Secondly, we also need to decide which specific tools should be used for evaluation.Consider a real-world scenario where we pose the question: "How much budget is required to offer a $100 incentive to each employee who has been with the company for over five years?".To answer this, we first need to retrieve the relevant data from a database, typically using SQL, to find the number of eligible employees.Then, we need to perform a mathematical calculation to estimate the total budget.Such scenarios are quite common in daily life where the formulation and resolution of a question often involve SQL and mathematical tools.</p>
<p>Recognizing the importance of these tools, we have chosen to focus our evaluation on SQL and Python generators, which represent the capabilities of database querying and mathematical computation, respectively.To this end, we have prepared 120 question-answer pairs that vary in complexity.These pairs provide a rigorous assessment of the LLM-based AI agents in understanding, generating, and utilizing these essential tools.For further information on these queries and their corresponding demonstrations, please refer to Appendix A.</p>
<p>Tools</p>
<p>We have defined a total of 12 available tools for the selection of the LLM-based AI agents for evaluation.They are defined as follows:</p>
<p>• SQL generator: Given an input question and a database, create a syntactically correct SQLite query statement.</p>
<p>• Python generator: Given an input question and some information, generate a syntactically correct Python code.</p>
<p>• Weather query tool: Given a location, output the current real-time weather at that location.</p>
<p>• Image generator: Given a text description, generate a related image.</p>
<p>• Text extractor: Given a link to an image, extract the corresponding text and its position coordinates.</p>
<p>• Translator: Given a piece of text, translate it into other languages.</p>
<p>• Bing Searcher: Given a piece of text, conduct a search on the Bing browser and return content.</p>
<p>• Shell generator: Given an input question and some information, generate a syntactically correct Shell code.</p>
<p>• Java generator: Given an input question and some information, generate a syntactically correct Java code.</p>
<p>• Wikipedia searcher: Given a piece of text, conduct a search on Wikipedia and return content.</p>
<p>• Office software: Given a text description, automatically generate corresponding long documents or spreadsheets or PPTs.</p>
<p>• Movie player: Given a movie name, automatically play the corresponding movie resources.</p>
<p>LLMs</p>
<p>The LLMs evaluated in this paper are listed in Table 2, elaborated as follows:</p>
<p>• GPT series developed by OpenAI boasts a powerful language model with a vast number of parameters, enabling it to tackle intricate problems efficiently.This paper aims to evaluate the performance of ChatGPT, which balances the performance with costs (the number of OpenAI API calls).</p>
<p>• Claude is committed to maintaining honesty and ensuring user safety, which is developed by Anthropic.With its impressive size, Claude ranks among the largest language models globally and poses a formidable challenge to ChatGPT as a strong competitor.</p>
<p>• InternLM, a sophisticated language model developed by Shanghai AI Lab, boasts a multiround dialogue capability and an impressive ability to comprehend super-long text.This language model is meticulously designed to cater to the nuances of the Chinese language, enabling it to comprehensively understand and effectively process Chinese text.Here, we adopted the version with 120 billion parameters.</p>
<p>• Ziya is an expansive and robust pre-training model developed by IDEA, derived from the LLaMa with 13 billion parameters.This comprehensive model exhibits a wide range of capabilities, including translation, programming, and mathematical calculations.Notably, it stands out as a bilingual LLM, highlighting its ability to effectively process and comprehend text in Chinese.</p>
<p>Evaluation on Task Planning Ability</p>
<p>In this section, to evaluate the planning capabilities of the LLM-based AI agents, we have structured the evaluations as follows.</p>
<p>For TPTU-OA, we begin by examining the agents' ability to plan the order of tool use.This is followed by an evaluation of the agents' capacity to not only plan the sequence of tools but also the corresponding subtask descriptions.Subsequently, we conduct a specialized planning evaluation where the agents must generate multiple sequences of key-value pairs of the form {tool: subtask description} in complex problem teardowns.Moreover, we expand the toolset with additional, unrelated tools to further challenge and reassess the planning ability of the LLM-based AI agents.</p>
<p>For TPTU-SA, we follow the regime that the agent should generate multiple sequences of key-value pairs of the form {tool: subtask description} for evaluation.</p>
<p>TPTU-OA: Tool Order Planning</p>
<p>Here, we utilize two kinds of tools for problem-solving: the SQL generator, which retrieves data from databases, and the Python generator, adept at addressing mathematical questions.</p>
<p>To validate the capacity of the LLM-based AI agents to strategically plan for the tool order, we designed the prompt as shown in Figure 8 of Appendix B. This design is motivated by the goal to assess the ability of LLM-based AI agents to understand complex problems, subsequently decomposing them into a sequence of simpler tasks executed by appropriately selected tools.Specifically, we require the LLM-based AI agent to follow our instructions, select tools from our pre-defined tool set with detailed function descriptions, conform to the given format strictly, and understand the demonstrations to learn from them.</p>
<p>Upon feeding these prompts into the LLM-based AI agents under evaluation, we obtained the following accuracy rates for the tool planning, as shown in Table 3.The results of our experiments indicate that models, notably Ziya and ChatGLM, frequently grapple with the generation of lists in the correct format.For other models, the predominant challenges lie in generating tools in the correct sequence or in the occasional omission of necessary tools.Nonetheless, the issue of parsing list formats is generally negligible.</p>
<p>These findings suggest that the majority of LLM-based AI agents possess a fundamental capability to analyze the tool needs of a given problem and understand its task requirements.To further explore whether these LLM-based AI agents can effectively break down the original problem into sub-tasks, we proceed to the following section.</p>
<p>TPTU-OA: Tool Order Planning and Subtask Description Generation</p>
<p>Simply planning the order of tool usage is not sufficient to fully address a problem.To truly solve it, we need to provide a guide or instructions for the usage of each tool, that is, a decomposed subtask description.Therefore, we can decompose the original complex problem into two separate sequences.One sequence represents the order in which the tools are utilized, while the other sequence corresponds to the subtask descriptions that each tool in the tool sequence aims to resolve.A problem is only truly solved when both the tool and subtask description sequences have been successfully planned.In order to verify whether LLM-based AI agents truly have the ability to solve complex problems, we designed a new prompt as shown in Figure 9 of Appendix B. The main improvement is to plan the corresponding subtask description for each tool after the tool planning is completed.After feeding the prompt to these LLM-based AI agents, we get results shown in Table 4.</p>
<p>Although the generation of tool sequences and their corresponding subtask descriptions might be an effective way to problem-solving, there is a significant decrease in accuracy for all LLMs as can be seen.We hypothesize that there are a few potential drawbacks to this method:</p>
<ol>
<li>
<p>Difficulty in Error Tracking and Debugging.Generating the complete tool and subtask sequences may make it more challenging to track and debug errors.If an error arises within the sequence, it might require a total regeneration instead of a simple modification or repair to the erroneous part.</p>
</li>
<li>
<p>Tool-Subtask Pairing Issue.If all tool sequences and subtask descriptions are generated independently, there's an inherent risk of misalignment between the tools and their corresponding subtasks.This could potentially lead to an improper pairing, which, in turn, could result in a flawed or ineffective solution that fails to appropriately resolve the given problem.</p>
</li>
<li>
<p>Lack of Flexibility.The approach may lack this flexibility when facing complex problems requiring adjustments to the tool or subtask sequence.</p>
</li>
<li>
<p>Dependency on Global Information.Generating the entire tool and subtask sequences requires a global understanding and planning of the entire problem.However, in some instances, certain parts of the problem might not be clear at the early stages of problemsolving, which could pose challenges within this framework.</p>
</li>
</ol>
<p>TPTU-OA: The Planning of Tool-Subtask Pair</p>
<p>To mitigate the aforementioned issue, we propose a novel approach to foster flexible problem-solving with the LLM-based AI agent.We prompt the agent to generate multiple sequences, each consisting of a key-value pair in the format of {tool: subtask description} that associates a tool with its respective subtask description.This allows us to simultaneously plan the tool choice and subtask without the risk of improper matching.Moreover, it offers the flexibility to update the planned sequences in real-time based on evolving problem feedback, enhancing adaptability and efficiency when addressing complex tasks.</p>
<p>With this consideration, we have designed a unique prompt that encourages this advanced problemsolving strategy.In the following section, we delve into the specifics of this prompt design in Figure 10 of Appendix B. The key improvement in this prompt is its directive for the LLM-based AI agents to stringently adhere to the predefined dictionary format.To facilitate this, we offer several demonstrations in our desired format, serving as references for the language model to follow.After feeding the prompt to these LLM-based AI agents, we get results shown in Table 5.</p>
<p>Analyzing the results from Tables 4 and 5, we observe a marked improvement of 52.9% when the tool-subtask pairs are generated in a unified format compared to separate generation of tools and subtasks.</p>
<p>This significant performance enhancement can likely be attributed to the close coupling between tools and their associated subtasks in our unified generation strategy.When tools and subtasks are generated separately, there is a potential disconnect or lack of coherence between the two, which could lead to less accurate or efficient solutions.In contrast, by generating tool-subtask pairs together, we ensure that each tool is directly tied to its relevant subtask, leading to a more coordinated and effective problem-solving approach.This might explain the observed increase in overall performance.</p>
<p>TPTU-OA: The Planning of Tool-Subtask Pair with Unrelated Tools</p>
<p>So far, our analysis and evaluation have been primarily focused on the LLM-based AI agents' proficiency in planning with specific tools.However, we are also interested in how it would perform when faced with many irrelevant or similar tools.Therefore, for a more comprehensive assessment, we expanded the prompt in Table 10 to include an additional ten unrelated tools, as illustrated in Figure 11 of Appendix B. After feeding the prompt to these LLM-based AI agents, we get results shown in Table 6.The results from our expanded evaluation demonstrate that even when presented with irrelevant or similar tools and descriptions, LLM-based AI agents consistently avoid selecting these unrelated tools (i.e., the accuracy has remained unchanged or exhibited only a marginal decrease compared with Table 5).This outcome indicates the effectiveness of our designed prompt, which successfully guides the LLM-based agents to understand the appropriate tool sequence for complex problem decomposition.</p>
<p>This observation reinforces the notion that a well-structured and informative prompt can efficiently guide AI agents to understand the core essence of the problem, thereby enabling them to sift through irrelevant information and focus on key tasks.This successful discrimination against unrelated tools also points towards the models' ability to understand the specific context of a problem and select the appropriate tools, thereby enhancing the overall problem-solving process.</p>
<p>TPTU-SA: The Planning of Tool-Subtask Pair Generation</p>
<p>Upon identifying the drawbacks of first generating a list of tools and then generating corresponding subtask descriptions, we decided to focus subsequent tests on the generation of tool-subtask pairs.Consequently, in this section, we evaluate the capability of TPTU-SA to generate these tool-subtask pairs.</p>
<p>To achieve the goal of recursively generating tool-subtask pairs, we have designed prompts as illustrated in Figure 12 of Appendix B. These points of analysis suggest that the structure and operation of sequential agents inherently confer certain advantages in complex problem-solving scenarios, leading to their superior performance.</p>
<p>Evaluation on Tool Usage Ability</p>
<p>Before evaluating the end-to-end multi-tool usage ability of LLM-based AI agents, we first evaluate the effectiveness of single-tool usage for SQL generation and mathematical code generation.</p>
<p>Subsequently, to assess the end-to-end performance of LLMs across various tools, two types of agents (TPTU-OA and TPTU-SA) were developed and several LLMs were subjected to testing under these agents.The role of the agents is to break down complex questions into simpler sub-questions and plan corresponding tools to solve them, based on the available toolset and corresponding tool descriptions.</p>
<p>The effectiveness of Single Tool Usage</p>
<p>Our aim is to systematically assess how effectively these models can use various tools, focusing on their proficiency with SQL and other coding languages.</p>
<p>The Effectiveness of simple SQL Creation Using the schemas provided in Table 12 and Table 13, we construct questions similar to those in Table 14, and refer readers to Appendix A. These questions are posed to various LLMs using our specifically designed prompts in Appendix B.</p>
<p>Following the tailored prompts, the LLMs are evaluated based on their responses to the presented queries.The results of this comprehensive assessment are compiled and exhibited in Figure 8.</p>
<p>This verifies the capabilities of each LLM in handling varying simple single-table SQL queries, thus providing a basis for comparison and analysis.The Effectiveness of Complex Nested SQL Creation Using the schemas provided in Table 15, 16, 17, and 18, we construct questions similar to those in Table 19, and refer readers to Appendix A. For complex nested SQL questions, to further verify the SQL tool creation capability of LLMs, we have designed two types of prompts.One is the direct-guidance type, which explicitly informs the model that it needs to generate nested SQL query statements, as shown in Figure 14 in Appendix B.</p>
<p>The other is based on the Chain-of-Thought (CoT) [26] approach, which leverages the model's ability to reason step by step to comprehend and craft SQL tools, and the prompt is shown in Figure 15 in Appendix B. This method guides the model to sequentially generate SQL query clauses based on the problem context, thus breaking down the complex query generation task into smaller and manageable subtasks.This approach provides the model with a structured way to handle complex SQL tasks and showcases its capacity to engage in incremental reasoning and problem-solving.</p>
<p>The design of these two types of prompts serves as the backbone of our evaluation for complex nested SQL questions.While the direct-guidance approach focuses on testing the model's raw ability to generate SQL queries when explicitly instructed, the CoT-based approach evaluates a more nuanced capability: the model's reasoning and problem-solving skills in a step-by-step manner.Both these methods present unique challenges and offer valuable insights into the strengths and potential areas of improvement for the large language model's SQL tool generation ability.Subsequently, we will explore these two dimensions based on our experimental evaluations shown in Table 9.From the above results in Table 9, it is clear that different models possess varying levels of proficiency in handling complex nested SQL tasks.Some models, like Claude, exhibit a robust capability in SQL generation, no matter whether the approach is direct or CoT-based.Most of these models demonstrate the SQL tool usage capability.</p>
<p>Specifically, some models such as ChatGLM show a distinct preference for the CoT-based approach, their performance improves when problems are broken down into smaller, manageable sub-tasks.This suggests that these models may have a stronger ability in sequential problem-solving and benefit more from step-by-step guidance.Conversely, models like Ziya and InternLM show a drop in performance when tasks are guided in the CoT-based format.This might indicate challenges in managing dependencies between sub-tasks or handling the continuity in sequential problem-solving.Lastly, Chinese-Alpaca-Plus shows significant room for improvement in complex SQL generation tasks.This shows that not all models are equally suited to handle advanced problem-solving involving nested SQL queries.</p>
<p>Overall, these findings underscore the importance of tailoring evaluation and training methodologies to the individual strengths and weaknesses of each model.By adopting this approach, we can better understand the performance variations across different models and provide targeted improvements to enhance their problem-solving abilities.Furthermore, this analysis highlights the potential of LLM-based agents in real-world applications, and the need to push their boundaries through continued research and development.</p>
<p>The Effectiveness of Mathematical Code Creation Following our evaluation of the LLM's proficiency in creating complex SQL queries, we now shift our focus to another tool creation: the creation of mathematical code.To the best of our knowledge, while large language models possess significant capabilities, they often fall short of providing highly accurate solutions to mathematical problems.Guiding these LLMs to generate mathematical code, and subsequently leveraging external tools to execute and derive the solutions, could significantly enhance their ability to tackle mathematical challenges.</p>
<p>In the upcoming section, we will conduct a detailed evaluation of guiding these LLMs to generate mathematical code.We aim to shed light on the true capability of these models in generating mathematical code and to elucidate the extent to which they can be utilized to aid in mathematical problem-solving.The prompt about how to guide LLMs is shown in Figure 16 in Appendix B. The results shown in Table 10 indicate that the capabilities of LLM-based agents to generate mathematical code vary considerably.High-performing models like ChatGPT, Claude, and InternLM display excellent proficiency, suggesting their potent ability to solve complex mathematical tasks.Middle-tier models, such as Ziya, show moderate success, indicating the potential for improvement and adaptability with the right training and optimization.Surprisingly, Alpaca demonstrated a notable proficiency in mathematical tasks, despite its poor performance in SQL generation, suggesting a possible inclination towards mathematical problems.In contrast, ChatGLM struggles significantly with mathematical code generation, underlining a potential weak spot in its capabilities and the need for focused improvement in this area.</p>
<p>Overall, these results underscore the task-dependent nature of LLMs' capabilities and highlight the importance of recognizing their individual strengths and weaknesses for optimal model guidance and enhanced problem-solving.</p>
<p>TPTU-OA and TPTU-SA: Tool Usage for Multiple Tools</p>
<p>We now aim to utilize the one-step agent and sequential agent, which we designed, to conduct an evaluation involving multiple tools.Corresponding prompts for each agent type have been crafted and are presented in Figure 17 and Figure 18 of Appendix B, respectively.</p>
<p>In this phase of the evaluation, we need to automatically invoke the respective tools through code and produce the results.Given that user interface-based LLMs lack the capability to call external tools, we will only utilize the following four API-based LLMs (ChatGPT, Ziya, Chinese-Alpaca, and InternLM) for this comprehensive evaluation of external tool usage ability.With agents mentioned above, the final results are presented in Table 11.The evaluation results demonstrate varying levels of task planning and tool usage capabilities among the four API-based LLMs.In the TPTU-OA evaluation, ChatGPT achieved a performance rate of 50%, significantly outperforming the other models, with InternLM at 15%, while both Ziya and Chinese-Alpaca did not manage to complete any tasks successfully, resulting in a score of 0%.In the TPTU-SA evaluation, an overall slight improvement was observed.ChatGPT maintained its leading position, with a slightly improved performance rate of 55%.InternLM also exhibited better performance, achieving a score of 20%, whereas Ziya and Chinese-Alpaca-Plus again failed to register any successful task completion.</p>
<p>These results reflect a notable discrepancy in the performance of LLMs when it comes to using external tools.ChatGPT and InternLM have demonstrated some ability to navigate these tasks, but their performance rates suggest there is significant room for improvement.Ziya and Chinese-Alpaca-Plus' performance indicates a struggle to effectively utilize external tools in their current state.</p>
<p>The differential performance between the TPTU-OA and TPTU-SA evaluation hints at the possible impact of the agent design on the LLMs' task execution ability.In particular, the performance increase under the sequential agent framework suggests that breaking down tasks into sequential steps might help LLM-based AI agents better utilize external tools.This insight could prove valuable in future improvements and developments of LLM-based AI agents.However, even with this approach, it is clear that LLM-based AI agents are far from perfect when it comes to effectively using external tools for complex tasks.This finding underlines the importance of further investigation and improvement in this domain.</p>
<p>Insightful Observations</p>
<p>Upon closer observation of our experimental results, we have identified several phenomena that deserved further exploration.These findings serve to broaden our understanding of LLM-based agents' behavior and capabilities and provide essential insights that could shape future research in this field.In the following, we will dissect these phenomena as shown in Figure 4 -7, casting light on the weaknesses of LLM-based agents in the context of task planning and tool usage.</p>
<ol>
<li>Misunderstanding Output Formats: LLMs frequently encounter difficulty when output is required in specific formats such as lists or dictionaries.One such example includes inconsistencies between the number of tools and corresponding subtasks, leading to formatting issues that hinder the correct execution of tasks.</li>
</ol>
<p>How many more concerts has Jay Chou held than Li Ronghao?Is this number bigger than the square root of 10?</p>
<p>Tools: ["Python generator", "SQL generator"]</p>
<p>Subtasks:["How many concerts did Jay Chou perform?", "How many concerts did Li Ronghao perform?", "How many more concerts did Jay Chou perform than Li Ronghao?", "Is the number bigger than the square root of 10?"] 3. Endless Extensions: LLMs tend to overutilize a particular tool, even in instances where a single use would suffice for the correct result.This issue can lead to extended and nonsensical planning, where the same subtask is repeatedly solved.</p>
<ol>
<li>Lack of Summary Skills: LLMs do not take into account the responses to subproblems, relying instead on their internalized knowledge to generate the final answer.This may lead to a scenario where the final response only addresses a portion of the original query.</li>
</ol>
<p>By identifying and addressing these common issues, we stand a better chance at improving and refining LLMs, thereby unlocking their full potential.</p>
<p>How many singers have the average number of albums of singers in</p>
<p>Beijing? Gives the square root of this number.</p>
<p>Tools: ["SQL generator", "SQL generator", "SQL generator"] Subtasks:["What is the average number of albums by singers in Beijing?", "How many singers have the average number of albums by singers in Beijing?", "What is the square root of this number?"]The Tool_Query for the first execution of the tool is: {{"SQL Generator": "Not the two birthplaces with the most singers"}}</p>
<p>The Tool_Query for the second execution of the tool is: {{"SQL Generator": "Exclude the two birthplaces with the most singers, provide the number of singers from other birthplaces"}}</p>
<p>The Tool_Query for the third execution of the tool is: {{"SQL Generator": "Exclude the two birthplaces with the most singers, provide the number of singers from other birthplaces, and calculate the factorial of this number"}} …… Please use SQL language to query who are the singers who have not been nominated in the Golden Melody Awards?Give their names.</p>
<p>Answer: Jay Chou, Cui Jian</p>
<p>Related Work</p>
<p>The remarkable capacity for usage and creation of tools have facilitated the transcendence of our innate physical and cognitive constraints, thereby profoundly advancing the progress and prosperity of human civilization and society.The swift advancement of LLM has rendered it feasible to use and create tools like humans.The integration of specialized tools with LLM has unlocked substantial potential in addressing intricate tasks.In this section, we offer a concise synopsis of the relevant research pertaining to tool learning based on LLMs.</p>
<p>Tool Usage</p>
<p>The initial advancements in tool learning have been constrained by the capabilities of artificial intelligence (AI) models.[27] Traditional deep learning approaches exhibit limitations in terms of comprehension of tool functionality and user intentions, and common sense reasoning abilities.Consequently, these limitations directly result in a notable decline in the stability and precision of tool learning methodologies.Recently, the advent of LLM has marked a pivotal juncture in the realm of tool learning.LLMs encompass a broad spectrum of common sense cognitive capabilities and exhibit remarkable proficiencies in natural language processing, reasoning, and interactive decision-making [28][29][30][31][32].These attributes furnish indispensable prerequisites for LLMs to comprehend user intentions and effectively employ tools in tackling intricate tasks [33].Simultaneously, the advancement of fine-tuning [34][35][36][37][38] and in-context learning [39,40] technology has offered robust support to LLM in addressing increasingly intricate challenges.In addition, tool usage can mitigate the inherent limitations of LLMs, encompassing the acquisition of up-to-date information from real-world events, refined mathematical computational abilities, and the mitigation of potential hallucinatory phenomena.[41] Within the realm of embodied intelligence [42][43][44], LLM engages in direct interactions with tangible tools like robots in order to enhance their cognitive abilities, optimize work productivity, and expand functional capacities.LLM possesses the capability to automatically devise action steps based on user intentions, enabling the guidance of robots in the completion of tasks [45][46][47][48][49][50][51][52][53], or alternatively, to directly generate underlying code that can be executed by robots [54][55][56][57][58]. Palm-E [50] introduced a multimodal language model which seamlessly integrates sensor data into its framework, enabling efficient planning of robot actions and task completion.Code as Policies (CaP) [58] facilitates the transformation of natural language instructions into code fragments that can be directly compiled and executed on robots.As for Inner Monologue [48], LLM incorporates diverse environmental feedback to construct inner monologues, thereby formulating effective robot control strategies.Furthermore, LP-SLAM [45] proposes a simultaneous localization and mapping (SLAM) system empowered with language perception capabilities, exploiting the potential of ChatGPT.PromptCraft [57], on the other hand, devises a function library tailored to ChatGPT on the robot platform, streamlining the conversion of user intentions into executable tasks via the underlying backend API.</p>
<p>In addition to directly changing the real environment through interaction with tools in the physical world, LLM can also utilize software tools such as search engines [59][60][61][62][63][64][65][66][67], mobile [68,69], Microsoft Office [70,71], calculators [72][73][74], deep models [19, 75-79, 13, 80, 81] and other versatile APIs [82,5,83,84,20,85] to enhance model performance or complete complex workflows through flexible control of the software.Toolformer [5] employs a self-supervised methodology to fine-tune the language model, enabling it to acquire the ability to automatically invoke APIs.ART [86] leverages CoT [26] and In-context Learning [81,41] techniques to automatically generate multi-step reasoning processes for new tasks, while also selecting and utilizing the most appropriate available tool at each step.ASH [62] utilizes LLM for sequence hierarchical decision-making to achieve web navigation tasks.WebGPT [66] and WebCPM [64] use network search to assist in implementing Question Answering tasks.In addition, RCI [87] recursively criticizes and improves itself to execute computer tasks guided by natural language according to the prompting scheme.To achieve the analysis and processing of tables, TableGPT [71] employs a table encoder to transform tabular data into vector representations, which are then fed into an LLM for inference in combination with user queries.</p>
<p>Tool Creation</p>
<p>The usage of tools is contingent upon the accessibility of external tools.Recently, efforts have been made to employ LLM as a tool creator in order to generate tools that can be utilized for diverse requests [88][89][90][91][92][93][94][95].This development has consequently raised the demands placed on LLM.And these created tools are typically implemented as Python or SQL functions.LATM [88], for example, leverages the prowess of GPT-4 to create tools, and the usage of more cost-effective models has shown potential in exhibiting performance on par with larger models for these tool applications.EVAPORATE [94] involves the synthesis of multiple functions, which are subsequently utilized at a large scale to efficiently process documents and generate structured views.</p>
<p>Conclusion</p>
<p>In this paper, we have introduced a structured framework specially designed for LLM-based AI Agents, with an emphasis on their abilities in task planning and tool usage.This framework, coupled with our design of two distinct types of agents assigned for the inference process, allows for a comprehensive evaluation of the capabilities of current open-source LLMs, thereby yielding critical insights into their effectiveness.Furthermore, our research highlights the significant potential of LLMs in managing complex tasks, revealing the exciting prospects they hold for future research and development.As we continue to explore and improve upon these models, we move closer to unlocking their full potential in a wide range of real-world applications.</p>
<p>B Prompts Design</p>
<p>Figure 1 :
1
Figure 1: Our LLM-based agents plan tasks and use tools.</p>
<p>"Figure 2 :
2
Figure 2: The proposed framework for LLM-based AI Agents.</p>
<p>Figure 3 :
3
Figure 3: The workflows of the One-step Agent and the Sequential Agent are specifically designed to assess the Task Planning and Tool Usage abilities of LLMs.</p>
<p>Figure 4 : 2 .
42
Figure 4: Issue-1: Inconsistencies between the number of tools and corresponding subtasks.</p>
<p>Figure 5 :
5
Figure 5: Issue-2:Solve a purely mathematical problem by employing a SQL generator.</p>
<p>Figure 6 :
6
Figure 6: Issue-3: Unnecessary repetition of subtasks.</p>
<p>Figure 7 :
7
Figure 7: Issue-4: Answering questions using common sense instead of generating code.</p>
<p>Figure 8 :
8
Figure 8: The evaluation prompt for tool order planning.</p>
<p>Figure 9 :
9
Figure 9: The evaluation prompt for tool order and subtask description planning.</p>
<p>Figure 10 :
10
Figure 10: The evaluation prompt for one-step tool-subtask pair planning.</p>
<p>Figure 11 :
11
Figure 11: The prompt added to Figure 10 for tool-subtask pair planning with other unrelated tools.</p>
<p>Figure 12 :
12
Figure 12: The prompt for the tool-subtask pair generation with TPTU-SA.</p>
<p>Figure 13 :
13
Figure 13: The evaluation prompt for simple SQL questions.</p>
<p>Figure 14 :
14
Figure 14: The evaluation prompt for complex nested SQL questions.</p>
<p>Figure 15 :
15
Figure 15: The evaluation CoT-based prompt for complex nested SQL questions.</p>
<p>Figure 17 :
17
Figure 17: The system prompt for one-step agent.</p>
<p>Figure 18 :
18
Figure 18: The system prompt for the sequential agent.</p>
<p>Table 1 :
1
A simple illustration of the techniques for endowing the key ability.
AbilityPossible TechniquesPerceptionMulti-input FusionTask PlaningZero-shot CoT and Few-shot CoTTool UsageText Matching/Code Generation/(Selection/Creation/Execution)Action GroundingLearning/Reflection/MemoryRLHF/Multi-agent Debate/ Vector DatabaseSummarizationAttention Mechanism and Natural Language Generation</p>
<p>Sequential Plan 1 Python generator: "Calculating the value of 100×𝑋 with a calculator." Sequential Plan 2 ... ...
ProblemOne-step Plans1. SQL generator: "Figuring out"How much budget is requiredhow many colleague who hasto provide a 100$ incentiveworked for five years from thefor each colleague who hasdatabase; taking it as X."worked for five years?"2. Python generator:"Calculating the value of 100*Xwith a calculator"(a) One-step Agent (TPTU-OA)Problem"How much budget is requiredSQL generator: "Figuring outto provide a 100$ incentivehow many colleague who hasfor each colleague who hasworked for five years from theworked for five years?"database; taking it as 𝑋."
(b) Sequential Agent (TPTU-SA)</p>
<p>Table 2 :
2
Chinese-Alpaca-Plus is achieved by extending LLaMA's existing vocabulary with an additional 20,000 Chinese tokens from Meta AI (formerly known as Facebook AI Research Laboratory).In this version, we use a model with 33 billion parameters.The training text has been expanded to 120GB, and the fine-tuning instruction data has been increased to 4.3M.The LLMs evaluated in this paper.
OrganizationModel NameModel ParametersOpenAIChatGPT[21]200BAnthropicClaude[22]&gt;52BShanghai AI LabInternLM120BIDEAZiya-13B13BTsinghua UniversityChatGLM-130B[23]130B-Chinese-Alpaca-Plus-33B[24, 25]33B
• ChatGLM, developed by Tsinghua University, is an open-source dialogue language model that supports bilingual Q&amp;A in Chinese and English, with a particular focus on Chinese optimization.Built on the General Language Model (GLM) architecture and utilizing model quantization technology, the ChatGLM can be easily deployed on consumer-grade graphics cards, enabling local implementation by users.•</p>
<p>Table 3 :
3
The evaluation results for the planning of tool order generation.
ModelChatGPTClaudeZiyaAccuracy100%100%45%ModelChatGLM Chinese-Alpaca-Plus InternLMAccuracy45%20%80%</p>
<p>Table 4 :
4
The evaluation results for the planning of tool order and subtask description generation.
ModelChatGPTClaudeZiyaAccuracy55%15%10%ModelChatGLM Chinese-Alpaca-Plus InternLMAccuracy10%0%45%</p>
<p>Table 5 :
5
The evaluation results for the planning of Tool-Subtask pair.
ModelChatGPTClaudeZiyaAccuracy75%90%20%ModelChatGLM Chinese-Alpaca-Plus InternLMAccuracy0%5%55%</p>
<p>Table 6 :
6
The evaluation results for the planning of Tool-Subtask pair with unrelated tools.
ModelChatGPTClaudeZiyaAccuracy70%90%10%ModelChatGLM Chinese-Alpaca-Plus InternLMAccuracy0%5%50%</p>
<p>Table 7 :
7
The evaluation results for the planning of Tool-Subtask with the sequential agent.
ModelChatGPTClaudeZiyaAccuracy80%100%10%ModelChatGLM Chinese-Alpaca-Plus InternLMAccuracy0%0%65%The evaluation results are shown in</p>
<p>Table 7 .
7
Compared with results shown in Table5, TPTU-SA generally performs better than TPTU-OA especially for high-performing LLMs (e.g., ChatGPT, Claude and InternLM).We propose the following potential reasons for this observation: 1. Sequentiality Mimics Human Problem-Solving: In real-world scenarios, humans tend to solve complex problems by breaking them down into smaller, manageable subtasks which are often handled sequentially.Sequential agents are designed to mimic this step-by-step approach, which might inherently suit complex problem-solving better.2. Richer Contextual Understanding: Sequential agents are exposed to the outcome of each previous subtask before moving on to the next one.This iterative process could facilitate a richer understanding of the problem context, enabling more accurate task planning and tool usage.3. Flexibility in Task Management: In comparison to one-step agents, sequential agents might have more flexibility in managing tasks.They have the opportunity to correct errors or adjust their strategy after each step, which can lead to improved overall performance.4. Improved Learning From History: The sequential process provides a history of actions and results which can be beneficial in learning.The agent can use this history to make better predictions about what tool to use next or what subtask to tackle, leading to more accurate and efficient problem-solving.</p>
<p>Table 8 :
8
The evaluation results for simple SQL questions.
ModelChatGPTClaudeZiyaAccuracy90%100%50%ModelChatGLM Chinese-Alpaca-Plus InternLMAccuracy30%20%90%</p>
<p>Table 9 :
9
The evaluation results for complex nested SQL questions.
ModelChatGPTClaudeZiyaDirect-based80%100%50%CoT-based80%100%40%ModelChatGLM Chinese-Alpaca-Plus InternLMDirect-based60%0%60%CoT-based70%0%50%</p>
<p>Table 10 :
10
The evaluation results for mathematical questions.
ModelChatGPTClaudeZiyaAccuracy90%85%50%ModelChatGLM Chinese-Alpaca-Plus InternLMAccuracy0%55%95%</p>
<p>Table 11 :
11
The evaluation results for end-to-end ability of multiple tools.
ModelChatGPT Ziya Chinese-Alpaca-Plus InternLMTPTU-OA50%0%0%15%TPTU-SA55%0%0%20%
https://github.com/Significant-Gravitas/Auto-GPT
https://github.com/yoheinakajima/babyagi
https://openai.com/blog/chatgpt-plugins
One can also combine the two strategies to design a hierarchical agent, but this is beyond the scope of this paper.
AcknowledgementsThis work was conducted collaboratively among the authors.Hangyu Mao and Rui Zhao led the project, formulating the central idea and laying out the framework for the primary literature review.Regarding the literature review phase, the surveys were conducted by various team members.Guoqing Du and Jingqing Ruan explored DNN-based Tool Scheduling by LLMs; Tianpeng Bao and Yihong Chen investigated Physical/Robot Tool Scheduling by LLMs; and Shiwei Shi and Zhiwei Xu handled the survey of API or GUI-based Tool Scheduling by LLMs.Bin Zhang summarized these papers and synthesized an overarching summary.As for the evaluation phase, Yihong Chen, Tianpeng Bao, Jingqing Ruan, Guoqing Du, Zhiwei Xu, Shiwei Shi, and Bin Zhang performed the experiments and analyzed the data.Hangyu Mao assisted in the analysis of the experimental phenomena and offered constructive suggestions for improvements.Xingyu Zeng and Rui Zhao provided invaluable feedback, contributed to the direction of the research.All authors participated in the discussion.Regarding the manuscript phase, Hangyu Mao organized the overall chapters of the manuscript and mainly wrote the methodology part, and provided assistance in other parts.Jingqing Ruan and Yihong Chen wrote the evaluation section.Bin Zhang wrote the summary of the literature review.Each author read and approved the final manuscript.The authors would like to thank Feng Zhu, Kun Wang, Yuhang Ran, Mengying Xu, Pengfei Jia, and Shaobo Lin for their valuable feedback, discussion, and participation in this project.A Detailed Dataset DescriptionSimple SQL queries: These queries typically involve basic operations such as SELECT, FROM, WHERE, GROUP BY, etc.They are used to retrieve, filter, group, and sort data from a single table.We give the Schema of two tables in the SQL database in Table12import math; math.log10(5)Figure16: The evaluation prompt for mathematical questions.
W X Zhao, K Zhou, J Li, T Tang, X Wang, Y Hou, Y Min, B Zhang, J Zhang, Z Dong, arXiv:2303.18223A survey of large language models. 2023arXiv preprint</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Finetuned language models are zero-shot learners. J Wei, M Bosma, V Y Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, arXiv:2109.016522021arXiv preprint</p>
<p>Gpt-4 technical report. Openai, 2023</p>
<p>Toolformer: Language models can teach themselves to use tools. T Schick, J Dwivedi-Yu, R Dessì, R Raileanu, M Lomeli, L Zettlemoyer, N Cancedda, T Scialom, arXiv:2302.047612023arXiv preprint</p>
<p>A roadmap of agent research and development. N R Jennings, K Sycara, M Wooldridge, Autonomous agents and multi-agent systems. 11998</p>
<p>Applying agent technology. N R Jennings, M Wooldridge, Applied Artificial Intelligence an International Journal. 941995</p>
<p>Is it an agent, or just a program?: A taxonomy for autonomous agents. S Franklin, A Graesser, International workshop on agent theories, architectures, and languages. Springer1996</p>
<p>Modelling social action for ai agents. C Castelfranchi, Artificial intelligence. 1031-21998</p>
<p>Multi-agent systems: an introduction to distributed artificial intelligence. J Ferber, G Weiss, 1999Addison-wesley Reading1</p>
<p>Cooperative multi-agent learning: The state of the art. L Panait, S Luke, Autonomous agents and multi-agent systems. 112005</p>
<p>Din-sql: Decomposed in-context learning of text-to-sql with self-correction. M Pourreza, D Rafiei, arXiv:2304.110152023arXiv preprint</p>
<p>Visual chatgpt: Talking, drawing and editing with visual foundation models. C Wu, S Yin, W Qi, X Wang, Z Tang, N Duan, arXiv:2303.046712023arXiv preprint</p>
<p>Vizability: Multimodal accessible data visualization with keyboard navigation and conversational interaction. J Gorniak, Y Kim, S Gwon, D Wei, N W Kim, arXiv:2310.096112023arXiv preprint</p>
<p>Internlm: A multilingual language model with progressively enhanced capabilities. I Team, 2023</p>
<p>On the tool manipulation capability of open-source large language models. Q Xu, F Hong, B Li, C Hu, Z Chen, J Zhang, arXiv:2305.165042023arXiv preprint</p>
<p>Y Qin, S Liang, Y Ye, K Zhu, L Yan, Y Lu, Y Lin, X Cong, X Tang, B Qian, arXiv:2307.16789Toolllm: Facilitating large language models to master 16000+ real-world apis. 2023arXiv preprint</p>
<p>Api-bank: A benchmark for tool-augmented llms. M Li, F Song, B Yu, H Yu, Z Li, F Huang, Y Li, arXiv:2304.082442023arXiv preprint</p>
<p>Gorilla: Large language model connected with massive apis. S G Patil, T Zhang, X Wang, J E Gonzalez, arXiv:2305.153342023arXiv preprint</p>
<p>Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. Q Tang, Z Deng, H Lin, X Han, Q Liang, L Sun, arXiv:2306.053012023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Y Bai, S Kadavath, S Kundu, A Askell, J Kernion, A Jones, A Chen, A Goldie, A Mirhoseini, C Mckinnon, arXiv:2212.08073Constitutional ai: Harmlessness from ai feedback. 2022arXiv preprint</p>
<p>Glm-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, arXiv:2210.024142022arXiv preprint</p>
<p>Llama: Open and efficient foundation language models. H Touvron, T Lavril, G Izacard, X Martinet, M.-A Lachaux, T Lacroix, B Rozière, N Goyal, E Hambro, F Azhar, arXiv:2302.139712023arXiv preprint</p>
<p>Efficient and effective text encoding for chinese llama and alpaca. Y Cui, Z Yang, X Yao, arXiv:2304.081772023arXiv preprint</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, B Ichter, F Xia, E H Chi, Q V Le, D Zhou, Neural Information Processing Systems. 2022</p>
<p>On the opportunities and risks of foundation models. R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.072582021arXiv preprint</p>
<p>Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation. M Mosbach, T Pimentel, S Ravfogel, D Klakow, Y Elazar, arXiv:2305.169382023arXiv preprint</p>
<p>Harnessing the power of llms in practice: A survey on chatgpt and beyond. J Yang, H Jin, R Tang, X Han, Q Feng, H Jiang, B Yin, X Hu, arXiv:2304.137122023arXiv preprint</p>
<p>One small step for generative ai, one giant leap for agi: A complete survey on chatgpt in aigc era. C Zhang, C Zhang, C Li, Y Qiao, S Zheng, S K Dam, M Zhang, J U Kim, S T Kim, J Choi, arXiv:2304.064882023arXiv preprint</p>
<p>Nature language reasoning, a survey. F Yu, H Zhang, B Wang, arXiv:2303.147252023arXiv preprint</p>
<p>Z Wang, G Zhang, K Yang, N Shi, W Zhou, S Hao, G Xiong, Y Li, M Y Sim, X Chen, arXiv:2305.13246Interactive natural language processing. 2023arXiv preprint</p>
<p>Tool learning with foundation models. Y Qin, S Hu, Y Lin, W Chen, N Ding, G Cui, Z Zeng, Y Huang, C Xiao, C Han, arXiv:2304.083542023arXiv preprint</p>
<p>A survey of knowledge-enhanced text generation. W Yu, C Zhu, Z Li, Z Hu, Q Wang, H Ji, M Jiang, ACM Computing Surveys. 5411s2022</p>
<p>Lora: Low-rank adaptation of large language models. E J Hu, Y Shen, P Wallis, Z Allen-Zhu, Y Li, S Wang, L Wang, W Chen, arXiv:2106.096852021arXiv preprint</p>
<p>Parameter-efficient transfer learning for nlp. N Houlsby, A Giurgiu, S Jastrzebski, B Morrone, Q De Laroussilhe, A Gesmundo, M Attariyan, S Gelly, International Conference on Machine Learning. PMLR2019</p>
<p>Prefix-tuning: Optimizing continuous prompts for generation. X L Li, P Liang, arXiv:2101.001902021arXiv preprint</p>
<p>X Liu, Y Zheng, Z Du, M Ding, Y Qian, Z Yang, J Tang, arXiv:2103.10385Gpt understands, too. 2021arXiv preprint</p>
<p>React: Synergizing reasoning and acting in language models. S Yao, J Zhao, D Yu, N Du, I Shafran, K Narasimhan, Y Cao, arXiv:2210.036292022arXiv preprint</p>
<p>Decomposed prompting: A modular approach for solving complex tasks. T Khot, H Trivedi, M Finlayson, Y Fu, K Richardson, P Clark, A Sabharwal, arXiv:2210.024062022arXiv preprint</p>
<p>Augmented language models: a survey. G Mialon, R Dessì, M Lomeli, C Nalmpantis, R Pasunuru, R Raileanu, B Rozière, T Schick, J Dwivedi-Yu, A Celikyilmaz, arXiv:2302.078422023arXiv preprint</p>
<p>A survey of embodied ai: From simulators to research tasks. J Duan, S Yu, H L Tan, H Zhu, C Tan, IEEE Transactions on Emerging Topics in Computational Intelligence. 622022</p>
<p>Habitat: A platform for embodied ai research. M Savva, A Kadian, O Maksymets, Y Zhao, E Wijmans, B Jain, J Straub, J Liu, V Koltun, J Malik, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer vision2019</p>
<p>Autonomous agents as embodied ai. S Franklin, Cybernetics &amp; Systems. 2861997</p>
<p>W Zhang, Y Guo, L Niu, P Li, C Zhang, Z Wan, J Yan, F U D Farrukh, D Zhang, arXiv:2303.10089Lp-slam: Language-perceptive rgb-d slam system based on large language model. 2023arXiv preprint</p>
<p>Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. D Shah, B Osiński, S Levine, Conference on Robot Learning. PMLR2023</p>
<p>Do as i can, not as i say: Grounding language in robotic affordances. A Brohan, Y Chebotar, C Finn, K Hausman, A Herzog, D Ho, J Ibarz, A Irpan, E Jang, R Julian, Conference on Robot Learning. PMLR2023</p>
<p>Inner monologue: Embodied reasoning through planning with language models. W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.056082022arXiv preprint</p>
<p>Open-vocabulary queryable scene representations for real world planning. B Chen, F Xia, B Ichter, K Rao, K Gopalakrishnan, M S Ryoo, A Stone, D Kappler, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023522</p>
<p>Palm-e: An embodied multimodal language model. D Driess, F Xia, M S Sajjadi, C Lynch, A Chowdhery, B Ichter, A Wahid, J Tompson, Q Vuong, T Yu, arXiv:2303.033782023arXiv preprint</p>
<p>Chatgpt empowered long-step robot control in various environments: A case application. N Wake, A Kanehira, K Sasabuchi, J Takamatsu, K Ikeuchi, arXiv:2304.038932023arXiv preprint</p>
<p>Sayplan: Grounding large language models using 3d scene graphs for scalable task planning. K Rana, J Haviland, S Garg, J Abou-Chakra, I Reid, N Suenderhauf, arXiv:2307.061352023arXiv preprint</p>
<p>Llm-planner: Fewshot grounded planning for embodied agents with large language models. C H Song, J Wu, C Washington, B M Sadler, W.-L Chao, Y Su, arXiv:2212.040882022arXiv preprint</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.068172022arXiv preprint</p>
<p>Open-world object manipulation using pre-trained vision-language models. A Stone, T Xiao, Y Lu, K Gopalakrishnan, K.-H Lee, Q Vuong, P Wohlhart, B Zitkovich, F Xia, C Finn, arXiv:2303.009052023arXiv preprint</p>
<p>A generalist agent. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.061752022arXiv preprint</p>
<p>Chatgpt for robotics: Design principles and model abilities. S Vemprala, R Bonatti, A Bucker, A Kapoor, Microsoft Auton. Syst. Robot. Res. 2202023</p>
<p>Code as policies: Language model programs for embodied control. J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE2023</p>
<p>Retrieval augmented language model pre-training. K Guu, K Lee, Z Tung, P Pasupat, M Chang, International conference on machine learning. PMLR2020</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. P Lewis, E Perez, A Piktus, F Petroni, V Karpukhin, N Goyal, H Küttler, M Lewis, W -T. Yih, T Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Improving language models by retrieving from trillions of tokens. S Borgeaud, A Mensch, J Hoffmann, T Cai, E Rutherford, K Millican, G B Van Den Driessche, J.-B Lespiau, B Damoc, A Clark, International conference on machine learning. PMLR2022</p>
<p>A Sridhar, R Lo, F F Xu, H Zhu, S Zhou, arXiv:2305.14257Hierarchical prompting assists large language model on web navigation. 2023arXiv preprint</p>
<p>Multimodal web navigation with instruction-finetuned foundation models. H Furuta, O Nachum, K.-H Lee, Y Matsuo, S S Gu, I Gur, arXiv:2305.118542023arXiv preprint</p>
<p>Webcpm: Interactive web search for chinese long-form question answering. Y Qin, Z Cai, D Jin, L Yan, S Liang, K Zhu, Y Lin, X Han, N Ding, H Wang, arXiv:2305.068492023arXiv preprint</p>
<p>Webshop: Towards scalable real-world web interaction with grounded language agents. S Yao, H Chen, J Yang, K Narasimhan, Advances in Neural Information Processing Systems. 202235</p>
<p>Webgpt: Browser-assisted question-answering with human feedback. R Nakano, J Hilton, S Balaji, J Wu, L Ouyang, C Kim, C Hesse, S Jain, V Kosaraju, W Saunders, arXiv:2112.093322021arXiv preprint</p>
<p>Hotpotqa: A dataset for diverse, explainable multi-hop question answering. Z Yang, P Qi, S Zhang, Y Bengio, W W Cohen, R Salakhutdinov, C D Manning, arXiv:1809.096002018arXiv preprint</p>
<p>Enabling conversational interaction with mobile ui using large language models. B Wang, G Li, Y Li, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems. the 2023 CHI Conference on Human Factors in Computing Systems2023</p>
<p>Mobile-env: A universal platform for training and evaluation of mobile interaction. D Zhang, L Chen, K Yu, arXiv:2305.081442023arXiv preprint</p>
<p>Sheetcopilot: Bringing software productivity to the next level through large language models. H Li, J Su, Y Chen, Q Li, Z Zhang, arXiv:2305.193082023arXiv preprint</p>
<p>Tablegpt: Towards unifying tables, nature language and commands into one gpt. L Zha, J Zhou, L Li, R Wang, Q Huang, S Yang, J Yuan, C Su, X Li, A Su, arXiv:2307.086742023arXiv preprint</p>
<p>Chatcot: Toolaugmented chain-of-thought reasoning on\chat-based large language models. Z Chen, K Zhou, B Zhang, Z Gong, W X Zhao, J.-R Wen, arXiv:2305.143232023arXiv preprint</p>
<p>Talm: Tool augmented language models. A Parisi, Y Zhao, N Fiedel, arXiv:2205.122552022arXiv preprint</p>
<p>Training verifiers to solve math word problems. K Cobbe, V Kosaraju, M Bavarian, M Chen, H Jun, L Kaiser, M Plappert, J Tworek, J Hilton, R Nakano, arXiv:2110.141682021arXiv preprint</p>
<p>Mm-react: Prompting chatgpt for multimodal reasoning and action. Z Yang, L Li, J Wang, K Lin, E Azarnasab, F Ahmed, Z Liu, C Liu, M Zeng, L Wang, arXiv:2303.113812023arXiv preprint</p>
<p>Internchat: Solving vision-centric tasks by interacting with chatbots beyond language. Z Liu, Y He, W Wang, W Wang, Y Wang, S Chen, Q Zhang, Y Yang, Q Li, J Yu, arXiv:2305.056622023arXiv preprint</p>
<p>Openagi: When llm meets domain experts. Y Ge, W Hua, J Ji, J Tan, S Xu, Y Zhang, arXiv:2304.043702023arXiv preprint</p>
<p>Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. Y Shen, K Song, X Tan, D Li, W Lu, Y Zhuang, arXiv:2303.175802023arXiv preprint</p>
<p>Vipergpt: Visual inference via python execution for reasoning. D Surís, S Menon, C Vondrick, arXiv:2303.081282023arXiv preprint</p>
<p>Visual programming: Compositional visual reasoning without training. T Gupta, A Kembhavi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition202314962</p>
<p>Language models are visual reasoning coordinators. L Chen, B Li, S Shen, J Yang, C Li, K Keutzer, T Darrell, Z Liu, ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models. 2023</p>
<p>Chameleon: Plug-and-play compositional reasoning with large language models. P Lu, B Peng, H Cheng, M Galley, K.-W Chang, Y N Wu, S.-C Zhu, J Gao, arXiv:2304.098422023arXiv preprint</p>
<p>Critic: Large language models can self-correct with tool-interactive critiquing. Z Gou, Z Shao, Y Gong, Y Shen, Y Yang, N Duan, W Chen, arXiv:2305.117382023arXiv preprint</p>
<p>Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis. Y Liang, C Wu, T Song, W Wu, Y Xia, Y Liu, Y Ou, S Lu, L Ji, S Mao, arXiv:2303.164342023arXiv preprint</p>
<p>Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings. S Hao, T Liu, Z Wang, Z Hu, arXiv:2305.115542023arXiv preprint</p>
<p>Art: Automatic multi-step reasoning and tool-use for large language models. B Paranjape, S Lundberg, S Singh, H Hajishirzi, L Zettlemoyer, M T Ribeiro, arXiv:2303.090142023arXiv preprint</p>
<p>Language models can solve computer tasks. G Kim, P Baldi, S Mcaleer, arXiv:2303.174912023arXiv preprint</p>
<p>Large language models as tool makers. T Cai, X Wang, T Ma, X Chen, D Zhou, arXiv:2305.171262023arXiv preprint</p>
<p>Computegpt: A computational chat model for numerical problems. R H Lewis, J Jiao, arXiv:2305.062232023arXiv preprint</p>
<p>Pal: Programaided language models. L Gao, A Madaan, S Zhou, U Alon, P Liu, Y Yang, J Callan, G Neubig, International Conference on Machine Learning. PMLR202310799</p>
<p>Voyager: An open-ended embodied agent with large language models. G Wang, Y Xie, Y Jiang, A Mandlekar, C Xiao, Y Zhu, L Fan, A Anandkumar, arXiv:2305.162912023arXiv preprint</p>
<p>Creator: Disentangling abstract and concrete reasonings of large language models through tool creation. C Qian, C Han, Y R Fung, Y Qin, Z Liu, H Ji, arXiv:2305.143182023arXiv preprint</p>
<p>Low-code llm: Visual programming over llms. Y Cai, S Mao, W Wu, Z Wang, Y Liang, T Ge, C Wu, W You, T Song, Y Xia, arXiv:2304.081032023arXiv preprint</p>
<p>Language models enable simple systems for generating structured views of heterogeneous data lakes. S Arora, B Yang, S Eyuboglu, A Narayan, A Hojel, I Trummer, C Ré, arXiv:2304.094332023arXiv preprint</p>
<p>Data-copilot: Bridging billions of data and humans with autonomous workflow. W Zhang, Y Shen, W Lu, Y Zhuang, arXiv:2306.072092023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
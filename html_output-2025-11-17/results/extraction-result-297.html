<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-297 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-297</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-297</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-14.html">extraction-schema-14</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <p><strong>Paper ID:</strong> paper-cfa097ad84c0056e8e37bdf3285543eb85a063c5</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/cfa097ad84c0056e8e37bdf3285543eb85a063c5" target="_blank">Methods for Numeracy-Preserving Word Embeddings</a></p>
                <p><strong>Paper Venue:</strong> Conference on Empirical Methods in Natural Language Processing</p>
                <p><strong>Paper TL;DR:</strong> DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: evaluating the ability to capture numeration and magnitude; and to perform list maximum, decoding, and addition.</p>
                <p><strong>Paper Abstract:</strong> Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in the text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (the model is referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream tasks, by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e297.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e297.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DICE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic Independent-of-Corpus Embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A deterministic method that maps numeric magnitude to embedding angle (and optionally radius) so cosine similarity reflects numeric distance; used as fixed numeric embeddings (DICE-D, DICE-2) and evaluated with downstream neural models for arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>DICE numerical embeddings (used with Bi-LSTM / feed-forward nets)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Deterministic numeric embeddings (angle-based mapping) used as input to standard discriminative architectures (Bi-LSTM for list-maximum, linear/FFN for decoding, FFN for addition)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>list maximum (classification of max among inputs), decoding (regress numeric value from embedding), addition (predict sum from two embeddings)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>[0,99], [0,999], [0,9999] (positive integers only in Wallace et al. style experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Handcrafted deterministic embedding mapping magnitude -> angle (polar-to-Cartesian in D dimensions) used as fixed input embeddings; supervised training of downstream nets on tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>List maximum accuracies: 0.98 ([0,99]), 0.87 ([0,999]), 0.96 ([0,9999]); Decoding RMSE: 0.43, 0.83, 3.16 for [0,99],[0,999],[0,9999] respectively; Addition RMSE: 0.75, 2.79, 29.95 for [0,99],[0,999],[0,9999] respectively (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Encodes numeric magnitude as angle on hypersphere so cosine distance is monotonic in numeric distance; arithmetic (e.g., addition) can be achieved by consistent rotations, scalings, and shifts in embedding space, which neural networks can learn more easily than identity mappings; design makes nearby numbers have small cosine distance and far numbers larger cosine distance.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance remains strong across tested ranges but error (RMSE) increases substantially with larger integer ranges (notably addition RMSE grows from 0.75 to 29.95 when moving to [0,9999]); DICE outperforms many baselines especially on small-to-moderate ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Addition RMSE grows with number range (large ranges cause large errors); embedding-based deterministic approach still yields non-negligible error for large ranges—networks must still learn transformations (scale/rotate/shift) and performance degrades as numeric range increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against random vectors, untrained/ trained char models, value embedding (scalar), Word2Vec/GloVe/ELMo/BERT, NAQANet; outperforms most baselines across tasks (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A deterministic angle-based numeric embedding that aligns cosine similarity with numeric distance enables simple networks to perform numeracy tasks (max, decode, add) better than many standard embeddings, although errors grow with numeric range.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Methods for Numeracy-Preserving Word Embeddings', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e297.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e297.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Value embedding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Value embedding (scalar value as embedding)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple embedding that maps each number to a vector representing its scalar value (i.e., encodes raw numeric value directly), used as a baseline for numeracy tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Value embedding (scalar encoded vectors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Scalar-based embedding treated as input to same downstream architectures (Bi-LSTM, FFNs)</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>list maximum, decoding (regression to numeric value), addition (predict sum)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>[0,99], [0,999], [0,9999]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Direct encoding of numeric value into embedding (no learnt mapping beyond numeric-to-vector)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>List maximum accuracies: 0.99, 0.88, 0.68 for the three ranges; Decoding RMSE: 1.20, 11.23, 275.50; Addition RMSE: 0.30, 15.98, 654.33 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Directly exposes numeric magnitude to the network; works well on small ranges but breaks down as range increases because networks struggle to learn identity mapping or to scale to large absolute values.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performs well for small ranges but degrades drastically with larger numeric ranges (decoding and addition RMSE explode for [0,9999]).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Breaks down for large ranges: huge RMSE for decoding/addition as numeric range grows; networks likely cannot learn identity or handle scale without specialized inductive biases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against DICE and other embedding approaches; value embedding competitive on small ranges but inferior on large ranges relative to DICE.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Encoding numeric value directly is an effective baseline for small ranges but fragile for large-range arithmetic because networks struggle to represent/transform large absolute values.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Methods for Numeracy-Preserving Word Embeddings', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e297.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e297.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Char-models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Character-level models (Char-CNN, Char-LSTM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Character-based neural architectures that embed numerals via their digit/character sequences and are trained (or untrained) to perform numeracy tasks; evaluated as both learned and untrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Char-CNN, Char-LSTM (character-level models)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Character-level CNN and LSTM encoders producing embeddings for numerals, followed by task-specific prediction layers</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>list maximum, decoding, addition</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>[0,99], [0,999], [0,9999]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Learn digit-by-digit representations via character-level networks, trained on the tasks</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Trained Char-CNN (List max acc): 0.97,0.93,0.88; Decoding RMSE: 2.50,4.92,11.57; Addition RMSE:1.19,7.75,15.09 (Table 2). Untrained char models perform worse.</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Digit-wise representations can capture compositional numeric structure (digit patterns) and remain competitive, suggesting digit-by-digit encodings may help arithmetic generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Trained char models remain relatively robust as range increases compared to some baselines, but decoding/addition error still increases with range.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Untrained versions perform poorly; trained versions still accumulate error on larger ranges and are not perfect for addition across large numeric ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to DICE, value embeddings, pretrained embeddings, NAQANet; char models competitive especially when trained, but DICE often outperforms across many settings.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Character-level digit-aware models provide a viable way to encode numerals and perform arithmetic tasks, indicating digitwise structure is a useful inductive bias, but specialized numeric embeddings like DICE can outperform them.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Methods for Numeracy-Preserving Word Embeddings', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e297.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e297.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pretrained embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Standard pretrained word/contextual embeddings (Word2Vec, GloVe, ELMo, BERT)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Common non-contextual and contextual pretrained embeddings evaluated as baselines for numeracy tasks; generally perform poorly on explicit arithmetic compared to numeric-aware embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Word2Vec / GloVe / ELMo / BERT (as embedding baselines)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Non-contextual (Word2Vec, GloVe) and contextual (ELMo: biLSTM-based contextualizer; BERT: Transformer encoder) embeddings used as fixed inputs or contextual representations</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>list maximum, decoding, addition; also NUM and MAG probing (numeration, magnitude)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>[0,99], [0,999], [0,9999] for list/decoding/addition; NUM/MAG tests across sets used by Naik et al.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Off-the-shelf pretrained embeddings used as numeric token embeddings or contextual outputs; evaluated without numeric-specific augmentation (except when fine-tuned in other experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Examples from Table 2: Word2Vec/GloVe list max ≈0.90 accuracy on [0,99], lower at larger ranges; decoding RMSE around 2-3 for [0,99] growing to hundreds for [0,9999]; BERT list max 0.95/0.62/0.52 across ranges, decoding/addition RMSE worse for larger ranges (Table 2). On NUM/MAG tests, non-contextual embeddings score poorly compared to DICE (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Pretrained embeddings do not encode numeric magnitude reliably—numbers are treated as tokens; contextual models sometimes learn task-specific patterns but lack built-in numeric distance structure.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Contextual models like ELMo/ BERT can help a bit for small ranges but generally performance degrades with increasing numeric range; pretrained non-contextual embeddings fail to represent magnitude meaningfully.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Fail to capture numeration (mapping between numeral and word forms) and magnitude reliably; produce unintuitive similarities and large errors on arithmetic/regression tasks, especially for larger numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Benchmarked against DICE, value embeddings, char models, NAQANet; DICE and numerically-aware methods outperform these pretrained baselines on arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Standard pretrained embeddings (non-numeric-aware) are poor at arithmetic: they lack magnitude-aware geometry, causing failures in numeration, decoding, and arithmetic especially as numeric complexity rises.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Methods for Numeracy-Preserving Word Embeddings', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e297.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e297.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BERT+L_num</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BERT with numeracy auxiliary loss (L_num)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Fine-tuning BERT with an auxiliary numeracy loss that enforces cosine distance between contextual numeric embeddings to be proportional to scaled absolute numeric difference, improving numerical QA performance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BERT-base-uncased + L_num regularization</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>110M (BERT-base typical, but paper does not report exact params)</td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Transformer encoder (BERT-base) fine-tuned with added auxiliary loss on numeric token embeddings</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>numerical question answering (numeric span extraction), implicitly requiring numeric comparison and selection; not explicit arithmetic operation regression in this experiment</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Answers pruned to numeric ranges up to 30,000 in sub-splits; Sub-split1 and Sub-split2 training setups described (numbers in [1,30000])</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Fine-tuning with auxiliary loss L_num = || 2*|x-y|/(|x|+|y|) - d_cos(x,y) ||_2 evaluated on pairs of contextual numeric embeddings within a minibatch; tuned with λ=1e-3</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>On SQuAD sub-splits, BERT+L_num improved F1 by up to 0.48 points on pure-numeric-answer split and up to 1.12 F1 points when trained on mixed answers and evaluated on numeric-only answers; Table 5 reports F1 improvements across epochs (e.g., Sub-split1 epoch3: BERT 91.12 vs BERT+L_num 91.46 F1).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Encouraging numeric geometry in contextual representations (cosine distance proportional to numeric distance) helps BERT better localize/select numeric answers—suggests that shaping embedding geometry is a viable mechanism for improving numeric behavior in transformers.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Small but consistent improvements across epochs and splits; gains depend on availability of numeric pairs in batches (signal weak when numeric pairs are sparse). Paper does not report model-size scaling.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>When numeric pairs are sparse in training batches, auxiliary loss yields weak signal and smaller gains; baseline BERT can overfit to selecting whole spans/patterns rather than precise numbers.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against BERT-base fine-tuned without L_num on pruned SQuAD sub-splits; improvements reported as ablation across epochs and training splits.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A simple auxiliary loss that enforces numeric-distance geometry in contextual embeddings measurably improves BERT's numerical question answering, showing that shaping embedding geometry is an effective intervention.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Methods for Numeracy-Preserving Word Embeddings', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e297.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e297.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAQANet</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Numerically Augmented QANet (NAQANet)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A QA model augmented to perform arithmetic operations (addition/subtraction) over numbers in passages (developed for the DROP dataset), cited and used as a strong numerical-reasoning baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NAQANet (QANet-based numeracy model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>QANet backbone with additional output heads for numeric operations (addition/subtraction) over passage numbers</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>addition and subtraction over numbers in text (discrete reasoning required in DROP-style tasks)</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>Designed for DROP-style QA; in comparisons here used in list/decoding/addition benchmarks across integer ranges [0,99] to [0,9999]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Model includes heads to predict arithmetic operations and extract numeric answers; trained on DROP-style supervision</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>In Table 2: NAQANet list maximum accuracies 0.91,0.81,0.72 across ranges; Decoding RMSE 2.99,14.19,62.17; Addition RMSE 1.11,11.33,90.01 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Explicitly models numeric operations (addition/subtraction) as specialized output predictions rather than relying on representation geometry alone; this specialization helps on discrete reasoning tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance degrades with larger numeric ranges (RMSE and accuracy worsen with increasing range), but remains a strong numeracy baseline for QA-style discrete reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Higher error for larger numeric ranges; relies on explicit supervision for arithmetic operations and may not generalize to arbitrary numeric computations outside training distribution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against DICE, value embedding, pretrained embeddings, char models in the arithmetic benchmarks; DICE outperforms NAQANet in many of the tested tasks/ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>A model explicitly augmented to predict arithmetic operations is a strong baseline for numeric QA, but embedding-level numeric geometry (DICE) can yield competitive or superior performance in some arithmetic tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Methods for Numeracy-Preserving Word Embeddings', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e297.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e297.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how language models perform arithmetic operations, including the types of arithmetic tasks, model properties, performance results, methods used, and any mechanistic insights about how the models solve arithmetic problems.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Untrained models</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Untrained CNN / Untrained LSTM baselines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectures with random/untrained parameters used as weak baselines to probe whether architecture alone induces numeracy without task training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Untrained CNN, Untrained LSTM (randomly initialized, not task-trained)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_architecture</strong></td>
                            <td>Randomly initialized convolutional and LSTM encoders (no training) used to embed numerals, followed by direct evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>arithmetic_operation_type</strong></td>
                            <td>list maximum, decoding, addition</td>
                        </tr>
                        <tr>
                            <td><strong>number_range_or_complexity</strong></td>
                            <td>[0,99], [0,999], [0,9999]</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>No training (untrained), used to assess architectural inductive biases</td>
                        </tr>
                        <tr>
                            <td><strong>performance_result</strong></td>
                            <td>Untrained CNN list max acc: 0.97,0.87,0.84; Decoding RMSE: 2.64,9.67,44.40; Addition RMSE:1.41,14.43,69.14. Untrained LSTM performs worse (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>mechanistic_insight</strong></td>
                            <td>Even untrained convolutional architectures can capture some digit-level regularities via parameterized filters, but without training they are far from competitive on precise regression tasks as range increases.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_scaling</strong></td>
                            <td>Performance is limited and deteriorates with larger numeric range; training substantially improves results.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>High RMSE for large ranges and lower accuracy compared to trained/numeric-aware methods; cannot perform precise arithmetic without training.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Serves as lower-bound compared to trained char models, DICE, value embeddings, pretrained embeddings; DICE and trained models outperform untrained baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>key_finding</strong></td>
                            <td>Architectural inductive bias alone (untrained CNN/LSTM) gives some signal for small-range tasks but is insufficient for accurate arithmetic—training and numeric-aware embeddings are necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Methods for Numeracy-Preserving Word Embeddings', 'publication_date_yy_mm': '2020-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do nlp models know numbers? <em>(Rating: 2)</em></li>
                <li>Exploring numeracy in word embeddings <em>(Rating: 2)</em></li>
                <li>Numeracy-600k: Learning numeracy for detecting exaggerated information in market comments <em>(Rating: 2)</em></li>
                <li>Neural arithmetic logic units <em>(Rating: 2)</em></li>
                <li>DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-297",
    "paper_id": "paper-cfa097ad84c0056e8e37bdf3285543eb85a063c5",
    "extraction_schema_id": "extraction-schema-14",
    "extracted_data": [
        {
            "name_short": "DICE",
            "name_full": "Deterministic Independent-of-Corpus Embeddings",
            "brief_description": "A deterministic method that maps numeric magnitude to embedding angle (and optionally radius) so cosine similarity reflects numeric distance; used as fixed numeric embeddings (DICE-D, DICE-2) and evaluated with downstream neural models for arithmetic tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "DICE numerical embeddings (used with Bi-LSTM / feed-forward nets)",
            "model_size": null,
            "model_architecture": "Deterministic numeric embeddings (angle-based mapping) used as input to standard discriminative architectures (Bi-LSTM for list-maximum, linear/FFN for decoding, FFN for addition)",
            "arithmetic_operation_type": "list maximum (classification of max among inputs), decoding (regress numeric value from embedding), addition (predict sum from two embeddings)",
            "number_range_or_complexity": "[0,99], [0,999], [0,9999] (positive integers only in Wallace et al. style experiments)",
            "method_or_intervention": "Handcrafted deterministic embedding mapping magnitude -&gt; angle (polar-to-Cartesian in D dimensions) used as fixed input embeddings; supervised training of downstream nets on tasks",
            "performance_result": "List maximum accuracies: 0.98 ([0,99]), 0.87 ([0,999]), 0.96 ([0,9999]); Decoding RMSE: 0.43, 0.83, 3.16 for [0,99],[0,999],[0,9999] respectively; Addition RMSE: 0.75, 2.79, 29.95 for [0,99],[0,999],[0,9999] respectively (Table 2).",
            "mechanistic_insight": "Encodes numeric magnitude as angle on hypersphere so cosine distance is monotonic in numeric distance; arithmetic (e.g., addition) can be achieved by consistent rotations, scalings, and shifts in embedding space, which neural networks can learn more easily than identity mappings; design makes nearby numbers have small cosine distance and far numbers larger cosine distance.",
            "performance_scaling": "Performance remains strong across tested ranges but error (RMSE) increases substantially with larger integer ranges (notably addition RMSE grows from 0.75 to 29.95 when moving to [0,9999]); DICE outperforms many baselines especially on small-to-moderate ranges.",
            "failure_modes": "Addition RMSE grows with number range (large ranges cause large errors); embedding-based deterministic approach still yields non-negligible error for large ranges—networks must still learn transformations (scale/rotate/shift) and performance degrades as numeric range increases.",
            "comparison_baseline": "Compared against random vectors, untrained/ trained char models, value embedding (scalar), Word2Vec/GloVe/ELMo/BERT, NAQANet; outperforms most baselines across tasks (Table 2).",
            "key_finding": "A deterministic angle-based numeric embedding that aligns cosine similarity with numeric distance enables simple networks to perform numeracy tasks (max, decode, add) better than many standard embeddings, although errors grow with numeric range.",
            "uuid": "e297.0",
            "source_info": {
                "paper_title": "Methods for Numeracy-Preserving Word Embeddings",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Value embedding",
            "name_full": "Value embedding (scalar value as embedding)",
            "brief_description": "A simple embedding that maps each number to a vector representing its scalar value (i.e., encodes raw numeric value directly), used as a baseline for numeracy tasks.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Value embedding (scalar encoded vectors)",
            "model_size": null,
            "model_architecture": "Scalar-based embedding treated as input to same downstream architectures (Bi-LSTM, FFNs)",
            "arithmetic_operation_type": "list maximum, decoding (regression to numeric value), addition (predict sum)",
            "number_range_or_complexity": "[0,99], [0,999], [0,9999]",
            "method_or_intervention": "Direct encoding of numeric value into embedding (no learnt mapping beyond numeric-to-vector)",
            "performance_result": "List maximum accuracies: 0.99, 0.88, 0.68 for the three ranges; Decoding RMSE: 1.20, 11.23, 275.50; Addition RMSE: 0.30, 15.98, 654.33 (Table 2).",
            "mechanistic_insight": "Directly exposes numeric magnitude to the network; works well on small ranges but breaks down as range increases because networks struggle to learn identity mapping or to scale to large absolute values.",
            "performance_scaling": "Performs well for small ranges but degrades drastically with larger numeric ranges (decoding and addition RMSE explode for [0,9999]).",
            "failure_modes": "Breaks down for large ranges: huge RMSE for decoding/addition as numeric range grows; networks likely cannot learn identity or handle scale without specialized inductive biases.",
            "comparison_baseline": "Compared against DICE and other embedding approaches; value embedding competitive on small ranges but inferior on large ranges relative to DICE.",
            "key_finding": "Encoding numeric value directly is an effective baseline for small ranges but fragile for large-range arithmetic because networks struggle to represent/transform large absolute values.",
            "uuid": "e297.1",
            "source_info": {
                "paper_title": "Methods for Numeracy-Preserving Word Embeddings",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Char-models",
            "name_full": "Character-level models (Char-CNN, Char-LSTM)",
            "brief_description": "Character-based neural architectures that embed numerals via their digit/character sequences and are trained (or untrained) to perform numeracy tasks; evaluated as both learned and untrained baselines.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Char-CNN, Char-LSTM (character-level models)",
            "model_size": null,
            "model_architecture": "Character-level CNN and LSTM encoders producing embeddings for numerals, followed by task-specific prediction layers",
            "arithmetic_operation_type": "list maximum, decoding, addition",
            "number_range_or_complexity": "[0,99], [0,999], [0,9999]",
            "method_or_intervention": "Learn digit-by-digit representations via character-level networks, trained on the tasks",
            "performance_result": "Trained Char-CNN (List max acc): 0.97,0.93,0.88; Decoding RMSE: 2.50,4.92,11.57; Addition RMSE:1.19,7.75,15.09 (Table 2). Untrained char models perform worse.",
            "mechanistic_insight": "Digit-wise representations can capture compositional numeric structure (digit patterns) and remain competitive, suggesting digit-by-digit encodings may help arithmetic generalization.",
            "performance_scaling": "Trained char models remain relatively robust as range increases compared to some baselines, but decoding/addition error still increases with range.",
            "failure_modes": "Untrained versions perform poorly; trained versions still accumulate error on larger ranges and are not perfect for addition across large numeric ranges.",
            "comparison_baseline": "Compared to DICE, value embeddings, pretrained embeddings, NAQANet; char models competitive especially when trained, but DICE often outperforms across many settings.",
            "key_finding": "Character-level digit-aware models provide a viable way to encode numerals and perform arithmetic tasks, indicating digitwise structure is a useful inductive bias, but specialized numeric embeddings like DICE can outperform them.",
            "uuid": "e297.2",
            "source_info": {
                "paper_title": "Methods for Numeracy-Preserving Word Embeddings",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Pretrained embeddings",
            "name_full": "Standard pretrained word/contextual embeddings (Word2Vec, GloVe, ELMo, BERT)",
            "brief_description": "Common non-contextual and contextual pretrained embeddings evaluated as baselines for numeracy tasks; generally perform poorly on explicit arithmetic compared to numeric-aware embeddings.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Word2Vec / GloVe / ELMo / BERT (as embedding baselines)",
            "model_size": null,
            "model_architecture": "Non-contextual (Word2Vec, GloVe) and contextual (ELMo: biLSTM-based contextualizer; BERT: Transformer encoder) embeddings used as fixed inputs or contextual representations",
            "arithmetic_operation_type": "list maximum, decoding, addition; also NUM and MAG probing (numeration, magnitude)",
            "number_range_or_complexity": "[0,99], [0,999], [0,9999] for list/decoding/addition; NUM/MAG tests across sets used by Naik et al.",
            "method_or_intervention": "Off-the-shelf pretrained embeddings used as numeric token embeddings or contextual outputs; evaluated without numeric-specific augmentation (except when fine-tuned in other experiments).",
            "performance_result": "Examples from Table 2: Word2Vec/GloVe list max ≈0.90 accuracy on [0,99], lower at larger ranges; decoding RMSE around 2-3 for [0,99] growing to hundreds for [0,9999]; BERT list max 0.95/0.62/0.52 across ranges, decoding/addition RMSE worse for larger ranges (Table 2). On NUM/MAG tests, non-contextual embeddings score poorly compared to DICE (Table 1).",
            "mechanistic_insight": "Pretrained embeddings do not encode numeric magnitude reliably—numbers are treated as tokens; contextual models sometimes learn task-specific patterns but lack built-in numeric distance structure.",
            "performance_scaling": "Contextual models like ELMo/ BERT can help a bit for small ranges but generally performance degrades with increasing numeric range; pretrained non-contextual embeddings fail to represent magnitude meaningfully.",
            "failure_modes": "Fail to capture numeration (mapping between numeral and word forms) and magnitude reliably; produce unintuitive similarities and large errors on arithmetic/regression tasks, especially for larger numbers.",
            "comparison_baseline": "Benchmarked against DICE, value embeddings, char models, NAQANet; DICE and numerically-aware methods outperform these pretrained baselines on arithmetic tasks.",
            "key_finding": "Standard pretrained embeddings (non-numeric-aware) are poor at arithmetic: they lack magnitude-aware geometry, causing failures in numeration, decoding, and arithmetic especially as numeric complexity rises.",
            "uuid": "e297.3",
            "source_info": {
                "paper_title": "Methods for Numeracy-Preserving Word Embeddings",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "BERT+L_num",
            "name_full": "BERT with numeracy auxiliary loss (L_num)",
            "brief_description": "Fine-tuning BERT with an auxiliary numeracy loss that enforces cosine distance between contextual numeric embeddings to be proportional to scaled absolute numeric difference, improving numerical QA performance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BERT-base-uncased + L_num regularization",
            "model_size": "110M (BERT-base typical, but paper does not report exact params)",
            "model_architecture": "Transformer encoder (BERT-base) fine-tuned with added auxiliary loss on numeric token embeddings",
            "arithmetic_operation_type": "numerical question answering (numeric span extraction), implicitly requiring numeric comparison and selection; not explicit arithmetic operation regression in this experiment",
            "number_range_or_complexity": "Answers pruned to numeric ranges up to 30,000 in sub-splits; Sub-split1 and Sub-split2 training setups described (numbers in [1,30000])",
            "method_or_intervention": "Fine-tuning with auxiliary loss L_num = || 2*|x-y|/(|x|+|y|) - d_cos(x,y) ||_2 evaluated on pairs of contextual numeric embeddings within a minibatch; tuned with λ=1e-3",
            "performance_result": "On SQuAD sub-splits, BERT+L_num improved F1 by up to 0.48 points on pure-numeric-answer split and up to 1.12 F1 points when trained on mixed answers and evaluated on numeric-only answers; Table 5 reports F1 improvements across epochs (e.g., Sub-split1 epoch3: BERT 91.12 vs BERT+L_num 91.46 F1).",
            "mechanistic_insight": "Encouraging numeric geometry in contextual representations (cosine distance proportional to numeric distance) helps BERT better localize/select numeric answers—suggests that shaping embedding geometry is a viable mechanism for improving numeric behavior in transformers.",
            "performance_scaling": "Small but consistent improvements across epochs and splits; gains depend on availability of numeric pairs in batches (signal weak when numeric pairs are sparse). Paper does not report model-size scaling.",
            "failure_modes": "When numeric pairs are sparse in training batches, auxiliary loss yields weak signal and smaller gains; baseline BERT can overfit to selecting whole spans/patterns rather than precise numbers.",
            "comparison_baseline": "Compared against BERT-base fine-tuned without L_num on pruned SQuAD sub-splits; improvements reported as ablation across epochs and training splits.",
            "key_finding": "A simple auxiliary loss that enforces numeric-distance geometry in contextual embeddings measurably improves BERT's numerical question answering, showing that shaping embedding geometry is an effective intervention.",
            "uuid": "e297.4",
            "source_info": {
                "paper_title": "Methods for Numeracy-Preserving Word Embeddings",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "NAQANet",
            "name_full": "Numerically Augmented QANet (NAQANet)",
            "brief_description": "A QA model augmented to perform arithmetic operations (addition/subtraction) over numbers in passages (developed for the DROP dataset), cited and used as a strong numerical-reasoning baseline.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "NAQANet (QANet-based numeracy model)",
            "model_size": null,
            "model_architecture": "QANet backbone with additional output heads for numeric operations (addition/subtraction) over passage numbers",
            "arithmetic_operation_type": "addition and subtraction over numbers in text (discrete reasoning required in DROP-style tasks)",
            "number_range_or_complexity": "Designed for DROP-style QA; in comparisons here used in list/decoding/addition benchmarks across integer ranges [0,99] to [0,9999]",
            "method_or_intervention": "Model includes heads to predict arithmetic operations and extract numeric answers; trained on DROP-style supervision",
            "performance_result": "In Table 2: NAQANet list maximum accuracies 0.91,0.81,0.72 across ranges; Decoding RMSE 2.99,14.19,62.17; Addition RMSE 1.11,11.33,90.01 (Table 2).",
            "mechanistic_insight": "Explicitly models numeric operations (addition/subtraction) as specialized output predictions rather than relying on representation geometry alone; this specialization helps on discrete reasoning tasks.",
            "performance_scaling": "Performance degrades with larger numeric ranges (RMSE and accuracy worsen with increasing range), but remains a strong numeracy baseline for QA-style discrete reasoning.",
            "failure_modes": "Higher error for larger numeric ranges; relies on explicit supervision for arithmetic operations and may not generalize to arbitrary numeric computations outside training distribution.",
            "comparison_baseline": "Compared against DICE, value embedding, pretrained embeddings, char models in the arithmetic benchmarks; DICE outperforms NAQANet in many of the tested tasks/ranges.",
            "key_finding": "A model explicitly augmented to predict arithmetic operations is a strong baseline for numeric QA, but embedding-level numeric geometry (DICE) can yield competitive or superior performance in some arithmetic tasks.",
            "uuid": "e297.5",
            "source_info": {
                "paper_title": "Methods for Numeracy-Preserving Word Embeddings",
                "publication_date_yy_mm": "2020-11"
            }
        },
        {
            "name_short": "Untrained models",
            "name_full": "Untrained CNN / Untrained LSTM baselines",
            "brief_description": "Architectures with random/untrained parameters used as weak baselines to probe whether architecture alone induces numeracy without task training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Untrained CNN, Untrained LSTM (randomly initialized, not task-trained)",
            "model_size": null,
            "model_architecture": "Randomly initialized convolutional and LSTM encoders (no training) used to embed numerals, followed by direct evaluation",
            "arithmetic_operation_type": "list maximum, decoding, addition",
            "number_range_or_complexity": "[0,99], [0,999], [0,9999]",
            "method_or_intervention": "No training (untrained), used to assess architectural inductive biases",
            "performance_result": "Untrained CNN list max acc: 0.97,0.87,0.84; Decoding RMSE: 2.64,9.67,44.40; Addition RMSE:1.41,14.43,69.14. Untrained LSTM performs worse (Table 2).",
            "mechanistic_insight": "Even untrained convolutional architectures can capture some digit-level regularities via parameterized filters, but without training they are far from competitive on precise regression tasks as range increases.",
            "performance_scaling": "Performance is limited and deteriorates with larger numeric range; training substantially improves results.",
            "failure_modes": "High RMSE for large ranges and lower accuracy compared to trained/numeric-aware methods; cannot perform precise arithmetic without training.",
            "comparison_baseline": "Serves as lower-bound compared to trained char models, DICE, value embeddings, pretrained embeddings; DICE and trained models outperform untrained baselines.",
            "key_finding": "Architectural inductive bias alone (untrained CNN/LSTM) gives some signal for small-range tasks but is insufficient for accurate arithmetic—training and numeric-aware embeddings are necessary.",
            "uuid": "e297.6",
            "source_info": {
                "paper_title": "Methods for Numeracy-Preserving Word Embeddings",
                "publication_date_yy_mm": "2020-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do nlp models know numbers?",
            "rating": 2
        },
        {
            "paper_title": "Exploring numeracy in word embeddings",
            "rating": 2
        },
        {
            "paper_title": "Numeracy-600k: Learning numeracy for detecting exaggerated information in market comments",
            "rating": 2
        },
        {
            "paper_title": "Neural arithmetic logic units",
            "rating": 2
        },
        {
            "paper_title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs",
            "rating": 2
        }
    ],
    "cost": 0.014095,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Methods for Numeracy-Preserving Word Embeddings</h1>
<p>Dhanasekar Sundararaman ${ }^{1}$, Shijing $\mathrm{Si}^{1}$, Vivek Subramanian ${ }^{1}$, Guoyin Wang ${ }^{2}$, Devamanyu Hazarika ${ }^{3}$, Lawrence Carin ${ }^{1}$<br>${ }^{1}$ Duke University<br>${ }^{2}$ Amazon Alexa AI<br>${ }^{3}$ National University of Singapore<br>dhanasekar.sundararaman@duke.edu</p>
<h4>Abstract</h4>
<p>Word embedding models are typically able to capture the semantics of words via the distributional hypothesis, but fail to capture the numerical properties of numbers that appear in a text. This leads to problems with numerical reasoning involving tasks such as question answering. We propose a new methodology to assign and learn embeddings for numbers. Our approach creates Deterministic, Independent-of-Corpus Embeddings (referred to as DICE) for numbers, such that their cosine similarity reflects the actual distance on the number line. DICE outperforms a wide range of pre-trained word embedding models across multiple examples of two tasks: (i) evaluating the ability to capture numeration and magnitude; and (ii) to perform list maximum, decoding, and addition. We further explore the utility of these embeddings in downstream applications by initializing numbers with our approach for the task of magnitude prediction. We also introduce a regularization approach to learn model-based embeddings of numbers in a contextual setting.</p>
<h2>1 Introduction</h2>
<p>Word embeddings capture semantic relationships between words by operationalizing the distributional hypothesis (Harris, 1954; Firth, 1957). They can be learned either non-contextually (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017) or contextually (Devlin et al., 2018; Peters et al., 2018). Non-contextual embeddings have worked well on various language understanding and semantic tasks (Rumelhart et al., 1988; Mikolov et al., 2013a,b). More recently, they have also been used as pre-trained word embeddings to aid more sophisticated contextual models for solving rigorous natural language processing (NLP) problems, including translation, paraphrasing, and sentence-similarity tasks (Kiros et al., 2015; Wieting et al., 2015).</p>
<p>While word embeddings effectively capture semantic relationships between words, they are less effective at capturing numeric properties associated with numbers. Though numbers represent a significant percentage of tokens in a corpus, they are often overlooked. In non-contextual word embedding models, they are treated like any other word, which leads to misinterpretation. For instance, they exhibit unintuitive similarities with other words and do not contain strong prior information about the magnitude of the number they encode. In sentence similarity and reasoning tasks, failure to handle numbers causes as much as $29 \%$ of contradictions (De Marneffe et al., 2008). In other data-intensive tasks where numbers are abundant, like neural machine translation, they are masked to hide the translation models inefficiency in dealing with them (Mitchell and Lapata, 2009).</p>
<p>There are a variety of tests proposed to measure the efficiency of number embeddings. For instance, Naik et al. (2019) shows that GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013b), and fastText (Joulin et al., 2016; Bojanowski et al., 2017) fail to capture numeration and magnitude properties of a number. Numeration is the property of associating numbers with their corresponding word representations (" 3 " and "three") while magnitude represents a number's actual value $(3&lt;4)$. Further, Wallace et al. (2019) proposes several tests for analyzing numerical reasoning of number embeddings that include list maximum, decoding, and addition.</p>
<p>In this paper, we experimentally demonstrate that if the cosine similarity between word embeddings of two numbers reflects their actual distance on the number line, the resultant word embeddings are useful in downstream tasks. We first demonstrate how Deterministic, Independent-of-Corpus Embeddings (DICE) can be constructed such that they almost perfectly capture properties of numera-</p>
<p>tion and magnitude. These non-contextual embeddings also perform well on related tests for numeracy (Wallace et al., 2019).</p>
<p>To demonstrate the efficacy of DICE for downstream tasks, we explore its utility in two experiments. First, we design a DICE embedding initialized Bi-LSTM network to classify the magnitude of masked numbers in the 600 K dataset (Chen et al., 2019). Second, given the popularity of modern contextual model-based embeddings, we devise a regularization procedure that emulates the hypothesis proposed by DICE and can be employed in any task-based fine-tuning process. We demonstrate that adding such regularization helps the model internalize notions of numeracy while learning task-based contextual embeddings for the numbers present in the text. We find promising results in a numerical reasoning task that involves numerical question answering based on a sub-split of the popular SQuAD dataset (Rajpurkar et al., 2016).</p>
<p>Our contribution can be summarized as follows:</p>
<ul>
<li>We propose a deterministic technique to learn numerical embeddings. DICE embeddings are learned independently of corpus and effectively capture properties of numeracy.</li>
<li>We prove experimentally that the resultant embeddings learned using the above methods improve a model's ability to reason about numbers in a variety of tasks, including numeration, magnitude, list maximum, decoding, and addition.</li>
<li>We also demonstrate that properties of DICE can be adapted to contextual models, like BERT (Devlin et al., 2018), through a novel regularization technique for solving tasks involving numerical reasoning.</li>
</ul>
<h2>2 Related Work</h2>
<p>The major research lines in this area have been dedicated to $(i)$ devising probing tests and curating resources to evaluate the numerical reasoning abilities of pre-trained embeddings, and (ii) proposing new models that learn these properties.</p>
<p>Naik et al. (2019) surveyed a number of noncontextual word embedding models and highlighted the failure of those models in capturing two essential properties of numbers - numeration and magnitude. Chen et al. (2019) created a novel
dataset named Numeracy-600k, a collection of approximately 600,000 sentences from market comments with a diverse set of numbers representing age, height, weight, year, etc. The authors use neural network models, including a GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRUcapsule, to classify the magnitude of each number. Wallace et al. (2019) compares and contrasts the numerical reasoning ability of a variety of noncontextual as well as contextual embedding models. The authors also proposed three tests - list maximum, decoding, and addition - to judge the numerical reasoning ability of embeddings of numerals. They infer that word embedding models that perform the best on these three tests have captured the numerical properties of numbers well. Therefore, we consider these proposed tests in our evaluation. (Spithourakis and Riedel, 2018) used a variety of models to distinguish numbers from words, and demonstrated that this ability reduces model perplexity with neural machine translation. Weiss et al. (2018) found that neural networks are capable of reasoning numbers with explicit supervision.</p>
<p>Numerically Augmented QANet (NAQANet) (Dua et al., 2019) was built by adding an output layer on top of QANet (Yu et al., 2018) to predict answers based on addition and subtraction over numbers in the DROP dataset. Our work, in contrast, offers a simple methodology that can be added to any model as a regularization technique. Our work is more similar to Jiang et al. (2019), where embedding of a number is learned as a simple weighted average of its prototype embeddings. Such embeddings are used in tasks like word similarity, sequence labeling and have been proven to be effective.</p>
<h2>3 Methods</h2>
<p>To overcome NLP models inefficiency in dealing with numbers, we consider our method DICE to form embeddings. To begin, we embed numerals and word forms of numbers as vectors $\mathbf{e}_{i} \in \mathbb{R}^{D}$, where $i$ indexes numerals identified within a corpus. We first preprocess by parsing the corpora associated with each of our tasks (described below) for numbers in numeral and word forms to populate a number vocabulary. Then, the dimensionality of the embeddings required for that task is fixed. We explicitly associate the embeddings of a numeral and word forms of numbers to have the same embedding.</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Proposed DICE embeddings. Vectors are colored according to numeral magnitude. Note that addition of two numbers in this embedding is performed by a shift, scaling, and rotation. Scaling depends only on the vector being added, as illustrated in sub-figure (c) in which the two black lines, corresponding to identical $\mathbf{e}_j$, have the same length.</p>
<h3>3.1 DICE embeddings</h3>
<p>In designing embeddings that capture the aforementioned properties of numeration and magnitude, we consider a deterministic, handcrafted approach (depicted in Figures 1a and 1b). This method relies on the fact that tests for both numeration and magnitude are concerned with the correspondence in similarity between numbers in token space and numbers in embedding space. In token space, two numbers $x, y \in \mathbb{R}$, in numeral or word form (with the latter being mapped to its corresponding numeral form for comparison), can be compared using absolute difference, i.e.:</p>
<p>$$d_n(x, y) = |x - y| \tag{1}$$</p>
<p>The absolute value ensures that two numbers are treated as equally distant regardless of whether $x \geq y$ or $y \geq x$. On the other hand, two embeddings $x, y \in \mathbb{R}^D$ are typically compared via cosine similarity, given by:</p>
<p>$$s_e(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x}^T \mathbf{y}}{||\mathbf{x}||<em 2="2">{2}||\mathbf{y}||</em>$$}} = \cos(\theta) \tag{2</p>
<p>$$d_e(\mathbf{x}, \mathbf{y}) = 1 - \cos(\theta) \tag{3}$$</p>
<p>where $\theta$ is the angle between $\mathbf{x}$ and $\mathbf{y}$ and $d_e(\mathbf{x}, \mathbf{y})$ is their cosine distance. Normalization by the vector lengths ensures that the metric is independent of the lengths of the two vectors.</p>
<p>Note that numerals are compared in terms of distance while their embeddings are compared by similarity. As cosine distance increases, the angle between $\mathbf{x}$ and $\mathbf{y}$ increases monotonically. A distance of zero is achieved when $\mathbf{x}$ and $\mathbf{y}$ are oriented in the same direction. When $\mathbf{x} \perp \mathbf{y}$, the cosine distance is 1; and when $\mathbf{x}$ and $\mathbf{y}$ are antiparallel, cosine distance is 2.</p>
<p>We seek a mapping $(x, y) \mapsto (\mathbf{x}, \mathbf{y})$ such that $d_e$ monotonically increases as $d_n$ increases. We first bound the range of numbers for which we wish to compute embeddings by $[a, b] \subset \mathbb{R}$ and, without loss of generality, restrict $\mathbf{x}$ and $\mathbf{y}$ to be of unit length (i.e., $||\mathbf{x}||<em 2="2">{2} = ||\mathbf{y}||</em> = 1$). Since the cosine function decreases monotonically between 0 and $\pi$, we can simply employ a linear mapping to map distances $s_n \in [0, |a - b|]$ to angles $\theta \in [0, \pi]$:</p>
<p>$$\theta(s_n) = \frac{s_n}{|a - b|} \pi \tag{4}$$</p>
<p>This mapping achieves the desired direct relationship between $s_n$ and $d_e$. Since there are infinitely many choices for $\mathbf{x}$ and $\mathbf{y}$ with angle $\theta$, we simply fix the direction of the vector corresponding to the numeral $a$. Numbers that fall outside $[a, b]$ are mapped to a random angle in $[-\pi, \pi]$. In the corpora we considered, $a$ and $b$ are chosen such that numbers outside $[a, b]$ represent a small fraction of the total set of numbers (approximately 2%).</p>
<p>We employ this mapping to generate numeral embeddings in $\mathbb{R}^D$. Figure 1a shows deterministic, independent-of-corpus embeddings of rank 2 (DICE-2). In this approach we represent angles as vectors in $\mathbb{R}^2$ using the polar-to-Cartesian coordinate transformation:</p>
<p>$$[r, \theta] \mapsto [x_1, x_2] = [r \cos(\theta), r \sin(\theta)]\mathbf{v} \tag{5}$$</p>
<p>where we choose $r = 1$ without loss of generality. We then sample a random matrix $\mathbf{M} \in \mathbb{R}^{D \times D}$ where $D \geq 2$ and $m_{ij} \sim \mathcal{N}(0, 1)$ and perform a QR decomposition on $\mathbf{M}$ to obtain a matrix $\mathbf{Q}$</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">OVA</th>
<th style="text-align: center;">SC</th>
<th style="text-align: center;">BC</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Random</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">48.92</td>
<td style="text-align: center;">49.34</td>
</tr>
<tr>
<td style="text-align: center;">Glove 6B-200D</td>
<td style="text-align: center;">15.88</td>
<td style="text-align: center;">62.21</td>
<td style="text-align: center;">83.94</td>
</tr>
<tr>
<td style="text-align: center;">Glove 6B-300D</td>
<td style="text-align: center;">18.41</td>
<td style="text-align: center;">62.92</td>
<td style="text-align: center;">83.98</td>
</tr>
<tr>
<td style="text-align: center;">Glove-840B-300D</td>
<td style="text-align: center;">5.18</td>
<td style="text-align: center;">55.58</td>
<td style="text-align: center;">91.86</td>
</tr>
<tr>
<td style="text-align: center;">FastText-Wiki</td>
<td style="text-align: center;">13.94</td>
<td style="text-align: center;">59.96</td>
<td style="text-align: center;">96.15</td>
</tr>
<tr>
<td style="text-align: center;">FastText-CC</td>
<td style="text-align: center;">7.83</td>
<td style="text-align: center;">53.89</td>
<td style="text-align: center;">85.40</td>
</tr>
<tr>
<td style="text-align: center;">Skip-gram-5</td>
<td style="text-align: center;">8.85</td>
<td style="text-align: center;">55.40</td>
<td style="text-align: center;">96.42</td>
</tr>
<tr>
<td style="text-align: center;">Skip-gram-Dep</td>
<td style="text-align: center;">3.32</td>
<td style="text-align: center;">51.99</td>
<td style="text-align: center;">94.60</td>
</tr>
<tr>
<td style="text-align: center;">DICE- $D$ (ours)</td>
<td style="text-align: center;">$\mathbf{9 5 . 6 3}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6 6}$</td>
<td style="text-align: center;">$\mathbf{9 9 . 6 4}$</td>
</tr>
</tbody>
</table>
<p>Table 1: Performance (\% accuracy) on numeracy tests.
whose columns $\mathbf{q}<em 1:="1:" 2="2">{i}, i=1, \ldots, D$ constitute an orthonormal basis for $\mathbb{R}^{D}$. The DICE-2 embedding $\mathbf{e} \in \mathbb{R}^{D}$ of each numeral is then given by $\mathbf{e}=\mathbf{Q}</em>$.} \mathbf{v}$, where the subscript on $\mathbf{Q}$ indicates taking the first two columns of $\mathbf{Q</p>
<p>In Figure 1b we consider DICE- $D$, in which we generate vectors in $\mathbb{R}^{D}$ by applying a polar-to-Cartesian transformation in $D$ dimensions (Blumenson, 1960):</p>
<p>$$
v_{d}= \begin{cases}{[\sin (\theta)]^{d-1} \cos (\theta),} &amp; 1 \leq d&lt;D \ {[\sin (\theta)]^{D},} &amp; d=D\end{cases}
$$</p>
<p>where the subscripts indicate the coordinate in $\mathbf{v}$. We again apply a QR-decomposition on a random matrix M generated as above, except here we project $\mathbf{v}$ using all $D$ basis vectors. This allows for a random rotation of the embeddings to avoid bias due to choosing $e_{a 1}=1, e_{a i}=0 \forall i \neq 1$. We employ DICE-D embeddings throughout this paper as word embeddings are practically not 2 dimensional.</p>
<h2>4 Experiments</h2>
<p>To observe the numerical properties of DICE, we consider two tasks: Task 1 deals with the numeration (NUM) and magnitude (MAG) properties as proposed by (Naik et al., 2019); Task 2 performs list maximum, decoding, and addition as proposed by (Wallace et al., 2019). We then experiment on two additional tasks to demonstrate the applications of DICE.</p>
<h3>4.1 Task 1: Exploring Numeracy</h3>
<p>In this task, proposed by Naik et al. (2019), there are three tests for examining each property of numeration (NUM, $3=$ "three") and magnitude (MAG, $3&lt;4$ ). For each of these tests, target
numbers in its word or numeral form are evaluated against other numbers as follows:</p>
<ul>
<li>One-vs-All (OVA): The distance between the embedding vector of the target and its nearest neighbor should be smaller than the distance between the target and any other numeral in the data.</li>
<li>Strict Contrastive (SC): The distance of the embedding vector of the target from its nearest neighbor should be smaller than its second nearest neighbor numeral.</li>
<li>Broad Contrastive (BC): The distance of the embedding vector of the target numeral from its nearest neighbor should be smaller than its furthest neighbor.</li>
</ul>
<p>Training Details. We use the Gigaword corpus obtained from the Linguistic Data Consortium to populate the list of numbers from the dataset. Parsing was performed using the text2digits ${ }^{1}$ Python module. As done by Naik et al. (2019), we employ $D=300$ for the DICE- $D$ embeddings. Embeddings of numerals are assigned using the principle explained in Section 3.1, while the embedding of words that denote numbers (word form) simply points to the embedding of that numeral itself. We then perform the six tests (OVA-NUM / OVAMAG, SC-NUM/ SC-MAG, BC-NUM / BC-MAG) on 130 combinations of numbers for NUM and 31,860 combinations of numbers for MAG.</p>
<p>Evaluation. Following Naik et al. (2019), we use accuracy to measure the efficiency of the embeddings. These tests require the fulfillment of certain clauses which are defined in Naik et al. (2019).</p>
<p>Results. Table 1 shows comparisons of the performance of embeddings created by each of the DICE methods on the MAG tests. Compared to the baselines, both DICE methods outperform all commonly employed non-contextual word embedding models in OVA, SC, and BC tests. This is attributed to the cosine distance property addressed in the DICE embeddings. Specifically, because the magnitude of the number is linearly related to its angle, sweeping through numbers in order guarantees an increase in angle along each axis. Numbers that are close to each other in magnitude are rotated further but in proportion to their magnitude. Thus, small and large numbers are ensured to lie</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">List maximum (accuracy)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Decoding (RMSE)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Addition (RMSE)</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Integer range</td>
<td style="text-align: center;">$[0,99]$</td>
<td style="text-align: center;">$[0,999]$</td>
<td style="text-align: center;">$[0,9999]$</td>
<td style="text-align: center;">$[0,99]$</td>
<td style="text-align: center;">$[0,999]$</td>
<td style="text-align: center;">$[0,9999]$</td>
<td style="text-align: center;">$[0,99]$</td>
<td style="text-align: center;">$[0,999]$</td>
<td style="text-align: center;">$[0,9999]$</td>
</tr>
<tr>
<td style="text-align: center;">Random vectors</td>
<td style="text-align: center;">0.16</td>
<td style="text-align: center;">0.23</td>
<td style="text-align: center;">0.21</td>
<td style="text-align: center;">29.86</td>
<td style="text-align: center;">292.88</td>
<td style="text-align: center;">2882.62</td>
<td style="text-align: center;">42.03</td>
<td style="text-align: center;">410.33</td>
<td style="text-align: center;">4389.39</td>
</tr>
<tr>
<td style="text-align: center;">Untrained CNN</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;">2.64</td>
<td style="text-align: center;">9.67</td>
<td style="text-align: center;">44.40</td>
<td style="text-align: center;">1.41</td>
<td style="text-align: center;">14.43</td>
<td style="text-align: center;">69.14</td>
</tr>
<tr>
<td style="text-align: center;">Untrained LSTM</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">0.66</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">7.61</td>
<td style="text-align: center;">46.5</td>
<td style="text-align: center;">210.34</td>
<td style="text-align: center;">5.11</td>
<td style="text-align: center;">45.69</td>
<td style="text-align: center;">510.19</td>
</tr>
<tr>
<td style="text-align: center;">Value embedding</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.68</td>
<td style="text-align: center;">1.20</td>
<td style="text-align: center;">11.23</td>
<td style="text-align: center;">275.50</td>
<td style="text-align: center;">0.30</td>
<td style="text-align: center;">15.98</td>
<td style="text-align: center;">654.33</td>
</tr>
<tr>
<td style="text-align: center;">Pretrained</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Word2Vec</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;">2.34</td>
<td style="text-align: center;">18.77</td>
<td style="text-align: center;">333.47</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">21.23</td>
<td style="text-align: center;">210.07</td>
</tr>
<tr>
<td style="text-align: center;">GloVE</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">2.23</td>
<td style="text-align: center;">13.77</td>
<td style="text-align: center;">174.21</td>
<td style="text-align: center;">0.80</td>
<td style="text-align: center;">16.51</td>
<td style="text-align: center;">180.31</td>
</tr>
<tr>
<td style="text-align: center;">ELMo</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">2.35</td>
<td style="text-align: center;">13.48</td>
<td style="text-align: center;">62.20</td>
<td style="text-align: center;">0.94</td>
<td style="text-align: center;">15.50</td>
<td style="text-align: center;">45.71</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.52</td>
<td style="text-align: center;">3.21</td>
<td style="text-align: center;">29.00</td>
<td style="text-align: center;">431.78</td>
<td style="text-align: center;">4.56</td>
<td style="text-align: center;">67.81</td>
<td style="text-align: center;">454.78</td>
</tr>
<tr>
<td style="text-align: center;">Learned</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">Char-CNN</td>
<td style="text-align: center;">0.97</td>
<td style="text-align: center;">0.93</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">2.50</td>
<td style="text-align: center;">4.92</td>
<td style="text-align: center;">11.57</td>
<td style="text-align: center;">1.19</td>
<td style="text-align: center;">7.75</td>
<td style="text-align: center;">15.09</td>
</tr>
<tr>
<td style="text-align: center;">Char-LSTM</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">2.55</td>
<td style="text-align: center;">8.65</td>
<td style="text-align: center;">18.33</td>
<td style="text-align: center;">1.21</td>
<td style="text-align: center;">15.11</td>
<td style="text-align: center;">25.37</td>
</tr>
<tr>
<td style="text-align: center;">DROP-trained</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">NAQANet</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">2.99</td>
<td style="text-align: center;">14.19</td>
<td style="text-align: center;">62.17</td>
<td style="text-align: center;">1.11</td>
<td style="text-align: center;">11.33</td>
<td style="text-align: center;">90.01</td>
</tr>
<tr>
<td style="text-align: center;">NAQANet (w/out GloVe)</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;">0.90</td>
<td style="text-align: center;">0.82</td>
<td style="text-align: center;">2.87</td>
<td style="text-align: center;">5.34</td>
<td style="text-align: center;">35.39</td>
<td style="text-align: center;">1.45</td>
<td style="text-align: center;">9.91</td>
<td style="text-align: center;">60.70</td>
</tr>
<tr>
<td style="text-align: center;">Ours</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">DICE- $D$</td>
<td style="text-align: center;">0.98</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">0.43</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">3.16</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">2.79</td>
<td style="text-align: center;">29.95</td>
</tr>
</tbody>
</table>
<p>Table 2: Experimental results on list maximum, decoding, and addition using the DICE- $D$ method.
near other small and large numbers, respectively, in terms of cosine distance.</p>
<p>On the NUM tests, DICE achieves perfect accuracy. The primary reason DICE embeddings perform so well on numeracy tasks is that the preprocessing steps taken allow us to parse a corpus for word forms of numbers and explicitly set matching embeddings for both word and numeral forms of numbers. Each of these embeddings is guaranteed to be unique since a number's embedding is based on its magnitude, i.e., the larger the magnitude, the greater the angle of the embedding, with a maximum angle of $\pi$. This ensures that the numeral form of a number is always able to correctly identify its word form among all word forms in the corpus as that with the smallest cosine distance (which equals zero). Performance on OVA-NUM is a lower bound on the performance of SC-NUM and BC-NUM, so those tests are guaranteed to pass under our approach.</p>
<h3>4.2 Task 2: List Maximum, Decoding, and Addition</h3>
<p>This task considers the operations proposed by (Wallace et al., 2019) - list maximum, decoding, and addition. List maximum deals with the task of predicting the maximum number given the embedding of five different numbers. Decoding deals with regressing the value of a number given its embedding. An additional task involves predicting the sum of two numbers given their embeddings.</p>
<p>Training Details. The list-maximum test presents to a Bi-LSTM neural network a set of five numbers of the same magnitude, and the network is trained to report the index of the maximum number. In the decoding test, a linear model and a feed-forward network are each trained to output the numeral corresponding to the word form of a number based on its embedding. Finally, in the addition test, a feed-forward network is trained to take in the embeddings of two numbers as its input and report the sum of the two numbers as its output. Each test is performed on three ranges of integers $[0,99],[0,999]$, and $[0,9999]$, with an 80/20 split of training and testing data sampled randomly. The neural network is fed with the embedding of numbers; the task is either classification (in the case of list maximum) or prediction of a continuous number (in case of addition and decoding). We replicate the exact experimental conditions and perform the three tests with DICE embeddings. For the sake of consistency with the tests proposed by (Wallace et al., 2019), we also only deal with positive in this experiment.</p>
<p>Evaluation. List maximum again uses accuracy as its metric while decoding and addition use root mean squared error (RMSE), since predictions are continuous.</p>
<p>Results. Given the strong performance of the DICE- $D$ method on the NUM and MAG tests, we next consider its performance on tasks involv-</p>
<p>ing neural network models. In their empirical study, (Wallace et al., 2019) compared a wide range of models that included a random baseline; character level models such as a character-CNN and character-LSTM, which were both untrained and trained; a so-called value embedding model in which numbers are embedded as their scalar value; traditional non-contextual word embedding models including Word2Vec and GloVe; contextual word embedding models including ELMo and BERT; and the Numerically Aware Question Answering (NAQA) Network, a strong numerical reasoning model proposed on the Discrete Reasoning over Paragraphs (DROP) dataset.</p>
<p>We compare the performance of our DICE- $D$ embedding to that of the other models on each of the three tasks proposed by (Wallace et al., 2019). Results are presented in Table 2. We find that our DICE embedding exceeds the performance of more sophisticated models by large margins in all but four cases. In two of those four, our model fell short by only a few percentage points. We attribute the success of the DICE- $D$ approach to the fact that the model is, by design, engineered to handle numeracy. Just as the value embedding model which proved to be reasonably successful in all three tasks across a wide range of numbers - captures numeracy through the magnitude of embeddings, our model captures numeracy through the angle corresponding to the embeddings.</p>
<p>The value embedding model, however, breaks down as the range of the processed numbers grows. This is likely because, as demonstrated by Trask et al. (2018), networks trained on numeracy tasks typically struggle to learn an identity mapping. We reason that our model outperforms the value embedding model because the network learns to associate features between the set of inputs such that the input vectors can be scaled, rotated, and translated in $D$ dimensions to achieve the desired goal.</p>
<p>More precisely, for a neural network to learn addition, numbers must be embedded such that their vector embeddings can be consistently shifted, rotated, and scaled to yield the embedding of another number (see Figure 1c). The choice of embedding is essential as it may be impractical for a network to learn a transformation for all embeddings that obeys this property (without memorization).</p>
<p>DICE is quite similar to the value embedding system, which directly encodes a number's value in its embeddings. However, DICE performs bet-
ter due to its compatibility with neural networks, whose layers are better suited for learning rotations and scaling than identity mappings.</p>
<p>Finally, both the value embedding models for a small number range and the character level models remain somewhat competitive, suggesting again that exploring a digit-by-digit embedding of numerals may provide a means of improving our model further.</p>
<h2>5 Applications of DICE</h2>
<h3>5.1 Magnitude Classification</h3>
<p>We examine the importance of good initialization for number embedding vectors (Kocmi and Bojar, 2017), particularly for better contextual understanding. In particular, we experiment on the magnitude classification task, which requires the prediction of magnitudes for masked numbers. The task is based on the 600 K dataset proposed by Chen et al. (2019), which requires classification into one of seven categories corresponding to powers of 10 in ${0,1,2,3,4,5,6}$.</p>
<p>Training Details. We use a bi-LSTM (Hochreiter and Schmidhuber, 1997) with soft attention (Chorowski et al., 2015) to classify the magnitude of masked numbers. Numerals are initialized with corresponding DICE embeddings, and the target number is masked by substituting a random vector. Each token $x_{n}$ in a sequence of length $N$ is associated with a forward and backward LSTM cell. The hidden state $\mathbf{h}<em n="n">{\mathbf{n}}$ of each token is given by the sum of the hidden states of the forward and backward cells: $\mathbf{h}</em>}=\overrightarrow{\mathbf{h}<em n="n">{n}+\overrightarrow{\mathbf{h}}</em>}$. To generate a context vector $\mathbf{c}$ for the entire sentence, we compute attention scores $\alpha_{n}$ by taking the inner product of each hidden state $\mathbf{h<em n="n">{n}$ with a learned weight vector $\mathbf{w}$. The resulting scores are passed through a softmax function, and the weights are used to form a convex combination of the $\mathbf{h}</em>$ with trained embeddings for each of the seven categories, and cross-entropy loss is minimized. More details on training can be found in Appendix A.}$ that represents the context $\mathbf{c}$ of the sentence. Logits are obtained by taking the inner product of $\mathbf{c</p>
<p>Evaluation. Following Chen et al. (2019), we use micro and macro F1 scores for classifying the magnitude of a number.</p>
<p>Results. Table 3 shows significant improvements in the F1 score achieved by the model. To investigate the effects of dimensions of the embedding</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: center;">Micro-F1</th>
<th style="text-align: center;">Macro-F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">LR</td>
<td style="text-align: center;">62.49</td>
<td style="text-align: center;">30.81</td>
</tr>
<tr>
<td style="text-align: left;">CNN</td>
<td style="text-align: center;">69.27</td>
<td style="text-align: center;">35.96</td>
</tr>
<tr>
<td style="text-align: left;">GRU</td>
<td style="text-align: center;">70.92</td>
<td style="text-align: center;">38.43</td>
</tr>
<tr>
<td style="text-align: left;">BiGRU</td>
<td style="text-align: center;">71.49</td>
<td style="text-align: center;">39.94</td>
</tr>
<tr>
<td style="text-align: left;">CRNN</td>
<td style="text-align: center;">69.50</td>
<td style="text-align: center;">36.15</td>
</tr>
<tr>
<td style="text-align: left;">CNN-capsule</td>
<td style="text-align: center;">63.11</td>
<td style="text-align: center;">29.41</td>
</tr>
<tr>
<td style="text-align: left;">GRU-capsule</td>
<td style="text-align: center;">70.73</td>
<td style="text-align: center;">33.57</td>
</tr>
<tr>
<td style="text-align: left;">BiGRU-capsule</td>
<td style="text-align: center;">71.49</td>
<td style="text-align: center;">34.18</td>
</tr>
<tr>
<td style="text-align: left;">BiLSTM with DICE</td>
<td style="text-align: center;">$\mathbf{7 5 . 5 6}$</td>
<td style="text-align: center;">$\mathbf{4 6 . 8 0}$</td>
</tr>
</tbody>
</table>
<p>Table 3: Performance (\%) on classifying number magnitude on the Numeracy-600k dataset.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Embedding Size</th>
<th style="text-align: center;">Micro-F1</th>
<th style="text-align: center;">Macro-F1</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">32</td>
<td style="text-align: center;">74.63</td>
<td style="text-align: center;">45.92</td>
</tr>
<tr>
<td style="text-align: center;">64</td>
<td style="text-align: center;">74.90</td>
<td style="text-align: center;">45.99</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">75.55</td>
<td style="text-align: center;">46.36</td>
</tr>
<tr>
<td style="text-align: center;">256</td>
<td style="text-align: center;">$\mathbf{7 5 . 5 6}$</td>
<td style="text-align: center;">45.56</td>
</tr>
<tr>
<td style="text-align: center;">512</td>
<td style="text-align: center;">74.14</td>
<td style="text-align: center;">$\mathbf{4 6 . 8 0}$</td>
</tr>
</tbody>
</table>
<p>Table 4: Performance (\%) of BiLSTM-attention with DICE model on the Numeracy-600k dataset by varying the embedding dimensions of input tokens.
and hidden vectors within the LSTM cells on the performance of the BiLSTM-attention model, we perform ablation experiments. We vary the embedding size of tokens while keeping other hyperparameters constant, and observe the results on Tables 4. From Table 4 the BiLSTM with DICE model achieves the best micro-F1 score when the embedding dimension is 256. However, the macroF1 score peaks when the embedding dimension is 512.</p>
<p>These results suggest that while DICE embeddings yield superior performance in non-contextual numerical tasks, such as computing the maximum and performing basic mathematical operations, data agnostic embeddings such as DICE may not be ideal for textual reasoning tasks in which words surrounding a number provide important information regarding the magnitude of the number. Hence, we introduce a model-based regularization method that utilizes the DICE principles to learn number embeddings in 5.2.</p>
<h3>5.2 Model-Based Numeracy Embeddings</h3>
<p>In the previous section, we demonstrated how DICE could be explicitly incorporated for numbers in the text. Here, we propose a methodology that help models implicitly internalize the properties of DICE. Our approach involves a regularization method (an auxiliary loss) that can be adopted in the fine-tuning of any contextual NLP model, such
as BERT. Auxiliary losses have shown to work well for a variety of NLP downstream tasks (Shen et al., 2019).</p>
<p>During the task-specific training of any model, the proposed auxiliary loss $\mathcal{L}<em _num="{num" _text="\text">{\text {num }}$ can be applied to the input embeddings of numbers available in a minibatch. For any two contextual numerical embeddings $\mathbf{x}, \mathbf{y}$ obtained from the final hidden layer of the model, the $\mathcal{L}</em>$ loss for the pair of numbers $(x, y)$ is calculated as:}</p>
<p>$$
\mathcal{L}<em _cos="\cos">{\text {num }}=\left|2 \frac{|x-y|}{|x|+|y|}-d</em>
$$}(\mathbf{x}, \mathbf{y})\right|_{2</p>
<p>where $d_{\cos }(\mathbf{x}, \mathbf{y})=1-\frac{\mathbf{x}^{T} \mathbf{y}}{|\mathbf{x}|<em 2="2">{2}|\mathbf{y}|</em>$ follows the same motivation as DICE where cosine distance between the embeddings of two numbers are encouraged to be proportional to their (scaled) absolute magnitude distance on the number line.}}$ is the cosine distance between the embeddings $\mathbf{x}$ and $\mathbf{y}$. In essence, $\mathcal{L}_{\text {num }</p>
<p>Training Details. To evaluate the proposed $\mathcal{L}<em _num="{num" _text="\text">{\text {num }}$, we test the regularization on the task of question answering (QA) involving numerical answers. In particular, we take the popular Stanford Question Answering Dataset (SQuAD 1.1) (Rajpurkar et al., 2016) dataset and create sub-splits (ranges from [1, 30000]) where the ( $i$ ) training QA pairs have answers strictly containing numerical digits (Sub-split 1, less than 10K examples), and (ii) training QA pairs have answers containing a number as one of their tokens, for e.g. "10 apples" (Sub-split 2, slightly more than 10K examples). We create these splits to evaluate BERT model's reasoning involving numbers to pick these answers. We choose BERT-base-uncased as baseline model and train it on both the datasets. Within each batch, we calculate $\mathcal{L}</em>$. The scores are reported on the development set (less than 1000 examples) as the test set cannot be pruned for our purpose. The assumption here is that the BERT model needs to perform numerical reasoning to come up with answers for these particular kinds of QA pairs. The models were trained on Nvidia Tesla P100 GPU. More details on choosing}}$ by randomly sampling a pair of numbers $x, y$ from the available numbers in the contexts. The corresponding embeddings of the numbers are $\mathbf{x}$ and $\mathbf{y}$, which are extracted from the last hidden layer of the BERT model. We then enforce the distance of embeddings to match the difference between number values by $\mathcal{L}_{\text {num }</p>
<table>
<thead>
<tr>
<th style="text-align: center;">A) Context</th>
<th style="text-align: center;">Question <br> What is the smallest <br> number of Bolivians <br> it's estimated live in <br> Newcastle?</th>
<th style="text-align: center;">Question <br> How many cylinders <br> does the Energiprojekt <br> AB engine have?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">According to the same statistics, the average age of <br> people living in Newcastle is 37.8 (the national <br> average being 38.6). Many people in the city have <br> Scottish or Irish ancestors. There is a strong <br> presence of Border Reiver surnames, such as <br> Armstrong, Charlton, Elliot, Johnstone, Kerr, Hall, <br> Nixon, Little and Robson. There are also small but <br> significant Chinese, Jewish and Eastern European <br> (Polish, Czech Roma) populations. There are also <br> estimated to be between 500 and 2,000 Bolivians in <br> Newcastle, forming up to $1 \%$ of the population- <br> the largest such percentage of any UK city.</td>
<td style="text-align: center;"><img alt="img-1.jpeg" src="img-1.jpeg" /></td>
<td style="text-align: center;">Answer <br> Ground truth: <br> 5 <br> BERT : <br> $27-30 \%$ on high - <br> pressure engines . it <br> is a single - step, 5 <br> BERT $+\mathcal{L}_{\text {num }}: 5$</td>
</tr>
</tbody>
</table>
<p>Figure 2: Qualitative examples where BERT $+\mathcal{L}<em _num="{num" _text="\text">{\text {num }}$ performed better than BERT-base
hyper-parameter for BERT $+\mathcal{L}</em>$ is discussed in Appendix B.}</p>
<p>Evaluation. Exact Match is a binary measure (i.e., true/false) of whether the predicted output matches the ground truth answer exactly. This evaluation is performed after the string normalization (uncased, articles removed, etc.). F1 is the harmonic mean of precision and recall.</p>
<p>Results. Results in Table 5 show that the BERT model with numeracy objective achieves an improvement of 0.48 F 1 points when the answers are purely numerical digits. When the BERT model is trained on QA pairs with answers containing at least a number with several words, and evaluated on pairs with answers containing only numbers, we see an improvement of 1.12 F 1 points over the baseline model.</p>
<p>The BERT-base model on the original SQuAD data was finetuned for 3 epochs owing to its complexity. However, we find that 1 epoch is sufficient to capture the complexity of the pruned SQuAD data. Table 5 shows BERT $+\mathcal{L}_{\text {num }}$ consistently performs better than BERT-base across epochs.</p>
<p>Interestingly, BERT-base performs worse when finetuned with QA pairs containing a mix of words and numbers as answers (sub-split 2). This informs us that the baseline model learns to pick numbers better but fails to do as well when fine-tuned with a mix of words and numbers. In both the cases, the evaluation set consists of pruned SQuAD dev set QA pairs with answers strictly containing numerical digits only. We find that BERT $+\mathcal{L}_{\text {num }}$ gives the maximum improvement on sub-split 2 data highlighting the efficiency of our regularization technique to learn numerical embeddings.</p>
<p>Figure 2 shows some qualitative examples where the BERT $+\mathcal{L}_{\text {num }}$ performs better than BERT-base (Sub-split 2). In this analysis, we found that the</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">$\frac{\sigma}{2}$</th>
<th style="text-align: center;">Sub-split 1</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Sub-split 2</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Exact</td>
<td style="text-align: center;">F1</td>
<td style="text-align: center;">Exact</td>
</tr>
<tr>
<td style="text-align: center;">BERT</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">89.75</td>
<td style="text-align: center;">89.40</td>
<td style="text-align: center;">89.03</td>
<td style="text-align: center;">86.50</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">90.71</td>
<td style="text-align: center;">90.66</td>
<td style="text-align: center;">90.32</td>
<td style="text-align: center;">88.09</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">91.12</td>
<td style="text-align: center;">91.04</td>
<td style="text-align: center;">90.28</td>
<td style="text-align: center;">88.02</td>
</tr>
<tr>
<td style="text-align: center;">BERT $+\mathcal{L}_{\text {num }}$</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">90.23</td>
<td style="text-align: center;">90.16</td>
<td style="text-align: center;">89.90</td>
<td style="text-align: center;">87.26</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">91.05</td>
<td style="text-align: center;">90.92</td>
<td style="text-align: center;">90.49</td>
<td style="text-align: center;">88.52</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">91.46</td>
<td style="text-align: center;">91.29</td>
<td style="text-align: center;">91.40</td>
<td style="text-align: center;">89.15</td>
</tr>
</tbody>
</table>
<p>Table 5: F1 scores of BERT-base model on SQuAD 1.1 sub-splits (all scores are statistically significant with a variance of 0.01 ). Sub-split 1: both training and testing splits contains only numerical answers; Sub-split 2: train split contains atleast one number in the answer and testing split contains only numerical answers.
baseline model picks the whole sentence or paragraph involving the numerical value (Figure 2 B) as the answer. Our method picks numbers within the classification span (Figure 2 B) and sometimes helps the BERT model to accurately pick up correct numbers (Figure 2 A), contributing to exact match and F1. More such examples are shown in Appendix C.</p>
<p>During our experiments, we observed the potential issue of weak signals from the loss when the availability of numerical pairs is sparse. In the future, our efforts would be to overcome this issue to ensure further gains.</p>
<h2>6 Conclusion</h2>
<p>In this work, we methodologically assign and learn embeddings for numbers to reflect their numerical properties. We validate our proposed approach with several experiments that test number embeddings. The tests that evaluate the numeral embeddings are fundamentally applicable to all real numbers. Finally, we introduced an approach to jointly learn embeddings of numbers and words that preserve numerical properties and evaluated them on a contextual word embedding based model. In our future</p>
<p>work, we would like to extend this idea to unseen numbers in vocabulary as a function of seen ones.</p>
<h2>Acknowledgments</h2>
<p>The authors would like to thank Aakanksha Naik for her help in the early stages of this work, and the anonymous reviewers as well for their insightful comments.</p>
<h2>References</h2>
<p>LE Blumenson. 1960. A derivation of n-dimensional spherical coordinates. The American Mathematical Monthly, 67(1):63-66.</p>
<p>Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135-146.</p>
<p>Chung-Chi Chen, Hen-Hsen Huang, Hiroya Takamura, and Hsin-Hsi Chen. 2019. Numeracy-600k: Learning numeracy for detecting exaggerated information in market comments. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6307-6313.</p>
<p>Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. 2015. Attention-based models for speech recognition. In Advances in neural information processing systems, pages 577-585.</p>
<p>Marie-Catherine De Marneffe, Anna N Rafferty, and Christopher D Manning. 2008. Finding contradictions in text. In Proceedings of ACL-08: HLT, pages 1039-1047.</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proc. of NAACL.</p>
<p>John R Firth. 1957. A synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis.</p>
<p>ZS Harris. 1954. Distributional structure. word, 10 (2-3): 146-162. reprinted in fodor, j. a and katz, jj (eds.), readings in the philosophy of language.</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780.</p>
<p>Chengyue Jiang, Zhonglin Nian, Kaihao Guo, Shanbo Chu, Yinggong Zhao, Libin Shen, Haofen Wang, and Kewei Tu. 2019. Learning numeral embeddings. arXiv preprint arXiv:2001.00003.</p>
<p>Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.</p>
<p>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages 3294-3302.</p>
<p>Tom Kocmi and Ondřej Bojar. 2017. An exploration of word embedding initialization in deep-learning tasks. arXiv preprint arXiv:1711.09160.</p>
<p>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</p>
<p>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111-3119.</p>
<p>Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1, pages 430-439. Association for Computational Linguistics.</p>
<p>Aakanksha Naik, Abhilasha Ravichander, Carolyn Rose, and Eduard Hovy. 2019. Exploring numeracy in word embeddings. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3374-3380.</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532-1543.</p>
<p>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.</p>
<p>David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. 1988. Learning representations by back-propagating errors. Cognitive modeling, 5(3):1.</p>
<p>Dinghan Shen, Pengyu Cheng, Dhanasekar Sundararaman, Xinyuan Zhang, Qian Yang, Meng Tang, Asli Celikyilmaz, and Lawrence Carin. 2019. Learning compressed sentence representations for on-device text processing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 107-116.</p>
<p>Georgios P Spithourakis and Sebastian Riedel. 2018. Numeracy for language models: Evaluating and improving their ability to predict numbers. arXiv preprint arXiv:1805.08154.</p>
<p>Andrew Trask, Felix Hill, Scott E Reed, Jack Rae, Chris Dyer, and Phil Blunsom. 2018. Neural arithmetic logic units. In Advances in Neural Information Processing Systems, pages 8035-8044.</p>
<p>Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do nlp models know numbers? probing numeracy in embeddings. arXiv preprint arXiv:1909.07940.</p>
<p>Gail Weiss, Yoav Goldberg, and Eran Yahav. 2018. On the practical computational power of finite precision rnns for language recognition. arXiv preprint arXiv:1805.04908.</p>
<p>John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. Towards universal paraphrastic sentence embeddings. arXiv preprint arXiv:1511.08198.</p>
<p>Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. 2018. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541.</p>
<h1>A Training details for Magnitude Classification Experiment</h1>
<p>The Bi-LSTM with attention model initialized with DICE embeddings were trained on the market comments data. The model was trained for a fixed number of 9 epochs. We found that the micro and macro F1 scores peaked for a certain epoch and then flattened out. We picked the best micro and macro pair the model obtained in that certain epoch.</p>
<h2>B Hyperparameter for BERT $+\mathcal{L}_{\text {num }}$</h2>
<p>Our model involves a regularization method (an auxiliary loss) that can be adopted in the fine-tuning of BERT. This loss was finetuned with a hyperparameter $\lambda$ and added to the existing BERT classification loss for detecting the correct span. The hyperparameter search space is between 0,1 . We sweeped through the values manually within the search space and found that the best model that gave the maximum improvement in F1 scores had a hyperparameter value of $10^{-3}$. The values were sweeped based on the observed performance. The performance faded as the hyperparameter was set to a higher value (closer to 1).</p>
<h2>C Examples for BERT vs. BERT $+\mathcal{L}_{\text {num }}$</h2>
<p>Figure 3 provides additional samples where BERT $+\mathcal{L}_{\text {num }}$ outperformed the baseline BERT model. Similar to previous observations, our regularized approach is able to pinpoint the correct number as opposed to selecting a substring via pattern matching.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Qualitative examples where BERT $+\mathcal{L}_{\text {num }}$ performed better than BERT base.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{1}$ https://pypi.org/project/text2digits/&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
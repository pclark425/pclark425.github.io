<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1694 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1694</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1694</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-33.html">extraction-schema-33</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <p><strong>Paper ID:</strong> paper-272911275</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2409.17348v1.pdf" target="_blank">Language Grounded Multi-agent Communication for Ad-hoc Teamwork</a></p>
                <p><strong>Paper Abstract:</strong> Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks. However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios. In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios. Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication. Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states. This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1694.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1694.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (gpt-4-0125-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (OpenAI API model gpt-4-0125-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large pre-trained generative language model used here as an embodied agent via a rule-based text interface to produce expert (observation, action, language) trajectories for grounding MARL communication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>gpt-4-0125-preview (embodied LLM agent)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Backbone large language model (GPT-4) used as an embodied agent: prompted with general task descriptions, maintains a simple belief/memory state, outputs a high-level action keyword and a natural-language communication message each timestep; temperature was set to 0 for deterministic outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained large-scale natural language model (general language data); exact pretraining corpora not specified in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper uses OpenAI's GPT-4 via the API (gpt-4-0125-preview) as the backbone model. The study does not report GPT-4's pretraining corpus or size; instead GPT-4 is used in-context with prompts to interact with the task text interface and generate expert trajectories. Collected dataset sizes from these embodied LLM runs: USAR — 50 episodes → 2,550 (observation,action) pairs with messages; Predator Prey pp_v0 — 1,893 pairs; pp_v1 — 2,493 pairs. Outputs were deterministic (temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Predator Prey (pp_v0, pp_v1) and USAR (Urban Search & Rescue)</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>Predator Prey: a 5x5 gridworld with 3 predator agents searching for 1 static prey under partial observability (vision range 0 or 1); discrete movement actions; episodes end when all predators reach prey or timeout. USAR: a 5-room graph search-and-defuse task with 5 bombs of varying phase lengths and colored tools; 3 heterogeneous agents each possess specific tools, actions include moving between adjacent rooms, inspecting a bomb, and applying a tool; team must coordinate to defuse bombs under partial observability.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>High-level textual commands used in the text interface: e.g., 'Move to Room X', 'Inspect Bomb', 'Apply <color> Tool' for USAR; for Predator Prey textual descriptions of movement (e.g., 'move up', 'move down') and communication sentences describing intentions/locations.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Discrete embodied environment actions: grid movements (up/down/left/right), room-to-room moves (graph transitions), 'inspect' action, and 'apply tool' actions; communications are sent as natural language messages (embedded to vectors for MARL alignment).</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Rule-based text interface: maps LLM's formatted textual response to environment actions via keyword matching and templates. Communication mapping: LLM natural-language messages are embedded with an off-the-shelf embedding API into 256-D vectors and stored as (observation, action) → message pairs (dataset D); during MARL training, agent communication vectors are aligned to these language embeddings via a cosine-similarity supervised loss (L_sup) added to the RL loss.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Textual partial observations provided to the LLM: agent's local observation (grid window or current room contents), last-round messages from teammates, round number, team score, and teammate locations; no raw visual (RGB) inputs in this work — observations are symbolic/textual descriptions produced by the text interface.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Using LLM-generated supervised language grounding (LangGround), MARL agents achieved comparable or improved task performance and faster/stabler communication emergence compared to baselines: example task completion episode lengths (mean ± std) from Table 4 for LangGround teams — pp_v1: 4.3 ± 1.20; pp_v0: 10.9 ± 4.53; USAR: 22.0 ± 4.24 (these numbers describe task completion steps when the trained MARL team is evaluated). The paper reports LangGround outperforms baselines in the most challenging USAR environment and stabilizes learning (lower variance).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines (trained without LLM language grounding) obtain comparable final task utility in some environments but worse stability and interpretability; example ad-hoc team results (from Table 4) for no-language-grounding teams paired with an LLM teammate: pp_v1 noComm+LLM 10.6 ± 5.73, pp_v0 noComm+LLM 20.0 ± 0.00, USAR noComm+LLM 32.4 ± 13.47. The paper also reports other learned-communication baselines (IC3Net, aeComm, protoComm, VQ-VIB) typically underperform LangGround in USAR and have lower topographic similarity and BLEU/cosine similarity to human language.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>Qualitative improvement reported: language grounding accelerates the emergence of communication and stabilizes learning. Training budgets reported for experiments that include LangGround: pp_v0 and USAR training used up to 1e7 timesteps (~4 hours) over 2000 epochs; pp_v1 used 2.5e6 timesteps (~1.5 hours). The paper does not report a numeric factor reduction in sample count required versus baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Baselines were trained under the same training budgets (same number of timesteps/epochs) for fair comparison; the paper does not report explicit sample counts required by baselines to reach specific thresholds, only qualitative/curve comparisons (LangGround converges faster in some settings, notably USAR).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative: LangGround shows faster convergence and lower variance in communication learning (improved sample efficiency), but no quantified multiplicative or percentage reduction in samples is reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Key contributors: (1) using embodied LLMs to produce expert (observation,action,language) pairs that provide a strong supervised signal, (2) aligning MARL continuous communication vectors to precomputed language embedding vectors via a cosine-loss auxiliary objective, (3) semantic consistency between language descriptions and task observations (textual interface reduces modality gap), and (4) expert quality of LLM trajectories (deterministic outputs at temperature=0).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations noted: (1) modality gap if moving to real sensory (RGB/3D) inputs — this study uses symbolic/textual observations, (2) potential LLM hallucinations and lack of grounding in real-world sensorimotor dynamics (mitigated here by text interface but still a concern), (3) dependence on a static offline dataset D (limits capturing richer, multi-agent online interactions unless replaced by online LLM queries), and (4) action-space mismatch if transferring to continuous 3D motor control — not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a pretrained LLM (GPT-4) as an embodied text-agent to generate supervised (observation,action,language) pairs and aligning MARL agents' communication vectors to LLM language embeddings yields human-interpretable communication that: (a) retains or improves task utility (notably in complex USAR), (b) accelerates the emergence and stabilizes communication learning, (c) produces communication with higher topographic similarity and BLEU/cosine similarity to natural language, and (d) enables zero-shot and ad-hoc teamwork with unseen LLM teammates. The study is limited to symbolic/textual interfaces (gridworld and graph-room tasks) and does not evaluate transfer to continuous 3D sensorimotor embodied tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_evidence</strong></td>
                            <td>["Sections: 'Grounded communication from LLM agents', 'A.1.1 Embodied LLM agents', 'A.3.1 LangGround dataset', and Tables 3-4 and Figures 2-3."]</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Grounded Multi-agent Communication for Ad-hoc Teamwork', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1694.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1694.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of models or agents that are pretrained on text-based environments or language data and then transferred to 3D embodied tasks, including details about the pretraining, the embodied tasks, transfer performance, action mappings, and sample complexity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>text-embedding-3-large</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI text-embedding-3-large (embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pretrained text embedding model used to convert LLM-generated natural language messages into fixed 256-D vectors that serve as the language-grounding target for MARL agent communication.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_name</strong></td>
                            <td>text-embedding-3-large (OpenAI embedding model)</td>
                        </tr>
                        <tr>
                            <td><strong>model_agent_description</strong></td>
                            <td>Off-the-shelf pretrained text embedding model (OpenAI) used to embed natural-language messages from LLM agents into 256-dimensional vectors that are directly comparable to MARL agents' 256-D continuous communication vectors.</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_type</strong></td>
                            <td>Pretrained text embedding model trained on natural language corpora (exact corpora not specified in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>pretraining_data_details</strong></td>
                            <td>The paper uses OpenAI's word embedding API (text-embedding-3-large) to translate each LLM natural-language message into a 256-D vector. The paper does not specify the embedding model's training data or size beyond API use.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_name</strong></td>
                            <td>Predator Prey (pp_v0, pp_v1) and USAR (Urban Search & Rescue) — same downstream tasks where embeddings are used as supervision targets.</td>
                        </tr>
                        <tr>
                            <td><strong>embodied_task_description</strong></td>
                            <td>See GPT-4 entry: gridworld predator-prey and graph-based USAR tasks; embedding vectors are used as supervised references (D) during MARL training to align communication.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_text</strong></td>
                            <td>N/A for embedding model itself; embeddings represent natural-language communication messages (sentences) produced by LLM agents.</td>
                        </tr>
                        <tr>
                            <td><strong>action_space_embodied</strong></td>
                            <td>Embeddings are used to supervise MARL agents whose embodied action space is discrete navigation, inspection, and tool application actions; communication vectors are 256-D continuous vectors matched to the text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>action_mapping_method</strong></td>
                            <td>Natural-language messages from LLM agents are embedded into 256-D vectors via text-embedding-3-large; during MARL training, agent communication vectors are encouraged to be similar (cosine similarity) to the corresponding language-embedding vector for the same (observation,action) pair using a supervised cosine-loss term.</td>
                        </tr>
                        <tr>
                            <td><strong>perception_requirements</strong></td>
                            <td>Embeddings operate on textual messages; the perception pipeline feeding the messages to the embedding model is the textual game-state descriptions produced by the text interface (local observation, teammate messages, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_successful</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_pretraining</strong></td>
                            <td>Embedding-based supervision contributed to the LangGround method's improved interpretability and higher topographic similarity and BLEU/cosine metrics; Table 1 reports LangGround topographic similarity ρ=0.67±0.07 and Table 2/BLEU & cosine improvements (exact baseline numbers not provided for all methods).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_pretraining</strong></td>
                            <td>Baselines without language-embedding supervision (e.g., IC3Net, aeComm) show lower topographic similarity and near-random BLEU/cosine alignment to natural language; exact numeric comparisons are reported in the paper where applicable.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_with_pretraining</strong></td>
                            <td>The embedding supervision was part of the LangGround training that used up to 1e7 timesteps for pp_v0 and USAR; the paper reports faster emergence/stabilization of communication with embedding supervision but no quantified sample-count reduction.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_without_pretraining</strong></td>
                            <td>Baselines trained with the same budgets; no explicit sample-count-to-threshold numbers reported.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_complexity_gain</strong></td>
                            <td>Qualitative improvement in convergence speed and stability; no quantitative multiplier reported.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_factors</strong></td>
                            <td>Providing direct embedding-space targets (256-D) allowed straightforward vector-space alignment via cosine loss, reducing the difficulty of mapping between continuous communication vectors and language semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_failure_factors</strong></td>
                            <td>Limitations include reliance on offline static embeddings (dataset D) which may not capture richer multi-agent communicative dynamics if not refreshed, and potential mismatch if moving to modalities that require grounding in visual or continuous sensorimotor data.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Embedding LLM-generated messages with a pretrained text embedding model (text-embedding-3-large) and using those embeddings as supervised targets for MARL communication vectors is an effective and simple mechanism to align emergent agent communication with natural language, improving interpretability, topographic similarity, and ad-hoc teaming capabilities; however, quantitative sample-efficiency gains are reported qualitatively rather than as precise factors.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_evidence</strong></td>
                            <td>["Sections: 'A.3.1 LangGround dataset', 'Multi-agent Reinforcement Learning with aligned communication', and Appendix implementation details describing use of OpenAI's word embedding API (text-embedding-3-large) to produce 256-D vectors."]</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Language Grounded Multi-agent Communication for Ad-hoc Teamwork', 'publication_date_yy_mm': '2024-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Grounding large language models in interactive environments with online reinforcement learning <em>(Rating: 2)</em></li>
                <li>True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning <em>(Rating: 2)</em></li>
                <li>Language models meet world models: Embodied experiences enhance language models <em>(Rating: 2)</em></li>
                <li>Generative agents: Interactive simulacra of human behavior <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1694",
    "paper_id": "paper-272911275",
    "extraction_schema_id": "extraction-schema-33",
    "extracted_data": [
        {
            "name_short": "GPT-4 (gpt-4-0125-preview)",
            "name_full": "GPT-4 (OpenAI API model gpt-4-0125-preview)",
            "brief_description": "A large pre-trained generative language model used here as an embodied agent via a rule-based text interface to produce expert (observation, action, language) trajectories for grounding MARL communication.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "gpt-4-0125-preview (embodied LLM agent)",
            "model_agent_description": "Backbone large language model (GPT-4) used as an embodied agent: prompted with general task descriptions, maintains a simple belief/memory state, outputs a high-level action keyword and a natural-language communication message each timestep; temperature was set to 0 for deterministic outputs.",
            "pretraining_data_type": "Pretrained large-scale natural language model (general language data); exact pretraining corpora not specified in this paper.",
            "pretraining_data_details": "The paper uses OpenAI's GPT-4 via the API (gpt-4-0125-preview) as the backbone model. The study does not report GPT-4's pretraining corpus or size; instead GPT-4 is used in-context with prompts to interact with the task text interface and generate expert trajectories. Collected dataset sizes from these embodied LLM runs: USAR — 50 episodes → 2,550 (observation,action) pairs with messages; Predator Prey pp_v0 — 1,893 pairs; pp_v1 — 2,493 pairs. Outputs were deterministic (temperature=0).",
            "embodied_task_name": "Predator Prey (pp_v0, pp_v1) and USAR (Urban Search & Rescue)",
            "embodied_task_description": "Predator Prey: a 5x5 gridworld with 3 predator agents searching for 1 static prey under partial observability (vision range 0 or 1); discrete movement actions; episodes end when all predators reach prey or timeout. USAR: a 5-room graph search-and-defuse task with 5 bombs of varying phase lengths and colored tools; 3 heterogeneous agents each possess specific tools, actions include moving between adjacent rooms, inspecting a bomb, and applying a tool; team must coordinate to defuse bombs under partial observability.",
            "action_space_text": "High-level textual commands used in the text interface: e.g., 'Move to Room X', 'Inspect Bomb', 'Apply &lt;color&gt; Tool' for USAR; for Predator Prey textual descriptions of movement (e.g., 'move up', 'move down') and communication sentences describing intentions/locations.",
            "action_space_embodied": "Discrete embodied environment actions: grid movements (up/down/left/right), room-to-room moves (graph transitions), 'inspect' action, and 'apply tool' actions; communications are sent as natural language messages (embedded to vectors for MARL alignment).",
            "action_mapping_method": "Rule-based text interface: maps LLM's formatted textual response to environment actions via keyword matching and templates. Communication mapping: LLM natural-language messages are embedded with an off-the-shelf embedding API into 256-D vectors and stored as (observation, action) → message pairs (dataset D); during MARL training, agent communication vectors are aligned to these language embeddings via a cosine-similarity supervised loss (L_sup) added to the RL loss.",
            "perception_requirements": "Textual partial observations provided to the LLM: agent's local observation (grid window or current room contents), last-round messages from teammates, round number, team score, and teammate locations; no raw visual (RGB) inputs in this work — observations are symbolic/textual descriptions produced by the text interface.",
            "transfer_successful": true,
            "performance_with_pretraining": "Using LLM-generated supervised language grounding (LangGround), MARL agents achieved comparable or improved task performance and faster/stabler communication emergence compared to baselines: example task completion episode lengths (mean ± std) from Table 4 for LangGround teams — pp_v1: 4.3 ± 1.20; pp_v0: 10.9 ± 4.53; USAR: 22.0 ± 4.24 (these numbers describe task completion steps when the trained MARL team is evaluated). The paper reports LangGround outperforms baselines in the most challenging USAR environment and stabilizes learning (lower variance).",
            "performance_without_pretraining": "Baselines (trained without LLM language grounding) obtain comparable final task utility in some environments but worse stability and interpretability; example ad-hoc team results (from Table 4) for no-language-grounding teams paired with an LLM teammate: pp_v1 noComm+LLM 10.6 ± 5.73, pp_v0 noComm+LLM 20.0 ± 0.00, USAR noComm+LLM 32.4 ± 13.47. The paper also reports other learned-communication baselines (IC3Net, aeComm, protoComm, VQ-VIB) typically underperform LangGround in USAR and have lower topographic similarity and BLEU/cosine similarity to human language.",
            "sample_complexity_with_pretraining": "Qualitative improvement reported: language grounding accelerates the emergence of communication and stabilizes learning. Training budgets reported for experiments that include LangGround: pp_v0 and USAR training used up to 1e7 timesteps (~4 hours) over 2000 epochs; pp_v1 used 2.5e6 timesteps (~1.5 hours). The paper does not report a numeric factor reduction in sample count required versus baselines.",
            "sample_complexity_without_pretraining": "Baselines were trained under the same training budgets (same number of timesteps/epochs) for fair comparison; the paper does not report explicit sample counts required by baselines to reach specific thresholds, only qualitative/curve comparisons (LangGround converges faster in some settings, notably USAR).",
            "sample_complexity_gain": "Qualitative: LangGround shows faster convergence and lower variance in communication learning (improved sample efficiency), but no quantified multiplicative or percentage reduction in samples is reported.",
            "transfer_success_factors": "Key contributors: (1) using embodied LLMs to produce expert (observation,action,language) pairs that provide a strong supervised signal, (2) aligning MARL continuous communication vectors to precomputed language embedding vectors via a cosine-loss auxiliary objective, (3) semantic consistency between language descriptions and task observations (textual interface reduces modality gap), and (4) expert quality of LLM trajectories (deterministic outputs at temperature=0).",
            "transfer_failure_factors": "Limitations noted: (1) modality gap if moving to real sensory (RGB/3D) inputs — this study uses symbolic/textual observations, (2) potential LLM hallucinations and lack of grounding in real-world sensorimotor dynamics (mitigated here by text interface but still a concern), (3) dependence on a static offline dataset D (limits capturing richer, multi-agent online interactions unless replaced by online LLM queries), and (4) action-space mismatch if transferring to continuous 3D motor control — not evaluated in this paper.",
            "key_findings": "Using a pretrained LLM (GPT-4) as an embodied text-agent to generate supervised (observation,action,language) pairs and aligning MARL agents' communication vectors to LLM language embeddings yields human-interpretable communication that: (a) retains or improves task utility (notably in complex USAR), (b) accelerates the emergence and stabilizes communication learning, (c) produces communication with higher topographic similarity and BLEU/cosine similarity to natural language, and (d) enables zero-shot and ad-hoc teamwork with unseen LLM teammates. The study is limited to symbolic/textual interfaces (gridworld and graph-room tasks) and does not evaluate transfer to continuous 3D sensorimotor embodied tasks.",
            "citation_evidence": [
                "Sections: 'Grounded communication from LLM agents', 'A.1.1 Embodied LLM agents', 'A.3.1 LangGround dataset', and Tables 3-4 and Figures 2-3."
            ],
            "uuid": "e1694.0",
            "source_info": {
                "paper_title": "Language Grounded Multi-agent Communication for Ad-hoc Teamwork",
                "publication_date_yy_mm": "2024-09"
            }
        },
        {
            "name_short": "text-embedding-3-large",
            "name_full": "OpenAI text-embedding-3-large (embedding model)",
            "brief_description": "A pretrained text embedding model used to convert LLM-generated natural language messages into fixed 256-D vectors that serve as the language-grounding target for MARL agent communication.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_agent_name": "text-embedding-3-large (OpenAI embedding model)",
            "model_agent_description": "Off-the-shelf pretrained text embedding model (OpenAI) used to embed natural-language messages from LLM agents into 256-dimensional vectors that are directly comparable to MARL agents' 256-D continuous communication vectors.",
            "pretraining_data_type": "Pretrained text embedding model trained on natural language corpora (exact corpora not specified in paper).",
            "pretraining_data_details": "The paper uses OpenAI's word embedding API (text-embedding-3-large) to translate each LLM natural-language message into a 256-D vector. The paper does not specify the embedding model's training data or size beyond API use.",
            "embodied_task_name": "Predator Prey (pp_v0, pp_v1) and USAR (Urban Search & Rescue) — same downstream tasks where embeddings are used as supervision targets.",
            "embodied_task_description": "See GPT-4 entry: gridworld predator-prey and graph-based USAR tasks; embedding vectors are used as supervised references (D) during MARL training to align communication.",
            "action_space_text": "N/A for embedding model itself; embeddings represent natural-language communication messages (sentences) produced by LLM agents.",
            "action_space_embodied": "Embeddings are used to supervise MARL agents whose embodied action space is discrete navigation, inspection, and tool application actions; communication vectors are 256-D continuous vectors matched to the text embeddings.",
            "action_mapping_method": "Natural-language messages from LLM agents are embedded into 256-D vectors via text-embedding-3-large; during MARL training, agent communication vectors are encouraged to be similar (cosine similarity) to the corresponding language-embedding vector for the same (observation,action) pair using a supervised cosine-loss term.",
            "perception_requirements": "Embeddings operate on textual messages; the perception pipeline feeding the messages to the embedding model is the textual game-state descriptions produced by the text interface (local observation, teammate messages, etc.).",
            "transfer_successful": true,
            "performance_with_pretraining": "Embedding-based supervision contributed to the LangGround method's improved interpretability and higher topographic similarity and BLEU/cosine metrics; Table 1 reports LangGround topographic similarity ρ=0.67±0.07 and Table 2/BLEU & cosine improvements (exact baseline numbers not provided for all methods).",
            "performance_without_pretraining": "Baselines without language-embedding supervision (e.g., IC3Net, aeComm) show lower topographic similarity and near-random BLEU/cosine alignment to natural language; exact numeric comparisons are reported in the paper where applicable.",
            "sample_complexity_with_pretraining": "The embedding supervision was part of the LangGround training that used up to 1e7 timesteps for pp_v0 and USAR; the paper reports faster emergence/stabilization of communication with embedding supervision but no quantified sample-count reduction.",
            "sample_complexity_without_pretraining": "Baselines trained with the same budgets; no explicit sample-count-to-threshold numbers reported.",
            "sample_complexity_gain": "Qualitative improvement in convergence speed and stability; no quantitative multiplier reported.",
            "transfer_success_factors": "Providing direct embedding-space targets (256-D) allowed straightforward vector-space alignment via cosine loss, reducing the difficulty of mapping between continuous communication vectors and language semantics.",
            "transfer_failure_factors": "Limitations include reliance on offline static embeddings (dataset D) which may not capture richer multi-agent communicative dynamics if not refreshed, and potential mismatch if moving to modalities that require grounding in visual or continuous sensorimotor data.",
            "key_findings": "Embedding LLM-generated messages with a pretrained text embedding model (text-embedding-3-large) and using those embeddings as supervised targets for MARL communication vectors is an effective and simple mechanism to align emergent agent communication with natural language, improving interpretability, topographic similarity, and ad-hoc teaming capabilities; however, quantitative sample-efficiency gains are reported qualitatively rather than as precise factors.",
            "citation_evidence": [
                "Sections: 'A.3.1 LangGround dataset', 'Multi-agent Reinforcement Learning with aligned communication', and Appendix implementation details describing use of OpenAI's word embedding API (text-embedding-3-large) to produce 256-D vectors."
            ],
            "uuid": "e1694.1",
            "source_info": {
                "paper_title": "Language Grounded Multi-agent Communication for Ad-hoc Teamwork",
                "publication_date_yy_mm": "2024-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Grounding large language models in interactive environments with online reinforcement learning",
            "rating": 2,
            "sanitized_title": "grounding_large_language_models_in_interactive_environments_with_online_reinforcement_learning"
        },
        {
            "paper_title": "True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning",
            "rating": 2,
            "sanitized_title": "true_knowledge_comes_from_practice_aligning_llms_with_embodied_environments_via_reinforcement_learning"
        },
        {
            "paper_title": "Language models meet world models: Embodied experiences enhance language models",
            "rating": 2,
            "sanitized_title": "language_models_meet_world_models_embodied_experiences_enhance_language_models"
        },
        {
            "paper_title": "Generative agents: Interactive simulacra of human behavior",
            "rating": 1,
            "sanitized_title": "generative_agents_interactive_simulacra_of_human_behavior"
        }
    ],
    "cost": 0.015360249999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Language Grounded Multi-agent Communication for Ad-hoc Teamwork
25 Sep 2024</p>
<p>Huao Li 
University of Pittsburgh</p>
<p>Hossein Nourkhiz Mahjoub hossein_nourkhizmahjoub@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Behdad Chalaki behdad_chalaki@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Vaishnav Tadiparthi vaishnav_tadiparthi@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Kwonjoon Lee kwonjoon_lee@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Ehsan Moradi-Pari emoradipari@honda-ri.com 
Honda Research Institute USA, Inc</p>
<p>Charles Michael Lewis 
University of Pittsburgh</p>
<p>Katia P Sycara sycara@andrew.cmu.edu 
Carnegie Mellon University</p>
<p>Language Grounded Multi-agent Communication for Ad-hoc Teamwork
25 Sep 2024393953E61565D576E7529AB9D258EDAFarXiv:2409.17348v1[cs.MA]
Multi-Agent Reinforcement Learning (MARL) methods have shown promise in enabling agents to learn a shared communication protocol from scratch and accomplish challenging team tasks.However, the learned language is usually not interpretable to humans or other agents not co-trained together, limiting its applicability in ad-hoc teamwork scenarios.In this work, we propose a novel computational pipeline that aligns the communication space between MARL agents with an embedding space of human natural language by grounding agent communications on synthetic data generated by embodied Large Language Models (LLMs) in interactive teamwork scenarios.Our results demonstrate that introducing language grounding not only maintains task performance but also accelerates the emergence of communication.Furthermore, the learned communication protocols exhibit zero-shot generalization capabilities in ad-hoc teamwork scenarios with unseen teammates and novel task states.This work presents a significant step toward enabling effective communication and collaboration between artificial agents and humans in real-world teamwork settings.</p>
<p>Introduction</p>
<p>Effective communication is crucial for multiple agents to collaborate and solve team tasks.Especially in ad-hoc teamwork scenarios where agents do not coordinate a priori, the ability to share information via communication is the keystone for successful team coordination and good team performance [29].Multi-Agent Reinforcement Learning (MARL) methods have shown promise in allowing agents to learn a shared communication protocol by maximizing the task reward [37,36].However, merely maximizing the utility of communication in specific task goals might compromise task-agnostic objectives such as optimizing language complexity and informativeness [39,15], making the learned communication protocols 1) hard to interpret for humans or other agents that are not co-trained together [17,19,6] and 2) highly data-inefficient [9].In addition, most previous works learn com-munication language with atomic symbols or a combination leaving the relation between symbols unexplored [49], while only a few researches attempt to learn a semantic space for zero-shot communication of unseen states [41,16].One of the popular directions for interpretable communication is to regulate the learning process with external knowledge from human languages [40,21,27,1].However, this process is challenging due to the divergent characteristics between human and machine languages by nature [5].For example, most agent training methods (e.g., deep reinforcement learning) require a huge amount of data that is impractical for human-in-the-loop training or even collecting from humans [27].</p>
<p>The rise of Large Language Models (LLMs) provides new opportunities in grounding agent communication with human language.Recent generative text models fine-tuned with human instructions (e.g., GPT-4, Llama 3) show reasonable capabilities in completing team tasks and communicating in a human-like fashion via embodied interaction [35,24].Essentially, LLMs encapsulate a highly trained model of human language patterns in teamwork, allowing them to generate descriptions and responses that are well-grounded in natural language.For the purpose of guiding multi-agent communication, they represent the most generally available reference based on a vast corpus of human language data that would be infeasible to collect through other means.However, LLMs are known to suffer from a lack of grounding with the task environments (a.k.a.hallucinations), which prevents embodied agents from generating actionable plans [28].While attempts have been made to ground LLMs with reinforcement learning or interactive data collected from environments [46,4,38], none of them involve teamwork nor communication among multiple embodied agents.</p>
<p>In this paper, we propose a novel computational pipeline for artificial agents to learn humaninterpretable communication for ad-hoc human-agent teamwork.Specifically, we use synthetic data generated by LLM agents in interactive teamwork scenarios to align the communication space between MARL agents with human natural language.Learning signals from both language grounding and environment reinforcement regulate the emergence of a communication protocol to optimize both team performance and alignment with human language.We have also evaluated the learned communication protocol in ad-hoc teamwork scenarios with unseen teammates and novel task states.The aligned communication space enables translation between high-dimensional embeddings and natural language sentences, which facilitates ad-hoc teamwork.The proposed computational pipeline does not depend on specific MARL architecture or LLMs and should be generally compatible.We have sought to minimize the influence of prompt engineering to ensure the seamless applicability of our approach in diverse environments.To the best of our knowledge, this work is among the very first attempts at training MARL agents with human-interpretable natural language communication and evaluating them in ad-hoc teamwork experiments.</p>
<p>Related Work</p>
<p>Multi-Agent Communication</p>
<p>Reinforcement learning has been used to coordinate the teamwork and communication among multiple agents in partially observable environments.In earlier works such as DIAL [11], CommNet [37], and IC3Net [36], agents learn differentiable communication in an end-to-end fashion under the pressure of maximizing task reward.Other works use shared parameters [16] or a centralized controller [32] to stabilize the non-stationary learning process of multi-agent communication.More recently, representation learning methods such as autoencoder [25] and contrastive learning [26] are used to ground an agent's communication on individual observations.However, comm-MARL methods usually suffer from overfitting to specific interlocutors trained together [18].The learned communication protocols can not be understood by unseen teammates in ad-hoc teams, let alone another human.</p>
<p>Another relevant trend of research is Emergent Communication (EC), where researchers focus more on simulating the development of natural (i.e., symbolic) language with artificial agents [18,14].The most common task scenarios used in the EC community are reference games or Lewis signaling games [23], in which a speaker must describe an object to a listener, who must then recognize it among a set of distractions [41].However, previous research has shown that learning EC in more complicated, scaled-up, and multi-round interactive task scenarios can be challenging or even infeasible [9,7].Even in situations where agents can learn to communicate, the learned protocols are usually either not human-interpretable [17] or semantically drifting from human language [22].</p>
<p>Human-Interpretable Communication</p>
<p>To address the above-mentioned challenges, several recent works propose human-interpretable communication in RL settings.Lazaridou et al. [21] leverage pre-trained task-specific language models to provide high-level guidance for natural language communication.A few other works align lowlevel communication tokens with human language [20,40], or learn discrete prototype communication in a semantically meaningful space [41,16].But as pointed out in several studies [8,48], the low mutual intelligibility between human language and neural agent communication makes the alignment process non-trivial.Our work is closest to [27], in which researchers alternate imitating human data via supervised learning and self-play to maximize reward in a reference game.The differences are that in [27] authors try to train agents for reference games in an end-to-end fashion with backpropagation, while we train MARL agents in interactive team tasks.The exploration of this research direction is still very limited, as no previous work has ever evaluated natural language communication agents within interactive task environments and for ad-hoc human-agent teams.</p>
<p>Language-Grounded Reinforcement Learning</p>
<p>Reinforcement learning is known to struggle with long-horizon problems with sparse reward signals [30].Natural language guidance has been used to provide auxiliary rewards to improve the data efficiency and learning robustness [43].Goyal et al. [12] use step-by-step natural language instructions provided by human annotators to construct auxiliary reward-learning modules, encouraging agents to learn from expert trajectories.Narasimhan et al. [31] research the impact of language grounding on representation learning and transfer learning of RL agents in a 2D game environment.Additional work has explored grounding RL with other formats of materials such as game manuals [13,45] and human commands [47].However, none of those works has ever used the communication messages as the language ground nor guided the information-sharing process in multi-agent teamwork.</p>
<p>Preliminaries</p>
<p>We formulate the problem as a decentralized partially observable Markov Decision Process [34], which can be formally defined by the tuple (I, S, A, C, T , Ω, O, R, γ), where I is the finite set of n agents, s ∈ S is the global state space, A = × i∈I A i is the set of actions, and C = × i∈I C i is the set of communication messages for each of n agents.T : S × A → S is the transition function that maps the current state s t into next state s t+1 given the joint agent action.In our partially observable environments, each agent receives a local observation o i ∈ Ω according to observation function O : S × C × I → Ω.Finally, R : S × A → R is the reward function, while γ ∈ [0, 1) is the discount factor.At each time stamp t, each agent i takes an action a i t and sends out a communication message c i t after receiving the partial observation of task state s t along with all messages sent by other agents from last time stamp c t−1 .Each agent then receives the individual reward r i t ∈ R(s t , a t ).We consider fully cooperative settings in which the reinforcement learning objective is to maximize the total expected return of all agents:
max π i :Ω→A×C E t∈T i∈I γ t R(s i t , a i t )|a i t ∼ π i , o i t ∼ Ω(1)
We borrow the definition of language learning from [27].We define a target language L * ∈ L that we want the agents to learn, assuming L is the set of natural language and L * is the optimal communication language for achieving a good team performance in the specific task.Specifically, we consider a language L ∈ L to be a set of communication messages C which are mapped from agent observations to communication messages defined as L : Ω → C. In typical RL settings, this can be thought of as the mapping between input observation vectors and English descriptions of the observation.We consider a dataset D consisting of |D| (observation, action) pairs, which comes from expert trajectories generated by LLM embodied agents using the target language L * .The language learning objective is to train agents to speak language L * in order to collaborate with experts in ad-hoc teamwork.It is worth noting that we want the learned language to generalize to unseen examples that are not contained in D.</p>
<p>Figure 1: Illustrations of our proposed computational pipeline and evaluation environments.Predator Prey is a gridworld environment conceptualizing a team of predators with partial observation trying to search for a static prey.The task goal is for all predators to reach the prey location within the time limit.USAR simulates a team of specialists searching for and defusing bombs in an unknown environment.Because each specialist has the unique capability of defusing bombs in different colors, the team must coordinate to complete the task efficiently.</p>
<p>To train agents that perform well on the team task and speak human-interpretable language, we need to solve a multi-objective learning problem by combining the learning signals from both environment reward and supervised dataset D. The process of training an optimal communication-action policy can be defined as solving an optimization problem with two constraints 1) agents must learn to communicate effectively to maximize team performance and 2) agents must learn to use similar language as in the supervised dataset D.</p>
<p>Language Grounded Multi-agent Communication</p>
<p>In this section, we propose a computational pipeline for learning language-grounded multi-agent communication (LangGround).The general framework of LangGround is illustrated in Figure 1 consisting of two parts: collecting grounded communication from LLM agents and aligning MARL communication with language grounds.</p>
<p>Grounded communication from LLM agents</p>
<p>We use embodied LLM agents to collect samples of target language L * .To allow LLM agents to interact with the task environments, a text interface I is implemented following [24] to translate between abstract representations and English descriptions.Essentially, each of the n LLM agents is provided with general prompts about the team task and instructed to collaborate with others to achieve the common goal.At each time stamp t, the LLM agent i receives English descriptions of its own observation of the environment I(o i t ) which also includes communication messages from teammates in the last time stamp C t−1 .The LLM agent is prompted to output its next action and communication message which are then encoded into abstract A i t and C i t and used to update the task environment.</p>
<p>Theoretically, we construct a text environment in parallel with the actual task environment to ensure that embodied LLM agents are exposed to the equivalent information as RL agents, albeit in a different format.The action and communication policy of LLM agents emerge from the backbone LLM, since the provided prompts do not include any explicit instructions on team coordination or communication strategy.In the results section we will confirm findings from previous literature that modern LLMs (e.g., GPT-4) is able to perform reasonably well and communicate effectively in collaborative tasks.The expert trajectories generated by LLMs are used to construct the supervised dataset D which maps individual agent's (observation, action) pairs to natural language communication messages.D is used during the training of MARL to provide supervised learning signals to align the learned communication protocols with human language.More implementation details of LLM agents and data collection can be found in the Appendix.</p>
<p>Multi-agent Reinforcement Learning with aligned communication</p>
<p>The MARL with communication pipeline is similar to IC3Net [36] in which each agent has an independent controller model to learn how and when to communicate.During each time-step, input observation o i t is encoded and passed into each agent's individual LSTM.The hidden state of LSTM h i t is then passed to the probabilistic gating function to decide whether to pass a message to other agents.A single-layer communication network transforms h i t into communication vector c i t .The mean communication vector of all agents is finally used by each agent's LSTM to produce the action a i t .To shape the learned communication protocol toward human language, we introduce an additional supervised learning loss during the training of MARL.Specifically, at each time step, we sample a reference communication from the dataset based on each agent's observation and action, D(o i t , a i t ), representing how a human (LLM) would communicate in the same situation.We then calculate the cosine similarity between the agent communication vector c i t and the word embedding of the reference communication c h .To align the communication space learned by MARL with the high-dimensional embedding space of natural language, we construct the supervised loss function as follows:
L sup = t∈T i∈I 1 − cos(c i t , D(o i t , a i t ))(2)D(o i t , a i t ) = c h if (o i t , a i t ) ∈ D 0 otherwise (3)
The construction of the communication message is shaped by two learning signals: 1) the reinforcement learning objective which determines useful information to share with other agents based on the policy loss gradient, and 2) the supervised learning objective which imitates the communication messages used by LLM agents in dataset D. The total loss function is formulated as follows:
L = L RL + λL sup(4)
The hyperparameter λ is used to scale the supervised loss.Each agent's policy is optimized with backpropagation to minimize the joint loss.</p>
<p>Experiments</p>
<p>Environments</p>
<p>In this section, we evaluate our proposed method in two multi-agent collaborative tasks with varied setups and different characteristics.The first environment, Predator Prey [36], is widely used in comm-MARL research as a benchmark.We include this environment to represent team tasks that require all agents to share their partial observations for the team to form a complete picture of the task state.The second environment, Urban Search &amp; Rescue (USAR) [24,33], presents a more demanding challenge due to the inclusion of heterogeneous team members and the temporal dependence between their behaviors.Here, agents must communicate not only their observations but also their intentions and requests to effectively coordinate.Illustrations of the two environments are shown in Figure 1, and more details are provided in the Appendix.</p>
<p>Experiment setups</p>
<p>We compare our proposed pipeline LangGround against previous methods, including IC3Net [36], autoencoded communication (aeComm) [25], Vector-Quantized Variational Information Bottleneck (VQ-VIB) [39], prototype communication (protoComm) [41], and a control baseline of independent agents without communication (noComm).aeComm represents the state-of-the-art multi-agent communication methods that grounds communication by reconstructing encoded observations.It has been shown to outperform end-to-end RL methods and inductive biased methods in independent, decentralized settings.VQ-VIB and prototype-based method are representative solutions for humaninterpretable communication, which learn a semantic space for discrete communication tokens and perform reasonably well in human-agent teams.Finally, IC3Net has a similar architecture to our proposed pipeline representing an ablating baseline without language grounding.</p>
<p>All methods are implemented with the same centralized training decentralized execution (CTDE) architecture for a fair comparison.Each agent has an observation encoder, an LSTM for action policy, a single-layer fully-connected neural network for transferring hidden states into communication messages, and a gate function for selectively sharing messages.The parameters of the action policy and obs encoder are shared during training for a more stable learning process.We use REINFORCE [44] to train both the gating function and policy network.The communication messages are continuous vectors of dimension D = 256.</p>
<p>Results</p>
<p>As for the evaluation matrices, we first consider if LangGround allows MARL agents to complete collaborative tasks successfully (i.e., task utility) and converge to a shared communication protocol quickly (i.e., data-efficiency), in comparison with other state-of-the-art methods as the baselines.</p>
<p>Then we analyze the properties of aligned communication space such as human interpretability, topographic similarity, and zero-shot generalizability, to show how close the learned language is to the target human natural language.Finally, we evaluate the ad-hoc teamwork performance in which MARL agents must communicate and collaborate with unseen LLM teammates via natural language.</p>
<p>Task performance</p>
<p>In Figure 2, we compare the task performance of multi-agent teams using different communication methods by plotting out the average episode length during training over 3 random seeds with standard errors.</p>
<p>In Predator Prey vision = 1 (i.e., pp v1 ), our method LangGround achieves a similar final performance with IC3Net and aeComm, outperforming other baselines.However, the improvement is not outstanding due to the simplicity of the task environment.In Predator Prey vision = 0 (i.e., pp v0 ), the predator's vision range is limited to their own location making effective information sharing more important in solving this search task.As shown in the middle figure, LangGround has a comparable final performance with aeComm and outperforms other baselines.Finally, in the most challenging USAR environment, LangGround outperforms all baselines in solving the task in fewer steps with the same amount of training time-steps.In addition, language grounding also stabilizes the communication learning process such that the variance of LangGround is much smaller than other methods.</p>
<p>In summary, LangGround enables multi-agent teams to achieve on-par performance in comparison with SOTA multi-agent communication methods.Introducing language grounds as an auxiliary learning objective does not compromise the task utility of learned communication protocols while providing interpretability.We also present the most similar reference message from dataset D to illustrate the alignment between the agent communication space and the human language embedding space.</p>
<p>Aligned communication space</p>
<p>In addition to task utility, we are also interested in other properties of the learned communication space, such as human interpretability, topographic similarity, and zero-shot generalizability.</p>
<p>Semantically meaningful space</p>
<p>To evaluate whether the learned communication space is semantically meaningful, we visualize the learned communication embedding space by clustering message vectors sent by agents over 100 evaluation episodes in pp v0 , following [25].The high dimensional vectors are reduced to a two dimension space via t-SNE [42], and clustered with DBSCAN [10].As shown in Figure 3, agent communication messages can be clustered into several classifications with explicit meanings associated with agent observation from the environment.For example, the pink cluster on the right side corresponds to the situation where the agent locates in coordinates (0, 3) without vision of prey.We can look up from dataset D for the reference message that has the most similar word embedding with agent communication vectors in the pink cluster.The reference message (i.e., "moving down from (0, 3)") accurately refers to the agent observation, indicating the learned communication space is semantically meaningful and highly aligned with natural language embedding space.</p>
<p>Topographic similarity</p>
<p>The topographic similarity is defined by the correlation between object distances in the observation space and their associated signal distances in the communication space [2].This property is usually associated with language compositionality and ease of generalization.The intuition behind this measure is that agents should emit similar communication messages given semantically similar observations.We calculate this measure following [19], based on agent trajectories collected from 100 evaluation episodes in pp v0 .We first calculate 1) the cosine similarity between all pairs of communication vectors, and 2) the Euclidean distance between all pairs of agent locations.Then, we calculate the negative Spearman correlation ρ as the measure of topographic similarity.Table 1 indicates that our method (i.e., LangGround) results in the highest topographic similarity ρ = 0.67 among all other baselines, exhibiting a relatively more similar property as human language.</p>
<p>Human interpretabiliy</p>
<p>Given the goal of aligning agent communication with human language, it is intuitive to evaluate the human interpretability of language-grounded agent communication.We use the offline dataset D as the reference to calculate the similarity between communication messages shared by LangGround agents and LLM agents in same situations.Given 100 evaluation episodes in pp v0 , we calculate 1) the cosine similarity between word embedding and agent communication vectors, and 2) BLEU score between natural language messages and reference messages in D with the most similar word embedding as agent communication vectors.The results are shown in Table 2, demonstrating that LangGround achieves significant gains in both metrics, cosine similarity and BLEU score, compared to baselines without alignment.These measures for other baseline methods would be equivalent to random chance because none of them are grounded with language during training, and hence we do not compute their performance on these metrics.</p>
<p>Ad-hoc Teamwork</p>
<p>The ultimate goal of our proposed pipeline is to facilitate ad-hoc teamwork between unseen agents without pre-coordination.Here, we propose two experiments to evaluate the zero-shot generalizability and ad-hoc collaboration capability of our trained agents.</p>
<p>Zero-shot generalizability</p>
<p>One prerequisite of ad-hoc teamwork is the ability to communicate about unseen states to their teammates.We evaluate this capability by removing a subset of prey spawn locations from environment initialization of pp v0 and training LangGround agents from scratch.In this condition, agents would neither be exposed to nor receive any language grounding for those situations during training.During the evaluation, we record the communication messages used by agents in those unseen situations and compare them with ground truth communication from dataset D. Results show that agents are still able to complete tasks when the prey spawns in those 4 unseen locations.As shown in Table 3, the communication messages agents used to refer to those unseen locations are similar to natural language sentences generated by LLMs.These findings confirm the alignment between the agent communication space and the human language word embedding space in zero-shot conditions.More importantly, we show that LangGround is not merely a memorization of one-to-one mapping between observations and communications but also shapes the continuous communication space in a semantically meaningful way.</p>
<p>Ad-hoc teamwork between MARL and LLM agents</p>
<p>Finally, we evaluate the performance of our agents to work with unseen teammates in ad-hoc teamwork settings.Ad-hoc teams were evaluated on 8 episodes over 3 random seeds, resulting in a total of 24 evaluation episodes per condition.Ad-hoc teamwork performance is measured by the number of steps taken to complete the task; therefore, lower is better.Means and standard deviations of each condition are reported in Table 4.</p>
<p>We find that 1) homogeneous teams (i.e., LangGround and LLM) achieve better performance than ad-hoc teams because of their common understanding of both action and communication.As those agents are either co-trained together or duplicates of the same network, they form a stable strategy for team coordination and information sharing.Since ad-hoc teams (e.g., LangGround + LLM) were not co-trained together nor speak the same language, their decreased performance is expected.</p>
<p>2) The ad-hoc team performance of LangGround agents is better than noComm and aeComm agents in at least two out of three evaluation scenarios.Because aeComm is not aligned with human language, it serves as a baseline with a coordinated action policy and a random communication policy.The advantage of our method over aeComm and noComm merely comes from effective information sharing via shared language with unseen teammates.The empirical evidence presented in this section confirms the application of our method in ad-hoc teamwork.</p>
<p>Discussion</p>
<p>In this work, we developed a novel computational pipeline that enhanced the capabilities of MARL agents to interact with unseen teammates in ad-hoc teamwork scenarios.Our approach aligns the communication space of MARL agents with an embedding of human natural language by grounding agent communications on synthetic data generated by embodied LLMs in interactive teamwork scenarios.</p>
<p>Through extensive evaluations of the learned communication protocols, we observed a trade-off between utility and informativeness.According to the Information Bottleneck principle [39], informativeness corresponds to how well a language can be understood in task-agnostic situations, while utility corresponds to the degree to which a language is optimized for solving a specific task.By introducing the additional supervised learning signal, our method pushes the trade-off toward informativeness compared to traditional comm-MARL methods that merely optimize for utility.This results in the "inconsistent" patterns we observe in Figure 2 and Table 4 across different task scenarios.As language emerges under different pressures, the learned communication protocols fall at different points on the spectrum between utility and informativeness.In relatively easy tasks such as predator-prey, the learned communication is more optimized for informativeness, aligning better with human language and generalizing better in ad-hoc teamwork.In more challenging tasks such as USAR, the learned communication is more shaped toward task utility, resulting in faster convergence but worse generalizability to unseen teammates.</p>
<p>Additionally, we found that introducing language grounding does not compromise task performance but even accelerates the emergence of communication, unlike the results reported in previous literature, where jointly optimizing communication reconstruction loss and RL loss leads to a drop in task performance [25,26].The main reason for this contradiction is that our dataset D consists of expert trajectories from LLM embodied agents with a well-established grounding on the task.Therefore, the language grounding loss not only shapes the communication but also guides the action policy of MARL agents by rewarding behavior cloning and providing semantic representations of input observations.</p>
<p>We believe our work would benefit the broader society for the following reasons.This research provides empirical evidence of linguistic principles during language evolution among neural agents, which might provide insights for broader research communities, including computational linguistics, cognitive science, and social psychology.The usage of embodied LLM agents as interactive simulacra of human team behaviors has a broad impact since it has potential applications in social science and may deepen our understanding of modern LLMs.Most importantly, our proposed pipeline takes initial steps in enabling artificial agents to communicate and collaborate with humans via natural language, shedding light on the broad research direction of Human-centered AI.</p>
<p>As for future directions, we plan to evaluate our proposed pipeline in more complicated task environments at scale and experiment with different selections of MARL algorithms, backbone LLMs, and word embeddings.Particularly, we plan to replace the use of a static dataset D by querying LLMs online during the training of MARL.This may allow us to capture complex information exchanged among team members in addition to individual observations, such as beliefs, intents, and requests.</p>
<p>A Appendix</p>
<p>A.1 Implementation details</p>
<p>A.1.1 Embodied LLM agents</p>
<p>Large language models are prompted to interact with the task environments in team tasks.We implement embodied LLM agents based on the pipeline proposed in [24], where agents are augmented with explicit belief state and communication for better team collaboration capability.Each agent keeps a memory of his own observations from the environment and communication messages from other team members.Exact prompts can be found in the code within the supplementary materials.</p>
<p>The design principles are that we only provide general rules about the task environments without explicitly instructing them on any coordination or communication strategy.We attempt to minimize the influence of prompt engineering to ensure the seamless applicability of our approach in diverse environments.In our language grounding data collection and ad-hoc teamwork experiments, we use OpenAI's API to call gpt-4-0125-preview as the backbone pre-trained model and set the temperature parameter to 0 to ensure consistent outputs.</p>
<p>Example prompts for LLM agents in the U SAR environment are provided below:</p>
<p>Welcome to our interactive text game!In this game, you'll assume the role of a specialist on a search and rescue team.Alongside two other players, you'll navigate a five-room environment with a mission to defuse five hidden bombs.</p>
<p>The Map: Imagine a network of rooms represented by a connected graph where each node corresponds to a room, and the edges between nodes depict hallways.</p>
<p>The rooms are numbered 0, 3, 6, 5, and 8. Room 0 is connected to all other rooms.Room 5 shares a hallway with room 6.Room 3 is linked to room 8.And room 8 is also connected with room 6.You can only travel to adjacent, directly connected rooms at each turn.</p>
<p>The Challenge: Scattered among these rooms are five bombs, each coded with different phases represented by colors.To defuse them, you'll need to use the correct wire-cutting tools in the correct sequence.There are one-phase, two-phase, and three-phase bombs, needing 1, 2, or 3 color-coded tool applications in sequence to disarm.For instance, a bomb with a red-green phase sequence requires the red tool first, then the green one.Points are awarded based on the number of tools used for defusing a bomb, with each tool use worth 10 points.Your task is to maximize the team score as soon as possible.The challenge is that the bomb locations and sequences are unknown to players at the start.Tools: Each player is equipped with two color-coded wire cutters.As player Alpha, you have red and green tools, player Bravo wields green and blue, and player Charlie possesses blue and red.Actions: Each round, you can opt to do one of the following: 1) Move to an adjacent room, 2) Inspect a bomb's phase sequence in your current room, or 3) Apply your wire cutters to a bomb in the current room.</p>
<p>Communications: In addition to selecting an action to take from the above list, you can also send communication message texts to both of your teammates in each round.The message text you sent will be shared with both of your teammates in their observation in the next round.Observation: While you can only see what's in your current room and read text messages from teammates.You'll also be informed of the current round number, team score and the current location of your teammates.Your teammates have the same observability as you.They will not be able to know your action and its consequences unless you explicitly communicate.</p>
<p>To facilitate our interaction, reply your action selection and communication messages in this fixed format: Action selection: Your action.Message to Team: "Your Message".To move to an adjacent room, say: 'Move to Room X'.To inspect the sequence of a bomb in your current room, say: 'Inspect Bomb'.To apply a wire cutter tool, say: 'Apply X Tool'.Remember, your replies must adhere strictly to these rules.Feel free to ask clarifying questions if needed.I'll supply the necessary information as we progress.Are you ready to take on this explosive challenge?</p>
<p>Example interactions between LLM agents and environments are provided below: In this task, n predators with a limited range of vision v need to search for stationary prey on an x by x grid-world environment.Each predator receives a positive reward upon reaching the prey location.Each episode ends when all predators reach the prey or exceed the maximum number of steps T .The initial locations of predators and the prey might spawn anywhere on the map.At each timestamp, the predator agent receives a partial observation of v by v grids around its own location and may select a movement action to navigate through the map.Since this is a collaborative task, predators must learn to communicate their partial observations to allow for optimal navigation of the team.We consider Predator Prey to be a more challenging task since it has a higher-dimensional observation space and a more complex action space [25].Through this environment, we aim to demonstrate that aligning agents' communication with human language is a straightforward yet effective method of grounding their communication with task observations.
Env</p>
<p>A.2.2 USAR</p>
<p>The USAR task environment is designed to simulate the collaborative and problem-solving dynamics of a search and rescue mission.Three agents (i.e., Alpha, Bravo, and Charlie) need to collaborate in locating and defusing color-coded bombs hidden in an unexplored environment.Each bomb has unique phase sequences in m colors, which are not revealed until inspected by agents.Agents start with different colored cutters and must use them in the correct sequence to defuse bombs.The environment is represented as a graph, where each of the n rooms is a node, and the hallways connecting them are edges.At each timestamp, every agent can choose from three different types of actions: moving to one of the n rooms, inspecting a bomb's sequence in the current room, or utilizing one of the m wire-cutters.The size of the action space depends on the problem scale (i.e., n + m + 1).Agents' observations are limited to their current room's contents and agent status.The team is rewarded 10*x points when an x-phase bomb is successfully defused.An episode ends when the team has defused all bombs or exceeded the time limit.This task is designed to force team coordination since each team member has unique observations and capabilities.For example, each agent only has a subset of wire cutters and must coordinate with other teammates to defuse bombs with multiple phases.Therefore, an effective communication protocol is required for efficient information sharing and team synchronization.</p>
<p>A.2.3 Environment configurations</p>
<p>For the predator and prey environment, we use a map size of 5 by 5 with 3 predators and 1 prey.The predator's range of vision is manipulated to be either 0 or 1 to create two variants of the task environment.In the situation of vision = 0, predators cannot observe the prey until they jump onto the same location.We set the maximum episode length to 20 based on previous research [16].The USAR environment comprises five rooms (n = 5) and five bombs, including two single-phase, two double-phase, and one triple-phase bombs.The bomb phase might have three different colors (m = 3).Each of the 3 agents spawns with 2 different wire cutters, forcing the team to collaborate in defusing bombs with multiple phases.Each successfully defused bomb awards the team 10 points per processed phase, resulting in 90 as the maximum score per mission.We set the maximum episode length to 100 based on previous research [24].</p>
<p>A.2.4 Text game interface</p>
<p>The initial task environments of Predator Prey and USAR are implemented for MARL agents based on Gym API [3].To facilitate interaction between LLM-based agents with the environment, we implement a rule-based text interface for each task.At each timestamp, LLM agents sequentially interact with the environment, receiving observations and performing actions via natural language interaction.Additionally, they are allowed to broadcast communication messages in natural language which are appended with the observation text and sent to all team members in the next round.It is worth noting that LLM agents receive equivalent information as MARL agents, limited to individual agent's partial observation.</p>
<p>The text interface facilitates communication between the game engine and the language model agents by converting game state observations into natural language descriptions and mapping agent responses back to valid game actions.To generate observations, the interface extracts relevant state features from the game engine, such as the current round number, cumulative team score, action feedback, visible objects, and communication messages from other agents.It then populates predefined sentence templates with these extracted features to produce a structured natural language description of the current game state.Action encoding relies on keyword matching, as the language models are instructed to format their responses using specific keywords and structures.The interface scans the agent's response for these predefined keywords and maps them to corresponding game actions.In cases where an agent's response is invalid or ambiguous, such as attempting to perform an action in an incorrect location, the interface generates an informative error message based on predefined rules and templates.For instance, if an agent attempts to inspect a non-existent bomb, the interface might respond with the following error message: "There is no bomb in the current location, Room X, for you to inspect."This targeted feedback helps the agents refine their actions to comply with the game's rules and constraints.</p>
<p>A.3 Data collection details</p>
<p>A.3.1 LangGround dataset</p>
<p>In order to construct dataset D, we collected expert trajectories from embodied LLM agents powered by GPT-4 in interactive task scenarios.As shown in Table 4, teams consisting of pure LLM agents perform reasonably well in comparison to MARL methods.Therefore, we believe their action and communication policy can be used in guiding MARL agents.In U SAR, we collected 50 episodes resulting in 2550 pairs of (observation, action) and communication messages of individual agents.</p>
<p>The number of data pairs is 1893 for pp v0 and 2493 for pp v1 , respectively.To facilitate the alignment of agent communication space and natural language, we use OpenAI's word embedding api (i.e., text-embedding-3-large) to translate each natural language message into a high-dimensional vector with the same dimension (i.e., D = 256) as agent communication vectors.</p>
<p>A.3.2 Ad-hoc teamwork</p>
<p>Due to the restrictions of resources and time, we use embodied LLM agents to emulate human behaviors in human-agent teams.Using a similar framework described earlier, we put 2 MARL agents and 1 LLM agent into a team and expose them to the task environment via Gym API and text interface.Natural language communication messages from LLMs are embedded using OpenAI's word embedding API and sent to MARL agents.The communication vectors from MARL agents are translated to English sentences via cosine similarity matching in dataset D.</p>
<p>Figure 2 :
2
Figure 2: Learning curves of LangGround in comparison with baseline methods.The y-axis is task performance measured by the episode length until task completion, which is lower the better.The x-axis is the number of training timestamps.Shaded areas are standard errors over three random seeds.</p>
<p>Figure 3 :
3
Figure 3: Learned communication embedding space.Communication vectors between agents in pp v0 are visualized with t-SNE and clustered with DBSCAN.Two semantically meaningful clusters are identified as examples, each corresponding to a specific agent observation.We also present the most similar reference message from dataset D to illustrate the alignment between the agent communication space and the human language embedding space.</p>
<p>Table 1 :
1
Topographic similarity in pp v0
MethodsTopo Sim ρLangGround 0.67±0.07Env∆ Cos sim ∆ Bleu scoreIC3Net0.54±0.14pp v10.82±0.02 0.52±0.03aeComm0.37±0.05pp v00.81±0.03 0.45±0.12protoComm 0.35±0.35U SAR 0.79±0.12 0.42±0.04</p>
<p>Table 2 :
2
Similarity gain w/ LangGround</p>
<p>Table 3 :
3
Zero-shot generalizability in pp v0 Prey Loc Cos sim Bleu score Example message
(1,1)0.810.41Moving up to converge on prey location at (1,0) for capture(1,3)0.810.27Converging on prey location at (1,3)(3,1)0.820.51Moving up toward prey location at (3,1)(3,3)0.780.72Converging on prey location at (3,3)</p>
<p>Table 4 :
4
Ad-hoc teamwork performance (lower is better)
Team compositionpp v1pp v0U SARLangGround4.3 ± 1.2010.9 ± 4.53 22.0 ± 4.24LLM6.8 ± 5.2011.6 ± 5.30 15.9 ± 3.37LangGround + LLM 8.5 ± 5.7615.5 ± 4.80 23.2 ± 10.61aeComm + LLM10.3 ± 6.46 17.5 ± 4.60 20.3 ± 9.07noComm + LLM10.6 ± 5.73 20.0 ± 0.00 32.4 ± 13.47</p>
<p>:</p>
<p>109,15,41,36,33,25] 3 with Bomb 5. What is your next action?Alpha: Action selection: Move to Room 5. Message to Team: "I am moving to Room 5 t help Charlie with Bomb 3.The implementation of LangGround and baseline MARL methods is based on open-sourced codebase provided by the original paper authors[39,15,41,36,33,25].Agents were trained with the same setup and hyperparameters after empirical searching.The batch size is 500, and the number of update iterations in an epoch is10.Training on pp v0 and U SAR takes 2000 epochs and 1e7 timestamps, which takes about 4 hours to complete.Training on pp v1 takes 500 epochs and 2.5e6 timestamps, which takes about 1.5 hours to complete.We use a learning rate of 0.0001 for U SAR and 0.001 for pp.MARL agent's action policy is an LSTM with a hidden layer of size 256.Communication vectors are exchanged one round at each timestamp.The supervised learning weight λ is 1 in pp and 10 in U SAR. VQ-VIB and prototype communication agents were allowed to use 58 different discrete tokens to share information during teamwork.All experiments were conducted on a machine with a 14-core Intel(R) Core(TM) i9-12900H CPU and 64GB memory.
Env: You can not directly move to Room 5 because it is not adjacent to yourcurrent location; Room 3. Consider taking a detour to another room first and thenmove to your destination.Example communication among LLM agents in U SAR:Alpha: I am inspecting Bomb 1 in Room 0. Bravo; please move to Room 3.Charlie; please move to Room 5.Bravo: I am moving to Room 3 as instructed. Alpha; please defuse Bomb 1 inRoom 0 using the red tool.Charlie: I am moving to Room 5 to search for more bombs.A.1.2 LangGround and baseline MARL agentsA.2 Environment detailsA.2.1 Predator Prey</p>
<p>Community regularization of visually-grounded dialog. Akshat Agarwal, Swaminathan Gurumurthy, Vasu Sharma, Mike Lewis, Katia Sycara, Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. the 18th International Conference on Autonomous Agents and MultiAgent Systems2019</p>
<p>Understanding linguistic evolution by visualizing the emergence of topographic mappings. Henry Brighton, Simon Kirby, Artificial life. 1222006</p>
<p>Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, Wojciech Zaremba, Openai gym. 2016</p>
<p>Grounding large language models in interactive environments with online reinforcement learning. Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, Pierre-Yves Oudeyer, International Conference on Machine Learning. PMLR2023</p>
<p>Anti-efficient encoding in emergent communication. Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, Marco Baroni, Advances in Neural Information Processing Systems. 201932</p>
<p>Communicating artificial neural networks develop efficient color-naming systems. Rahma Chaabouni, Eugene Kharitonov, Emmanuel Dupoux, Marco Baroni, Proceedings of the National Academy of Sciences. 11812e20165691182021</p>
<p>Angeliki Lazaridou, and Bilal Piot. Emergent communication at scale. Rahma Chaabouni, Florian Strub, Florent Altché, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, International conference on learning representations. 2021</p>
<p>Communication breakdown: On the low mutual intelligibility between human and neural captioning. Roberto Dessì, Eleonora Gualdoni, Francesca Franzon, Gemma Boleda, Marco Baroni, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Angeliki Lazaridou, and Thore Graepel. Biases for emergent communication in multi-agent reinforcement learning. Tom Eccles, Yoram Bachrach, Guy Lever, Advances in neural information processing systems. 201932</p>
<p>A density-based algorithm for discovering clusters in large spatial databases with noise. Martin Ester, Hans-Peter Kriegel, Jörg Sander, Xiaowei Xu, kdd. 199696</p>
<p>Learning to communicate with deep multi-agent reinforcement learning. Jakob Foerster, Alexandros Ioannis, Nando De Assael, Shimon Freitas, Whiteson, Advances in neural information processing systems. 292016</p>
<p>Using natural language for reward shaping in reinforcement learning. Prasoon Goyal, Scott Niekum, Raymond J Mooney, Proceedings of the 28th International Joint Conference on Artificial Intelligence. the 28th International Joint Conference on Artificial Intelligence2019</p>
<p>Grounding language to entities and dynamics for generalization in reinforcement learning. Victor Y Austin W Hanjie, Karthik Zhong, Narasimhan, International Conference on Machine Learning. PMLR2021</p>
<p>On the role of emergent communication for social learning in multi-agent reinforcement learning. Seth Karten, Siva Kailas, Huao Li, Katia Sycara, arXiv:2302.142762023arXiv preprint</p>
<p>Intent-grounded compositional communication through mutual information in multi-agent teams. Seth Karten, Katia Sycara, Workshop on Decision Making in Multi-Agent Systems at International Conference on Intelligent Robots and Systems (IROS). 2022</p>
<p>Interpretable learned emergent communication for human-agent teams. Seth Karten, Mycal Tucker, Huao Li, Siva Kailas, Michael Lewis, Katia Sycara, IEEE Transactions on Cognitive and Developmental Systems. 2023</p>
<p>Natural language does not emerge 'naturally'in multi-agent dialog. Satwik Kottur, José Moura, Stefan Lee, Dhruv Batra, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language Processing2017</p>
<p>Emergent multi-agent communication in the deep learning era. Angeliki Lazaridou, Marco Baroni, arXiv:2006.024192020arXiv preprint</p>
<p>Emergence of linguistic communication from referential games with symbolic and pixel input. Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, Stephen Clark, International Conference on Learning Representations. 2018</p>
<p>Multi-agent cooperation and the emergence of (natural) language. Angeliki Lazaridou, Alexander Peysakhovich, Marco Baroni, arXiv:1612.071822016arXiv preprint</p>
<p>Multi-agent communication meets natural language: Synergies between functional and structural language learning. Angeliki Lazaridou, Anna Potapenko, Olivier Tieleman, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Countering language drift via visual grounding. Jason Lee, Kyunghyun Cho, Douwe Kiela, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)2019</p>
<p>Convention: A philosophical study. David Lewis, 2008John Wiley &amp; Sons</p>
<p>Theory of mind for multi-agent collaboration via large language models. Huao Li, Yu Chong, Simon Stepputtis, Joseph P Campbell, Dana Hughes, Charles Lewis, Katia Sycara, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. the 2023 Conference on Empirical Methods in Natural Language Processing2023</p>
<p>Learning to ground multi-agent communication with autoencoders. Toru Lin, Jacob Huh, Christopher Stauffer, Ser , Nam Lim, Phillip Isola, Advances in Neural Information Processing Systems. 202134</p>
<p>Learning multi-agent communication with contrastive learning. Long Yat, Biswa Lo, Jakob Nicolaus Sengupta, Michael Foerster, Noukhovitch, The Twelfth International Conference on Learning Representations. 2023</p>
<p>On the interaction between supervision and self-play in emergent communication. Ryan Lowe, Abhinav Gupta, Jakob Foerster, Douwe Kiela, Joelle Pineau, International Conference on Learning Representations. 2019</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, arXiv:2301.06627Dissociating language and thought in large language models: a cognitive perspective. 2023arXiv preprint</p>
<p>Harel Yedidsion, and Peter Stone. A penny for your thoughts: The value of communication in ad hoc teamwork. Reuth Mirsky, William Macke, Andy Wang, International Joint Conference on Artificial Intelligence. 2020</p>
<p>Human-level control through deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, nature. 51875402015</p>
<p>Grounding language for transfer in deep reinforcement learning. Karthik Narasimhan, Regina Barzilay, Tommi Jaakkola, Journal of Artificial Intelligence Research. 632018</p>
<p>Multi-agent graph-attention communication and teaming. Yaru Niu, Matthew C Rohan R Paleja, Gombolay, AAMAS. 20212120</p>
<p>Deep interpretable models of theory of mind. Ini Oguntola, Dana Hughes, Katia Sycara, 2021 30th IEEE international conference on robot &amp; human interactive communication (RO-MAN). IEEE2021</p>
<p>A concise introduction to decentralized POMDPs. Christopher Frans A Oliehoek, Amato, 2016Springer1</p>
<p>Generative agents: Interactive simulacra of human behavior. Sung Joon, Park, O' Joseph, Carrie Jun Brien, Meredith Ringel Cai, Percy Morris, Michael S Liang, Bernstein, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. the 36th Annual ACM Symposium on User Interface Software and Technology2023</p>
<p>Learning when to communicate at scale in multiagent cooperative and competitive tasks. Amanpreet Singh, Tushar Jain, Sainbayar Sukhbaatar, International Conference on Learning Representations. 2018</p>
<p>Learning multiagent communication with backpropagation. Sainbayar Sukhbaatar, Rob Fergus, Advances in neural information processing systems. 292016</p>
<p>True knowledge comes from practice: Aligning llms with embodied environments via reinforcement learning. Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo An, arXiv:2401.141512024arXiv preprint</p>
<p>Trading off utility, informativeness, and complexity in emergent communication. Mycal Tucker, Roger Levy, Julie A Shah, Noga Zaslavsky, Advances in neural information processing systems. 202235</p>
<p>Generalization and translatability in emergent communication via informational constraints. Mycal Tucker, Roger P Levy, Julie Shah, Noga Zaslavsky, NeurIPS 2022 Workshop on Information-Theoretic Principles in Cognitive Systems. 2022</p>
<p>Emergent discrete communication in semantic spaces. Mycal Tucker, Huao Li, Siddharth Agrawal, Dana Hughes, Katia Sycara, Michael Lewis, Julie A Shah, Advances in Neural Information Processing Systems. 342021</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9112008</p>
<p>Grounding natural language commands to starcraft ii game states for narration-guided reinforcement learning. Nicholas Waytowich, Sean L Barton, Vernon Lawhern, Ethan Stump, Garrett Warnell, Artificial intelligence and machine learning for multi-domain operations applications. SPIE201911006</p>
<p>Simple statistical gradient-following algorithms for connectionist reinforcement learning. Williams Ronald, Machine learning. 831992</p>
<p>Read and reap the rewards: Learning to play atari with the help of instruction manuals. Yue Wu, Yewen Fan, Paul Pu Liang, Amos Azaria, Yuanzhi Li, Tom M Mitchell, Advances in Neural Information Processing Systems. 202436</p>
<p>Language models meet world models: Embodied experiences enhance language models. Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, Zhiting Hu, Advances in neural information processing systems. 202436</p>
<p>Grounded reinforcement learning: Learning to win the game under human commands. Shusheng Xu, Huaijie Wang, Yi Wu, Advances in Neural Information Processing Systems. 202235</p>
<p>Linking emergent and natural languages via corpus transfer. Shunyu Yao, Mo Yu, Yang Zhang, Joshua B Karthik R Narasimhan, Chuang Tenenbaum, Gan, arXiv:2203.133442022arXiv preprint</p>
<p>A survey of multi-agent deep reinforcement learning with communication. Changxi Zhu, Mehdi Dastani, Shihan Wang, Autonomous Agents and Multi-Agent Systems. 38142024</p>            </div>
        </div>

    </div>
</body>
</html>
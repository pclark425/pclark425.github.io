<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5743 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5743</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5743</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-115.html">extraction-schema-115</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <p><strong>Paper ID:</strong> paper-257364842</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.03378v1.pdf" target="_blank">PaLM-E: An Embodied Multimodal Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5743.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5743.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E (failure detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E used for detecting execution failures in mobile manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper uses PaLM-E — a multimodal embodied language model that injects image and state embeddings into a PaLM decoder — to perform binary failure-detection of low-level robot skills from an image-conditioned prompt (Given <img>. Q: Was <skill> successful?). The model is applied as a VQA-style classifier over multimodal prefixes and evaluated with precision/recall/F1 against baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E (PaLM backbone + ViT/OSRT encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only large language model (PaLM) into which continuous observations (images, state vectors, or object-centric slots) are injected as embedding vectors interleaved with text tokens; experiments use variants combining PaLM LLMs (pretrained) and Vision Transformer (ViT) or OSRT encoders. Training regimes include freezing the LLM and training encoders (soft prompting) or end-to-end finetuning.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>12B / 84B / 562B (experimented variants: PaLM-E-12B, PaLM-E-84B, PaLM-E-562B)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompted VQA-style binary classification (zero-shot / finetuned): image (and optionally state/object tokens) are injected into the LLM embedding space and the model is prompted with a natural-language question 'Was <skill> successful?'; model outputs yes/no (interpreted as detecting a failure/anomaly). Experiments use frozen-LLM soft-prompting and end-to-end finetuning variants; encoders include ViT and OSRT.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Multimodal robot observation (images ± state or object-centric slots) conditioned on action/skill sequences — i.e., detecting anomalies in execution of action sequences (structured sequential outcomes), not tabular data.</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Execution failures / semantic failures of low-level skills (binary failure vs success), i.e., semantic anomaly in an action outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>Mobile-manipulation failure-detection dataset (internal mobile manipulation data; comparisons reference Ahn et al., 2022 data and Xiao et al., 2022 evaluations)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Precision / Recall / F1. Reported baselines: PaLI (zero-shot) precision≈0.59 recall≈0.98 F1≈0.73 (other table variants show PaLI ≈0.57/0.69/0.62); CLIP-FT reported precision≈0.50 recall≈0.95 F1≈0.65. PaLM-E (trained on the full mixture) achieves substantially higher scores (examples from tables: precision≈0.95 recall≈0.80 F1≈0.87 and in a stronger condition precision≈0.92 recall≈0.88 F1≈0.91). Exact per-checkpoint numbers vary by training mixture and whether LLM is frozen or finetuned.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>PaLM-E outperforms zero-shot PaLI, finetuned CLIP baselines (CLIP-FT, CLIP-FT-hindsight), value-function thresholding from QT-OPT, and a method from Xiao et al. (2022). The paper emphasizes PaLM-E's superior precision/recall/F1 on the mobile manipulation failure-detection task under comparable settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>No claims that this generalizes to generic tabular or list-structured anomaly detection; evaluation is limited to robot vision-and-action datasets. Performance depends on model pretraining, encoder choice (ViT vs OSRT), and training mixture — frozen LLM variants sometimes underperform on robotics tasks; ViT-only inputs performed poorly in low-data regimes without co-training; dataset and evaluation details are specific to embodied tasks and may not transfer to non-visual structured/tabular anomaly detection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaLM-E: An Embodied Multimodal Language Model', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5743.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5743.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of using language models for detecting anomalies in lists, sequences, or structured/tabular data, including details of the models, methods, datasets, types of anomalies, performance, and comparisons to traditional methods.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PaLM-E (plan-feasibility detection)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PaLM-E used to detect infeasible action sequences / plan feasibility in TAMP</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PaLM-E is used in TAMP (task-and-motion planning) VQA-style queries to answer questions about whether a specified sequence of actions is possible (e.g., 'Is it possible to first grasp A, then place it on B, then grasp B?'). This functions as detection of infeasible action-sequence anomalies (logical/physical impossibility) from a single scene image and object/state encodings.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>PaLM-E (PaLM backbone + ViT / OSRT encoders)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Same PaLM-E architecture as above: continuous scene observations encoded into the language embedding space of a PaLM decoder; encoders tested include ViT variants and object-centric OSRT; experiments run with frozen LLM and finetuned LLM variants (8B, 62B, 540B PaLM backbones combined with ViT sizes).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>Experiments report PaLM-E-12B (8B LLM + 4B ViT), PaLM-E-84B (62B LLM + 22B ViT), PaLM-E-562B (540B LLM + 22B ViT)</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_detection_method</strong></td>
                            <td>Prompt-based VQA: the scene is encoded and inserted into the LLM prefix; the question asks about plan feasibility (binary yes/no) or asks for a plan; PaLM-E answers whether a given sequence is possible — effectively classifying sequences as feasible vs infeasible given scene geometry and dynamics. Models are trained on TAMP data (planning examples) and evaluated on planning success and VQA-style queries.</td>
                        </tr>
                        <tr>
                            <td><strong>data_type</strong></td>
                            <td>Visual scene representations plus symbolic object descriptions (object-centric tokens) and candidate action sequences (structured sequences of actions).</td>
                        </tr>
                        <tr>
                            <td><strong>anomaly_type</strong></td>
                            <td>Infeasible / impossible action sequences (logical/physical anomalies in proposed sequences), e.g., sequences violating geometry or grasp constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>dataset_name</strong></td>
                            <td>TAMP dataset (Task and Motion Planning simulated scenes used in this paper; training scenes with 3–5 cube-shaped objects; experiments include a 96,000-scene TAMP dataset and low-data settings with 1% = 320 examples per planning task).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Planning success rates and VQA accuracy for feasibility questions. Reported trends: OSRT object-centric encodings produce the best planning performance in low-data regimes; pretraining LLM and encoder improves OOD generalization; 62B LLM generalizes better out-of-distribution than 8B; PaLM-E substantially outperforms PaLI and SayCan baselines on planning tasks. Exact numeric success rates vary by input representation and data mixture (figures/tables in paper show large improvements from co-training and OSRT encodings).</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_comparison</strong></td>
                            <td>Compared to PaLI (state-of-the-art general vision-language model not trained on robot data) and SayCan (affordance-based algorithm using oracle affordances), PaLM-E yields higher planning success and better feasibility-answering; SayCan struggled on TAMP because affordance functions do not provide sufficient long-horizon planning information.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>PaLM-E's ability to judge plan feasibility depends on the input representation (object-centric OSRT helps), pretraining, and amount of in-domain data; ViT-only encodings and frozen-LLM variants can struggle in low-data regimes. The method addresses feasibility via learned embodied reasoning and does not directly address anomaly detection in conventional tabular/list data formats.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'PaLM-E: An Embodied Multimodal Language Model', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Do as i can, not as i say: Grounding language in robotic affordances <em>(Rating: 2)</em></li>
                <li>Robotic skill acquisition via instruction augmentation with vision-language models <em>(Rating: 2)</em></li>
                <li>Pali: A jointly-scaled multilingual language-image model <em>(Rating: 1)</em></li>
                <li>QT-OPT: Scalable deep reinforcement learning for vision-based robotic manipulation <em>(Rating: 1)</em></li>
                <li>Object Scene Representation Transformer <em>(Rating: 2)</em></li>
                <li>Vima: General robot manipulation with multimodal prompts <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5743",
    "paper_id": "paper-257364842",
    "extraction_schema_id": "extraction-schema-115",
    "extracted_data": [
        {
            "name_short": "PaLM-E (failure detection)",
            "name_full": "PaLM-E used for detecting execution failures in mobile manipulation",
            "brief_description": "This paper uses PaLM-E — a multimodal embodied language model that injects image and state embeddings into a PaLM decoder — to perform binary failure-detection of low-level robot skills from an image-conditioned prompt (Given &lt;img&gt;. Q: Was &lt;skill&gt; successful?). The model is applied as a VQA-style classifier over multimodal prefixes and evaluated with precision/recall/F1 against baselines.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-E (PaLM backbone + ViT/OSRT encoders)",
            "model_description": "Decoder-only large language model (PaLM) into which continuous observations (images, state vectors, or object-centric slots) are injected as embedding vectors interleaved with text tokens; experiments use variants combining PaLM LLMs (pretrained) and Vision Transformer (ViT) or OSRT encoders. Training regimes include freezing the LLM and training encoders (soft prompting) or end-to-end finetuning.",
            "model_size": "12B / 84B / 562B (experimented variants: PaLM-E-12B, PaLM-E-84B, PaLM-E-562B)",
            "anomaly_detection_method": "Prompted VQA-style binary classification (zero-shot / finetuned): image (and optionally state/object tokens) are injected into the LLM embedding space and the model is prompted with a natural-language question 'Was &lt;skill&gt; successful?'; model outputs yes/no (interpreted as detecting a failure/anomaly). Experiments use frozen-LLM soft-prompting and end-to-end finetuning variants; encoders include ViT and OSRT.",
            "data_type": "Multimodal robot observation (images ± state or object-centric slots) conditioned on action/skill sequences — i.e., detecting anomalies in execution of action sequences (structured sequential outcomes), not tabular data.",
            "anomaly_type": "Execution failures / semantic failures of low-level skills (binary failure vs success), i.e., semantic anomaly in an action outcome.",
            "dataset_name": "Mobile-manipulation failure-detection dataset (internal mobile manipulation data; comparisons reference Ahn et al., 2022 data and Xiao et al., 2022 evaluations)",
            "performance_metrics": "Precision / Recall / F1. Reported baselines: PaLI (zero-shot) precision≈0.59 recall≈0.98 F1≈0.73 (other table variants show PaLI ≈0.57/0.69/0.62); CLIP-FT reported precision≈0.50 recall≈0.95 F1≈0.65. PaLM-E (trained on the full mixture) achieves substantially higher scores (examples from tables: precision≈0.95 recall≈0.80 F1≈0.87 and in a stronger condition precision≈0.92 recall≈0.88 F1≈0.91). Exact per-checkpoint numbers vary by training mixture and whether LLM is frozen or finetuned.",
            "baseline_comparison": "PaLM-E outperforms zero-shot PaLI, finetuned CLIP baselines (CLIP-FT, CLIP-FT-hindsight), value-function thresholding from QT-OPT, and a method from Xiao et al. (2022). The paper emphasizes PaLM-E's superior precision/recall/F1 on the mobile manipulation failure-detection task under comparable settings.",
            "limitations_or_failure_cases": "No claims that this generalizes to generic tabular or list-structured anomaly detection; evaluation is limited to robot vision-and-action datasets. Performance depends on model pretraining, encoder choice (ViT vs OSRT), and training mixture — frozen LLM variants sometimes underperform on robotics tasks; ViT-only inputs performed poorly in low-data regimes without co-training; dataset and evaluation details are specific to embodied tasks and may not transfer to non-visual structured/tabular anomaly detection.",
            "uuid": "e5743.0",
            "source_info": {
                "paper_title": "PaLM-E: An Embodied Multimodal Language Model",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "PaLM-E (plan-feasibility detection)",
            "name_full": "PaLM-E used to detect infeasible action sequences / plan feasibility in TAMP",
            "brief_description": "PaLM-E is used in TAMP (task-and-motion planning) VQA-style queries to answer questions about whether a specified sequence of actions is possible (e.g., 'Is it possible to first grasp A, then place it on B, then grasp B?'). This functions as detection of infeasible action-sequence anomalies (logical/physical impossibility) from a single scene image and object/state encodings.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "PaLM-E (PaLM backbone + ViT / OSRT encoders)",
            "model_description": "Same PaLM-E architecture as above: continuous scene observations encoded into the language embedding space of a PaLM decoder; encoders tested include ViT variants and object-centric OSRT; experiments run with frozen LLM and finetuned LLM variants (8B, 62B, 540B PaLM backbones combined with ViT sizes).",
            "model_size": "Experiments report PaLM-E-12B (8B LLM + 4B ViT), PaLM-E-84B (62B LLM + 22B ViT), PaLM-E-562B (540B LLM + 22B ViT)",
            "anomaly_detection_method": "Prompt-based VQA: the scene is encoded and inserted into the LLM prefix; the question asks about plan feasibility (binary yes/no) or asks for a plan; PaLM-E answers whether a given sequence is possible — effectively classifying sequences as feasible vs infeasible given scene geometry and dynamics. Models are trained on TAMP data (planning examples) and evaluated on planning success and VQA-style queries.",
            "data_type": "Visual scene representations plus symbolic object descriptions (object-centric tokens) and candidate action sequences (structured sequences of actions).",
            "anomaly_type": "Infeasible / impossible action sequences (logical/physical anomalies in proposed sequences), e.g., sequences violating geometry or grasp constraints.",
            "dataset_name": "TAMP dataset (Task and Motion Planning simulated scenes used in this paper; training scenes with 3–5 cube-shaped objects; experiments include a 96,000-scene TAMP dataset and low-data settings with 1% = 320 examples per planning task).",
            "performance_metrics": "Planning success rates and VQA accuracy for feasibility questions. Reported trends: OSRT object-centric encodings produce the best planning performance in low-data regimes; pretraining LLM and encoder improves OOD generalization; 62B LLM generalizes better out-of-distribution than 8B; PaLM-E substantially outperforms PaLI and SayCan baselines on planning tasks. Exact numeric success rates vary by input representation and data mixture (figures/tables in paper show large improvements from co-training and OSRT encodings).",
            "baseline_comparison": "Compared to PaLI (state-of-the-art general vision-language model not trained on robot data) and SayCan (affordance-based algorithm using oracle affordances), PaLM-E yields higher planning success and better feasibility-answering; SayCan struggled on TAMP because affordance functions do not provide sufficient long-horizon planning information.",
            "limitations_or_failure_cases": "PaLM-E's ability to judge plan feasibility depends on the input representation (object-centric OSRT helps), pretraining, and amount of in-domain data; ViT-only encodings and frozen-LLM variants can struggle in low-data regimes. The method addresses feasibility via learned embodied reasoning and does not directly address anomaly detection in conventional tabular/list data formats.",
            "uuid": "e5743.1",
            "source_info": {
                "paper_title": "PaLM-E: An Embodied Multimodal Language Model",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Do as i can, not as i say: Grounding language in robotic affordances",
            "rating": 2
        },
        {
            "paper_title": "Robotic skill acquisition via instruction augmentation with vision-language models",
            "rating": 2
        },
        {
            "paper_title": "Pali: A jointly-scaled multilingual language-image model",
            "rating": 1
        },
        {
            "paper_title": "QT-OPT: Scalable deep reinforcement learning for vision-based robotic manipulation",
            "rating": 1
        },
        {
            "paper_title": "Object Scene Representation Transformer",
            "rating": 2
        },
        {
            "paper_title": "Vima: General robot manipulation with multimodal prompts",
            "rating": 1
        }
    ],
    "cost": 0.014662999999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>PaLM-E: An Embodied Multimodal Language Model Scene Unde Visual Q&amp;A Task and Motion Planning</p>
<p>Danny Driess 
Robotics at Google</p>
<p>Berlin</p>
<p>Fei Xia 
Robotics at Google</p>
<p>Mehdi S M Sajjadi 
Google Research</p>
<p>Corey Lynch 
Robotics at Google</p>
<p>Aakanksha Chowdhery 
Google Research</p>
<p>Brian Ichter 
Robotics at Google</p>
<p>Ayzaan Wahid 
Robotics at Google</p>
<p>Jonathan Tompson 
Robotics at Google</p>
<p>Quan Vuong 
Robotics at Google</p>
<p>Tianhe Yu 
Robotics at Google</p>
<p>Wenlong Huang 
Robotics at Google</p>
<p>Yevgen Chebotar 
Robotics at Google</p>
<p>Pierre Sermanet 
Robotics at Google</p>
<p>Daniel Duckworth 
Google Research</p>
<p>Sergey Levine 
Robotics at Google</p>
<p>Vincent Vanhoucke 
Robotics at Google</p>
<p>Karol Hausman 
Robotics at Google</p>
<p>Marc Toussaint 
Berlin</p>
<p>Klaus Greff 
Google Research</p>
<p>Andy Zeng 
Robotics at Google</p>
<p>Igor Mordatch 
Google Research</p>
<p>Pete Florence 
Robotics at Google</p>
<p>PaLM-E: An Embodied Multimodal Language Model Scene Unde Visual Q&amp;A Task and Motion Planning
Given <img>. Q: What's in the image? Answer in emojis. A: . TBD TBD TBD</p>
<p>Introduction</p>
<p>Large language models (LLMs) demonstrate strong reasoning capabilities across various domains, including dialogue (Glaese et al., 2022;Thoppilan et al., 2022), step-by-step reasoning Kojima et al., 2022), math problem solving (Lewkowycz et al., 2022;Polu et al., 2022), and code writing (Chen et al., 2021a). However, a limitation of such models for inference in the real world is the issue of grounding: while training LLMs on massive textual data may lead to representations that relate to our physical world, connecting those representations to real-world visual and physical sensor modalities is essential to solving a wider range of grounded real-world problems in computer vision and robotics (Tellex et al., 2020). Previous work (Ahn et al., 2022) interfaces the output of LLMs with learned robotic policies and affordance functions to make decisions, but is limited in that the LLM itself is only provided with textual input, which is insufficient for many tasks where the geometric configuration of the scene is important. Further, in our experiments we show that current state-of-the-art visuallanguage models trained on typical vision-language tasks such as visual-question-answering (VQA) cannot directly solve robotic reasoning tasks.</p>
<p>In this paper we propose embodied language models, which arXiv:2303.03378v1 [cs.LG] 6 Mar 2023 Given <img>. Q: Who are the two teams playing in this photo, and which was the last to win a championship, and which year did they win, and who was their star player that year? A: Let's think step by step. 1. team in white is new Zero-shot: Multi-image Relationships york knick. 2. team in green is boston celtic. 3. last to win championship was boston celtic. 4. year was 2008. 5. star player was paul pierce. 6. i am not sure if this is correct. i am not basketball fan. i just google it.</p>
<p>Photo 1: <img1>. Photo 2: <img2>. Q: What is in Photo 1 but NOT in Photo 2? A: sunglasses. Photo 1: <img1>. Photo 2: <img2>. Q: What is in Photo 1 but NOT in Photo 2? A: Let's think step by step and explain. Photo 1 has sunglasses on top of folded clothes. Photo 2 does not have sunglasses on top of folded clothes. Therefore, sunglasses are in Photo 1 but not in Photo 2. Q: Where is <img1> in this scene <img2>? A: a stove top with a pot on it.</p>
<p>Q: I am getting just two custom pizza for me and my friend. How much should I pay in total? A: Let's think step by step. 1st step: 2 custom pizzas. 2nd step: 2 x $9.99. 3rd step: $19.98.</p>
<p>OCR-free Math Reasoning</p>
<p>End-to-End Egocentric Q&amp;A, Dialogue flour, and a bowl of sugar.. and a spatula.. and a spoon.. and a bag of flour.. and a bag of sugar. When a human asks me to do a task, I will respond with the sequence of actions I would do to accomplish the task with only the items I see. Human: Use all of the ingredients you see to make a cake batter. 1. crack egg. 2. put egg in bowl. 3. put flour in bowl. 4. put sugar in bowl. 5. mix. 6. put in pan. 7. bake. 8. eat. 9. clean up.</p>
<p>Physical Prediction Spatial Grounding</p>
<p>Given <img> . Q: What flavor is the doughnut on the right? Strawberry.. Q: What flavor is the doughnut on the left? A blueberry.</p>
<p>Given <img> . Q: What will the robot do next? A: fall.</p>
<p>Given <img> . Figure 2: PaLM-E-562B can do zero-shot multimodal chain-of-thought reasoning, can tell visually-conditioned jokes given an image, and demonstrates an array of robot-relevant multimodal-informed capabilities including perception, visually-grounded dialogue, and planning. PaLM-E also generalizes, zero-shot, to multi-image prompts despite only being trained on single-image prompts. PaLM-E can also perform math given an image with textually-interleaved handwritten numbers. In addition, the model can perform, zero-shot, question and answering on temporally-annotated egocentric vision, similar to what was shown in  but end-to-end all in one model. directly incorporate continuous inputs from sensor modalities of an embodied agent and thereby enable the language model itself to make more grounded inferences for sequential decision making in the real world. Inputs such as images and state estimates are embedded into the same latent embedding as language tokens and processed by the self-attention layers of a Transformer-based LLM in the same way as text. We start from a pre-trained LLM in which we inject the continuous inputs through an encoder. These encoders are trained end-to-end to output sequential decisions in terms of natural text that can be interpreted by the embodied agent by conditioning low-level policies or give an answer to an embodied question. We evaluate the approach in a variety of settings, comparing different input representations (e.g. standard vs. object-centric ViT encodings for visual input), freezing vs. finetuning the language model while training the encoders, and investigating whether co-training on multiple tasks enables transfer.</p>
<p>To investigate the approach's breadth, we evaluate on three robotic manipulation domains (two of which are closedloop in the real-world), standard visual-language tasks such as VQA and image captioning, as well as language tasks. Our results indicate that multi-task training improves performance compared to training models on individual tasks. We show that this transfer across tasks can lead to high data-efficiency for robotics tasks, e.g. significantly increasing learning success from handfuls of training examples, and even demonstrating one-shot or zero-shot generalization to novel combinations of objects or unseen objects.</p>
<p>We scale PaLM-E up to 562B parameters, integrating the 540B PaLM (Chowdhery et al., 2022) LLM and the 22B Vision Transformer (ViT) (Dehghani et al., 2023) into, to our knowledge, the largest vision-language model currently reported. PaLM-E-562B achieves state-of-the-art performance on the OK-VQA (Marino et al., 2019) benchmark, without relying on task-specific finetuning. Although not the focus of our experimentation, we also find ( Fig. 2) that PaLM-E-562B exhibits a wide array of capabilities including zero-shot multimodal chain-of-thought (CoT) reasoning, few-shot prompting, OCR-free math reasoning, and multiimage reasoning, despite being trained on only single-image examples. Zero-shot CoT (Kojima et al., 2022), originally a language-only concept, has been shown on multimodal data with task-specific programs  but to our knowledge, not via an end-to-end model.</p>
<p>To summarize our main contributions, we (1) propose and demonstrate that a generalist, transfer-learned, multiembodiment decision-making agent can be trained via mixing in embodied data into the training of a multimodal large language model. We show that, (2) while current state-ofthe-art general-purpose visual-language models out-of-thebox (zero-shot) do not well address embodied reasoning problems, it is possible to train a competent general-purpose visual-language model that is also an efficient embodied reasoner. In studying how to best train such models, we (3) introduce novel architectural ideas such as neural scene representations and entity-labeling multimodal tokens. Finally, in addition to our focus on PaLM-E as an embodied reasoner we (4) show that PaLM-E is also a quantitatively competent vision and language generalist, and (5) demonstrate that scaling the language model size enables multimodal finetuning with less catastrophic forgetting.</p>
<p>Related Work</p>
<p>General vision-language modeling. Building on successes in large language (Brown et al., 2020;Devlin et al., 2018) and vision  models, recent years have seen a growing interest in large vision-language models (VLMs) (Li et al., 2019;Lu et al., 2019;Hao et al., 2022;Gan et al., 2022). Unlike their predecessors, VLMs are capable of simultaneously understanding both images and text, and can be applied to tasks such as visual question answering (Zhou et al., 2020;Zellers et al., 2021b), captioning (Hu et al., 2022), optical character recognition (Li et al., 2021), and object detection (Chen et al., 2021b). The methods by which images are integrated varies. For example, Alayrac et al. (2022) augments pretrained language models with a mechanism to directly attend to a single context image. In contrast, PaLM-E represents images and text as "multimodal sentences" of latent vectors, allowing it to process multiple images in a flexible way within any part of a sentence. More closely related to our work is Frozen (Tsimpoukelli et al., 2021) where vision encoder parameters are optimized via backpropagation through a frozen LLM (Lu et al., 2021). Inspired by this work, we investigate the design in a broader scope by introducing alternative input modalities (e.g. neural scene representations), and our proposed approach empirically outperforms Frozen by more than 45% on the VQAv2 benchmark. More importantly we demonstrate that PaLM-E is applicable not only to perceptual but also embodied tasks.</p>
<p>Actions-output models. Prior works focus on combining vision and language inputs in an embodied setting with the goal of direct action prediction (Guhur et al., 2022;Shridhar et al., 2022b;a;Zhang &amp; Chai, 2021;Silva et al., 2021;Jang et al., 2022;Nair et al., 2022;Lynch et al., 2022;Brohan et al., 2022). Among these methods, VIMA (Jiang et al., 2022) explores multimodal prompts similar to PaLM-E. The role of language is perhaps most aptly described as task specification in these works. In contrast, PaLM-E generates high-level instructions as text; in doing so, the model is able to naturally condition upon its own predictions and di-rectly leverage the world knowledge embedded in its parameters. This enables not only embodied reasoning but also question answering, as demonstrated in our experiments. Among works that output actions, perhaps most similar is the approach proposed in Gato (Reed et al., 2022) which, like PaLM-E, is a generalist multi-embodiment agent. In contrast to Gato, we demonstrate positive transfer across different tasks where the model benefits from diverse joint training across multiple domains.</p>
<p>LLMs in embodied task planning. There have been several methods proposed to leverage LLMs in embodied domains. While many works focus on understanding natural language goals (Lynch &amp; Sermanet, 2020;Shridhar et al., 2022a;Nair et al., 2022;Lynch et al., 2022), fewer consider natural language as a representation for planningthe focus of this work. LLMs contain vast amounts of internalized knowledge about the world (Bommasani et al., 2021), but without grounding, generated plans may be impossible to execute. One line of research has employed prompting to elicit a sequence of instructions directly from an LLM either by leveraging semantic similarity between an LLM's generation and an eligible set of instructions (Huang et al., 2022b), incorporating affordance functions (Ahn et al., 2022), visual feedback (Huang et al., 2022c), generating world models (Nottingham et al., 2023;Zellers et al., 2021a), planning over graphs and maps (Shah et al., 2022;Huang et al., 2022a), visual explanations (Wang et al., 2023), program generation (Liang et al., 2022;Singh et al., 2022), or injecting information into the prompt . In contrast, PaLM-E is trained to generate plans directly without relying on auxiliary models for grounding. This in turn enables direct integration of the rich semantic knowledge stored in pretrained LLMs into the planning process.</p>
<p>With few exceptions, the parameters of the LLMs employed in many of these works are employed as-is without further training. In LID , this constraint is relaxed and LLM parameters are finetuned to produce a planning network for generating high-level instructions. (SL) 3 (Sharma et al., 2021) tackles the more challenging task of simultaneously finetuning two LLMs: a planning network, which produces high-level instructions, and a low-level policy network, which selects actions. With PaLM-E, our interests are distinct and complementary: we investigate a generalist, multi-embodiment model, across multiple modalities.</p>
<p>PaLM-E: An Embodied Multimodal Language Model</p>
<p>The main architectural idea of PaLM-E is to inject continuous, embodied observations such as images, state estimates, or other sensor modalities into the language embedding space of a pre-trained language model. This is realized by encoding the continuous observations into a sequence of vectors with the same dimension as the embedding space of the language tokens. The continuous information is hence injected into the language model in an analogous way to language tokens. PaLM-E is a decoder-only LLM that generates textual completions autoregressively given a prefix or prompt. We call our model PaLM-E, since we use PaLM (Chowdhery et al., 2022) as the pre-trained language model, and make it Embodied.</p>
<p>The inputs to PaLM-E consist of text and (multiple) continuous observations. The multimodal tokens corresponding to these observations are interleaved with the text to form multi-modal sentences. An example of such a multi-modal sentence is Q: What happened between <img 1> and <img 2>? where <img i> represents an embedding of an image. The output of PaLM-E is text generated auto-regressively by the model, which could be an answer to a question, or a sequence of decisions produced by PaLM-E in textual form that should be executed by a robot. When PaLM-E is tasked with producing decisions or plans, we assume that there exists a low-level policy or planner that can translate these decisions into low-level actions. Prior work has discussed a variety of ways to train such low-level policies (Lynch &amp; Sermanet, 2020;Brohan et al., 2022), and we use these prior methods directly without modification.</p>
<p>In the following, we describe our approach more formally.</p>
<p>Decoder-only LLMs. Decoder-only large language models (LLMs) are generative models trained to predict the probability p(w 1:L ) of a piece of text w 1:L = (w 1 , . . . , w L ) that is represented as a sequence of tokens w i ∈ W. Typical neural architectures realize this by factorizing into
p(w 1:L ) = L l=1 p LM (w l |w 1:l−1 ),(1)
where p LM is a large transformer network.</p>
<p>Prefix-decoder-only LLMs. Since the LLM is autoregressive, a pre-trained model can be conditioned on a prefix w 1:n without the necessity to change the architecture
p(w n+1:L |w 1:n ) = L l=n+1 p LM (w l |w 1:l−1 ).(2)
The prefix or prompt w 1:n provides the context based on which the LLM continues to predict the subsequent tokens w n+1:L . This is often used for inference to steer the predictions of the model. For example, the prompt can contain a description of the task the LLM should solve or examples of desired text completions for similar tasks.</p>
<p>Token embedding space. The tokens w i are elements of a fixed vocabulary W which is a discrete, finite set corresponding to (sub)words in natural language. Internally, the LLM embeds w i into a word token embedding space X ⊂ R k via
γ : W → X , i.e. p LM (w l |x 1:l−1 ) with x i = γ(w i ) ∈ R k .
The mapping γ is typically represented as a large embedding matrix of size k × |W| and trained end-to-end. In our case, |W| = 256 000 (Chowdhery et al., 2022).</p>
<p>Multi-modal sentences: injection of continuous observations. Multi-modal information such as image observations can be injected into the LLM by skipping the discrete token level and directly mapping the continuous observations into the language embedding space X . To this end, we train an encoder φ : O → X q that maps a (continuous) observation space O (refer to Sec. 4 for details) into a sequence of q-many vectors in X . These vectors are then interleaved with normal embedded text tokens to form the prefix for the LLM. This means that each vector x i in the prefix is formed from either the word token embedder γ or an encoder φ i :
x i = γ(w i ) if i a is text token, or φ j (O j ) i if i corresponds to observation O j .(3)
Note that a single observation O j is usually encoded into multiple embedding vectors. It is possible to interleave different encoders φ i at different locations in the prefix to combine, e.g., information from different observation spaces. Injecting the continuous information this way into the LLM reuses its existing positional encodings. In contrast to other VLM approaches (e.g, ), the observation embeddings are not inserted at fixed positions, but instead placed dynamically within the surrounding text.</p>
<p>Embodying the output: PaLM-E in a robot control loop.</p>
<p>PaLM-E is a generative model producing text based on multi-model sentences as input. In order to connect the output of the model to an embodiment, we distinguish two cases. If the task can be accomplished by outputting text only as, e.g., in embodied question answering or scene description tasks, then the output of the model is directly considered to be the solution for the task.</p>
<p>Alternatively, if PaLM-E is used to solve an embodied planning or control task, it generates text that conditions lowlevel commands. In particular, we assume to have access to policies that can perform low-level skills from some (small) vocabulary, and a successful plan from PaLM-E must consist of a sequence of such skills. Note that PaLM-E must determine on its own which skills are available based on the training data and the prompt, and no other mechanism is used to constrain or filter its outputs. Although these policies are language conditioned, they are not capable of solving long-horizon tasks or taking in complex instructions.</p>
<p>PaLM-E is hence integrated into a control-loop, where its predicted decisions are executed through the low-level policies by a robot, leading to new observations based on which PaLM-E is able to replan if necessary. In this sense, PaLM-E can be understood as a high-level policy that sequences and controls the low-level policies.</p>
<p>Input &amp; Scene Representations for Different Sensor Modalities</p>
<p>In this section, we describe the individual modalities that we incorporate into PaLM-E, and how we set up their encoders. We propose different architectural choices for each encoder φ : O → X to map the corresponding modality into the language embedding space. We investigate state estimation vectors, Vision Transformers (ViTs) Chen et al., 2022;Ryoo et al., 2021) for 2D image features, and the 3D-aware Object Scene Representation Transformer (OSRT) (Sajjadi et al., 2022a). In addition to encoders that represent the input scene globally, we consider object-centric representations that factor observations into tokens that represent individual objects in the scene.</p>
<p>State estimation vectors. State vectors, e.g. from a robot or a state estimate for objects, are perhaps the simplest to input into PaLM-E. Let s ∈ R S be a vector describing the state of the objects in a scene. For example, s could contain the pose, size, color etc. of those objects. Then, the MLP φ state maps s into the language embedding space.</p>
<p>Vision Transformer (ViT). ViTφ ViT ) is a transformer architecture mapping an image I into a number of token embeddingsx 1:m =φ ViT (I) ∈ R m×k . We consider several variants, including the 4 billion parameter model from Chen et al. (2022), which we refer to as ViT-4B, and a similar 22 billion parameter model, ViT-22B (Dehghani et al., 2023), both of which have been pretrained on image classification. We further investigate the ViT token learner architecture (ViT + TL) (Ryoo et al., 2021) which is trained end-to-end from scratch. Note that the dimensionalityk of the ViT embeddings is not necessarily the same as that of the language model. We therefore project each embedding into
x i = φ ViT (I) i = ψ(φ ViT (I) i ) with ψ being a learned affine transformation.
Object-centric representations. Unlike language, visual input is not pre-structured into meaningful entities and relationships: while ViT may capture semantics, the structure of the representation resembles a static grid rather than a collection of object instances. This poses a challenge both for interfacing with LLMs which have been pre-trained on symbols, and for solving embodied reasoning which requires interaction with physical objects. We therefore also explore structured encoders that aim to separate visual inputs into distinct objects before injecting them into the LLM. Given ground-truth object instance masks M j , we can decompose ViT's representation into x j 1:m = φ ViT (M j • I) for object j. Object Scene Representation Transformer (OSRT). An alternative that does not require ground-truth segmentations is OSRT (Sajjadi et al., 2022a): rather than relying on external knowledge about objects, they are discovered in an unsupervised way through inductive biases in the archi-tecture (Locatello et al., 2020). Based on SRT (Sajjadi et al., 2022b), OSRT learns 3D-centric neural scene representations on in-domain data through a novel view synthesis task. Its scene representations consist of object slots o j =φ OSRT (I 1:v ) j ∈ Rk. We project each of these slots into x j 1:m = ψ(φ OSRT (I 1:v ) j ) with an MLP ψ. Note that individual objects are always tokenized into multiple embeddings each, i.e. ψ : Rk → R m×k for OSRT maps into m-many embeddings.</p>
<p>Entity referrals. For embodied planning tasks, PaLM-E must be able to reference objects in its generated plan. In many cases, including the majority of our experiments, objects in a scene can be identified in natural language by some of their unique properties. However, there also exist settings where objects are not easily identifiable by language in few words, e.g. if there are multiple blocks on a table of the same color at different locations. For object-centric representations such as OSRT, we label the multi-modal tokens corresponding to an object in the input prompt as follows: Object 1 is <obj 1>. . . . Object j is <obj j>. This enables PaLM-E to reference objects via special tokens of the form obj j in its generated output sentences. In this case, we assume that the low-level policies operate on these tokens as well.</p>
<p>Training Recipes</p>
<p>PaLM-E is trained on a dataset of the form D = I i 1:ui , w i 1:Li , n i N i=1 , where each example i consists of u i -many continuous observations I i j , a text w i 1:Li , and an index n i . Despite being a decoder-only model, the text consists of a prefix part up to index n i that is formed from multi-modal sentences, and the prediction target, which only contains text tokens. The loss function is therefore a crossentropy loss averaged over the individual non-prefix tokens w i ni+1:Li . To form the multi-modal sentences within the model, we have special tokens in the text that get replaced by the embedding vectors of the encoders at the locations in the text of those tokens. We base PaLM-E on the pretrained 8B, 62B, and 540B parameter variants of PaLM as the decoder-only LLM into which we inject the continuous observations through the input encoders. Those encoders are either pre-trained or trained from scratch, see Sec. 4. We refer to an 8B LLM combined with a 4B ViT as PaLM-E-12B, similarly a 62B LLM + 22B ViT as PaLM-E-84B, and 540B LLM + 22B ViT as PaLM-E-562B.</p>
<p>Variation with Model freezing. Most of our architectures consist of three parts, an encoderφ, a projector ψ, and the LLM p LM . When training PaLM-E, one way is to update the parameters of all these components. However, LLMs show impressive reasoning capabilities if supplied with a suitable prompt . Therefore, we investigate whether it is possible to freeze the LLM and to just train the input encoders, and if so, how different-modality encoders compare. In this case, the encoder has to produce embedding vectors such that the frozen LLM is grounded on the observations, and also propagate information to the LLM about the capabilities of an embodiment. Training such encodings can be understood as a form of input-conditioned soft-prompting (Tsimpoukelli et al., 2021), in relation to normal soft prompts (Lester et al., 2021). In experiments with φ OSRT , we also freeze the slot representation, i.e. we only update the small projector ψ which serves as the interface between OSRT and the LLM.</p>
<p>Co-training across tasks. In our experiments, we investigate the effects of co-training our models on a variety of diverse data. The "full mixture", see App. A, consists primarily of a diverse set of internet-scale vision-and-language data, from a variety of tasks. The sampling frequencies are set such that only 8.9% of the full mixture is embodied data, and there are several tasks for each embodiment.</p>
<p>Experiments</p>
<p>Our experiments consider diverse robotic (mobile) manipulation tasks across three different robot embodiments, in simulation and with two different real robots. We refer to https://palm-e.github.io for videos showing the capabilities of PaLM-E on those tasks. Although not the focus of our work, we evaluate PaLM-E also on general vision-language tasks such as visual-question-answering (VQA), image captioning, and established language modeling tasks.</p>
<p>We split our experimental investigation into two broad categories. First, we compare the different input representations from Sec. 4 with respect to performance, generalization, and data-efficiency. The second thread of experiments focuses on one architecture, the main PaLM-E version, consisting of a pre-trained ViT and PaLM language model that takes in raw images as the continuous inputs. Here we show that a single model, trained on a mixture of many datasets, across diverse tasks, and across robot embodiments, can simultaneously achieve high performance on all of those tasks. Crucially, we investigate whether co-training on these datasets enables transfer (Fig. 3): despite different tasks and embodiments, the performance on the individual tasks increases by training on the mixture of tasks. We study the influence on performance, generalization, and data efficiency with respect to co-training strategies and model parameter size. Finally, we consider if freezing the LLM and just training the ViT that injects vision into the LLM is a viable path.</p>
<p>As baselines, we consider the state-of-the art visual language model PaLI , which has not been trained on embodiment robot data, as well as the SayCan algorithm (Ahn et al., 2022), supplied with oracle affordances.</p>
<p>TAMP Data Only</p>
<p>Lang.  </p>
<p>Robot Environments / Tasks</p>
<p>Our three robot environments (Fig. 1) include a Task and Motion Planning (TAMP) domain where a robot has to manipulate (grasp and stack) objects, a table-top pushing environment, and a mobile manipulation domain. In each domain, PaLM-E is trained on expert data from that domain. In many cases, this is a sparse amount of data per task. The TAMP tasks involve large combinatorics over possible plans, and many decision sequences are infeasible. PaLM-E has to generate plans that consist of multiple steps, with complicated decision boundaries. The multi-object tabletop pushing environment is taken from the publicly available Language-Table dataset  and is challenging since it includes several objects, large cardinality of language, and complex pushing dynamics. For both the TAMP and Language-Table environment, PaLM-E has to reason about the poses of the objects. It is not sufficient to know which objects are on the table or knowing their rough relationships, the more fine-grained details about the scene geometry are important for solving the tasks. Finally, we consider a mobile manipulation domain similar to SayCan (Ahn et al., 2022), where a robot has to solve a variety of tasks in a kitchen environment, including finding objects in drawers, picking them, and bringing them to a human. For all domains we consider both planning and VQA tasks in those environments. For the mobile manipulation and Language-Table environments, PaLM-E is integrated into the control loop to execute the plans in the real world, and has to adjust the plan in presence of external disturbances or failures of the low-level control policies.</p>
<p>TAMP Environment</p>
<p>Tab. 7 (appendix) shows planning success rates and VQA performance for the TAMP environment. The LLM is frozen in these experiments (for pre-trained LLM). For the results reported in Tab. 7, the input representations are trained on a dataset containing 96,000 training scenes of solely the TAMP environment, i.e. no other data is part of the mixture. For 3-5 objects in the scene, which is the same number as in the training set, most input representations perform similarly well. However, when increasing the number of objects, it turns out that using a pre-trained LLM improves performance considerably, especially with entity referrals. Furthermore, we show that a 62B LLM shows better out-of-distribution generalization compared to the 8B variant, while a non-pretrained LLM shows basically no outof-distribution generalization. The SayCan baseline (Ahn et al., 2022) utilizes oracle affordance functions and has difficulties solving this environment, since affordance functions only constrain what is possible right now, but are not informative enough for the LLM to construct long-horizon plans in TAMP environments.</p>
<p>Tab. 1 shows results for 3-5 objects when training on 1% of the dataset, which corresponds to only 320 examples for each of the two planning tasks. Here we see that there are significant differences between the input representations, especially for the planning tasks. First, pre-training the LLM is beneficial in the low data regime for state inputs. Second, both ViT variants (ViT+TL, ViT-4B) do not perform well in solving the planning tasks for this little data. However, if we co-train on all other robot environments as well as general vision-language datasets (ViT-4B generalist), then the performance of the ViT-4B more than doubles. This shows a significant transfer effect between different robot embodiments and tasks. Finally, using OSRT as the input representation leads to the best performance here, demonstrating the strengths of 3D-aware object representations. We also observe another instance of transfer here: when we remove the TAMP VQA data and only train on the 640 planning tasks examples, there is a (slight) drop in performance. The state-of-the art vision-language model PaLI  that was not trained on robot data is not able to solve the tasks. We only evaluated it on q 2 (objects left/right/center on the table) and q 3 (vertical object relations), since those most resemble typical VQA tasks.</p>
<p>Language-Table Environment</p>
<p>Tab. 2 reports success rates on long-horizon tasks from the Language-Table environment . PaLM-E is integrated into a control loop that takes as input the longhorizon task and the current image, and outputs an instruction for the low-level policy. We see that joint training on internet-scale vision and language results in a more effective model for robot planning, particularly in the few-shot regime with only 10 demos per task. Scaling the 12B model to the 84B model leads to improvements on 2 of 3 tasks. As with the TAMP environment, neither SayCan nor zero-shot PaLI are effective, unable to solve the easiest task tested.</p>
<p>TAMP Data Only</p>
<p>Lang. Real Robot Results and Few-Shot Generalization. In Fig. 7, a), we see PaLM-E is capable of guiding a real robot through a multi-stage tabletop manipulation task, while remaining robust to adversarial disturbances. Given the observed image and a long-horizon goal, e.g. "sort the blocks by colors into corners", PaLM-E outputs language subgoals at 1 Hz to the policies from Lynch et al. (2022), that output low-level robot actions at 5 Hz. Prior work  instead involved a human in the loop to interactively guide subgoals and corrections. In Fig. 5, b) we see PaLM-E is capable of one-shot and zero-shot learning. Here, we finetuned PaLM-E on 100 different long horizon tasks with a single training example each, e.g. "put all the blocks in the center", "remove the blue blocks from the line". We additionally see that PaLM-E can generalize zero-shot to tasks involving novel object pairs (Fig. 7, c) and to tasks involving objects that were unseen in either the original robot dataset or the finetuning datasets, e.g. a toy turtle (Fig. 5, d).</p>
<p>Mobile Manipulation Environment</p>
<p>We demonstrate the performance of PaLM-E on challenging and diverse mobile manipulation tasks. We largely follow the setup in Ahn et al. (2022), where the robot needs to plan a sequence of navigation and manipulation actions based on an instruction by a human. For example, given the instruction "I spilled my drink, can you bring me something to clean it up?", the robot needs to plan a sequence containing "1. Find a sponge, 2. Pick up the sponge, 3. Bring it to the user, 4. Put down the sponge." Inspired by these tasks, we develop 3 use cases to test the embodied reasoning abilities of PaLM-E: affordance prediction, failure detection, and long-horizon planning. The low-level policies are from RT-1 ), a transformer model that takes RGB image and natural language instruction, and outputs end-effector control commands.  Affordance prediction. We investigate PaLM-E's performance at affordance prediction, i.e. whether a skill of the low-level policy can be executed in the current environment. This can be formulated as the VQA problem Given <img>.</p>
<p>Q: Is it possible to <skill> here?.</p>
<p>PaLM-E outperforms PaLI (zero-shot), as well as thresholding on value functions trained with QT-OPT (Tab. 4).</p>
<p>Failure detection. For a robot to do closed-loop planning, it is also important to detect failures, as is shown in (Huang et al., 2022c). The multi-modal prompt is Given <img>. Q: Was <skill> successful?. Tab. 4 shows that PaLM-E outperforms PaLI (zero-shot), as well as a finetuned version of CLIP on this dataset. PaLM-E also outperforms the algorithm proposed in Xiao et al. (2022) that leverages two CLIP models trained with hindsight relabeled data. This method has access to more information than our method, and was specifically designed to just solve failure detection on this dataset.</p>
<p>Real robot results: Long-horizon planning. Finally, we use PaLM-E to perform embodied planning end-to-end for mobile manipulation tasks. The prompt structure for this task is Human: <instruction> Robot: <step history>. I see <img>. PaLM-E is trained to generate the next step of the plan, conditioned on the history of taken steps and the current image observation of the scene. After each step is decoded, we map them to a low-level policy as defined in Ahn et al. (2022). This process is done in an autoregressive manner, until PaLM-E outputs "terminate". We train the model by using the runs from (Ahn et al., 2022), which contains 2912 sequences. We qualitatively evaluated the model in a real kitchen and found the model can carry out long-horizon mobile manipulation tasks, even under adversarial disturbances (Fig. 5).</p>
<p>Performance on General Visual-Language Tasks</p>
<p>Although it is not the focus of our work, we report in Tab. 5 results on general vision-language tasks, including OK-VQA (Marino et al., 2019), VQA v2 (Goyal et al., 2017) and COCO captioning (Chen et al., 2015). A single, generalist Zero-shot Baselines Task 1  Task 2  Task 3 SayCan (oracle afford.) (Ahn et al., 2022) 0.0 --PaLI  0.0 --   </p>
<p>Baselines</p>
<p>Failure det. Affordance PaLI (Zero-shot)  0.73 0.62 CLIP-FT (Xiao et al., 2022) 0.65 -CLIP-FT-hindsight (Xiao et al., 2022) 0.89 -QT-OPT (Kalashnikov et al., 2018 Table 5: Results on general visual-language tasks. For the generalist models, they are the same checkpoint across the different evaluations, while task-specific finetuned models use differentfinetuned models for the different tasks. COCO uses Karpathy splits. † is 32-shot on OK-VQA (not finetuned).</p>
<p>PaLM-E-562B model achieves the highest reported number on OK-VQA, including outperforming models finetuned specifically on OK-VQA. Compared to (Tsimpoukelli et al., 2021), PaLM-E achieves the highest performance on VQA v2 with a frozen LLM to the best of our knowledge. This establishes that PaLM-E is a competitive visual-language generalist, in addition to being an embodied reasoner on robotic tasks.</p>
<p>Performance on General Language Tasks</p>
<p>Tab. 8 reports the averaged performance of PaLM-E on 21 general language benchmarks for Natural Language Understanding (NLU) and Natural Language Generation (NLG) tasks. The notable trend is that with increasing model scale, there is considerably less catastrophic forgetting of language capabilities. As seen in Fig. 6 </p>
<p>Summary of Experiments &amp; Discussion</p>
<p>Generalist vs specialist models -transfer. As summarized in Fig. 3, we have shown several instances of transfer in this work, meaning that PaLM-E trained on different tasks and datasets at the same time leads to significantly increased performance relative to models trained separately on the different tasks alone. In Fig. 4, co-training on the "full mixture" achieves more than double the performance. In Tab. 9, we see significant improvements in performance if we add LLM/ViT pre-training, and training on the full mixture instead of the mobile manipulation data alone. For the Language- Table experiment in Tab. 2, we observe analogous behaviour.</p>
<p>Data efficiency. Compared to available massive language or vision-language datasets, robotics data is significantly less abundant. As discussed in the last paragraph, our model exhibits transfer, which aids PaLM-E to solve robotics tasks from very few training examples in the robotics domain, e.g. between 10 and 80 for Language Table or 320 for TAMP. The OSRT results show another instance of data-efficiency by using a geometric input representation. A promising opportunity for future work is to combine this with a method benefitting from large-scale visual data.</p>
<p>Retaining language capabilities. We have shown two paths to retain the language capabilities of the model during multimodal training. As one option, freezing the LLM and only training the input encoders is a viable path for building embodied language models, although this approach occasionally struggled for robotics tasks (Tab. 2). As an alternative route, when the whole model is trained end-to-end, the model retains significantly more of its original language performance with increasing model scale (Fig. 6).</p>
<p>Conclusion</p>
<p>We proposed to build an embodied language model by injecting multi-modal information such as images into the embedding space of a pre-trained LLM. Experiments showed that off-the-shelf state-of-the-art vision-language models trained on general VQA and captioning tasks are not sufficient for embodied reasoning tasks, as well as limitations of a recent proposal for grounding language models through affordances. To overcome these limitations, we proposed PaLM-E, a single model that is able to control different robots in simulation and in the real world, while at the same time being quantitatively competent at general VQA and captioning tasks. In particular the novel architectural idea of ingesting neural scene representations (i.e., OSRT) into the model is particularly effective, even without large-scale data.</p>
<p>PaLM-E is trained on a mixture of diverse tasks across multiple robot embodiments as well as general vision-language tasks. Importantly, we have demonstrated that this diverse training leads to several avenues of transfer from the visionlanguage domains into embodied decision making, enabling robot planning tasks to be achieved data efficiently. While our results indicate that frozen language models are a viable path towards general-purpose embodied multimodal models that fully retain their language capabilities, we have also surfaced an alternative route with unfrozen models: scaling up the language model size leads to significantly less catastrophic forgetting while becoming an embodied agent. Our largest model, PaLM-E-562B, showcases emergent capabilities like multimodal chain of thought reasoning, and the ability to reason over multiple images, despite being trained on only single-image prompts.  Table, while remaining robust to adversarial disturbances. We find evidence that PaLM-E is capable of one-shot and zero shot generalization.</p>
<p>A. Data Mixture</p>
<p>Tab. 6 shows the dataset and sampling frequency for the "full mixture" as referred to in the experiments. The majority of the data distribution is general vision-language tasks, with less than 10% robot data.</p>
<p>B. Environment Details</p>
<p>B.1. Task and Motion Planning (TAMP)</p>
<p>The training scenes for the TAMP environment contain 3-5 cube-shaped objects of different sizes, colors and sampled initial poses. Fig. 8 show an example test scene that contains 6 objects.</p>
<p>In the global version, we consider the following three VQA tasks:</p>
<p>Dataset in full mixture</p>
<p>Sampling frequency % Webli  100 52.4 VQ 2 A  25 13.1 VQG  10 5.2 CC3M (Sharma et al., 2018) 25 13.1 Object Aware  10 5.2 OKVQA (Marino et al., 2019) 1 0.5 VQAv2 (Goyal et al., 2017) 1 0.5 COCO (Chen et al., 2015) 1 0.5 Wikipedia text 1 0.5 (robot) Mobile Manipulator, real 6 3.1 (robot) Language Table , sim and real 8 4.2 (robot) TAMP, sim 3 1.6 • q 3 : object-object relations. Example prompt: Given <img>. Q: Is the yellow object below the blue object?. Target: A: No, the yellow object is not below the blue object.</p>
<p>• q 4 : plan feasibility. Example prompt: Given <img>. Q: Is it possible to first grasp the blue object, then place it on the yellow object, and then grasp the yellow object?. Target: A: No, this is not possible.</p>
<p>as well as the two planning tasks • p 1 : grasping. Example prompt: Given <img>. Q: How to grasp the green object?. Target: A: First grasp the orange object and place it on the table, then grasp the green object.</p>
<p>• p 2 : stacking. Example prompt: Given <img>. Q: How to stack the white object on top of the red object?. Target: A: First grasp the green object and place it on the table, then grasp the white object and place it on the red object.</p>
<p>For the object-centric version with entity referrals, all prompts contain the prefix <prefix> = Obj 1 is <obj 1 >. . . . Obj j is <obj j >., and the VQA task q 1 is about the color of an object. The other tasks (except with the different prefix, and entity referrals), remain the same.</p>
<p>We utilize the planner from Driess et al. (2020) to generate the dataset for the planning tasks. The low-level policies are also obtained with the method of Driess et al. (2020).  </p>
<p>B.2. Interactive Language Table</p>
<p>We use the Language-Table real-world tabletop setup and simulated environment from Interactive Language .</p>
<p>Data collection. For each task, given the long horizon instruction, we prompt a labeler to enter a short horizon command every 4 seconds. We pass the short horizon instructions to an Interactive Language policy trained using the same procedure as in Lynch et al. (2022). The policy executes 40 steps (10Hz for 4 seconds) before requiring another command from the labeler. This is repeated until the labeler determines the long horizon instruction is complete and issues a 'done' instruction. The data collection procedure for the real world experiments are the same as in simulation.</p>
<p>Train and Evaluation. To train the finetuned versions of these models, we train a pretrained PaLM-E model for 9,000 additional steps, in order to support a data complexity sweep without training several separate models from scratch on slightly different versions of the full mixture. For Tasks 2 and 3 in simulation, we implement an automated reward to measure the success rate, and we evaluate PaLM-E by running 80 rollouts for each task. Given the current image and high level task, PaLM-E issues a text instruction which a trained low-level policy executes for 4 seconds before PaLM-E issues a new text instruction. For Task 1, we use a test-set and report validation accuracy. This is because the task only requires one step to solve, despite being a complicated visual and linguistic processing task and cannot be solved by the low-level policy from the prompt alone.  </p>
<p>C. Natural Language Generation and Understanding Results</p>
<p>D. Additional Data for Affordance and Success Detection</p>
<p>Model Precision Recall F1-score PaLI (Zero-shot)  0.59 0.98 0.73 CLIP-FT (Xiao et al., 2022) 0.50 0.95 0.65 CLIP-FT-hindsight (Xiao et al., 2022) 1   </p>
<p>E. Image Attribution</p>
<p>The image of the New York Knicks and Boston Celtics in Figure 2 is under the terms CC-by-2.0 (https:// creativecommons.org/licenses/by/2.0/), and was posted to Flickr by kowarski at https://www.flickr. com/photos/27728232@N00/8666371367. The egocentric video images are from https://youtu.be/ -UXKmqBPk1w, as in , via permission from creator Cody Wanner.</p>
<p>Figure 3 :
3Overview of transfer learning demonstrated by PaLM-E: across three different robotics domains, using PaLM and ViT pretraining together with the full mixture of robotics and general visual-language data provides a significant performance increase compared to only training on the respective in-domain data. See Tab. 1,Fig. 4, Tab. 2, Tab. 4 for additional data in each domain.</p>
<p>Figure 5 :
5A single PaLM-E model directs the low-level policies of two real robots. Shown is a long-horizon mobile manipulation task in a kitchen, and one-shot / zero-shot generalization with a tabletop manipulation robot.</p>
<p>Figure 7 :
7PaLM-E interactively guides a real robot through long-horizon manipulation tasks on Language-</p>
<p>Photo 1, at 10:30 am: <img1>. Photo 2, at 12:45 pm: <img2>. Photo 3, at 3:45 pm: <img3>. Q: I forget, what did I have for lunch, and what time was it? A: Let's think step by step. 1. you had a sandwich for lunch. 2. it was 12:45 pm.</p>
<p>Table Data
DataOnly 
SayCan Data Only 
Full Mixture 
(All robots + WebLI, 
VQA, COCO, etc.) </p>
<p>0% </p>
<p>25% </p>
<p>50% </p>
<p>75% </p>
<p>100% </p>
<p>PaLM-E 
Training 
Data 
Success Rate or Accuracy </p>
<p>TAMP Success (Table 1) 
Language-Table Success (Table 2) </p>
<p>SayCan Affordances (Table 4) </p>
<p>Different models for different robots, 
trained from scratch </p>
<p>One model for all robots 
with ViT + PaLM pre-training </p>
<p>Table Data
DataOnly 
SayCan Data Only 
Full Mixture 
(All robots + WebLI, 
VQA, COCO, etc.) </p>
<p>0% </p>
<p>25% </p>
<p>50% </p>
<p>75% </p>
<p>100% </p>
<p>PaLM-E Training Data </p>
<p>Success Rate or Accuracy </p>
<p>LLM finetune (full mixture) </p>
<p>LLM finetune (single robot) </p>
<p>without pretraining </p>
<p>LLM frozen (full mixture) </p>
<p>LLM frozen (single robot) </p>
<p>20% 
40% 
60% 
80% 
100% </p>
<p>94.9% </p>
<p>48.6% </p>
<p>42.9% </p>
<p>74.3% </p>
<p>31.8% </p>
<p>Figure 4: Planning success results in the TAMP environment 
(1% data) for PaLM-E-12B, comparing of the effects of PaLM-E 
models (i) using the full training mixture, (ii) pre-training (ViT 
and PaLM), and (iii) freezing or finetuning the language model. 
Transfer from full mixture is particularly effective. Note that full 
mixture contains only 1% of the training data (320 examples each) 
for the tasks evaluated here. Shown is the mean of tasks p 1 , p 2 . </p>
<p>Table 1 :
1Comparison of different input representations on TAMP 
environment (in terms of success rates), where data from TAMP 
constitutes only 1% (i.e., 320 samples for p 1 , p 2 each) of total 
training data size. PaLM-E outperforms both PaLI and SayCan 
on embodied VQA and planning tasks. Cross-domain transfer 
is observed, since the PaLM-E with ViT-4B trained on our full 
data mixture improves planning performance. OSRT, despite using 
no large-scale data, provides the most effective input encodings 
for learning. (GT) means ground-truth object-centric information 
provided. In all experiments, the LLM is frozen. The non-object 
centric ViT-4B variant utilizes color to reference objects, hence q 1 
cannot be evaluated here. The LLM is frozen in these experiments 
(except for the case where it is not pre-trained). Sec. B.1 describes 
the tasks q 1 -q 4 , p 1 , q 2 . </p>
<p>Table 2 :
2Results on planning tasks in the simulated environment fromLynch et al. (2022).Task 1. Q: There is a block that is closest to 
{i.e., top right corner}. Push that block to 
the other block of the same color. </p>
<p>Task 2. Q: How to sort the blocks by colors 
into corners? </p>
<p>Task 3. Q: How to push all the blocks that 
are on the {left/right} side together, 
without bringing over any of the blocks 
that are on the {right/left} side? </p>
<p>Table 3 :
3Task prompts for Tab. 2.</p>
<p>Table 4 :
4Mobile manipulation environment: failure detection and 
affordance prediction (F1 score). </p>
<p>VQAv2 
OK-VQA 
COCO 
Model 
test-dev test-std 
val 
Karpathy test 
Generalist (one model) 
PaLM-E-12B 
76.2 
-
55.5 
135.0 
PaLM-E-562B 
80.0 
-
66.1 
138.7 
Task-specific finetuned models 
Flamingo (Alayrac et al., 2022) 82.0 
82.1 
57.8 † 
138.1 
PaLI (Chen et al., 2022) 
84.3 
84.3 
64.5 
149.1 
PaLM-E-12B 
77.7 
77.9 
60.1 
136.0 
PaLM-E-66B 
-
-
62.9 
-
PaLM-E-84B 
80.5 
-
63.3 
138.0 
Generalist (one model), with frozen LLM 
(Tsimpoukelli et al., 2021) 
48.4 
-
-
-
PaLM-E-12B frozen 
70.3 
-
51.5 
128.0 </p>
<p>Table 6 :
6Dataset sampling frequency and ratio for the "full mixture" referred to in experiments.Figure 8: Two TAMP environment test examples. Left with 6 objects (training data contains 3-5 objects), right with 4 objects. • q 2 : object-table relation. Example prompt: Given <img>. Q: Is the red object left, right, or center of the table?. Target: A: The red object is in the center of the table.</p>
<p>Table 7 :
7Success rates on TAMP environment for different input representations. 3-5 objects in the scene correspond to the training distribution. OOD tasks means out-of-distribution tasks where the objects are referenced by color, although in the trainig data they have been referenced by their special tokens objj in the object-centric case. The SayCan baseline(Ahn et al., 2022) utilizes oracle, one-step affordance functions.</p>
<p>Table 8 :
8Full language evaluation task results on both NLU and NLG tasks, for both the original PaLM models and for associated PaLM-E (unfrozen) models. The PaLM-E models with a frozen LLM have the same performance as their corresponding underlying PaLM models.</p>
<p>Table 9 :
9Mobile manipulation environment: failure detection, showing individual precision and recall scores.Model 
Precision 
Recall 
F1-score 
PaLI (Zero-shot) (Chen et al., 2022) 
0.57 
0.69 
0.62 
QT-OPT (Kalashnikov et al., 2018) 
0.60 
0.67 
0.63 
PaLM-E-12B 
from 
LLM+ViT 
LLM 
trained on 
scratch 
pretrain 
frozen </p>
<p>Single robot </p>
<p>n/a 
0.67 
0.35 
0.46 
Single robot </p>
<p>0.90 
0.69 
0.78 
Full mixture </p>
<p>0.95 
0.80 
0.87 
Full mixture </p>
<p>0.92 
0.88 
0.91 </p>
<p>Table 10 :
10Mobile manipulation environment: affordance prediction, showing individual precision and recall scores.
AcknowledgementsThe authors would like to thank, for their advice, help and support: Xi Chen, Etienne Pot, Sebastian Goodman, Maria Attarian, Ted Xiao, Keerthana Gopalakrishnan, Kehang Han, Henryk Michalewski, Neil Houlsby, Basil Mustafa, Justin Gilmer, Yonghui Wu, Erica Moreira, Victor Gomes, Tom Duerig, Henning Meyer, and Kendra Byrne.
Do as i can, not as i say. M Ahn, A Brohan, N Brown, Y Chebotar, O Cortes, B David, C Finn, K Gopalakrishnan, K Hausman, A Herzog, arXiv:2204.01691Grounding language in robotic affordances. arXiv preprintAhn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al. Do as i can, not as i say: Ground- ing language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.</p>
<p>Flamingo: a visual language model for few-shot learning. J.-B Alayrac, J Donahue, P Luc, A Miech, I Barr, Y Hasson, K Lenc, A Mensch, K Millican, M Reynolds, arXiv:2204.14198arXiv preprintAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et al. Flamingo: a visual language model for few-shot learning. arXiv preprint arXiv:2204.14198, 2022.</p>
<p>R Bommasani, D A Hudson, E Adeli, R Altman, S Arora, S Von Arx, M S Bernstein, J Bohg, A Bosselut, E Brunskill, arXiv:2108.07258On the opportunities and risks of foundation models. arXiv preprintBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosse- lut, A., Brunskill, E., et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.</p>
<p>Rt-1: Robotics transformer for real-world control at scale. A Brohan, N Brown, J Carbajal, Y Chebotar, J Dabis, C Finn, K Gopalakrishnan, K Hausman, A Herzog, J Hsu, arXiv:2212.06817arXiv preprintBrohan, A., Brown, N., Carbajal, J., Chebotar, Y., Dabis, J., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., Hsu, J., et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022.</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 33Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural information processing systems, 33: 1877-1901, 2020.</p>
<p>All you may need for vqa are image captions. S Changpinyo, D Kukliansky, I Szpektor, X Chen, N Ding, R Soricut, Changpinyo, S., Kukliansky, D., Szpektor, I., Chen, X., Ding, N., and Soricut, R. All you may need for vqa are image captions, 2022. URL https://arxiv.org/ abs/2205.01883.</p>
<p>Evaluating large language models trained on code. M Chen, J Tworek, H Jun, Q Yuan, H P D O Pinto, J Kaplan, H Edwards, Y Burda, N Joseph, G Brockman, arXiv:2107.03374arXiv preprintChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a.</p>
<p>Pix2seq: A language modeling framework for object detection. T Chen, S Saxena, L Li, D J Fleet, G Hinton, arXiv:2109.10852arXiv preprintChen, T., Saxena, S., Li, L., Fleet, D. J., and Hinton, G. Pix2seq: A language modeling framework for object detection. arXiv preprint arXiv:2109.10852, 2021b.</p>
<p>Microsoft COCO captions: Data collection and evaluation server. X Chen, H Fang, T Lin, R Vedantam, S Gupta, P Dollár, C L Zitnick, abs/1504.00325CoRRChen, X., Fang, H., Lin, T., Vedantam, R., Gupta, S., Dollár, P., and Zitnick, C. L. Microsoft COCO captions: Data collection and evaluation server. CoRR, abs/1504.00325, 2015.</p>
<p>X Chen, X Wang, S Changpinyo, A Piergiovanni, P Padlewski, D Salz, S Goodman, A Grycner, B Mustafa, L Beyer, arXiv:2209.06794A jointly-scaled multilingual language-image model. arXiv preprintChen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A., Mustafa, B., Beyer, L., et al. Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022.</p>
<p>A Chowdhery, S Narang, J Devlin, M Bosma, G Mishra, A Roberts, P Barham, H W Chung, C Sutton, S Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>
<p>Scaling vision transformers to 22 billion parameters. M Dehghani, J Djolonga, B Mustafa, P Padlewski, J Heek, J Gilmer, A Steiner, M Caron, R Geirhos, I Alabdulmohsin, arXiv:2302.05442arXiv preprintDehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R., Alabdulmohsin, I., et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023.</p>
<p>J Devlin, M.-W Chang, K Lee, K Toutanova, Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. arXiv preprint arXiv:1810.04805, 2018.</p>
<p>A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.11929An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprintDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.</p>
<p>Deep visual reasoning: Learning to predict action sequences for task and motion planning from an initial scene image. D Driess, J.-S Ha, M Toussaint, Proc. of Robotics: Science and Systems (R:SS). of Robotics: Science and Systems (R:SS)Driess, D., Ha, J.-S., and Toussaint, M. Deep visual rea- soning: Learning to predict action sequences for task and motion planning from an initial scene image. In Proc. of Robotics: Science and Systems (R:SS), 2020.</p>
<p>Vision-language pre-training: Basics, recent advances, and future trends. Z Gan, L Li, C Li, L Wang, Z Liu, J Gao, Foundations and Trends® in Computer Graphics and Vision. 143-4Gan, Z., Li, L., Li, C., Wang, L., Liu, Z., Gao, J., et al. Vision-language pre-training: Basics, recent advances, and future trends. Foundations and Trends® in Computer Graphics and Vision, 14(3-4):163-352, 2022.</p>
<p>Improving alignment of dialogue agents via targeted human judgements. A Glaese, N Mcaleese, M Trebacz, J Aslanides, V Firoiu, T Ewalds, M Rauh, L Weidinger, M Chadwick, P Thacker, arXiv:2209.14375arXiv preprintGlaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., Chadwick, M., Thacker, P., et al. Improving alignment of dialogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022.</p>
<p>Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. Y Goyal, T Khot, D Summers-Stay, D Batra, D Parikh, Conference on Computer Vision and Pattern Recognition (CVPR. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D. Making the V in VQA matter: Elevating the role of image understanding in Visual Question An- swering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.</p>
<p>Instruction-driven history-aware policies for robotic manipulations. P.-L Guhur, S Chen, R Garcia, M Tapaswi, I Laptev, C Schmid, arXiv:2209.04899arXiv preprintGuhur, P.-L., Chen, S., Garcia, R., Tapaswi, M., Laptev, I., and Schmid, C. Instruction-driven history-aware policies for robotic manipulations. arXiv preprint arXiv:2209.04899, 2022.</p>
<p>Language models are general-purpose interfaces. Y Hao, H Song, L Dong, S Huang, Z Chi, W Wang, S Ma, Wei , F , arXiv:2206.06336arXiv preprintHao, Y., Song, H., Dong, L., Huang, S., Chi, Z., Wang, W., Ma, S., and Wei, F. Language models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022.</p>
<p>Scaling up vision-language pre-training for image captioning. X Hu, Z Gan, J Wang, Z Yang, Z Liu, Y Lu, Wang , L , Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionHu, X., Gan, Z., Wang, J., Yang, Z., Liu, Z., Lu, Y., and Wang, L. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pp. 17980-17989, 2022.</p>
<p>Visual language maps for robot navigation. C Huang, O Mees, A Zeng, W Burgard, arXiv:2210.05714arXiv preprintHuang, C., Mees, O., Zeng, A., and Burgard, W. Vi- sual language maps for robot navigation. arXiv preprint arXiv:2210.05714, 2022a.</p>
<p>Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. W Huang, P Abbeel, D Pathak, I Mordatch, arXiv:2201.07207arXiv preprintHuang, W., Abbeel, P., Pathak, D., and Mordatch, I. Lan- guage models as zero-shot planners: Extracting action- able knowledge for embodied agents. arXiv preprint arXiv:2201.07207, 2022b.</p>
<p>W Huang, F Xia, T Xiao, H Chan, J Liang, P Florence, A Zeng, J Tompson, I Mordatch, Y Chebotar, arXiv:2207.05608Inner monologue: Embodied reasoning through planning with language models. arXiv preprintHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608, 2022c.</p>
<p>Bc-z: Zero-shot task generalization with robotic imitation learning. E Jang, A Irpan, M Khansari, D Kappler, F Ebert, C Lynch, S Levine, C Finn, Conference on Robot Learning. PMLRJang, E., Irpan, A., Khansari, M., Kappler, D., Ebert, F., Lynch, C., Levine, S., and Finn, C. Bc-z: Zero-shot task generalization with robotic imitation learning. In Conference on Robot Learning, pp. 991-1002. PMLR, 2022.</p>
<p>Y Jiang, A Gupta, Z Zhang, G Wang, Y Dou, Y Chen, L Fei-Fei, A Anandkumar, Y Zhu, L Fan, Vima, arXiv:2210.03094General robot manipulation with multimodal prompts. arXiv preprintJiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L., Anandkumar, A., Zhu, Y., and Fan, L. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022.</p>
<p>Scalable deep reinforcement learning for vision-based robotic manipulation. D Kalashnikov, A Irpan, P Pastor, J Ibarz, A Herzog, E Jang, D Quillen, E Holly, M Kalakrishnan, V Vanhoucke, Conference on Robot Learning. PMLRKalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Van- houcke, V., et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning, pp. 651-673. PMLR, 2018.</p>
<p>Large language models are zero-shot reasoners. T Kojima, S S Gu, M Reid, Y Matsuo, Y Iwasawa, arXiv:2205.11916arXiv preprintKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>
<p>The power of scale for parameter-efficient prompt tuning. B Lester, R Al-Rfou, N Constant, arXiv:2104.08691arXiv preprintLester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021.</p>
<p>Solving quantitative reasoning problems with language models. A Lewkowycz, A Andreassen, D Dohan, E Dyer, H Michalewski, V Ramasesh, A Slone, C Anil, I Schlag, T Gutman-Solo, arXiv:2206.14858arXiv preprintLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A., Anil, C., Schlag, I., Gutman-Solo, T., et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.</p>
<p>L H Li, M Yatskar, D Yin, C.-J Hsieh, Chang , K.-W , arXiv:1908.03557Visualbert: A simple and performant baseline for vision and language. arXiv preprintLi, L. H., Yatskar, M., Yin, D., Hsieh, C.-J., and Chang, K.-W. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.</p>
<p>M Li, T Lv, J Chen, L Cui, Y Lu, D Florencio, C Zhang, Z Li, F Wei, Trocr, arXiv:2109.10282Transformer-based optical character recognition with pre-trained models. arXiv preprintLi, M., Lv, T., Chen, J., Cui, L., Lu, Y., Florencio, D., Zhang, C., Li, Z., and Wei, F. Trocr: Transformer-based optical character recognition with pre-trained models. arXiv preprint arXiv:2109.10282, 2021.</p>
<p>Pre-trained language models for interactive decision-making. S Li, X Puig, Y Du, C Wang, E Akyurek, A Torralba, J Andreas, I Mordatch, arXiv:2202.01771arXiv preprintLi, S., Puig, X., Du, Y., Wang, C., Akyurek, E., Torralba, A., Andreas, J., and Mordatch, I. Pre-trained language models for interactive decision-making. arXiv preprint arXiv:2202.01771, 2022.</p>
<p>PaLM-E: An Embodied Multimodal Language Model. PaLM-E: An Embodied Multimodal Language Model</p>
<p>J Liang, W Huang, F Xia, P Xu, K Hausman, B Ichter, P Florence, A Zeng, arXiv:2209.07753Code as policies: Language model programs for embodied control. arXiv preprintLiang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P., and Zeng, A. Code as policies: Language model programs for embodied control. arXiv preprint arXiv:2209.07753, 2022.</p>
<p>Object-centric learning with slot attention. F Locatello, D Weissenborn, T Unterthiner, A Mahendran, G Heigold, J Uszkoreit, A Dosovitskiy, T Kipf, Advances in Neural Information Processing Systems. 33Locatello, F., Weissenborn, D., Unterthiner, T., Mahendran, A., Heigold, G., Uszkoreit, J., Dosovitskiy, A., and Kipf, T. Object-centric learning with slot attention. Advances in Neural Information Processing Systems, 33:11525- 11538, 2020.</p>
<p>Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Vilbert, Advances in neural information processing systems. 32Lu, J., Batra, D., Parikh, D., and Lee, S. Vilbert: Pre- training task-agnostic visiolinguistic representations for vision-and-language tasks. Advances in neural informa- tion processing systems, 32, 2019.</p>
<p>Pretrained transformers as universal computation engines. K Lu, A Grover, P Abbeel, I Mordatch, arXiv:2103.0524712021arXiv preprintLu, K., Grover, A., Abbeel, P., and Mordatch, I. Pretrained transformers as universal computation engines. arXiv preprint arXiv:2103.05247, 1, 2021.</p>
<p>Language conditioned imitation learning over unstructured data. C Lynch, P Sermanet, arXiv:2005.07648arXiv preprintLynch, C. and Sermanet, P. Language conditioned imi- tation learning over unstructured data. arXiv preprint arXiv:2005.07648, 2020.</p>
<p>C Lynch, A Wahid, J Tompson, T Ding, J Betker, R Baruch, T Armstrong, Florence , P , arXiv:2210.06407Interactive language: Talking to robots in real time. arXiv preprintLynch, C., Wahid, A., Tompson, J., Ding, T., Betker, J., Baruch, R., Armstrong, T., and Florence, P. Interactive language: Talking to robots in real time. arXiv preprint arXiv:2210.06407, 2022.</p>
<p>Okvqa: A visual question answering benchmark requiring external knowledge. K Marino, M Rastegari, A Farhadi, R Mottaghi, Conference on Computer Vision and Pattern Recognition (CVPR). Marino, K., Rastegari, M., Farhadi, A., and Mottaghi, R. Ok- vqa: A visual question answering benchmark requiring external knowledge. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.</p>
<p>Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. S Nair, E Mitchell, K Chen, S Savarese, C Finn, Conference on Robot Learning. PMLRNair, S., Mitchell, E., Chen, K., Savarese, S., Finn, C., et al. Learning language-conditioned robot behavior from offline data and crowd-sourced annotation. In Conference on Robot Learning, pp. 1303-1315. PMLR, 2022.</p>
<p>Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. K Nottingham, P Ammanabrolu, A Suhr, Y Choi, H Hajishirzi, S Singh, R Fox, arXiv:2301.12050arXiv preprintNottingham, K., Ammanabrolu, P., Suhr, A., Choi, Y., Ha- jishirzi, H., Singh, S., and Fox, R. Do embodied agents dream of pixelated sheep?: Embodied decision making using language guided world modelling. arXiv preprint arXiv:2301.12050, 2023.</p>
<p>Pre-training image-language transformers for open-vocabulary tasks. A Piergiovanni, W Kuo, Angelova , A , Piergiovanni, A., Kuo, W., and Angelova, A. Pre-training image-language transformers for open-vocabulary tasks, 2022. URL https://arxiv.org/abs/2209.</p>
<p>S Polu, J M Han, K Zheng, M Baksys, I Babuschkin, I Sutskever, arXiv:2202.01344Formal mathematics statement curriculum learning. arXiv preprintPolu, S., Han, J. M., Zheng, K., Baksys, M., Babuschkin, I., and Sutskever, I. Formal mathematics statement curricu- lum learning. arXiv preprint arXiv:2202.01344, 2022.</p>
<p>. S Reed, K Zolna, E Parisotto, S G Colmenarejo, A Novikov, G Barth-Maron, M Gimenez, Y Sulsky, J Kay, J T Springenberg, arXiv:2205.06175A generalist agent. arXiv preprintReed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. A generalist agent. arXiv preprint arXiv:2205.06175, 2022.</p>
<p>M S Ryoo, A Piergiovanni, A Arnab, M Dehghani, A Angelova, Tokenlearner, arXiv:2106.11297What can 8 learned tokens do for images and videos. arXiv preprintRyoo, M. S., Piergiovanni, A., Arnab, A., Dehghani, M., and Angelova, A. Tokenlearner: What can 8 learned tokens do for images and videos? arXiv preprint arXiv:2106.11297, 2021.</p>
<p>Object Scene Representation Transformer. M S M Sajjadi, D Duckworth, A Mahendran, S Van Steenkiste, F Pavetić, M Lučić, L J Guibas, K Greff, T Kipf, NeurIPS. Sajjadi, M. S. M., Duckworth, D., Mahendran, A., van Steenkiste, S., Pavetić, F., Lučić, M., Guibas, L. J., Greff, K., and Kipf, T. Object Scene Representa- tion Transformer. NeurIPS, 2022a. URL https: //osrt-paper.github.io/.</p>
<p>Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. M S M Sajjadi, H Meyer, E Pot, U Bergmann, K Greff, N Radwan, S Vora, M Lučić, D Duckworth, A Dosovitskiy, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSajjadi, M. S. M., Meyer, H., Pot, E., Bergmann, U., Greff, K., Radwan, N., Vora, S., Lučić, M., Duckworth, D., Dosovitskiy, A., et al. Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6229-6238, 2022b.</p>
<p>D Shah, B Osinski, B Ichter, S Levine, Lmnav, arXiv:2207.04429Robotic navigation with large pre-trained models of language, vision, and action. arXiv preprintShah, D., Osinski, B., Ichter, B., and Levine, S. Lm- nav: Robotic navigation with large pre-trained mod- els of language, vision, and action. arXiv preprint arXiv:2207.04429, 2022.</p>
<p>Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. P Sharma, N Ding, S Goodman, R Soricut, Proceedings of ACL. ACLSharma, P., Ding, N., Goodman, S., and Soricut, R. Con- ceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.</p>
<p>Skill induction and planning with latent language. P Sharma, A Torralba, Andreas , J , arXiv:2110.01517arXiv preprintSharma, P., Torralba, A., and Andreas, J. Skill induc- tion and planning with latent language. arXiv preprint arXiv:2110.01517, 2021.</p>
<p>Cliport: What and where pathways for robotic manipulation. M Shridhar, L Manuelli, D Fox, Conference on Robot Learning. PMLRShridhar, M., Manuelli, L., and Fox, D. Cliport: What and where pathways for robotic manipulation. In Conference on Robot Learning, pp. 894-906. PMLR, 2022a.</p>
<p>Perceiver-actor: A multi-task transformer for robotic manipulation. M Shridhar, L Manuelli, D Fox, arXiv:2209.05451arXiv preprintShridhar, M., Manuelli, L., and Fox, D. Perceiver-actor: A multi-task transformer for robotic manipulation. arXiv preprint arXiv:2209.05451, 2022b.</p>
<p>Lancon-learn: Learning with language to enable generalization in multi-task manipulation. A Silva, N Moorman, W Silva, Z Zaidi, N Gopalan, M Gombolay, IEEE Robotics and Automation Letters. 72Silva, A., Moorman, N., Silva, W., Zaidi, Z., Gopalan, N., and Gombolay, M. Lancon-learn: Learning with language to enable generalization in multi-task manipulation. IEEE Robotics and Automation Letters, 7(2):1635-1642, 2021.</p>
<p>I Singh, V Blukis, A Mousavian, A Goyal, D Xu, J Tremblay, D Fox, J Thomason, A Garg, arXiv:2209.11302Generating situated robot task plans using large language models. arXiv preprintSingh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A. Prog- Prompt: Generating situated robot task plans using large language models. arXiv preprint arXiv:2209.11302, 2022.</p>
<p>Robots that use language. S Tellex, N Gopalan, H Kress-Gazit, C Matuszek, Robotics, and Autonomous Systems. 3Annual Review of ControlTellex, S., Gopalan, N., Kress-Gazit, H., and Matuszek, C. Robots that use language. Annual Review of Control, Robotics, and Autonomous Systems, 3:25-55, 2020.</p>
<p>. R Thoppilan, D De Freitas, J Hall, N Shazeer, A Kulshreshtha, H.-T Cheng, A Jin, T Bos, L Baker, Thoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kul- shreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L.,</p>
<p>Y Du, arXiv:2201.08239Language models for dialog applications. arXiv preprintDu, Y., et al. Lamda: Language models for dialog appli- cations. arXiv preprint arXiv:2201.08239, 2022.</p>
<p>Multimodal few-shot learning with frozen language models. M Tsimpoukelli, J L Menick, S Cabi, S Eslami, O Vinyals, F Hill, Advances in Neural Information Processing Systems. 34Tsimpoukelli, M., Menick, J. L., Cabi, S., Eslami, S., Vinyals, O., and Hill, F. Multimodal few-shot learn- ing with frozen language models. Advances in Neural Information Processing Systems, 34:200-212, 2021.</p>
<p>Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. Z Wang, S Cai, A Liu, X Ma, Y Liang, arXiv:2302.01560arXiv preprintWang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. arXiv preprint arXiv:2302.01560, 2023.</p>
<p>Chain of thought prompting elicits reasoning in large language models. J Wei, X Wang, D Schuurmans, M Bosma, E Chi, Q Le, D Zhou, arXiv:2201.11903arXiv preprintWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elic- its reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>
<p>Robotic skill acquisition via instruction augmentation with visionlanguage models. T Xiao, H Chan, P Sermanet, A Wahid, A Brohan, K Hausman, S Levine, J Tompson, arXiv:2211.11736arXiv preprintXiao, T., Chan, H., Sermanet, P., Wahid, A., Brohan, A., Hausman, K., Levine, S., and Tompson, J. Robotic skill acquisition via instruction augmentation with vision- language models. arXiv preprint arXiv:2211.11736, 2022.</p>
<p>R Zellers, A Holtzman, M Peters, R Mottaghi, A Kembhavi, A Farhadi, Y Choi, Piglet, arXiv:2106.00188Language grounding through neuro-symbolic interaction in a 3d world. arXiv preprintZellers, R., Holtzman, A., Peters, M., Mottaghi, R., Kem- bhavi, A., Farhadi, A., and Choi, Y. Piglet: Language grounding through neuro-symbolic interaction in a 3d world. arXiv preprint arXiv:2106.00188, 2021a.</p>
<p>R Zellers, X Lu, J Hessel, Y Yu, J S Park, J Cao, A Farhadi, Y Choi, Merlot, Multimodal neural script knowledge models. Advances in Neural Information Processing Systems. 34Zellers, R., Lu, X., Hessel, J., Yu, Y., Park, J. S., Cao, J., Farhadi, A., and Choi, Y. Merlot: Multimodal neural script knowledge models. Advances in Neural Informa- tion Processing Systems, 34:23634-23651, 2021b.</p>
<p>Socratic models: Composing zeroshot multimodal reasoning with language. A Zeng, A Wong, S Welker, K Choromanski, F Tombari, A Purohit, M Ryoo, V Sindhwani, J Lee, V Vanhoucke, arXiv:2204.00598arXiv preprintZeng, A., Wong, A., Welker, S., Choromanski, K., Tombari, F., Purohit, A., Ryoo, M., Sindhwani, V., Lee, J., Van- houcke, V., et al. Socratic models: Composing zero- shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.</p>
<p>Hierarchical task learning from language instructions with unified transformers and selfmonitoring. Y Zhang, J Chai, arXiv:2106.03427arXiv preprintZhang, Y. and Chai, J. Hierarchical task learning from language instructions with unified transformers and self- monitoring. arXiv preprint arXiv:2106.03427, 2021.</p>
<p>Unified vision-language pre-training for image captioning and vqa. L Zhou, H Palangi, L Zhang, H Hu, J Corso, J Gao, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceZhou, L., Palangi, H., Zhang, L., Hu, H., Corso, J., and Gao, J. Unified vision-language pre-training for image caption- ing and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2445 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2445</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2445</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-67.html">extraction-schema-67</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <p><strong>Paper ID:</strong> paper-215787556</p>
                <p><strong>Paper Title:</strong> A Bayesian experimental autonomous researcher for mechanical design</p>
                <p><strong>Paper Abstract:</strong> Automated testing, Bayesian optimization, and additive manufacturing combine for the autonomous design of structures.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2445.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2445.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BEAR</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian experimental autonomous researcher</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autonomous experimental platform that couples an automated additive-manufacturing + robotic testing pipeline with Gaussian-process-based Bayesian optimization to select, execute, and learn from experiments without a human in the loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>BEAR (Bayesian experimental autonomous researcher)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An integrated hardware+software autonomous research system: five dual-extruder FDM printers and a six-axis robot arm perform 3D printing, retrieval, weighing and Instron mechanical tests; all instruments are coordinated by MATLAB. A Gaussian process regression (GPR) surrogate (squared-exponential kernel, hyperparameters optimized by MLE) models toughness U(x) as a function of design parameters x=(n,θ,r,t). A decision module selects subsequent experiments by evaluating acquisition strategies (expected improvement, maximum predictive variance, or random candidates) drawn from a finite, uniformly random candidate pool; selected designs are converted to STL/gcode and executed automatically. The system supports multi-agent parallel operation (multiple printers) with a shared belief model and batch selection policy.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental mechanics / materials and design optimization (additive manufacturing structural toughness optimization)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>A GPR-based Bayesian optimization loop selects next experiments from a uniformly random finite candidate set using acquisition functions: Expected Improvement (EI) for improvement-driven allocation and Maximum Variance (MV) to prioritize uncertainty reduction. For parallel (multi-printer) operation, agents draw from the shared model; practical protocol used here: an initial MV 'burn-in' (32 samples over ~12 hours) to explore and train the GP, then switch to EI to allocate remaining experiments in parallel.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Measured primarily as number of physical experiments and wall-clock experimental time (hours); GP fitting and acquisition evaluations are treated as lightweight and are not measured in FLOPs or monetary cost in the paper. Experimental budget is expressed as number of experiments (e.g., 32, 64, 100, 1800) and campaign duration in hours (e.g., 12, 24, 36 h).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP predictive variance (used by MV) and Expected Improvement (EI) computed from the GP posterior (used as acquisition function); information about uncertainty is encoded in the GP posterior variance and exploited by MV/EI.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit acquisition-policy driven balance: MV directly prioritizes exploration by selecting points of highest posterior variance; EI balances exploitation and exploration by selecting points with largest expected improvement over the incumbent best; an initial MV exploration phase (approx. 32 points / 12 h) is used to reduce model uncertainty before switching to EI to focus on high-potential designs. Additionally, acquisition selection is restricted to a random finite candidate pool to reduce EI greediness.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Multiple mechanisms to promote diversity: (1) pure random sampling / Pure Exploration (PE) for stochastic coverage in simulations; (2) MV acquisition which targets high-uncertainty (diverse) regions; (3) selecting next point from a uniformly random finite candidate set (stochastic candidate pool) to avoid greedy convergence; (4) parallel agents performing different selected experiments in batch.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget (number of experiments) and time budget (wall-clock campaign duration); resource parallelism (number of printers) is also an explicit constraint/value for speed.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The system enforces an experiment budget or time limit and optimizes acquisition selection under that constraint; in parallel campaigns it uses batch selection (initial MV burn-in then EI) to best use multiple printers over a given time horizon (explicit example: 12 h burn-in with MV (32 samples), then EI; reported evaluations at T=12, 24, 36 h).</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Toughness U (energy absorbed, J) measured experimentally; a normalized campaign performance P(i)=U_grid(x_i)/max(U_grid) (in simulations) is used to quantify how close the predicted optimum is to the true surrogate maximum. Expected Improvement (EI) serves as the acquisition proxy for breakthrough potential (probability/expectation of surpassing current best).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Reported numbers: toughness U range in grid campaign 0.3–51.5 J (mean 15.3 J); GP-predicted optimum from grid U_grid = 43.4 ± 6.0 J. Experimental campaign comparisons: BEAR found superior structures in five of six runs; BEAR found top structures in 100 experiments vs grid-based search of 1800 experiments. Efficiency/time metrics: BEAR matched grid campaign performance after 32 experiments (12 h) and outperformed grid after 64 experiments (24 h). Simulation metrics: in low-noise (σ=0.1 J) simulations, all policies median P ≥ ~85% in 100 experiments, EI ~P>98%; in high-noise (σ=5 J), EI achieved ~P>90% in 100 experiments while LHS achieved ~P≈75%. Measurement variability: toughness SD 12.8% of mean; manufacturing-uncorrelated toughness variation ~5.8% of mean.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Grid-based exhaustive search (1800 experiments), Latin hypercube sampling (LHS), Pure Exploration (random sampling), and simulated policies (MV and EI).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BEAR (EI-guided) achieved higher experimental performance than the best structure predicted by the grid model even though BEAR used ~100 experiments vs grid's 1800; simulations show EI outperforms LHS and PE (e.g., in high-noise EI ~P>90% vs LHS ~P≈75% after 100 experiments). Multi-agent BEAR matched grid performance after 32 experiments (12 h) and outperformed grid after 64 experiments (24 h).</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Almost 60-fold reduction in number of experiments needed to identify high-performing structures relative to the grid-based search (100 vs 1800 experiments reported); time-to-equivalent-performance reduced to tens of hours (matched in 12 h, outperformed in 24 h) using parallel printers.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>The paper analyzes tradeoffs qualitatively and in simulation: EI balances exploration/exploitation but can be too greedy with an uninformative prior; MV promotes exploration and uncertainty reduction but may require large budgets to find optima; PE (random) is robust but inefficient. Noise level affects policy performance (EI remains strong in moderate-high noise). The authors highlight model misspecification issues (homoscedastic GP vs observed heteroscedastic experimental noise) which affect uncertainty quantification and acquisition effectiveness. They recommend placing equal emphasis on uncertainty quantification and acquisition selection under limited budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Practical recommendations: (1) perform an initial exploration phase (they used ~30 samples / 12 h with MV) to train the GP and obtain reasonable uncertainty estimates before aggressive exploitation; (2) use EI for directed improvement after a burn-in; (3) use stochastic candidate pools for EI to mitigate greediness; (4) when parallel resources are available, apply batch selection informed by a shared belief model (multi-agent) to reduce wall-clock time; (5) uncertainty quantification should be considered an equal partner with exploitation when budgets are small or noise/model mismatch is present.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian experimental autonomous researcher for mechanical design', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2445.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2445.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BO (GPR)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Bayesian optimization with Gaussian process regression</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Bayesian optimization framework that uses a Gaussian process surrogate (squared-exponential kernel) to model experimental response and acquisition functions (EI, MV) to allocate experiments sequentially or in batches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian experimental autonomous researcher for mechanical design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Gaussian-process-based Bayesian optimization (BO)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A GPR surrogate with zero mean and squared-exponential covariance kernel models U(x); kernel hyperparameters (signal variance α and length scales β_i) and assumed homoscedastic noise σ are initialized and then optimized by maximum likelihood after each observation. Acquisition functions evaluated on a finite random candidate set guide experiment selection (EI for improvement, MV for uncertainty).</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Experimental design and optimization in materials, mechanics, and other low-data experimental sciences</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Acquisition-function-driven sequential allocation: select candidate with maximum acquisition value (EI or MV) from a random finite candidate pool; hyperparameters updated after each observation to refine allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of physical experiments and wall-clock time; GP fitting and acquisition evaluations treated as negligible for cost accounting in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Posterior variance (predictive uncertainty) and acquisition-derived expected improvement from GP posterior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Choice of acquisition function: MV = exploration (maximize posterior variance), EI = tradeoff of exploitation/exploration via expected improvement over incumbent.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Use of MV and random candidate pools to encourage coverage; explicit batch/parallel selection when multiple experimental resources are available.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget (number of experiments) and time budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>The BO loop is run until an experiment budget or time limit is reached; initial exploration samples are allocated to better estimate hyperparameters and noise prior to exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Expected improvement (EI) and direct measured performance U (toughness in J) used to quantify high-impact discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>In simulation: low-noise (σ=0.1 J) EI achieved ~P>98% in 100 experiments; high-noise (σ=5 J) EI achieved ~P>90% in 100 experiments. In experiment: BO (EI) campaigns found structures outperforming grid-derived best using far fewer experiments (100 vs 1800).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Grid search, Latin hypercube sampling, pure random sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>BO (EI) outperformed LHS and PE in simulations and outperformed grid-based experimental campaign in experiments when accounting for number of experiments/time.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>BO required far fewer experiments to approach optimum (e.g., ~100 vs 1800 grid experiments), and EI showed stronger convergence behavior in the simulations reported.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper reports sensitivity to noise and prior; EI can be greedy under uninformative priors and MV can require many samples; GP assumptions (homoscedastic noise) can degrade performance if violated.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use initial exploratory MV samples to calibrate the GP, then switch to EI for directed improvement; using a finite random candidate pool for EI improves convergence by reducing greedy selection.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian experimental autonomous researcher for mechanical design', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2445.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2445.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EI (stochastic candidate)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expected Improvement acquisition with stochastic candidate selection</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An improvement-based acquisition function (Expected Improvement) modified by this work to evaluate EI only over a finite uniformly random candidate set to reduce greedy behavior and improve convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian experimental autonomous researcher for mechanical design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expected Improvement (EI) with finite random candidate pool</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>EI computes the expected positive improvement over the current best result under the GP posterior; here EI is evaluated not over the continuous domain but over a finite, uniformly random set of candidate designs each acquisition step, which reduces excessive greediness and improves practical convergence.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active experimental selection for materials/mechanics experiments and other low-data experimental campaigns</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Select the candidate design (from the random candidate pool) with highest EI to allocate the next experiment; in multi-agent campaigns this is used after an initial MV exploration phase.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of experiments and wall-clock experimental time; acquisition evaluation cost limited by candidate pool size (computational overhead small relative to experiment cost).</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>Expected Improvement computed using the GP posterior predictive mean and variance.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>EI balances exploitation and exploration via expectation over the posterior distribution; stochastic candidate pool injects diversity and mitigates premature exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Finite random candidate pool (uniform sampling each acquisition) forces consideration of multiple diverse options; initial MV burn-in before EI further promotes diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget (number of experiments) and time-limited campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>In practice they performed MV-guided sampling for the initial budgeted exploration (32 samples / 12 h) then switched to EI given remaining budget/time to exploit promising regions.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>EI itself measures expected chance to exceed current best; experimental breakthrough measured by toughness U (J) and normalized P metric in simulations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simulated: EI achieved ~P>98% in low-noise (σ=0.1 J) and ~P>90% in high-noise (σ=5 J) within 100 experiments. Experimental: all three EI-guided physical campaigns produced optimum structures outperforming grid-derived best when tested.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>MV, PE, LHS, and grid search.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>EI converged faster and to higher performance than LHS and PE in simulation; in experiments EI-guided BEAR runs found better structures using far fewer experiments than grid.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>EI-based campaigns reached high P values with ~100 experiments (versus 1800 grid experiments); in multi-agent mode contributed to matching/outperforming grid within 12–24 hours.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>EI is powerful but can be too greedy with an uninformative prior and insufficient uncertainty estimates; stochastic candidate selection and initial MV burn-in help ameliorate this.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Evaluate EI on a randomized finite candidate set and perform an initial exploratory MV phase to calibrate the GP prior before exploiting with EI.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian experimental autonomous researcher for mechanical design', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2445.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2445.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MV</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Maximum variance acquisition (uncertainty sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An acquisition policy that selects the next experiment at the input location where the GP predictive variance is maximal, prioritizing reduction of model uncertainty (exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian experimental autonomous researcher for mechanical design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Maximum Variance (MV) acquisition</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>At each acquisition step, compute the GP posterior predictive variance across candidate designs and select the candidate with maximum variance; used both in simulations and as an initial exploration policy in multi-agent experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Active learning / experimental design in noisy low-data experimental domains (mechanics here)</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experimental runs to points of highest predictive uncertainty to maximize information gain about the response surface.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of experiments and experimental wall-clock time; MV may require many experiments to adequately explore the space.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP posterior predictive variance (proxy for expected information gain).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration; MV prioritizes exploration exclusively, potentially at the expense of immediate exploitation.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>By targeting highest-variance regions MV promotes coverage of uncertain/different hypotheses; used as burn-in to generate diverse data.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget and time limits.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Used either as stand-alone policy or for initial exploration (burn-in) to provide training data for the GP before switching to an exploitative acquisition like EI.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Indirect: by reducing uncertainty MV can enable later discovery via other acquisition functions; not directly aimed at immediate breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simulations: in low-noise (σ=0.1 J) MV along with other policies achieved median P ≳ 85% after 100 experiments; in benchmarks MV serves as an effective initial exploration policy (authors used ≈32 MV samples in practice).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>PE, EI, LHS, grid.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>MV outperforms pure random sampling in structured exploration and is helpful as a burn-in to improve EI performance; MV alone may be slower in identifying top optima than EI for the same budget.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>When used as an initial calibration phase (~30 samples) it improves downstream efficiency of EI exploitation, enabling faster convergence in wall-clock time for the multi-agent campaigns.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>MV favors information gain at the expense of immediate improvement; authors recommend combining MV burn-in with subsequent EI exploitation under tight budgets.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Use MV for early exploration to calibrate the GP and quantify uncertainty; around ~30 exploratory samples sufficed in their domain to saturate initial convergence in simulation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian experimental autonomous researcher for mechanical design', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2445.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2445.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PE</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pure exploration (random sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline decision policy that selects each subsequent experiment uniformly at random, ensuring broad coverage but without model-guided allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian experimental autonomous researcher for mechanical design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pure Exploration (PE) / random sampling</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Select the next experiment randomly from the design space (uninformed sampling). Used in simulations as a baseline to compare against BO-informed policies.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Baseline experimental allocation strategy for experimental design studies</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Uniform random selection across the domain for each experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of experiments; wall-clock time is dictated solely by the experiments performed.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>None explicit; any information gain arises passively from random coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration by definition (no exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Randomness ensures diverse sampling across the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed experimental budget/time.</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>No optimization under budget — used as a baseline allocation until budget exhausted.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Measured outcomes (toughness U) after sampling; PE has no directed metric for breakthroughs.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simulations: PE achieved similar performance to LHS (space-filling) in high-noise settings, but inferior to EI in terms of speed to high P.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against EI, MV, LHS, grid.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Inferior to EI; comparable to LHS in certain noisy regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>None inherent; provides baseline efficiency for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>PE is robust to model misspecification but inefficient when experiments are expensive; the paper uses PE to illustrate the value of BO-guided allocation.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Not recommended as an allocation strategy when experiments are costly; useful only as a baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian experimental autonomous researcher for mechanical design', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2445.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2445.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LHS</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Latin hypercube sampling</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A space-filling sampling scheme used as a baseline in simulations to compare against BO-driven sequential policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian experimental autonomous researcher for mechanical design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Latin hypercube sampling (LHS)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A stratified sampling method that generates evenly distributed samples in high-dimensional parameter space; used in simulated campaigns with the same experimental budget to benchmark BO policies.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Baseline experimental design / space-filling sampling in materials and experimental design studies</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Precompute a fixed set of space-filling samples (LHS) and perform experiments on this set.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Number of experiments and experimental wall-clock time; no online computation required.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>None explicit; aims to maximize coverage so information gain is distributed over the domain.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Pure exploration via space-filling design (no exploitation).</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Design ensures diverse coverage across dimensions.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Fixed number of experiments (preallocated).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Budget allocated a priori across a space-filling set of designs.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Measured discovery of high U values in the sampled set; no acquisition for improvement.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Simulations: in high-noise (σ=5 J) LHS achieved ~P≈75% after 100 experiments (inferior to EI, similar to PE).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against BO policies (EI/MV), PE, and grid.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Inferior to EI in simulations; similar to PE in noisy regimes.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>None relative to BO; serves as a simple benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>LHS provides robust coverage but lacks targeted acquisition; appropriate when model-driven selection is infeasible or when one wants to build a uniform surrogate.</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Authors use LHS as a baseline to show BO's advantage under limited budgets and noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian experimental autonomous researcher for mechanical design', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2445.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2445.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems or methods for automated scientific discovery, experimental design, or active learning that involve resource allocation decisions, balancing computational costs against information gain, breakthrough potential, and hypothesis diversity.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-agent / batch BO (BEAR parallel)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-agent batch Bayesian optimization (parallel BEAR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parallel/batch experimental allocation protocol where multiple experimental agents (printers) draw from a shared GP belief and execute experiments in parallel, with an initial MV exploration phase followed by EI exploitation to minimize wall-clock time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A Bayesian experimental autonomous researcher for mechanical design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-agent batch BO (parallel BEAR)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Six agents (printers) operate in concert, sharing a global GP model updated with all collected data. Because experiments are executed in parallel, a batch selection policy is required; the implemented protocol uses MV-guided sampling for an initial 12-hour (32-sample) burn-in across agents and then switches to EI for subsequent allocations. The paper references recent batch-EI methods (local penalization, Thompson sampling variants, asynchronous batch BO) but uses the described practical MV-then-EI protocol for experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>application_domain</strong></td>
                            <td>Parallelized autonomous experimentation for rapid materials/mechanics discovery and optimization</td>
                        </tr>
                        <tr>
                            <td><strong>resource_allocation_strategy</strong></td>
                            <td>Allocate experiments concurrently across available agents using a shared surrogate: initial parallel MV sampling to maximize variance coverage, then parallel EI allocations for improvement; acquisition selection performed from random finite candidate pools to limit greediness and coordinate batch diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_metric</strong></td>
                            <td>Wall-clock time (hours) to reach target performance and number of concurrent experimental resources (number of printers); experiments-per-hour is an implicit metric.</td>
                        </tr>
                        <tr>
                            <td><strong>information_gain_metric</strong></td>
                            <td>GP posterior variance for MV phase and EI for exploitation; both are computed from the shared model and used to prioritize batch allocations.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_information_gain</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_mechanism</strong></td>
                            <td>Explicit two-phase mechanism: MV burn-in across agents (exploration) followed by EI exploitation in parallel; candidate-pool stochasticization to preserve diversity within batches.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_mechanism</strong></td>
                            <td>Parallel agents naturally increase diversity; MV burn-in enforces exploration across batch; stochastic candidate pool selection reduces redundant greedy picks.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_diversity_promotion</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_type</strong></td>
                            <td>Time-limited campaigns and parallel resource constraints (number of available printers/agents).</td>
                        </tr>
                        <tr>
                            <td><strong>budget_constraint_handling</strong></td>
                            <td>Optimizes wall-clock time by running experiments in parallel and sequencing acquisition strategies (explore then exploit) to make effective use of limited time; reported results at T=12,24,36 h illustrate handling.</td>
                        </tr>
                        <tr>
                            <td><strong>breakthrough_discovery_metric</strong></td>
                            <td>Measured experimental toughness U and comparison to grid-based max; campaign-level performance P evaluated versus time T and number of parallel experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Multi-agent BEAR matched grid-based campaign performance after 32 experiments (12 h) and outperformed the grid after 64 experiments (24 h). Overall BEAR experimental campaigns found top structures with 100 experiments versus grid's 1800.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Grid search (serial, exhaustive), single-agent BO campaigns, LHS, and literature batch BO methods (cited but not all implemented).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Parallel BEAR substantially reduced wall-clock time to reach and exceed grid performance (matched in 12 h; outperformed in 24 h) and reduced total experiments compared to grid.</td>
                        </tr>
                        <tr>
                            <td><strong>efficiency_gain</strong></td>
                            <td>Wall-clock time reduced from an implicit multi-day/month grid campaign to tens of hours; found comparable/better solutions using an order-of-magnitude fewer experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>tradeoff_analysis</strong></td>
                            <td>Paper highlights challenges of batch selection (possible redundancy, need for coordinated acquisition), the importance of initial exploration to provide reliable uncertainty estimates, and notes that recent batch-EI algorithms may further improve parallel allocation (references cited).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_allocation_findings</strong></td>
                            <td>Practical protocol: use an initial MV burn-in across parallel agents (~30 samples) to calibrate uncertainty estimates, then switch to EI for directed batch allocations; stochastic candidate selection reduces duplicate greedy picks in batch.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'A Bayesian experimental autonomous researcher for mechanical design', 'publication_date_yy_mm': '2020-04'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Batch Bayesian optimization via local penalization <em>(Rating: 2)</em></li>
                <li>Parallelised Bayesian optimisation via Thompson sampling <em>(Rating: 2)</em></li>
                <li>Asynchronous batch Bayesian optimisation with improved local penalisation <em>(Rating: 2)</em></li>
                <li>Nested-Batch-Mode learning and stochastic optimization with an application to sequential MultiStage testing in materials science <em>(Rating: 2)</em></li>
                <li>Phoenics: A bayesian optimizer for chemistry <em>(Rating: 1)</em></li>
                <li>Bayesian optimization for materials design <em>(Rating: 1)</em></li>
                <li>Taking the human out of the loop: A review of bayesian optimization <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2445",
    "paper_id": "paper-215787556",
    "extraction_schema_id": "extraction-schema-67",
    "extracted_data": [
        {
            "name_short": "BEAR",
            "name_full": "Bayesian experimental autonomous researcher",
            "brief_description": "An autonomous experimental platform that couples an automated additive-manufacturing + robotic testing pipeline with Gaussian-process-based Bayesian optimization to select, execute, and learn from experiments without a human in the loop.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "BEAR (Bayesian experimental autonomous researcher)",
            "system_description": "An integrated hardware+software autonomous research system: five dual-extruder FDM printers and a six-axis robot arm perform 3D printing, retrieval, weighing and Instron mechanical tests; all instruments are coordinated by MATLAB. A Gaussian process regression (GPR) surrogate (squared-exponential kernel, hyperparameters optimized by MLE) models toughness U(x) as a function of design parameters x=(n,θ,r,t). A decision module selects subsequent experiments by evaluating acquisition strategies (expected improvement, maximum predictive variance, or random candidates) drawn from a finite, uniformly random candidate pool; selected designs are converted to STL/gcode and executed automatically. The system supports multi-agent parallel operation (multiple printers) with a shared belief model and batch selection policy.",
            "application_domain": "Experimental mechanics / materials and design optimization (additive manufacturing structural toughness optimization)",
            "resource_allocation_strategy": "A GPR-based Bayesian optimization loop selects next experiments from a uniformly random finite candidate set using acquisition functions: Expected Improvement (EI) for improvement-driven allocation and Maximum Variance (MV) to prioritize uncertainty reduction. For parallel (multi-printer) operation, agents draw from the shared model; practical protocol used here: an initial MV 'burn-in' (32 samples over ~12 hours) to explore and train the GP, then switch to EI to allocate remaining experiments in parallel.",
            "computational_cost_metric": "Measured primarily as number of physical experiments and wall-clock experimental time (hours); GP fitting and acquisition evaluations are treated as lightweight and are not measured in FLOPs or monetary cost in the paper. Experimental budget is expressed as number of experiments (e.g., 32, 64, 100, 1800) and campaign duration in hours (e.g., 12, 24, 36 h).",
            "information_gain_metric": "GP predictive variance (used by MV) and Expected Improvement (EI) computed from the GP posterior (used as acquisition function); information about uncertainty is encoded in the GP posterior variance and exploited by MV/EI.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit acquisition-policy driven balance: MV directly prioritizes exploration by selecting points of highest posterior variance; EI balances exploitation and exploration by selecting points with largest expected improvement over the incumbent best; an initial MV exploration phase (approx. 32 points / 12 h) is used to reduce model uncertainty before switching to EI to focus on high-potential designs. Additionally, acquisition selection is restricted to a random finite candidate pool to reduce EI greediness.",
            "diversity_mechanism": "Multiple mechanisms to promote diversity: (1) pure random sampling / Pure Exploration (PE) for stochastic coverage in simulations; (2) MV acquisition which targets high-uncertainty (diverse) regions; (3) selecting next point from a uniformly random finite candidate set (stochastic candidate pool) to avoid greedy convergence; (4) parallel agents performing different selected experiments in batch.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experimental budget (number of experiments) and time budget (wall-clock campaign duration); resource parallelism (number of printers) is also an explicit constraint/value for speed.",
            "budget_constraint_handling": "The system enforces an experiment budget or time limit and optimizes acquisition selection under that constraint; in parallel campaigns it uses batch selection (initial MV burn-in then EI) to best use multiple printers over a given time horizon (explicit example: 12 h burn-in with MV (32 samples), then EI; reported evaluations at T=12, 24, 36 h).",
            "breakthrough_discovery_metric": "Toughness U (energy absorbed, J) measured experimentally; a normalized campaign performance P(i)=U_grid(x_i)/max(U_grid) (in simulations) is used to quantify how close the predicted optimum is to the true surrogate maximum. Expected Improvement (EI) serves as the acquisition proxy for breakthrough potential (probability/expectation of surpassing current best).",
            "performance_metrics": "Reported numbers: toughness U range in grid campaign 0.3–51.5 J (mean 15.3 J); GP-predicted optimum from grid U_grid = 43.4 ± 6.0 J. Experimental campaign comparisons: BEAR found superior structures in five of six runs; BEAR found top structures in 100 experiments vs grid-based search of 1800 experiments. Efficiency/time metrics: BEAR matched grid campaign performance after 32 experiments (12 h) and outperformed grid after 64 experiments (24 h). Simulation metrics: in low-noise (σ=0.1 J) simulations, all policies median P ≥ ~85% in 100 experiments, EI ~P&gt;98%; in high-noise (σ=5 J), EI achieved ~P&gt;90% in 100 experiments while LHS achieved ~P≈75%. Measurement variability: toughness SD 12.8% of mean; manufacturing-uncorrelated toughness variation ~5.8% of mean.",
            "comparison_baseline": "Grid-based exhaustive search (1800 experiments), Latin hypercube sampling (LHS), Pure Exploration (random sampling), and simulated policies (MV and EI).",
            "performance_vs_baseline": "BEAR (EI-guided) achieved higher experimental performance than the best structure predicted by the grid model even though BEAR used ~100 experiments vs grid's 1800; simulations show EI outperforms LHS and PE (e.g., in high-noise EI ~P&gt;90% vs LHS ~P≈75% after 100 experiments). Multi-agent BEAR matched grid performance after 32 experiments (12 h) and outperformed grid after 64 experiments (24 h).",
            "efficiency_gain": "Almost 60-fold reduction in number of experiments needed to identify high-performing structures relative to the grid-based search (100 vs 1800 experiments reported); time-to-equivalent-performance reduced to tens of hours (matched in 12 h, outperformed in 24 h) using parallel printers.",
            "tradeoff_analysis": "The paper analyzes tradeoffs qualitatively and in simulation: EI balances exploration/exploitation but can be too greedy with an uninformative prior; MV promotes exploration and uncertainty reduction but may require large budgets to find optima; PE (random) is robust but inefficient. Noise level affects policy performance (EI remains strong in moderate-high noise). The authors highlight model misspecification issues (homoscedastic GP vs observed heteroscedastic experimental noise) which affect uncertainty quantification and acquisition effectiveness. They recommend placing equal emphasis on uncertainty quantification and acquisition selection under limited budgets.",
            "optimal_allocation_findings": "Practical recommendations: (1) perform an initial exploration phase (they used ~30 samples / 12 h with MV) to train the GP and obtain reasonable uncertainty estimates before aggressive exploitation; (2) use EI for directed improvement after a burn-in; (3) use stochastic candidate pools for EI to mitigate greediness; (4) when parallel resources are available, apply batch selection informed by a shared belief model (multi-agent) to reduce wall-clock time; (5) uncertainty quantification should be considered an equal partner with exploitation when budgets are small or noise/model mismatch is present.",
            "uuid": "e2445.0",
            "source_info": {
                "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "BO (GPR)",
            "name_full": "Bayesian optimization with Gaussian process regression",
            "brief_description": "A Bayesian optimization framework that uses a Gaussian process surrogate (squared-exponential kernel) to model experimental response and acquisition functions (EI, MV) to allocate experiments sequentially or in batches.",
            "citation_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "mention_or_use": "use",
            "system_name": "Gaussian-process-based Bayesian optimization (BO)",
            "system_description": "A GPR surrogate with zero mean and squared-exponential covariance kernel models U(x); kernel hyperparameters (signal variance α and length scales β_i) and assumed homoscedastic noise σ are initialized and then optimized by maximum likelihood after each observation. Acquisition functions evaluated on a finite random candidate set guide experiment selection (EI for improvement, MV for uncertainty).",
            "application_domain": "Experimental design and optimization in materials, mechanics, and other low-data experimental sciences",
            "resource_allocation_strategy": "Acquisition-function-driven sequential allocation: select candidate with maximum acquisition value (EI or MV) from a random finite candidate pool; hyperparameters updated after each observation to refine allocation.",
            "computational_cost_metric": "Number of physical experiments and wall-clock time; GP fitting and acquisition evaluations treated as negligible for cost accounting in experiments.",
            "information_gain_metric": "Posterior variance (predictive uncertainty) and acquisition-derived expected improvement from GP posterior.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Choice of acquisition function: MV = exploration (maximize posterior variance), EI = tradeoff of exploitation/exploration via expected improvement over incumbent.",
            "diversity_mechanism": "Use of MV and random candidate pools to encourage coverage; explicit batch/parallel selection when multiple experimental resources are available.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experimental budget (number of experiments) and time budgets.",
            "budget_constraint_handling": "The BO loop is run until an experiment budget or time limit is reached; initial exploration samples are allocated to better estimate hyperparameters and noise prior to exploitation.",
            "breakthrough_discovery_metric": "Expected improvement (EI) and direct measured performance U (toughness in J) used to quantify high-impact discoveries.",
            "performance_metrics": "In simulation: low-noise (σ=0.1 J) EI achieved ~P&gt;98% in 100 experiments; high-noise (σ=5 J) EI achieved ~P&gt;90% in 100 experiments. In experiment: BO (EI) campaigns found structures outperforming grid-derived best using far fewer experiments (100 vs 1800).",
            "comparison_baseline": "Grid search, Latin hypercube sampling, pure random sampling.",
            "performance_vs_baseline": "BO (EI) outperformed LHS and PE in simulations and outperformed grid-based experimental campaign in experiments when accounting for number of experiments/time.",
            "efficiency_gain": "BO required far fewer experiments to approach optimum (e.g., ~100 vs 1800 grid experiments), and EI showed stronger convergence behavior in the simulations reported.",
            "tradeoff_analysis": "Paper reports sensitivity to noise and prior; EI can be greedy under uninformative priors and MV can require many samples; GP assumptions (homoscedastic noise) can degrade performance if violated.",
            "optimal_allocation_findings": "Use initial exploratory MV samples to calibrate the GP, then switch to EI for directed improvement; using a finite random candidate pool for EI improves convergence by reducing greedy selection.",
            "uuid": "e2445.1",
            "source_info": {
                "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "EI (stochastic candidate)",
            "name_full": "Expected Improvement acquisition with stochastic candidate selection",
            "brief_description": "An improvement-based acquisition function (Expected Improvement) modified by this work to evaluate EI only over a finite uniformly random candidate set to reduce greedy behavior and improve convergence.",
            "citation_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "mention_or_use": "use",
            "system_name": "Expected Improvement (EI) with finite random candidate pool",
            "system_description": "EI computes the expected positive improvement over the current best result under the GP posterior; here EI is evaluated not over the continuous domain but over a finite, uniformly random set of candidate designs each acquisition step, which reduces excessive greediness and improves practical convergence.",
            "application_domain": "Active experimental selection for materials/mechanics experiments and other low-data experimental campaigns",
            "resource_allocation_strategy": "Select the candidate design (from the random candidate pool) with highest EI to allocate the next experiment; in multi-agent campaigns this is used after an initial MV exploration phase.",
            "computational_cost_metric": "Number of experiments and wall-clock experimental time; acquisition evaluation cost limited by candidate pool size (computational overhead small relative to experiment cost).",
            "information_gain_metric": "Expected Improvement computed using the GP posterior predictive mean and variance.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "EI balances exploitation and exploration via expectation over the posterior distribution; stochastic candidate pool injects diversity and mitigates premature exploitation.",
            "diversity_mechanism": "Finite random candidate pool (uniform sampling each acquisition) forces consideration of multiple diverse options; initial MV burn-in before EI further promotes diversity.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experimental budget (number of experiments) and time-limited campaigns.",
            "budget_constraint_handling": "In practice they performed MV-guided sampling for the initial budgeted exploration (32 samples / 12 h) then switched to EI given remaining budget/time to exploit promising regions.",
            "breakthrough_discovery_metric": "EI itself measures expected chance to exceed current best; experimental breakthrough measured by toughness U (J) and normalized P metric in simulations.",
            "performance_metrics": "Simulated: EI achieved ~P&gt;98% in low-noise (σ=0.1 J) and ~P&gt;90% in high-noise (σ=5 J) within 100 experiments. Experimental: all three EI-guided physical campaigns produced optimum structures outperforming grid-derived best when tested.",
            "comparison_baseline": "MV, PE, LHS, and grid search.",
            "performance_vs_baseline": "EI converged faster and to higher performance than LHS and PE in simulation; in experiments EI-guided BEAR runs found better structures using far fewer experiments than grid.",
            "efficiency_gain": "EI-based campaigns reached high P values with ~100 experiments (versus 1800 grid experiments); in multi-agent mode contributed to matching/outperforming grid within 12–24 hours.",
            "tradeoff_analysis": "EI is powerful but can be too greedy with an uninformative prior and insufficient uncertainty estimates; stochastic candidate selection and initial MV burn-in help ameliorate this.",
            "optimal_allocation_findings": "Evaluate EI on a randomized finite candidate set and perform an initial exploratory MV phase to calibrate the GP prior before exploiting with EI.",
            "uuid": "e2445.2",
            "source_info": {
                "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "MV",
            "name_full": "Maximum variance acquisition (uncertainty sampling)",
            "brief_description": "An acquisition policy that selects the next experiment at the input location where the GP predictive variance is maximal, prioritizing reduction of model uncertainty (exploration).",
            "citation_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "mention_or_use": "use",
            "system_name": "Maximum Variance (MV) acquisition",
            "system_description": "At each acquisition step, compute the GP posterior predictive variance across candidate designs and select the candidate with maximum variance; used both in simulations and as an initial exploration policy in multi-agent experiments.",
            "application_domain": "Active learning / experimental design in noisy low-data experimental domains (mechanics here)",
            "resource_allocation_strategy": "Allocate experimental runs to points of highest predictive uncertainty to maximize information gain about the response surface.",
            "computational_cost_metric": "Number of experiments and experimental wall-clock time; MV may require many experiments to adequately explore the space.",
            "information_gain_metric": "GP posterior predictive variance (proxy for expected information gain).",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Pure exploration; MV prioritizes exploration exclusively, potentially at the expense of immediate exploitation.",
            "diversity_mechanism": "By targeting highest-variance regions MV promotes coverage of uncertain/different hypotheses; used as burn-in to generate diverse data.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experimental budget and time limits.",
            "budget_constraint_handling": "Used either as stand-alone policy or for initial exploration (burn-in) to provide training data for the GP before switching to an exploitative acquisition like EI.",
            "breakthrough_discovery_metric": "Indirect: by reducing uncertainty MV can enable later discovery via other acquisition functions; not directly aimed at immediate breakthroughs.",
            "performance_metrics": "Simulations: in low-noise (σ=0.1 J) MV along with other policies achieved median P ≳ 85% after 100 experiments; in benchmarks MV serves as an effective initial exploration policy (authors used ≈32 MV samples in practice).",
            "comparison_baseline": "PE, EI, LHS, grid.",
            "performance_vs_baseline": "MV outperforms pure random sampling in structured exploration and is helpful as a burn-in to improve EI performance; MV alone may be slower in identifying top optima than EI for the same budget.",
            "efficiency_gain": "When used as an initial calibration phase (~30 samples) it improves downstream efficiency of EI exploitation, enabling faster convergence in wall-clock time for the multi-agent campaigns.",
            "tradeoff_analysis": "MV favors information gain at the expense of immediate improvement; authors recommend combining MV burn-in with subsequent EI exploitation under tight budgets.",
            "optimal_allocation_findings": "Use MV for early exploration to calibrate the GP and quantify uncertainty; around ~30 exploratory samples sufficed in their domain to saturate initial convergence in simulation.",
            "uuid": "e2445.3",
            "source_info": {
                "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "PE",
            "name_full": "Pure exploration (random sampling)",
            "brief_description": "A baseline decision policy that selects each subsequent experiment uniformly at random, ensuring broad coverage but without model-guided allocation.",
            "citation_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "mention_or_use": "use",
            "system_name": "Pure Exploration (PE) / random sampling",
            "system_description": "Select the next experiment randomly from the design space (uninformed sampling). Used in simulations as a baseline to compare against BO-informed policies.",
            "application_domain": "Baseline experimental allocation strategy for experimental design studies",
            "resource_allocation_strategy": "Uniform random selection across the domain for each experiment.",
            "computational_cost_metric": "Number of experiments; wall-clock time is dictated solely by the experiments performed.",
            "information_gain_metric": "None explicit; any information gain arises passively from random coverage.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Pure exploration by definition (no exploitation).",
            "diversity_mechanism": "Randomness ensures diverse sampling across the domain.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed experimental budget/time.",
            "budget_constraint_handling": "No optimization under budget — used as a baseline allocation until budget exhausted.",
            "breakthrough_discovery_metric": "Measured outcomes (toughness U) after sampling; PE has no directed metric for breakthroughs.",
            "performance_metrics": "Simulations: PE achieved similar performance to LHS (space-filling) in high-noise settings, but inferior to EI in terms of speed to high P.",
            "comparison_baseline": "Compared against EI, MV, LHS, grid.",
            "performance_vs_baseline": "Inferior to EI; comparable to LHS in certain noisy regimes.",
            "efficiency_gain": "None inherent; provides baseline efficiency for comparison.",
            "tradeoff_analysis": "PE is robust to model misspecification but inefficient when experiments are expensive; the paper uses PE to illustrate the value of BO-guided allocation.",
            "optimal_allocation_findings": "Not recommended as an allocation strategy when experiments are costly; useful only as a baseline.",
            "uuid": "e2445.4",
            "source_info": {
                "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "LHS",
            "name_full": "Latin hypercube sampling",
            "brief_description": "A space-filling sampling scheme used as a baseline in simulations to compare against BO-driven sequential policies.",
            "citation_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "mention_or_use": "use",
            "system_name": "Latin hypercube sampling (LHS)",
            "system_description": "A stratified sampling method that generates evenly distributed samples in high-dimensional parameter space; used in simulated campaigns with the same experimental budget to benchmark BO policies.",
            "application_domain": "Baseline experimental design / space-filling sampling in materials and experimental design studies",
            "resource_allocation_strategy": "Precompute a fixed set of space-filling samples (LHS) and perform experiments on this set.",
            "computational_cost_metric": "Number of experiments and experimental wall-clock time; no online computation required.",
            "information_gain_metric": "None explicit; aims to maximize coverage so information gain is distributed over the domain.",
            "uses_information_gain": false,
            "exploration_exploitation_mechanism": "Pure exploration via space-filling design (no exploitation).",
            "diversity_mechanism": "Design ensures diverse coverage across dimensions.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Fixed number of experiments (preallocated).",
            "budget_constraint_handling": "Budget allocated a priori across a space-filling set of designs.",
            "breakthrough_discovery_metric": "Measured discovery of high U values in the sampled set; no acquisition for improvement.",
            "performance_metrics": "Simulations: in high-noise (σ=5 J) LHS achieved ~P≈75% after 100 experiments (inferior to EI, similar to PE).",
            "comparison_baseline": "Compared against BO policies (EI/MV), PE, and grid.",
            "performance_vs_baseline": "Inferior to EI in simulations; similar to PE in noisy regimes.",
            "efficiency_gain": "None relative to BO; serves as a simple benchmark.",
            "tradeoff_analysis": "LHS provides robust coverage but lacks targeted acquisition; appropriate when model-driven selection is infeasible or when one wants to build a uniform surrogate.",
            "optimal_allocation_findings": "Authors use LHS as a baseline to show BO's advantage under limited budgets and noise.",
            "uuid": "e2445.5",
            "source_info": {
                "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
                "publication_date_yy_mm": "2020-04"
            }
        },
        {
            "name_short": "Multi-agent / batch BO (BEAR parallel)",
            "name_full": "Multi-agent batch Bayesian optimization (parallel BEAR)",
            "brief_description": "A parallel/batch experimental allocation protocol where multiple experimental agents (printers) draw from a shared GP belief and execute experiments in parallel, with an initial MV exploration phase followed by EI exploitation to minimize wall-clock time.",
            "citation_title": "A Bayesian experimental autonomous researcher for mechanical design",
            "mention_or_use": "use",
            "system_name": "Multi-agent batch BO (parallel BEAR)",
            "system_description": "Six agents (printers) operate in concert, sharing a global GP model updated with all collected data. Because experiments are executed in parallel, a batch selection policy is required; the implemented protocol uses MV-guided sampling for an initial 12-hour (32-sample) burn-in across agents and then switches to EI for subsequent allocations. The paper references recent batch-EI methods (local penalization, Thompson sampling variants, asynchronous batch BO) but uses the described practical MV-then-EI protocol for experiments.",
            "application_domain": "Parallelized autonomous experimentation for rapid materials/mechanics discovery and optimization",
            "resource_allocation_strategy": "Allocate experiments concurrently across available agents using a shared surrogate: initial parallel MV sampling to maximize variance coverage, then parallel EI allocations for improvement; acquisition selection performed from random finite candidate pools to limit greediness and coordinate batch diversity.",
            "computational_cost_metric": "Wall-clock time (hours) to reach target performance and number of concurrent experimental resources (number of printers); experiments-per-hour is an implicit metric.",
            "information_gain_metric": "GP posterior variance for MV phase and EI for exploitation; both are computed from the shared model and used to prioritize batch allocations.",
            "uses_information_gain": true,
            "exploration_exploitation_mechanism": "Explicit two-phase mechanism: MV burn-in across agents (exploration) followed by EI exploitation in parallel; candidate-pool stochasticization to preserve diversity within batches.",
            "diversity_mechanism": "Parallel agents naturally increase diversity; MV burn-in enforces exploration across batch; stochastic candidate pool selection reduces redundant greedy picks.",
            "uses_diversity_promotion": true,
            "budget_constraint_type": "Time-limited campaigns and parallel resource constraints (number of available printers/agents).",
            "budget_constraint_handling": "Optimizes wall-clock time by running experiments in parallel and sequencing acquisition strategies (explore then exploit) to make effective use of limited time; reported results at T=12,24,36 h illustrate handling.",
            "breakthrough_discovery_metric": "Measured experimental toughness U and comparison to grid-based max; campaign-level performance P evaluated versus time T and number of parallel experiments.",
            "performance_metrics": "Multi-agent BEAR matched grid-based campaign performance after 32 experiments (12 h) and outperformed the grid after 64 experiments (24 h). Overall BEAR experimental campaigns found top structures with 100 experiments versus grid's 1800.",
            "comparison_baseline": "Grid search (serial, exhaustive), single-agent BO campaigns, LHS, and literature batch BO methods (cited but not all implemented).",
            "performance_vs_baseline": "Parallel BEAR substantially reduced wall-clock time to reach and exceed grid performance (matched in 12 h; outperformed in 24 h) and reduced total experiments compared to grid.",
            "efficiency_gain": "Wall-clock time reduced from an implicit multi-day/month grid campaign to tens of hours; found comparable/better solutions using an order-of-magnitude fewer experiments.",
            "tradeoff_analysis": "Paper highlights challenges of batch selection (possible redundancy, need for coordinated acquisition), the importance of initial exploration to provide reliable uncertainty estimates, and notes that recent batch-EI algorithms may further improve parallel allocation (references cited).",
            "optimal_allocation_findings": "Practical protocol: use an initial MV burn-in across parallel agents (~30 samples) to calibrate uncertainty estimates, then switch to EI for directed batch allocations; stochastic candidate selection reduces duplicate greedy picks in batch.",
            "uuid": "e2445.6",
            "source_info": {
                "paper_title": "A Bayesian experimental autonomous researcher for mechanical design",
                "publication_date_yy_mm": "2020-04"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Batch Bayesian optimization via local penalization",
            "rating": 2,
            "sanitized_title": "batch_bayesian_optimization_via_local_penalization"
        },
        {
            "paper_title": "Parallelised Bayesian optimisation via Thompson sampling",
            "rating": 2,
            "sanitized_title": "parallelised_bayesian_optimisation_via_thompson_sampling"
        },
        {
            "paper_title": "Asynchronous batch Bayesian optimisation with improved local penalisation",
            "rating": 2,
            "sanitized_title": "asynchronous_batch_bayesian_optimisation_with_improved_local_penalisation"
        },
        {
            "paper_title": "Nested-Batch-Mode learning and stochastic optimization with an application to sequential MultiStage testing in materials science",
            "rating": 2,
            "sanitized_title": "nestedbatchmode_learning_and_stochastic_optimization_with_an_application_to_sequential_multistage_testing_in_materials_science"
        },
        {
            "paper_title": "Phoenics: A bayesian optimizer for chemistry",
            "rating": 1,
            "sanitized_title": "phoenics_a_bayesian_optimizer_for_chemistry"
        },
        {
            "paper_title": "Bayesian optimization for materials design",
            "rating": 1,
            "sanitized_title": "bayesian_optimization_for_materials_design"
        },
        {
            "paper_title": "Taking the human out of the loop: A review of bayesian optimization",
            "rating": 1,
            "sanitized_title": "taking_the_human_out_of_the_loop_a_review_of_bayesian_optimization"
        }
    ],
    "cost": 0.02049575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>A P P L I E D S C I E N C E S A N D E N G I N E E R I N G A Bayesian experimental autonomous researcher for mechanical design
2020. eaaz1708 10 April 2020</p>
<p>Aldair E Gongora 
Bowen Xu 
Wyatt Perry 
Chika Okoye 
Patrick Riley 
Kristofer G Reyes 
Elise F Morgan 
Keith A Brown 
A P P L I E D S C I E N C E S A N D E N G I N E E R I N G A Bayesian experimental autonomous researcher for mechanical design</p>
<p>Sci. Adv
62020. eaaz1708 10 April 2020S C I E N C E A D V A N C E S | R E S E A R C H A R T I C L E 1 of 6
While additive manufacturing (AM) has facilitated the production of complex structures, it has also highlighted the immense challenge inherent in identifying the optimum AM structure for a given application. Numerical methods are important tools for optimization, but experiment remains the gold standard for studying nonlinear, but critical, mechanical properties such as toughness. To address the vastness of AM design space and the need for experiment, we develop a Bayesian experimental autonomous researcher (BEAR) that combines Bayesian optimization and high-throughput automated experimentation. In addition to rapidly performing experiments, the BEAR leverages iterative experimentation by selecting experiments based on all available results. Using the BEAR, we explore the toughness of a parametric family of structures and observe an almost 60-fold reduction in the number of experiments needed to identify high-performing structures relative to a grid-based search. These results show the value of machine learning in experimental fields where data are sparse.RESULTSAutomating mechanical testing of additively manufactured partsToughness is difficult to optimize because it requires maximizing a combination of two properties that tend to be inversely correlated, namely, strength and ductility(14,15). Defined as the area under the force (F)-displacement (D) curve, toughness represents how much energy a component can absorb before failure, which makes it an important property to optimize in the context of design for safety and A Bayesian experimental autonomous researcher for mechanical design. Sci. Adv. 6, eaaz1708 (2020).</p>
<p>INTRODUCTION</p>
<p>The processes by which mechanical structures are designed have evolved to include a variety of computational tools that have been successful in producing structures with highly tuned properties (1)(2)(3)(4)(5)(6)(7)(8)(9)(10). However, realizing high-performance mechanical structures often involves optimizing properties that cannot be reliably and rapidly predicted using computation, namely, nonlinear mechanical properties (11)(12)(13)(14)(15)(16). Phenomena such as dynamic self-contacts during large deformation and the dominance of stochastic defects in determining failure in real samples make computation difficult and necessitate experiments. Additive manufacturing (AM) has compounded this problem by both vastly increasing the available design space and introducing a host of previously unknown defects for which researchers and practitioners do not have the benefit of empirical engineering guidelines built on decades of intense study (17)(18)(19). This raises the question of how best to design and optimize structures for properties that are difficult to simulate. One approach that has been successful in chemistry, biology, and, more recently, materials science has been autonomous research in which experiments are selected by machine learning and carried out without human intervention (20)(21)(22)(23)(24). Autonomous research systems have been beneficial in these fields because many properties of interest must be experimentally determined, the vast size of the parameter space limits the effectiveness of brute-force experimentation, and the necessary experiments are compatible with automation. However, autonomous research systems are highly specific to certain classes of experiments and have not been realized in the mechanical domain. Moreover, most prior experimental autonomous research systems have not used Bayesian optimization (BO) to guide the selection of experiments, although simulations have revealed that using BO would be more efficient (25)(26)(27)(28).</p>
<p>Here, we test the hypothesis that combining automated experimentation and BO can accelerate the pace of structural design. Conceptually, realizing a Bayesian experimental autonomous researcher (BEAR) involves two steps: the development of an automated system that performs experiments without human intervention and the incorporation of active learning to choose subsequent experiments in a Bayesian framework. First, we report the design and realization of a mechanical testing system that automatically three-dimensionally (3D) prints and tests parts to determine their mechanical properties such as toughness (Fig. 1A). The highthroughput nature of this system, relative to manual testing, allows for the comprehensive exploration of a large family of structures (Fig. 1B) and the determination of uncertainty inherent to AM using thousands of experiments, a previously impractical concept. Using this experimental data, we run a series of simulations to find that BO should use experiments more efficiently than grid-based searching. Subsequently, we instruct the BEAR to perform experimental campaigns and find that these campaigns resulted in higherperformance structures than those identified through a grid-based campaign that involved 18 times more experiments than were allotted to the BEAR. Last, rather than evaluating a campaign by the required number of experiments, we investigate how campaign duration can be reduced by multiple printers acting in parallel in a multiagent approach and find that the BEAR identifies high-performing structures within 24 hours. Collectively, this work shows the potential for BEARs to affect fields where computational tools are imperfect and experiments are slow and complex. failure tolerance (29,30). Further compounding the design challenge, it is commonly important to maximize toughness while not exceeding a specified force threshold to avoid damaging more sensitive elements elsewhere in the system. For example, the crumple zone of a car is designed to maximize toughness by absorbing the impact of a collision while not transmitting harmful reactionary forces to the passengers. Because of the importance of dynamic self-contacts, the stochastic influence of defects, and other variations from processing, computational optimization of toughness is extremely difficult (13,16). To illustrate this, we designed a crossed barrel family of structures ( Fig. 1B) with two platforms that are held apart by n hollow columns of outer radius r and thickness t and that are twisted with an angle . Structures in this family include those with a wide array of F-D responses, including structures that exceed a ~5-kN force threshold before yielding ( Fig. 1C) and weak structures that fail in a brittle manner (Fig. 1D). The inclusion of a force threshold, as well as the subsequent definition of structure as "too strong," was incorporated to reflect the presence of a force constraint in designing for toughness, such as in the design of crumple zones. Considering that superlative toughness requires both high ductility and high strength, the best crossed barrel in terms of toughness is not simple to predict. Crossed barrels with high toughness exhibit complex F-D responses (Fig. 1E) with a number of reentrant contacts and local buckling events.</p>
<p>Acknowledging that toughness needs to be evaluated using experiment, we sought to explore the degree to which the pace of mechanical testing could be accelerated. In particular, we designed and constructed an automated testing system that combines AM, robotics, and mechanical testing (movie S1). In particular, fused deposition modeling (FDM) 3D printers are among the most commonly used 3D printers due to their low cost, versatility, and reliability. Furthermore, FDM-printed parts can be used without additional processing, enabling rapid testing. Thus, five dual extruder FDM printers (M3, MakerGear) were positioned in the working radius of a six-axis robotic arm (UR5e, Universal Robotics). To perform testing and characterization of parts, a scale (CP225D, Sartorius) and a universal testing machine (5965, Instron Inc.) were also positioned in the working radius of the arm. All instruments were coordinated using custom software (MATLAB) ( fig. S1).</p>
<p>Before undertaking more complex design or optimization processes, it is necessary to consider that quality control is a pervasive challenge in AM. The extensive exploration of manufacturing uncertainty can be onerous in some cases; however, the automated testing system provided an avenue for rapidly quantifying the uncertainty inherent to properties such as toughness. Thus, we initially performed a series of experiments in which the same design was printed and tested 240 times using all available printers ( Fig. 2A) and the mass of each part-measured in situ-had a standard deviation (SD) equal to 4.6% of the mean. The toughness U was found to have an SD equal to 12.8% of the mean (Fig. 2B). Because of the empirical nature of these quantities, it would be difficult or impossible to predict the sensitivity with which toughness depends on mass. Having this vast dataset, which is made possible by the high-throughput nature of the system, allows us to approximate the variation in toughness that is uncorrelated with mass, which we find to be 5.8% of the mean. The individual printer mass and toughness variations are reported in fig. S2. This study allowed for the exploration of the correlation between these two properties ( Fig. 2B), revealing a correlation coefficient of 0.71 between U and m, indicating that measurable deviations in print outcome are at least partially responsible for the observed variation in mechanical behavior. The fit in Fig. 2B is insensitive to the removal of the seemingly spurious data.</p>
<p>High-throughput experimentation and sequential design selection</p>
<p>Despite the large observed variability in performance, it is conceivable that the high-throughput nature of this research platform could allow for sufficient experiments to empirically identify an optimal design within a family of structures. To test this, we performed a grid-based experimental campaign in which 600 distinct designs were tested in triplicate without human intervention ( fig. S3). During this automated experimental campaign, a wide variation in U was observed, ranging from 0.3 to 51.5 J with a mean value of 15.3 J (Fig. 2C). To Model "crossed barrel" family of parametric structures with two circular platforms that are held apart by a series of n hollow columns of outer radius r and thickness t and that are twisted with an angle . Force F and corresponding displacement D from the testing of (C) a crossed barrel that did not yield before ~5 kN (designated too strong), (D) a crossed barrel that failed in a brittle manner (designated "brittle"), and (E) a crossed barrel that exhibited appreciable strength after an initial yield point (designated "ductile").</p>
<p>estimate the value and uncertainty of the experimental response U(x), where x = (n, , r, t) was approximated using a Gaussian process regression (GPR) model U grid with a squared exponential kernel (Fig. 2D) (25). U grid revealed a complex response with several highperforming regions (Fig. 2E) and a predicted optimum of 43.4 ± 6.0 J at (12,85°, 2.45 mm, 0.7 mm).</p>
<p>While brute-force experimentation allowed us to predict an optimum design, the active learning community has shown through simulations that sequentially selecting experiments using BO finds optima using fewer samples. The BO framework is composed of two components, a belief model that captures the relationship between parameters and response and a decision-making policy that guides the selection of experiments (25). While U(x) is ultimately an experimentally observed function, U grid (x) represents an approximation of U(x) that can be used to evaluate BO strategies in simulation. Thus, we performed a series of simulations using the BO framework with U grid (x) treated as a surrogate for the ground truth U(x). To approximate experimental variations, we added a zero-mean Gaussian noise with SD  to each simulated measurement.</p>
<p>We studied three principal decision-making policies: pure exploration (PE), maximum variance (MV) (Fig. 3A), and expected improvement (EI) (Fig. 3B). These decision-making policies were selected because of their popularity in the optimization community and their distinctive explorative and exploitative qualities (25,26). PE is a purely explorative decision-making policy, where each subsequent experiment was chosen randomly. While PE will eventually explore the parameter space and is unlikely to get trapped by local maxima, an appropriate experimental budget is often unknown or too large. The MV decision-making policy also prioritizes exploration but takes the surrogate model into account by choosing experiments in regions with the largest uncertainty. An advantage of this approach is the exploration of undiscovered regions that might have high-performing designs; however, the number of experiments necessary to adequately explore parameter space is also often unknown or too large. The EI decision-making policy is an improvementbased policy in which subsequent experiments are selected on the basis of the likelihood of surpassing previously observed responses. In contrast with MV and PE, EI is more likely to get trapped by local maxima. Purely exploitative policies were not considered because of the use of an uninformative prior in the experimental campaigns.</p>
<p>The performance P of a given campaign after i experiments was given by its predicted optimum x i and was defined as P(i) = U grid (x i )/ max (U grid ). On the basis of this definition, P = 1 indicated that the campaign had found the optimum design. In the low noise limit ( = 0.1 J), all policies achieved median performance ~ P ≥ 85% in 100 experiments, with EI achieving ~ P &gt; 98% (Fig. 3C). However, when the noise level was increased ( = 5 J), EI alone achieved ~ P &gt; 90% within 100 experiments (Fig. 3D). The incomplete convergence in the low noise and high noise limits is a result of the limited experimental budget allotted to the simulated campaigns. As a comparison to BO, we also simulated campaigns based on Latin hypercube sampling (LHS) with the same experimental budget. For the high noise limit, LHS-based campaigns achieved ~ P = 75% , which is similar to PE but is inferior to EI-based campaigns. Note that the EI approach used here differs from standard approaches, which have been reported to result in EI being too greedy (31), by selecting from a finite number of random candidate experiments to be evaluated by EI. This stochastic approach was seen to markedly improve the convergence of EI-based campaigns ( fig. S4).</p>
<p>Realizing and benchmarking autonomous experimental optimization</p>
<p>While simulation predicts that BO will outperform grid-based approaches, such as LHS, these simulations were based on a number of assumptions, namely, the model of ground truth, the noise profile, and the sampling strategy. Thus, it is imperative to experimentally explore the utility of the BO framework. We therefore integrated the BO framework with the automated research system to produce a BEAR that chose, performed, and learned from experiments. We performed six experimental campaigns, three that were guided by EI and three that were guided by MV. The results of the BEAR's experimental campaigns were compared with the predicted bestperformance structure according to U grid , where P, on average, increased with i (Fig. 4, A and B). However, it is worth emphasizing that, first, unlike in the simulated campaigns where U grid is treated as ground truth, U grid here is a statistically regressed model and, second, the only reliable method to assess the performance of an experimental campaign is to experimentally test the predicted best-performance structure. Here, we accomplished this by testing 10 copies of the optimum structures predicted by each experimental campaign (Fig. 4C). On the basis of the experimental tests, five of the six optimum structures found by the experimental campaigns outperformed the best structure predicted by U grid , showing that, in five instancesincluding all three based on the EI decision-making policy-100 well-chosen experiments were superior to 1800 experiments chosen on a grid.</p>
<p>While the BEAR was successful in optimizing the crossed barrel, the superiority of experimental performance (Fig. 4C) over simulated performance (Fig. 4, A and B) was likely because the results of experiment (U) differed markedly from the experimentally derived surrogate model (U grid ). Two possible reasons for this discrepancy were insufficient data to build an accurate surrogate model and that the experimental function was observed to be heteroscedastic, while U grid was fit using a homoscedastic model. While, in principle, one could perform enough experiments to accurately model the noise profile and obtain a more faithful model of the truth more generally, this would likely require a prohibitive number of experiments. Thus, these results not only highlight the need for experiments in benchmarking optimization strategies but also emphasize the importance of experiment selection when the design space is high dimensional and when building an accurate surrogate model is impossible or impractical. Moreover, these results position uncertainty quantification as an equal partner to balance exploration and exploitation in an optimization process.</p>
<p>While the BEAR was found to be efficient with respect to the number of experiments, it is often more important to be efficient in terms of time when optimizing a property. Thus, we explored how multiple printers could be used in concert to optimize performance in minimal time. We note that using all printers at all times necessitated selecting samples in parallel. The decision-making policy associated with choosing jobs in such a batch system is the topic of current research, with recently developed batch-based EI policies showing promise in simulation (32)(33)(34)(35). The BEAR provides an experimental platform to evaluate these policies. As an initial campaign to serve as a benchmark, we performed a Bayesian experimental campaign in which six agents selected experiments based on the data collected from all agents. While, in theory, EI should balance exploration and exploitation if uncertainties are properly quantified, the use of an initially uninformative prior belief limits uncertainty quantification. In practice, a standard approach is to select a number of randomly chosen samples to explore the parameter space and train the Gaussian process. Here, we spend the first 12 hours of the campaign selecting experiments (32 samples) using MV. MV-guided simulated campaigns show that convergence saturates at ~30 experiments (Fig. 3D), further motivating this practice. After this initial 12-hour period, we switched to an EI decision-making policy. The predicted best-performance structures at T = 12, 24, and 36 hours were each experimentally tested to determine their performance. The BEAR matched the toughness from the grid-based experimental campaign (1800 experiments) after 12 hours (32 experiments) and outperformed the grid-based experimental campaign after 24 hours (64 experiments) (Fig. 4D).</p>
<p>Ultimately, the extensive experiments described herein allowed us to identify the member of the crossed barrel family with the largest U, namely, (12, 131°, 1.95 mm, 1.4 mm). Inspecting the F-D curve corresponding to one such sample, a number of interesting features were evident (Fig. 4E). In particular, a series of six inflection points were observed, which corresponded to different mechanical processes including the initial yield point and a series of buckling and reentrant contact modes. Notably, the precise parameter values that produce the largest net positive effect of these sometimes competing and sometimes synergistic processes on U would be difficult to predict in the absence of experiment.</p>
<p>DISCUSSION</p>
<p>The observation that a BEAR can, in the case of optimizing toughness, reduce the number of experiments needed by a factor of almost 60 has potentially far-reaching implications spanning mechanics and the field of autonomous experimentation more broadly. In mechanics, the combination of a BEAR and simulation-based approaches such as topological optimization could allow for the rapid optimization and discovery of novel properties that are difficult or impossible to find using other means. More generally, the use of AM in a BEAR adds a critical degree of versatility that is analogous to the use of automated liquid handling in chemistry. Building an autonomous research system is most justified in fields such as mechanics where even a single AM instrument is versatile enough to allow a wide range of experiments. In this way, a BEAR may have a transformative impact on mechanics. Last, however, it is worth emphasizing that throughout this work, &gt;2500 experiments were spent, proving that only 32 experiments were required to reach an optimal structure. As an emerging field, it is critical that autonomous experimentation provides these benchmarks to illustrate the possible improvement using BEARs. Looking forward, this validation of the transformative acceleration inherent to BEARs, at least in this class of problems, will allow future work to transition from benchmarking to discovery.</p>
<p>Through the combination of a high-throughput automated experimental system and BO to select experiments, we have developed a BEAR that reduced the experimental time and experiments needed to optimize toughness, a mechanical property that is difficult or impossible to simulate. This work is based on (i) a system that combines an array of 3D printers with robotics and testing equipment such that samples can be tested without human intervention and (ii) a BO framework that guides the action of the high-throughput system. In addition to addressing how to effectively choose a decisionmaking policy when the ground truth function is unknown, the highthroughput nature of this process allowed us to quantify and explore a large parameter space of AM parts. From a learning perspective, realizing a BEAR required advancing several facets including the development of modifications to standard EI algorithms and facile processes for performing BO in batch. Considering the ubiquity of properties that cannot be effectively simulated at present, we anticipate that BEARs based on the principles describe herein could have a transformative impact in mechanics and in fields ranging from chemistry, materials, and biology.</p>
<p>MATERIALS AND METHODS</p>
<p>All structures were printed using a MakerGear M3 FDM printers out of polylactic acid (PLA). The diameters of the printer nozzle and the PLA filament were 0.35 and 1.75 mm, respectively. The structures were printed with a rectilinear infill pattern at 100% infill. During printing, the printer bed was set to 85°C for the first layer and 75°C for all subsequent layers. The PLA filament was extruded at 215°C. After the print completed, the structures were retrieved when the bed temperature was below 40°C. The structures were uniaxially compressed at a speed of 3 mm/min with a maximum force threshold of 4.8 kN. Toughness was computed as the area under the force-displacement curve, where the force-displacement curve was truncated if the force was below 50 N (1% of the maximum allowable force) after an initial 2 mm of displacement. The threshold was used to avoid including the loads from the compression of fragments of the fractured barrel.</p>
<p>Gaussian process priors in the BO framework were specified with a zero-mean function and a squared exponential covariance kernel,
∑(x, x') =  2 exp ( − 1 _ 2 ∑ i=1 d ( x i − x' i ) 2 _  i 2 )
, where x = (n, , r, t). The kernel is parametrized by d + 1 parameters, ,  i , …,  d , where d = 4 is the dimensionality of the design parameter space. The parameters were initialized as  = 50,  i = ( max (x i ) − min (x i ))/(10). In addition, the Gaussian process formulation assumed independent, homoscedastic noise, and the SD of the noise was initialized as 5 J. The parameters of the kernel and the noise were optimized using maximum likelihood estimation after every subsequent observation. The parameters in the optimization were bounded to be greater than or less than their individual initial values by a factor of 10 or were individually reset to their initial values. This was performed to avoid obtaining extremal hyperparameters. The decision-making policy selected the next experiment from a uniformly random finite number of candidate designs ( fig. S4).</p>
<p>For each autonomous experiment ( fig. S1), the crossed barrel design input x = (n, , r, t) was converted to a standard triangle language (STL) file using OpenSCAD, an open-source software for parametric computer-aided design. The generated STL file was then converted to g-code using Slic3r, an open-source tool for converting a 3D model into g-code. The g-code was then uploaded for 3D printing using OctoPrint, a web-based software to interface with the 3D printer. Computed P from six experimental campaigns carried out by the BEAR using (A) MV and (B) EI. (C) Average U measured from 10 samples of the best predicted structure from each of the six experimental campaigns and the best-performance structure predicted by the grid search. (D) Experimental optimization of U versus time T with ticks to the left of each bar denoting measurements taken before that time, ticks to the right denoting the 10 samples taken at the end of the campaign to evaluate the best predicted sample, and bars denoting the average measurement of the 10 samples. In (C) and (D), error bars correspond to SD. (E) Photographs overlaid on the F versus D curve corresponding to a structure printed with the best-performance design (12, 131°, 1.95 mm, 1.4 mm). (Photo credit: Aldair E. Gongora, Boston University).</p>
<p>Fig. 1 .
1BEAR for studying the mechanics of additively manufactured components. (A) Experimental system composed of (i) five dual extruder fused deposition modeling (FDM) printers (M3, MakerGear), (ii) a six-axis robotic arm (UR5e, Universal Robotics), (iii) a scale (CP225D, Sartorius), and (iv) a universal testing machine (5965, Instron Inc.). (Photo credit: Aldair E. Gongora and Bowen Xu, Boston University). (B)</p>
<p>Fig. 2 .
2Experimental exploration of the toughness of a family of parametric structures. (A) Overlaid F versus D curves for 240 samples printed with x = (n, , r, t) = (8,100 ° ,2 mm,1.05 mm). (B) Experimental toughness U versus component mass m for the samples shown in (A). Red line denotes a linear fit with a correlation coefficient of 0.71. (C) U versus m for 1800 samples taken in a grid across the entire parameter space. Marker shape denotes the category of mechanical response. (D) Predicted toughness U grid based on a Gaussian process regression (GPR) trained on the 1800 experimental data points evaluated at x versus average U(x). The red line has zero intercept and a slope of one as a guide to the eye. (E) Surface plot of U grid across the entire 4D parameter space with the discretization of the experimental grid represented as white circles in the top right panel.</p>
<p>Fig. 3 .
3Simulated learning using BO. Distribution of experimental points when guided using (A) MV and (B) EI decision-making policies. The color gradient indicates the start and end of the campaign. Axis limits are the same as in Fig. 2E. Performance P versus experiment number i of simulated Bayesian campaigns with noise added to each simulated measurement drawn from a zero-mean Gaussian with (C) SD  = 0.1 J and (D)  = 5 J. EI-and MV-guided campaigns are benchmarked against PE and the average result of selecting 100 experiments using Latin hypercube sampling (LHS). Shaded regions correspond to the middle two quartiles of 100 simulated campaigns. The inset bar charts show the distribution in P at i = 100.</p>
<p>Fig. 4 .
4Optimization of a family of mechanical structures using the BEAR.
After the structure was printed, the robotic arm retrieved the structure when the bed temperature was below 40°C. The structure was then weighed on the scale and then tested on the universal testing machine (5965, Instron Inc.). The weight reading, the forcedisplacement curve, and the computed toughness were all saved to a local database. Using the database, the BEAR built a belief model using GPR and selected the design parameters of the next experiment using a decision-making policy. This process was repeated for a given experiment budget or experiment run time and was operated without a human in the loop. A custom script written in MATLAB was used to coordinate the operation of these components.SUPPLEMENTARY MATERIALSSupplementary material for this article is available at http://advances.sciencemag.org/cgi/ content/full/6/15/eaaz1708/DC1
Topology optimization approaches: A comparative review. O Sigmund, K Maute, Struct. Multidiscip. Optim. 48O. Sigmund, K. Maute, Topology optimization approaches: A comparative review. Struct. Multidiscip. Optim. 48, 1031-1055 (2013).</p>
<p>Spin-It: Optimizing moment of inertia for spinnable objects. M Bächer, E Whiting, B Bickel, O Sorkine-Hornung, ACM Trans. Graph. 33M. Bächer, E. Whiting, B. Bickel, O. Sorkine-Hornung, Spin-It: Optimizing moment of inertia for spinnable objects. ACM Trans. Graph. 33, 1-10 (2014).</p>
<p>Design of manufacturable 3D extremal elastic microstructure. E Andreassen, B S Lazarov, O Sigmund, Mech. Mater. 69E. Andreassen, B. S. Lazarov, O. Sigmund, Design of manufacturable 3D extremal elastic microstructure. Mech. Mater. 69, 1-10 (2014).</p>
<p>Data-driven bending elasticity design by shell thickness. X Zhang, X Le, Z Wu, E Whiting, C C L Wang, Eurographics Symp. Geom. Process. 35X. Zhang, X. Le, Z. Wu, E. Whiting, C. C. L. Wang, Data-driven bending elasticity design by shell thickness. Eurographics Symp. Geom. Process. 35, 157-166 (2016).</p>
<p>Algorithm-driven design of fracture resistant composite materials realized through additive manufacturing. G X Gu, S Wettermark, M J Buehler, Addit. Manuf. 17G. X. Gu, S. Wettermark, M. J. Buehler, Algorithm-driven design of fracture resistant composite materials realized through additive manufacturing. Addit. Manuf. 17, 47-54 (2017).</p>
<p>Rational design of reconfigurable prismatic architected materials. J T B Overvelde, J C Weaver, C Hoberman, K Bertoldi, Nature. 541J. T. B. Overvelde, J. C. Weaver, C. Hoberman, K. Bertoldi, Rational design of reconfigurable prismatic architected materials. Nature 541, 347-352 (2017).</p>
<p>Simultaneous digital design and additive manufacture of structures and materials. N Boddeti, Z Ding, S Kaijima, K Maute, M L Dunn, Sci. Rep. 815560N. Boddeti, Z. Ding, S. Kaijima, K. Maute, M. L. Dunn, Simultaneous digital design and additive manufacture of structures and materials. Sci. Rep. 8, 15560 (2018).</p>
<p>Computational discovery of extremal microstructure families. D Chen, M Skouras, B Zhu, W Matusik, Sci. Adv. 47005D. Chen, M. Skouras, B. Zhu, W. Matusik, Computational discovery of extremal microstructure families. Sci. Adv. 4, eaao7005 (2018).</p>
<p>Materials-bydesign: Computation, synthesis, and characterization from atoms to structures. J Yeo, G S Jung, F J Martín-Martínez, S Ling, G X Gu, Z Qin, M J Buehler, Phys. Scr. 9353003J. Yeo, G. S. Jung, F. J. Martín-Martínez, S. Ling, G. X. Gu, Z. Qin, M. J. Buehler, Materials-by- design: Computation, synthesis, and characterization from atoms to structures. Phys. Scr. 93, 053003 (2018).</p>
<p>Effect of constituent materials on composite performance: Exploring design strategies via machine learning. C.-T Chen, G X Gu, Adv. Theory Simul. 21900056C.-T. Chen, G. X. Gu, Effect of constituent materials on composite performance: Exploring design strategies via machine learning. Adv. Theory Simul. 2, 1900056 (2019).</p>
<p>Numerical instabilities in topology optimization: A survey on procedures dealing with checkerboards, mesh-dependencies and local minima. O Sigmund, J Petersson, Struct. Optim. 16O. Sigmund, J. Petersson, Numerical instabilities in topology optimization: A survey on procedures dealing with checkerboards, mesh-dependencies and local minima. Struct. Optim. 16, 68-75 (1998).</p>
<p>Topology optimization of nonlinear structures. D Jung, H C Gea, Finite Elem. Anal. Des. 40D. Jung, H. C. Gea, Topology optimization of nonlinear structures. Finite Elem. Anal. Des. 40, 1417-1427 (2004).</p>
<p>Impact mechanics and high-energy absorbing materials: Review. P Qiao, M Yang, F Bobaru, J. Aerosp. Eng. 21P. Qiao, M. Yang, F. Bobaru, Impact mechanics and high-energy absorbing materials: Review. J. Aerosp. Eng. 21, 235-248 (2008).</p>
<p>The conflicts between strength and toughness. R O Ritchie, Nat. Mater. 10R. O. Ritchie, The conflicts between strength and toughness. Nat. Mater. 10, 817-822 (2011).</p>
<p>U G K Wegst, H Bai, E Saiz, A P Tomsia, R O Ritchie, Bioinspired structural materials. 14U. G. K. Wegst, H. Bai, E. Saiz, A. P. Tomsia, R. O. Ritchie, Bioinspired structural materials. Nat. Mater. 14, 23-36 (2015).</p>
<p>Topology optimization of energy absorbing structures with maximum damage constraint. L Li, G Zhang, K , Int. J. Numer. Meth. Engng. 112L. Li, G. Zhang, K. Khandelwal, Topology optimization of energy absorbing structures with maximum damage constraint. Int. J. Numer. Meth. Engng. 112, 737-775 (2017).</p>
<p>Challenges of additive manufacturing technologies from an optimisation perspective. S Guessasma, W Zhang, J Zhu, S Belhabib, H Nouri, Int. J. Simul. Multisci. Des. Optim. 69S. Guessasma, W. Zhang, J. Zhu, S. Belhabib, H. Nouri, Challenges of additive manufacturing technologies from an optimisation perspective. Int. J. Simul. Multisci. Des. Optim. 6, A9 (2015).</p>
<p>I Gibson, D Rosen, B Stucker, Additive Manufacturing Technologies. Springer-Verlag9I. Gibson, D. Rosen, B. Stucker, Additive Manufacturing Technologies (Springer-Verlag, ed. 2, 2015), vol. 9.</p>
<p>Additive manufacturing: Scientific and technological challenges, market uptake and opportunities. S A M Tofail, E P Koumoulos, A Bandyopadhyay, S Bose, L O&apos;donoghue, C Charitidis, Mater. Today. 21S. A. M. Tofail, E. P. Koumoulos, A. Bandyopadhyay, S. Bose, L. O'Donoghue, C. Charitidis, Additive manufacturing: Scientific and technological challenges, market uptake and opportunities. Mater. Today 21, 22-37 (2018).</p>
<p>Functional genomic hypothesis generation and experimentation by a robot scientist. R D King, K E Whelan, F M Jones, P G K Reiser, C H Bryant, S H Muggleton, D B Kell, S G Oliver, Nature. 427R. D. King, K. E. Whelan, F. M. Jones, P. G. K. Reiser, C. H. Bryant, S. H. Muggleton, D. B. Kell, S. G. Oliver, Functional genomic hypothesis generation and experimentation by a robot scientist. Nature 427, 247-252 (2004).</p>
<p>The automation of science. R D King, J Rowland, S G Oliver, M Young, W Aubrey, E Byrne, M Liakata, M Markham, P Pir, L N Soldatova, A Sparkes, K E Whelan, A Clare, Science. 324R. D. King, J. Rowland, S. G. Oliver, M. Young, W. Aubrey, E. Byrne, M. Liakata, M. Markham, P. Pir, L. N. Soldatova, A. Sparkes, K. E. Whelan, A. Clare, The automation of science. Science 324, 85-89 (2009).</p>
<p>Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. K Williams, E Bilsland, A Sparkes, W Aubrey, M Young, L N Soldatova, K De Grave, J Ramon, M De Clare, W Sirawaraporn, S G Oliver, R D King, J. R. Soc. Interface. 1220141289K. Williams, E. Bilsland, A. Sparkes, W. Aubrey, M. Young, L. N. Soldatova, K. De Grave, J. Ramon, M. de Clare, W. Sirawaraporn, S. G. Oliver, R. D. King, Cheaper faster drug development validated by the repositioning of drugs against neglected tropical diseases. J. R. Soc. Interface 12, 20141289 (2015).</p>
<p>Autonomy in materials research: A case study in carbon nanotube growth. P Nikolaev, D Hooper, F Webber, R Rao, K Decker, M Krein, J Poleski, R Barto, B Maruyama, Comput. Mater. 216031P. Nikolaev, D. Hooper, F. Webber, R. Rao, K. Decker, M. Krein, J. Poleski, R. Barto, B. Maruyama, Autonomy in materials research: A case study in carbon nanotube growth. npj Comput. Mater. 2, 16031 (2016).</p>
<p>Controlling an organic synthesis robot with machine learning to search for new reactivity. J M Granda, L Donina, V Dragone, D.-L Long, L Cronin, Nature. 559J. M. Granda, L. Donina, V. Dragone, D.-L. Long, L. Cronin, Controlling an organic synthesis robot with machine learning to search for new reactivity. Nature 559, 377-381 (2018).</p>
<p>Bayesian optimization for materials design. P I Frazier, J Wang, Springer Ser. Mater. Sci. 225P. I. Frazier, J. Wang, Bayesian optimization for materials design. Springer Ser. Mater. Sci. 225, 45-75 (2015).</p>
<p>Taking the human out of the loop: A review of bayesian optimization. B Shahriari, K Swersky, Z Wang, R P Adams, N Freitas, Proc. IEEE. 104B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, N. de Freitas, Taking the human out of the loop: A review of bayesian optimization. Proc. IEEE 104, 148-175 (2016).</p>
<p>Nested-Batch-Mode learning and stochastic optimization with an application to sequential MultiStage testing in materials science. Y Wang, K G Reyes, K A Brown, C A Mirkin, W B Powell, SIAM J. Sci. Comput. 37Y. Wang, K. G. Reyes, K. A. Brown, C. A. Mirkin, W. B. Powell, Nested-Batch-Mode learning and stochastic optimization with an application to sequential MultiStage testing in materials science. SIAM J. Sci. Comput. 37, 361-381 (2015).</p>
<p>F Häse, L M Roch, C Kreisbeck, A Aspuru-Guzik, Phoenics: A bayesian optimizer for chemistry. 4F. Häse, L. M. Roch, C. Kreisbeck, A. Aspuru-Guzik, Phoenics: A bayesian optimizer for chemistry. ACS Cent. Sci. 4, 1134-1145 (2018).</p>
<p>Plastic deformation, failure and energy absorption of sandwich structures with metallic cellular cores. F Zhu, G Lu, D Ruan, Z Wang, Int. J. Prot. Struct. 1F. Zhu, G. Lu, D. Ruan, Z. Wang, Plastic deformation, failure and energy absorption of sandwich structures with metallic cellular cores. Int. J. Prot. Struct. 1, 507-541 (2010).</p>
<p>Printing nature: Unraveling the role of nacre's mineral bridges. G X Gu, F Libonati, S D Wettermark, M J Buehler, J. Mech. Behav. Biomed. Mater. 76G. X. Gu, F. Libonati, S. D. Wettermark, M. J. Buehler, Printing nature: Unraveling the role of nacre's mineral bridges. J. Mech. Behav. Biomed. Mater. 76, 135-144 (2017).</p>
<p>On the convergence rates of expected improvement methods. I O Ryzhov, Oper. Res. 64I. O. Ryzhov, On the convergence rates of expected improvement methods. Oper. Res. 64, 1515-1528 (2016).</p>
<p>Practical Bayesian optimization of machine learning algorithms. J Snoek, H Larochelle, R P Adams, Adv. Neural Inf. Process. Syst. J. Snoek, H. Larochelle, R. P. Adams, Practical Bayesian optimization of machine learning algorithms. Adv. Neural Inf. Process. Syst. 2951-2959 (2012).</p>
<p>atch Bayesian optimization via local penalization. J González, Z Dai, P Hennig, N D Lawrence, AISTATS. 648657J. González, Z. Dai, P. Hennig, N. D. Lawrence, atch Bayesian optimization via local penalization. AISTATS 648-657 (2016).</p>
<p>Parallelised Bayesian optimisation via Thompson sampling. K Kandasamy, A Krishnamurthy, J Schneider, B Poczos, International Conference on Artificial Intelligence and Statistics. K. Kandasamy, A. Krishnamurthy, J. Schneider, B. Poczos, Parallelised Bayesian optimisation via Thompson sampling, in International Conference on Artificial Intelligence and Statistics (2018), pp. 133-142.</p>
<p>Asynchronous batch Bayesian optimisation with improved local penalisation. A S Alvi, B Ru, J Calliess, S J Roberts, M A Osborne, International Conference on Machine Learning. PMLRA. S. Alvi, B. Ru, J. Calliess, S. J. Roberts, M. A. Osborne, Asynchronous batch Bayesian optimisation with improved local penalisation, in International Conference on Machine Learning (PMLR, 2019), pp. 253-262.</p>            </div>
        </div>

    </div>
</body>
</html>
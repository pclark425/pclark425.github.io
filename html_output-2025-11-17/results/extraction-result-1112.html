<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1112 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1112</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1112</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-25.html">extraction-schema-25</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <p><strong>Paper ID:</strong> paper-12346678</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1710.02210v1.pdf" target="_blank">Exploration in Feature Space for Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> The infamous exploration-exploitation dilemma is one of the oldest and most important problems in reinforcement learning (RL). Deliberate and effective exploration is necessary for RL agents to succeed in most environments. However, until very recently even very sophisticated RL algorithms employed simple, undirected exploration strategies in large-scale RL tasks. We introduce a new optimistic count-based exploration algorithm for RL that is feasible in high-dimensional MDPs. The success of RL algorithms in these domains depends crucially on generalization from limited training experience. Function approximation techniques enable RL agents to generalize in order to estimate the value of unvisited states, but at present few methods have achieved generalization about the agent's uncertainty regarding unvisited states. We present a new method for computing a generalized state visit-count, which allows the agent to estimate the uncertainty associated with any state. In contrast to existing exploration techniques, our $\phi$-$\textit{pseudocount}$ achieves generalization by exploiting the feature representation of the state space that is used for value function approximation. States that have less frequently observed features are deemed more uncertain. The resulting $\phi$-$\textit{Exploration-Bonus}$ algorithm rewards the agent for exploring in feature space rather than in the original state space. This method is simpler and less computationally expensive than some previous proposals, and achieves near state-of-the-art results on high-dimensional RL benchmarks. In particular, we report world-class results on several notoriously difficult Atari 2600 video games, including Montezuma's Revenge.</p>
                <p><strong>Cost:</strong> 0.015</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1112.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1112.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI agents using adaptive experimental design methods in unknown or partially observable environments, including the specific adaptation strategies, environment characteristics, and performance results.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>φ-EB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>phi-Exploration Bonus (φ-EB)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A directed count-based exploration algorithm that computes generalized visit-counts in the agent's feature space by maintaining a factorized feature visit-density (KT estimators per feature) and deriving φ-pseudocounts to produce an optimism-based intrinsic exploration bonus compatible with linear function approximation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>SARSA(λ) + φ-EB (denoted SARSA-φ-EB)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A value-based RL agent using SARSA(λ) with Linear Function Approximation (LFA, Blob-PROST features) for Q-value estimation, augmented by an intrinsic exploration module φ-EB which computes feature-visit densities (factorized per feature using KT estimators), derives φ-pseudocounts and produces an exploration bonus added to the environment reward; action selection uses greedy-on-optimistic-Q with added stochasticity via annealed ε-greedy or Boltzmann sampling (on exploratory Q-component).</td>
                        </tr>
                        <tr>
                            <td><strong>adaptive_design_method</strong></td>
                            <td>Optimism in the Face of Uncertainty implemented as count-based intrinsic motivation (φ-pseudocounts) / directed exploration (count-based OFU); related to intrinsic motivation / information-seeking heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_strategy_description</strong></td>
                            <td>The agent builds and updates a sequential density model over feature vectors (ρ_t(φ) = ∏_i ρ_{i,t}(φ_i)) using Krichevsky–Trofimov (KT) estimators per feature; from the density it computes a φ-pseudocount N^φ_t(s) (per Bellemare et al.'s pseudocount derivation) and computes an exploration bonus (same functional form as MBIE-EB, controlled by hyperparameter β) that is added to extrinsic reward. The bonus decreases as features become familiar (pseudocount increases), thus steering action selection toward novel feature combinations; stochasticity is added via annealed ε-greedy or Boltzmann sampling over the exploratory-value component to avoid deterministic loops.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Arcade Learning Environment (Atari 2600) — selected games: Montezuma's Revenge, Venture, Freeway, Frostbite, Q*bert</td>
                        </tr>
                        <tr>
                            <td><strong>environment_characteristics</strong></td>
                            <td>Unknown (model-free) high-dimensional discrete MDPs presented as raw frames (processed into features); large / sparse state spaces, sparse and dense reward variants (Montezuma's Revenge & Venture: sparse-reward, long-horizon credit assignment; Freeway: high feature churn; Frostbite & Q*bert: denser rewards or other difficulties); stochasticity from game dynamics; episodic; partial observability not explicitly assumed but practical perceptual aliasing and high dimensionality make generalization necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_complexity</strong></td>
                            <td>Large discrete action set (standard Atari 18 discrete actions) and very high-dimensional observation space (raw frames 160x210 with 7-bit pixels, but reduced to Blob-PROST binary features — Blob-PROST has ~114.7M possible binary features though only a sparse subset is active per step); episodes vary by game length; training was performed up to 100 million frames (80M for Q*bert) per agent run; number of observed unique features M_t varies per game (sparse feature vectors exploited for O(M_t) runtime).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_adaptive_design</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_adaptation</strong></td>
                            <td>Montezuma's Revenge: peak reported score 6600, average evaluation score reported 2745.4 after training (evaluation protocol: trained up to 100M frames, evaluated over 500 episodes); visited 14 distinct rooms (vs prior best 15 rooms by Bellemare et al. 2016). Venture: average evaluation score 1169.2 after 100M frames. Freeway: required β tuning (β=0.035) to obtain nonzero scores; Q*bert and Frostbite: modest improvements over SARSA-ε but less dramatic (Q*bert trained 80M frames). Results reported as average episodic scores; plots show learning curves with mean and ±1 std.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_adaptation</strong></td>
                            <td>Baseline SARSA(λ)+ε-greedy: Montezuma's Revenge and Venture scored near 0 (failed to obtain rewards) under comparable training (SARSA-ε often failed to discover key sequences); in other games (Frostbite, Q*bert, Freeway with tuned β) SARSA-ε achieved lower or similar scores; specific numeric baselines: SARSA-ε converged (in Montezuma case) to policies that sometimes obtain the first key rarely but overall scores remained ≈0 in evaluation described.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training budgets used: typically 100 million frames per agent (80M for Q*bert); φ-EB discovered substantial progress in Montezuma's Revenge by ~40M frames (peak score observed around 40M frames), while baseline remained effectively stuck; evaluation used 500-episode averages after training. Multiple independent trials: Montezuma's Revenge and Venture each run for 5 independent trials; other games 2 trials. This indicates φ-EB achieves nontrivial exploration behavior and positive rewards within tens of millions of frames where baseline random/ε-greedy does not.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_exploitation_tradeoff</strong></td>
                            <td>Balance implemented via OFU-style intrinsic bonus derived from φ-pseudocounts: intrinsic reward R^φ_t(s,a) (functional form same as MBIE-EB, controlled by hyperparameter β) is added to extrinsic reward, making the agent greedy with respect to augmented Q; β controls degree of optimism (higher β => more exploration). To avoid deterministic exploitation of spurious high Q-values early in training, stochasticity is injected either by annealed ε-greedy (GLIE) or by Boltzmann sampling over an exploration-value component Q^I (agent maintains separate θ_E and θ_I for extrinsic and intrinsic value) so exploratory actions are biased but not uniform random.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_methods</strong></td>
                            <td>Compared against SARSA(λ)+ε-greedy (baseline), and contrasted in discussion with leading count-based and intrinsic-exploration methods from literature: DQN+CTS-EB (Bellemare et al., 2016), DQN+neural density / PixelCNN methods (Ostrovski et al., 2017), #Exploration (Locality Sensitive Hashing) (Tang et al., 2016), MP-EB (Stadie et al., 2015), MBIE-EB (Strehl & Littman, 2008), and standard DQN variants (DQN, Double DQN, A3C etc.) as reported in literature tables.</td>
                        </tr>
                        <tr>
                            <td><strong>key_results</strong></td>
                            <td>Measuring novelty in the same feature space used for value approximation yields efficient generalization of visit counts and strong empirical gains in hard sparse-reward Atari domains: SARSA-φ-EB achieves state-of-the-art or near-state-of-the-art results on Montezuma's Revenge (peak 6600, 14 rooms visited; avg eval score 2745.4) and Venture (avg eval 1169.2), dramatically outperforming SARSA-ε (which scored ≈0) and outperforming several prior exploration strategies on these games. φ-EB is computationally efficient (time complexity O(M_t) per bonus computation, space O(M) for factor densities), domain-independent (works with any LFA feature map), and avoids separate exploration-specific representation by reusing value features.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Requires an explicit feature map (designed for LFA); performance depends on the relevance of features to value (relies on Blob-PROST in experiments); independence assumption in factorized density (ρ = ∏ ρ_i) is an approximation that may ignore feature correlations — authors argue empirically it's acceptable. Hyperparameter sensitivity: β needs tuning per-game family (e.g., Freeway required β reduction from 0.05 to 0.035 to avoid staying still and harvesting novelty). Less dramatic gains in games where non-linear function approximation provides strong advantages (e.g., Q*bert where deep networks excel). Also, although computationally cheaper than full state density models, φ-EB still requires maintaining factor statistics for many observed features and can be slow on games with huge observed feature sets (training took weeks on some games).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unifying Count-Based Exploration and Intrinsic Motivation <em>(Rating: 2)</em></li>
                <li>Count-Based Exploration with Neural Density Models <em>(Rating: 2)</em></li>
                <li>#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning <em>(Rating: 2)</em></li>
                <li>Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models <em>(Rating: 2)</em></li>
                <li>An analysis of model-based Interval Estimation for Markov Decision Processes <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1112",
    "paper_id": "paper-12346678",
    "extraction_schema_id": "extraction-schema-25",
    "extracted_data": [
        {
            "name_short": "φ-EB",
            "name_full": "phi-Exploration Bonus (φ-EB)",
            "brief_description": "A directed count-based exploration algorithm that computes generalized visit-counts in the agent's feature space by maintaining a factorized feature visit-density (KT estimators per feature) and deriving φ-pseudocounts to produce an optimism-based intrinsic exploration bonus compatible with linear function approximation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "SARSA(λ) + φ-EB (denoted SARSA-φ-EB)",
            "agent_description": "A value-based RL agent using SARSA(λ) with Linear Function Approximation (LFA, Blob-PROST features) for Q-value estimation, augmented by an intrinsic exploration module φ-EB which computes feature-visit densities (factorized per feature using KT estimators), derives φ-pseudocounts and produces an exploration bonus added to the environment reward; action selection uses greedy-on-optimistic-Q with added stochasticity via annealed ε-greedy or Boltzmann sampling (on exploratory Q-component).",
            "adaptive_design_method": "Optimism in the Face of Uncertainty implemented as count-based intrinsic motivation (φ-pseudocounts) / directed exploration (count-based OFU); related to intrinsic motivation / information-seeking heuristics.",
            "adaptation_strategy_description": "The agent builds and updates a sequential density model over feature vectors (ρ_t(φ) = ∏_i ρ_{i,t}(φ_i)) using Krichevsky–Trofimov (KT) estimators per feature; from the density it computes a φ-pseudocount N^φ_t(s) (per Bellemare et al.'s pseudocount derivation) and computes an exploration bonus (same functional form as MBIE-EB, controlled by hyperparameter β) that is added to extrinsic reward. The bonus decreases as features become familiar (pseudocount increases), thus steering action selection toward novel feature combinations; stochasticity is added via annealed ε-greedy or Boltzmann sampling over the exploratory-value component to avoid deterministic loops.",
            "environment_name": "Arcade Learning Environment (Atari 2600) — selected games: Montezuma's Revenge, Venture, Freeway, Frostbite, Q*bert",
            "environment_characteristics": "Unknown (model-free) high-dimensional discrete MDPs presented as raw frames (processed into features); large / sparse state spaces, sparse and dense reward variants (Montezuma's Revenge & Venture: sparse-reward, long-horizon credit assignment; Freeway: high feature churn; Frostbite & Q*bert: denser rewards or other difficulties); stochasticity from game dynamics; episodic; partial observability not explicitly assumed but practical perceptual aliasing and high dimensionality make generalization necessary.",
            "environment_complexity": "Large discrete action set (standard Atari 18 discrete actions) and very high-dimensional observation space (raw frames 160x210 with 7-bit pixels, but reduced to Blob-PROST binary features — Blob-PROST has ~114.7M possible binary features though only a sparse subset is active per step); episodes vary by game length; training was performed up to 100 million frames (80M for Q*bert) per agent run; number of observed unique features M_t varies per game (sparse feature vectors exploited for O(M_t) runtime).",
            "uses_adaptive_design": true,
            "performance_with_adaptation": "Montezuma's Revenge: peak reported score 6600, average evaluation score reported 2745.4 after training (evaluation protocol: trained up to 100M frames, evaluated over 500 episodes); visited 14 distinct rooms (vs prior best 15 rooms by Bellemare et al. 2016). Venture: average evaluation score 1169.2 after 100M frames. Freeway: required β tuning (β=0.035) to obtain nonzero scores; Q*bert and Frostbite: modest improvements over SARSA-ε but less dramatic (Q*bert trained 80M frames). Results reported as average episodic scores; plots show learning curves with mean and ±1 std.",
            "performance_without_adaptation": "Baseline SARSA(λ)+ε-greedy: Montezuma's Revenge and Venture scored near 0 (failed to obtain rewards) under comparable training (SARSA-ε often failed to discover key sequences); in other games (Frostbite, Q*bert, Freeway with tuned β) SARSA-ε achieved lower or similar scores; specific numeric baselines: SARSA-ε converged (in Montezuma case) to policies that sometimes obtain the first key rarely but overall scores remained ≈0 in evaluation described.",
            "sample_efficiency": "Training budgets used: typically 100 million frames per agent (80M for Q*bert); φ-EB discovered substantial progress in Montezuma's Revenge by ~40M frames (peak score observed around 40M frames), while baseline remained effectively stuck; evaluation used 500-episode averages after training. Multiple independent trials: Montezuma's Revenge and Venture each run for 5 independent trials; other games 2 trials. This indicates φ-EB achieves nontrivial exploration behavior and positive rewards within tens of millions of frames where baseline random/ε-greedy does not.",
            "exploration_exploitation_tradeoff": "Balance implemented via OFU-style intrinsic bonus derived from φ-pseudocounts: intrinsic reward R^φ_t(s,a) (functional form same as MBIE-EB, controlled by hyperparameter β) is added to extrinsic reward, making the agent greedy with respect to augmented Q; β controls degree of optimism (higher β =&gt; more exploration). To avoid deterministic exploitation of spurious high Q-values early in training, stochasticity is injected either by annealed ε-greedy (GLIE) or by Boltzmann sampling over an exploration-value component Q^I (agent maintains separate θ_E and θ_I for extrinsic and intrinsic value) so exploratory actions are biased but not uniform random.",
            "comparison_methods": "Compared against SARSA(λ)+ε-greedy (baseline), and contrasted in discussion with leading count-based and intrinsic-exploration methods from literature: DQN+CTS-EB (Bellemare et al., 2016), DQN+neural density / PixelCNN methods (Ostrovski et al., 2017), #Exploration (Locality Sensitive Hashing) (Tang et al., 2016), MP-EB (Stadie et al., 2015), MBIE-EB (Strehl & Littman, 2008), and standard DQN variants (DQN, Double DQN, A3C etc.) as reported in literature tables.",
            "key_results": "Measuring novelty in the same feature space used for value approximation yields efficient generalization of visit counts and strong empirical gains in hard sparse-reward Atari domains: SARSA-φ-EB achieves state-of-the-art or near-state-of-the-art results on Montezuma's Revenge (peak 6600, 14 rooms visited; avg eval score 2745.4) and Venture (avg eval 1169.2), dramatically outperforming SARSA-ε (which scored ≈0) and outperforming several prior exploration strategies on these games. φ-EB is computationally efficient (time complexity O(M_t) per bonus computation, space O(M) for factor densities), domain-independent (works with any LFA feature map), and avoids separate exploration-specific representation by reusing value features.",
            "limitations_or_failures": "Requires an explicit feature map (designed for LFA); performance depends on the relevance of features to value (relies on Blob-PROST in experiments); independence assumption in factorized density (ρ = ∏ ρ_i) is an approximation that may ignore feature correlations — authors argue empirically it's acceptable. Hyperparameter sensitivity: β needs tuning per-game family (e.g., Freeway required β reduction from 0.05 to 0.035 to avoid staying still and harvesting novelty). Less dramatic gains in games where non-linear function approximation provides strong advantages (e.g., Q*bert where deep networks excel). Also, although computationally cheaper than full state density models, φ-EB still requires maintaining factor statistics for many observed features and can be slow on games with huge observed feature sets (training took weeks on some games).",
            "uuid": "e1112.0"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unifying Count-Based Exploration and Intrinsic Motivation",
            "rating": 2,
            "sanitized_title": "unifying_countbased_exploration_and_intrinsic_motivation"
        },
        {
            "paper_title": "Count-Based Exploration with Neural Density Models",
            "rating": 2,
            "sanitized_title": "countbased_exploration_with_neural_density_models"
        },
        {
            "paper_title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning",
            "rating": 2,
            "sanitized_title": "exploration_a_study_of_countbased_exploration_for_deep_reinforcement_learning"
        },
        {
            "paper_title": "Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models",
            "rating": 2,
            "sanitized_title": "incentivizing_exploration_in_reinforcement_learning_with_deep_predictive_models"
        },
        {
            "paper_title": "An analysis of model-based Interval Estimation for Markov Decision Processes",
            "rating": 1,
            "sanitized_title": "an_analysis_of_modelbased_interval_estimation_for_markov_decision_processes"
        }
    ],
    "cost": 0.015169499999999999,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploration in Feature Space for Reinforcement Learning Declaration
2017. August 19-25 2017. 2017</p>
<p>; Suraj Narayanan Sasikumar 
Martin 
Exploration in Feature Space for Reinforcement Learning Declaration</p>
<p>the 26th International Joint Conference on Artificial Intelligence (IJCAI)
Melbourne, Australia2017. August 19-25 2017. 2017The work presented in this thesis represents work conducted between July 2016 and May 2017. This work has been accepted for publication as "Count-Based Exploration in Feature Space for Reinforcement</p>
<p>• My supervisor Marcus Hutter, whose lectures inspired me to pursue Reinforcement Learning. Hearing him talk about Mathematics and AI with childlike enthusiasm has always been an inspiration to me.</p>
<p>• Tom Everitt, his steadfast and professional approach to supervising my thesis helped me a great deal in making this a success.</p>
<p>• Jarryd Martin, my collaborator in this Project. Words cannot express the happiness that I feel, to have a friend in you. The days we toiled over broken theories and random agents were not for nothing. We did it. . . and it could never have been achieved without you. Your determination, motivation and quite frankly the sheer energy is something that I aspire to.</p>
<p>• John Aslanides, first a huge thank-you for talking your time to provide your comments and feedback on this thesis. Your rationality and cut-to-the-chase attitude. Your clarity of thought, work-ethic and passion for growth has been inspirational for me.</p>
<p>• Lulu Huang, for talking care of me throughout this whole endeavour, and being a wonderful roommate.</p>
<p>• Boris Repasky, the man with infinite rigour. All the arguments and debates we've had throughout the year has only been for the better. Sina Eghbal, for being there through tough times as an unassuming friend. Darren Lawton, for forcing me to play Basketball. My AI Labmates, Sultan Javed, Owen Cameron, Arie Slobbe, Elliot Catt, for the motivation and support.</p>
<p>• My father Sasikumar, sister Sumitha, for believing in me and being a vocal supporter of my decisions. My in-laws Rasmi, M Vasudevan, and Geetha, for supporting my decision to study. I miss you all very much.</p>
<p>• My wife Ramya Vasudevan, her sacrifices and compromises are the reason I am able to do what I want to do, and I am forever indebted. None of this would even make sense without you in my life.</p>
<p>Introduction</p>
<p>'No great discovery was ever made without a bold guess.'</p>
<p>Isaac Newton</p>
<p>Reinforcement Learning</p>
<p>Machine Learning is a field in computer science that allows computers to dynamically generate novel algorithms that otherwise cannot be explicitly programmed. These algorithms, called hypotheses, generalize patterns and regularities from observed realworld data using statistical techniques (Bishop, 2007). Reinforcement Learning (RL) is a field of machine learning that deals with optimal sequential decision making in an unknown environment with no explicitly labelled training data. The RL framework is one of the fundamental models that best describes how intelligent beings interact with their world to achieve a goal. An RL algorithm is given agency to interact with its surroundings, and is aptly called an agent. The world with which the agent interacts is called its environment. The unsupervised nature of RL algorithms means that the agent has to develop a policy for acting in an unknown environment by trialand-error (Sutton and Barto, 1998). In every such interaction the agent performs an action on the environment and receives a percept. The percept consists of the current configuration of the environment, called state, and a scalar feedback signal, called reward. The reward signal indicates how good the sequence of actions of the agent was. The goal of an RL agent is based on the concept of the reward hypothesis:</p>
<p>Definition 1 (Reward Hypothesis). (Sutton, 1999) Any notion of a goal or purpose of an intelligent agent can be described as the maximization of expected cumulative reward.</p>
<p>The existence of an extrinsic feedback signal makes RL algorithms also somewhat supervised in nature -thus RL algorithms are in some sense both supervised and unsupervised (Barto and Dietterich, 2004).</p>
<p>As an example, consider an agent playing a car racing game in which the goal is to reach the finish line as soon as possible. To model the goal as a cumulative reward maximization problem, we give the agent a negative reward every time step, thereby Introduction incentivizing the agent to reach the finish line as quickly as possible. This example illustrates how an objective can be modelled as the maximization of expected cumulative reward, and the goal of an agent as a sequence of actions that achieves it. The interaction between agent and environment continues until the agent converges to an optimal sequence of actions for each state in the environment. This interaction is called the agent-environment interaction cycle, as illustrated in Figure 1.1. Each iteration of the interaction is called a time-step, often denoted by the subscript t to distinguish states, actions, and percepts between time-steps.  (Sutton and Barto, 1998) </p>
<p>The Exploration/Exploitation Dilemma</p>
<p>In an online decision-making setting such as the reinforcement learning problem, an agent is faced with two choices -explore or exploit. The term exploration in an active learning system is defined as the process of deliberately taking a non-greedy action with the sole aim of gathering more information about the environment. Exploration plays a fundamental role in reinforcement learning algorithms. It is born out of the notion that an optimal long-term policy might involve short-term sacrifices. Alternatively, exploitation is the act of taking the best possible action given the current information about the environment. A central challenge in reinforcement learning is to find the sweet spot between exploration and exploitation, i.e., to figure out when to explore and when to exploit. This problem is known as the exploration-exploitation dilemma.</p>
<p>At present there are a number of provably efficient exploration methods that are effective in environments with low-dimensional state-action spaces. Most of the exploration algorithms which enjoy strong theoretical guarantees implement the socalled "Optimism in the Face of Uncertainty" (OFU) principle. This heuristic encourages the agent to be optimistic about the reward it might attain in less explored parts of the environment. The agent seeks out states with higher associated uncertainty, and in doing so reduces its uncertainty in a very efficient way. Many algorithms that implement this heuristic do so by adding an exploration bonus to the agent's reward signal. This bonus is usually a function of a state visit-count; the agent receives higher exploration bonuses for exploring less frequently visited states (about which it is less certain).</p>
<p>Unfortunately, these algorithms do not scale well to high-dimensional environments. In these domains, the agent can only visit a small portion of the state space while it is training. The visit-count for most states is always zero, even after training is finished. Nearly all states will be assigned the same exploration bonus throughout training. This renders the bonus useless as a tool for efficient exploration. All unvisited states appear to the agent as equally uncertain. This problem arises because these count-based OFU algorithms fail to generalise the agent's uncertainty from one context to another. Even if an unvisited state has very similar features to a frequently visited one, the agent will treat the former as a complete unknown. Consequently even the sophisticated algorithms that are suitable for the high-dimensional settinge.g. those that use deep neural networks for policy evaluation -tend to use simple, inefficient exploration strategies.</p>
<p>Success in the high-dimensional setting demands that the agent represent the state space in a way that allows generalisation about uncertainty. This sort of generalisation would allow that the agent's uncertainty be lower for states with familiar features, and higher for states with novel features, even if those exact states haven't been visited. What we require, then, is an efficient method for computing a suitable similarity measure for states. That is the key challenge addressed in this thesis.</p>
<p>Summary of Contributions</p>
<p>This thesis presents a new count-based exploration algorithm that is feasible in environments with large state-action spaces. It can be combined with any value-based RL algorithm that uses linear function approximation (LFA). The principal contribution is a new method for computing generalised visit-counts. Following Bellemare et al. (2016), we construct a visit-density model in order to measure the similarity between states. Our approach departs from theirs in that we do not construct our density model over the raw state space. Instead, we exploit the feature map that is used for value function approximation, and construct a density model over the transformed feature space. This model assigns higher probability to state feature vectors that share features with visited states. Generalised visit-counts are then computed from these probabilities; states with frequently observed features are assigned higher counts. These counts serve as a measure of the uncertainty associated with a state. Exploration bonuses are then computed from these counts in order to encourage the agent to visit regions of the state-space with less familiar features.</p>
<p>Our density model can be trivially derived from any feature map used for LFA, regardless of the application domain, and requires little or no additional design. In contrast to existing algorithms, there is no need to perform a special dimensionality reduction of the state space in order to compute our generalised visit-counts. Our method uses the same lower-dimensional feature representation to estimate value and to estimate uncertainty. This makes it simpler to implement and less computationally expensive than some existing proposals. Our evaluation demonstrates that Introduction this simple approach achieves near state-of-the-art performance on high-dimensional RL benchmarks.</p>
<p>'Each night, when I go to sleep, I die. And the next morning, when I wake up, I am reborn.'</p>
<p>Mahatma Gandhi</p>
<p>In this chapter we give a formal treatment of the RL problem. We then provide a taxonomy of RL algorithms and the challenges posed by classical RL algorithms. Further, we talk about relevant research findings on how to solve these challenges.</p>
<p>Classical Reinforcement Learning</p>
<p>In Classical RL (CRL), the environment is assumed to be fully observable, ergodic, and every state has the Markov property. The branch of reinforcement learning where these assumptions are lifted is called General Reinforcement Learning (GRL) (Hutter, 2005).</p>
<p>Definition 2 (Markov property). Future states are only dependent on the current states and action, and are independent of the history of percepts. Formally, P(s t+1 = s , r t+1 = r | s t , a t , r t , s t−1 , a t−1 , . . . , r 1 , s 0 , a 0 ) = P(s t+1 = s , r t+1 = r | s t , a t ) for all s , r, and histories s t , a t , r t , s t−1 , a t−1 , . . . , r 1 , s 0 , a 0 A Markov Decision Process (MDP) captures the above assumptions about the environment, and so in the CRL context the environment is modelled as an MDP (Puterman, 1994). Thus, the CRL problem now reduces to the problem of finding an optimal policy for an unknown MDP.</p>
<p>Definition 3 (Markov Decision Process). A Markov Decision</p>
<p>Process is a Tuple S, A, P, R, γ representative of a fully-observable environment in which all states are Markov.</p>
<p>• S is a finite set of states • A is a finite set of actions 6 Background and Related Work • P a ss = P[s t+1 = s | s t = s, a t = a] are the transition probabilities • R a ss = E[r t+1 | s t = s, a t = a, s t+1 = s ] is the expected value of the reward resulting from the transition s, a, s • γ is the discount factor which weights the relative importance of immediate rewards to future rewards.</p>
<p>If the dynamics (transition and reward distributions) of the MDP are known, then we can use dynamic programming methods to directly plan on the MDP to find an optimal policy. In the RL context, in which the system dynamics are unknown, we have to use iterative RL algorithms such as TD-learning (Sutton, 1988) to find a good policy asymptotically. 1 Definition 4 (Policy). A policy may be deterministic or stochastic. A deterministic policy is a mapping from the states to actions.</p>
<p>π : S → A</p>
<p>A stochastic policy is a probability distribution over the set of actions given a state.</p>
<p>π(a | s t = s)</p>
<p>Value</p>
<p>The most common way to characterize the quality of a given policy is to define a function that computes how valuable it is to follow the policy from a given state (or state-action pair). This notion of value is expressed in terms of future rewards the agent could expect, if it had chosen to follow the given policy.</p>
<p>Definition 5 (State-Value Function). The state-value function, V π (s) is a mapping from states to R. The value of a state s ∈ S under policy π is the expected discounted cumulative reward given that the agent starts in state s and follows policy π thereafter.
V π (s) = E π ∞ ∑ k=0 γ k r t+k+1 | s t = s
Definition 6 (Action-Value Function). The action-value function, Q π (s, a) is a mapping from state-action pairs to R. The action-value of the state-action pair (s, a) under policy π is the expected discounted cumulative reward given that the agent starts in state s, takes action a, and follows policy π thereafter.
Q π (s, a) = E π ∞ ∑ k=0 γ k r t+k+1 | s t = s, a t = a
1 Asymptotic analysis is one of the few theoretical tools we have to analyse RL algorithms in a domain-agnostic way.</p>
<p>Bellman Equations</p>
<p>Bellman equations form the basis for how to compute, approximate, and learn value functions in the RL setup (Sutton and Barto, 1998). They arise naturally from the structure of an MDP by capturing the recursive relationship between the value of a state and the value of its successor states. The two Bellman equations for the statevalues and action-values can be defined as follows.</p>
<p>Definition 7 (Bellman Equation for state-value function of an MDP).
V π (s) = ∑ a∈A π(a | s) ∑ s ∈S P a
ss R a ss + γV π (s ) Definition 8 (Bellman Equation for action-value function of an MDP).
Q π (s, a) = ∑ s ∈S P a ss R a ss + γ ∑ a ∈A π(a | s )Q π (s , a )
We can now use the value function to define a partial ordering over policies. A policy is said to be better than another when the expected return of one policy is greater than or equal to the other for all states. Formally, π π ⇐⇒ V π (s) ≥ V π (s) ∀s ∈ S. From the imposed partial ordering it has been shown that there exists at least one policy, π * , such that π * π for all policies π, although it might not be unique (Bertsekas and Tsitsiklis, 1996). The Bellman Optimality Equations provide a mathematical framework for talking about the optimal policy just by replacing the sum over actions with a max operator. Intuitively, this represents a policy that is greedy with respect to the value of its successor states.</p>
<p>Definition 9 (Bellman Optimality Equation for state-values).</p>
<p>V π * (s) ≡ V * (s) = max a∈A ∑ s ∈S P a ss R a ss + γV * (s ) Definition 10 (Bellman Optimality Equation for action-values).
Q π * (s, a) ≡ Q * (s, a) = ∑ s ∈S P a ss R a ss + γ max a ∈A Q * (s , a )
For finite MDPs with known environment dynamics, the Bellman Optimality Equations have a unique solution. Unfortunately in the RL setup we deal with an unknown MDP. Thus, almost all of the RL algorithms approximate the Bellman Optimality Equations for an unknown MDP and try to iteratively find an optimal policy asymptotically.</p>
<p>Reinforcement Learning Algorithms</p>
<p>The fundamental difference between an RL problem and a planning problem is the knowledge of the environment dynamics. In a planning problem the model of the environment is already known and the problem boils down to finding an optimal policy in the environment. In an RL problem, the agent is dropped into an unknown environment the dynamics of which is unknown. This makes reinforcement learning a hard problem. This distinction gives rise to two categories of RL algorithms, namely model-based and model-free.</p>
<p>The class of algorithms that learns the model of the environment, and then does planning within the learned model are called model-based RL algorithms. These algorithms learn the transition probabilities (P a ss ) and reward functions (R a ss ) of the MDP by iteratively simulating the environment and updating the simulation to better represent the true environment. This approach to solve unknown MDP's is computationally intensive, especially in large or continuous problems. Value iteration and policy iteration are two dynamic programming algorithms that have a planningbased approach to the RL problem. On the other hand, model-free algorithms directly learn the optimal policy using an intermediary quantity (usually the value-function).</p>
<p>Generalized Policy Iteration (GPI)</p>
<p>The overarching theme of almost all value-function based CRL algorithms is the backand-forth between two interacting processes, prediction and control, eventually resulting in convergence. Prediction refers to policy-evaluation where the value-function is estimated for the current policy. Control on the other hand aims to find a policy that is greedy with respect to the current value-function (state-value or action-value).  (Sutton and Barto, 1998) The 'Prediction and Control' process converges when it produces no significant change, that is, the value-function is consistent with the current policy and the policy is greedy with respect to the current value-function.</p>
<p>Temporal Difference Learning</p>
<p>Temporal Difference learning (TD learning) is a common RL algorithm; it is a modelfree algorithm that combines Monte Carlo methods with the ideas from dynamic programming. TD learning allows the agent to directly learn from its experience of the environment. Following the GPI theme, we need a strategy for prediction and control. In TD prediction we use the sampling of Monte Carlo methods and bootstrapping (updating from an existing estimate) of DP algorithms to estimate the current value-function.</p>
<p>Definition 11 (Update formula for state-value function).
V(s t ) ← V(s t ) + α[r t+1 + γV(s t+1 ) − V(s t )]
TD(0) is a TD learning algorithm that updates state-values after each time-step, so the learning process is fast and on-line. The target for the TD(0) update formula uses the existing estimate of V(s t+1 ), hence we say the algorithm bootstraps.</p>
<p>As the agent interacts with the environment more, TD learning is able to generate a better estimate of the value-functions. In the limit, if each state (or state-action pair) is visited infinitely often with some additional constraints on the learning rates, convergence to the true value-function is guaranteed (Bertsekas and Tsitsiklis, 1996).</p>
<p>In TD control we want to optimize the value-function of an unknown environment. There are two classes of policy control methods, namely, on-policy and off-policy. On-policy control uses the policy derived from the current value-function estimate to update the future estimates. Alternatively, off-policy control uses a policy that is greedy with respect to the current value-function to estimate future value-functions. SARSA (on-policy) and Q-learning (off-policy) are two popular TD control algorithms that are known to learn an MDP asymptotically (Sutton and Barto, 1998;Watkins and Dayan, 1992).</p>
<p>The important concept of why we are able to do model-free TD control lies in the fact that we use (state,action)-value functions instead of state-value functions.</p>
<p>Definition 12 (Greedy policy control). Policy improvement is done by considering a new policy, π , which is greedy with respect to the current value-function. π (s) = arg max a∈A R a s + P a ss V(s ) (Greedy w.r.t. state-value function) π (s) = arg max a∈A Q(s, a) (Greedy w.r.t. action-value function)</p>
<p>From the above policy improvement equations we can see that in order to be greedy with respect to the state-value function, we require the model of the MDP. In contrast, if the policy is greedy with respect to the action-value function, the model dynamics of the MDP is not needed, and hence, it is model-free. Thus, optimizing action-value functions to learn the optimal policy is at the heart of all model-free TD control algorithms.</p>
<p>Challenges and Drawbacks</p>
<p>All the Classical Reinforcement Learning algorithms that we discussed above can be categorized as tabular algorithms. That is, the algorithms use a table data structure to associate each state (or state-action pair) with its current value estimate. As the agent interacts with the environment and gains experience, the table values are updated with better estimates of its value.</p>
<p>The main drawback of such a method is that it scales poorly. When the state-space is very large or continuous, the fundamental requirement that the agent visits each state (or state-action pair) multiple times (or infinitely often) is not satisfied; states are at most visited once. An agent following a policy derived from these value estimates would do no better than a random policy. Moreover, the table size grows with the number of states, making storage infeasible for problems with large/continuous state-space.</p>
<p>A common approach to solving this problem is to find a way to generalize the value-function from the limited experience of the agent (Sutton and Barto, 1998). That is, we want to approximate the value-function for an unseen state (or stateaction pair) from the example values it has observed so far. Function approximation is a generalization technique that does exactly this; it takes in observed values of a desired function and attempts to generalize an approximation of the function.</p>
<p>Function Approximation</p>
<p>Function approximation (FA) is an instance of supervised learning (Sutton and Barto, 1998). It is viewed as a class of techniques used to approximate functions by using example values of the desired function. In the RL context tabular methods become infeasible in large or continuous state spaces. This challenge is mitigated by employing FA techniques to predict the value-function at unseen states. However, not all FA methods are applicable to the RL setting. We require a training method which can learn efficiently from on-line, non-i.i.d. data, and also handle non-stationary target functions. The following are some of the function approximators that are used in the RL context. State aggregation is a method of generalizing function approximation in which states are grouped based on a criterion and then value is estimated as an attribute of the group. When a state is re-visited the value corresponding to the state's group gets updated.</p>
<p>Linear combination of features, also known as Linear Function Approximation (LFA), is essentially a linear mapping from the state space (of dimensionality D) to a feature space of dimension M, where often M &lt; D. Each basis function of the feature space is a mapping from the state space to a real-valued number that represents some feature of the state-space.</p>
<p>Definition 13 (Linear-Approximate state(action)-value function). The approximate statevalue function of a state s ∈ S under a policy π is given by:
V π (s) = θ T φ(s) = M ∑ i=1 θ i φ i (s) Q π (s, a) = θ T φ(s, a) = M ∑ i=1 θ i φ i (s, a)
Where φ : S(×A) → R M , is a feature map, and θ ∈ R M is the parameter vector.</p>
<p>LFA has sound theoretical guarantees and also is very efficient in terms of both data and computation (Sutton and Barto, 1998), making it a good candidate for the implementation of our algorithm.</p>
<p>As mentioned previously, FA can be regarded as a technique to develop a generalization regarding value. In order to have a good capacity to generalize, a function approximator must have relevant data about the state-space. Consider a pathological case in which the agent does not explore at all: as a result the only data available for FA would be concentrated in one region of the state space. This results in the estimation of values of unseen states being highly biased. In order to avoid this problem we have to make sure that the agent visits most regions of the state-space, that is, the agent has to explore the state-space efficiently. The main goal of this thesis is to address the problem of how to explore efficiently in large state-spaces.</p>
<p>Exploration Strategies for Reinforcement Learning</p>
<p>In Section 1.2 we described the exploration/exploitation dilemma, which is a fundamental problem in RL. All exploration strategies attempt to manage the trade-off between these two often opposed objectives. The simplest and most widely-used exploration strategy is known as -greedy. At each time-step t the agent chooses a greedy action with probability 1 − and with probability the agent chooses a completely random action. To ensure that the policy converges to the optimal policy it has to satisfy the GLIE assumptions (Singh et al., 2000):</p>
<p>Definition 14 (Greedy in the Limit with Infinite Exploration). A policy is GLIE if it satisfies the following two assumptions.</p>
<p>• Each action is taken infinitely often in every state that is visited infinitely often, lim t→∞ N t (s, a) = ∞ Where N t (s, a) is the number of times action a has been chosen in state s up-to time-step t.</p>
<p>• In the limit, the learning policy is greedy with respect to the Q-value function with probability 1. lim
t→∞ π t (a | s) = 1, when, a = arg max a ∈A Q t (s, a )
For example, -greedy satisfies the GLIE assumptions when is annealed to zero. A common way to do this is by setting t ∝ 1/t.</p>
<p>In small, finite MDPs -greedy satisfies the GLIE assumptions, but when the stateaction space is large/continuous the first GLIE assumption is violated and hence the convergence guarantee is lost. -greedy is a naïve approach to solving the exploration problem, but we still use it in large MDPs because of its low resource requirements when compared with alternatives (Bellemare et al., 2016). In this thesis we propose a novel exploration strategy that improves upon -greedy, and provides state-of-the-art results in large problems with low computational overhead.</p>
<p>We now provide an exposition of various explorations strategies, their foundational principles, and an analysis of recent breakthroughs in the field of exploration.</p>
<p>Taxonomy of Exploration Strategies</p>
<p>The exploration-exploitation dilemma is still an open problem, but researchers have made significant inroads into understanding the nature of the problem. Sebastian Thrun classified exploration techniques into two families of exploration schemes, directed and undirected (Thrun, 1992). Undirected exploration strategies do not use any information from the environment to make an informed exploratory action; they predominantly rely on randomness to do exploration. Softmax methods and -greedy are examples of undirected exploration techniques. The softmax action is sampled from the Boltzman distribution Boltz s (a) = exp(Q(s, a)) ∑ a ∈A exp(Q(s, a )) .</p>
<p>On the other hand, directed exploration strategies use the knowledge about the learning process to form an exploration-specific heuristic for action selection. This heuristic directs the agent to take those actions that maximizes the information gain about the environment. The exploration algorithm introduced in this thesis falls into the category of directed exploration algorithms. In order to put it into context, we first present an overview of the existing directed exploration strategies used in the literature.</p>
<p>The Optimism in the Face of Uncertainty Principle</p>
<p>In the following chapter we present our directed exploration method, which implements the principle of "Optimism in the Face of Uncertainty" (OFU) as a heuristic for exploration. In this section we review existing work on the OFU heuristic. The principle is succinctly captured in :</p>
<p>"When at a state, the agent assigns to each action an optimistically biased while statistically plausible estimate of future value and selects the action with the greatest estimate."</p>
<p>OFU is a heuristic to direct exploratory actions. OFU directs the agent to take actions which have more uncertain value estimates. Instead of greedily taking the action that has the highest estimated value, that agent is encouraged to take actions which have a high probability of being optimal. To see that an apparently suboptimal action may indeed have a high probability of being optimal, let us take an example. Suppose that the agent has taken an action a ∈ A very often from a particular state s ∈ S, and suppose that a also currently has the highest value-estimateQ π (s, a) among the available actions. Now consider an alternative action a ∈ A that has only been tried once from the state s, and suppose that the reward received was lower thanQ π (s, a). Action a has higher estimated value, but having tried it many times, the agent's uncertainty about its value is quite low. In contrast, the uncertainty about the value of the alternative actionQ π (s, a ) is very high, since it has been taken so rarely. Thus, while the current estimateQ π (s, a ) may be lower thanQ π (s, a), there is a good chance that the agent was unlucky when taking a the first time, and that the true action-value Q π (s, a ) is much higher than both estimates. Thus it may be that a has a higher probability of being the optimal action than does a, especially if their estimated values are quite close. The OFU heuristic would bias the agent toward taking action a instead of the greedy action a. An agent following this heuristic will behave as if it is optimistic about action a , or more precisely, about its true actionvalue Q π (s, a ). This optimism drives the agent to explore regions of the environment about which it is more uncertain.</p>
<p>OFU using Count-Based Exploration Bonuses</p>
<p>Most of the exploration algorithms that enjoy strong theoretical efficiency guarantees, implement the OFU heuristic. Many do so by augmenting the estimated value of a state(-action pair) with an exploration bonus that quantifies the uncertainty in that value estimate. An agent which acts greedily with respect to this augmented value function will be biased to take actions with higher associated uncertainty. Most of these algorithms are tabular and count-based in that they compute their exploration bonuses using a table of state(-action) visit-counts. The visit-count serves as an approximate measure of the uncertainty associated with a state(-action), because more novel state(-action) pairs will have lower visit-counts. State(-actions) with lower visit counts are assigned higher exploration bonuses. This drives the agent to behave optimistically and explore less frequently visited regions of the environment, which may yet prove to have higher value than familiar regions. Moreover, even if those regions turn out to yield little reward when explored, the agent will have greatly reduced its uncertainty about those regions. Indeed, the reduction in uncertainty would be much smaller if the agent were to take an action that had already been tried many times. The OFU heuristic is therefore a win-win approach for the agent. OFU algorithms are more efficient than undirected exploration strategies like -greedy because the agent avoids actions that yield neither large rewards nor large reductions in uncertainty ).</p>
<p>Tabular Count-based Exploration Algorithms</p>
<p>One of the best known OFU methods is the UCB1 bandit algorithm, which selects an action that maximises an upper confidence boundQ t (a) + 2 log t N(a) , whereQ t (a) is the estimated mean reward and N(a) is the visit-count (Lai and Robbins, 1985). The dependence of the bonus term on the inverse square-root of the visit-count is justified using Chernoff bounds. In the MDP setting, the tabular OFU algorithm most closely resembling our method is Model-Based Interval Estimation with Exploration Bonuses (MBIE-EB) (Strehl and Littman, 2008). 2 Empirical estimatesP andR of the transition and reward functions are maintained, andR(s, a) is augmented with a bonus term
β √ N(s,a)
, where N(s, a) is the state-action visit-count, and β ∈ R is a theoretically derived constant. The Bellman optimality equation for the augmented action-value function is
Q π (s, a) =R(s, a) + β N(s, a) + γ ∑ s P (s | s, a) max a ∈AQ π (s , a )
Here the dependence of the bonus on the inverse square-root of the visit-count is provably optimal (Kolter and Ng, 2009). This equation can be solved using any MDP solution method.</p>
<p>While tabular OFU algorithms perform well in practice on small MDPs (Strehl and Littman, 2004), their sample complexity becomes prohibitive for larger problems (Bellemare et al., 2016). The sample complexity of an algorithm is a bound on the number of timesteps at which the agent is not taking an -optimal action with high probability (Kakade, 2003). Loosely speaking, it measures the amount of experience the agent must have before one can be confident it is basically performing optimally. MBIE-EB, for example, has a sample complexity bound ofÕ |S| 2 |A| 3 (1−γ) 6 . In the high-dimensional setting -where the agent cannot hope to visit every state during training -this bound offers no guarantee that the trained agent will perform well. The prohibitive complexity of these tabular OFU algorithms is due in part to the fact that a table of visit-counts is not useful if the state-action space is too large. Since the agent will only visit a small fraction of that space, the visit-count for most states will always be zero. These algorithms are therefore unable to usefully compare the novelty of two unvisited states. All unvisited states have the same visit-count, and hence the same exploration bonus. The optimistic agent will treat them all as equally novel and equally appealing.</p>
<p>Generalized Visit-counts for Exploration in Large MDPs</p>
<p>Tabular OFU algorithms fail on high-dimensional problems because they do not allow for generalization across the state space regarding uncertainty. Every unvisited state is treated as entirely novel, regardless of any similarity between the unvisited states and the visited states in the history. In order to explore efficiently in large domains, the agent must be able to make use of the fact that some unvisited states share many features with visited states, while others share very few. If an unvisited state has almost exactly the same features as a very frequently visited one, then it should not be considered to be as uncertain as a state with unfamiliar features. An effective OFU method for these problems would not just encourage the agent to visit unvisited states, but rather would drive the agent to visit states with novel or uncommon features. We discuss this issue further in section Section 3.1.1.</p>
<p>Several very recent extensions of count-based exploration methods have achieved this sort of generalisation regarding uncertainty, and have produced impressive results on high-dimensional RL benchmarks. These algorithms closely resemble MBIE-EB, but they substitute the state-action visit-count for a generalised visit-count which quantifies the similarity of a state to previously visited states. Bellemare et al. (2016) construct a Context Tree Switching (CTS) density model over the state space such that higher probability is assigned to states that are more similar to visited states (Veness et al., 2012). A state pseudocount is then derived from this density. A subsequent extension of this work replaces the CTS density model with a neural network (Ostrovski et al., 2017). Another recent proposal uses locality sensitive hashing (LSH) to cluster similar states, and the number of visited states in a cluster serves as a generalised visit-count (Tang et al., 2016). As in the MBIE-EB algorithm, these counts are used to compute exploration bonuses. These three algorithms outperform random strategies, and are currently the leading exploration methods in large discrete domains where exploration is hard.</p>
<p>Before presenting our optimistic count-based exploration method in the following chapter, we now briefly canvas two alternative frameworks for directed exploration, and discuss their limitations.</p>
<p>Bayes-Adaptive RL</p>
<p>In the Bayesian approach to model-based reinforcement learning, we maintain a posterior distribution over the possible models of the environment given the experience of the agent (Dearden et al., 1998). Bayesian inference is used to update the posterior with new information as the agent interacts with the environment, and also to incorporate the agent's prior distribution over the transition models.</p>
<p>Since the posterior is maintained over all possible models we can now talk about the uncertainty pertaining to what is the best action to take. This uncertainty is modelled as a Markov Decision Process defined over a set of hyper-states. A hyperstate acts as an information state which summarizes the information accumulated so far. This augmented MDP, often referred to as the Bayes-Adaptive MDP (BAMDP), can be solved with standard RL algorithms (Duff, 2002). In this framework an agent acting greedily in the BAMDP whilst updating the posterior acts optimally (according to its prior belief) in the original MDP. The Bayes-optimal policy for the unknown environment is the optimal policy of the BAMDP, thereby providing an elegant solution to the exploration-exploitation trade-off.</p>
<p>Unfortunately, the cardinality of the hyper-states grows exponentially with the planning horizon thereby rendering exact solution to the BAMDP computationally intractable for large problems (Duff, 2002).</p>
<p>Intrinsic Motivation</p>
<p>The final directed exploration heuristic that we discuss is born out of the so-called intrinsic motivation framework. There appears to be a growing scientific consensus in developmental psychology that human beings, from infants to adults, develop their understanding of the world using certain cognitive systems such as intuitive theories, social-structures, spatial systems, etc. (Spelke and Kinzler, 2007;Lake et al., 2016). During curiosity-driven, creative, or risk-taking activities, rational agents use this understanding to generate intrinsic goals. Accomplishing these intrinsic goals leads to the accumulation of intrinsic rewards, thereby exhibiting an innate desire to explore, manipulate, or probe their environment (Oudeyer, 2007).</p>
<p>Drawing parallels to reinforcement learning, the goal of a traditional RL agent is to maximize its expected cumulative reward. This behaviour is extrinsically motivated since the reward signal is external to an agent. We say that an agent is intrinsically motivated if it has intrinsic goals and rewards. In the context of exploration for RL, the aim of the intrinsic motivation approach is to use intrinsic reward as a heuristic that assigns an exploratory value to the agent's actions. For example, an agent may receive intrinsic rewards for visiting novel parts of the environment that need further exploration (Thrun, 1992).</p>
<p>Many formulations that quantify the exploratory value of an action has been put forth, and most of them augment the environment's reward function so as to motivate directed exploration. Schmidhuber (2010) proposed a measure for intrinsic motivation by taking into account the improvement a learning algorithm effected on its predictive world model. This measure tracks the progress of an agent's ability to better compress the history of states and actions (Steunebrink et al., 2013). Another framework for intrinsically motivated learning is to maximize the mutual information. An intrinsic reward measure called empowerment is formulated by searching for the maximal mutual information (Mohamed and Rezende, 2015). The notion of maximizing information gain was demonstrated in a humanoid robot by the introduction of artificial curiosity (Schmidhuber, 1991) as an intrinsic goal (Frank et al., 2014).</p>
<p>These formulations have some major drawbacks which hinder their suitability as exploration heuristics. Firstly, they fail to provide any strong theoretical guarantees of efficient exploration. Leike (2016) pointed out that since none of these heuristics take into account the reward structure of the problem, they do not distinguish be-tween regions of high and low expected reward. Secondly, these algorithms require that we maintain the environment dynamics of the underlying MDP, which prevents us from easily integrating them with model-free algorithms. Another major drawback is the computational overhead associated with calculating the heuristic. For problems with large state/action spaces, computing the intrinsic reward becomes intractable for many heuristics (Bellemare et al., 2016). Most problems of interest have extremely large state spaces, and hence the intrinsic motivation heuristic is currently impractical as an exploration strategy in these domains.</p>
<p>Chapter 3</p>
<p>Exploration in Feature Space</p>
<p>'To wander is to be alive.'</p>
<p>Roman Payne, Europa</p>
<p>In this chapter we introduce a simple, optimistic, count-based exploration strategy that achieves state-of-the-art results on high-dimensional RL benchmarks. In Section 3.1 we begin by discussing the drawbacks of current exploration strategies. In Section 3.2 we provide an exposition of the core ideas that underpin our algorithm. Finally, in Section 3.3 we present our algorithm, as well as a number of related theoretical results.</p>
<p>Drawbacks of Existing Exploration Methods for Large MDPs</p>
<p>We introduced count-based exploration strategies for large MDPs in section Section 2.3.3. Even though they are the current state-of-the-art exploration algorithms in these domains, we consider that there are some potential drawbacks to their common approach to estimating novelty. The motivation for our algorithm arises from trying to avoid these drawbacks.</p>
<p>Choosing a Novelty Measure</p>
<p>The aforementioned algorithms compute a generalized visit-count. This generalized count is a novelty measure that quantifies the (dis)similarity of a state to those in the history. These algorithms drive the agent towards regions of the state space with high novelty. However, the effectiveness of these novelty measures depends on the way in which they measure the similarity between states. If this similarity measure is not chosen in a principled way, states may deemed similar in ways that are not relevant to the given problem. Let us explore this issue by taking an example.</p>
<p>Example 1 (Confounded novelty). Alice is a foodie. She wants to explore the myriad restaurants that are open in her city. Suppose that Alice's novelty measure treats restaurants as similar if they are geographically close. Alice consults her novelty measure to choose a restaurant she has not tried yet, and it returns a Chinese restaurant in a distant suburb that she has not visited before. Alice scratches her head thinking: 'I have been to a tonne of Chinese restaurants; if only my novelty measure understood that and suggested a different cuisine!' Unfortunately, her novelty measure considers this restaurant very dissimilar from the Chinese restaurants she has visited, simply because it is geographically distant from them.</p>
<p>The problem here is that Alice's novelty measure does not know anything about which features matter when evaluating the novelty of a restaurant. Let us now look at an example from the recent exploration literature where this problem can be clearly observed.</p>
<p>Inappropriate Novelty Measures in Practice</p>
<p>The problems that can arise from an unprincipled choice of novelty measure are well illustrated in the experimental evaluation of Stadie et al. (2015). Their algorithm uses an autoencoder to encode the state-space into a lower dimensional representation. The encoding is then fed into a model dynamics prediction neural network which estimates the novelty by providing an error-based bonus. This method, called Model Prediction Exploration Bonuses (MP-EB), uses an error based estimator and is different from the visit-density model of Bellemare et al. (2016), but they both estimate novelty. To generalize regarding value they use the DQN network, and so we refer to their algorithm as DQN+MP-EB. During empirical evaluation of their algorithm an anomaly was detected in the game Q<em>bert 1 from the Arcade Learning Environment 2 (ALE) benchmarking suit. DQN+MP-EB algorithm scored lower than the baseline algorithm, DQN+ -greedy. They attributed this anomaly to the fact that during each level change of Q</em>bert, the color of the game changes dramatically, but neither the objective nor the structure of the level changes (Figure 3.3). When their agent reached level 2 (Figure 3.2), it perceived the state to be completely novel because MP-EB is sensitive to color. This tricked MP-EB into assigning high exploration bonus to all the states even though the action-values of the states hadn't changed. Hence the policy of the agent was impacted adversely.</p>
<p>The pathology of DQN+MP-EB in the Q*bert game highlights a serious problem with current novelty estimators they do not take into account the relevancy of a state to the task an agent is trying to accomplish. We argue that a measure of novelty should not just be an arbitrary generalized representation of how many times an agent has visited a state, but should ideally be a measure of dissimilarity in facets that are relevant to the agent's goal. Two states can be different in many ways; the challenge is to find out a similarity metric which is effective in achieving the agent's goal optimally. In Example 1, Alice's novelty measure did not know that suggesting a restaurant with a different cuisine would be more relevant to her task, thereby naively suggesting a geographically distant unvisited restaurant.</p>
<p>Separate Generalization Methods for Value and Uncertainty</p>
<p>We contend that this deficiency is not peculiar to MP-EB, but rather that it may arise whenever the novelty measure is not designed to be task-relevant. Indeed, all of the aforementioned algorithms which compute a novelty measure share a common structure which leaves them vulnerable to this problem. Each algorithm has two quite unrelated components: a value estimator (an RL algorithm which performs policy evaluation), and a novelty estimator. Each component involves an entirely separate generalization method. The value estimator makes use of a feature representation of the state space in order to generalize about value. The novelty estimator separately utilizes a different, exploration-specific state space representation to measure the similarity between states. For example, the #Exploration algorithm of Tang et al. (2016) uses the DQN algorithm for value estimation. In order to estimate novelty, however, #Exploration maps the state space into a lower-dimensional representation using locality sensitive hashing. The similarity measure induced by the choice of hash codes is unlikely to resemble that which is induced by the features learnt by DQN. The DQN-CTS-EB algorithm of Bellemare et al. (2016) has a similar structure: DQN is used to estimate value, but the CTS density model is used to estimate novelty. Again, it is not obvious that there should be much in common between the two similarity measures induced by these different state space representations. One might think that this is natural; after all, each representation is used for a different purpose. However, there are two questions we can ask here. Firstly, is there redundant computation due to performing a dimensionality reduction of the same state-space twice? If so, can we reuse the same state space representation for both value and novelty estimation? We address these questions in the following section.</p>
<p>Before moving on we should note that the concerns we express in this section have already been raised in the literature. In their empirical evaluation Bellemare et al. (2016) observed that their value estimator (DQN) was learning at a much slower rate than their CTS density model (their novelty measure). The authors attribute this mismatch to the incompatibility between novelty and value estimators. They further go on to suggest that designing density models to be compatible with value function would be beneficial and a promising research direction.</p>
<p>The drawbacks we presented in this section suggest that there may be much room for improvement in the design of novelty estimators for exploration. In the following sections we describe our technique for estimating novelty by factoring in the insights we gained from analyzing these drawbacks. We first provide a solid footing for some of the assumptions that we made while designing the algorithm. We then go on to present our core exploration algorithm, and then combine it with a model-free RL algorithm (SARSA(λ)). In the coming chapters we present empirical evidence that our RL algorithm achieves world-leading results on the ALE benchmarking suite.</p>
<p>Estimating Novelty in Feature Space</p>
<p>Motivation</p>
<p>Which representation of the state space is appropriate for novelty estimation? Intuitively, if we use some parameters to determine the value of a state, then naturally, two such objects are considered dissimilar only if they differ in these parameters. Analogously, if the agent is using certain features to determine the value of a state, then naturally, two such states should be considered dissimilar only if they differ in those value-relevant features. This motivates us to construct a similarity measure that exploits the feature representation that is used for value function approximation. These features are explicitly designed to be relevant for estimating value. If they were not, they would not permit a good approximation to the true value function. This sets our method apart from the approaches described in Section 2.3.3, which measure novelty with respect to a separate, exploration-specific representation of the state space, one that bears no relation to the value function or the reward structure of the MDP. We argue that measuring novelty in feature space is a simpler and more principled approach, and hypothesise that more efficient exploration will result. Our proposal ensures that generalization regarding novelty is done in the same space as generalization regarding value. Figure 3.4 illustrates the basic structure of our proposed novelty estimator. Let us make the idea more concrete with our running example.</p>
<p>Example 2 (Value-relevant exploration). After Alice's disappointing restaurant visit last time, she tweaked her novelty estimator such that it now generalizes based on value-relevant features like the type of cuisine, the star rating, and the other features that truly determine the quality of Alice's dining experience. When Alice is ready to try something new, she can rest assured that it's going to be something novel in a way that is meaningful.</p>
<p>Design Decisions</p>
<p>Our exploration strategy, henceforth known as φ-exploration bonus (φ-EB), can be thought of as exploration in the feature space. This makes the existence of a feature map crucial to our strategy. Therefore we require that our algorithm be compatible with Linear Function Approximation (LFA). Before the advent of neural networks and subsequently DQN, large RL problems used linear function approximation to estimate the value of a state. Our decision to use LFA as our value prediction module has the following desirable benefits:</p>
<p>• Domain Independence: The visit-density models that we have seen so far (MP-EB, CTS-EB, PixelCNN, etc.) are designed to work with RGB pixel values from a video input. Though there are many domains that use video input to train the agent, there are equally many other domains that have nothing to do with a video input. For example, reinforcement learning is used in the financial sector to optimize portfolios, asset allocations, and trading systems (Moody and Saffell, 2001). Therefore developing a visit-density model that is domain independent is a key challenge. Our φ-EB method estimates the novelty using the same features that LFA uses to approximate the value function. This allows our exploration strategy to be compatible with any value-based RL algorithm that uses LFA.</p>
<p>• Indirect dependence on LFA: LFA is essentially a linear combination of features. The only requirement φ-EB has is the existence of a feature map, which is implicitly satisfied with LFA. Because of this indirect dependence on LFA, we hypothesis that it is possible to extend φ-EB to be compatible with valuenetworks that perform representation learning as well (e.g., DQN). Due to resource and time constraints we do not pursue empirical evidence for this claim, but rather leave this as a possible future extension of our research.</p>
<p>• Single point of change: The best way to assess the performance impact of changes to a system is to confine the change to a single module and then run performance tests. Following this principle, we know that SARSA(λ) is a valuebased RL algorithm which uses LFA for value prediction and -greedy for exploration (Sutton and Barto, 1998). SARSA(λ) has been studied, perfected and validated through-out the ages. Therefore showcasing the performance gains achieved by replacing -greedy with our φ-EB exploration strategy allows for a sound empirical proof for the efficacy of our algorithm.</p>
<p>One drawback of using LFA for value prediction is that it requires a set of handcrafted features. This is easily mitigated by choosing the Arcade Learning Environment (ALE) as our evaluation platform (Bellemare et al., 2013), combined with the Blob-PROST feature set (Liang et al., 2015). Using Blob-PROST as our feature set has and added advantage. Blob-PROST is designed to mimic the features learned by DQN, thus making our algorithm comparable with those using DQN for representation learning and value prediction. We'll discuss in depth about the ALE and Blob-PROST in Chapter 4.</p>
<p>The φ-EB Algorithm</p>
<p>The main original contribution of this work is a method for estimating novelty in feature space. The challenge is to do so without explicitly computing the distance between each new feature vector and all the feature vectors in the history. That approach quickly becomes infeasible because the cost of computing all these distances grows with the size of the history. Our method instead constructs a density model over feature space that assigns higher probability to states that share more features with more frequently observed states. In order to formally describe our method we first introduce some notation.</p>
<p>Notation</p>
<p>• φ : S → T ⊆ R M , The feature map used in LFA. Maps the state space into an M-dimensional feature space, T .</p>
<p>• φ t ≡ φ(s t ), Feature vector observed at time t, whose i th component is denoted by φ t,i</p>
<p>• φ 1:t ≡ (φ 1 , . . . , φ t ) ∈ T t , Sequence of feature vectors observed after t timesteps.</p>
<p>• φ 1:t φ ≡ (φ 1 , . . . , φ t , φ) ∈ T t+1 , Sequence where φ 1:t is followed by φ.</p>
<p>• T * , Set of all finite sequences of feature vectors.</p>
<p>• ρ : T * × T → [0, 1], The sequential density model (SDM) that maps a finite sequence of feature vectors to a probability distribution.</p>
<p>We will now present the key component of our algorithm that allows us to estimate novelty in feature space.</p>
<p>Feature Visit-Density</p>
<p>Definition 15 (Feature visit-density). The feature visit-density ρ t (φ) ≡ ρ(φ ; φ 1:t ) at time t is a probability distribution over the feature space T , representing the probability of observing the feature vector φ after observing the sequence φ 1:t . It is modelled as a product of independent factor distributions ρ
i t (φ i ) over individual features φ i ρ t (φ) = M ∏ i=1 ρ i t (φ i )
This density model induces a similarity measure on the feature space. Loosely speaking, feature vectors that share component features are deemed similar. This enables us to use ρ t (φ) as a novelty measure for states, because it represents the frequency with which features are observed in the history. When confronted with a new state, we are able to estimate how frequently its component features have occurred in the history. If φ(s) has more novel component features, ρ t (φ) will be lower. By using a density model we are therefore able to measure novelty in a way that usefully generalizes the agent's uncertainty across the state space. To illustrate this, let us consider an example.</p>
<p>Example 3. Suppose we use a 3-D binary feature map and that after 3 timesteps the history of observed feature vectors is φ 1:3 = (0, 1, 0), (0, 1, 0), (0, 1, 0). Let us estimate the feature visit densities of two unobserved feature vectors φ = (1, 1, 0), and φ = (1, 0, 1). Using the KT estimator for the factor models, we have ρ 3
(φ ) = ρ 1 3 (1) · ρ 2 3 (1) · ρ 3 3 (0) = 1 8 · 7 8 · 7 8 ≈ 0.1, and ρ 3 (φ ) = ρ 1 3 (1) · ρ 2 3 (0) · ρ 3 3 (1) = ( 1 8 ) 3 ≈ 0.002. Note that ρ 3 (φ ) &gt; ρ 3 (φ )
because the component features of φ are more similar to those in the history. As desired, our novelty measure generalizes across the state space.</p>
<p>Each factor distribution ρ i t (φ i ) is modelled using a count-based estimator. A naive option would be to use the empirical estimator which is the ratio of the number of times a feature has occurred to the total number of time steps. Another class of count-based estimators are the Dirichlet estimators which enjoy strong theoretical guarantees (Hutter, 2013). We use the Krichevsky-Trofimov(KT) estimator which is a Dirichlet-like estimator that is simple, easy to implement, scalable, and data efficient (Krichevsky and Trofimov, 1981). If N t (φ i ) is the number of times the feature φ i has been observed, then the KT estimator is given by:
ρ i t (φ i ) = N t (φ i ) + 1 2 t + 1
Using independent factor distributions for modelling the probability of each feature component inherently assumes that the features are independently distributed. This is not always the case, especially in video-input based domains such as the ALE we have many features that are strongly correlated. This doesn't mean that we cannot use fully factorized distributions. One of the early assumptions made by Bellemare et al. (2016) about the density model is that the states are independently distributed. This allowed them to factorize the states, and model each factor using a positiondependent CTS 3 density model. Moreover, our empirical evaluations show that we achieve world leading results in hard exploration games suggesting that independent factored distributions produce good novelty measures. Thus by precedence and by empirical data the independence assumption on the features is a well-justified trade-off that makes the computation of novelty fast and data efficient.</p>
<p>The φ-pseudocount</p>
<p>Here we adopt a recently proposed method for computing generalised visit-counts from density models (Bellemare et al., 2016). By analogy with the pseudocounts presented in that work, we derive two φ-pseudocounts from our feature visit-density. Both variants presented generalize the same quantity, the state visitation count function N t (s). The expression given in the following definition is derived in Bellemare et al. (2016). We emphasize that our approach constitutes a departure from theirs, because while they derive pseudocounts from a state visit-density model, we do so using a feature visit-density model. 4 be the probability that the feature visit-density model would assign φ if it was observed one more time. Then the φpseudocount for a state s ∈ S is given by:
Definition 16 (φ-pseudocount). Let ρ t (φ) ≡ ρ t (φ ; φ 1:t φ)N φ t (s) = ρ t (φ(s))(1 − ρ t (φ t (s))) ρ t (φ(s)) − ρ t (φ(s))</p>
<p>The φ-Exploration Bonus algorithm (φ-EB)</p>
<p>Equipped with all the tools necessary for the construction of an exploration bonus we now proceed to define the φ-EB algorithm. We provide a high level flow-chart for the construction of the bonus in Figure 3.5, and the corresponding pseudo-code in Algorithm 1. Having defined the φ-pseudocount (a generalised visit-count), we follow traditional count-based exploration algorithms by computing an exploration bonus that depends on this count. The functional form of the bonus is the same as in MBIE-EB; we merely replace the empirical state-visit count with our φ-pseudocount.</p>
<p>Definition 17 (φ-exploration bonus). The exploration bonus for a state-action pair (s, a) ∈ S × A at time t is
R φ t (s, a) = β N φ t (s) where β</p>
<p>is a hyper-parameter that controls the agents level of optimism.</p>
<p>Loosely speaking, the hyper-parameter β can be viewed as a knob that tunes the agent's confidence in its estimate of the true action-value function. Higher values of β makes the agent under-confident about value, and result in too much exploration. Very low β values do not encourage enough exploration because the exploration bonus is too small to dissuade the agent from acting greedily with respect to its current value estimates. In both scenarios the final policy of the agent is affected adversely. The goal is to find a β value that gives good results across domains. We performed a coarse parameter sweep among the games in the ALE evaluation platform and concluded that β = 0.05 was the best value. Further details regarding the selection of β value is discussed in Section 5.2.1. 
∏ i=1 ρ i (φ i ) 3 end Input: Feature Visit Count N t ; Density Model ρ; Current Timestep t 1 function UpdateFeatureVisitDensity(φ) 2 for i=1 to M do 3 ρ i (φ i ) ← N t (φ i ) + 1 2 t + 1 4 end 5 end 1 function PseudoCount(p, p ) 2 return p(1 − p) p − p 3 end Input: LFA Feature Map φ; Exploration Coefficient β 1 function ExplorationBonus(s) 2 ρ(φ) ← FeatureVisitDensity(φ(s)) 3 UpdateFeatureVisitDensity(φ(s)) 4 ρ (φ) ← FeatureVisitDensity(φ(s)) 5N φ (s) ← PseudoCount(ρ(φ), ρ (φ)) 6 return β Nφ (s) 7 end</p>
<p>LFA with φ-EB</p>
<p>One the advantages that we have in developing our algorithm for use with LFA is that our exploration strategy is compatible with all value-based RL algorithms that use LFA. As we will see, our empirical performance across a range of environments suggests that one can plug our exploration strategy with little to no modification into any of these algorithms and expect considerable gains in exploration efficiency.</p>
<p>In our empirical evaluation we use SARSA(λ) with replacing traces as our valuebased reinforcement learning algorithm. Algorithm 2 presents the pseudo-code for a generic RL algorithm that uses the augmented reward r + for updating the function parameters θ of the approximate action-value functionQ(s, a) = θ T φ(s, a).</p>
<p>Algorithm 2: LFA with φ-EB Input: LFA Feature Map φ; Training Horizon t end 1 t ← 0 2 Initialize arbitrary θ t 3 s t , a t ← initial state, action 4 while t &lt; t end do 5 r t+1 , s t+1 ← Act(a t )
6 R φ t (s t , a t ) ← ExplorationBonus(s t ) 7 r + t+1 ← r t+1 + R φ t (s t , a t ) 8 a t+1 ← NextAction(s t+1 , θ t ) 9 θ t+1 ←UpdateTheta(r + t+1 , φ) 10 t ← t + 1</p>
<p>end 12 return θ t end</p>
<p>The functions NextAction and UpdateTheta are specific to the underlying value-based RL algorithm used, hence left unspecified. Act(a t ) performs action a t in the environment.</p>
<p>Complexity Analysis Time Complexity</p>
<p>From Algorithm 1 it is trivial to see that a call to ExplorationBonus has a worstcase time complexity of O(M), where M is the dimension of the feature space. This suggests that the time needed to compute the novelty of a state is independent of the dimension of the state-space. Also, more often than not, the dimension of the feature space is far smaller than that of the state space. Therefore, our algorithms generates significant savings in computation over other density models whose time-complexity scales with the number states. In practice, for a binary feature set like Blob-PROST we process only those features that have been observed before. This is achieved by maintaining a single prototypical factor density estimator for all previously unseen features. We'll discuss the implementation specific details in depth in Chapter 4.</p>
<p>Space Complexity</p>
<p>We look at Algorithm 2 to analyze what objects are needed to be persisted across iterations so as to facilitate calculation of the exploration bonus. Clearly the factor density estimators ρ i (φ i ), and the feature visit count N t (φ i ) are needed to evaluate and update the feature visit density. Therefore it can be seen that our algorithm has a worst case space complexity of O(M). Again, because the features in Blob-PROST are binary valued, the KT estimator can be defined recursively. This allows for updating the factor density online without the need to maintain a feature visit count N t (φ i ). We'll discuss more on this in Chapter 4.</p>
<p>Summary</p>
<p>In this chapter we have presented the main contribution of our research. Motivated by the drawbacks of current state-of-the-art exploration algorithms, we introduced our novel exploration algorithm called φ-EB. Later, we provided an exposition on the various components of the algorithm and also analysed its time and space complexity. Now that we have presented our algorithm, we move on to implementation aspects. The next chapter provides a detailed overview of the evaluation test-bed, the software architecture, and the implementation challenges faced during the Research &amp; Development of the algorithm.</p>
<p>Chapter 4</p>
<p>Implementation 'Any A.I. smart enough to pass a Turing test is smart enough to know to fail it.'</p>
<p>Ian McDonald, River of Gods</p>
<p>This chapter is dedicated to developing a technically correct implementation of our exploration strategy φ-EB, and its surrounding infrastructure. This allows us to perform a sound empirical evaluation which is the focus of the next chapter.</p>
<p>In Section 4.1 we present the high-level architecture of the whole system, and how the various components interact with each other. Later, in Section 4.2, we present the implementation of our exploration strategy, φ-EB. Throughout the section we also talk about the design aspects, and optimization's that went into implementing φ-EB.</p>
<p>Software Architecture</p>
<p>Our implementation goal is to develop an RL software agent that uses φ-EB as its exploration strategy. We present the high-level design of the algorithm in Figure 4.1. The presented diagram is analogous to the Agent-Environment interaction cycle (Figure 1.1), but with more granularity. From an exploration-centric standpoint, we first provide a concise overview of the components presented in the architecture, and then an exposition on the implementation details for φ-EB. </p>
<p>Implementation</p>
<p>Modular Overview Control</p>
<p>We use SARSA(λ) with replacing traces (Sutton and Barto, 1998) as our learning algorithm. This decision was driven primarily by two factors. First, using the Blob-PROST 2 feature set meant that we are locked into the framework provided by Liang et al. (2015). In our case this is in fact desirable. Replacing the exploration module of an open source, peer-reviewed and published implementation with our own exploration module enhances the credibility of any performance gains that result. Second, we need a learning algorithm that works well with Linear Function Approximation (LFA). When coupled with LFA, SARSA(λ) has better convergence guarantees than Q-learning (Melo et al., 2008). Hence, SARSA(λ) is a suitable value estimation algorithm for our agent.</p>
<p>Exploration Strategy</p>
<p>This component is our φ-EB exploration strategy that was proposed in Chapter 3. We implement it using the C++11 3 programming language. C++ offers a significant edge over other languages in terms of efficiency and greater control over memory management. Due to the high dimensional nature of our problem, we need to extract as much performance as possible from our code. Therefore implementing the exploration strategy in C++ is critical to the empirical success of our algorithm. Moreover, a lock-in with the framework provided by Liang et al. (2015) meant that there was no compelling reason to choose a different programming language. In the coming sections we provide a detailed look at the design and implementation of φ-EB.</p>
<p>Prediction</p>
<p>We use LFA to generalize the action-value function for unknown state-action pairs. For further discussion on LFA we refer the reader to Section 2.1.2. LFA uses the Blob-PROST feature set from Liang et al. (2015) to approximate the action-value function.</p>
<p>Feature Map</p>
<p>We consider the feature map to be an integral part of the agent. The ability to discern different features of a state is imperative to generalization regarding value, and by extension, to exploration. Our agent explores in the feature space, and we want to use LFA for value prediction. This necessitated the need for an efficient and effective feature set. The Blob-PROST feature set from Liang et al. (2015) is the best feature set available to date for the Arcade Learning Environment (ALE) evaluation platform. More details on Blob-PROST in Section 5.1.2.</p>
<p>Environment</p>
<p>We chose the Arcade Learning Environment as our evaluation platform for the following reasons.</p>
<p>• ALE contains many games which vary in degree of exploration hardness. This allows us to test the efficacy of our algorithm on a broad spectrum of games (Bellemare et al., 2016).</p>
<p>• ALE is widely accepted as the standard for testing RL algorithms. The vast majority of exploration specific research that is published post Bellemare et al. (2013) has adopted the ALE platform to report empirical results (Mnih et al., 2013(Mnih et al., , 2015Stadie et al., 2015;Bellemare et al., 2016;Tang et al., 2016;Ostrovski et al., 2017). Therefore, in order to compare and contrast our results with existing research, it is crucial that we choose ALE as our evaluation platform.</p>
<p>More details on the Arcade Learning Environment in Section 5.1.1.</p>
<p>Agent-Environment Work-flow</p>
<p>We want to seamlessly integrate φ-EB into the control module. Therefore understanding the nuances of what happens in an agents cycle from an implementation perspective is critical. Figure 4.1 also doubles as a work-flow diagram for our agent SARSA(λ)+φ-EB. Following the usual agent-environment interaction process at timestep t, the agent performs an action a t on the environment and receives an extrinsic reward r t+1 . The agent also observes the new state of the environment, s t+1 . Inside the agent, the Blob-PROST feature map consumes the current state s t+1 and returns a feature vector φ t+1 . The feature vector is then used by the LFA module to do value prediction. The φ-EB module uses the stored feature φ t to generate the exploration bonus R t (s t , a t ) 4 . SARSA(λ) updates the parameters of LFA with the TD update and chooses the next action optimistically.</p>
<p>φ-EB</p>
<p>In the exploration strategy module the feature visit-density ρ is a hash map that persists in memory across cycles and episodes. Each entry of ρ is a key-value pair mapping individual features φ i to its corresponding factor distribution ρ i . In the φ-EB module shown in Figure 4.1, the flow of control is as follows: compute ρ(φ) as product of factors, update ρ with the observation φ, then compute ρ(φ) again. Now calculate the pseudo-count and subsequently the exploration bonus R t (s t ). The bonus R t (s t ) is considered as an intrinsic reward and is sent to the control module, SARSA(λ).</p>
<p>LFA</p>
<p>The prediction module (LFA) approximates the next state action-value function using the parameter vector θ t asQ π (s t+1 , a t+1 ) = θ T t φ(s t+1 , a t+1 ). LFA sends the next state Q-value to the control module, SARSA(λ). The parameter vector θ t is also an object that is saved in memory and persisted across cycles and episodes.</p>
<p>SARSA(λ) 5</p>
<p>All the results from the various modules flow into the control module SARSA(λ). The control module essentially has two tasks.</p>
<p>• Choose the next action a t+1</p>
<p>The next state is chosen by being greedy with respect to next state action-value obtained from LFA. a t+1 = arg max a∈A Q π (s t+1 , a)</p>
<p>• Update θ of LFA First we augment the extrinsic reward r t+1 with the intrinsic reward R t (s t ) obtained from φ-EB module.
r + t+1 = r t+1 + R t (s t )
The augmented reward r + t+1 , the next state action-valueQ π (s t+1 , a t+1 ), and the current state action-valueQ π (s t , a t ), both from LFA, is used to calculate the TD error.</p>
<p>δ t+1 = r + t+1 + γQ π (s t+1 , a t+1 ) −Q π (s t , a t ) Where γ is the discount factor. Next we update θ, and is updated using the usual TD update formula.
θ t+1 ← θ t + αδ t+1
Where α is the learning rate. Now that we have a clear idea about the surrounding infrastructure, let's move on to the implementation details of φ-EB</p>
<p>Implementation Details</p>
<p>Feature Visit-Density</p>
<p>The central data structure that stores the factor distribution of each individual feature is an unordered _ map 6 called fvd _ map φ i , ρ i . Each entry is a key-value pair mapping individual features to its corresponding factor distribution. This allows us to have constant time look-up for the factor distribution of any feature. At first glance of the theoretical formulation of feature visit-density (Section 3.3.1, Algorithm 1), the implementation looks straight forward. Unfortunately that is not the case. We need to take into account certain implementation specific aspects that are often subsumed by mathematical formulation. Following are some of the important implementation details that need to be considered for computing feature visit-density. 5 For brevity we have left out the discussion on eligibility traces. 6 Essentially a hash map</p>
<p>• Sparse Feature Vector</p>
<p>In practice the feature vector φ is the list of features that are active in the current timestep. Most of the time the set of observed features is in a vastly smaller subspace of the feature space R M . Therefore, iterating till M to compute the product of the factor distributions is quite wasteful. In order to overcome this we maintain a prototype 7 function that computes the KT-estimate of observing the feature give that it has never been observed in t timesteps. Now whenever a new feature is observed it is added to fvd _ map with the current value of the prototype. If M t is the total number of features observed till timestep t, then we can compute the feature visit-density in O(M t ) time.</p>
<p>• Numerical Stability</p>
<p>Experience has taught us that when dealing with probabilities, innocent looking formulas such as ours can be deceiving. Since we are taking product of probabilities, they are bound to numerically underflow. In our implementation, rather than computing
M ∏ i=1 ρ i (φ i ) we compute M ∑ i=1 log ρ i (φ i )
. This allows us to safely perform probability calculations without the worry of underflow.</p>
<p>• Inactive Features</p>
<p>During the evaluation of the feature visit-density we need to consider the factor distributions for the features that are inactive but previously observed. Since we have already observed φ t , we can identify the features in fvd _ map that are not active. The probability density stored in fvd _ map against some feature φ i , is the probability of φ i being active. Assuming φ i ∈ φ t , the probability of φ i not being active is given by 1−fvd _ map[φ i ] . Therefore when evaluating feature visit-density for φ t we should also factor in the probability of inactive features not occurring.</p>
<p>Algorithm 3 presents the implementation for computing the feature visit-density with all the above mentioned optimization/requirements. One key observation is that we return the log-probability. This is done to facilitate further log based proba-bility computation that occur in other modules.</p>
<p>Algorithm 3: Implementation of Feature Visit Density</p>
<p>Input: Current Timestep t 1 function KT_Prototype() </p>
<p>Updating Factor Densities</p>
<p>Recall that we use the Krichevsky-Trofimov (KT) estimator to compute the factor densities. Given a sequence of symbols, the KT-estimator computes the probability of the next symbol. For a binary symbol-set, the KT-estimator is given by.</p>
<p>Pr(x t+1 = 1 | x 1:t ) = n 1 + 1 2 n 0 + n 1 + 1</p>
<p>Where n 1 is the number of 1's seen so far in the sequence, and n 0 is the number of 0's seen so far.</p>
<p>The Blob-PROST feature set is binary valued, making the use of KT-estimators ideal. Therefore, our factor density for a feature φ i being active is given by.
ρ i (φ i ) ≡ Pr(φ i = 1 | φ i 1:t ) = N t (φ i ) + 1 2 t + 1
And the probability for the feature being inactive is.
ρ i (φ i = 0) = 1 − ρ i (φ i = 1)
Where N t (φ i ) is the number of times feature φ i has been seen, and φ i 1:t is the complete sequence of past observations for feature φ i .</p>
<p>The factor density equation is neat and simple, but it requires that we maintain a count for each feature. This is an unnecessary overhead and we can do better. We now propose an update formula for ρ i (φ i ) and derive it.</p>
<p>Proposition 1 (Update formula for KT-estimate ρ i (φ i )). The factor distribution ρ i t at timestep t for feature φ i can be updated using the following update formula.
ρ i t+1 (φ i ) = ρ i t (φ i ) t + 1 t + 2 + φ i t + 2 Where φ i ∈ {0, 1} Proof. From the equation for KT-estimates of ρ i t (φ i ) we have, ρ i t (φ i ) = N t (φ i ) + 1 2 t + 1 ρ i t (φ i ) t + 1 t + 2 = N t (φ i ) + 1 2 t + 2(1)
In the next timestep t + 1, depending on the value of φ i we have two cases.</p>
<p>• Case 1: Feature φ i is active, i.e., φ i = 1 The KT-estimate ρ i t+1 (φ i ) can be written as,
ρ i t+1 (φ i ) = N t+1 (φ i ) + 1 2 (t + 1) + 1 = N t (φ i ) + 1 + 1 2 t + 2 (Since φ i = 1) = N t (φ i ) + 1 2 + 1 t + 2 ρ i t+1 (φ i ) = N t (φ i ) + 1 2 t + 2 + 1 t + 2
(2)</p>
<p>• Case 2: φ i = 0</p>
<p>The KT-estimate ρ i t+1 (φ i ) can be written as,
ρ i t+1 (φ i ) = N t+1 (φ i ) + 1 2 (t + 1) + 1 = N t (φ i ) + 0 + 1 2 t + 2 (Since φ i = 0) = N t (φ i ) + 1 2 + 0 t + 2 ρ i t+1 (φ i ) = N t (φ i ) + 1 2 t + 2 + 0 t + 2(3)
In both cases, from Eq. (2) and (3) we can see that the value φ i decides the existence of an additional term. Therefore by observation we can combine the two cases as follows.
ρ i t+1 (φ i ) = N t (φ i ) + 1 2 t + 2 + φ i t + 2(4)
Therefore, from Eq. (1) and (4) we get,
ρ i t+1 (φ i ) = ρ i t (φ i ) t + 1 t + 2 + φ i t + 2
Algorithm 4 presents the algorithm for updating the factor distributions. It uses the update formula presented in Proposition 1 to efficiently update the factor distributions. In the implementation we can see that the update is performed in a two part manner with linear time complexity, rather than a naive double-loop search. </p>
<p>Algorithm 4: Factor Distribution Update
Input: Factor Distribution Map fvd _ map φ i , ρ i ; Current Timestep t 1 function Update(φ) 2 for i = 1 to size(fvd _ map.keys) do 3 fvd _ map[φ i ] = fvd _ map[φ i ] · t+1</p>
<p>Exploration Bonus</p>
<p>This is the entry point for our exploration strategy φ-EB. Due to the modular design of our algorithm, this function mostly acts like a hub that calls other functions sequentially to get the data required to calculate the exploration bonus. Algorithm 5 presents the implementation to calculate exploration bonus. Note that the probabilities are in log space to avoid numerical stability issues.</p>
<p>Algorithm 5: Exploration Bonus</p>
<p>Input: Exploration Coefficient β 1 function ExplorationBonus(φ) </p>
<p>Action Selection</p>
<p>In the early stages of the project, our agent was facing some inexplicable issues. It had really slow learning progress, and was getting stuck with a single action for long periods of time. Fortunately, we had rich logs that helped us in identifying a pattern to the problem. We observed that during the initial training cycles, the value predictions from LFA had very high variance due to lack of enough samples. In cases when there was an abnormally high Q-value, our greedy optimistic agent always kept taking the same action over and over again in a loop. We initially thought that, decay in the corresponding exploration bonus, coupled with increase in optimistic estimates for other states would lead to the agent breaking out of the loop. Even though eventually the agent got out of the loop, it happened only after an exorbitantly large number of episodes. From the logs we observed that each TD update only effected a small change, and hence the reason why it took a large number of episodes to overcome abnormally high Q-value.</p>
<p>If we were using -greedy as the exploration strategy this would not be a problem. With -greedy, the agent takes more exploratory action in the initial training cycles. Even if LFA produces highly varying Q-values initially, the agent doesn't get stuck for more that a few cycles. Thus, it can be noted that random exploration at the beginning helps stabilize the action-values predicted by LFA.</p>
<p>Our goal is to replace -greedy with our intrinsically motivated exploration strategy φ-EB. Unfortunately, the removal of -greedy meant that the agent's policy is now deterministic and has the above debilitating side-effect. In order to solve this crippling issue we experimented two approaches.</p>
<p>• Combine φ-EB with -greedy</p>
<p>A similar problem was reported by Bellemare et al. (2016). Their solution was to use -greedy, not as an exploration strategy, but as a tool to introduce stochasticity in the agents policy. During the initial training cycles, when there is high variance from the LFA estimates, taking a purely random action allows the agent to get out of the greedy action loop. In this experiment -greedy is implemented in the usual way -with probability take a random action, and a greedy optimistic action otherwise. Algorithm 6 presents the implementation.</p>
<p>• Combine φ-EB with Boltzmann distributed action selection</p>
<p>One motivation for our research is to make sure that the agent does not take purely random actions. The approach from Bellemare et al. (2016) described above introduces purely random actions. We present an alternate approach which introduces stochasticity but in a directed manner.</p>
<p>We split our optimistic Q functions into two functions, Q E and Q I . Q E is trained using the extrinsic reward, whereas Q I is trained on the exploration bonus from φ-EB 8 . The motivation here is that we now have a value-function Q I that directs the exploratory actions of the agent. For action selection we construct the optimistic value function as the summed value function Q = Q E + Q I . During action selection, with probability (1 − ) the agent takes the action that is greedy with respect to Q, otherwise the agent takes a Boltzmann distributed random action. The Boltzmann distribution is constructed from the Q I values using the discrete _ distribution standard library. Hence the selected random action is more likely to be an action that has higher exploratory value. Algorithm 7 presents implementation for this approach.</p>
<p>Theoretically, the only difference between the above two approaches is the action selection process during exploration. The first approach takes a uniformly random action, whereas the second one takes a Boltzmann-distributed random action. Therefore during the implementation of the learning algorithm we implement the second approach, and swap the action selection process with the first for experimentation.</p>
<p>Algorithm 8 presents the final version of the algorithm that we have implemented, and for which empirical results are presented. In the algorithm shown we use the Boltzmann distributed action selection approach. We can disable Line 13 and enable the two comments below it to use the -greedy action selection approach.</p>
<p>In the next chapter we discuss the experimental evaluation framework we used to perform empirical evaluation. Then we showcase the state-of-the-art results that our algorithms enjoys.</p>
<p>Chapter 5</p>
<p>Empirical Evaluation</p>
<p>'What can be asserted without evidence can also be dismissed without evidence.'</p>
<p>Christopher Hitchens</p>
<p>Empirical evidence is one of the fundamental requirements for validating any scientific hypothesis. In order to validate the efficacy of our exploration algorithm, this chapter showcases empirical results that represent a significant improvement over existing algorithms.</p>
<p>In Section 5.1 we talk about the evaluation platform and the feature set that we used to evaluate our exploration strategy. Section 5.1.1 introduces the Arcade Learning Environment (ALE) as our evaluation platform. We provides justification for choosing ALE as an environment for our agent. Further, in Section 5.1.2 we introduce the Blob-PROST feature set, and the benefits our agent enjoys from using it.</p>
<p>In Section 5.2 we provide the necessary foundations needed to evaluate Algorithm 8. We discuss the aspects that need to be considered in choosing a particular game for evaluation. Further, we talk about the parameters for empirical evaluation, such are number of trial, training frames, etc.</p>
<p>Lastly, in Section 5.3 we discuss the results of our various experiments. We compare the two action selection process discussed in Section 4.2.4 and compare their empirical performance. Then we compare the learning performance of our agent with SARSA(λ)+ -greedy. Finally, we compare the evaluation scores for our agent with other leading algorithms.</p>
<p>Evaluation Framework</p>
<p>Arcade Learning Algorithm (ALE)</p>
<p>The Arcade Learning Environment (ALE) (Bellemare et al., 2013) is a software framework that interfaces with the Stella emulator (Mott and Team, 1996) for the Atari 2600 games (Montfort and Bogost, 2009). The Atari 2600 platform contains hundreds of games that vary in many aspects of game-playing such as sports, puzzle, action, adventure, arcade, strategy etc. (Figure 5.1). Some of the games are quite challenging for human players (Bellemare et al., 2013). Due to the diverse nature of the games, a learning algorithm that can play the entire gamut of the Atari 2600 games can be considered to be generally competent. The goal of the ALE framework is to provide a platform for AI researchers to test their learning algorithm for general competence, share empirical data with the research community, and further the goal of achieving artificial general intelligence (Bellemare et al., 2013).  The ALE contains a game-layer to facilitate reinforcement learning. The gamelayer takes in the action from the agent and validates if it is one of the predefined 18 discrete actions. The game-layer sends the validated action to the Stella emulator which performs the action on the chosen Atari 2600 game. The resulting screen frame and the RAM state is sent to the game-layer by the emulator. Each screen frame is a 160 × 210 2D array, with each element representing a 7-bit pixel. The game-layer analyses the frame and RAM state to identify the game score. It then transform the game score into an appropriate reward signal expected by an RL agent. Based on the configuration settings, the game-layer returns the frame array and/or RAM state as the observation.</p>
<p>After the advent of the ALE, the majority of reinforcement learning publications have used ALE to provide empirical evidence (Mnih et al., 2013(Mnih et al., , 2015Stadie et al., 2015;Bellemare et al., 2016;Tang et al., 2016;Ostrovski et al., 2017). Now, ALE enjoys the status as the standard test bed for testing RL algorithms. Therefore, in order to compare and contrast our results with existing research, it is crucial that we choose ALE as our evaluation platform.</p>
<p>Blob-PROST Feature Set</p>
<p>We observed breakthrough performance by DQN to achieve human level performance on majority of ALE games (Mnih et al., 2015). Subsequently Liang et al. (2015) did a systematic study to analyse the factors that resulted in such a dramatic increase in performance. As part of this study, they created feature sets that incorporated key representational biases encoded by DQN. One such data set is called Blob-PROST.</p>
<p>PROST in Blob-PROST stands for Pairwise Relative Offset in Space and Time. They argue that in most games absolute position of objects are not as important as their relative distance. Therefore, by taking into account the relative distance between two objects on screen, they are able to encode information like "there is a green pixel 5 tiles above a blue pixel". To encode the movements of objects, they take the relative distance of an object in the current frame to the object five frames in the past.</p>
<p>In an Atari game screen we can assume that there are many blocks of contiguous pixels with the same color. This is a common continuity assumption that is typically made in the context of computer vision. Liang et al. (2015) exploits this assumption and calls such blocks blobs. From a high level, Liang et al. (2015) first pre-process a frame to find blobs, and then find features based on pairwise relative distances. We refer the reader to Liang et al. (2015) for a full treatment on the construction of the feature set.</p>
<p>The Blob-PROST feature set contains a total of 114,702,400 binary features. Even though there are a large number of potential features, due to the sparsity of blobs, most of the features are never generated. Also, given a specific game only a relatively small number of features would be observed. Therefore, using Blob-PROST with LFA is far more computationally efficient that DQN. Moreover, empirical results from Liang et al. (2015) suggest that the fixed representations constructed in Blob-PROST has the same quality as those learned by DQN.</p>
<p>Empirical Evaluation</p>
<p>The evaluation methodology was designed to investigate and answer the following research questions:</p>
<p>• Does the novelty measure generalize state visit-counts when the generalization is performed in the same space (feature space) as the generalization regarding value?</p>
<p>• Is there performance improvement over different kinds of environments?</p>
<p>• How does the performance of our algorithm fare when compared to state-ofthe-art Deep RL algorithms?</p>
<p>Evaluation Methodology</p>
<p>We evaluate Algorithm 8 in the ALE using the Blob-PROST feature set. We evaluate our algorithm on a subset of the games that are most relevant to the problem of efficient directed exploration. An important factor that determines our selection is the time and computational resources required to perform the evaluation. Finally, we perform hyperparameter sweeps to appropriately tune our algorithm. </p>
<p>Choosing Evaluation Games</p>
<p>ALE contains many games which vary in the degree to which exploration is difficult (Bellemare et al., 2016). At the lower end of the difficulty spectrum are games for which undirected exploration ( -greedy) is sufficient to learn a good policy (Mnih et al., 2015). We evaluate our algorithm on hard games, i.e., games where -greedy fails to improve substantially on a random policy. Hence we focus our evaluation on games that are classified as hard in the taxonomy provided by Bellemare et al. (2016). In their taxonomy they further split hard exploration games into games that have sparse and dense rewards.</p>
<p>In dense-reward games, our RL agent can expect rewards, on average, every few cycles. Dense-reward games are easier for an RL agent because policy iteration techniques such as SARSA generally require regular feedback on the agent's policy to learn; this is known as the issue of temporal credit assignment (Sutton and Barto, 1998).</p>
<p>On the other hand, sparse-reward games only dispense rewards infrequently. In this setting, the agent must often perform long sequences of actions in the correct order in order to receive a reward 1 . Without a good exploration strategy, stumbling upon a productive sequence of action is very challenging. Therefore, our main focus will be on sparse-reward games in which exploration is difficult.</p>
<p>We compare our performance with that of state-of-the-art Deep RL algorithms reported in the literature, most of which are variants of DQN (van Hasselt et al., 2016). Recall that we use LFA for value prediction, while DQN uses neural networks. As discussed previously, the Blob-PROST feature set that we selected for LFA has the property that it closely models the representations learned by DQN (Liang et al., 2015). Since the Blob-PROST feature set has this property, it is appropriate to compare our φ-EB algorithm to the other algorithms that use DQN. Now from the empirical data presented in Liang et al. (2015), we look for games that perform similar to DQN, and meet the exploration difficult criterion outlined above.</p>
<p>With these in mind, we choose the following five hard exploration games to evaluate our exploration strategy. </p>
<p>Computational Roadblocks</p>
<p>The computational requirement for games in the ALE are very demanding, especially in the absence of graphical processing units (GPUs), which excel at the dense linear algebra computations common in vision-related tasks (Liang et al., 2015). Given the high-dimensional nature of the problem, agents must be trained for several days on end to obtain satisfactory performance. Frostbite and Q*bert were especially computationally intensive, and we had to train for several weeks to obtain sufficient data. Given our limited time and computational resources, we had to place the following constraints on our evaluation.</p>
<p>• In Section 4.2.1 we remarked that due to sparse nature of the feature vector our algorithm runs in O(M t ) where M t is the number of unique features observed till time t. This means that different games run at different speeds. We trained our algorithm on all games for 100 million frames except for Q*bert which was trained only for 80 million frames.</p>
<p>• Our main focus is to showcase performance gains in sparse reward hard exploration games. Therefore we focused bulk of our computational resources into running multiple trial for Montezuma's Revenge and Venture. This is where our algorithm leads other state-of-the-art exploration strategies.</p>
<p>Tuning the hyper-parameter β</p>
<p>We performed independent evaluation of all the chosen games for discrete values of β in the range [0.0001, 0.5]. Recall that β is a parameter that controls the magnitude of the exploration bonus. We observed that β = 0.05 is the best performing value for all the games except for Freeway. Recall that because of the nature of the game, there is a large number of unique Blob-PROST features active. If β is high enough, the chicken just remains stationary and receives novelty rewards for observing all the changes in the traffic. When we set β = 0.035, our agent performed much better and delivered comparable results with the baseline algorithm.</p>
<p>Training Methodology</p>
<p>Training and evaluation of learning algorithms in high-dimensional spaces is computationally demanding. Due to the constantly evolving nature of the field, there is no general consensus on how many cycles is required to train an agent. Some of the major exploration algorithms published recently report training till 200 million frames. Due to limited time and computational resources, this amount of training is not feasible for us. We perform empirical evaluation of Montezuma's Revenge and Venture for five independent trials, and two independent trials for Frostbite, Freeway, and Q<em>bert. With the exception of Q</em>bert, all agents are trained for 100 Million frames, and then evaluated for 500 episodes. The result for Q*bert is reported after training for 80 million frames with subsequent 500 episodes of training.</p>
<p>We use the average score per episode, which is a common metric used to report scores (Bellemare et al., 2013;Mnih et al., 2015;Bellemare et al., 2016).</p>
<p>Sparse Reward Games</p>
<p>Montezuma's Revenge</p>
<p>Montezuma's Revenge is widely regarded as one of the most difficult games in the Atari 2600 suite. Learning algorithms typically suffer here due to the problem of long term credit assignment and sparse rewards. For example, DQN with -greedy exploration achieves a score of 0 after training for 200 million frames (Mnih et al., 2015). In order to get the very first reward of the game the agent must climb down a ladder, jump onto the pole, jump onto a raised platform, climb down a ladder, walk left and jump to avoid an enemy, climb another ladder, and finally obtain a golden key. This long sequence of complex actions is required to simply achieve the first reward in the first of 24 rooms, arranged in a pyramid structure (Figure 5.4). It is evident from the complexity of the game that a random exploration strategy will fail miserably. The challenges posed by this game make the game central to our evaluation. Our agent showed steady learning progress and was relatively quick to learn the initial sequence of actions needed to get the golden key. Analysis from the logs showed increasing novelty measure associated with novel events. For example, one of the rooms had a dead-end for one side, but there was a reward there. When our agent reached the dead-end and received the reward once, it kept getting stuck in that region for a few episodes. During the episodes where the agent was stuck, we could observe the novelty values of the other end slowly rising. After a few episodes the agent started going in the opposite direction based on the novelty rewards it was getting, and proceeded to explore further.</p>
<p>In Montezuma's revenge, the total number of rooms visited is a good measure of exploration efficiency (Bellemare et al., 2016). Prior to the work of Bellemare et al. (2016) no agent had visited more that 3 rooms without domain specific tailoring. Bellemare et al. (2016)'s agent visited 15 room while our agent visited 14 rooms in total (Figure 5.4). Both Bellemare et al. (2016)'s and our agent enjoy a peak score of 6600, which is the highest reported score.</p>
<p>Venture</p>
<p>Venture is another hard, sparse reward exploration game with complex visual representations. Figure 5.7 represents the two different visual states of venture. In Figure 5.5 the agent is depicted by the small tiny pink pixel towards the bottom middle of the screen. When the agent is in the outer level it is powerless. The only way to stay alive is to navigate the maze and/or entering one of the rooms to avoid the green goblins. Once the agent enters the room, the screen changes to Figure 5.6. Now our agent is transformed into a smiley face with the ability to fire a projectile weapon. Here the agent is chased by evil blue robots which can be destroyed by the agent's weapon. The small tiny cup-like structure in the bottom left corner of the room is the goal and the first reward. Destroying the blue monsters does not result in any reward. The game presents some intricate dynamics, with a large state-space to explore, and so is difficult for a naive learning algorithm. Therefore Venture is a must-have game in our evaluation roster. The agent initially moves around the black region to the bottom of the screen where it starts out ( Figure 5.5). The novelty of the agent with empty black space around it quickly reduces. We observed that the agent starts to hug the outer walls of the rooms. The room walls together with the agents are novel features. Then the agent moves along the room wall towards the entry of the room. When the agent enters the room the screen now transforms as shown in Figure 5.6. We later observe that, with subsequent visits to the lower room our agent learned to shoot and kill the blue robots and get the reward. Our agent achieves substantial improvement over Bellemare et al. (2016). Even though they do not report their evaluation score for Venture, Ostrovski et al. (2017) reports the score of DQN+CTS-EB as 82.2, whereas our evaluation score is 1169.2. Here the agent is a chicken that must cross a busy highway ( Figure 5.3 Top layer, third game). The agent receives the reward only when it reaches the other end of the road. Being hit by a car sets you back some positions down the road. Even though this is not a complex game, it is still a hard exploration game. In freeway, cars are always moving from left to right in every frame. Therefore, there is a large number of active unique Blob-PROST features.</p>
<p>Freeway</p>
<p>When tuning the hyper-parameter β we talked about the large number of unique Blob-PROST features created due to the constant movement of traffic. These changing unique features constantly floods the agent with high novelty rewards. Therefore the agent is willing to just stand and observe the traffic. When we reduced the exploration coefficient β, the agent's extrinsic reward is no longer overwhelmed by the novelty reward. Also, due to the initial optimism, the agent is encouraged to move upwards. Once the agent manages to cross the road once, it reinforces the up action and there the agent starts learning faster. In Q*Bert, the agent stands on a pyramid of cubes ( Figure 5.9). The goal of the agent is to jump on all the cubes without falling off the edge or being captured by an adversary. When the agent has highlighted all the cubes by jumping on them at least once, the level is cleared. The game has multiple levels. On higher levels, the task still remain the same, but the enemies become smarter, making it increasingly difficult to accomplish the task while avoiding capture. As discussed in Section 3.1.1 the only other difference between levels is the choice of color scheme. Figure 5.10: Frostbite (Atari2600)</p>
<p>Dense Reward Games Q*bert</p>
<p>Frostbite</p>
<p>In Frostbite, the agent is an Eskimo jumping up and down on an ice platform which magically results in the building of an igloo (Figure 5.10). Each time the agent jumps on a pure white ice platform, the agent receives a reward. The agents has to consistently do this by avoiding obstacles/dangers, and picking up bonus points. Once the igloo has been built the agent need to perform a level-end move by entering into the igloo. Since there are multiple ways of maximizing the score, and the existence of a level-end move makes it a hard exploration game. Our main aim with this game is to confirm that our exploration strategy improves upon the baseline performance.</p>
<p>Results</p>
<p>From here on we denote our agent SARSA(λ)+φ-EB as SARSA-φ-EB and SARSA(λ)+greedy as SARSA-for notational brevity. Recall that in Section 4.2.4 we discussed the problem of action selection and presented two methods that introduce stochasticity into the agent's policy. Here we compare two agents differing only in the action selection functionality. SARSA-Boltz-φ-EB introduces stochasticity via a Boltzmann-distributed random action, whereas SARSAφ-EB does via -greedy. Figure 5.11 shows the learning curve for the two agent evaluated for the games Montezuma's Revenge and Venture. From the plots we can observe that during the initial training period Boltzmann-distributed action selection has an advantage over epsilon-greedy. In the long run both have essentially the same average score. This behavior is expected since is annealed with training cycles, and hence with time the agent takes fewer purely exploratory actions.</p>
<p>Boltzmann vs. -greedy Action Selection</p>
<p>What we hoped to see in this experiment is for the Boltzmann distributed action selection to send the agent into a steeper learning trajectory. Unfortunately, this experiment concludes that there is no long term gain to having a Boltzmann-distributed action selection process.</p>
<p>Comparison with -greedy</p>
<p>Overview Figure 5.12 shows the comparison between learning curves for our agent SARSAφ-EB, and the benchmark implementation SARSA-. The plots clearly illustrates that SARSA-φ-EB significantly outperforms SARSA-on both Montezuma's Revenge and Venture; two of hardest exploration games in the ALE. In Q*bert and Frostbite SARSA-φ-EB consistently outperforms SARSA-, but not by a huge margin. Frequent rewards from these games give a constant feedback to SARSA-, helping it to chart a positive learning path. With β = 0.05 freeway fails to obtain any score, but with β = 0.035 SARSA-φ-EB achieved a marginally better performance that the baseline SARSA-. </p>
<p>Montezuma's Revenge</p>
<p>After an average of 20 Million frames the policy of SARSA-converges. It learns the policy to consistently get the golden key, buy rarely leaves the room. Even when SARSA-leaves the room it is always to the left room and gets immediately killed by the laser beams. SARSA-never learns how to get through the laser beams. SARSAφ-EB performs remarkably well, visiting 14 rooms in total and observing a peak score of 6600 around the 40 Million frame mark. SARSA-φ-EB learns to go through laser doors by timing the movements perfectly, avoid dead-ends, duck and jump over skulls, etc.</p>
<p>Venture</p>
<p>SARSA-fails to score any points while SARSA-φ-EB performs exceptionally well in Venture. It is the most impressive performance gain in this evaluation. SARSA-φ-EB can quickly identify that spending time in the black region is not novel. It is attracted (because of novelty) to the walls of the room quickly. The agent then moves along the wall until it sees a nearby opposite wall or the room entrance. Once the agent starts entering a room consistently, it quickly learns how to attack blue robots and obtains the reward. The positive learning trend suggest that even higher scores are possible with more training.</p>
<p>Freeway</p>
<p>With β = 0.05 our agent SARSA-φ-EB fails to cross the road for reasons previously mentioned in Section 5.2 and Section 5.2.2. But with β = 0.035, SARSA-φ-EB explores quickly initially and received the reward by crossing the road for the first time. Due to the undirected nature of exploration, SARSA-has to first random walk across the road to receive a reward. Once both the algorithms receive a reward they perform similarly, with SARSA-φ-EB doing marginally better.</p>
<p>Recall that the components of SARSA-φ-EB were designed such that there is only a single point of change from SARSA-. The only difference between SARSA-φ-EB and SARSA-is our exploration module, φ-EB. Hence we can conclude that the empirical performance gains is solely due to the introduction of our exploration algorithm. Further we can conclude that φ − EB is significantly better that -greedy in sparse reward hard exploration domains, and consistently outperforms -greedy in all domains. Table 5.1 shows the evaluation score for the final policy learnt by our agent. In order to compare and contrast the efficacy of our algorithm, Table 5.1 also presents the evaluation scores reported by leading RL algorithms. Table 5.1: Average evaluation score for leading algorithms. Sarsa-φ-EB and Sarsawere evaluated after 100M training frames on all games except Q*bert, for which they trained for 80M frames. All other algorithms were evaluated after 200M frames. (Martin et al., 2017) Our agent reports an average evaluation score of 2745.4 on Montezuma's Revenge. This score is the second-highest reported score for Montezuma, the leading score reported for DQN+CTS-EB by Bellemare et al. (2016). Algorithms such as MP-EB (Stadie et al., 2015), TRPO-Hash (Tang et al., 2016), A3C+ (Bellemare et al., 2016) does not score more than 200 points in Montezuma's Revenge, despite the presence of an exploration strategy module.</p>
<p>Comparison with Leading Algorithms</p>
<p>SARSA-φ-EB evaluated on Venture reported a score of 1169.2, the third-highest reported score. Again it outperforms all the other exploration algorithms. Bellemare et al. (2016) does not report their score for Venture. A recent evaluation of DQN+CTS-EB done by Ostrovski et al. (2017) reported that DQN+CTS-EB obtained a score of 82.2 on venture.</p>
<p>It is known that non-linear algorithms perform much better on Q*bert therefore our results is not surprising (Wang et al., 2015). Frostbite is notoriously slow to train because of the large number of features. Given the dense reward nature of the game and the small training time, we achieve competitive results.</p>
<p>Our results for Montezuma and Venture are extremely competitive and can be considered state-of-the-art. The key observation from Table 5.1 is this, if we look at the score of Montezuma and Venture together, ours is the only algorithm that achieve state-of-the-art results in both the games.</p>
<p>In this thesis we have presented the φ-Exploration Bonus (φ-EB) method, a novel approach to perform directed exploration in large problem domains. The algorithm is simple to implement, and is compatible with any value-based RL algorithm that uses Linear Function Approximation (LFA) to predict value. Our method also enjoys lower computational requirements when compared to other leading exploration strategies. Our empirical evaluation demonstrates that measuring novelty in feature space is a simple and effective way to drive efficient exploration on MDPs of practical interest. It also lends support to our hypothesis that defining a novelty measure in feature space is a principled way to generalize state-visit counts to large problems. In contrast to other approaches, measuring novelty in feature space avoids building an exploration-specific state-representation. Instead, our method exploits the taskrelevant features that are already being used for value estimation.</p>
<p>There are myriad ways in which this work could be extended, and the problem of efficient exploration in large MDPs is still wide open. A promising direction for future research would be a rigorous empirical comparison of the various generalized count-based algorithms which we have discussed. At present, many of the reported results are not helpful in deciding which is the better approach to measuring novelty or generalizing visit-counts. Different exploration algorithms are presented in conjunction with totally different value-estimation methods, and it can be difficult to discern whether or not the exploration method used is responsible for the quality of the results. More theoretical understanding of the problem of exploration in the high-dimensional setting is also sorely needed, and we hope to build upon the results presented here in future work.</p>
<p>Figure 1 . 1 :
11The agent-environment interaction cycle</p>
<p>Figure 2 . 1 :
21Generalized Policy Iteration</p>
<p>•
Gradient</p>
<p>Figure 3 Figure 3 . 3 :
333Two levels of the Atari2600 game Q*bert</p>
<p>Figure 3 . 4 :
34Novelty Measure in Feature Space</p>
<p>Figure 3 . 5 :
35Flow Chart for computing the exploration bonus of φ-EB Algorithm 1: φ-exploration bonus Input: Density Model ρ 1 function FeatureVisitDensity(</p>
<p>Figure 4 . 1 :
41Agent-Environment interaction framework for SARSA(λ)+φ-EB. 1</p>
<p>Factor Distribution Map fvd _ map φ i , ρ i 1 function LogFeatureVisitDensity(= 1 to |φ| do 4 if φ i ∈ fvd _ map.keys then // O(1) = 1 to size(fvd _ map.keys) do 10 if φ i ∈ φ then // O(1) look-up with flag trick 11 sum_log_rho ← sum_log_rho + log 1 − fvd _ map[φ i ]</p>
<p>Figure 5 . 1 :
51Game screens from 55 Atari 2600 games(Defazio and Graepel, 2014).</p>
<p>Figure 5 . 2 :
52High level working of ALE for an RL algorithm.</p>
<p>Figure 5 . 3 :
53Five games in which exploration is both difficult and crucial to performance. From top left: Montezuma's Revenge, Venture, Freeway, Frostbite, and Q*Bert.</p>
<p>Figure 5 . 4 :
54Montezuma's Revenge: Rooms visited by undirected exploration (DQN+ -greedy, above) vs. directed exploration (DQN+CTS-EB, below)(Bellemare et al., 2016).</p>
<p>Figure 5 Figure 5 . 7 :
557Two visual states of venture (Atari 2600)</p>
<p>Figure 5
5Figure 5.8: Freeway (Atari2600)</p>
<p>Figure 5
5Figure 5.9: Qbert (Atari2600)</p>
<p>Figure 5 .
511: Average training score for Action Selection using Boltzmann vs.greedy. Shaded regions describe one standard deviation. Dashed lines represent min/max scores.</p>
<p>Figure 5 .
512: Average training score for SARSA-φ-EB vs. SARSA-. Shaded regions describe one standard deviation. Dashed lines represent min/max scores.(Martin et al., 2017) 
To the best of our knowledge, the first work to use exploration bonuses in the MDP setting was the Dyna-Q+ algorithm, in which the bonus is a function of the recency of visits to a state, rather than the visit-count(Sutton, 1990) 
In Q*bert, the goal of the agent is to jump on all the cubes without falling off the edge, or being captured.2 ALE is a performance evaluation platform consisting of Atari2600 games. It is considered as the standard performance test bed for RL algorithms. We'll discuss in depth about ALE in Chapter 4.
A Bayesian variable-order Markov model.
Also called the recoding probability.
Boxes with a tiny row and column, on top and left edges respectively, denote objects stored in RAM. They can persists across cycles and episodes. Dotted arrows with instruction on them denote operation on such objects. 2 Discussed in Section 5.1.2.
C++11 is a major revision of C++. This particular version was chosen because it makes several useful additions to core language libraries.
Here we can see that generalization regarding value and novelty are being done in the same space.
In this context, a prototype function creates an object of a specified type. Here, a KT estimator which has seen t zeros.
When using LFA, training is done on the LFA parameters. Therefore we essentially maintain two sets of parameters, θ E and θ I
Classic examples of such games are the board games Chess and Go; rewards are only dispensed once, at the very end of the game.
AcknowledgmentsAt the culmination of two years of hard work, I would like to take this opportunity to acknowledge the role they have played in my life./ * Alternatively: -greedy action selection.M t is the number feature observed til timestep t. We have removed the details regarding eligibility traces for brevity and clarity.
Reinforcement learning and its relationship to supervised learning. Handbook of Learning and Approximate Dynamic Programming. A Barto, T Dietterich, Barto, A. and Dietterich, T., 2004. Reinforcement learning and its relationship to supervised learning. Handbook of Learning and Approximate Dynamic Programming, (2004), 47-64. (cited on page 1)</p>
<p>The Arcade Learning Environment: An Evaluation Platform for General Agents. M G Bellemare, Y Naddaf, J Veness, M Bowling, 10.1613/jair.3912Journal of Artificial Intelligence Research. 47cited on pages 24, 34, 45, 46, and 50Bellemare, M. G.; Naddaf, Y.; Veness, J.; and Bowling, M., 2013. The Arcade Learning Environment: An Evaluation Platform for General Agents. Journal of Artificial Intelligence Research, 47 (7 2013), 253-279. doi:10.1613/jair.3912. (cited on pages 24, 34, 45, 46, and 50)</p>
<p>Unifying Count-Based Exploration and Intrinsic Motivation. M G Bellemare, S Srinivasan, G Ostrovski, T Schaul, D Saxton, R Munos, Bellemare, M. G.; Srinivasan, S.; Ostrovski, G.; Schaul, T.; Saxton, D.; and Munos, R., 2016. Unifying Count-Based Exploration and Intrinsic Motivation.</p>
<p>. Corr, abs/1606.0cited on pages xiiiCoRR, abs/1606.0 (6 2016), 1-26. (cited on pages xiii, 3, 12, 14, 15, 17, 20, 21, 25, 26, 33, 34, 41, 47, 48, 49, 50, 51, 52, 53, and 58)</p>
<p>. D P Bertsekas, J N Tsitsiklis, 10.1109/MCSE.1998.683749Neuro-Dynamic Programming. 59Bertsekas, D. P. and Tsitsiklis, J. N., 1996. Neuro-Dynamic Programming, vol. 5. Athena Scientific. ISBN 1886529108. doi:10.1109/MCSE.1998.683749. (cited on pages 7 and 9)</p>
<p>Pattern Recognition and Machine Learning. C M Bishop, 10.1117/1.2819119Journal of Electronic Imaging. 1649901Bishop, C. M., 2007. Pattern Recognition and Machine Learning. Journal of Electronic Imaging, 16, 4 (1 2007), 049901. doi:10.1117/1.2819119. (cited on page 1)</p>
<p>Bayesian Q-learning. R Dearden, N Friedman, S Russell, Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence. the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence15Dearden, R.; Friedman, N.; and Russell, S., 1998. Bayesian Q-learning. Proceedings of the fifteenth national/tenth conference on Artificial intelligence/Innovative applications of artificial intelligence, (1998), 761-768. (cited on page 15)</p>
<p>A Comparison of learning algorithms on the Arcade Learning Environment. A Defazio, T Graepel, abs/1410CoRR46Defazio, A. and Graepel, T., 2014. A Comparison of learning algorithms on the Arcade Learning Environment. CoRR, abs/1410.8 (10 2014). (cited on page 46)</p>
<p>Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. M Duff, 16University of Massachusetts at AmherstPh.D. thesisDuff, M., 2002. Optimal Learning: Computational procedures for Bayes-adaptive Markov decision processes. Ph.D. thesis, University of Massachusetts at Amherst. (cited on page 16)</p>
<p>Curiosity driven reinforcement learning for motion planning on humanoids. M Frank, J Leitner, M Stollenga, A Förster, J Schmidhuber, 10.3389/fnbot.2013.00025Frontiers in Neurorobotics. 716Frank, M.; Leitner, J.; Stollenga, M.; Förster, A.; and Schmidhuber, J., 2014. Curiosity driven reinforcement learning for motion planning on humanoids. Fron- tiers in Neurorobotics, 7, JAN (2014), 1-15. doi:10.3389/fnbot.2013.00025. (cited on page 16)</p>
<p>Universal Artificial Intellegence. Texts in Theoretical Computer Science An EATCS Series. M Hutter, 10.1007/b138233SpringerBerlin Heidelberg; Berlin, HeidelbergHutter, M., 2005. Universal Artificial Intellegence. Texts in Theoretical Computer Science An EATCS Series. Springer Berlin Heidelberg, Berlin, Heidelberg. ISBN 978-3-540-22139-5. doi:10.1007/b138233. (cited on page 5)</p>
<p>Sparse Adaptive Dirichlet-Multinomial-like Processes. M Hutter, Journal of Machine Learning Research. 30cited on page 25Hutter, M., 2013. Sparse Adaptive Dirichlet-Multinomial-like Processes. Journal of Machine Learning Research, 30 (5 2013). (cited on page 25)</p>
<p>On the Sample Complexity of Reinforcement Learning. S M Kakade, doi:10.1.1.164.7844International Conference on Machine Learning. 14Kakade, S. M., 2003. On the Sample Complexity of Reinforcement Learning. In International Conference on Machine Learning, 133. doi:10.1.1.164.7844. (cited on page 14)</p>
<p>Near-Bayesian exploration in polynomial time. J Z Kolter, A Y Ng, 10.1145/1553374.1553441Proceedings of the 26th Annual International Conference on Machine Learning -ICML '09. the 26th Annual International Conference on Machine Learning -ICML '09New York, New York, USAACM Press14Kolter, J. Z. and Ng, A. Y., 2009. Near-Bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning -ICML '09, 1-8. ACM Press, New York, New York, USA. doi:10.1145/1553374.1553441. (cited on page 14)</p>
<p>The Performance of Universal Encoding. R E Krichevsky, V K Trofimov, 10.1109/TIT.1981.1056331IEEE Transactions on Information Theory. 2725Krichevsky, R. E. and Trofimov, V. K., 1981. The Performance of Universal En- coding. IEEE Transactions on Information Theory, 27, 2 (1981), 199-207. doi: 10.1109/TIT.1981.1056331. (cited on page 25)</p>
<p>Asymptotically efficient adaptive allocation rules. T Lai, H Robbins, 10.1016/0196-8858(85)90002-8Advances in Applied Mathematics. 614Lai, T. and Robbins, H., 1985. Asymptotically efficient adaptive allocation rules. Ad- vances in Applied Mathematics, 6, 1 (3 1985), 4-22. doi:10.1016/0196-8858(85)90002-8. (cited on page 14)</p>
<p>Building Machines That Learn and Think Like People. B M Lake, T D Ullman, J B Tenenbaum, S J Gershman, 10.1017/S0140525X16001837Behavioral and Brain Sciences. 1116Lake, B. M.; Ullman, T. D.; Tenenbaum, J. B.; and Gershman, S. J., 2016. Building Machines That Learn and Think Like People. Behavioral and Brain Sciences, (11 2016), 1-101. doi:10.1017/S0140525X16001837. (cited on page 16)</p>
<p>Exploration Potential. CoRR. J Leike, abs/1609.016Leike, J., 2016. Exploration Potential. CoRR, abs/1609.0 (9 2016), 1-9. (cited on page 16)</p>
<p>State of the Art Control of Atari Games Using Shallow Reinforcement Learning. Y Liang, M C Machado, E Talvitie, M Bowling, Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems. the 2016 International Conference on Autonomous Agents &amp; Multiagent Systemscited on pages 24, 32, 33, 47, and 49Liang, Y.; Machado, M. C.; Talvitie, E.; and Bowling, M., 2015. State of the Art Control of Atari Games Using Shallow Reinforcement Learning. Proceedings of the 2016 International Conference on Autonomous Agents &amp; Multiagent Systems, (12 2015), 485-493. (cited on pages 24, 32, 33, 47, and 49)</p>
<p>Count-Based Exploration in Feature Space for Reinforcement Learning. J Martin, S Narayanan, S Everitt, T Hutter, M , Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence. the Twenty-Sixth International Joint Conference on Artificial IntelligenceAAAI Presscited on pages iii, xiii, xv, 56, and 58Martin, J.; Narayanan S, S.; Everitt, T.; and Hutter, M., 2017. Count-Based Ex- ploration in Feature Space for Reinforcement Learning. In Proceedings of the Twenty- Sixth International Joint Conference on Artificial Intelligence. AAAI Press. (cited on pages iii, xiii, xv, 56, and 58)</p>
<p>An analysis of reinforcement learning with function approximation. F S Melo, S P Meyn, M I Ribeiro, 10.1145/1390156.1390240Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learning33Melo, F. S.; Meyn, S. P.; and Ribeiro, M. I., 2008. An analysis of reinforcement learning with function approximation. Proceedings of the 25th international conference on Machine learning, (2008), 664-671. doi:10.1145/1390156.1390240. (cited on page 33)</p>
<p>. V Mnih, K Kavukcuoglu, D Silver, A Graves, I Antonoglou, D Wierstra, M Riedmiller, arXiv:1312.560247Playing Atari with Deep Reinforcement Learning. arXiv preprintMnih, V.; Kavukcuoglu, K.; Silver, D.; Graves, A.; Antonoglou, I.; Wierstra, D.; and Riedmiller, M., 2013. Playing Atari with Deep Reinforcement Learning. arXiv preprint arXiv:1312.5602, (12 2013). (cited on pages 34 and 47)</p>
<p>Human-level control through deep reinforcement learning. V Mnih, K Kavukcuoglu, D Silver, A A Rusu, J Veness, M G Bellemare, A Graves, M Riedmiller, A K Fidjeland, G Ostrovski, S Petersen, C Beattie, A Sadik, I Antonoglou, H King, D Kumaran, D Wierstra, S Legg, D Hassabis, 10.1038/nature14236Nature. 518cited on pages 34, 47, 48, 50, and 51Mnih, V.; Kavukcuoglu, K.; Silver, D.; Rusu, A. A.; Veness, J.; Bellemare, M. G.; Graves, A.; Riedmiller, M.; Fidjeland, A. K.; Ostrovski, G.; Petersen, S.; Beat- tie, C.; Sadik, A.; Antonoglou, I.; King, H.; Kumaran, D.; Wierstra, D.; Legg, S.; and Hassabis, D., 2015. Human-level control through deep reinforcement learning. Nature, 518, 7540 (2015), 529-533. doi:10.1038/nature14236. (cited on pages 34, 47, 48, 50, and 51)</p>
<p>Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning. S Mohamed, D J Rezende, 16ArXiv e-printsMohamed, S. and Rezende, D. J., 2015. Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning. ArXiv e-prints, (9 2015). (cited on page 16)</p>
<p>Racing the beam: the Atari Video computer system. N Montfort, I Bogost, 10.1162/leon.2010.43.2.188Platform studies. 45Montfort, N. and Bogost, I., 2009. Racing the beam: the Atari Video computer system. Platform studies, (2009), xii, 180 p. doi:10.1162/leon.2010.43.2.188. (cited on page 45)</p>
<p>Learning to trade via direct reinforcement. J Moody, M Saffell, 10.1109/72.935097IEEE Transactions on Neural Networks. 1223Moody, J. and Saffell, M., 2001. Learning to trade via direct reinforcement. IEEE Transactions on Neural Networks, 12, 4 (2001), 875-889. doi:10.1109/72.935097. (cited on page 23)</p>
<p>Stella: a multiplatform Atari 2600 VCS emulator. B W Mott, S Team, 45Mott, B. W. and Team, S., 1996. Stella: a multiplatform Atari 2600 VCS emulator. (cited on page 45)</p>
<p>Deep Exploration via Bootstrapped DQN. I Osband, C Blundell, A Pritzel, B Van Roy, Advances In Neural Information Processing Systems. 2cited on pages 14, 34, and 47Osband, I.; Blundell, C.; Pritzel, A.; and Van Roy, B., 2016. Deep Exploration via Bootstrapped DQN. Advances In Neural Information Processing Systems, (2 2016). (cited on pages 14, 34, and 47)</p>
<p>Why is Posterior Sampling Better than Optimism for Reinforcement Learning. I Osband, B Van Roy, 12arXiv preprintOsband, I. and Van Roy, B., 2016. Why is Posterior Sampling Better than Optimism for Reinforcement Learning. arXiv preprint, (7 2016). (cited on page 12)</p>
<p>Count-Based Exploration with Neural Density Models. G Ostrovski, M G Bellemare, A V Oord, R Munos, arXiv preprintcited on pages 15, 34, 47, 53, and 58Ostrovski, G.; Bellemare, M. G.; Oord, A. v. d.; and Munos, R., 2017. Count-Based Exploration with Neural Density Models. arXiv preprint, (3 2017). (cited on pages 15, 34, 47, 53, and 58)</p>
<p>What is intrinsic motivation? A typology of computational approaches. P.-Y Oudeyer, 10.3389/neuro.12.006.2007Frontiers in Neurorobotics. 116Oudeyer, P.-Y., 2007. What is intrinsic motivation? A typology of computational approaches. Frontiers in Neurorobotics, 1, NOV (2007), 1-14. doi:10.3389/neuro.12. 006.2007. (cited on page 16)</p>
<p>Markov Decision Processes: Discrete Stochastic Dynamic Programming. M Puterman, 10.1002/9780470316887Wiley Series in Probability and Statistics. Hoboken, NJ, USA, 1 ednJohn Wiley &amp; Sons, IncPuterman, M., 1994. Markov Decision Processes: Discrete Stochastic Dynamic Program- ming. Wiley Series in Probability and Statistics. John Wiley &amp; Sons, Inc., Hoboken, NJ, USA, 1 edn. ISBN 9780470316887. doi:10.1002/9780470316887. (cited on page 5)</p>
<p>A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers. J Schmidhuber, ISBN 0-262-63138-5.From Animals to Animats:Proceedings of the First International Conference on Simulation of Adaptive Behavior. Paris, FranceMIT Press16Schmidhuber, J., 1991. A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers. In From Animals to Animats:Proceedings of the First International Conference on Simulation of Adaptive Behavior, 222-227. MIT Press, Paris, France. ISBN 0-262-63138-5. (cited on page 16)</p>
<p>Formal Theory of Creativity, Fun, and Intrinsic Motivation. J Schmidhuber, 10.1109/TAMD.2010.2056368IEEE Transactions on Autonomous Mental Development. 2316Schmidhuber, J., 2010. Formal Theory of Creativity, Fun, and Intrinsic Motivation. IEEE Transactions on Autonomous Mental Development, 2, 3 (9 2010), 230-247. doi: 10.1109/TAMD.2010.2056368. (cited on page 16)</p>
<p>Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms. S Singh, T Jaakkola, M L Littman, C Szepes, A S Hu, Machine Learning. 3911Singh, S.; Jaakkola, T.; Littman, M. L.; Szepes, C.; and Hu, A. S., 2000. Con- vergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms. Machine Learning, 39, 1998 (2000), 287-308. (cited on page 11)</p>
<p>Core knowledge. E S Spelke, K D Kinzler, 10.1111/j.1467-7687.2007.00569.xDevelopmental Science. 1016Spelke, E. S. and Kinzler, K. D., 2007. Core knowledge. Developmental Science, 10, 1 (1 2007), 89-96. doi:10.1111/j.1467-7687.2007.00569.x. (cited on page 16)</p>
<p>B C Stadie, S Levine, P Abbeel, Incentivizing Exploration In Reinforcement Learning With Deep Predictive Models. arXiv. cited on pages 20, 34, 47, and 58Stadie, B. C.; Levine, S.; and Abbeel, P., 2015. Incentivizing Exploration In Rein- forcement Learning With Deep Predictive Models. arXiv, (2015), 1-11. (cited on pages 20, 34, 47, and 58)</p>
<p>Resource-Bounded Machines are Motivated to be Effective, Efficient, and Curious. B R Steunebrink, J Koutník, K R Thórisson, E Nivel, J Schmidhuber, 10.1007/978-3-642-39521-5{<em>}13Artificial General Intelligence: 6th International Conference, AGI 2013, Beijing. China; Berlin HeidelbergSpringer799916ProceedingsSteunebrink, B. R.; Koutník, J.; Thórisson, K. R.; Nivel, E.; and Schmidhuber, J., 2013. Resource-Bounded Machines are Motivated to be Effective, Efficient, and Curious. In Artificial General Intelligence: 6th International Conference, AGI 2013, Bei- jing, China, July 31 -August 3, 2013 Proceedings, vol. 7999 LNAI, 119-129. Springer Berlin Heidelberg. ISBN 978-3-642-39521-5. doi:10.1007/978-3-642-39521-5{</em>}13. (cited on page 16)</p>
<p>An empirical evaluation of interval estimation for Markov decision processes. A L Strehl, M L Littman, 10.1109/ICTAI.2004.28Proceedings -International Conference on Tools with Artificial Intelligence. -International Conference on Tools with Artificial IntelligenceICTAI14Strehl, A. L. and Littman, M. L., 2004. An empirical evaluation of interval esti- mation for Markov decision processes. In Proceedings -International Conference on Tools with Artificial Intelligence, ICTAI, 128-135. doi:10.1109/ICTAI.2004.28. (cited on page 14)</p>
<p>An analysis of model-based Interval Estimation for Markov Decision Processes. A L Strehl, M L Littman, 10.1016/j.jcss.2007.08.009Journal of Computer and System Sciences. 74814Strehl, A. L. and Littman, M. L., 2008. An analysis of model-based Interval Esti- mation for Markov Decision Processes. Journal of Computer and System Sciences, 74, 8 (12 2008), 1309-1331. doi:10.1016/j.jcss.2007.08.009. (cited on page 14)</p>
<p>Reinforcement Learning: An Introduction. R Sutton, A Barto, Cambridge. ISBN 0262193981.MIT press1cited on pages xiii, 1, 2, 7, 8, 9, 10, 11, 23, 32, and 49Sutton, R. and Barto, A., 1998. Reinforcement Learning: An Introduction, vol. 1. MIT press Cambridge. ISBN 0262193981. (cited on pages xiii, 1, 2, 7, 8, 9, 10, 11, 23, 32, and 49)</p>
<p>Learning to Predict by the Methods of Temporal Differences. R S Sutton, 10.1023/A:1022633531479Machine Learning. 3cited on page 6Sutton, R. S., 1988. Learning to Predict by the Methods of Temporal Differences. Machine Learning, 3, 1 (1988), 9-44. doi:10.1023/A:1022633531479. (cited on page 6)</p>
<p>Integrated Architectures for Learning, Planning, and Reacting Based on. R S Sutton, doi:10.1.1. 51.7362Approximating Dynamic Programming. ML. 14Sutton, R. S., 1990. Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. ML, (1990), 216-224. doi:10.1.1. 51.7362. (cited on page 14)</p>
<p>Reinforcement Learning: Past, Present and Future. R S Sutton, 10.1007/3-540-48873-1{<em>}26doi:10. 1007/3-540-48873-1{</em>}26.Asia-Pacific Conference on Simulated Evolution and Learning. Springercited on page 1Sutton, R. S., 1999. Reinforcement Learning: Past, Present and Future. In Asia- Pacific Conference on Simulated Evolution and Learning, 195-197. Springer. doi:10. 1007/3-540-48873-1{_}26. (cited on page 1)</p>
<h1>Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning. H Tang, R Houthooft, D Foote, A Stooke, X Chen, Y Duan, J Schulman, F De Turck, P Abbeel, arXiv:1611.04717arXiv preprintcited on pages 15, 21, 34, 47, and 58Tang, H.; Houthooft, R.; Foote, D.; Stooke, A.; Chen, X.; Duan, Y.; Schulman, J.; De Turck, F.; and Abbeel, P., 2016. #Exploration: A Study of Count-Based Explo- ration for Deep Reinforcement Learning. arXiv preprint arXiv:1611.04717, (2016), 1-11. (cited on pages 15, 21, 34, 47, and 58)</h1>
<p>Efficient Exploration In Reinforcement Learning. S B Thrun, Science. Thrun, S. B., 1992. Efficient Exploration In Reinforcement Learning. In Science, January, 1-44. (cited on pages 12 and 16)</p>
<p>Deep Reinforcement Learning with Double Q-learning. H Van Hasselt, A Guez, D Silver, AAAI. 49van Hasselt, H.; Guez, A.; and Silver, D., 2016. Deep Reinforcement Learning with Double Q-learning. In AAAI, 2094-2100. (cited on page 49)</p>
<p>Context tree switching. J Veness, K S Ng, M Hutter, M Bowling, 10.1109/DCC.2012.39Data Compression Conference Proceedings. 15Veness, J.; Ng, K. S.; Hutter, M.; and Bowling, M., 2012. Context tree switching. In Data Compression Conference Proceedings, 327-336. doi:10.1109/DCC.2012.39. (cited on page 15)</p>
<p>Dueling Network Architectures for Deep Reinforcement Learning. CoRR, abs/1511. Z Wang, T Schaul, M Hessel, H Van Hasselt, M Lanctot, N Freitas, 10.1109/MCOM.2016.7378425058Wang, Z.; Schaul, T.; Hessel, M.; van Hasselt, H.; Lanctot, M.; and de Freitas, N., 2015. Dueling Network Architectures for Deep Reinforcement Learning. CoRR, abs/1511.0, 9 (11 2015), 1-16. doi:10.1109/MCOM.2016.7378425. (cited on page 58)</p>
<p>Q-learning. C J C H Watkins, P Dayan, 10.1007/BF00992698Machine Learning. 89Watkins, C. J. C. H. and Dayan, P., 1992. Q-learning. Machine Learning, 8, 3-4 (1992), 279-292. doi:10.1007/BF00992698. (cited on page 9)</p>            </div>
        </div>

    </div>
</body>
</html>
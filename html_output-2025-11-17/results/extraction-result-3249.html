<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3249 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3249</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3249</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-232352537</p>
                <p><strong>Paper Title:</strong> <a href="https://www.aclweb.org/anthology/2021.naacl-main.247.pdf" target="_blank">Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</a></p>
                <p><strong>Paper Abstract:</strong> Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3249.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3249.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Q-learning style RL agent for text games that encodes observations and candidate actions with separate GRU encoders and scores action-observation pairs with an MLP; trained using temporal-difference loss and a replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with a natural language action space</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Q-network Q_phi(o,a) using two GRU encoders f_o (observation) and f_a (action), concatenated and decoded by an MLP g to produce Q-values; trained with TD loss, softmax action selection, prioritized experience replay across 8 parallel environment instances.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho benchmark (multiple interactive fiction games, e.g. ZORK I, ZORK III)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maximize in-game score in interactive fiction text-adventure games via selecting text actions given textual observations (standard RL episodic gameplay).</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay (replay buffer) and implicit memorization of Q-values via function approximator</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Stores (o,a,r,o') tuples in a prioritized replay buffer used for TD updates; agent behavior and learned Q-values effectively 'memorize' high-value observation-action tuples; uses GRU encoders but no explicit long-term external memory module.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score across games: 0.21 (final) and 0.38 (max observed) as reported in Table 1; per-game final/max scores reported (e.g. ZORK I final 39.4 / max 53).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Replay buffer and stable representations enable DRRN to learn to reach and repeat high-scoring trajectories in the same environment; allows memorization of high Q-values for observed (o,a) pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Representations trained only with TD loss can degenerate/overfit to game-specific reward structure (poor transfer), representing unseen states in a different subspace from seen states (limited generalization).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use of replay buffer helps memorization but can lead to overfitting; authors suggest semantic regularization (e.g., inverse-dynamics) and evaluation protocols that require generalization across games to avoid reward-driven overfitting.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3249.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3249.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HASH</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hashing ablation (random fixed encodings)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A semantic-ablation in which observation and action texts are replaced by hash-derived fixed random vectors, breaking semantic continuity but providing stable, distinct identifiers for states and actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>HASH (DRRN with hashed inputs)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Same DRRN architecture but with GRU encoders replaced/composed with a fixed hashing encoder: text strings hashed to integers then mapped deterministically to random Gaussian vectors (G seeded by hash); these vectors are used as representations (not learned) so the network learns Q-values over fixed random identifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho benchmark (multiple interactive fiction games, e.g. ZORK I, PENTARI)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maximize in-game score with observations/actions replaced by non-semantic hashed identifiers; evaluate whether agents can learn without language semantics.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit memorization of Q-values (episodic/instance-level memorization) supported by experience replay; fixed vector identifiers act as stable keys enabling memorization</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Text -> Python hash -> seed PyTorch RNG -> generate fixed Gaussian vector; these fixed vectors identify observations/actions so the Q-network can memorize which identifier-action pairs yield high Q-values; training still uses prioritized replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score across games: 0.25 (final) and 0.39 (max observed) in Table 1; outperforming DRRN on average final score and achieving notably higher final score on some games (e.g., PENTARI final 51.9 vs DRRN 26.5).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Stable, fixed identifiers make Q-learning easier and enable strong memorization of high-value observation-action pairs, leading to equal or better final performance than semantics-aware DRRN on many games.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Breaks semantic continuity and thus generalization and transfer: HASH transfer to a new but similar game is equivalent to training from scratch (representations not learned), and hashed representations are scattered (no clustering by semantic similarity).</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Stable representations can aid Q-learning, but relying on fixed non-semantic identifiers prevents transfer and semantic generalization; evaluate agents on transfer or generative-action settings to require semantic understanding.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3249.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3249.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MIN-OB</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Minimized Observation (location-only) ablation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An ablation that reduces each textual observation to only a short location phrase (loc(o)), removing descriptive semantic detail to isolate action semantics and test importance of observation detail.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>MIN-OB (DRRN with location-only observations)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN where observations are replaced with only the location phrase loc(o) (a single token/short phrase) while action candidates remain full text; GRU encoders still used for actions/locations; trained with TD loss and replay buffer.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho benchmark (multiple interactive fiction games, e.g. ZORK I, DEEPHOME, DRAGON)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maximize game score when observation information is minimized to location names, isolating the role of action semantics and testing the need for observation semantic detail.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>implicit memorization of Q-values via replay buffer; reduced observation space constrains what can be memorized</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Same prioritized replay buffer and training regime; observation input truncated to location phrase which reduces observation discriminability and hinders memorization of distinct game contexts.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score across games: 0.12 (final) and 0.35 (max observed) in Table 1; on most games explored similar maximum scores but had lower final episodic scores (failed to 'memorize' good experience).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Not applicable as MIN-OB reduces observational detail; authors interpret that distinguishing observations with more detailed text aids memorization of high-value experiences.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>MIN-OB often fails to memorize and reach high episodic scores despite exploring similar maximums — reducing observations to locations removes identifying information that helps an agent recall previously good trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Do not overly minimize observation inputs if the goal is stable episodic performance; observation detail can be important for enabling memorization of distinct states and rewarding experiences.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3249.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3249.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>INV-DY</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Inverse Dynamics Regularization (INV-DY)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An auxiliary inverse-dynamics decoder and reconstruction loss added to DRRN to regularize text representations, encourage encoding of action-relevant observation semantics, and provide intrinsic curiosity-like reward for exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>INV-DY (DRRN + inverse dynamics auxiliary task)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN augmented with: (1) an inverse-dynamics MLP g_inv that predicts the action representation from concatenated f_o(o) and f_o(o'), (2) a GRU decoder d to reconstruct action text from predicted representation (L_inv), (3) action reconstruction loss L_dec from f_a(a), and (4) intrinsic reward r_plus = L_inv to encourage exploration; total loss = L_TD + lambda1 L_inv + lambda2 L_dec.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Jericho benchmark (multiple interactive fiction games, e.g. ZORK I, DRAGON, OMNIQUEST)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Maximize game score while learning semantically regularized representations that (a) decode back to action text and (b) provide intrinsic rewards to guide exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>experience replay (replay buffer) plus improved generalization of learned representations (semantic memory in encoder space); implicit memory via Q-values and replay is retained</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>Same prioritized replay buffer used for TD training; auxiliary inverse-dynamics loss regularizes encoder representations so that f_o encodes action-relevant semantics and f_a can be reconstructed; intrinsic reward based on inverse-dynamics loss drives exploration to states where inverse dynamics is not yet learned.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Average normalized final score across games: 0.23 (final) and 0.40 (max observed) in Table 1; showed clear exploration gains on some games (e.g., ZORK I maximum score seen = 87 for INV-DY compared to <=55 for other models).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>INV-DY improved exploration (found higher maximum scores on several games), produced representations that generalize to unseen states by semantic similarity (t-SNE), and produced better transfer to ZORK III when encoders were fixed (score ~1 vs ~0.4 baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>While INV-DY improves exploration and representation semantics on some games, its final average performance is similar to HASH and DRRN overall; benefits are game-dependent. Auxiliary reconstruction losses and intrinsic rewards add complexity and may not uniformly improve final episodic scores.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use auxiliary inverse-dynamics decoding to regularize representations and provide intrinsic curiosity-like rewards to encourage exploration and semantic encoding; combine with evaluation protocols that test transfer/generalization rather than within-game memorization.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep reinforcement learning with a natural language action space <em>(Rating: 2)</em></li>
                <li>Curiosity-driven exploration by self-supervised prediction <em>(Rating: 2)</em></li>
                <li>Keep CALM and explore: Language models for action generation in text-based games <em>(Rating: 2)</em></li>
                <li>Learning dynamic knowledge graphs to generalize on text-based games <em>(Rating: 2)</em></li>
                <li>Nail: A general interactive fiction agent <em>(Rating: 1)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3249",
    "paper_id": "paper-232352537",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "A Q-learning style RL agent for text games that encodes observations and candidate actions with separate GRU encoders and scores action-observation pairs with an MLP; trained using temporal-difference loss and a replay buffer.",
            "citation_title": "Deep reinforcement learning with a natural language action space",
            "mention_or_use": "use",
            "agent_name": "DRRN",
            "agent_description": "Q-network Q_phi(o,a) using two GRU encoders f_o (observation) and f_a (action), concatenated and decoded by an MLP g to produce Q-values; trained with TD loss, softmax action selection, prioritized experience replay across 8 parallel environment instances.",
            "game_or_benchmark_name": "Jericho benchmark (multiple interactive fiction games, e.g. ZORK I, ZORK III)",
            "task_description": "Maximize in-game score in interactive fiction text-adventure games via selecting text actions given textual observations (standard RL episodic gameplay).",
            "uses_memory": true,
            "memory_type": "experience replay (replay buffer) and implicit memorization of Q-values via function approximator",
            "memory_implementation_details": "Stores (o,a,r,o') tuples in a prioritized replay buffer used for TD updates; agent behavior and learned Q-values effectively 'memorize' high-value observation-action tuples; uses GRU encoders but no explicit long-term external memory module.",
            "performance_with_memory": "Average normalized final score across games: 0.21 (final) and 0.38 (max observed) as reported in Table 1; per-game final/max scores reported (e.g. ZORK I final 39.4 / max 53).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "Replay buffer and stable representations enable DRRN to learn to reach and repeat high-scoring trajectories in the same environment; allows memorization of high Q-values for observed (o,a) pairs.",
            "memory_limitations_or_failures": "Representations trained only with TD loss can degenerate/overfit to game-specific reward structure (poor transfer), representing unseen states in a different subspace from seen states (limited generalization).",
            "best_practices_or_recommendations": "Use of replay buffer helps memorization but can lead to overfitting; authors suggest semantic regularization (e.g., inverse-dynamics) and evaluation protocols that require generalization across games to avoid reward-driven overfitting.",
            "uuid": "e3249.0"
        },
        {
            "name_short": "HASH",
            "name_full": "Hashing ablation (random fixed encodings)",
            "brief_description": "A semantic-ablation in which observation and action texts are replaced by hash-derived fixed random vectors, breaking semantic continuity but providing stable, distinct identifiers for states and actions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "HASH (DRRN with hashed inputs)",
            "agent_description": "Same DRRN architecture but with GRU encoders replaced/composed with a fixed hashing encoder: text strings hashed to integers then mapped deterministically to random Gaussian vectors (G seeded by hash); these vectors are used as representations (not learned) so the network learns Q-values over fixed random identifiers.",
            "game_or_benchmark_name": "Jericho benchmark (multiple interactive fiction games, e.g. ZORK I, PENTARI)",
            "task_description": "Maximize in-game score with observations/actions replaced by non-semantic hashed identifiers; evaluate whether agents can learn without language semantics.",
            "uses_memory": true,
            "memory_type": "implicit memorization of Q-values (episodic/instance-level memorization) supported by experience replay; fixed vector identifiers act as stable keys enabling memorization",
            "memory_implementation_details": "Text -&gt; Python hash -&gt; seed PyTorch RNG -&gt; generate fixed Gaussian vector; these fixed vectors identify observations/actions so the Q-network can memorize which identifier-action pairs yield high Q-values; training still uses prioritized replay buffer.",
            "performance_with_memory": "Average normalized final score across games: 0.25 (final) and 0.39 (max observed) in Table 1; outperforming DRRN on average final score and achieving notably higher final score on some games (e.g., PENTARI final 51.9 vs DRRN 26.5).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "Stable, fixed identifiers make Q-learning easier and enable strong memorization of high-value observation-action pairs, leading to equal or better final performance than semantics-aware DRRN on many games.",
            "memory_limitations_or_failures": "Breaks semantic continuity and thus generalization and transfer: HASH transfer to a new but similar game is equivalent to training from scratch (representations not learned), and hashed representations are scattered (no clustering by semantic similarity).",
            "best_practices_or_recommendations": "Stable representations can aid Q-learning, but relying on fixed non-semantic identifiers prevents transfer and semantic generalization; evaluate agents on transfer or generative-action settings to require semantic understanding.",
            "uuid": "e3249.1"
        },
        {
            "name_short": "MIN-OB",
            "name_full": "Minimized Observation (location-only) ablation",
            "brief_description": "An ablation that reduces each textual observation to only a short location phrase (loc(o)), removing descriptive semantic detail to isolate action semantics and test importance of observation detail.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "MIN-OB (DRRN with location-only observations)",
            "agent_description": "DRRN where observations are replaced with only the location phrase loc(o) (a single token/short phrase) while action candidates remain full text; GRU encoders still used for actions/locations; trained with TD loss and replay buffer.",
            "game_or_benchmark_name": "Jericho benchmark (multiple interactive fiction games, e.g. ZORK I, DEEPHOME, DRAGON)",
            "task_description": "Maximize game score when observation information is minimized to location names, isolating the role of action semantics and testing the need for observation semantic detail.",
            "uses_memory": true,
            "memory_type": "implicit memorization of Q-values via replay buffer; reduced observation space constrains what can be memorized",
            "memory_implementation_details": "Same prioritized replay buffer and training regime; observation input truncated to location phrase which reduces observation discriminability and hinders memorization of distinct game contexts.",
            "performance_with_memory": "Average normalized final score across games: 0.12 (final) and 0.35 (max observed) in Table 1; on most games explored similar maximum scores but had lower final episodic scores (failed to 'memorize' good experience).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "Not applicable as MIN-OB reduces observational detail; authors interpret that distinguishing observations with more detailed text aids memorization of high-value experiences.",
            "memory_limitations_or_failures": "MIN-OB often fails to memorize and reach high episodic scores despite exploring similar maximums — reducing observations to locations removes identifying information that helps an agent recall previously good trajectories.",
            "best_practices_or_recommendations": "Do not overly minimize observation inputs if the goal is stable episodic performance; observation detail can be important for enabling memorization of distinct states and rewarding experiences.",
            "uuid": "e3249.2"
        },
        {
            "name_short": "INV-DY",
            "name_full": "Inverse Dynamics Regularization (INV-DY)",
            "brief_description": "An auxiliary inverse-dynamics decoder and reconstruction loss added to DRRN to regularize text representations, encourage encoding of action-relevant observation semantics, and provide intrinsic curiosity-like reward for exploration.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "INV-DY (DRRN + inverse dynamics auxiliary task)",
            "agent_description": "DRRN augmented with: (1) an inverse-dynamics MLP g_inv that predicts the action representation from concatenated f_o(o) and f_o(o'), (2) a GRU decoder d to reconstruct action text from predicted representation (L_inv), (3) action reconstruction loss L_dec from f_a(a), and (4) intrinsic reward r_plus = L_inv to encourage exploration; total loss = L_TD + lambda1 L_inv + lambda2 L_dec.",
            "game_or_benchmark_name": "Jericho benchmark (multiple interactive fiction games, e.g. ZORK I, DRAGON, OMNIQUEST)",
            "task_description": "Maximize game score while learning semantically regularized representations that (a) decode back to action text and (b) provide intrinsic rewards to guide exploration.",
            "uses_memory": true,
            "memory_type": "experience replay (replay buffer) plus improved generalization of learned representations (semantic memory in encoder space); implicit memory via Q-values and replay is retained",
            "memory_implementation_details": "Same prioritized replay buffer used for TD training; auxiliary inverse-dynamics loss regularizes encoder representations so that f_o encodes action-relevant semantics and f_a can be reconstructed; intrinsic reward based on inverse-dynamics loss drives exploration to states where inverse dynamics is not yet learned.",
            "performance_with_memory": "Average normalized final score across games: 0.23 (final) and 0.40 (max observed) in Table 1; showed clear exploration gains on some games (e.g., ZORK I maximum score seen = 87 for INV-DY compared to &lt;=55 for other models).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "INV-DY improved exploration (found higher maximum scores on several games), produced representations that generalize to unseen states by semantic similarity (t-SNE), and produced better transfer to ZORK III when encoders were fixed (score ~1 vs ~0.4 baseline).",
            "memory_limitations_or_failures": "While INV-DY improves exploration and representation semantics on some games, its final average performance is similar to HASH and DRRN overall; benefits are game-dependent. Auxiliary reconstruction losses and intrinsic rewards add complexity and may not uniformly improve final episodic scores.",
            "best_practices_or_recommendations": "Use auxiliary inverse-dynamics decoding to regularize representations and provide intrinsic curiosity-like rewards to encourage exploration and semantic encoding; combine with evaluation protocols that test transfer/generalization rather than within-game memorization.",
            "uuid": "e3249.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep reinforcement learning with a natural language action space",
            "rating": 2
        },
        {
            "paper_title": "Curiosity-driven exploration by self-supervised prediction",
            "rating": 2
        },
        {
            "paper_title": "Keep CALM and explore: Language models for action generation in text-based games",
            "rating": 2
        },
        {
            "paper_title": "Learning dynamic knowledge graphs to generalize on text-based games",
            "rating": 2
        },
        {
            "paper_title": "Nail: A general interactive fiction agent",
            "rating": 1
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01056775,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents
June 6-11, 2021</p>
<p>Shunyu Yao shunyuy@princeton.edu 
Princeton University ‡ Microsoft Research</p>
<p>Karthik Narasimhan karthikn@princeton.edu 
Princeton University ‡ Microsoft Research</p>
<p>Matthew Hausknecht matthew.hausknecht@microsoft.com 
Princeton University ‡ Microsoft Research</p>
<p>Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents</p>
<p>Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies
the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJune 6-11, 20213097
Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including ZORK I. We discuss the implications of our findings for designing future agents with stronger semantic understanding.</p>
<p>Introduction</p>
<p>Text adventure games such as ZORK I (Figure 1 (a)) have been a testbed for developing autonomous agents that operate using natural language. Since interactions in these games (input observations, action commands) are through text, the ability to understand and use language is deemed necessary and critical to progress through such games. Previous work has deployed a spectrum of methods for language processing in this domain, including word vectors (Fulda et al., 2017), recurrent neural networks (Narasimhan et al., 2015;, pre-trained language models (Yao * Work partly done during internship at Microsoft Research. Project page: https://blindfolded.cs. princeton.edu. et al., 2020), open-domain question answering systems , knowledge graphs Adhikari et al., 2020), and reading comprehension systems .</p>
<p>Meanwhile, most of these models operate under the reinforcement learning (RL) framework, where the agent explores the same environment in repeated episodes, learning a value function or policy to maximize game score. From this perspective, text games are just special instances of a partially observable Markov decision process (POMDP) (S, T, A, O, R, γ), where players issue text actions a ∈ A, receive text observations o ∈ O and scalar rewards r = R(s, a), and the underlying game state s ∈ S is updated by transition s = T (s, a).</p>
<p>However, what distinguishes these games from other POMDPs is the fact that the actions and observations are in language space L. Therefore, a certain level of decipherable semantics is attached to text observations o ∈ O ⊂ L and actions a ∈ A ⊂ L. Ideally, these texts not only serve as observation or action identifiers, but also provide clues about the latent transition function T and reward function R. For example, issuing an action "jump" based on an observation "on the cliff" would likely yield a subsequent observation such as "you are killed" along with a negative reward. Human players often rely on their understanding of language semantics to inform their choices, even on games they have never played before, while replacing texts with non-semantic identifiers such as their corresponding hashes (Figure 1 (c)) would likely render games unplayable for people. However, would this type of transformation affect current RL agents for such games? In this paper, we ask the following question: To what extent do current reinforcement learning agents leverage semantics in text-based games?</p>
<p>To shed light on this question, we investi-(a) ZORK I Observation 21: You are in the living room. There is a doorway to the east, a wooden door with strange gothic lettering to the west, which appears to be nailed shut, a trophy case, and a large oriental rug in the center of the room. You are carrying: A brass lantern . . .</p>
<p>Action 21: move rug</p>
<p>Observation 22: With a great effort, the rug is moved to one side of the room, revealing the dusty cover of a closed trap door... Living room... You are carrying: ... gate the Deep Reinforcement Relevance Network (DRRN) (He et al., 2016), a top-performing RL model that uses gated recurrent units (GRU) (Cho et al., 2014) to encode texts. We conduct three experiments on a set of interactive fiction games from the Jericho benchmark  to probe the effect of different semantic representations on the functioning of DRRN. These include (1) using just a location phrase as the input observation (Figure 1 (b)), (2) hashing text observations and actions (Figure 1 (c)), and (3) regularizing vector representations using an auxiliary inverse dynamics loss. While reducing observations to location phrases leads to decreased scores and enforcing inverse dynamics decoding leads to increased scores on some games, hashing texts to break semantics surprisingly matches or even outperforms the baseline DRRN on almost all games considered. This implies current RL agents for textbased games might not be sufficiently leveraging the semantic structure of game texts to learn good policies, and points to the need for developing better experiment setups and agents that have a finer grasp of natural language.</p>
<p>2 Models DRRN Baseline Our baseline RL agent DRRN (He et al., 2016) learns a Q-network Q φ (o, a) parametrized by φ. The model encodes the observation o and each action candidate a using two separate GRU encoders f o and f a , and then aggregates the representations to derive the Q-value through a MLP decoder g:
Q φ (o, a) = g(concat(f o (o), f a (a)))(1)
For learning φ, tuples (o, a, r, o ) of observation, action, reward and the next observation are sampled from an experience replay buffer and the following temporal difference (TD) loss is minimized:
L TD (φ) = (r + γ max a ∈A Q φ (o , a ) − Q φ (o, a)) 2
(2) During gameplay, a softmax exploration policy is used to sample an action:
π φ (a|o) = exp(Q φ (o, a)) a ∈A exp(Q φ (o, a ))(3)
Note that when the action space A is large, (2) and (3) become intractable. A valid action handicap  or a language model (Yao et al., 2020) can be used to generate a reduced action space for efficient exploration. For all the modifications below, we use the DRRN with the valid action handicap as our base model.</p>
<p>Reducing Semantics via Minimizing Observation (MIN-OB)</p>
<p>Unlike other RL domains such as video games or robotics control, at each step of text games the (valid) action space is constantly changing, and it reveals useful information about the current state. For example, knowing "unlock box" is valid leaks the existence of a locked box. Also, sometimes action semantics indicate its value even unconditional on the state, e.g. "pick gold" usually seems good. Given these, we minimize the observation to only a location phrase o → loc(o) (Figure 1 (b)) to isolate the action semantics: given a hash function from strings to integers h : L → Z, and a pseudo-random generator G : Z → R d that turns an integer seed to a random Gaussian vector, a hashing encoder (1) are trainable,f is fixed throughout RL, and ensures two texts that only differ by a word would have completely different representations. In this sense, hashing breaks semantics and only serves to identify different observations and actions in an abstract MDP problem (Figure 1 (c)):
Q loc φ (o, a) = g(f o (loc(o)), f a (a))f = G • h : L → R d can be composed. While f o and f a inQ hash φ (o, a) = g(f (o),f (a))
.</p>
<p>Regularizing Semantics via Inverse Dynamics</p>
<p>Decoding (INV-DY) The GRU representations in DRRN f o (o), f a (a) are only optimized for the TD loss (2). As a result, text semantics can degenerate during encoding, and the text representation might arbitrarily overfit to the Q-values. To regularize and encourage more game-related semantics to be encoded, we take inspiration from Pathak et al. (2017) and propose an inverse dynamics auxiliary task during RL. Given representations of current and next observations f o (o), f o (o ), we use a MLP g inv to predict the action representation, and a GRU decoder d to decode the action back to text * . The inverse dynamics loss is defined as
L inv (φ, θ) = − log p d (a|g inv (concat(f o (o), f o (o )))
where θ denote weights of g inv and d, and p d (a|x) is the probability of decoding token sequence a using GRU decoder d with initial hidden state as x. To also regularize the action encoding, action reconstruction from f a is also used as a loss term:</p>
<p>L dec (φ, θ) = − log p d (a|f a (a)) * Directly defining an L1/L2 loss between fa(a) and ginv(concat(fo(o), fo(o ))) in the representation space will collapse text representations together. And during experience replay, these two losses are optimized along with the TD loss:
L(φ, θ) = L TD (φ) + λ 1 L inv (φ, θ) + λ 2 L dec (φ, θ)
An intrinsic reward r + = L inv (φ, θ) is also used to explore toward where the inverse dynamics is not learned well yet. All in all, the aim of INV-DY is threefold: (1) regularize both action and observation representations to avoid degeneration by decoding back to the textual domain, (2) encourage f o to encode action-relevant parts of observations, and (3) provide intrinsic motivation for exploration.</p>
<p>Results</p>
<p>Setup We train on 12 games † from the Jericho benchmark . These human-written interactive fictions are rich, complex, and diverse in semantics. ‡ For each game, we train DRRN asynchronously on 8 parallel instances of the game environment for 10 5 steps, using a prioritized replay buffer. Following prior practice , we augment observations with location and inventory descriptions by issuing the 'look' and 'inventory' commands. We train three independent runs for each game and report their average score. For HASH, we use the Python built-in hash function to process text as a tuple of token IDs, and implement the random vector generator G by seeding PyTorch with the hash value. For INV-DY, we use λ 1 = λ 2 = 1.</p>
<p>Scores Table 1 reports the final score (the average score of the final 100 episodes during training), and the maximum score seen in each game for different models. Average normalized score (raw score divided by game total score) over all games is also reported. Compared to the base DRRN, MIN-OB turns out to explore similar maximum scores on † We omit games where DRRN cannot score. ‡ Please refer to  for more details about these games. most games (except DEEPHOME and DRAGON), but fails to memorize the good experience and reach high episodic scores, which suggests the importance of identifying different observations using language details. Most surprisingly, HASH has a lower final score than DRRN on only one game (ZORK I), while on PENTARI it almost doubles the DRRN final score. It is also the model with the best average normalized final score across games, which indicates that the DRRN model can perform as well without leveraging any language semantics, but instead simply by identifying different observations and actions with random vectors and memorizing the Q-values. Lastly, we observe on some games (DRAGON, OMNIQUEST, ZORK I) INV-DY can explore high scores that other models cannot. Most notably, on ZORK I the maximum score seen is 87 (average of 54, 94, 113), while any run of other models does not explore a score more than 55. This might indicate potential benefit of developing RL agents with more semantic representations.</p>
<p>Transfer We also investigate if representations of different models can transfer to a new language environment, which is a potential benefit of learning natural language semantics. So we consider the two most similar games in Jericho, ZORK I and ZORK III, fix the language encoders of different ZORK I models, and re-train the Q-network on ZORK III for 10,000 steps. As shown in Figure 2, INV-DY representations can achieve a score around 1, which surpasses the best result of models trained from scratch on ZORK III for 100,000 steps (around 0.4), showing great promise in better gameplay by leveraging language understanding from other games. HASH transfer is equivalent to training from scratch as the representations are not learnt, and a score around 0.3 is achieved. Finally, DRRN representations transfer worse than HASH, possibly due to overfitting to the TD loss (2).</p>
<p>Visualizations Finally, we use t-SNE (Maaten and Hinton, 2008) to visualize representations of some ZORK I walkthrough states in Figure 3. The first 30 walkthrough states (red, score 0-45) are well experienced by the models during exploration, whereas the last 170 states (blue, score 157-350) are unseen § . We also encircle the subset of states at location 'living room' for their shared semantics.</p>
<p>First, we note that the HASH representations for living room states are scattered randomly, unlike the other two models with GRU language encoders. Further, the base DRRN overfits to the TD loss (2), representing unseen states (blue) in a different subspace to seen states (red) without regarding their semantic similarity. IND-DY is able to extrapolate to unseen states and represent them similarly to seen states for their shared semantics, which may explain its better performance on this game.</p>
<p>Game stochasticity All the above experiments were performed using a fixed game random seed for each game, following prior work . To investigate if randomness in games affects our conclusions, we run one trial of each game with episode-varying random seeds ¶ . We find the average normalized score for the base DRRN, HASH, INV-DY to be all around 17%, with performance drop mainly on three stochastic games (DRAGON, ZORK I, ZORK III). Notably, the core finding that the base DRRN and HASH perform similarly still holds. Intuitively, even though the Q-values would be lower overall with unexpected transitions, RL would still memorize observations and actions that lead to high Q-values.</p>
<p>Discussion</p>
<p>At a high level, RL agents for text-based games succeed by (1) exploring trajectories that lead to high scores, and (2) learning representations to stably reach high scores. Our experiments show that a semantics-regularized INV-DY model manages to explore higher scores on some games (DRAGON, OMNIQUEST, ZORK I), while the HASH model manages to memorize scores better on other games (LIBRARY, LUDICORP, PENTARI) using just a fixed, random, non-semantic representation. This leads us to hypothesize two things. First, fixed, stable representations might make Q-learning easier. Second, it might be desirable to represent similar texts very differently for better gameplay, e.g. the Q-value can be much higher when a key object is mentioned, even if it only adds a few words to a long observation text. This motivates future thought into the structural vs. functional use of language semantics in these games.</p>
<p>Our findings also urge a re-thinking of the popular 'RL + valid action handicap' setup for these games. On one hand, RL sets training and evaluation in the same environment, with limited text corpora, and sparse, mostly deterministic rewards as the only optimization objective. Such a combination easily results in overfitting to the reward system of a specific game (Figure 2), or even just a specific stage of the game (Figure 3). On the other hand, the valid action handicap reduces the action set to a small size tractable for memorization, and reduces the language understanding challenge for the RL agent. Thus for future research on text-based games, we advocate for more attention towards alternative setups without RL or handicaps (Hausknecht et al., 2019;Yao et al., 2020;. Particularly, in a 'RL + no valid action handicap' setting, generating action candidates rather than simply choosing from a set entails more opportunities and challenges with respect to learning grounded language semantics (Yao et al., 2020). Additionally, training agents on a distribution of games and evaluating them on a separate set of unseen games would require more general semantic understanding. Semantic evaluation of these proposed paradigms is outside the scope of this paper, but we hope it will spark a productive discussion on the next steps toward building agents with stronger semantic understanding.</p>
<p>Ethical Considerations</p>
<p>Autonomous decision-making agents are potentially impactful in our society, and it is of great ethical consideration to make sure their understanding of the world and their objectives align with humans. Humans use natural language to convey and understand concepts as well as inform decisions, and in this work we investigate whether autonomous agents leverage language semantics similarly to humans in the environment of text-based games. Our findings suggest that the current generation of agents optimized for reinforcement learning objectives might not exhibit human-like language understanding, a phenomenon we should pay attention to and further study.</p>
<p>Figure 1 :
1(a): Sample original gameplay from ZORK I. (b) (c): Our proposed semantic ablations. (b) MIN-OB reduces observations to only the current location name, and (c) HASH replaces observation and action texts by their string hash values.</p>
<p>Figure 2 :
2Transfer results from ZORK I.</p>
<p>Figure 3
3: t-SNE visualization of seen and unseen state observations of ZORK I. DRRN (base) represents unseen states separated from seen states while INV-DY mixes them by semantic similarity.</p>
<p>). Breaking Semantics via Hashing (HASH) GRU encoders f o and f a in the Q-network (1) generally ensure that similar texts (e.g. a single word change) are given similar representations, and therefore similar values. To study whether such a semantics continuity is useful, we break it by hashing observation and action texts. Specifically,Table 1: Final/maximum score of different models.Game DRRN </p>
<p>MIN-OB 
HASH 
INV-DY </p>
<p>Max </p>
<p>balances 10 / 10 
10 / 10 
10 / 10 
10 / 10 
51 
deephome 57 / 66 
8.5 / 27 
58 / 67 
57.6 / 67 300 
detective 290 / 337 86.3 / 350 290 / 317 290 / 323 360 
dragon -5.0 / 6 
-5.4 / 3 
-5.0 / 7 
-2.7 / 8 
25 
enchanter 20 / 20 
20 / 40 
20 / 30 
20 / 30 
400 
inhumane 21.1 / 45 12.4 / 40 
21.9 / 45 19.6 / 45 90 
library 15.7 / 21 12.8 / 21 
17 / 21 
16.2 / 21 30 
ludicorp 12.7 / 23 11.6 / 21 
14.8 / 23 13.5 / 23 150 
omniquest 4.9 / 5 
4.9 / 5 
4.9 / 5 
5.3 / 10 
50 
pentari 26.5 / 45 21.7 / 45 
51.9 / 60 37.2 / 50 70 
zork1 39.4 / 53 29 / 46 
35.5 / 50 43.1 / 87 350 
zork3 0.4 / 4.5 
0.0 / 4 
0.4 / 4 
0.4 / 4 
7 </p>
<p>Avg. Norm .21 / .38 
.12 / .35 
.25 / .39 
.23 / .40 </p>
<p>§ The rest 150 states in the middle (score 45-157) are omitted as they might be seen by some model but not others. ¶ Randomness includes transition uncertainty (e.g. thief showing up randomly in ZORK I) and occasional paraphrasing of text observations.
AcknowledgementsWe appreciate helpful suggestions from anonymous reviewers as well as members of the Princeton NLP Group and MSR RL Group.
Learning dynamic knowledge graphs to generalize on text-based games. Ashutosh Adhikari, ( Xingdi, ) Eric, Marc-Alexandre Yuan, Mikulas Côté, Marc-Antoine Zelinka, Romain Rondeau, Pascal Laroche, Jian Poupart, Adam Tang, William L Trischler, Hamilton, NeurIPS. Ashutosh Adhikari, Xingdi (Eric) Yuan, Marc- Alexandre Côté, Mikulas Zelinka, Marc-Antoine Rondeau, Romain Laroche, Pascal Poupart, Jian Tang, Adam Trischler, and William L. Hamilton . 2020. Learning dynamic knowledge graphs to gen- eralize on text-based games. In NeurIPS 2020.</p>
<p>Graph constrained reinforcement learning for natural language action spaces. Prithviraj Ammanabrolu, Matthew J Hausknecht, 8th International Conference on Learning Representations. Addis Ababa, Ethiopia2020OpenReview.netPrithviraj Ammanabrolu and Matthew J. Hausknecht. 2020. Graph constrained reinforcement learning for natural language action spaces. In 8th Inter- national Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.</p>
<p>How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. Prithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, Mark O Riedl, arXiv:2002.08795arXiv preprintPrithviraj Ammanabrolu, Ethan Tien, Zhaochen Luo, and Mark O Riedl. 2020. How to avoid being eaten by a grue: Exploration strategies for text-adventure agents. arXiv preprint arXiv:2002.08795.</p>
<p>On the properties of neural machine translation: Encoder-decoder approaches. Kyunghyun Cho, Dzmitry Bart Van Merriënboer, Yoshua Bahdanau, Bengio, 10.3115/v1/W14-4012Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation. SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical TranslationDoha, QatarAssociation for Computational LinguisticsKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah- danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap- proaches. In Proceedings of SSST-8, Eighth Work- shop on Syntax, Semantics and Structure in Statisti- cal Translation, pages 103-111, Doha, Qatar. Asso- ciation for Computational Linguistics.</p>
<p>What can you do with a rock? affordance extraction via word embeddings. Nancy Fulda, Daniel Ricks, Ben Murdoch, David Wingate, 10.24963/ijcai.2017/144Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017. the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017Melbourne, Australiaijcai.orgNancy Fulda, Daniel Ricks, Ben Murdoch, and David Wingate. 2017. What can you do with a rock? affor- dance extraction via word embeddings. In Proceed- ings of the Twenty-Sixth International Joint Con- ference on Artificial Intelligence, IJCAI 2017, Mel- bourne, Australia, August 19-25, 2017, pages 1039- 1045. ijcai.org.</p>
<p>Interactive fiction game playing as multi-paragraph reading comprehension with reinforcement learning. Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Murray Campbell, Shiyu Chang, 10.18653/v1/2020.emnlp-main.624Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Xiaoxiao Guo, Mo Yu, Yupeng Gao, Chuang Gan, Mur- ray Campbell, and Shiyu Chang. 2020. Interac- tive fiction game playing as multi-paragraph read- ing comprehension with reinforcement learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7755-7765, Online. Association for Computa- tional Linguistics.</p>
<p>Nail: A general interactive fiction agent. Matthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, Jason D Williams, arXiv:1902.04259arXiv preprintMatthew Hausknecht, Ricky Loynd, Greg Yang, Adith Swaminathan, and Jason D Williams. 2019. Nail: A general interactive fiction agent. arXiv preprint arXiv:1902.04259.</p>
<p>Interactive fiction games: A colossal adventure. Matthew J Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, Xingdi Yuan, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference. New York, NY, USAAAAI Press2020The Tenth AAAI Symposium on Educational Advances in Artificial IntelligenceMatthew J. Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. 2020. In- teractive fiction games: A colossal adventure. In The Thirty-Fourth AAAI Conference on Artificial In- telligence, AAAI 2020, The Thirty-Second Innova- tive Applications of Artificial Intelligence Confer- ence, IAAI 2020, The Tenth AAAI Symposium on Ed- ucational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 7903-7910. AAAI Press.</p>
<p>Deep reinforcement learning with a natural language action space. Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, Mari Ostendorf, 10.18653/v1/P16-1153Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsJi He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Li- hong Li, Li Deng, and Mari Ostendorf. 2016. Deep reinforcement learning with a natural language ac- tion space. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1621-1630, Berlin, Germany. Association for Computational Linguis- tics.</p>
<p>Visualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 9Laurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-sne. Journal of machine learning research, 9(Nov):2579-2605.</p>
<p>Language understanding for text-based games using deep reinforcement learning. Karthik Narasimhan, Tejas Kulkarni, Regina Barzilay, 10.18653/v1/D15-1001Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalAssociation for Computational LinguisticsKarthik Narasimhan, Tejas Kulkarni, and Regina Barzi- lay. 2015. Language understanding for text-based games using deep reinforcement learning. In Pro- ceedings of the 2015 Conference on Empirical Meth- ods in Natural Language Processing, pages 1-11, Lisbon, Portugal. Association for Computational Linguistics.</p>
<p>Curiosity-driven exploration by self-supervised prediction. Deepak Pathak, Pulkit Agrawal, Alexei A Efros, Trevor Darrell, PMLRProceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine LearningSydney, NSW, Australia70Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. 2017. Curiosity-driven exploration by self-supervised prediction. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learn- ing Research, pages 2778-2787. PMLR.</p>
<p>Keep CALM and explore: Language models for action generation in text-based games. Shunyu Yao, Rohan Rao, Matthew Hausknecht, Karthik Narasimhan, 10.18653/v1/2020.emnlp-main.704Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsShunyu Yao, Rohan Rao, Matthew Hausknecht, and Karthik Narasimhan. 2020. Keep CALM and ex- plore: Language models for action generation in text-based games. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 8736-8754, Online. As- sociation for Computational Linguistics.</p>
<p>Deriving commonsense inference tasks from interactive fictions. Mo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan, Murray Campbell, arXiv:2010.09788arXiv preprintMo Yu, Xiaoxiao Guo, Yufei Feng, Xiaodan Zhu, Michael Greenspan, and Murray Campbell. 2020. Deriving commonsense inference tasks from inter- active fictions. arXiv preprint arXiv:2010.09788.</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-429 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-429</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-429</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-17.html">extraction-schema-17</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <p><strong>Paper ID:</strong> paper-246823404</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/2202.05332v1.pdf" target="_blank">An Initial Description of Capabilities and Constraints for a Computational Auditory System (an Artificial Ear) for Cognitive Architectures</a></p>
                <p><strong>Paper Abstract:</strong> We present an initial set of factors, features, and constraints for developing a Computational Auditory System (CAS, aka less formally an artificial ear, AE) for use by cognitive architectures. We start to define a CAS and what tasks it should be able to perform. We then outline the features of a CAS for use by a cognitive architecture and factors that influence its performance. We conclude with an update on what has been created so far and insights on how to create and use a CAS in a cognitive architecture and include a set of functionalities for an artificial ear.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e429.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e429.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CAS+Cog (design)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Computational Auditory System preprocessor integrated with a Cognitive Architecture (design described here)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>This paper's primary hybrid design: a modular CAS (ear + preprocessor performing signal processing, feature extraction, target lists and asynchronous event generation) coupled to a cognitive architecture that provides top-down expectations, memory and action selection; integration is via function calls, sockets or JSON with synchronous/asynchronous communication and shared lists of targets/ignored items.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CAS preprocessor + Cognitive Architecture (modular hybrid)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A modular hybrid system in which an auditory front-end (microphones/ear) and a preprocessor perform imperative signal-processing tasks (cochleagram/spectrogram generation, binaural localization, feature extraction, cluster analysis, maintenance of target/ignore lists, asynchronous event generation) while a separate cognitive architecture provides declarative constructs (expectations, long- and short-term targets, semantic labels, prioritization and action selection). Communication is via language-agnostic interfaces (function calls when colocated, or sockets/Unix pipes/JSON Network Interface when separate), with both synchronous and asynchronous channels so the preprocessor can continuously stream 'heard objects' to cognition and cognition can push listening goals or modify preprocessor state.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Cognitive architecture component (declarative memory, target lists, symbolic representations of sounds and alarms, goal/expectation-driven attention). The paper treats cognition as a conventional symbolic cognitive architecture (e.g., ARCADIA/other architectures with declarative memory and production-like control), representing sounds as symbolic 'heard objects' with IDs, semantic labels and attributes.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>CAS front-end and preprocessor performing procedural signal-processing algorithms: binaural preprocessing (interaural time/intensity differences), cochleagram/spectrogram generation, feature extraction, clustering / auditory scene analysis, target matching and asynchronous event dispatch. Implementation methods discussed include cluster analysis on cochleagrams, conventional DSP/localization algorithms and multi-process concurrent programs.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular, message-passing integration: the preprocessor and cognition are separate modules connected via function calls when co-located or via sockets/Unix pipes/JSON for distributed setups; asynchronous event delivery for 'heard objects'; cognition supplies top-down targets/ignore lists and can command actuator actions (e.g., turn head) back to the CAS; the architecture supports parallel processes in the preprocessor and bidirectional control (cognition influences listening parameters; preprocessor autonomously filters and forwards salient events).</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Emergent capabilities described include active hearing (interaction of attention, head-turning and auditory focus), robust single-stream tracking in noisy environments (cocktail-party-like behavior through preprocessor clustering combined with cognitive target guidance), contextually-prioritized alarm handling (ear+preprocessor provides enriched events to symbolic cognition which can prioritize and act), and asynchronous interrupting behavior where the ear can autonomously notify cognition of exogenous salient sounds while cognition maintains ongoing tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Design and intended tasks: auditory scene analysis, cocktail-party effect (separating and following a target speaker), localization and distance/bearing estimation, name-word spotting, alarm detection and prioritization, active listening (head turning + re-focus).</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not quantitatively characterized in this paper; the authors argue modular hybridization supports context-sensitive recognition (cognition can load context-specific target lists and expectations), which should aid generalization to task contexts, but no empirical OOD or compositional generalization results are reported.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>High-level interpretability is retained because cognition operates on symbolic 'heard objects' (IDs, labels, timestamps, location) which can be inspected and reasoned over; the preprocessor's procedural operations (clusters, DSP features) are algorithmic and thus auditable, but do not provide human-readable causal explanations beyond feature traces unless instrumentation is added.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No numeric evaluation provided; current implementation incomplete for many listed functions (habituation, phonological loop, audio priming absent); complexity introduced by asynchronous communication and parallel preprocessor processes; potential mismatch between representations used by preprocessor and cognitive architecture (necessitating conversion); lack of end-to-end learning makes adapting to new sound classes rely on explicit addition of sounds or separate learning modules.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Principle of division-of-labor / modular complementarity: imperative modules handle continuous, high-bandwidth signal processing and low-level scene analysis, while declarative cognition handles symbolic labeling, expectations, prioritization and decision-making; integration via bidirectional interfaces enables active perception (top-down attention) and embodied actions (e.g., head turning) to close the perception-action loop.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e429.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e429.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARCADIA+CAS (integration)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Integration of a Computational Auditory Scene Analysis Software Architecture with the ARCADIA cognitive model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A software-level integration (reported in cited technical reports) that couples a computational auditory scene analysis module to the ARCADIA cognitive architecture so that auditory events (from imperative CAS methods) can drive cognitive actions and cognition can issue auditory goals/commands.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Integration of a Computational Auditory Scene Analysis Software Architecture with the ARCADIA cognitive model: Software design, usage, implementation, demonstration.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CAS (computational auditory scene analysis) integrated with ARCADIA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A hybrid system implemented by integrating a CAS (auditory scene analysis software architecture that performs cochleagram-based processing, clustering and source segregation) with the ARCADIA cognitive architecture; integration allows the CAS to generate events that ARCADIA consumes (e.g., 'heard object' with ID/location/words) and allows ARCADIA to request listening targets and actuate behavior (like turning a head) to improve perception.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>ARCADIA cognitive architecture (symbolic cognitive architecture providing task control, memory for targets/ignored items, expectations and decision-making).</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Computational Auditory Scene Analysis software: cochleagram generation, cluster analysis for source separation, feature extraction, binaural localization algorithms; implemented as a separate software module.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular software integration (tech report describes software design and usage): messaging/API-based connection between CAS and ARCADIA with structured 'heard object' messages; suggested connectors include JSON-based network interface or other IPC mechanisms, with asynchronous event delivery and commands passed from cognition to CAS.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Reported emergent behavior includes the capacity for perception-action coupling (e.g., ARCADIA turning a head toward a localized sound) and enabling cognitive-level tasks dependent on hearing (vigilance, alarm handling, name spotting) that neither component alone accomplishes in an integrated manner.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Demonstration tasks: auditory localization and orienting (turning head toward sound), auditory scene analysis including separating concurrent sources (cocktail-party-style demonstrations), and integration demonstrations rather than standardized benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not quantified; the integration is presented as an engineering demonstration to enable cognitive tasks that require auditory grounding rather than evaluated for OOD generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Because ARCADIA is symbolic, decisions about alarms, prioritization and actions are interpretable at the cognitive level; the CAS provides structured event data but internal cluster decisions require inspection of DSP/cluster outputs for low-level interpretability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No standardized evaluations reported; integration complexity and potential representational mismatches noted; details and quantitative assessments are in the cited technical report rather than this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Engineering/modular integration approach: use specialized imperative auditory processing modules coupled with a symbolic cognitive architecture to leverage complementary strengths (signal processing for low-level segregation, symbolic reasoning for task-level decisions).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e429.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e429.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cluster CAS (Daley2021)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cluster analysis for the separation of auditory scenes (cochleagram clustering)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An imperative algorithmic approach using cluster analysis on cochleagrams (audio spectrogram-like representations) to perform source separation and reproduce cocktail-party effects; cited as the technical implementation used by the authors to produce auditory scene separation capability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Cluster analysis for the separation of auditory scenes.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cochleagram-clustering CAS</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A purely algorithmic/imperative CAS implementation that performs cochleagram (auditory spectrogram) calculation followed by cluster analysis to segregate simultaneous sound sources; can be integrated with a cognitive architecture to allow symbolic-level responses to segregated sources (e.g., name spotting and attention switching).</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>When integrated with a cognitive architecture (as in the authors' work), the declarative component is the architecture that receives clustered sources as symbolic 'heard objects' and applies target lists/semantic labels; the clustering system itself is not declarative.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Cochleagram generation (auditory spectrogram) + unsupervised cluster analysis (algorithmic clustering methods) for source separation; procedural DSP/localization routines to extract features used by clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>Modular integration: clustered source outputs are sent to cognition as structured events; the paper and cited report describe integration with ARCADIA via the software architecture (API/JSON interfaces) so cognition can act on separated streams.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Enables cocktail-party-like segregation such that cognitive modules can attend to and follow a single separated stream, support name spotting in multi-stream environments, and permit action (e.g., head turning) to improve subsequent segregation when closed-loop.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td>Auditory scene separation / cocktail party effect reproduction / source segregation demonstrations.</td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Not reported quantitatively here; unsupervised clustering generalization depends on feature choice and clustering algorithm, and performance on diverse environmental conditions is not provided in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Cluster outputs and cochleagram visualizations are inspectable; the mapping from clusters to semantic labels requires symbolic cognition, which provides interpretable labels and decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>No numeric benchmarks provided; robustness to varied noise conditions, speaker variability, reverberation and real-world acoustic complexity not quantified in this paper; relies on post-hoc symbolic labeling for meaning.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Practical signal-processing + symbolic labeling: use unsupervised signal-segregation to produce candidate auditory objects which symbolic cognition can interpret, prioritize and act upon (complementary strengths approach).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e429.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e429.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of hybrid reasoning systems that combine declarative (symbolic, logic-based, rule-based) and imperative (procedural, neural, step-by-step) approaches, including their architectures, integration methods, emergent properties, and performance characteristics.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Hybrid Connectionist-Symbolic (Sun1996)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hybrid connectionist-symbolic modules (concept from IJCAI-95 workshop, Sun 1996)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced conceptual framework advocating modular hybrids that combine connectionist (neural) submodules with symbolic/declarative modules to capture complementary strengths of both paradigms; cited in the paper's discussion of hybrid approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Hybrid connectionist-symbolic modules: A report from the IJCAI-95 Workshop on Connectionist-Symbolic Integration.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hybrid connectionist-symbolic modules (concept)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A conceptual class of hybrid systems where connectionist/neural submodules are combined with symbolic/declarative modules; the workshop report summarizes architectures and proposals for integrating learned distributed representations and procedural neural computations with symbolic knowledge and rule-based processing.</td>
                        </tr>
                        <tr>
                            <td><strong>declarative_component</strong></td>
                            <td>Symbolic modules (rule systems, production systems, explicit symbolic knowledge representations) as described in the workshop literature.</td>
                        </tr>
                        <tr>
                            <td><strong>imperative_component</strong></td>
                            <td>Connectionist/neural network modules (distributed representations, learning via gradient descent, pattern recognition networks) as described in the workshop literature.</td>
                        </tr>
                        <tr>
                            <td><strong>integration_method</strong></td>
                            <td>General hybrid schemes discussed historically include modular composition, parallel/serial pipelines, embedding symbolic constraints into networks, and interface layers translating between distributed activations and discrete symbols; the cited report is conceptual and surveys many proposed integration methods rather than prescribing a single method.</td>
                        </tr>
                        <tr>
                            <td><strong>emergent_properties</strong></td>
                            <td>Argument (in the literature) is that hybrids can combine perceptual generalization and learning from neural networks with the interpretability, compositional reasoning and explicit manipulation capabilities of symbolic systems, enabling behaviors neither alone achieves robustly (e.g., grounded reasoning over percepts).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_benchmark</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hybrid_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>declarative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>imperative_only_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_comparative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>generalization_properties</strong></td>
                            <td>Conceptually claimed to improve generalization by combining pattern-generalization of neural modules with compositional generalization of symbolic modules, but no quantitative results provided in this citation within the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>interpretability_properties</strong></td>
                            <td>Hybrid approach is motivated in part to retain interpretability through symbolic modules while gaining perceptual capability via connectionist modules; specifics depend on architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>The report notes integration challenges (representation mismatch, training/learning across boundaries, how to pass structured information between modules) and that many proposals remained conceptual with few standardized evaluations at the time.</td>
                        </tr>
                        <tr>
                            <td><strong>theoretical_framework</strong></td>
                            <td>Survey/theoretical framing of connectionist-symbolic integration: complementary strengths, modular interface design, and attempts to overcome symbol grounding and representation mismatch problems.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Integration of a Computational Auditory Scene Analysis Software Architecture with the ARCADIA cognitive model: Software design, usage, implementation, demonstration. <em>(Rating: 2)</em></li>
                <li>Cluster analysis for the separation of auditory scenes. <em>(Rating: 2)</em></li>
                <li>Hybrid connectionist-symbolic modules: A report from the IJCAI-95 Workshop on Connectionist-Symbolic Integration. <em>(Rating: 2)</em></li>
                <li>ARTSTREAM: a neural network model of auditory scene analysis and source segregation. <em>(Rating: 1)</em></li>
                <li>Simplifying the interaction between cognitive models and task environments with the JSON Network Interface <em>(Rating: 1)</em></li>
                <li>A review of 40 years of cognitive architecture research: Focus on perception, attention, learning and applications. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-429",
    "paper_id": "paper-246823404",
    "extraction_schema_id": "extraction-schema-17",
    "extracted_data": [
        {
            "name_short": "CAS+Cog (design)",
            "name_full": "Computational Auditory System preprocessor integrated with a Cognitive Architecture (design described here)",
            "brief_description": "This paper's primary hybrid design: a modular CAS (ear + preprocessor performing signal processing, feature extraction, target lists and asynchronous event generation) coupled to a cognitive architecture that provides top-down expectations, memory and action selection; integration is via function calls, sockets or JSON with synchronous/asynchronous communication and shared lists of targets/ignored items.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CAS preprocessor + Cognitive Architecture (modular hybrid)",
            "system_description": "A modular hybrid system in which an auditory front-end (microphones/ear) and a preprocessor perform imperative signal-processing tasks (cochleagram/spectrogram generation, binaural localization, feature extraction, cluster analysis, maintenance of target/ignore lists, asynchronous event generation) while a separate cognitive architecture provides declarative constructs (expectations, long- and short-term targets, semantic labels, prioritization and action selection). Communication is via language-agnostic interfaces (function calls when colocated, or sockets/Unix pipes/JSON Network Interface when separate), with both synchronous and asynchronous channels so the preprocessor can continuously stream 'heard objects' to cognition and cognition can push listening goals or modify preprocessor state.",
            "declarative_component": "Cognitive architecture component (declarative memory, target lists, symbolic representations of sounds and alarms, goal/expectation-driven attention). The paper treats cognition as a conventional symbolic cognitive architecture (e.g., ARCADIA/other architectures with declarative memory and production-like control), representing sounds as symbolic 'heard objects' with IDs, semantic labels and attributes.",
            "imperative_component": "CAS front-end and preprocessor performing procedural signal-processing algorithms: binaural preprocessing (interaural time/intensity differences), cochleagram/spectrogram generation, feature extraction, clustering / auditory scene analysis, target matching and asynchronous event dispatch. Implementation methods discussed include cluster analysis on cochleagrams, conventional DSP/localization algorithms and multi-process concurrent programs.",
            "integration_method": "Modular, message-passing integration: the preprocessor and cognition are separate modules connected via function calls when co-located or via sockets/Unix pipes/JSON for distributed setups; asynchronous event delivery for 'heard objects'; cognition supplies top-down targets/ignore lists and can command actuator actions (e.g., turn head) back to the CAS; the architecture supports parallel processes in the preprocessor and bidirectional control (cognition influences listening parameters; preprocessor autonomously filters and forwards salient events).",
            "emergent_properties": "Emergent capabilities described include active hearing (interaction of attention, head-turning and auditory focus), robust single-stream tracking in noisy environments (cocktail-party-like behavior through preprocessor clustering combined with cognitive target guidance), contextually-prioritized alarm handling (ear+preprocessor provides enriched events to symbolic cognition which can prioritize and act), and asynchronous interrupting behavior where the ear can autonomously notify cognition of exogenous salient sounds while cognition maintains ongoing tasks.",
            "task_or_benchmark": "Design and intended tasks: auditory scene analysis, cocktail-party effect (separating and following a target speaker), localization and distance/bearing estimation, name-word spotting, alarm detection and prioritization, active listening (head turning + re-focus).",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not quantitatively characterized in this paper; the authors argue modular hybridization supports context-sensitive recognition (cognition can load context-specific target lists and expectations), which should aid generalization to task contexts, but no empirical OOD or compositional generalization results are reported.",
            "interpretability_properties": "High-level interpretability is retained because cognition operates on symbolic 'heard objects' (IDs, labels, timestamps, location) which can be inspected and reasoned over; the preprocessor's procedural operations (clusters, DSP features) are algorithmic and thus auditable, but do not provide human-readable causal explanations beyond feature traces unless instrumentation is added.",
            "limitations_or_failures": "No numeric evaluation provided; current implementation incomplete for many listed functions (habituation, phonological loop, audio priming absent); complexity introduced by asynchronous communication and parallel preprocessor processes; potential mismatch between representations used by preprocessor and cognitive architecture (necessitating conversion); lack of end-to-end learning makes adapting to new sound classes rely on explicit addition of sounds or separate learning modules.",
            "theoretical_framework": "Principle of division-of-labor / modular complementarity: imperative modules handle continuous, high-bandwidth signal processing and low-level scene analysis, while declarative cognition handles symbolic labeling, expectations, prioritization and decision-making; integration via bidirectional interfaces enables active perception (top-down attention) and embodied actions (e.g., head turning) to close the perception-action loop.",
            "uuid": "e429.0"
        },
        {
            "name_short": "ARCADIA+CAS (integration)",
            "name_full": "Integration of a Computational Auditory Scene Analysis Software Architecture with the ARCADIA cognitive model",
            "brief_description": "A software-level integration (reported in cited technical reports) that couples a computational auditory scene analysis module to the ARCADIA cognitive architecture so that auditory events (from imperative CAS methods) can drive cognitive actions and cognition can issue auditory goals/commands.",
            "citation_title": "Integration of a Computational Auditory Scene Analysis Software Architecture with the ARCADIA cognitive model: Software design, usage, implementation, demonstration.",
            "mention_or_use": "use",
            "system_name": "CAS (computational auditory scene analysis) integrated with ARCADIA",
            "system_description": "A hybrid system implemented by integrating a CAS (auditory scene analysis software architecture that performs cochleagram-based processing, clustering and source segregation) with the ARCADIA cognitive architecture; integration allows the CAS to generate events that ARCADIA consumes (e.g., 'heard object' with ID/location/words) and allows ARCADIA to request listening targets and actuate behavior (like turning a head) to improve perception.",
            "declarative_component": "ARCADIA cognitive architecture (symbolic cognitive architecture providing task control, memory for targets/ignored items, expectations and decision-making).",
            "imperative_component": "Computational Auditory Scene Analysis software: cochleagram generation, cluster analysis for source separation, feature extraction, binaural localization algorithms; implemented as a separate software module.",
            "integration_method": "Modular software integration (tech report describes software design and usage): messaging/API-based connection between CAS and ARCADIA with structured 'heard object' messages; suggested connectors include JSON-based network interface or other IPC mechanisms, with asynchronous event delivery and commands passed from cognition to CAS.",
            "emergent_properties": "Reported emergent behavior includes the capacity for perception-action coupling (e.g., ARCADIA turning a head toward a localized sound) and enabling cognitive-level tasks dependent on hearing (vigilance, alarm handling, name spotting) that neither component alone accomplishes in an integrated manner.",
            "task_or_benchmark": "Demonstration tasks: auditory localization and orienting (turning head toward sound), auditory scene analysis including separating concurrent sources (cocktail-party-style demonstrations), and integration demonstrations rather than standardized benchmarks.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not quantified; the integration is presented as an engineering demonstration to enable cognitive tasks that require auditory grounding rather than evaluated for OOD generalization.",
            "interpretability_properties": "Because ARCADIA is symbolic, decisions about alarms, prioritization and actions are interpretable at the cognitive level; the CAS provides structured event data but internal cluster decisions require inspection of DSP/cluster outputs for low-level interpretability.",
            "limitations_or_failures": "No standardized evaluations reported; integration complexity and potential representational mismatches noted; details and quantitative assessments are in the cited technical report rather than this paper.",
            "theoretical_framework": "Engineering/modular integration approach: use specialized imperative auditory processing modules coupled with a symbolic cognitive architecture to leverage complementary strengths (signal processing for low-level segregation, symbolic reasoning for task-level decisions).",
            "uuid": "e429.1"
        },
        {
            "name_short": "Cluster CAS (Daley2021)",
            "name_full": "Cluster analysis for the separation of auditory scenes (cochleagram clustering)",
            "brief_description": "An imperative algorithmic approach using cluster analysis on cochleagrams (audio spectrogram-like representations) to perform source separation and reproduce cocktail-party effects; cited as the technical implementation used by the authors to produce auditory scene separation capability.",
            "citation_title": "Cluster analysis for the separation of auditory scenes.",
            "mention_or_use": "use",
            "system_name": "Cochleagram-clustering CAS",
            "system_description": "A purely algorithmic/imperative CAS implementation that performs cochleagram (auditory spectrogram) calculation followed by cluster analysis to segregate simultaneous sound sources; can be integrated with a cognitive architecture to allow symbolic-level responses to segregated sources (e.g., name spotting and attention switching).",
            "declarative_component": "When integrated with a cognitive architecture (as in the authors' work), the declarative component is the architecture that receives clustered sources as symbolic 'heard objects' and applies target lists/semantic labels; the clustering system itself is not declarative.",
            "imperative_component": "Cochleagram generation (auditory spectrogram) + unsupervised cluster analysis (algorithmic clustering methods) for source separation; procedural DSP/localization routines to extract features used by clustering.",
            "integration_method": "Modular integration: clustered source outputs are sent to cognition as structured events; the paper and cited report describe integration with ARCADIA via the software architecture (API/JSON interfaces) so cognition can act on separated streams.",
            "emergent_properties": "Enables cocktail-party-like segregation such that cognitive modules can attend to and follow a single separated stream, support name spotting in multi-stream environments, and permit action (e.g., head turning) to improve subsequent segregation when closed-loop.",
            "task_or_benchmark": "Auditory scene separation / cocktail party effect reproduction / source segregation demonstrations.",
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": false,
            "generalization_properties": "Not reported quantitatively here; unsupervised clustering generalization depends on feature choice and clustering algorithm, and performance on diverse environmental conditions is not provided in this paper.",
            "interpretability_properties": "Cluster outputs and cochleagram visualizations are inspectable; the mapping from clusters to semantic labels requires symbolic cognition, which provides interpretable labels and decisions.",
            "limitations_or_failures": "No numeric benchmarks provided; robustness to varied noise conditions, speaker variability, reverberation and real-world acoustic complexity not quantified in this paper; relies on post-hoc symbolic labeling for meaning.",
            "theoretical_framework": "Practical signal-processing + symbolic labeling: use unsupervised signal-segregation to produce candidate auditory objects which symbolic cognition can interpret, prioritize and act upon (complementary strengths approach).",
            "uuid": "e429.2"
        },
        {
            "name_short": "Hybrid Connectionist-Symbolic (Sun1996)",
            "name_full": "Hybrid connectionist-symbolic modules (concept from IJCAI-95 workshop, Sun 1996)",
            "brief_description": "A referenced conceptual framework advocating modular hybrids that combine connectionist (neural) submodules with symbolic/declarative modules to capture complementary strengths of both paradigms; cited in the paper's discussion of hybrid approaches.",
            "citation_title": "Hybrid connectionist-symbolic modules: A report from the IJCAI-95 Workshop on Connectionist-Symbolic Integration.",
            "mention_or_use": "mention",
            "system_name": "Hybrid connectionist-symbolic modules (concept)",
            "system_description": "A conceptual class of hybrid systems where connectionist/neural submodules are combined with symbolic/declarative modules; the workshop report summarizes architectures and proposals for integrating learned distributed representations and procedural neural computations with symbolic knowledge and rule-based processing.",
            "declarative_component": "Symbolic modules (rule systems, production systems, explicit symbolic knowledge representations) as described in the workshop literature.",
            "imperative_component": "Connectionist/neural network modules (distributed representations, learning via gradient descent, pattern recognition networks) as described in the workshop literature.",
            "integration_method": "General hybrid schemes discussed historically include modular composition, parallel/serial pipelines, embedding symbolic constraints into networks, and interface layers translating between distributed activations and discrete symbols; the cited report is conceptual and surveys many proposed integration methods rather than prescribing a single method.",
            "emergent_properties": "Argument (in the literature) is that hybrids can combine perceptual generalization and learning from neural networks with the interpretability, compositional reasoning and explicit manipulation capabilities of symbolic systems, enabling behaviors neither alone achieves robustly (e.g., grounded reasoning over percepts).",
            "task_or_benchmark": null,
            "hybrid_performance": null,
            "declarative_only_performance": null,
            "imperative_only_performance": null,
            "has_comparative_results": null,
            "generalization_properties": "Conceptually claimed to improve generalization by combining pattern-generalization of neural modules with compositional generalization of symbolic modules, but no quantitative results provided in this citation within the paper.",
            "interpretability_properties": "Hybrid approach is motivated in part to retain interpretability through symbolic modules while gaining perceptual capability via connectionist modules; specifics depend on architecture.",
            "limitations_or_failures": "The report notes integration challenges (representation mismatch, training/learning across boundaries, how to pass structured information between modules) and that many proposals remained conceptual with few standardized evaluations at the time.",
            "theoretical_framework": "Survey/theoretical framing of connectionist-symbolic integration: complementary strengths, modular interface design, and attempts to overcome symbol grounding and representation mismatch problems.",
            "uuid": "e429.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Integration of a Computational Auditory Scene Analysis Software Architecture with the ARCADIA cognitive model: Software design, usage, implementation, demonstration.",
            "rating": 2,
            "sanitized_title": "integration_of_a_computational_auditory_scene_analysis_software_architecture_with_the_arcadia_cognitive_model_software_design_usage_implementation_demonstration"
        },
        {
            "paper_title": "Cluster analysis for the separation of auditory scenes.",
            "rating": 2,
            "sanitized_title": "cluster_analysis_for_the_separation_of_auditory_scenes"
        },
        {
            "paper_title": "Hybrid connectionist-symbolic modules: A report from the IJCAI-95 Workshop on Connectionist-Symbolic Integration.",
            "rating": 2,
            "sanitized_title": "hybrid_connectionistsymbolic_modules_a_report_from_the_ijcai95_workshop_on_connectionistsymbolic_integration"
        },
        {
            "paper_title": "ARTSTREAM: a neural network model of auditory scene analysis and source segregation.",
            "rating": 1,
            "sanitized_title": "artstream_a_neural_network_model_of_auditory_scene_analysis_and_source_segregation"
        },
        {
            "paper_title": "Simplifying the interaction between cognitive models and task environments with the JSON Network Interface",
            "rating": 1,
            "sanitized_title": "simplifying_the_interaction_between_cognitive_models_and_task_environments_with_the_json_network_interface"
        },
        {
            "paper_title": "A review of 40 years of cognitive architecture research: Focus on perception, attention, learning and applications.",
            "rating": 1,
            "sanitized_title": "a_review_of_40_years_of_cognitive_architecture_research_focus_on_perception_attention_learning_and_applications"
        }
    ],
    "cost": 0.0130425,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>An Initial Description of Capabilities and Constraints for a Computational Auditory System (an Artificial Ear) for Cognitive Architectures</p>
<p>Frank E Ritter 
FRANK@FRANKRITTER.COM
MATBRENER@GMAIL.COM Applied Cognitive Science
Naval Submarine Medical Research Labrartory, Groton
LLC, State College
PA, CTUSA, USA</p>
<p>Mathieu Brener 
FRANK@FRANKRITTER.COM
MATBRENER@GMAIL.COM Applied Cognitive Science
Naval Submarine Medical Research Labrartory, Groton
LLC, State College
PA, CTUSA, USA</p>
<p>Jeffrey B Bolkhovsky 
FRANK@FRANKRITTER.COM
MATBRENER@GMAIL.COM Applied Cognitive Science
Naval Submarine Medical Research Labrartory, Groton
LLC, State College
PA, CTUSA, USA</p>
<p>Jeffrey B Bolkhovsky 
FRANK@FRANKRITTER.COM
MATBRENER@GMAIL.COM Applied Cognitive Science
Naval Submarine Medical Research Labrartory, Groton
LLC, State College
PA, CTUSA, USA</p>
<p>Civ@mail 
FRANK@FRANKRITTER.COM
MATBRENER@GMAIL.COM Applied Cognitive Science
Naval Submarine Medical Research Labrartory, Groton
LLC, State College
PA, CTUSA, USA</p>
<p>Mil 
FRANK@FRANKRITTER.COM
MATBRENER@GMAIL.COM Applied Cognitive Science
Naval Submarine Medical Research Labrartory, Groton
LLC, State College
PA, CTUSA, USA</p>
<p>An Initial Description of Capabilities and Constraints for a Computational Auditory System (an Artificial Ear) for Cognitive Architectures
Submitted 9/2021; published 11/2021computational auditory systemartificial earcognitive architecture
We present an initial set of factors, features, and constraints for developing a Computational Auditory System (CAS, aka less formally an artificial ear, AE) for use by cognitive architectures. We start to define a CAS and what tasks it should be able to perform. We then outline the features of a CAS for use by a cognitive architecture and factors that influence its performance. We conclude with an update on what has been created so far and insights on how to create and use a CAS in a cognitive architecture and include a set of functionalities for an artificial ear. use by cognitive architectures.</p>
<p>The Design of a Computational Auditory System</p>
<p>Cognitive architectures and AI agents have had difficulties interacting with the outside world. For example, there is work to provide these agents with artificial eyes and hands. A next step is to look at providing a form of hearing for these agents as well. We provide an initial design for a Computational Auditory System (CAS). An artificial ear would allow models in a cognitive architecture to perform a large new set of tasks that they have not been able to do before. For example, to hear things, to interact with experimental apparatus that puts out a beep for a new trial, to participate in auditory vigilance tasks, to work with auditory alarms (Ritter, Baxter, &amp; Churchill, 2014, Ch. 4.6;Stanton, 1994), to be a more intelligent and grounded (Tehranchi, 2021b) agent in video games, to interact with other agents in the world (Trafton et al., 2006), and to recognize its name when spoken to. An artificial ear would thus help ground models and agents in the world.</p>
<p>We start by quickly describing cognitive architectures because computational auditory systems will provide them with an artificial ear, in the same way that previous work provided them with artificial eyes and hands (e.g., Byrne, 2001;Ritter, Tehranchi, Dancy, &amp; Kase, 2020;Tehranchi, 2021a). We then briefly review previous work on providing auditory input to cognitive architectures. Finally, we describe an initial design for the inputs and outputs for a computational auditory system, and an initial set of general constraints on a computational auditory system for</p>
<p>Prior Published Work on Modeling Ears and Audition</p>
<p>Sound has not often been used in cognitive models or architectures. For example, in Newell (1990) and Anderson (1983), "hear<em>", "aud</em>", and "sound" do not appear in the index. In Anderson (2007), the auditory system is mentioned but not used extensively. There is an auditory system in ACT-R/PM, but it is simply a buffer to put symbols into, not a wire to a microphone. Some reviews do not mention sound or auditory systems (Ritter et al., 2003) and others do (Pew &amp; Mavor, 1998). Grossberg's neural network framework, ART, appears to be an exception, because it can perform some auditory scene analysis (Grossberg, Govindarajan, Wyse, &amp; Cohen, 2004), and has lessons for how to implement an auditory scene analysis system. Kotseruba and Tsotsos (2020) note in their extensive review several architectures that include auditory processing at a simulation level (e.g., ACT-R), several that include simulated physical sensors, and others that include physical sensors. The ACT-R, Soar, and EPIC architectures include only simulated sensors (i.e., a hearing module can place a token into memory), but do not include active listening. The systems that include physical sensors should be examined for guidance, but they are not architectures typically seen at conferences on cognitive modeling and architectures such as the International Conference on Cognitive Modeling (ICCM) or Behavioral Representation in Modeling and Simulation (BRIMS). The first several architectures in their figure of architectures and features that have audition (Glair, DAC, T3, Ymir), at least in some references to these architectures, do not hear from a physical sensor, but these appear to simply receive auditory inputs as symbols (GLAIR, DAC) or only recognize speech (T3, Ymir). Further review could be done here through the 17 noted as having audition.</p>
<p>To find capabilities and factors used by previous models we checked (as much as possible) the CogSci (since 1979), ICCM (since 1998), and Artificial General Intelligence (since 2008) proceedings, and Google Scholar for "cognitive architecture" and "cocktail party effect" or "hearing" or "ear". We were unable to find papers that directly discuss an artificial ear and cognition within the context of a cognitive model. There are papers on natural language understanding as text, though. These lack of publications for hearing can be contrasted with modeling the Stroop effect in vision, where there are thousands of papers on the effect in general and several papers on how it would appear in a cognitive architecture. In addition, the models of natural language understanding is only part of an auditory scene and only captures only one type of stimulus, akin to if you only captured text from a visual scene. A useful start, but not a unified model of hearing.</p>
<p>A functional ear</p>
<p>Figure 1 provides a basic block diagram for a computational audio system. This diagram presents a specification, not yet a created system. It also does not specify how such a system should be implemented (e.g., neural network). We are working on creating such a system.</p>
<p>The ear itself (on the left) interfaces with the world, either through a microphone or microphones, or via a wire to a sound source. The sound source or "ear" may have a pinna (the external ear, what wiggles when you wiggle your ears) to better mimic human hearing. It might be modifiable to model different types of ears. There will be some value to allowing this ear to use different inputs for development, testing, and deployment, such as sets of prerecorded audio files of varying complexity and a live microphone. The ear module will pass information into the preprocessor, which is likely to have extensive memory and processing capabilities. The preprocessor can keep state of what is being listened for (top-down or intrinsic) and do the basic match between sounds and cognitive constructs. This information can probably be passed internally to the ear system.</p>
<p>The preprocessor has to pass its outputs to cognition. These outputs may have to be modified by the processor to support the representations used by the cognitive architecture, and they will have to be passed to the architecture using a connection.</p>
<p>The connection between the processor and cognition will vary based on the architecture and the system. The ear can be a function call to the cognitive architecture when the ear and architecture are implemented in the same programming language, or it might be a socket or Unix pipe. JSON can be a useful way to connect systems as well (Hope, Schoelles, &amp; Gray, 2014). The preprocessor may have multiple processes in it because it has many parallel tasks to perform concurrently. Alexa and other voice recognition systems do not address this problem of attention shifts, as they do not have to process another stream concurrently and are only looking for their name. Running a separate process to look for one's name is possible, but probably leaves out a lot of features of general listening.</p>
<p>Cognition interacts with the preprocessor. While cognition will vary by architecture it can be expected to provide a way to have intrinsic expectations about what sounds will occur and when. Cognition can also hold memory systems that can provide expectations as well as a way to produce behavior with respect to what is heard.</p>
<p>This block structure can mirror the structure we have seen in simulated eyes and hands (Ritter, Baxter, Jones, &amp; Young, 2000). Thus, there is also a role for a control panel or for a controller harness to specify what to listen to and what is heard, where the ear is demonstrated using ambient sounds or a file of sounds (played concurrently to the listener and the CAS). These tools can be useful when developing a system to test and exercise its parts more directly than running it as a complete system. More details on these types of testing and control systems are available in reports on using a simulated eye (Ritter, Van Rooy, &amp; St. Amant, 2002).</p>
<p>Listening can thus be driven by both endogenous and exogenous attention. Endogenous attention describes what a listener selects top-down, or rather, in a goal-or expectation-driven manner from cognition. These could include factors related to following a given speaker or listening for a series of sounds tied to an expected task. Exogenous attention describes selection based on factors outside the listener's goals or expectations, and include sound types or levels that draw a listener's attention to the stimulus automatically and are passed to cognition asynchronously. Figure 2 provides a functional schematic of a possible way to create an artificial ear for a cognitive architecture. Sekuler and Blake (2001) use this diagram to explain hearing. It starts with a hardware input, with binaural inputs. Their differences and their loudness and frequency are computed (the first blocks) and used by all later stages as well as being passed directly to perception within cognition. The loudness and frequency as well as interaural time and intensity are passed to a localization module. Localization is passed to perception as well as to the next stage of sound identification.  Figure 2 suggests that an artificial ear, like a natural auditory system, might not be best represented or implemented as a monolithic system, but that it might have several stages where the sound information is processed. It is also the case that there may be further substructures that would be useful in creating a computational audio system.</p>
<p>The mechanisms in Figures 1 and 2 would allow the pre-processor to be loaded with a set of permanent words (including the agent's name) and perhaps a few to a few dozen words that would also be recognized (family members' names, reserved words from work), and the ability to load in context specific words.</p>
<p>We would anticipate that the loaded in words would decay with time because all aspects of memory decay. As they decay, the ability to recognize them would decline. That is, recognition would be slower and less likely. The decay rate could be dependent on their base activation in cognition, or there could be a different rate in auditory memory. This difference suggests that a study could be run to measure this type of auditory individual differences.  Table 1 notes some functions that a simulated ear might provide, essentially a high-level outline of how it will work. These would serve as function points in a programming language or operators in an architecture. Note, this set of functions references some internal state variables, such as features to listen for and features to not listen for, which would be part of the system as well. This set of function calls from cognition uses three lists internal to the simulated ear: long-term targets, ignored targets, and short-term targets; and a flag to ignore interrupts. When something is heard, these functions synchronously or asynchronously return a heard object, which includes an ID that may be unique to the ear, spatial location or channel; onset time, duration, repetition-ornot; if language: speaker ID and sex, words spoken; modifiers of sound (e.g., walking in grass).</p>
<ol>
<li>Asynchronously return the current sound. 1. Return the current sound. 2. Listen for a primary target in noise short term, given the noise or list of noises, return it asynchronously when found (add target to list). Return that the command has been received. 3. Listen for a secondary target as a secondary task. Return it asynchronously when found. (add target to list). Return that the command has been received. 4. Take interrupts from a list [passed the interrupt list or a delta to the existing list]. Return it asynchronously when found. Return that the command has been received. 5. Ignore interrupts on a list [passed the ignore list or a delta to the original list]. Return that the command has been received. 6. Long-term vigilance for a target sound [passed the list or a delta to the original list]. Return that the command has been received, and return it asynchronously when found. 7. Add-Remove-List items from the three lists. Return that the list has been modified. 8. Turn the head, either to an absolute or relative heading related to current heading. Return that the command has been received. Return that it has been done when done.</li>
</ol>
<p>How well the simulated ear implements these functions defines what it means to have an ear. The functions might not always work, they might forget which list a found sound is on, for example, and might not be fully accurate on what is returned. The quality might be degraded to simulate a human, or the quality might be degraded because the auditory system used in the model is not even as good as a human's.</p>
<p>The auditory system also appears to run autonomously in many ways. Cognition will pass information to the auditory system, but the auditory system will be passing information continuously to cognition. Attention from cognition will decide when to use the auditory information.</p>
<p>The auditory system will include both asynchronous and synchronous communication with cognition. This may make the development and use of a CAS more complex. The human ear does more than just translate air pressure waves into "sound", but it also interprets and filters. A Computational Auditory System would have to replicate at least the basics of this.</p>
<p>Types of Sounds</p>
<p>We next describe the types of sounds that the ear can recognize. We draw our list from a longer report examining sound as used by Foley artists, in video games, and in the psychology literature. The constraints from the psychology literature are also taken up in Section 6.</p>
<p>The Appendix notes the types of sounds we found in our review. They include natural sounds, and sounds from humans and machines. This ontology appears to provide a way to organize all sounds, but it could be revised and is not currently complete. It might represent truly different kinds of sounds or it just might be a useful way to keep track of the diversity of sounds for system development. Table 1's functions and the ontology of sounds (Appendix) to recognize can be examined and expanded in more detail. Table 2 notes capabilities and constraints on performance. They are numbered for current and later reference, and to help in later scorecards of how advanced each implemented CAS is. They are explained in more detail in a longer report.</p>
<p>Suggested Functionalities</p>
<p>One way to design and present updates about a model is to have a scorecard of the features of the data that the model should exhibit. A classic example is the list of regularities that Salthouse (1986) prepared for transcription typing. In his paper he noted 29 regularities. In a related model John compared her model's performance to these regularities (John, 1996). Thus, any single system at any point in time might not have all the capabilities, but might have some boxes filled in like a Yahtzee scorecard.</p>
<p>Section 1 of the scorecard in Table 2 notes some basic hardware (or equivalent) requirements. These might be realized in software if the sound comes in via an audio line. There may be further constraints, including that the microphones are appropriately positioned, such as 1 foot apart, and that a surrounding ear provides additional nuances to the sound. The volume sensitivity should be limited to 10 dB and 130 dB above background. It might be useful to start with the same frequency response as a human ear, but allow this to be modified to explore aging, hearing loss, and other scenarios.</p>
<p>Section 2 of Table 2 basic sound capabilities, notes that the CAS should be able to recognize the sounds in the Appendix, including natural (e.g., leaves rustling), machine made, and humanmade sounds (e.g., dropped tool, walking). These sounds may be more different in the ontology for ease of manipulation than in how they are recognized. There are several aspects of sound that the CAS should be able to recognize. Not all sounds will be fully recognized. The spatial accuracy of recognition is not perfect and is affected by several factors (such as age and sound frequency) and is limited to approximately +/-1-5 for a source in front of the listener (depending on frequency) and up to +/-20 for any on the side or behind (Boff &amp; Lincoln, 1988, 2.812;Letowski &amp; Letowski, 2011, p. 72). Table 2. Capabilities and constraints on an artificial ear.</p>
<ol>
<li>Basic hardware implementation a. Consist of two microphones to implement binaural processing.</li>
</ol>
<p>b. Sensitivity to frequency should be similar to a real ear (RE), c. Have a "front" and "sides". Many auditory effects are enhanced if the sound source is in front, d. Have the ability for different sensitivity curves to simulate damaged, younger/older ears. 2. Basic capabilities a. Recognize ambient sounds in the ontology in Table 2 and be able to identify the approximate distance and bearing of a sound using binaural processes. b. Be able to lock on to a target speaker or single auditory object and be able to track it in the presence of a variable noisy environment. c. Recognize sounds on primary target list. d. Recognize sounds on secondary target list. e. Primary and secondary list decays with time making probability of recognition decrease and time to recognize increase. f. Follow the target audio stream if ear moves spatially itself or if the target moves g. Frequency changes, Doppler effects. Detecting these and tracking despite these. 3. Speaker-related capabilities a. Identify voice types, genders, accents b. Identify speaker (this may be a cognitive effect, but some aspects may be in the ear and pre-processor, such as prosody, timbre, changes in volume, harmonics, vocabulary, etc.) c. Detecting effects on speech such as whispering, muttering, or speech impediments including lisping and talking with food in mouth 4. Recognition and cocktail party capabilities a. Hear listener's name or selected words in an audio stream b. Be able to focus on one audio stream, even in a relatively noisy background c. Be able to switch focus to a different audio stream based on a target of interest (such as an alarm, or its "name"). It should then be able to re-focus on the original audio stream. 5. Alarm-related capabilities and regularities a. ID each alarm sound for cognition to recognize it b. Recognize sound as new type or previously known type (might belong in cognition, but type and ID should support this).</p>
<p>These assumption and constraints might be modified to model a super-ear, an ear that can hear better than humans. But also see Schooler and Anderson (2005) that suggests that forgetting can help memory and cognition, and this is probably how people function. Having a super-ear might be distracting for typical tasks.</p>
<p>Section 3 of Table 2 covers speaker recognition and related effects. The system should be able to recognize a range of features of spoken language. Not every system and particularly not early systems will be able to recognize and report all these features. This may include the ability to recognize what language is being spoken and with what accent before or in addition to recognizing the words. Table 2 is the ability to listen to a single audio stream in a complex environment. Here, there should be the ability to follow a single speaker and also to switch streams when one stream becomes more interesting.</p>
<p>Section 4 of</p>
<p>Section 5 of Table 2 is the ability to recognize alarms. This set of regularities is related to generating interruptions to cognition. Some of the capabilities have to be added to the ear or cognitive architecture. An auditory pre-processor can do some of these tasks, or they can be done in cognition. The meaning of each alarm would be decided in cognition, but what is returned from the ear prior to this has to be unique enough for cognition to do its job. How significant the alarm is also has to be in cognition and may be based on context, but the volume and distance the sound is coming from appears to be the ear or pre-processor's job. A human factors book on alarms will be helpful as this area of capabilities is developed (Stanton, 1994).</p>
<p>The CAS should support cognition to the point where the ear and cognition represent how the alarm is raised to cognition in a way that helps recognize the alarm. Simply relaying the alarm is not any more useful than the alarm itself. The ear's alarm (content, volume, distance) representation has to support reasoning about context, meaning, possible action paths, and automatically opening up and displaying the relevant emergency procedures. These activities may all be supported by:</p>
<ol>
<li>The ability to detune or not report an alarm stream (e.g., in the situation where there may be multiple ears paying attention in the same area like a hospital ward, or a submarine where each ear is focusing on a particular station). The ear should be able to recognize if the alarm is coming from its station and ignore other stations based on space or volume or another aspect of sound. 2. If there are many alarms at the same time, the CAS should be able to help cognition prioritize the alarms. Cognition or an auditory pre-processor might perform this task. If there are multiple alarms that are similar (such as all pumps have failed), the ear should consolidate them. Hollywell and Marshall (Stanton, 1994) concluded that operators can read alarms at a rate of 30 per min. if they had no other tasks, but preferred no more than 15 per min. Increased alarm rate does not decrease accuracy in assessing them, but does lead to missed alarms. 3. The ear can be programmed with knowledge of what alarms it will be listening for, but should be robust enough to identify an alarm that is not pre-programmed. New equipment can be installed, and current equipment could have a software update or other change that modified the sound or intensity of the alarm, which has happened with alarms.</li>
</ol>
<p>Conclusion</p>
<p>We have provided an overview of the design of a computational audio system (an artificial ear) for cognitive architectures. Table 1 provides a list of functions an ear should provide to cognition. It also includes an outline of the data structures. Appendix 1 provides a summary ontology of the types of sounds that an artificial ear could recognize and also example instances and sub-instances. We can see that we will need to be able to add sounds routinely to this list, and to code them and tie them to cognitive representations. Table 2 provides a scorecard to represent how sophisticated and complete a given instantiation of a simulated ear is. Together, they provide a design for a computational audio scene analysis for use by cognitive architectures and agents.</p>
<p>The list of capabilities and constraints are incomplete. As we develop this initial list we have found more challenging items that we have not yet added, including habituation, other types of learning, a phonological loop, and audio priming.</p>
<p>We have started work on implementing an artificial ear for use by cognitive architectures. We have created a capability to duplicate the cocktail party effect by using cluster analysis on cochleagram (audio spectrograms). Reports are available on its technical implementation (Daley, Bonacci, Gever, Diaz, &amp; Bolkhovsky, 2021) and how it is being integrated with the Arcadia architecture (Gever et al., 2020). In this integrated model the ear will be used to turn a head towards a sound and look at it. This work suggests several insights. The greatest is to the extent that there is active vision (Findlay &amp; Gilchrist, 2003), which there is, there will be active hearing as well. Thus, there will be interactions between the ear, cognition, and action; they will work together where cognition receives information from the ear, and in turn, passes to the ear sounds to listen for, or to modify the hearing apparatus (e.g., turn the head) to hear better. Where this information processing occurs might vary depending on the implementation of the CAS and the architecture. As we create this model ear, we will need example applications to show this interaction.</p>
<p>Fig. 2 .
2Diagram of the potential substructure of functional components for a pre-processing component of a computational audio system, based on Sekuler and Blake (1985,Figure 9.8).</p>
<p>Fig. 1. Diagram of the functional components for a computational audio system. Arrows indicates connections.Input 
L/R 
Perception 
Localization </p>
<p>Loudness 
and Frequency </p>
<p>Interaural Time 
and Intensity </p>
<p>Sound 
Identification 
Cognition </p>
<p>Ear 
! ==" 
Preprocessing 
! ======" 
Cognition </p>
<p>Constraints 
on the 
mechanics 
of sound are 
applied. </p>
<p>Part of 
the CAS </p>
<p>Preprocessing and 
also the memory of 
targets and non-
targets. 
Primarily matches 
sounds to targets 
from cognition. 
What to do with 
unknown sounds 
has to be an 
important area. </p>
<p>Passes 
commands and 
results between 
Pre-processor 
and Cognition. 
Depends on the 
base language of 
the cognitive 
architecture. </p>
<p>May pass information 
to the CAS. It will 
need to know what it 
can pass to the ear to 
listen to, and then use 
what is passed back. </p>
<p>Table 1 .
1Suggested functions of a model human ear for use with a cognitive architecture and agents.
AcknowledgmentsSupport for this project was provided as work unit F1103-Office of Naval Research, Code 34. Lia Bonacci provided very used comments. Steve Croker, David Gever, and the reviewers provided useful comments. Sue Van Vactor provided a useful proofreading, twice. The views expressed in this article reflect the results of research conducted by the authors and do not necessarily reflect the official policy or position of the Department of the Navy, Department of Defense, nor the United States Government. The authors are federal and contracted employees of the United States government. This work was prepared as a part of official duties. Title 17 U.S.C. 105 provides that "copyright protection under this title is not available for any work of the United States Government." Title 17 U.S.C. 101 defines a U.S. Government work as work prepared by a military service member or employee of the U.S. Government as part of that person's official duties.Appendix 1. Ontology of sounds (partial).
The architecture of cognition. J R Anderson, Harvard University PressCambridge, MAAnderson, J. R. (1983). The architecture of cognition. Cambridge, MA: Harvard University Press.</p>
<p>How can the human mind occur in the physical universe. J R Anderson, Oxford University PressNew York, NYAnderson, J. R. (2007). How can the human mind occur in the physical universe? New York, NY: Oxford University Press.</p>
<p>Engineering data compendium (User's guide). K R Boff, Lincoln, J. E.Boff, K. R., &amp; Lincoln, J. E. (Eds.). (1988). Engineering data compendium (User's guide).</p>
<p>Air Force Base. Wright-Patterson, Armstrong Aerospace Medical Research LaboratoryWright-Patterson Air Force Base, OH: Harry G. Armstrong Aerospace Medical Research Laboratory.</p>
<p>Towards an attention-driven model of task switching. W Bridewell, C Wasylyshyn, P F Bello, Advances in Cognitive Systems. 6Bridewell, W., Wasylyshyn, C., &amp; Bello, P. F. (2018). Towards an attention-driven model of task switching. Advances in Cognitive Systems, 6, 1-16.</p>
<p>ACT-R/PM and menu selection: Applying a cognitive architecture to HCI. M D Byrne, International Journal of Human-Computer Studies. 551Byrne, M. D. (2001). ACT-R/PM and menu selection: Applying a cognitive architecture to HCI. International Journal of Human-Computer Studies, 55(1), 41-84.</p>
<p>The human-computer interaction handbook: Fundamentals, evolving technologies and emerging applications. M D Byrne, J. Jacko &amp; A. SearsErlbaumMahwah, NJCognitive architectureByrne, M. D. (2003). Cognitive architecture. In J. Jacko &amp; A. Sears (Eds.), The human-computer interaction handbook: Fundamentals, evolving technologies and emerging applications (pp. 97- 117). Mahwah, NJ: Erlbaum.</p>
<p>Cluster analysis for the separation of auditory scenes. M S Daley, L M Bonacci, D H Gever, K Diaz, J B Bolkhovsky, IEEE Access. 9Daley, M. S., Bonacci, L. M., Gever, D. H., Diaz, K., &amp; Bolkhovsky, J. B. (2021). Cluster analysis for the separation of auditory scenes. IEEE Access, 9, 130959-130967.</p>
<p>Active Vision: The psychology of looking and seeing. J M Findlay, I D Gilchrist, Oxford University PressOxford, UKFindlay, J. M., &amp; Gilchrist, I. D. (2003). Active Vision: The psychology of looking and seeing. Oxford, UK: Oxford University Press.</p>
<p>Integration of a Computational Auditory Scene Analysis Software Architecture with the ARCADIA cognitive model: Software design, usage, implementation, demonstration. D Gever, M Daley, M Babina, B Snihurowych, L Bonacci, J Bolkhovsky, No. NSMRL/F1703/TR-2020-1340Groton, CTNaval Submarine Medical Research LabTech. ReportGever, D., Daley, M., Babina, M., Snihurowych, B., Bonacci, L., &amp; Bolkhovsky, J. (2020). Integration of a Computational Auditory Scene Analysis Software Architecture with the ARCADIA cognitive model: Software design, usage, implementation, demonstration (Tech. Report No. NSMRL/F1703/TR-2020-1340): Naval Submarine Medical Research Lab, Groton, CT.</p>
<p>ARTSTREAM: a neural network model of auditory scene analysis and source segregation. S Grossberg, K K Govindarajan, L L Wyse, M A Cohen, Neural Networks. 174Grossberg, S., Govindarajan, K. K., Wyse, L. L., &amp; Cohen, M. A. (2004). ARTSTREAM: a neural network model of auditory scene analysis and source segregation. Neural Networks, 17(4), 511-536.</p>
<p>Simplifying the interaction between cognitive models and task environments with the JSON Network Interface. R M Hope, M J Schoelles, W D Gray, Behaviour Research Methods. 464Hope, R. M., Schoelles, M. J., &amp; Gray, W. D. (2014). Simplifying the interaction between cognitive models and task environments with the JSON Network Interface. Behaviour Research Methods, 46(4), 1007-1012.</p>
<p>TYPIST: A theory of performance in skilled typing. B E John, Human-Computer Interaction. 114John, B. E. (1996). TYPIST: A theory of performance in skilled typing. Human-Computer Interaction, 11(4), 321-355.</p>
<p>A review of 40 years of cognitive architecture research: Focus on perception, attention, learning and applications. I Kotseruba, J K Tsotsos, AI Review. 53Kotseruba, I., &amp; Tsotsos, J. K. (2020). A review of 40 years of cognitive architecture research: Focus on perception, attention, learning and applications. AI Review, 53, 17-94.</p>
<p>The Soar cognitive architecture. J E Laird, MIT PressCambridge, MALaird, J. E. (2012). The Soar cognitive architecture. Cambridge, MA: MIT Press.</p>
<p>Localization error: Accuracy and precision of auditory localization In. T Letowski, S Letowski, Advances in sound localization. P. StrumilloInTechLetowski, T., &amp; Letowski, S. (2011). Localization error: Accuracy and precision of auditory localization In P. Strumillo (Ed.), Advances in sound localization (pp. 55-78): InTech.</p>
<p>Unified Theories of Cognition. A Newell, Harvard University PressCambridge, MANewell, A. (1990). Unified Theories of Cognition. Cambridge, MA: Harvard University Press.</p>
<p>CoJACK Software Specification. Human Variability within Computer Generated Forces Project. E Norling, F E Ritter, RT/COM/3/006: Agent Oriented Software Limited. Norling, E., &amp; Ritter, F. E. (2004). CoJACK Software Specification. Human Variability within Computer Generated Forces Project, Project No: RT/COM/3/006: Agent Oriented Software Limited.</p>
<p>Time-based resource sharing in ARCADIA. K O&apos;neill, W Bridewell, P Bello, Proceedings of the 40th Annual Meeting of the Cognitive Science Society. the 40th Annual Meeting of the Cognitive Science SocietyCogSci; Madison, WI, USAPaper presented at the. RetrievedO'Neill, K., Bridewell, W., &amp; Bello, P. (2018). Time-based resource sharing in ARCADIA. Paper presented at the Proceedings of the 40th Annual Meeting of the Cognitive Science Society, CogSci 2018, Madison, WI, USA, July 25-28, 2018. Retrieved.</p>
<p>Modeling human and organizational behavior: Application to military simulations. R W Pew, Mavor, A. S.National Academy PressWashington, DCPew, R. W., &amp; Mavor, A. S. (Eds.). (1998). Modeling human and organizational behavior: Application to military simulations. Washington, DC: National Academy Press. books.nap.edu/catalog/6173, checked Feb 2020.</p>
<p>Foundations for designing user-centered systems: What system designers need to know about people. F E Ritter, G D Baxter, E F Churchill, SpringerLondon, UKRitter, F. E., Baxter, G. D., &amp; Churchill, E. F. (2014). Foundations for designing user-centered systems: What system designers need to know about people. London, UK: Springer.</p>
<p>Supporting cognitive models as users. F E Ritter, G D Baxter, G Jones, R M Young, ACM Transactions on Computer-Human Interaction. 72Ritter, F. E., Baxter, G. D., Jones, G., &amp; Young, R. M. (2000). Supporting cognitive models as users. ACM Transactions on Computer-Human Interaction, 7(2), 141-173.</p>
<p>Techniques for modeling human performance in synthetic environments: A supplementary review. F E Ritter, N R Shadbolt, D Elliman, R M Young, F Gobet, G D Baxter, HSIACWright-Patterson Air Force Base, OHRitter, F. E., Shadbolt, N. R., Elliman, D., Young, R. M., Gobet, F., &amp; Baxter, G. D. (2003). Techniques for modeling human performance in synthetic environments: A supplementary review. Wright-Patterson Air Force Base, OH: Human Systems Information Analysis Center (HSIAC).</p>
<p>Some futures for cognitive modeling and architectures: Design patterns that you can too. F E Ritter, F Tehranchi, C L Dancy, S E Kase, Computational and Mathematical Organization Theory. 26Ritter, F. E., Tehranchi, F., Dancy, C. L., &amp; Kase, S. E. (2020). Some futures for cognitive modeling and architectures: Design patterns that you can too. Computational and Mathematical Organization Theory, 26, 278-306.</p>
<p>ACT-R: A cognitive architecture for modeling cognition. F E Ritter, F Tehranchi, J D Oury, Wiley Interdisciplinary Reviews: Cognitive Science. 103Paper e1488Ritter, F. E., Tehranchi, F., &amp; Oury, J. D. (2019). ACT-R: A cognitive architecture for modeling cognition. Wiley Interdisciplinary Reviews: Cognitive Science, 10(3), Paper e1488.</p>
<p>A user modeling design tool based on a cognitive architecture for comparing interfaces. F E Ritter, D Van Rooy, St, R Amant, Computer-Aided Design of User Interfaces III, Proceedings of the 4th International Conference on Computer-Aided Design of User Interfaces CADUI'2002. Dordrecht, NLKluwer Academics PublisherRitter, F. E., Van Rooy, D., &amp; St. Amant, R. (2002). A user modeling design tool based on a cognitive architecture for comparing interfaces. In Computer-Aided Design of User Interfaces III, Proceedings of the 4th International Conference on Computer-Aided Design of User Interfaces CADUI'2002, 111-118. Dordrecht, NL: Kluwer Academics Publisher.</p>
<p>Perceptual, cognitive, and motoric aspects of transcription typing. T A Salthouse, Psychological Bulletin. 33Salthouse, T. A. (1986). Perceptual, cognitive, and motoric aspects of transcription typing. Psychological Bulletin, 3(3), 303-319.</p>
<p>How forgetting aids heuristic inference. L J Schooler, R Hertwig, Psychological Review. 112Schooler, L. J., &amp; Hertwig, R. (2005). How forgetting aids heuristic inference. Psychological Review, 112, 610-628.</p>
<p>Perception. R Sekuler, R Blake, McGraw-HillNew York, NYSekuler, R., &amp; Blake, R. (2001). Perception. New York, NY: McGraw-Hill.</p>
<p>Human factors in alarm design. N Stanton, Taylor &amp; FrancisLondon, UKStanton, N. (Ed.). (1994). Human factors in alarm design. London, UK: Taylor &amp; Francis.</p>
<p>R Sun, Hybrid connectionist-symbolic modules: A report from the IJCAI-95 Workshop on Connectionist-Symbolic Integration. AI Magazine. 17Sun, R. (1996). Hybrid connectionist-symbolic modules: A report from the IJCAI-95 Workshop on Connectionist-Symbolic Integration. AI Magazine, 17(2), 99-103.</p>
<p>Encyclopedia of artificial intelligence -The past, present, and future of AI. F Tehranchi, P. L. Frana &amp; M. KleinABC-CLIOSanta Barbara, CAInteraction for cognitive agentsTehranchi, F. (2021a). Interaction for cognitive agents. In P. L. Frana &amp; M. Klein (Eds.), Encyclopedia of artificial intelligence -The past, present, and future of AI, (pp. 191-192). Santa Barbara, CA: ABC-CLIO.</p>
<p>Encyclopedia of artificial intelligence -The past, present, and future of AI. F Tehranchi, P. L. Frana &amp; M. KleinABC-CLIOSanta Barbara, CAThe symbol grounding problemTehranchi, F. (2021b). The symbol grounding problem. In P. L. Frana &amp; M. Klein (Eds.), Encyclopedia of artificial intelligence -The past, present, and future of AI, (pp. 310-311). Santa Barbara, CA: ABC-CLIO.</p>
<p>Children and robots learning to play hide and seek. J G Trafton, A C Schultz, D Perznowski, M D Bugajska, W Adams, N L Cassimatis, Proceedings of the 1st ACM SIGCHI/SIGART Conference on Human-Robot Interaction. the 1st ACM SIGCHI/SIGART Conference on Human-Robot InteractionNew York, NYACM PressTrafton, J. G., Schultz, A. C., Perznowski, D., Bugajska, M. D., Adams, W., Cassimatis, N. L., et al. (2006). Children and robots learning to play hide and seek. In Proceedings of the 1st ACM SIGCHI/SIGART Conference on Human-Robot Interaction, 242-249. New York, NY: ACM Press.</p>            </div>
        </div>

    </div>
</body>
</html>
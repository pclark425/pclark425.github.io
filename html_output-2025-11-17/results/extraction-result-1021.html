<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1021 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1021</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1021</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-268363886</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2403.07548v2.pdf" target="_blank">Online Continual Learning For Interactive Instruction Following Agents</a></p>
                <p><strong>Paper Abstract:</strong> In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information during training (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA). In the proposed Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state of the art in our empirical validations by noticeable margins. The project page including codes is https://github.com/snumprlab/cl-alfred.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1021.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1021.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CL-ALFRED agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Continual-Learning Instruction-Following Agent (CL-ALFRED)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulated embodied instruction-following agent trained in an online continual learning regime on the ALFRED benchmark (AI2-THOR scenes) using imitation learning with episodic memory and logit-distillation (with the proposed CAMA update). Evaluated on both seen and unseen environment splits using success- and goal-condition rates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CL-ALFRED instruction-following agent</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A multimodal policy that maps visual observations (surrounding RGB views) and step-by-step language instructions to action sequences and object-class predictions using behavior cloning (imitation learning / behavior cloning). The architecture factorizes action prediction and object-class modules (based on Singh et al. 2021 / Kim et al. 2021), uses episodic memory (M=500 exemplars) for replay and logit distillation, and integrates Confidence-Aware Moving Average (CAMA) to update stored logits online.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (AI2-THOR / ALFRED environments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ALFRED / AI2-THOR interactive instruction-following environments</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Full interactive instruction-following tasks combining natural language understanding, navigation, and object interaction. Episodes require multi-step behavior sequences (e.g., locate objects, pick/place, heat, cool, clean, etc.). The paper uses 4 high-level scene types (KITCHENS, LIVINGROOMS, BEDROOMS, BATHROOMS) with multiple layout/appearance variations and a suite of seven behavior types.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Characterized by: number of behavior types (7 behavior categories: EXAMINE, PICK&PLACE, HEAT, COOL, CLEAN, PICK2&PLACE, MOVABLE), multi-step episode length T (variable per demonstration), joint navigation+manipulation+language modalities, and distinct action set A and object-class set C (used in per-class logit maintenance). Also measured implicitly by task difficulty via Success Rate (SR) and Goal-Condition (GC).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (full multi-modal interactive instruction following with navigation + object interaction and seven distinct behavior categories)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number of environment types (4), number of environment instances per type (30 variations each in AI2-THOR), seen vs unseen scene splits (validation seen/unseen), and dataset imbalance across environment types (episode counts per type reported and subsampled to balance).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (30 environment variations per environment type; evaluation includes unseen environments to test generalization)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Success Rate (SR) and Goal-Condition success rate (GC), reported both as last-task (SR_last, GC_last) and averaged across incremental tasks (SR_avg, GC_avg); also evaluated in validation seen and unseen splits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Representative results for the proposed CAMA agent (means ± std): Validation Seen — Behavior-IL: SR_last 30.71 ± 0.78%, GC_last 40.85 ± 0.73%, SR_avg 29.67 ± 2.66%, GC_avg 38.17 ± 2.34%; Environment-IL: SR_last 29.48 ± 0.27%, GC_last 38.13 ± 0.85%, SR_avg 35.09 ± 1.92%, GC_avg 44.02 ± 2.21%. Validation Unseen — Behavior-IL: SR_last 13.64 ± 0.94%, GC_last 28.75 ± 0.92%, SR_avg 14.19 ± 1.38%, GC_avg 27.30 ± 1.38%; Environment-IL: SR_last 14.60 ± 0.43%, GC_last 30.99 ± 0.75%, SR_avg 15.67 ± 0.77%, GC_avg 33.40 ± 1.45%.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper discusses interactions and trade-offs: (1) More complex tasks (multi-step behaviors combining navigation and manipulation) exacerbate catastrophic forgetting when new tasks/environments arrive; (2) environment variation (many scene instances and unseen splits) reduces performance (unseen SR substantially lower than seen SR), motivating continual updates; (3) task-order and shared action/object-class overlap affect forward transfer — tasks with shared classes (e.g., HEAT and COOL) help later learning, while dissimilar preceding tasks can yield poor performance on subsequent tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Online, task-free continual learning (online disjoint stream), single-pass streaming of episodes with episodic memory replay (M=500), imitation learning (behavior cloning), and logit-distillation using Confidence-Aware Moving Average (CAMA) for memory logits update.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Evaluated on seen vs unseen splits. Performance on unseen scenes is substantially lower than on seen scenes (e.g., Validation Seen Behavior-IL SR_last ≈ 30.7% vs Validation Unseen SR_last ≈ 13.6%), but CAMA outperforms baselines on both splits, indicating improved generalization to unseen visual configurations compared to prior continual-learning baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Online single-pass training on ALFRED episodes; after subsampling the training split for Environment-IL the training set contained 12,564 episodes in total (3,141 per env type); episodic memory size M = 500 (~2.38% of ALFRED train episodes). Training uses batches of size 32 per streamed sample and Adam optimizer (LR 0.001).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Online task-free logit updating with confidence-aware moving average (CAMA) substantially reduces forgetting and improves SR/GC relative to baselines; 2) Naïve finetuning suffers large performance drops vs joint training (47.5–51% relative drops), indicating continual learning is necessary when environment/task distributions shift; 3) Environment variation (unseen scenes) and dataset imbalance hurt performance, but balancing/subsampling and CAMA mitigate some effects; 4) Task order and class-sharing among behaviors mediate forward transfer — shared object/action classes across behaviors improve later task learning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Continual Learning For Interactive Instruction Following Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1021.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1021.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Behavior-IL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Behavior Incremental Learning (Behavior-IL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continual-learning setup where the embodied agent receives streams of episodes grouped by behavior type and must learn new behaviors incrementally while preserving prior behaviors; uses online, task-free, disjoint streaming and evaluates retention (last/avg SR and GC).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Same CL-ALFRED instruction-following agent (evaluated under Behavior-IL sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent trained with behavior cloning and episodic memory replay; in Behavior-IL the stream is ordered by behavior categories (7 categories) and the agent must learn new behavior types sequentially without task boundary metadata (task-free).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (instruction-following in ALFRED/AI2-THOR)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ALFRED episodes across behaviors (EXAMINE, PICK&PLACE, HEAT, COOL, CLEAN, PICK2&PLACE, MOVABLE)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Complexity arises from distinct behavior goal-conditions and the multi-step composition of actions required to complete each behavior; behaviors may share object classes or actions which affects transfer/forgetting.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Behavior diversity: 7 discrete behavior types; per-episode multi-step length T; overlap in object usage across behaviors (visualized per-behavior object frequency).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (multiple distinct behavior types that require navigation+interaction and can be multi-step)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Variation arises from different episodes per behavior, differing object instances and scene placements; training uses multiple random orders of behavior sequences (5 random permutations) to avoid bias from specific orders.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>medium (behavior categories fixed but episodes within a behavior vary in layout, objects and instructions)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR_last, GC_last, SR_avg, GC_avg reported on validation seen & unseen splits.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CAMA (Behavior-IL) — Validation Seen: SR_last 30.71 ± 0.78%, GC_last 40.85 ± 0.73%, SR_avg 29.67 ± 2.66%, GC_avg 38.17 ± 2.34%. Validation Unseen: SR_last 13.64 ± 0.94%, GC_last 28.75 ± 0.92%, SR_avg 14.19 ± 1.38%, GC_avg 27.30 ± 1.38%.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Discussed — behavior complexity interacts with variation and task order: behaviors that share objects/actions (e.g., HEAT and COOL) produce positive forward transfer, making subsequent tasks easier; disjoint behavior sequences or behavior changes increase catastrophic forgetting and reduce performance. The paper reports dependency of performance on task order and class-sharing.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Online disjoint continual learning across behavior types (task-free), episodic memory replay (M=500), behavior cloning, and CAMA logit updates; training repeated across 5 random behavior-order seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Agents evaluated on seen and unseen scenes; Behavior-IL performance is noticeably lower on unseen scenes (~13–14% SR_last) compared to seen scenes (~30% SR_last), indicating limited generalization but relative improvement over baselines with CAMA.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Per-sequence training across ALFRED episodes; exact per-behavior episode counts follow ALFRED splits and the experiments use three random seeds; episodic memory size 500.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Behavior-IL highlights the importance of preserving prior behaviors while learning new ones; 2) CAMA improves retention vs baselines in this setup; 3) Task-order and shared object/action classes materially affect transfer and forgetting — learning related behaviors earlier helps future performance.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Continual Learning For Interactive Instruction Following Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1021.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1021.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Environment-IL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Environment Incremental Learning (Environment-IL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A continual-learning setup where the agent sequentially encounters different environment types (e.g., KITCHENS, LIVINGROOMS, BEDROOMS, BATHROOMS) and must learn to perform behaviors across expanding scene distributions while retaining competence in previously learned scene types.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Same CL-ALFRED instruction-following agent (evaluated under Environment-IL sequences)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Agent trained with behavior cloning and episodic memory replay; in Environment-IL the stream is ordered by environment type and the agent must adapt to new scene layouts/visual domains (task-free, online).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (AI2-THOR / ALFRED)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>ALFRED / AI2-THOR environment types (KITCHENS, LIVINGROOMS, BEDROOMS, BATHROOMS)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Each environment type has multiple layout/appearance variations (30 variations per type). Complexity comes from scene layouts, object placements, and visual domain shifts across environment types; dataset imbalance across environment types is present and addressed by subsampling.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of environment types (4), number of instances per type (30 variations), imbalance in episode counts per type before subsampling (e.g., Kitchens much larger than others), and seen/unseen splits to test domain shift.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>high (navigation + interaction across multiple distinct scene types with many per-type instantiations)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Per-type instance count (30), seen vs unseen scene splits, and subsampled balanced train set (3,141 episodes per env type after subsampling; total train 12,564 episodes); also an explicit 'imbalanced Environment-IL' experiment without subsampling.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (30 variations per environment type; explicit unseen environments evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>SR_last, GC_last, SR_avg, GC_avg on validation seen and unseen; main focus on unseen SR as generalization metric.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>CAMA (Environment-IL) — Validation Seen: SR_last 29.48 ± 0.27%, GC_last 38.13 ± 0.85%, SR_avg 35.09 ± 1.92%, GC_avg 44.02 ± 2.21%. Validation Unseen: SR_last 14.60 ± 0.43%, GC_last 30.99 ± 0.75%, SR_avg 15.67 ± 0.77%, GC_avg 33.40 ± 1.45%. In imbalanced Environment-IL, CAMA still outperforms baselines by noticeable margins (detailed table in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Yes — the paper explains that increased environment variation (many scene instances and unseen splits) yields lower generalization performance and can bias models toward majority environments when data are imbalanced; balancing episodes across environment types mitigates bias. They also show CAMA is robust under imbalance and improves retention across environment shifts.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Online, task-free incremental exposure to environment types (disjoint), subsampling to balance per-type episodes (when used), episodic memory replay (M=500), behavior cloning, and CAMA logit updates; experiments use five random environment-order sequences.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>Explicit seen vs unseen evaluation: performance drops on unseen environment scenes (Validation Unseen SR_last ~14.6% vs Validation Seen SR_last ~29.5% for CAMA). CAMA outperforms prior methods in both seen and unseen settings, and remains effective in an imbalanced training regime.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Training used subsampled train split (3,141 episodes per env type → total 12,564 training episodes) when balancing; episodic memory size M=500 (~2.38% of original ALFRED training episodes) and single-pass online stream.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Environment variation (many instances and unseen splits) is a major source of generalization difficulty; 2) Subsampling to balance environment types reduces bias but CAMA also shows robustness to imbalance; 3) CAMA's confidence-aware logit updating improves retention and adaptation across environment shifts relative to baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Online Continual Learning For Interactive Instruction Following Agents', 'publication_date_yy_mm': '2024-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>ALFRED: A benchmark for interpreting grounded instructions for everyday tasks <em>(Rating: 2)</em></li>
                <li>AI2-THOR: An interactive 3D environment for visual AI <em>(Rating: 2)</em></li>
                <li>CORA: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents <em>(Rating: 2)</em></li>
                <li>Dark Experience for General Continual Learning: a strong, simple baseline <em>(Rating: 1)</em></li>
                <li>Class-incremental continual learning into the extended DER-verse <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1021",
    "paper_id": "paper-268363886",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "CL-ALFRED agent",
            "name_full": "Continual-Learning Instruction-Following Agent (CL-ALFRED)",
            "brief_description": "A simulated embodied instruction-following agent trained in an online continual learning regime on the ALFRED benchmark (AI2-THOR scenes) using imitation learning with episodic memory and logit-distillation (with the proposed CAMA update). Evaluated on both seen and unseen environment splits using success- and goal-condition rates.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "CL-ALFRED instruction-following agent",
            "agent_description": "A multimodal policy that maps visual observations (surrounding RGB views) and step-by-step language instructions to action sequences and object-class predictions using behavior cloning (imitation learning / behavior cloning). The architecture factorizes action prediction and object-class modules (based on Singh et al. 2021 / Kim et al. 2021), uses episodic memory (M=500 exemplars) for replay and logit distillation, and integrates Confidence-Aware Moving Average (CAMA) to update stored logits online.",
            "agent_type": "simulated agent (AI2-THOR / ALFRED environments)",
            "environment_name": "ALFRED / AI2-THOR interactive instruction-following environments",
            "environment_description": "Full interactive instruction-following tasks combining natural language understanding, navigation, and object interaction. Episodes require multi-step behavior sequences (e.g., locate objects, pick/place, heat, cool, clean, etc.). The paper uses 4 high-level scene types (KITCHENS, LIVINGROOMS, BEDROOMS, BATHROOMS) with multiple layout/appearance variations and a suite of seven behavior types.",
            "complexity_measure": "Characterized by: number of behavior types (7 behavior categories: EXAMINE, PICK&PLACE, HEAT, COOL, CLEAN, PICK2&PLACE, MOVABLE), multi-step episode length T (variable per demonstration), joint navigation+manipulation+language modalities, and distinct action set A and object-class set C (used in per-class logit maintenance). Also measured implicitly by task difficulty via Success Rate (SR) and Goal-Condition (GC).",
            "complexity_level": "high (full multi-modal interactive instruction following with navigation + object interaction and seven distinct behavior categories)",
            "variation_measure": "Number of environment types (4), number of environment instances per type (30 variations each in AI2-THOR), seen vs unseen scene splits (validation seen/unseen), and dataset imbalance across environment types (episode counts per type reported and subsampled to balance).",
            "variation_level": "high (30 environment variations per environment type; evaluation includes unseen environments to test generalization)",
            "performance_metric": "Success Rate (SR) and Goal-Condition success rate (GC), reported both as last-task (SR_last, GC_last) and averaged across incremental tasks (SR_avg, GC_avg); also evaluated in validation seen and unseen splits.",
            "performance_value": "Representative results for the proposed CAMA agent (means ± std): Validation Seen — Behavior-IL: SR_last 30.71 ± 0.78%, GC_last 40.85 ± 0.73%, SR_avg 29.67 ± 2.66%, GC_avg 38.17 ± 2.34%; Environment-IL: SR_last 29.48 ± 0.27%, GC_last 38.13 ± 0.85%, SR_avg 35.09 ± 1.92%, GC_avg 44.02 ± 2.21%. Validation Unseen — Behavior-IL: SR_last 13.64 ± 0.94%, GC_last 28.75 ± 0.92%, SR_avg 14.19 ± 1.38%, GC_avg 27.30 ± 1.38%; Environment-IL: SR_last 14.60 ± 0.43%, GC_last 30.99 ± 0.75%, SR_avg 15.67 ± 0.77%, GC_avg 33.40 ± 1.45%.",
            "complexity_variation_relationship": "Yes — the paper discusses interactions and trade-offs: (1) More complex tasks (multi-step behaviors combining navigation and manipulation) exacerbate catastrophic forgetting when new tasks/environments arrive; (2) environment variation (many scene instances and unseen splits) reduces performance (unseen SR substantially lower than seen SR), motivating continual updates; (3) task-order and shared action/object-class overlap affect forward transfer — tasks with shared classes (e.g., HEAT and COOL) help later learning, while dissimilar preceding tasks can yield poor performance on subsequent tasks.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Online, task-free continual learning (online disjoint stream), single-pass streaming of episodes with episodic memory replay (M=500), imitation learning (behavior cloning), and logit-distillation using Confidence-Aware Moving Average (CAMA) for memory logits update.",
            "generalization_tested": true,
            "generalization_results": "Evaluated on seen vs unseen splits. Performance on unseen scenes is substantially lower than on seen scenes (e.g., Validation Seen Behavior-IL SR_last ≈ 30.7% vs Validation Unseen SR_last ≈ 13.6%), but CAMA outperforms baselines on both splits, indicating improved generalization to unseen visual configurations compared to prior continual-learning baselines.",
            "sample_efficiency": "Online single-pass training on ALFRED episodes; after subsampling the training split for Environment-IL the training set contained 12,564 episodes in total (3,141 per env type); episodic memory size M = 500 (~2.38% of ALFRED train episodes). Training uses batches of size 32 per streamed sample and Adam optimizer (LR 0.001).",
            "key_findings": "1) Online task-free logit updating with confidence-aware moving average (CAMA) substantially reduces forgetting and improves SR/GC relative to baselines; 2) Naïve finetuning suffers large performance drops vs joint training (47.5–51% relative drops), indicating continual learning is necessary when environment/task distributions shift; 3) Environment variation (unseen scenes) and dataset imbalance hurt performance, but balancing/subsampling and CAMA mitigate some effects; 4) Task order and class-sharing among behaviors mediate forward transfer — shared object/action classes across behaviors improve later task learning.",
            "uuid": "e1021.0",
            "source_info": {
                "paper_title": "Online Continual Learning For Interactive Instruction Following Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Behavior-IL",
            "name_full": "Behavior Incremental Learning (Behavior-IL)",
            "brief_description": "A continual-learning setup where the embodied agent receives streams of episodes grouped by behavior type and must learn new behaviors incrementally while preserving prior behaviors; uses online, task-free, disjoint streaming and evaluates retention (last/avg SR and GC).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Same CL-ALFRED instruction-following agent (evaluated under Behavior-IL sequences)",
            "agent_description": "Agent trained with behavior cloning and episodic memory replay; in Behavior-IL the stream is ordered by behavior categories (7 categories) and the agent must learn new behavior types sequentially without task boundary metadata (task-free).",
            "agent_type": "simulated agent (instruction-following in ALFRED/AI2-THOR)",
            "environment_name": "ALFRED episodes across behaviors (EXAMINE, PICK&PLACE, HEAT, COOL, CLEAN, PICK2&PLACE, MOVABLE)",
            "environment_description": "Complexity arises from distinct behavior goal-conditions and the multi-step composition of actions required to complete each behavior; behaviors may share object classes or actions which affects transfer/forgetting.",
            "complexity_measure": "Behavior diversity: 7 discrete behavior types; per-episode multi-step length T; overlap in object usage across behaviors (visualized per-behavior object frequency).",
            "complexity_level": "high (multiple distinct behavior types that require navigation+interaction and can be multi-step)",
            "variation_measure": "Variation arises from different episodes per behavior, differing object instances and scene placements; training uses multiple random orders of behavior sequences (5 random permutations) to avoid bias from specific orders.",
            "variation_level": "medium (behavior categories fixed but episodes within a behavior vary in layout, objects and instructions)",
            "performance_metric": "SR_last, GC_last, SR_avg, GC_avg reported on validation seen & unseen splits.",
            "performance_value": "CAMA (Behavior-IL) — Validation Seen: SR_last 30.71 ± 0.78%, GC_last 40.85 ± 0.73%, SR_avg 29.67 ± 2.66%, GC_avg 38.17 ± 2.34%. Validation Unseen: SR_last 13.64 ± 0.94%, GC_last 28.75 ± 0.92%, SR_avg 14.19 ± 1.38%, GC_avg 27.30 ± 1.38%.",
            "complexity_variation_relationship": "Discussed — behavior complexity interacts with variation and task order: behaviors that share objects/actions (e.g., HEAT and COOL) produce positive forward transfer, making subsequent tasks easier; disjoint behavior sequences or behavior changes increase catastrophic forgetting and reduce performance. The paper reports dependency of performance on task order and class-sharing.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Online disjoint continual learning across behavior types (task-free), episodic memory replay (M=500), behavior cloning, and CAMA logit updates; training repeated across 5 random behavior-order seeds.",
            "generalization_tested": true,
            "generalization_results": "Agents evaluated on seen and unseen scenes; Behavior-IL performance is noticeably lower on unseen scenes (~13–14% SR_last) compared to seen scenes (~30% SR_last), indicating limited generalization but relative improvement over baselines with CAMA.",
            "sample_efficiency": "Per-sequence training across ALFRED episodes; exact per-behavior episode counts follow ALFRED splits and the experiments use three random seeds; episodic memory size 500.",
            "key_findings": "1) Behavior-IL highlights the importance of preserving prior behaviors while learning new ones; 2) CAMA improves retention vs baselines in this setup; 3) Task-order and shared object/action classes materially affect transfer and forgetting — learning related behaviors earlier helps future performance.",
            "uuid": "e1021.1",
            "source_info": {
                "paper_title": "Online Continual Learning For Interactive Instruction Following Agents",
                "publication_date_yy_mm": "2024-03"
            }
        },
        {
            "name_short": "Environment-IL",
            "name_full": "Environment Incremental Learning (Environment-IL)",
            "brief_description": "A continual-learning setup where the agent sequentially encounters different environment types (e.g., KITCHENS, LIVINGROOMS, BEDROOMS, BATHROOMS) and must learn to perform behaviors across expanding scene distributions while retaining competence in previously learned scene types.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Same CL-ALFRED instruction-following agent (evaluated under Environment-IL sequences)",
            "agent_description": "Agent trained with behavior cloning and episodic memory replay; in Environment-IL the stream is ordered by environment type and the agent must adapt to new scene layouts/visual domains (task-free, online).",
            "agent_type": "simulated agent (AI2-THOR / ALFRED)",
            "environment_name": "ALFRED / AI2-THOR environment types (KITCHENS, LIVINGROOMS, BEDROOMS, BATHROOMS)",
            "environment_description": "Each environment type has multiple layout/appearance variations (30 variations per type). Complexity comes from scene layouts, object placements, and visual domain shifts across environment types; dataset imbalance across environment types is present and addressed by subsampling.",
            "complexity_measure": "Number of environment types (4), number of instances per type (30 variations), imbalance in episode counts per type before subsampling (e.g., Kitchens much larger than others), and seen/unseen splits to test domain shift.",
            "complexity_level": "high (navigation + interaction across multiple distinct scene types with many per-type instantiations)",
            "variation_measure": "Per-type instance count (30), seen vs unseen scene splits, and subsampled balanced train set (3,141 episodes per env type after subsampling; total train 12,564 episodes); also an explicit 'imbalanced Environment-IL' experiment without subsampling.",
            "variation_level": "high (30 variations per environment type; explicit unseen environments evaluation)",
            "performance_metric": "SR_last, GC_last, SR_avg, GC_avg on validation seen and unseen; main focus on unseen SR as generalization metric.",
            "performance_value": "CAMA (Environment-IL) — Validation Seen: SR_last 29.48 ± 0.27%, GC_last 38.13 ± 0.85%, SR_avg 35.09 ± 1.92%, GC_avg 44.02 ± 2.21%. Validation Unseen: SR_last 14.60 ± 0.43%, GC_last 30.99 ± 0.75%, SR_avg 15.67 ± 0.77%, GC_avg 33.40 ± 1.45%. In imbalanced Environment-IL, CAMA still outperforms baselines by noticeable margins (detailed table in paper).",
            "complexity_variation_relationship": "Yes — the paper explains that increased environment variation (many scene instances and unseen splits) yields lower generalization performance and can bias models toward majority environments when data are imbalanced; balancing episodes across environment types mitigates bias. They also show CAMA is robust under imbalance and improves retention across environment shifts.",
            "high_complexity_low_variation_performance": null,
            "low_complexity_high_variation_performance": null,
            "high_complexity_high_variation_performance": null,
            "low_complexity_low_variation_performance": null,
            "training_strategy": "Online, task-free incremental exposure to environment types (disjoint), subsampling to balance per-type episodes (when used), episodic memory replay (M=500), behavior cloning, and CAMA logit updates; experiments use five random environment-order sequences.",
            "generalization_tested": true,
            "generalization_results": "Explicit seen vs unseen evaluation: performance drops on unseen environment scenes (Validation Unseen SR_last ~14.6% vs Validation Seen SR_last ~29.5% for CAMA). CAMA outperforms prior methods in both seen and unseen settings, and remains effective in an imbalanced training regime.",
            "sample_efficiency": "Training used subsampled train split (3,141 episodes per env type → total 12,564 training episodes) when balancing; episodic memory size M=500 (~2.38% of original ALFRED training episodes) and single-pass online stream.",
            "key_findings": "1) Environment variation (many instances and unseen splits) is a major source of generalization difficulty; 2) Subsampling to balance environment types reduces bias but CAMA also shows robustness to imbalance; 3) CAMA's confidence-aware logit updating improves retention and adaptation across environment shifts relative to baselines.",
            "uuid": "e1021.2",
            "source_info": {
                "paper_title": "Online Continual Learning For Interactive Instruction Following Agents",
                "publication_date_yy_mm": "2024-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks",
            "rating": 2,
            "sanitized_title": "alfred_a_benchmark_for_interpreting_grounded_instructions_for_everyday_tasks"
        },
        {
            "paper_title": "AI2-THOR: An interactive 3D environment for visual AI",
            "rating": 2,
            "sanitized_title": "ai2thor_an_interactive_3d_environment_for_visual_ai"
        },
        {
            "paper_title": "CORA: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents",
            "rating": 2,
            "sanitized_title": "cora_benchmarks_baselines_and_metrics_as_a_platform_for_continual_reinforcement_learning_agents"
        },
        {
            "paper_title": "Dark Experience for General Continual Learning: a strong, simple baseline",
            "rating": 1,
            "sanitized_title": "dark_experience_for_general_continual_learning_a_strong_simple_baseline"
        },
        {
            "paper_title": "Class-incremental continual learning into the extended DER-verse",
            "rating": 1,
            "sanitized_title": "classincremental_continual_learning_into_the_extended_derverse"
        }
    ],
    "cost": 0.01691,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>ONLINE CONTINUAL LEARNING FOR INTERACTIVE INSTRUCTION FOLLOWING AGENTS
13 Mar 2024</p>
<p>Byeonghwi Kim byeonghwikim@yonsei.ac.kr 
Yonsei University</p>
<p>Minhyuk Seo 
Yonsei University</p>
<p>Jonghyun Choi jonghyunchoi@snu.ac.kr 
Seoul National University</p>
<p>ONLINE CONTINUAL LEARNING FOR INTERACTIVE INSTRUCTION FOLLOWING AGENTS
13 Mar 20244CBDDB7AB5012323639691D713FDFAF1arXiv:2403.07548v2[cs.AI]
In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning.We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it.To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks.However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available.Here, we propose to update them based on confidence scores without task boundary information during training (i.e., task-free) in a moving average fashion, named Confidence-Aware Moving Average (CAMA).In the proposed Behavior-IL and Environment-IL setups, our simple CAMA outperforms prior state of the art in our empirical validations by noticeable margins.The project page including codes is https://github.com/snumprlab/cl-alfred.</p>
<p>INTRODUCTION</p>
<p>Recent advances in computer vision, natural language processing, and embodied AI have led to various benchmarks for robotic agents, encompassing navigation (Savva et al., 2019;Deitke et al., 2020;Anderson et al., 2018;Krantz et al., 2020), object interaction (Zhu et al., 2017;Misra et al., 2017;Weihs et al., 2021;Ehsani et al., 2021), and interactive reasoning (Das et al., 2018;Gordon et al., 2018).To create more realistic agents, challenging benchmarks (Shridhar et al., 2020;Padmakumar et al., 2022) require all of these tasks to complete complex tasks based on language directives.However, most embodied AI literature assumes that all training data are available from the outset but it may be unrealistic as agents may encounter novel behaviors or environments after deployment.To learn new behaviors and environments, continual learning might be necessary for post-deployment.</p>
<p>To learn new tasks, one may finetune the agents.But the finetuned agents would suffer from catastrophic forgetting that loses previously learned knowledge (McCloskey &amp; Cohen, 1989;Ratcliff, 1990).To mitigate such forgetting, (Powers et al., 2022) introduced a continual reinforcement learning framework that incrementally updates agents for new tasks and evaluates their knowledge of current and past tasks.However, this operates in a simplified task setup of (Shridhar et al., 2020), excluding natural language understanding and object localization.</p>
<p>Taking a step forward to bring the instruction following task to real-world scenarios, we propose two continual learning scenarios for embodied agents: Behavior Incremental Learning (Behavior-IL) and Environment Incremental Learning (Environment-IL) as depicted in Figure 1.In Behavior-IL, the robot learns behaviors incrementally.For example, it may initially learn object movement and subsequently acquire the skill of object heating.In Environment-IL, instead of being limited to specific scenes such as bathrooms, the robot progressively learns to perform behaviors in diverse environments such as kitchens and bedrooms.Figure 1: Proposed two incremental learning setups.In the 'Behavior Incremental' setup, the agent is expected to learn new behaviors while preserving previously learned knowledge.In the 'Environment Incremental' setup, the agent is expected to learn tasks in new environments with the preservation of previously learned knowledge.Note that each image in the figure denotes an expert demonstration (i.e., a sequence of frames with natural language instructions followed by a corresponding sequence of actions and object class labels).</p>
<p>Behavior Incremental Environment Incremental</p>
<p>In the continual learning literature, significant progress (Mai et al., 2022;Biesialska et al., 2020) has been made in addressing continual learning by storing models learned in the previous task of extracting information about past data, requiring a substantial storage cost (Zhou et al., 2022a).To address this, Buzzega et al. (2020); Boschini et al. (2022a) propose to store logits of past models for knowledge distillation, reducing storage costs while maintaining learning efficacy.However, the stored logits may be the underfitted or insufficiently learned solution as the model has not sufficiently trained in the early stage of learning, hindering the effective use of prior knowledge.Moreover, such an update often exploits task boundary information that might not always be available, especially in the cases of streamed data without explicit task boundaries (Shanahan et al., 2021;Koh et al., 2023).</p>
<p>To develop continuously updating embodied agents, we propose to update logits by combining the previously stored logits and the newly obtained ones in the moving average, call 'Confidence-Aware Moving Average' (CAMA).In particular, we dynamically determine the moving average coefficients based on the classification confidence scores inferred by the agents as indicators of the 'quality' of the newly obtained logits (i.e., how much they contain accurate knowledge of the corresponding tasks), as empirically observed that high confidence tends to have high accuracy in Figure 3.</p>
<p>capacity to store all task data (Hayes &amp; Kanan, 2022).On the other hand, online CL involves processing individual or small batches of samples, each of which was used only once for training (Koh et al., 2022;Aljundi et al., 2019b).Considering memory constraints and the continuous arrival of limited data points over time in practical scenarios, we focus on the online CL setup.</p>
<p>Task-free continual learning setups.Approaches to continual learning can be categorized into task-free methods (Aljundi et al., 2019a;Koh et al., 2023;Ye &amp; Bors, 2022;Koh et al., 2022) and task-aware methods (Kirkpatrick et al., 2017;Li &amp; Hoiem, 2017;Wu et al., 2019;Boschini et al., 2022a) based on the use of task boundary information during training (Aljundi et al., 2018b).Several task-aware methods (Li &amp; Hoiem, 2017;Wu et al., 2019) that exploit task boundary information during training distill the knowledge of past tasks from the previously learned model saved in memory.However, in real-world scenarios, it is often impractical to know the task identity of streamed input data (Aljundi et al., 2018b).Therefore, even if tasks are discretely defined (e.g., blurry (Prabhu et al., 2020;Bang et al., 2021), disjoint (Parisi et al., 2019)), the task-free assumption indicates that no task-specific information or identifier is used during training.</p>
<p>Knowledge distillation in online continual learning.Continual learning has made significant progress, employing methods such as replay-based (Wu et al., 2019;Rolnick et al., 2019;Prabhu et al., 2020;Bang et al., 2021;Koh et al., 2022), distillation (Li &amp; Hoiem, 2017;Buzzega et al., 2020;Koh et al., 2023), and regularization (Zenke et al., 2017;Kirkpatrick et al., 2017;Lesort et al., 2019).In particular, distillation-based approaches (Li &amp; Hoiem, 2017;Koh et al., 2023;Boschini et al., 2022b) have been extensively investigated to leverage prior knowledge, but often require substantial memory and additional computation.Memory requirements make them unsuitable for settings with limited memory in edge devices (Zhou et al., 2022a).</p>
<p>To address these issues, (Buzzega et al., 2020;Michieli &amp; Zanuttigh, 2021;Boschini et al., 2022a) propose using logits instead of storing previous models, saving memory and inference overhead.However, the method of storing logits in memory without updating may hinder the current model in distilling outdated past information from stored logits, since the stored logits may represent incomplete learning for past tasks.To tackle this, a recent approach X-DER (Boschini et al., 2022a) updates logits through weighted summation with logits maintained in memory and those from the current model, preventing the previously stored logit from becoming outdated, as the model updates.</p>
<p>However, Boschini et al. (2022a) requires task boundary information during the training process for logit update, making it unsuitable for setups where data arrive in a continuous stream without task boundaries.In contrast, our approach updates logits based on the agent's confidence without task boundaries, making it suitable for more general setups where we do not have such information.</p>
<p>Lifelong learning for robotic agents.Going beyond relatively straightforward task setups such as image classification, a substantial amount of literature has emerged to construct a more realistic agent capable of incremental learning of novel tasks (Lesort et al., 2020) in various aspects including learning strategies (e.g., reinforcement learning (Khetarpal et al., 2018;Wołczyk et al., 2021;Xie &amp; Finn, 2022), imitation learning (Mendez et al., 2018;Gao et al., 2021), etc.) and task formulations (e.g., manipulation (Yang et al., 2022;Liu et al., 2023), walking (Zhou et al., 2022b), etc.).Typically, most of this research has focused mainly on relatively fine-grained manipulation tasks, while the navigation aspect (Krantz et al., 2020;Deitke et al., 2020) has received comparatively less attention.</p>
<p>Concurrently, there is recent literature that delves into the dual aspect of navigation and interaction (Powers et al., 2022;Wang et al., 2023) in 3D interactive environments (Kolve et al., 2017;Fan et al., 2022) to perform more demanding tasks.In this context, agents are required to become proficient in both navigating and interacting with task-related objects.Here, the tasks in our proposed continual learning setups are similar to (Powers et al., 2022) that simplifies the task setup of (Shridhar et al., 2020).While (Powers et al., 2022) excludes natural language understanding and object localization, we include them to train agents to complete the desired tasks in the challenging full-fledged interactive instruction following setup, along with navigation and object interaction.</p>
<p>We review more relevant literature and provide extended related work in Sec.A for space's sake.</p>
<p>CL-ALFRED: CONTINUAL LEARNING SETUPS FOR EMBODIED AGENTS</p>
<p>Continual learning enables agents to adapt to new behaviors and diverse environments after deployment, mitigating the risk of forgetting previously acquired knowledge.To foster active research on mitigating catastrophic forgetting, recent literature (Powers et al., 2022) proposes a benchmark that continuously learns new types of household tasks, but lacks natural language understanding and object localization, potentially limiting the deployability of agents.</p>
<p>To comprehensively address the combined challenges of continuous learning of an agent with natural language understanding and object localization, we use full-fledged interactive instruction following tasks and propose two incremental learning setups, Behavior Incremental Learning (Behavior-IL) and Environment Incremental Learning (Environment-IL).We outline our task formulation and detail these incremental learning setups in the following sections.</p>
<p>TASK FORMULATION</p>
<p>As the ALFRED dataset (Shridhar et al., 2020) requires a comprehensive understanding of natural language and visual environments for intelligent agents, we build our continual benchmark on top of it.The agent is first spawned at a random location in an environment and then given natural language instructions, l, that describe how to accomplish a task.For each time step t, the agent takes as input visual observation v t and predicts an action y a,t and a mask y m,t of an object class y c,t if object interaction is required.Here, we learn a policy parameterized by θ, π θ : x − → y, with input x t , i.e., (v t , l) and output y t , i.e., (y a,t , y m,t ).The goal for the policy π θ is to predict a sequence of actions and object masks to complete the task.Kindly refer to Shridhar et al. (2020) for more details.</p>
<p>Most previous methods (Singh et al., 2021;Pashevich et al., 2021;Min et al., 2022;Kim et al., 2023) for object localization utilize a two-stage approach, separating it into object class prediction and mask generation to enhance object localization.Since mask generation is handled by separate mask generators, however, continual updates of these networks are also required.Unfortunately, continuously updated instance segmentation models (Joseph et al., 2021;Cermelli et al., 2022) often noticeably underperform jointly trained models.Here, we only address class prediction, assuming the availability of object masks, leaving the continual updating of mask generators for future work.</p>
<p>CONTINUAL LEARNING SETUPS</p>
<p>There is significant progress in developing agents that can perform the desired tasks through language directives (Krantz et al., 2020;Shridhar et al., 2020;Padmakumar et al., 2022).It is often confronted with new behaviors and environments after being deployed and are required to learn them while maintaining previously learned knowledge.However, prior methods either presuppose the availability of pre-collected datasets or utilize simplified task configurations (Powers et al., 2022).</p>
<p>To address this limitation, we introduce two continual learning setups: 1) Behavior Incremental Learning (Behavior-IL) to incrementally learn what to do and 2) Environment Incremental Learning (Environment-IL) to incrementally learn where to do, as in Figure 1.In addition, we focus on online CL, which assumes a more realistic scenario where novel data are encountered in a streaming manner (Aljundi et al., 2019a;Koh et al., 2023;2022) rather than assuming an offline CL in which novel data are provided in chunks of tasks (Wu et al., 2019;Saha et al., 2021).More details about the continual setup can be found in Sec.B.1.</p>
<p>BEHAVIOR INCREMENTAL LEARNING</p>
<p>Behaviors described by instructions may exhibit considerable diversity as novel behaviors may arise over time.To address this scenario, we propose the Behavior-IL setup that facilitates the agent's incremental learning of new behaviors while retaining the knowledge obtained from previous tasks.Formally, for a set of behaviors, T , an agent sequentially receives N j training episodes, {s Here, we adopt seven different types of behavior from (Shridhar et al., 2020): EXAMINE, PICK&amp;PLACE, HEAT, COOL, CLEAN, PICK2&amp;PLACE, and MOVABLE.To ensure the adaptability of agents and avoid favoring particular behavior sequences, we train and evaluate agents using multiple randomly ordered behavior sequences.Refer to Sec.B.2 for more details about the sequences.</p>
<p>Confidence-Aware Coefficient Determination 0.9 0.8 0.9 0.7 0.9 0.7 0.8 0.7 0.7 0.9 0.8 0.7 0.6 0.8 0.9</p>
<p>Current Logits</p>
<p>Figure 2: Proposed Confidence-Aware Moving Average (CAMA).'Current Logits' denotes the model's logits obtained from the input samples from the current stream and episodic memory.'Previous Logits' denotes logits stored in episodic memory before an update.Qi denotes a queue that stores ground truth confidence scores acquired from the current logits, y1, y2, ..., for the i th class.To obtain γi, we maintain the recent N confidence scores for the i th class and calculate the mean value of the scores followed by a clip function.</p>
<p>Finally, we use all γi's to class-wisely weight-sum previously stored logits (i.e., 'Previous Logits') and newly obtained logits from the current stream (i.e., 'Current Logits').</p>
<p>ENVIRONMENT INCREMENTAL LEARNING</p>
<p>The Environment-IL setup allows agents to learn the environment incrementally.In the real world, agents often need to perform actions not only in the environment in which they were initially trained but also in new and different environments that are presented.For example, the agent may first learn various actions in a kitchen and then subsequently learn the actions in a bathroom.</p>
<p>Formally, for a set of environments, E, an agent sequentially receives M k training episodes, {s e k i } M k i=1 , for each environment type, e k ∈ E. When receiving the final episode (i.e., s e k M k ) for the current environment type, e k , the agent begins to sequentially receive episodes, {s
e k+1 i } M k+1
i=1 , for the next environment type, e k+1 .This is repeated until it reaches the last environment type, e |E| .</p>
<p>In this setup, we adopt four different types of environments supported by (Kolve et al., 2017): KITCHENS, LIVINGROOMS, BEDROOMS, and BATHROOMS.Like the Behavior-IL setup, we conduct training and evaluation using multiple sequences of randomly ordered environments.We also provide more details of the multiple environment sequences in Sec.B.3.However, we observe an imbalance in the training and evaluation episodes between different types of environment (Shridhar et al., 2020), where a majority of them originate from a specific environment type in many instances.The imbalance can potentially lead to biased learning of a model towards the dominant (i.e., majority) environment type (Chakraborty &amp; Chakraborty, 2020;Zhao et al., 2021a).To address the issue, we balance them by simply subsampling the training and evaluation episodes for each environment to match the number of episodes across environment types equally.1. 'Confidence' denotes the dynamically determined coefficients (here, γa) for the update in Equation 2.</p>
<p>APPROACH</p>
<p>To mitigate catastrophic forgetting, recent approaches (Li &amp; Hoiem, 2017;Koh et al., 2023;Boschini et al., 2022b) use knowledge distillation with the trained model until past tasks, but this often entails significant memory (Zhou et al., 2022a) and computational overhead caused by additional model inference (Prabhu et al., 2023).Due to the limitations in memory and computation on edge devices (Lee et al., 2022), logit distillation methods (Buzzega et al., 2020;Boschini et al., 2022a) have been proposed as alternatives to those that store entire models for distillation (Li &amp; Hoiem, 2017;Koh et al., 2023), offering improved memory and computation efficiency.Despite such improved efficiency, some of the logit distillation methods (Buzzega et al., 2020) often face an outdated logit problem, as the memory-stored logits are not updated to preserve information on previous tasks.</p>
<p>To address this issue, a recent approach (Boschini et al., 2022a) attempts to partially update logits stored in the past using the current model.It uses task boundary information (e.g., input's task identity) during training, but it may not always be available, especially in task-free CL setups including our proposed ones.Towards a general logit-update framework devoid of such information, we update the stored logits using the agent's confidence scores indicating how the newly obtained logits for update contain accurate knowledge of the corresponding tasks, as observed in Figure 3.</p>
<p>CONFIDENCE-AWARE MOVING AVERAGE</p>
<p>As illustrated in Figure 2, the overall process of Confidence-Aware Moving Average (CAMA) can be summarized as follows: Initially, exploiting the model's confidence scores of ground-truth labels, we evaluate the extent to which the model has acquired proficiency in the current samples.Subsequently, during the computation of the updated logits based on the previous and current logits, we allocate a higher weight to the current logits when exhibiting higher confidence scores, and conversely, assign a higher weight to the previous logits demonstrating lower confidence scores.For better understanding, we outline the high-level flow of our CAMA in Algorithm 1 in the appendix.</p>
<p>Following the common practice (Kirkpatrick et al., 2017;Rolnick et al., 2019;Buzzega et al., 2020), we construct an input batch, [x; x ′ ], by combining data from both the training data stream (x, y a , y c ) ∼ D and the episodic memory
(x ′ , y ′ a , y ′ c , z ′ old,a , z ′ old,c ) ∼ M
, where each a ∈ A and c ∈ C indicates an action and object class label from the action and object class sets, A and C, present in the input batch, [x; x ′ ].Here, x represents the input (i.e., images and language directives), y a and y c denote the corresponding action and object class labels, and z ′ old,a and z ′ old,c refers to the corresponding stored logits.z a , z c , z ′ a , and z ′ c denote the current model's logits for the input batch.To prevent the logits maintained in the episodic memory from becoming outdated, we obtain the updated logits, z ′ new,a and z ′ new,c , by weighted-summing z ′ old,a and z ′ old,c with z ′ a and z ′ c using coefficient vectors, γ a and γ c , using Hadamard product, denoted by ⊙, as in Equation 1:
z ′ new,a = (1 − γ a ) ⊙ z ′ old,a + γ a ⊙ z ′ a , z ′ new,c = (1 − γ c ) ⊙ z ′ new,c + γ c ⊙ z ′ c .(1)
To obtain γ a and γ c , we first maintain the most recent N confidence scores for each action and object class label for x.Then, to approximate the agent's proficiency in learning tasks over time, we compute the average of the scores associated with each action (i) and object class (j) label, denoted by sa i and sc j .We then set each element of γ a and γ c , denoted by γ a,i and γ c,j , to sa i and sc j followed by a CLIP function as in Equation 2:
γ a,i = α a CLIP sa i − |A| −1 , 0, 1 , γ c,j = α c CLIP sc j − |C| −1 , 0, 1 ,(2)
where CLIP(x, min, max) denotes the clip function that limits the value of x from min to max.</p>
<p>Here, the constants α a &lt; 1 and α c &lt; 1 are introduced to prevent γ a,i and γ c,j from reaching a value of 1 as this indicates complete replacement of the prior logits with the current logits, which implies that the updated logits would entirely forget the previously learned knowledge.The inclusion of these constants ensures that some information from the past is retained and not entirely overridden by the current logits during the update process.In addition, we subtract |A| −1 and |C| −1 enhances the effective utilization of confidence scores in comparison to a 'random' selection, which would otherwise be realized by a uniform distribution (Koh et al., 2022).</p>
<p>MODEL TRAINING</p>
<p>Given expert demonstrations, x as input, we train our agent, π θ , by minimizing the objective below:
min θ E (x,y)∼D [L(π θ (x), y)] + E (x,y)∼M [L(π θ (x), y)] + α E (x,z)∼M [||z − π θ (x)|| 2 2 ],(3)
where y denotes the ground-truth labels corresponding to x and z the logits maintained in the episodic memory.We provide more details of the training loss, L, in Sec.D.3 in the appendix.</p>
<p>Evaluation metrics.For evaluation of task completion ability, we follow the same evaluation protocol of (Shridhar et al., 2020).The primary metric is the success rate (SR) which measures the ratio of the succeeded episodes among the total ones.The second metric is the goal-condition success rate (GC) which measures the ratio of the satisfied goal conditions among the total ones.Furthermore, we evaluated all agents in two splits of environments: seen and unseen environments which are/are not used to train agents.We provide more details of the evaluation protocol in Sec.D.1.</p>
<p>To evaluate the last and intermediate performance of continual learning agents, we measure two variations of a metric, A: A last and A avg .A last (here, SR last and GC last ) indicates the metric achieved by the agent that finishes its training for the final task.A avg (here, SR avg and GC avg ) indicates the average of the metrics achieved by the agents that finish their training for each incremental task.</p>
<p>All the models are trained sequentially over a sequence of behaviors (Behavior-IL) and environments (Environment-IL) and then evaluated over the behaviors and environments that the models have learned so far.For evaluation, we use episodes different from those used for training.The same trained models are evaluated in both seen and unseen environments.For seen and unseen, we denote by seen the evaluation with the episodes generated from scenes used in training, while we denote by unseen the evaluation with the episodes generated from scenes not used in training.</p>
<p>Baselines.We compare our CAMA with competitive prior arts in continual learning literature: CLIB (Koh et al., 2022), DER++ (Buzzega et al., 2020), ER (Rolnick et al., 2019), MIR (Aljundi et al., 2019a), EWC (Kirkpatrick et al., 2017), and X-DER (Boschini et al., 2022a).In addition, we also compare our CAMA with two models: 'Joint' and 'Finetuning'.'Joint' denotes that the agent is trained with all task data jointly, which works as an upper bound.'Finetuning' denotes that the agent is fine-tuned for the new tasks or scene types, which can serve as one of the trivial solutions for continual setups.We provide further explanation for each baseline in Sec.D.2.We further detail the model architecture and training used for the methods above in Sec.D.3 for space's sake.</p>
<p>Implementation details.It is a common practice in continual learning literature (Bang et al., 2021;Koh et al., 2022;2023) to set the size of episodic memory to less than 5%.To align with previous works in continual learning, we set the size of the episodic memory to M = 500 for expert demonstrations, which is approximately 2.38% of the training episodes in the ALFRED benchmark (Shridhar et al., 2020).For our CAMA, we empirically set α a = 0.99 and α c = 0.99.We provide more implementation details such as hyperparameters in Sec.D.4 for space's sake.</p>
<p>COMPARISON WITH STATE OF THE ART</p>
<p>We present the quantitative results of our CAMA in Table 1-2.As mentioned in Sec.3.2, we train and evaluate our CAMA and baselines for three random seeds and report the results with their average and standard deviation to avoid favoring particular behavior and environment sequences.</p>
<p>We provide quantitative analyses in various aspects as follows.</p>
<p>Joint training vs. Finetuning.Before investigating the effectiveness of our CAMA, we first investigate how challenging the proposed Behavior-IL and Environment-IL setups are.We observe significant performance drops in 'Finetuning' compared to 'Joint' with 51.0% and 47.5% relative drops.This implies that simply finetuning agents to novel behaviors and environments cannot effectively address the forgetting caused by distribution shifts between behaviors and environments.</p>
<p>Ours vs. Regularization-based model.We observe that our CAMA achieves better performance than the regularization-based approach (i.e., EWC++) with noticeable margins in both seen and unseen environments for all metrics and setups, indicating that regularizing changes in importance parameters may not effectively prevent forgetting than distilling knowledge from updated logits.</p>
<p>Ours vs. Rehearsal-based models.We observe that our CAMA outperforms all rehearsal-based approaches (i.e., ER, MIR, and CLIB) for all metrics in both seen and unseen environments in both Behavior-IL and Environment-IL setups.We believe that this implies that solely depending on sample replay amidst rapid data distribution shifts can result in insufficient task forgetting mitigation and hinder the agent's ability to adapt to novel tasks, ultimately impeding effective task completion.</p>
<p>Ours vs. Distillation-based models.We compare our CAMA with the distillation-based approaches (i.e., DER and X-DER) to investigate the effectiveness of our logit-update approach.First, we observe noticeable performance drops in DER, which does not update logits, compared to our CAMA for all metrics in seen and unseen environments in both Behavior-IL and Environment-IL setups, which highlights the importance of updating logits to prevent them from being outdated.</p>
<p>In addition, we observe that our CAMA outperforms X-DER, which partially updates logits only for novel classes, with noticeable margins for all metrics and environments in both the Behavior-IL and Environment-IL setups, highlighting the efficacy of our CAMA.We note that while X-DER updates logits based on task boundary information during training, our CAMA does not assume the availability of such information (i.e., task-free), which highlights the generality of our CAMA.</p>
<p>THE EFFECTIVENESS OF DYNAMICALLY DETERMINED COEFFICIENTS</p>
<p>We investigate the effect of dynamically determined coefficients of our CAMA by fixing them with a constant value and provide the results ('CAMA' vs. 'CAMA w/o D.C.' in Table 1-2).'CAMA w/o D.C.' assumes that the agent is always 100% confident in what it learns.Consequently, we directly set γ a and γ c to α a and α c by omitting the process of dynamically determined coefficients.</p>
<p>We observe that the ablation of dynamically determined coefficients consistently yields performance drops in all metrics in seen and unseen environments in both Behavior-IL and Environment-IL setups, indicating the importance of the process of finding such coefficients.This could be attributed to the fact that while logit updating with a constant coefficient helps mitigate the obsolescence of the logits to some extent, it also combines them with logits from the current model that lacks sufficient training for novel tasks, particularly during the initial phase of learning these tasks.Consequently, this can lead to performance degradation due to incomplete knowledge of these new tasks.</p>
<p>QUALITATIVE ANALYSIS</p>
<p>We provide qualitative examples of our CAMA in each Behavior-IL and Environment-IL setup by comparison with the naïve (i.e., Finetuning) and prior best-performing (DER++) methods.For space's sake, we provide the qualitative example in the Environment-IL setup in Sec.D.5.</p>
<p>Finetuning DER++ CAMA (Ours)</p>
<p>Instructions:</p>
<p>Turn around to your right move forward then turn left, head to the coffee maker.Pick up the mug in front of the coffee maker on the counter.Turn to your left head to the microwave above the stove.Open the microwave then put in and out the mug then close the microwave.Turn to your right and head to the coffee maker.Put the mug on the coffee maker.</p>
<p>PICK2&amp;PLACE HEAT Interacted Object Irrelevant Action</p>
<p>Figure 4: Qualitative analysis of the proposed method (Behavior-IL).The agent, having already acquired knowledge of the behavior τj−1 = HEAT, proceeds to learn the new behavior τj = PICK2&amp;PLACE.Subsequently, we evaluate the agent's ability of the prior behavior τj−1 to determine if any forgetting has occurred.Irrelevant Action denotes an action that results in incorrect navigation.'Finetuning' fails to find a target object, 'Mug,' and eventually fails at the task.DER++ succeeds in navigating to and picking up the mug but fails to reach a microwave above the agent, also leading to task failure.On the contrary, our CAMA further succeeds in reaching the microwave, heating the mug, and putting it back on the coffee machine, leading to task success.</p>
<p>The Behavior-IL setup.In Figure 4, the agent is evaluated for the previous behavior, τ j−1 = HEAT while learning the current behavior, τ j = PICK2&amp;PLACE.Here, the agent is required to heat a mug and put it on the coffee machine.The agent can sequentially complete the task by 1) picking up a mug, 2) heating it using a microwave, and 3) putting it on the coffee machine.</p>
<p>'Finetuning' first explores the environment to find a mug.However, it fails to recognize the mug and therefore keeps wandering in the environment, eventually leading to task failure.Meanwhile, 'DER++' succeeds in finding the mug and picking it up but forgets how to reach a microwave above the agent to heat an object.The agent also keeps wandering in the environment and eventually, it fails at the task.In contrast, our CAMA succeeds in navigating to and picking up the mug.After grabbing the mug, our agent finds, reaches the microwave above, and successfully heats the mug.Finally, our agent then put the heated mug on the coffee machine, as described in the instructions, which implies that our CAMA enables the agent to maintain the knowledge of the previous behaviors.</p>
<p>CONCLUSION</p>
<p>We propose two continual learning setups that learn new behaviors (Behavior Incremental Learning, Behavior-IL) and environments (Environment Incremental Learning, Environment-IL) continually.Prior methods employ the storage of model logits from previous tasks but they are updated either only once or upon obtaining new logits, potentially resulting in learning with outdated data or utilizing logits from a model that has incompletely learned the new tasks.</p>
<p>To effectively update the logits, we propose Confidence-Aware Moving Average (CAMA), a simple yet effective approach that dynamically determines moving average coefficients based on the agent's confidence scores.We observe that the CAMA outperforms all prior arts by noticeable margins.</p>
<p>Limitation and future work.While the disjoint setup operates under the assumption that tasks in streaming data are non-overlapping (Parisi et al., 2019), posing a stringent test for catastrophic forgetting, such non-overlapping scenarios might not always be the case in real-world scenarios.To address this aspect, extending our proposed setups to feature overlapped tasks in streaming data, such as blurry setups (Prabhu et al., 2020;Bang et al., 2021) or Gaussian scheduled regimes (Shanahan et al., 2021;Wang et al., 2022), represents a promising avenue for future research.</p>
<p>ETHICS STATEMENT</p>
<p>This work introduces continual learning setups for interactive instruction following agents and a logit-update approach to enhance the effectiveness of knowledge distillation.While the authors do not aim for this, the increasing adoption of deep learning models in real-world contexts with streaming data could potentially raise concerns such as privacy and model robustness.There is a possibility of these deployed deep learning models inadvertently introducing biases or discrimination, as unresolved issues like model bias persist within deep learning.We are committed to implementing all feasible precautions to avert such consequences, as they are unequivocally contrary to our intentions.</p>
<p>REPRODUCIBILITY STATEMENT</p>
<p>We take reproducibility in deep learning very seriously and highlight some of the contents of the manuscript that might help to reproduce our work.We release the data splits of the proposed benchmarks in Sec. 3, our implementation of the proposed method in Sec. 4, and the baselines used in our experiments in Sec. 5 in https://github.com/snumprlab/cl-alfred.</p>
<p>APPENDIX A EXTENDED RELATED WORK</p>
<p>Continual learning setup.We can categorize continual learning setups into Task Incremental (task-IL) and Class Incremental (class-IL) depending on whether task ID is given during inference.In task-IL, task-ID is provided during inference (Aljundi et al., 2018a;Lopez-Paz &amp; Ranzato, 2017;Hossain et al., 2022), while in class-IL, task ID is not given (Koh et al., 2022;Buzzega et al., 2020;Koh et al., 2023;Bang et al., 2021).As task-ID is not provided during inference in the real world, class-IL is closer to the real setup and is more challenging because it requires classification across all classes.We focused on class-IL to deal with more realistic situations.</p>
<p>Continual learning methods.As the need for continual learning is increasingly highlighted across various fields, researchers have proposed a wide array of continual learning methods to prevent catastrophic forgetting.(1) Replay-Based Methods (Prabhu et al., 2020;Koh et al., 2022;Bang et al., 2021;Wu et al., 2019;Rolnick et al., 2019) store some stream data in episodic memory and replaying memory data during the future learning process to prevent forgetting about previous tasks.</p>
<p>(2) Distillation-Based Methods (Buzzega et al., 2020;Koh et al., 2023;Li &amp; Hoiem, 2017) retain knowledge about past data by storing logits (Buzzega et al., 2020) or models (Koh et al., 2023;Li &amp; Hoiem, 2017) to distill knowledge.(3) Regularization-Based Method (Zenke et al., 2017;Kirkpatrick et al., 2017;Lesort et al., 2019) prevents the overwriting of important parameters by imposing a penalty on changes to these crucial parameters.</p>
<p>Recently, continual learning has also been actively investigated in more challenging task setups such as video domains (Zhao et al., 2021b;Villa et al., 2022;Park et al., 2021).The focus of their work is generally on classification problems such as action recognition in a video by observing full video frames as given.In contrast, rather than receiving predetermined frames at once, the next observations (frames) of agents are determined by the actions that the agents take, which then require the agents to plan subsequent actions to complete tasks based on the next observations.Embodied AI.Embodied AI (EAI) has garnered substantial attention, and notable advancements have been made in various tasks (Anderson et al., 2018;Krantz et al., 2020;Jain et al., 2019;Savva et al., 2019;Deitke et al., 2020;Weihs et al., 2021;Ehsani et al., 2021;Shridhar et al., 2020;Padmakumar et al., 2022;Gao et al., 2022).For instance, visual navigation tasks necessitate that the agent use visual observation to reach designated locations (Savva et al., 2019) or objects (Savva et al., 2019;Deitke et al., 2020).Meanwhile, vision-language navigation (VLN) (Anderson et al., 2018;Jain et al., 2019;Krantz et al., 2020) augments visual observation with natural language descriptions, enabling the agent to plan a sequence of actions based on a comprehensive understanding of multiple modalities to successfully reach the target locations.</p>
<p>Furthermore, the scope of EAI tasks has expanded through the inclusion of object interaction.(Weihs et al., 2021) necessitates the agent to relocate objects to their original state by manipulating them, while (Ehsani et al., 2021) requires the agent to move objects to designated locations using six degrees of freedom (6-DoF) manipulation.(Shridhar et al., 2020) presents natural language descriptions, which the agent must comprehend to plan a sequence of actions and utilize predictive 2D object classes to locate objects for interaction.Meanwhile, (Padmakumar et al., 2022;Gao et al., 2022) provide natural language dialogues, in which the agent must engage in reasoning to determine the appropriate course of action and complete the tasks at hand.However, the agents evaluated in those benchmarks are typically trained using pre-existing datasets.Given that the data collection process can be both expensive and time-consuming, it may not always be feasible to pre-collect the requisite dataset, implying the need for continual learning for the agents.</p>
<p>B ADDITIONAL CL-ALFRED BENCHMARK DETAILS B.1 CONTINUAL LEARNING SETUPS</p>
<p>In alignment with the common practice in the prior arts (Aljundi et al., 2019a;Buzzega et al., 2020;Shim et al., 2021), we assume that both setups follow an online and disjoint paradigm.In an online setup, individual samples (here, expert demonstrations) are presented sequentially rather than being available simultaneously.While a portion of the data is retained in episodic memory, the streaming data is accessible for learning only once.In a disjoint setup, each task (here, each behavior and environment type) contains distinct and unrelated information from the others.In this setup, as the agent embarks on learning new tasks, it does not receive any samples from previously encountered tasks.Importantly, we do not rely on predefined task boundaries for training and evaluation.</p>
<p>B.2 BEHAVIOR INCREMENTAL LEARNING</p>
<p>In the ALFRED benchmark, episodes comprise seven distinct behavior types: EXAMINE, HEAT, PICK&amp;PLACE, COOL, CLEAN, PICK2&amp;PLACE, and MOVABLE.Each behavior type presents distinct goal conditions that the agents must fulfill by learning how to achieve them.In the Behavior-IL setup, we employ the original validation split of the ALFRED benchmark for validation and five randomly ordered sequences of behavior types for training as follows.
1. EXAMINE − → HEAT − → PICK2&amp;PLACE − → COOL − → PICK&amp;PLACE − → CLEAN − → MOVABLE 2. PICK&amp;PLACE − → PICK2&amp;PLACE − → CLEAN − → HEAT − → EXAMINE − → MOVABLE − → COOL 3. PICK&amp;PLACE − → EXAMINE − → MOVABLE − → CLEAN − → PICK2&amp;PLACE − → COOL − → HEAT 4. MOVABLE − → PICK2&amp;PLACE − → EXAMINE − → PICK&amp;PLACE − → HEAT − → COOL − → CLEAN 5. CLEAN − → PICK&amp;PLACE − → MOVABLE − → HEAT − → COOL − → PICK2&amp;PLACE − → EXAMINE B.3 ENVIRONMENT INCREMENTAL LEARNING
Episodes in the ALFRED benchmark are generated from four distinct environment types supported by AI2-THOR (Kolve et al., 2017): KITCHENS, LIVINGROOMS, BEDROOMS, and BATHROOMS.</p>
<p>Each environment type has 30 variations of the environment type that enable agents to learn behaviors in diverse room layouts and visual appearances.Similar to the Behavior-IL setup, for training, Environment-IL also employs five randomly ordered sequences of environment types as follows.
1. BEDROOMS − → BATHROOMS − → LIVINGROOMS − → KITCHENS 2. BATHROOMS − → BEDROOMS − → KITCHENS − → LIVINGROOMS 3. BEDROOMS − → LIVINGROOMS − → BATHROOMS − → KITCHENS 4. BEDROOMS − → BATHROOMS − → KITCHENS − → LIVINGROOMS 5. BATHROOMS − → KITCHENS − → BEDROOMS − → LIVINGROOMS
As discussed in Sec.3.2.2,we observe an imbalance in the 'train' and 'validation' splits of the original ALFRED across the environment types: for each KITCHENS, LIVINGROOMS, BEDROOMS,and BATHROOMS,11,056,3,456,3,370,and 3,141 episodes for the 'train' split and 432, 129, 106, and 153 for the 'validation' seen split, and 468, 146, 120, and 87 for the 'validation' unseen split.</p>
<p>To balance them, we subsample the train and validation episodes per environment type as follows.</p>
<p>For the 'train' split, we subsample 3, 141 episodes, leading to 12, 564 episodes in total.For the 'validation' seen split, we subsample 106 episodes, leading to 424 episodes in total.Finally, for the 'validation' unseen split, we subsample 87 episodes, leading to 348 episodes in total.</p>
<p>C EXTENDED APPROACH</p>
<p>As described in Sec.4.1, the high-level procedure of our CAMA is described in Algorithm 1.When new samples, denoted as x (representing expert demonstrations), are received, we retrieve samples, denoted by x ′ , from episodic memory.Subsequently, we obtain the respective logits, denoted by z and z ′ , from the model, denoted by π θ .With these logits, we compute the gradient of the joint loss, which combines cross entropy and knowledge distillation, to update the model parameters θ.</p>
<p>After updating θ, we maintain a record of the N most recent confidence scores in separate queues, denoted by Q, for each action and object class for x.Once these recent scores are maintained, we dynamically calculate the coefficients, γ a and γ c , to weight-sum the previous and current logits, denoted by z ′ old and z ′ , leading to the updated logits, denoted by z ′ new, which are then stored in episodic memory.For more details on the γ a and γ c calculations, kindly refer to Sec. 4.1.</p>
<p>D.3 MODEL ARCHITECTURE AND TRAINING</p>
<p>For model architecture, we adopt a recently proposed learning-based agent (Kim et al., 2021) that perceives the surrounding views and predicts a sequence of actions and object masks using factorized branches (Singh et al., 2021).Following the common practice of prior arts (Shridhar et al., 2020;Singh et al., 2021;Pashevich et al., 2021;Nguyen et al., 2021;Kim et al., 2021), we train our agent with imitation learning, specifically behavior cloning.We detail the architecture and training below.</p>
<p>Model Architecture.The architecture of our CAMA and the baselines is based on a recent approach (Singh et al., 2021) that uses separate modules for effective action prediction and object localization to better address different semantic understandings from each other.Specifically, let y t = f (x t ) be the agent that maps the input x t = (v t , l, y a,t−1 ) to the output y t = (y a,t , y c,t ).</p>
<p>For the input, each v t and l denotes the RGB images (i.e., surrounding views) and step-by-step instructions.For the output, each y a and y c denotes the action and object class to be interacted with.</p>
<p>As mentioned above, the agent f is comprised of two separate modules: the action prediction module a t = f action (v t , l, y a,t−1 ) and the object localization module c t = f class (v t , l, y a,t−1 ).Both modules encode the instructions l with a self-attention-based Bi-LSTM network, resulting in the attended language feature, l.To capture the correspondence between visual and textual information, we conduct point-wise convolution for v t by filters generated from l, resulting in the attended visual feature vt .The decoder of each module updates its hidden state based on the attended visual and textual features, vt and l, with the previous action y a,t−1 , resulting in h a t and h c t for the action prediction module and the object localization module.Finally, fully connected layers in f action take as input vt , l, y a,t−1 , and h a t and predict the current action y a,t .Similarly, fully connected layers in f class take as input h c t and predict the current object class y c,t with which to interact.For more details of the modules, kindly refer to (Singh et al., 2021).</p>
<p>Training.Following (Shridhar et al., 2020), we adopt imitation learning for training, specifically behavior cloning, that mimics an expert's behaviors in a teacher-forcing manner.Formally, let a and a * be a sequence of predicted actions and the corresponding ground-truth actions.Similarly, let c and c * be a sequence of predicted object classes to be interacted with and the corresponding labels.Then each loss of the action and object class prediction is obtained by a cross-entropy loss as below:
L action (a, a * ) = − T t=1 a * t log a t , L class (c, c * ) = − T t=1 1[a * t = interaction] • c * t log c t ,(4)
where T denotes the length of an episode that the agent conducts and 1[a * t = interaction] is an indicator function that activates when an action a * t is an object interaction action.In addition, (Ma et al., 2019) adopts progress monitoring.Formally, let p and p * be a sequence of predicted progress values and the corresponding ground-truth progress values.Then the progress loss is obtained by a mean square error (MSE) loss as below:
L progress (p, p * ) = 1 T T t=1 (p t − p * t ) 2 . (5)
Using them, the agent jointly minimizes the joint loss as follows:
L(y, y * ) = λ a L action (y a , y * a ) + λ c L class (y c , y * c ) + λ p L progress (y p , y * p ),(6)
where y indicates the output of a model including an action sequence, y a , a class sequence, y c , and a progress value sequence, y p , for an auxiliary task.y * a , y * c , and y * p denote the corresponding ground-truth labels.The loss terms are summed by the balancing coefficients λ a , λ c , and λ p .</p>
<p>D.4 IMPLEMENTATION DETAILS</p>
<p>For visual observation, inspired by (Nguyen et al., 2021;Kim et al., 2021;Bhambri et al., 2023), we allow the agent to perceive surrounding views (in this case, 5 views from the front, left, right, up, and down directions).For language instructions, the agents receive step-by-step instructions that describe how to accomplish the goal in detail.</p>
<p>Finetuning DER++</p>
<p>CAMA (Ours)</p>
<p>Instructions:</p>
<p>Head to the right side of the desk.Take the CD from the desk.Take a step to the right.Turn on the lamp.</p>
<p>BATHROOMS BEDROOMS Interacted Object Irrelevant Action</p>
<p>Figure 5: Qualitative analysis of the proposed method (Environment-IL).The agent that has already acquired knowledge of the environment e k−1 = BEDROOMS proceeds to learn the new environment e k = BATHROOMS.We then assess the agent's capability in the prior environment e k−1 to determine whether any forgetting has occurred.Irrelevant Action denotes an action that results in incorrect navigation.'Finetuning' fails to find a target object, 'CD,' eventually leading to task failure.DER++ can navigate to and pick up the CD, but fails to turn on the lamp.On the contrary, our CAMA can also turn on the lamp and complete the task.</p>
<p>For the training loss described in Section D.3, we set the balancing coefficients λ a = 1.0, λ c = 1.0, and λ p = 1.0 for our CAMA and the baselines.Following (Singh et al., 2021), we augment visual observations by adopting two strategies: AutoAugment (Cubuk et al., 2019) and RGB-channel swapping.For computational efficiency, we cache 6 types of augmented episodes per episode and choose one of them whenever we augment it.</p>
<p>To update the parameters of our CAMA and the baselines, we use the Adam optimizer with an initial learning rate of 0.001 and a batch size of 32 per streamed sample.We utilize the ExponentalLR (Li &amp; Arora, 2019) and ResetLR (Loshchilov &amp; Hutter, 2016) schedulers with γ = 0.95 and m = 10 for our CAMA and the baselines except CLIB with γ = 0.9999.</p>
<p>D.5 QUALITATIVE ANALYSIS</p>
<p>We provide a qualitative example of our CAMA in the Environment-IL setup by comparison with the naïve (i.e., Finetuning) and prior best-performing (DER++) methods.</p>
<p>The Environment-IL setup.In Figure 5, the agent is evaluated for the previous environment, e k−1 = BEDROOMS, while learning the current environment, e k = BATHROOMS.The agent is required to examine a CD under the light of a lamp.To complete the task, the agent needs to 1) pick up a CD and 2) turn on a lamp while holding the CD.</p>
<p>Similarly in the Behavior-IL setup, 'Finetuning' cannot find a CD and therefore navigates to other objects irrelevant to the task, which eventually leads to task failure.On the other hand, 'DER++' successfully picks up a CD and reaches the lamp in the close vicinity.However, the agent forgets to turn on the lamp and therefore starts to navigate to other irrelevant objects, which also leads to task failure.In contrast, our CAMA can pick up the CD and navigate to the lamp as 'DER++' does.Our agent then turns on the light to examine the held CD and succeeds in the task.</p>
<p>D.6 IMBALANCED SCENARIOS IN THE ENVIRONMENT-IL SETUP</p>
<p>To explore a data-imbalance scenario (Liu et al., 2022;He et al., 2023), we remove the subsampling process in Sec.3.2.2 for the Environment-IL setup and construct an imbalanced dataset that we name the imbalanced Environment-IL setup.In the imbalanced Environment-IL setup, we compare our CAMA with the baselines and summarize the result in Table 3.</p>
<p>We observe that even with such an imbalance, CAMA still outperforms the baselines by noticeable margins, highlighting the efficacy of the proposed approach.In particular, we observe significant improvements of SR avg and GC avg in both valid seen and unseen splits, implying that CAMA achieves promising performance in both partial and full task completion.The frequency of objects used for each behavior in the Behavior-IL setup.Each x-axis and y-axis denotes the index of an object and the object's frequency appearing in the corresponding behavior.While the behaviors, HEAT and COOL, have several shared objects (e.g., apples, tomatoes, etc.) during task completion, the behavior, EXAMINE, rarely have them with HEAT and COOL.</p>
<p>E DISCUSSION</p>
<p>E.1 CONFIDENCE SCORES AS A GOOD INDICATOR OF NEW LOGITS' QUALITY</p>
<p>We use the averaged class-wise confidence score as an indicator to estimate how much new logits are informative.This is because using the confidence scores for the ground truths, which we use for logit update, allows us to estimate how well the model has learned respective classes.</p>
<p>For example, If the model p predicts p(i) = 1 for the class i, it implies that the model has learned the class i well, i.e., it may contain ample information about the class i.Conversely, if the model predicts p(i) = 0, it implies that the model has learned the class i poorly, i.e., it may contain little information about the class i (Koh et al., 2023).</p>
<p>We can use the most single recent confidence score as the 'indicator,' but such a single confidence score could be noisy during training for various reasons such as the degree of augmentation and the difficulty of a sample.To alleviate this issue, we use the N recent confidence scores as an indicator of the quality of the new logits.</p>
<p>E.2 DEPENDENCY OF PERFORMANCE ON A TASK ORDER IN INCREMENTAL SETUPS</p>
<p>We agree that the performance improvements seem relatively marginal, possibly due to the large standard error of the means.We believe this is because, in an incremental setup in embodied tasks, some tasks may share relatively many action and object classes, while others may share fewer classes.Previously learning such shared action and object classes may help better learning the current task (i.e., forward transfer) and this implies that a model's performance may depend on the order of the tasks (i.e., how much the model learns the shared action and object classes ahead).</p>
<p>For example, while learning to cool an object, learning some actions (e.g., opening/closing a fridge) and object classes (e.g., apples, tomatoes, etc.) may help next learn to heat an object as such actions and object classes can also be used for heating (e.g., heat an 'apple,' a 'tomato,' etc. by 'opening/closing' a microwave).We empirically observe that for the behavior, 'Heat,' our agent achieves 3.70% Valid Unseen SR after learning the behavior, 'Cool,' while it achieves zero Valid Unseen SR after learning the behavior, 'Examine,' which does have fewer shared object classes as illustrated in Figure 6, implying the dependency of performance on a task order.</p>
<p>each type of behavior, τ j ∈ T .When receiving the final episode (i.e., s τj Nj ) for the current behavior type, τ j , the agent starts to sequentially receive episodes, {s τj+1 i } Nj+1 i=1 , for the next behavior type, τ j+1 .The episode stream ends with the last training episode, s τ |T | N |T | , of the last task type, τ |T | .</p>
<p>Figure 3 :
3
Figure3: Confidence and accuracy of logits used for logit update in CAMA.'Accuracy' denotes the mean of the frame-wise accuracies measured from the newly obtained logits (here, z ′ a ) in Equation1.'Confidence' denotes the dynamically determined coefficients (here, γa) for the update in Equation2.</p>
<p>Figure6: The frequency of objects used for each behavior in the Behavior-IL setup.Each x-axis and y-axis denotes the index of an object and the object's frequency appearing in the corresponding behavior.While the behaviors, HEAT and COOL, have several shared objects (e.g., apples, tomatoes, etc.) during task completion, the behavior, EXAMINE, rarely have them with HEAT and COOL.</p>
<dl>
<dt>mean mean mean ... ... ... Current Logits Logit Update</dt>
<dt>Classes Present in Current Batch:0.9 0.80.90.90.7Confidence Score Queues...0.80.9 0.6 0.7 0.8meanPrevious Logits Updated Logits</dt>
<dd>Confidence Scores ofGround-Truth Labels</dd>
</dl>
<p>Table 1 :
1
Finetuning 9.51 ± 1.09 20.39 ± 0.61 17.07 ± 0.86 26.11 ± 0.95 8.72 ± 2.12 15.56 ± 1.29 16.25 ± 3.95 21.40 ± 4.65 EWC++ 20.37 ± 5.19 29.32 ± 5.92 22.21 ± 4.34 30.97 ± 4.26 26.79 ± 2.24 36.79 ± 1.83 31.01 ± 2.76 40.56 ± 2.22 ER 26.71 ± 1.49 36.59 ± 1.36 27.67 ± 2.08 36.20 ± 1.96 30.28 ± 1.07 39.15 ± 0.83 34.72 ± 1.56 44.00 ± 1.52 MIR 30.27 ± 1.33 40.14 ± 2.00 28.12 ± 1.78 36.76 ± 1.73 27.50 ± 1.48 36.31 ± 1.43 31.81 ± 0.81 40.94 ± 0.95 CLIB 23.85 ± 2.02 34.25 ± 1.81 23.94 ± 2.36 32.65 ± 2.22 25.47 ± 1.42 34.63 ± 1.55 32.51 ± 2.40 41.25 ± 2.34 DER++ 29.15 ± 1.29 39.39 ± 1.16 27.49 ± 2.27 36.10 ± 1.92 28.25 ± 1.18 36.18 ± 0.60 28.68 ± 2.54 38.01 ± 3.16 X-DER 28.76 ± 1.25 38.45 ± 1.18 27.21 ± 2.53 35.88 ± 2.22 28.30 ± 0.87 37.02 ± 0.61 29.32 ± 2.36 39.13 ± 2.59 CAMA w/o D.C. 30.54 ± 1.27 39.92 ± 1.50 29.89 ± 2.32 38.16 ± 2.71 31.55 ± 0.87 39.29 ± 1.03 34.49 ± 2.29 43.28 ± 2.17 CAMA (Ours) 30.71 ± 0.78 40.85 ± 0.73 29.67 ± 2.66 38.17 ± 2.34 29.48 ± 0.27 38.13 ± 0.85 35.09 ± 1.92 44.02 ± 2.21 Comparison with state-of-the-art methods (validation seen).The highest value per metric is in bold.We report the averages and standard deviations of multiple runs for random seeds as in Sec.3.2.
Validation SeenModelBehavior-ILEnvironment-ILSR last ↑GC last ↑SR avg ↑GC avg ↑SR last ↑GC last ↑SR avg ↑GC avg ↑Joint60.47 ± 0.33 65.77 ± 0.78−− 56.25 ± 0.89 62.13 ± 0.84−−Validation UnseenModelBehavior-ILEnvironment-ILSR last ↑GC last ↑SR avg ↑GC avg ↑SR last ↑GC last ↑SR avg ↑GC avg ↑Finetuning1.18 ± 1.09 12.09 ± 1.653.03 ± 1.29 13.95 ± 1.012.01 ± 0.86 11.20 ± 1.922.90 ± 2.16 13.53 ± 4.33EWC++8.50 ± 2.15 21.63 ± 3.608.33 ± 1.33 20.71 ± 1.99 11.61 ± 1.29 28.47 ± 0.83 12.37 ± 1.30 29.90 ± 1.30ER9.43 ± 1.25 24.22 ± 1.549.47 ± 1.51 22.79 ± 1.39 11.44 ± 1.36 29.11 ± 0.96 14.25 ± 1.47 31.98 ± 1.64MIR11.01 ± 1.16 25.31 ± 1.14 10.88 ± 1.51 24.20 ± 1.45 12.01 ± 0.61 29.67 ± 0.47 12.58 ± 0.80 29.11 ± 1.26CLIB8.26 ± 1.03 22.00 ± 1.318.56 ± 0.66 21.03 ± 1.15 10.46 ± 1.18 27.40 ± 0.78 11.95 ± 1.51 29.93 ± 2.05DER++13.16 ± 5.56 28.70 ± 5.63 10.60 ± 4.04 24.94 ± 2.69 10.29 ± 1.05 26.90 ± 1.31 10.25 ± 1.72 26.83 ± 1.97X-DER12.59 ± 1.92 28.10 ± 2.05 12.04 ± 1.56 25.50 ± 1.48 10.75 ± 1.15 28.37 ± 1.05 11.14 ± 1.38 28.56 ± 1.44
CAMA w/o D.C. 14.06 ± 1.20 28.33 ± 1.58 12.52 ± 1.46 26.02 ± 1.38 13.57 ± 1.25 29.54 ± 1.41 12.78 ± 0.57 29.76 ± 0.84 CAMA (Ours) 13.64 ± 0.94 28.75 ± 0.92 14.19 ± 1.38 27.30 ± 1.38 14.60 ± 0.43 30.99 ± 0.75 15.67 ± 0.77 33.40 ± 1.45 Joint 24.60 ± 0.96 38.24 ± 1.55 − − 19.73 ± 2.31 39.02 ± 0.53 − − Table 2: Comparison with state-of-the-art methods (validation unseen).The highest value per metric is in bold.We report averages and standard deviations of multiple runs for random seeds as in Sec.3.2.</p>
<p>GC last ↑ SR avg ↑ GC avg ↑ SR last ↑ GC last ↑ SR avg ↑ GC avg ↑ EWC 27.58 ± 2.03 38.58 ± 2.40 35.44 ± 1.42 45.14 ± 1.58 9.76 ± 0.70 22.94 ± 0.61 12.32 ± 1.03 28.06 ± 1.31 ER 32.22 ± 1.98 43.03 ± 1.93 38.87 ± 0.29 49.06 ± 0.83 11.80 ± 0.86 26.43 ± 0.84 13.43 ± 0.61 30.75 ± 0.52 MIR 27.58 ± 2.03 38.58 ± 2.40 35.44 ± 1.42 45.14 ± 1.58 9.76 ± 0.70 22.94 ± 0.61 12.32 ± 1.03 28.06 ± 1.31 CLIB 24.70 ± 2.20 35.21 ± 2.78 35.32 ± 1.77 44.55 ± 1.78 8.24 ± 1.14 21.96 ± 1.11 10.97 ± 1.50 28.03 ± 1.11 DER++ 31.54 ± 1.42 42.99 ± 1.61 33.01 ± 1.97 43.89 ± 2.21 12.20 ± 0.69 27.28 ± 1.23 11.90 ± 1.46 28.33 ± 1.74 X-DER 31.10 ± 1.10 42.83 ± 1.13 32.96 ± 1.70 43.81 ± 1.64 12.93 ± 0.51 27.44 ± 0.97 12.49 ± 0.95 29.07 ± 1.30 CAMA w/o D.C. 35.40 ± 1.34 46.55 ± 1.40 39.54 ± 1.30 49.27 ± 1.33 14.19 ± 1.22 29.11 ± 1.42 16.02 ± 1.20 33.16 ± 1.25 CAMA (Ours) 32.32 ± 1.62 43.72 ± 1.81 38.67 ± 2.07 49.09 ± 1.90 14.53 ± 0.40 28.75 ± 0.92 17.24 ± 1.78 33.36 ± 1.47
Imbalanced Environment-ILModelValid SeenValid UnseenSR last ↑</p>
<p>Table 3 :
3
Comparison with state-of-the-art methods in the imbalanced Environment-IL setup.The highest value per metric is in bold.We report the means and standard errors of multiple runs for random seeds.</p>
<p>Contributions.We summarize our contributions as follows:• We propose behavior incremental (Behavior-IL) and environment incremental (Environment-IL) setups for online continual learning for interactive instruction following agents.• We propose Confidence-Aware Moving Average (CAMA) that dynamically determines coefficients for logit update to prevent logits from being outdated for effective knowledge distillation.• Our proposed method outperforms comparable methods in most metrics with noticeable margins.ACKNOWLEDGMENT This work was partly supported by the NRF grant (No.2022R1A2C4002300, 15%) and IITP grants (No.2020-0-01361 (10%, Yonsei AI), No.2021-0-01343 (5%, SNU AI), No.2022-0-00077 (10%), No.2022-0-00113 (20%), No.2022-0-00959 (15%), No.2022-0-00871 (15%), No.2021-0-02068 (5%, AI Innov.Hub), No.2022-0-00951 (5%)) funded by the Korea government (MSIT).// Obtain logits from the modelD EXTENDED EXPERIMENT RESULTSD.1 EVALUATION METRICSFor training and evaluation, the ALFRED dataset(Shridhar et al., 2020) consists of three splits; 'train,' 'validation,' and 'test.'Agents are trained with the 'train' split and validate their approaches in the 'validation' split with the ground-truth information of the tasks in those splits.The agents are then evaluated in the 'validation' and 'test' split, but they do not have any access to the ground-truth information of the tasks.As the ground-truth labels of the 'test' split are not publicly available, we evaluate our agent and baselines and report the results in the validation split.The validation split also consists of two folds: seen and unseen environments in which agents are/are not trained.Seen environments allow evaluating the task completion ability of agents in the same visual domain as training environments.Unseen environments further allow evaluating agents' task completion ability in different visual domains from training environments, which is considered more challenging than seen environments.For the evaluation of task completion ability, the primary metric is the success rate (SR) which measures the ratio of completed tasks, indicating the task completion ability of the agent.Another metric is the goal-condition success rate (GC) which measures the ratio of satisfied goal conditions, indicating the partial task completion ability of the agent.We evaluate all agents' performance in terms of SR and GC in both seen and unseen environments and the main metric is unseen SR.(Shridhar et al., 2020)also penalizes SR and GC with path-length-weighted (PLW) scores proportional to trajectory lengths.Considering the model(Kim et al., 2021)used in our experiments lags significantly behind human performance in terms of task completion ability (i.e., unseen success rates), however, we focus mainly on task completion ability and leave the examination of efficiency aspects for future investigations.D.2 BASELINES CLIB(Koh et al., 2022) is a method that maintains an optimal episodic memory based on the importance of each sample.DER++(Buzzega et al., 2020)aims to distill information about past data by storing not only images and labels but also logits, comparing them with the logits of the current model.ER(Rolnick et al., 2019)constructs the training batch with half of the current task stream and the other half of the data in memory, to remember past data while learning about a new task.MIR(Aljundi et al., 2019a)uses samples that received the most interference from previous learning, rather than randomly retrieving from memory when composing the training batch.EWC++(Kirkpatrick et al., 2017)prevents forgetting from parameter overwriting by penalizing changes of important parameters.X-DER(Boschini et al., 2022a)rewrites logits for the portions corresponding to the classes of past tasks to incorporate newly acquired experience information about past tasks while learning new tasks.Following the prior methods(Singh et al., 2021;Pashevich et al., 2021;Nguyen et al., 2021)that keep visual encoders frozen, our agent's visual encoder also remains frozen, and thus we omit the contrastive learning part(Chen et al., 2020)in X-DER.
Memory aware synapses: Learning what (not) to forget. Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, Tinne Tuytelaars, ECCV. 2018a</p>
<p>Rahaf Aljundi, Klaas Kelchtermans, Tinne Tuytelaars, arXiv:1812.03596Task-free continual learning. 2018b</p>
<p>Online continual learning with maximally interfered retrieval. Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin, Laurent Charlin, Tinne Tuytelaars, NeurIPS2019a</p>
<p>Gradient based sample selection for online continual learning. Rahaf Aljundi, Min Lin, Baptiste Goujaud, Yoshua Bengio, NeurIPS2019b</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, CVPR. 2018</p>
<p>Rainbow memory: Continual learning with a memory of diverse samples. Jihwan Bang, Heesu Kim, Youngjoon Yoo, Jung-Woo Ha, Jonghyun Choi, CVPR. 2021</p>
<p>Multi-level compositional reasoning for interactive instruction following. Suvvansh Bhambri, Byeonghwi Kim, Jonghyun Choi, AAAI. 2023</p>
<p>Magdalena Biesialska, Katarzyna Biesialska, Marta R Costa-Jussa , arXiv:2012.09823Continual lifelong learning in natural language processing: A survey. 2020</p>
<p>Classincremental continual learning into the extended der-verse. Matteo Boschini, Lorenzo Bonicelli, Pietro Buzzega, Angelo Porrello, Simone Calderara, IEEE TPAMI. 2022a</p>
<p>Transfer without forgetting. Matteo Boschini, Lorenzo Bonicelli, Angelo Porrello, Giovanni Bellitto, Matteo Pennisi, Simone Palazzo, Concetto Spampinato, Simone Calderara, ECCV. 2022b</p>
<p>Dark experience for general continual learning: a strong, simple baseline. Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, Simone Calderara, NeurIPS2020</p>
<p>Modeling missing annotations for incremental learning in object detection. Fabio Cermelli, Antonino Geraci, Dario Fontanel, Barbara Caputo, CVPR. 2022</p>
<p>Superensemble classifier for improving predictions in imbalanced datasets. Tanujit Chakraborty, Ashis Kumar, Chakraborty , Communications in Statistics: Case Studies, Data Analysis and Applications. 2020</p>
<p>Riemannian walk for incremental learning: Understanding forgetting and intransigence. Arslan Chaudhry, Thalaiyasingam Puneet K Dokania, Philip Ajanthan, Torr Hs, ECCV. 2018</p>
<p>A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, ICML. 2020</p>
<p>Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data. Ekin Dogus Cubuk, Barret Zoph, CVPR. 2019</p>
<p>Embodied question answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, CVPR. 2018</p>
<p>Robothor: An open simulation-to-real embodied ai platform. Matt Deitke, Winson Han, Alvaro Herrasti, Aniruddha Kembhavi, Eric Kolve, Roozbeh Mottaghi, Jordi Salvador, Dustin Schwenk, Eli Vanderbilt, Matthew Wallingford, Luca Weihs, Mark Yatskar, Ali Farhadi, CVPR. 2020</p>
<p>Manipulathor: A framework for visual object manipulation. Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli Vanderbilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi, CVPR. 2021</p>
<p>De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, NeurIPS. 2022</p>
<p>Cril: Continual robot imitation learning via generative and prediction model. Chongkai Gao, Haichuan Gao, Shangqi Guo, Tianren Zhang, Feng Chen, IROS. 2021</p>
<p>Dialfred: Dialogue-enabled agents for embodied instruction following. Xiaofeng Gao, Qiaozi Gao, Ran Gong, Kaixiang Lin, Govind Thattai, Gaurav S Sukhatme, RA-L. 2022</p>
<p>Iqa: Visual question answering in interactive environments. Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, Ali Farhadi, CVPR. 2018</p>
<p>Online continual learning for embedded devices. L Tyler, Christopher Hayes, Kanan, arXiv:2203.106812022</p>
<p>Long-tailed continual learning for visual food recognition. Jiangpeng He, Luotao Lin, Jack Ma, Heather A Eicher-Miller, Fengqing Zhu, arXiv:2307.001832023arXiv preprint</p>
<p>Rethinking task-incremental learning baselines. Md Sazzad Hossain, Pritom Saha, Townim Faisal Chowdhury, Shafin Rahman, Fuad Rahman, Nabeel Mohammed, ICPR. 2022</p>
<p>Stay on the path: Instruction fidelity in vision-and-language navigation. Vihan Jain, Gabriel Magalhaes, Alexander Ku, Ashish Vaswani, Eugene Ie, Jason Baldridge, ACL. 2019</p>
<p>Incremental object detection via meta-learning. Kj, Jathushan Joseph, Salman Rajasegaran, Fahad Khan, Shahbaz Khan, Vineeth N Balasubramanian, IEEE TPAMI. 2021</p>
<p>Khimya Khetarpal, Shagun Sodhani, arXiv:1811.10732Sarath Chandar, and Doina Precup. Environments for lifelong reinforcement learning. 2018</p>
<p>Agent with the big picture: Perceiving surroundings for interactive instruction following. Byeonghwi Kim, Suvaansh Bhambri, Pratap Kunal, Roozbeh Singh, Jonghyun Mottaghi, Choi, Embodied AI Workshop @ CVPR. 2021</p>
<p>Context-aware planning and environment-aware memory for instruction following embodied agents. Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, Jonghyun Choi, ICCV. 2023</p>
<p>Overcoming catastrophic forgetting in neural networks. James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. 2017PNAS</p>
<p>Online continual learning on class incremental blurry task configuration with anytime inference. Hyunseo Koh, Dahyun Kim, Jung-Woo Ha, Jonghyun Choi, ICLR2022</p>
<p>Online boundary-free continual learning by scheduled data prior. Hyunseo Koh, Minhyuk Seo, Jihwan Bang, Hwanjun Song, Deokki Hong, Seulki Park, Jung-Woo Ha, Jonghyun Choi, ICLR2023</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, arXiv:1712.05474Ai2-thor: An interactive 3d environment for visual ai. 2017</p>
<p>Beyond the nav-graph: Vision-and-language navigation in continuous environments. Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, Stefan Lee, ECCV. 2020</p>
<p>Carm: hierarchical episodic memory for continual learning. Soobee Lee, Minindu Weerakoon, Jonghyun Choi, Minjia Zhang, Di Wang, Myeongjae Jeon, DAC. 2022</p>
<p>Regularization shortcomings for continual learning. Timothée Lesort, Andrei Stoian, David Filliat, arXiv:1912.030492019</p>
<p>Continual learning for robotics: Definition, framework, learning strategies, opportunities and challenges. Timothée Lesort, Vincenzo Lomonaco, Andrei Stoian, Davide Maltoni, David Filliat, Natalia Díaz-Rodríguez, 2020In Information fusion</p>
<p>An exponential learning rate schedule for deep learning. Zhiyuan Li, Sanjeev Arora, arXiv:1910.074542019</p>
<p>Learning without forgetting. Zhizhong Li, Derek Hoiem, IEEE TPAMI. 2017</p>
<p>Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, Peter Stone, arXiv:2306.03310Libero: Benchmarking knowledge transfer for lifelong robot learning. 2023</p>
<p>Longtailed class incremental learning. Xialei Liu, Yu-Song Hu, Xu-Sheng Cao, Andrew D Bagdanov, Ke Li, Ming-Ming Cheng, ECCV. 2022</p>
<p>Gradient episodic memory for continual learning. David Lopez, - Paz, Marc'aurelio Ranzato, NeurIPS. 2017</p>
<p>Sgdr: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, arXiv:1608.039832016</p>
<p>Self-monitoring navigation agent via auxiliary progress estimation. Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan Alregib, Zsolt Kira, Richard Socher, Caiming Xiong, ICLR2019</p>
<p>Online continual learning in image classification: An empirical survey. Zheda Mai, Ruiwen Li, Jihwan Jeong, David Quispe, Hyunwoo Kim, Scott Sanner, Neurocomputing. 2022</p>
<p>Catastrophic interference in connectionist networks: The sequential learning problem. Michael Mccloskey, Neal J Cohen, Psychology of learning and motivation. 1989</p>
<p>Lifelong inverse reinforcement learning. Jorge Mendez, Shashank Shivkumar, Eric Eaton, NeurIPS. 2018</p>
<p>Knowledge distillation for incremental learning in semantic segmentation. Umberto Michieli, Pietro Zanuttigh, 2021CVIU</p>
<p>Yonatan Bisk, and Ruslan Salakhutdinov. Film: Following instructions in language with modular methods. So Yeon Min, Devendra Singh Chaplot, Pradeep Ravikumar, ICLR2022</p>
<p>Mapping instructions and visual observations to actions with reinforcement learning. Dipendra Misra, John Langford, Yoav Artzi, EMNLP. 2017</p>
<p>Look wide and interpret twice: Improving performance on interactive instruction-following tasks. Masanori Van-Quang Nguyen, Takayuki Suganuma, Okatani, IJCAI. 2021</p>
<p>Teach: Task-driven embodied agents that chat. Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, Dilek Hakkani-Tur, AAAI. 2022</p>
<p>Continual lifelong learning with neural networks: A review. Ronald German I Parisi, Jose L Kemker, Christopher Part, Stefan Kanan, Wermter, Neural networks. 2019</p>
<p>Class-incremental learning for action recognition in videos. Jaeyoo Park, Minsoo Kang, Bohyung Han, ICCV. 2021</p>
<p>Episodic transformer for vision-andlanguage navigation. Alexander Pashevich, Cordelia Schmid, Chen Sun, ICCV. 2021</p>
<p>Cora: Benchmarks, baselines, and metrics as a platform for continual reinforcement learning agents. Sam Powers, Eliot Xing, Eric Kolve, Roozbeh Mottaghi, Abhinav Gupta, CoLLAs2022</p>
<p>Gdumb: A simple approach that questions our progress in continual learning. Ameya Prabhu, Puneet K Philip Hs Torr, Dokania, ECCV. 2020</p>
<p>Computationally budgeted continual learning: What does matter. Ameya Prabhu, Abed Hasan, Kader Al, Hammoud, K Puneet, Dokania, Ser-Nam Philip Hs Torr, Bernard Lim, Adel Ghanem, Bibi, CVPR. 2023</p>
<p>Connectionist models of recognition memory: constraints imposed by learning and forgetting functions. Roger Ratcliff, Psychological review. 1990</p>
<p>icarl: Incremental classifier and representation learning. Sylvestre-Alvise, Alexander Rebuffi, Georg Kolesnikov, Christoph H Sperl, Lampert, CVPR. 2017</p>
<p>Experience replay for continual learning. David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, Gregory Wayne, NeurIPS2019</p>
<p>Gradient projection memory for continual learning. Gobinda Saha, Isha Garg, Kaushik Roy, arXiv:2103.097622021</p>
<p>Habitat: A Platform for Embodied AI Research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra, ICCV. 2019</p>
<p>Murray Shanahan, arXiv:2105.13327Christos Kaplanis, and Jovana Mitrović. Encoders and ensembles for task-free continual learning. 2021</p>
<p>Online class-incremental continual learning with adversarial shapley value. Dongsub Shim, Zheda Mai, Jihwan Jeong, Scott Sanner, Hyunwoo Kim, Jongseong Jang, AAAI. 2021</p>
<p>Alfred: A benchmark for interpreting grounded instructions for everyday tasks. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox, CVPR. 2020</p>
<p>Factorizing perception and policy for interactive instruction following. Pratap Kunal, Suvaansh Singh, Byeonghwi Bhambri, Roozbeh Kim, Jonghyun Mottaghi, Choi, ICCV. 2021</p>
<p>vclimb: A novel video class incremental learning benchmark. Andrés Villa, Kumail Alhamoud, Victor Escorcia, Fabian Caba, Juan León Alcázar, Bernard Ghanem, CVPR. 2022</p>
<p>Voyager: An open-ended embodied agent with large language models. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, Anima Anandkumar, arXiv:2305.162912023</p>
<p>Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, CVPR. 2022</p>
<p>Visual room rearrangement. Luca Weihs, Matt Deitke, Aniruddha Kembhavi, Roozbeh Mottaghi, CVPR. 2021</p>
<p>Continual world: A robotic benchmark for continual reinforcement learning. Maciej Wołczyk, Michał Zaj Ąc, Razvan Pascanu, Łukasz Kuciński, Piotr Miłoś, NeurIPS2021</p>
<p>Large scale incremental learning. Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, Yun Fu, CVPR. 2019</p>
<p>Lifelong robotic reinforcement learning by retaining experiences. Annie Xie, Chelsea Finn, CoLLAs. 2022</p>
<p>Evaluations of the gap between supervised and reinforcement lifelong learning on robotic manipulation tasks. Fan Yang, Chao Yang, Huaping Liu, Fuchun Sun, CoRL, 2022. Fei Ye and Adrian G Bors. 2022NeurIPS</p>
<p>Continual learning through synaptic intelligence. Friedemann Zenke, Ben Poole, Surya Ganguli, ICML. 2017</p>
<p>Energy aligning for biased models. Bowen Zhao, Chen Chen, Qi Ju, Shutao Xia, arXiv:2106.033432021a</p>
<p>When video classification meets incremental classes. Hanbin Zhao, Xin Qin, Shihao Su, Yongjian Fu, Zibo Lin, Xi Li, ACM MM. 2021b</p>
<p>Da-Wei Zhou, Qi-Wei Wang, Han-Jia Ye, De-Chuan Zhan, arXiv:2205.13218A model or 603 exemplars: Towards memory-efficient class-incremental learning. 2022a</p>
<p>Forgetting and imbalance in robot lifelong learning with off-policy data. Wenxuan Zhou, Steven Bohez, Jan Humplik, Nicolas Heess, Abbas Abdolmaleki, Dushyant Rao, Markus Wulfmeier, Tuomas Haarnoja, CoLLAs2022b</p>
<p>Visual semantic planning using deep successor representations. Yuke Zhu, Daniel Gordon, Eric Kolve, Dieter Fox, Li Fei-Fei, Abhinav Gupta, Roozbeh Mottaghi, Ali Farhadi, ICCV. 2017</p>            </div>
        </div>

    </div>
</body>
</html>
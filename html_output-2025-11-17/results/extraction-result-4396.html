<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4396 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4396</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4396</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-273185991</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2410.03761v2.pdf" target="_blank">Taxonomy Tree Generation from Citation Graph</a></p>
                <p><strong>Paper Abstract:</strong> Constructing taxonomies from citation graphs is essential for organizing scientific knowledge, facilitating literature reviews, and identifying emerging research trends. However, manual taxonomy construction is labor-intensive, time-consuming, and prone to human biases, often overlooking pivotal but less-cited papers. In this paper, to enable automatic hierarchical taxonomy generation from citation graphs, we propose HiGTL (Hierarchical Graph Taxonomy Learning), a novel end-to-end framework guided by human-provided instructions or preferred topics. Specifically, we propose a hierarchical citation graph clustering method that recursively groups related papers based on both textual content and citation structure, ensuring semantically meaningful and structurally coherent clusters. Additionally, we develop a novel taxonomy node verbalization strategy that iteratively generates central concepts for each cluster, leveraging a pre-trained large language model (LLM) to maintain semantic consistency across hierarchical levels. To further enhance performance, we design a joint optimization framework that fine-tunes both the clustering and concept generation modules, aligning structural accuracy with the quality of generated taxonomies. Extensive experiments demonstrate that HiGTL effectively produces coherent, high-quality taxonomies.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4396.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4396.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiGTL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Graph Taxonomy Learning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An end-to-end framework that constructs hierarchical taxonomies from citation graphs by jointly performing hierarchical graph clustering (GNN-based) and taxonomy-node verbalization via an LLM, with a two-phase optimization (pretrain clustering, then LoRA fine-tune LLM).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HiGTL (Hierarchical Graph Taxonomy Learning)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>HiGTL first encodes papers with a graph neural network (GAT) to produce node embeddings and performs recursive hierarchical citation-graph clustering (soft clustering at base level, hard clustering higher up) to produce hyper-nodes. Cluster features are aggregated (average + representative node) and projected into the LLM embedding space via an MLP projector (Φ). A pre-trained LLM Θ (used as taxonomy node verbalizer) consumes concatenated embeddings of cluster-level graph tokens, selected paper-level text embeddings, and user instruction tokens to iteratively generate concise central-concept labels for clusters in a bottom-up fashion. The pipeline is trained in two phases: pretrain the clustering module using paper-level labels and then jointly fine-tune clustering + LLM verbalizer using LoRA (parameter-efficient fine-tuning). Retrieval (graph-aware BM25/SentenceBert variants) is used upstream to select candidate papers for a given topic.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Llama-2-7b-hf (used as taxonomy node verbalizer, fine-tuned via LoRA); in evaluation/ablation GPT-4o and Claude-3.5 are used as baselines (not as internal module in the main HiGTL pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Hierarchical graph-aware retrieval (BM25 with neighbor-info) + GNN-based graph encoding (GAT) to produce cluster embeddings; paper texts encoded via LLM text embedder; MLP projector maps graph embeddings to LLM token space.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Iterative bottom-up hierarchical summarization / concept abstraction: cluster-level embeddings + selected base-paper text embeddings + instruction prompt are concatenated and fed into an LLM to generate central-concept labels for clusters; hierarchical likelihood objective enforces consistency across levels.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Evaluated on 518 citation-graph instances (each a literature-review's citation network); retrieval phase uses top-200 retrieved papers; citation graphs average ~6,658 papers and ~11,633 edges (per merged networks described).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature (literature-review citation graphs across CS subdomains).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Hierarchical taxonomies (taxonomy trees) with verbalized node labels (central concepts); can be used to guide literature-review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LLM-Score (coverage, structure, relevance), human evaluation (Adequacy binary, Validity 1–5), BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>HiGTL: Coverage 0.9357 ±0.012, Structure 0.9413 ±0.010, Relevance 0.8748 ±0.013, LLM Average 0.9173; Human Adequacy 0.7150, Human Validity 2.6700; BERTScore 0.8694 ±0.010 (Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to GPT-4o and Claude-3.5 (LLM baselines), and taxonomy-learning methods HiExpan, TaxoGen, NetTaxo.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HiGTL outperforms all baselines across metrics (e.g., LLM Avg 0.9173 vs GPT-4o 0.8958 and NetTaxo 0.8654; Coverage/Structure/Relevance improvements reported in Table 1).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Jointly combining hierarchical graph clustering (structural signal) with LLM-based verbalization (textual abstraction) yields taxonomies that are both structurally coherent and semantically consistent; pretraining the clustering module before fine-tuning the LLM stabilizes training; graph-aware retrieval (BM25 with neighbor info) reduces noise and improves downstream quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Fine-grained distinctions can be less precise than human-crafted taxonomies; limited labeled hierarchies make full LLM fine-tuning infeasible (necessitates LoRA); potential for LLM hallucinations in node verbalization is acknowledged; end-to-end joint training is optimization-challenging and unstable if not staged.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Evaluated across 1-hop to 3-hop citation graphs and merged large networks; graph-aware retrieval (200 docs) maintains stable performance even in large networks; model uses parameter-efficient fine-tuning (LoRA) to keep LLM adaptation tractable; explicit scaling with larger LLM sizes not explored in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxonomy Tree Generation from Citation Graph', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4396.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4396.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HiReview</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Hierarchical Taxonomy-Driven Automatic Literature Review Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A downstream system that uses HiGTL-generated taxonomies plus graph-context-aware retrieval to guide an LLM in generating structured literature reviews with high coverage, structure, and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>HiReview (Taxonomy-then-Generation literature review system)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>HiReview first retrieves a graph-aware subset of papers (graph context-aware retrieval, typically 200 papers) and constructs a hierarchical taxonomy using HiGTL. The taxonomy tree provides hierarchical context and topic delimiters; the LLM backbone (GPT-4o in the described experiments) then generates literature-review sections guided by taxonomy nodes and retrieved paper content. The model uses the taxonomy to drive outline/section generation rather than flat retrieval-then-generate approaches. Fine-tuning of the LLM backbone for taxonomy verbalization uses LoRA; the final content generation uses GPT-4o as the generator in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (content generator backbone in HiReview experiments); Llama-2-7b-hf (LLaMA) fine-tuned via LoRA is used for taxonomy verbalization in HiGTL; Claude-3.5 used as baseline judge/evaluator in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Graph-context-aware retrieval (BM25 with neighbor info preferred) to pick candidate papers; GNN-based hierarchical clustering produces clusters; selected paper abstracts/full-text are encoded and passed to the LLM as context.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Taxonomy-guided generation: use hierarchical taxonomy as scaffold to generate structured review sections; LLM composes summaries per taxonomy node and assembles them into a structured review (taxonomy-then-generation pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>System evaluated on 518 review instances; retrieval component typically returns 200 papers per review; in baselines some LLMs were provided top-500 via BM25 for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Full literature reviews (structured sections), and taxonomy trees as intermediate artifact.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LLM-Score (coverage, structure, relevance), human evaluation (compared to human-written reviews), BERTScore.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>HiReview: Coverage 0.9163 ±0.03, Structure 0.9484 ±0.02, Relevance 0.9428 ±0.01, Average 0.9358, BERTScore 0.8449 ±0.02 (Table 2). HiReview outperforms pure LLMs, naive RAG, and AutoSurvey on these metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Pure LLMs (GPT-4o, Claude-3.5 zero-shot), naive RAG-based LLMs (BM25 retrieval + same LLM backbone), and AutoSurvey (structured retrieval + outline generation pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>HiReview achieves higher scores than AutoSurvey and naive RAG: e.g., HiReview average 0.9358 vs AutoSurvey 0.8957, and HiReview Structure 0.9484 vs AutoSurvey 0.9122 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a hierarchical taxonomy as an explicit scaffold for LLM generation produces more stable, higher-structure and higher-relevance literature reviews than flat retrieval-then-generate or outline-then-generate baselines; graph-aware retrieval (200 docs) reduces noise and improves generation quality and stability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Ablations show retrieval and clustering critically affect review quality—removing retrieval or taxonomy degrades performance severely; reliance on high-quality retrieval and taxonomy quality; pure LLMs without fine-tuning are less stable; potential hallucination remains an issue.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Graph-aware retrieval keeps retrieval size small (200) and stable across larger citation networks (tested up to merged 2-hop networks), reducing need to scale retrieval to thousands of papers; does not explore scaling to much larger LLM backbones in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxonomy Tree Generation from Citation Graph', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4396.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4396.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AutoSurvey: Large Language Models Can Automatically Write Surveys</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prior system (referenced) that generates literature reviews by combining retrieval, outline generation, and section drafting using LLMs; used as a baseline in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>AutoSurvey: Large Language Models Can Automatically Write Surveys.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AutoSurvey</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>AutoSurvey follows a retrieve-then-outline-then-generate pipeline: retrieve candidate papers (large retrieval sets, e.g., 1200), generate an outline, and draft sections using an LLM (GPT-4o in comparisons). It emphasizes prompt design and multi-output generation to reduce instability.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o used as LLM backbone in cited comparison experiments (per paper references).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Dense/sparse retrieval (large sets), BM25 or dense retrievers were used in the original AutoSurvey pipeline (paper references).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Outline-then-generation: generate outline structure from retrieved content, then generate sections guided by outline.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>AutoSurvey in referenced work retrieves large candidate sets (e.g., ~1200) to avoid missing relevant content.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General automated literature review generation (used here on computer science reviews as baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated literature reviews (structured).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same LLM-Score metrics (coverage, structure, relevance) and BERTScore as used in this paper for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Reported in this paper: AutoSurvey average LLMScore 0.8957, Coverage 0.8646 ±0.07, Structure 0.9122 ±0.05, Relevance 0.9093 ±0.04, BERTScore 0.8256 ±0.02 (Table 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared against HiReview, naive RAG LLMs, and pure LLMs in literature-review generation experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>AutoSurvey performs better than naive/zero-shot LLMs but is outperformed by HiReview (e.g., HiReview average 0.9358 vs AutoSurvey 0.8957).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Outline-then-generation and careful prompt design improve quality over naive LLM pipelines, but taxonomy-guided generation (HiReview) yields superior structure and relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Needs large retrieval sets (incurs noise and computation); less effective at leveraging graph-structure context than taxonomy-driven approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Relies on scaling retrieval size (e.g., 1200) to maintain recall; this introduces computational cost and noise tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxonomy Tree Generation from Citation Graph', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4396.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4396.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o (as used)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o (OpenAI generative pre-trained transformer variant used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A strong commercial LLM used both as a content generator for literature review generation and as a baseline for taxonomy generation in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o (used as baseline/generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Used in experiments as a zero-shot/prompted generative backbone to (a) generate taxonomies directly in baseline comparisons, (b) serve as the content generator for review generation pipelines (HiReview uses GPT-4o for content generation in experiments), and (c) as an LLM-based clustering baseline in ablations.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o-2024-05-13 (content generator) per implementation details; in some baseline conditions provided with top-500 BM25 retrieved papers.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Zero-shot generation from its internal knowledge; or used with BM25 retrieval (top-500) as naive RAG baseline; in clustering baseline used as a clustering LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct generation or RAG-augmented generation; for clustering baseline used LLM-based clustering heuristics (details not elaborated).</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>In baseline experiments some LLMs were given top-500 retrieved papers; HiReview uses GPT-4o with top-200 retrieved papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Used across computer-science literature-review experiments in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated taxonomy labels (in baselines) and literature-review content.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LLM-Score (coverage/structure/relevance), human evaluation, BERTScore in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>As baseline: GPT-4o taxonomy generation: Coverage 0.9132 ±0.015, Structure 0.8914 ±0.018, Relevance 0.8828 ±0.012, LLM Avg 0.8958; literature-review generation (pure LLM): average 0.8000 (Table 1 & 2).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Used as main strong LLM baseline versus HiGTL/HiReview and AutoSurvey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>GPT-4o is competitive in relevance but underperforms HiGTL/HiReview on structure and coverage, and on human evaluation metrics (see Tables 1–2).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Powerful zero-shot generator, but taxonomy-guided and graph-aware retrieval approaches beat naive GPT-4o usage in structured tasks like literature-review generation and taxonomy construction.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Pure GPT-4o without graph-aware retrieval or taxonomy scaffolding is less stable and has lower structure/coverage; susceptible to noise when given large retrieval sets; clustering via LLM baseline underperforms graph-clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper tests GPT-4o as-is and as RAG with different retrieval sizes (500 vs 200); no systematic analysis of model-size scaling beyond using GPT-4o as a strong baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxonomy Tree Generation from Citation Graph', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4396.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4396.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Naive RAG-based LLMs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Naive Retrieval-Augmented Generation LLM pipelines (BM25 + LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Baseline approach that retrieves top documents with a simple retriever (BM25) and feeds them to an LLM to generate literature reviews or taxonomies; used for comparison.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Naive RAG-based LLMs (BM25 retrieval + LLM generation)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Retrieve a large set of candidate documents using a simple retriever (BM25 or SentenceBert) and provide retrieved content to a large LLM (e.g., GPT-4o, Claude-3.5) to generate summaries or reviews without taxonomy scaffolding or hierarchical clustering.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o, Claude-3.5 used in naive RAG baselines in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>BM25 sparse retrieval (top-500 in some baselines) or SentenceBert dense retrieval in other ablations, optionally with neighbor information (graph-aware).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Flat LLM summarization of retrieved content (no taxonomy scaffold); multi-output & prompt engineering (as in AutoSurvey) sometimes applied to reduce instability.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Baseline variants used top-500 retrieved papers; AutoSurvey uses larger retrieval (e.g., 1200) per discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science literature-review generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Generated literature reviews (flat summaries), sometimes taxonomies in single-shot generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>LLM-Score (coverage/structure/relevance), BERTScore, human evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Naive RAG-based LLMs (GPT-4o •) achieved average ~0.8495 (Coverage 0.8219, Structure 0.8293, Relevance 0.8972) and Claude-3.5 • average ~0.8535 in the paper's Table 2; lower than HiReview.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to HiReview and AutoSurvey.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Outperformed pure zero-shot LLMs but underperformed HiReview and AutoSurvey on stability and structure metrics; HiReview improvements attributed to taxonomy scaffold and graph-aware retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG helps over pure zero-shot generation, but naive retrieval with large sets introduces noise; graph-aware retrieval + taxonomy scaffolding (HiReview) reduces noise and improves quality.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Retrieval size trade-off (larger sets increase recall but also noise); no hierarchical scaffold leads to poorer structure and less stable outputs; susceptibility to irrelevant documents.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance depends heavily on retrieval size and retrieval quality; naive RAG often requires large retrieval sets (500–1200) to avoid missing content, increasing compute and noise.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxonomy Tree Generation from Citation Graph', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4396.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4396.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM clustering (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLM-based clustering baseline (GPT-4o used to cluster papers)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A baseline that uses an LLM (GPT-4o) to cluster papers into topics (as opposed to the paper's hierarchical GNN-based clustering), evaluated and found to underperform.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>LLM clustering baseline (GPT-4o)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Use an LLM to assign or group papers into clusters (topic groups) directly, varying the number of clusters to represent levels, rather than using a hierarchical graph-clustering algorithm. The paper treats this as a baseline to compare clustering quality.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>GPT-4o (used to cluster papers in baseline experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Prompt-based grouping using LLM semantic understanding of paper text (details not fully elaborated in paper).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Direct LLM-driven grouping (no explicit hierarchical coarsening via graph operations); lacks joint trainability with graph clustering module.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applied per citation-graph instance (same datasets), statistics reported as clustering performance across levels.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Computer science citation graphs.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Clusters of papers (intended to map to taxonomy nodes), not full taxonomy trees in the baseline evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Hierarchical clustering level scores (Level 1/Level 2 accuracy) used in Table 6-like ablation (clustering-specific metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>LLM clustering baseline: Level 1 0.4296, Level 2 0.4518, Average 0.4407, underperforming the paper's hierarchical GAT-based clustering (GAT average 0.6761).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared with GAT-based hierarchical clustering and k-means baseline for clustering quality.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Substantially worse than the proposed hierarchical clustering (GAT) and even worse than simple k-means for this task (per clustering metrics in ablation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>LLM-only clustering baseline fails to capture hierarchical structure and the graph connectivity signal required for robust taxonomy induction; not jointly trainable with taxonomy verbalizer.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Cannot be jointly trained with taxonomy verbalizer; fails to provide soft-to-hard clustering transition across levels; inferior clustering metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Performance degrades relative to structured GNN clustering as hierarchical complexity increases; not discussed for very large datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxonomy Tree Generation from Citation Graph', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4396.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4396.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphLLM / Graph-aware LLM encoding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GraphLLM / graph-to-LLM encoding approaches (related prior work)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Related works that adapt or boost LLMs to reason about graphs and structured data by encoding graph information into LLM-consumable formats; cited as background motivating HiGTL's graph-to-LLM projection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graphllm: Boosting graph reasoning ability of large language model.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GraphLLM / 'Let your graph do the talking' style approaches</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Research lines that transform structured graph signals into representations consumable by LLMs—examples include encoding nodes/edges as text tokens or projecting graph embeddings into LLM embedding spaces—enabling LLMs to perform graph reasoning. The paper cites these as related work that motivates projecting graph cluster embeddings into the LLM token space via MLP Φ.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Varies by referenced work (general statement); referenced GraphLLM work and 'Let your graph do the talking' use standard LLMs augmented with graph encoders or tokenization strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Projecting graph embeddings to LLM-friendly formats, encoding structured data as tokens, or combining GNN-produced embeddings with LLM text embeddings.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Use of LLM reasoning over encoded graph tokens to perform higher-level abstraction or answer generation; informs HiGTL's approach of concatenating h_c, h_pi, and h_q before feeding to LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not applicable (related-method mention).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Graph reasoning and structured-data augmentation of LLMs (general).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Graph-conditioned text outputs, improved graph reasoning capabilities, or structured summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not detailed in this paper (referenced works use graph reasoning/QA metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported in this paper; cited as motivating literature.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Referenced as alternative approaches to combine graph signals with LLMs; not experimentally compared here.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>N/A within this paper (cited as related work).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Inspiration for mapping graph embeddings into LLM token space and for combining structural and textual modalities for downstream generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>General challenges include how to faithfully encode graph structure without losing information, and how to train LLMs to use graph signals effectively; specifics are in referenced works.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Referenced works discuss scaling graph token encodings and the compute tradeoffs for larger LLMs; specifics are external to this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Taxonomy Tree Generation from Citation Graph', 'publication_date_yy_mm': '2024-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AutoSurvey: Large Language Models Can Automatically Write Surveys. <em>(Rating: 2)</em></li>
                <li>Graphllm: Boosting graph reasoning ability of large language model. <em>(Rating: 2)</em></li>
                <li>Let your graph do the talking: Encoding structured data for llms. <em>(Rating: 2)</em></li>
                <li>HiExpan <em>(Rating: 1)</em></li>
                <li>NetTaxo <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4396",
    "paper_id": "paper-273185991",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "HiGTL",
            "name_full": "Hierarchical Graph Taxonomy Learning",
            "brief_description": "An end-to-end framework that constructs hierarchical taxonomies from citation graphs by jointly performing hierarchical graph clustering (GNN-based) and taxonomy-node verbalization via an LLM, with a two-phase optimization (pretrain clustering, then LoRA fine-tune LLM).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "HiGTL (Hierarchical Graph Taxonomy Learning)",
            "system_description": "HiGTL first encodes papers with a graph neural network (GAT) to produce node embeddings and performs recursive hierarchical citation-graph clustering (soft clustering at base level, hard clustering higher up) to produce hyper-nodes. Cluster features are aggregated (average + representative node) and projected into the LLM embedding space via an MLP projector (Φ). A pre-trained LLM Θ (used as taxonomy node verbalizer) consumes concatenated embeddings of cluster-level graph tokens, selected paper-level text embeddings, and user instruction tokens to iteratively generate concise central-concept labels for clusters in a bottom-up fashion. The pipeline is trained in two phases: pretrain the clustering module using paper-level labels and then jointly fine-tune clustering + LLM verbalizer using LoRA (parameter-efficient fine-tuning). Retrieval (graph-aware BM25/SentenceBert variants) is used upstream to select candidate papers for a given topic.",
            "llm_model_used": "Llama-2-7b-hf (used as taxonomy node verbalizer, fine-tuned via LoRA); in evaluation/ablation GPT-4o and Claude-3.5 are used as baselines (not as internal module in the main HiGTL pipeline).",
            "extraction_technique": "Hierarchical graph-aware retrieval (BM25 with neighbor-info) + GNN-based graph encoding (GAT) to produce cluster embeddings; paper texts encoded via LLM text embedder; MLP projector maps graph embeddings to LLM token space.",
            "synthesis_technique": "Iterative bottom-up hierarchical summarization / concept abstraction: cluster-level embeddings + selected base-paper text embeddings + instruction prompt are concatenated and fed into an LLM to generate central-concept labels for clusters; hierarchical likelihood objective enforces consistency across levels.",
            "number_of_papers": "Evaluated on 518 citation-graph instances (each a literature-review's citation network); retrieval phase uses top-200 retrieved papers; citation graphs average ~6,658 papers and ~11,633 edges (per merged networks described).",
            "domain_or_topic": "Computer science literature (literature-review citation graphs across CS subdomains).",
            "output_type": "Hierarchical taxonomies (taxonomy trees) with verbalized node labels (central concepts); can be used to guide literature-review generation.",
            "evaluation_metrics": "LLM-Score (coverage, structure, relevance), human evaluation (Adequacy binary, Validity 1–5), BERTScore.",
            "performance_results": "HiGTL: Coverage 0.9357 ±0.012, Structure 0.9413 ±0.010, Relevance 0.8748 ±0.013, LLM Average 0.9173; Human Adequacy 0.7150, Human Validity 2.6700; BERTScore 0.8694 ±0.010 (Table 1).",
            "comparison_baseline": "Compared to GPT-4o and Claude-3.5 (LLM baselines), and taxonomy-learning methods HiExpan, TaxoGen, NetTaxo.",
            "performance_vs_baseline": "HiGTL outperforms all baselines across metrics (e.g., LLM Avg 0.9173 vs GPT-4o 0.8958 and NetTaxo 0.8654; Coverage/Structure/Relevance improvements reported in Table 1).",
            "key_findings": "Jointly combining hierarchical graph clustering (structural signal) with LLM-based verbalization (textual abstraction) yields taxonomies that are both structurally coherent and semantically consistent; pretraining the clustering module before fine-tuning the LLM stabilizes training; graph-aware retrieval (BM25 with neighbor info) reduces noise and improves downstream quality.",
            "limitations_challenges": "Fine-grained distinctions can be less precise than human-crafted taxonomies; limited labeled hierarchies make full LLM fine-tuning infeasible (necessitates LoRA); potential for LLM hallucinations in node verbalization is acknowledged; end-to-end joint training is optimization-challenging and unstable if not staged.",
            "scaling_behavior": "Evaluated across 1-hop to 3-hop citation graphs and merged large networks; graph-aware retrieval (200 docs) maintains stable performance even in large networks; model uses parameter-efficient fine-tuning (LoRA) to keep LLM adaptation tractable; explicit scaling with larger LLM sizes not explored in paper.",
            "uuid": "e4396.0",
            "source_info": {
                "paper_title": "Taxonomy Tree Generation from Citation Graph",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "HiReview",
            "name_full": "Hierarchical Taxonomy-Driven Automatic Literature Review Generation",
            "brief_description": "A downstream system that uses HiGTL-generated taxonomies plus graph-context-aware retrieval to guide an LLM in generating structured literature reviews with high coverage, structure, and relevance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "HiReview (Taxonomy-then-Generation literature review system)",
            "system_description": "HiReview first retrieves a graph-aware subset of papers (graph context-aware retrieval, typically 200 papers) and constructs a hierarchical taxonomy using HiGTL. The taxonomy tree provides hierarchical context and topic delimiters; the LLM backbone (GPT-4o in the described experiments) then generates literature-review sections guided by taxonomy nodes and retrieved paper content. The model uses the taxonomy to drive outline/section generation rather than flat retrieval-then-generate approaches. Fine-tuning of the LLM backbone for taxonomy verbalization uses LoRA; the final content generation uses GPT-4o as the generator in experiments.",
            "llm_model_used": "GPT-4o (content generator backbone in HiReview experiments); Llama-2-7b-hf (LLaMA) fine-tuned via LoRA is used for taxonomy verbalization in HiGTL; Claude-3.5 used as baseline judge/evaluator in experiments.",
            "extraction_technique": "Graph-context-aware retrieval (BM25 with neighbor info preferred) to pick candidate papers; GNN-based hierarchical clustering produces clusters; selected paper abstracts/full-text are encoded and passed to the LLM as context.",
            "synthesis_technique": "Taxonomy-guided generation: use hierarchical taxonomy as scaffold to generate structured review sections; LLM composes summaries per taxonomy node and assembles them into a structured review (taxonomy-then-generation pipeline).",
            "number_of_papers": "System evaluated on 518 review instances; retrieval component typically returns 200 papers per review; in baselines some LLMs were provided top-500 via BM25 for comparison.",
            "domain_or_topic": "Computer science literature reviews.",
            "output_type": "Full literature reviews (structured sections), and taxonomy trees as intermediate artifact.",
            "evaluation_metrics": "LLM-Score (coverage, structure, relevance), human evaluation (compared to human-written reviews), BERTScore.",
            "performance_results": "HiReview: Coverage 0.9163 ±0.03, Structure 0.9484 ±0.02, Relevance 0.9428 ±0.01, Average 0.9358, BERTScore 0.8449 ±0.02 (Table 2). HiReview outperforms pure LLMs, naive RAG, and AutoSurvey on these metrics.",
            "comparison_baseline": "Pure LLMs (GPT-4o, Claude-3.5 zero-shot), naive RAG-based LLMs (BM25 retrieval + same LLM backbone), and AutoSurvey (structured retrieval + outline generation pipeline).",
            "performance_vs_baseline": "HiReview achieves higher scores than AutoSurvey and naive RAG: e.g., HiReview average 0.9358 vs AutoSurvey 0.8957, and HiReview Structure 0.9484 vs AutoSurvey 0.9122 (Table 2).",
            "key_findings": "Using a hierarchical taxonomy as an explicit scaffold for LLM generation produces more stable, higher-structure and higher-relevance literature reviews than flat retrieval-then-generate or outline-then-generate baselines; graph-aware retrieval (200 docs) reduces noise and improves generation quality and stability.",
            "limitations_challenges": "Ablations show retrieval and clustering critically affect review quality—removing retrieval or taxonomy degrades performance severely; reliance on high-quality retrieval and taxonomy quality; pure LLMs without fine-tuning are less stable; potential hallucination remains an issue.",
            "scaling_behavior": "Graph-aware retrieval keeps retrieval size small (200) and stable across larger citation networks (tested up to merged 2-hop networks), reducing need to scale retrieval to thousands of papers; does not explore scaling to much larger LLM backbones in the paper.",
            "uuid": "e4396.1",
            "source_info": {
                "paper_title": "Taxonomy Tree Generation from Citation Graph",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "AutoSurvey",
            "name_full": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
            "brief_description": "A prior system (referenced) that generates literature reviews by combining retrieval, outline generation, and section drafting using LLMs; used as a baseline in experiments.",
            "citation_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys.",
            "mention_or_use": "mention",
            "system_name": "AutoSurvey",
            "system_description": "AutoSurvey follows a retrieve-then-outline-then-generate pipeline: retrieve candidate papers (large retrieval sets, e.g., 1200), generate an outline, and draft sections using an LLM (GPT-4o in comparisons). It emphasizes prompt design and multi-output generation to reduce instability.",
            "llm_model_used": "GPT-4o used as LLM backbone in cited comparison experiments (per paper references).",
            "extraction_technique": "Dense/sparse retrieval (large sets), BM25 or dense retrievers were used in the original AutoSurvey pipeline (paper references).",
            "synthesis_technique": "Outline-then-generation: generate outline structure from retrieved content, then generate sections guided by outline.",
            "number_of_papers": "AutoSurvey in referenced work retrieves large candidate sets (e.g., ~1200) to avoid missing relevant content.",
            "domain_or_topic": "General automated literature review generation (used here on computer science reviews as baseline).",
            "output_type": "Generated literature reviews (structured).",
            "evaluation_metrics": "Same LLM-Score metrics (coverage, structure, relevance) and BERTScore as used in this paper for comparison.",
            "performance_results": "Reported in this paper: AutoSurvey average LLMScore 0.8957, Coverage 0.8646 ±0.07, Structure 0.9122 ±0.05, Relevance 0.9093 ±0.04, BERTScore 0.8256 ±0.02 (Table 2).",
            "comparison_baseline": "Compared against HiReview, naive RAG LLMs, and pure LLMs in literature-review generation experiments.",
            "performance_vs_baseline": "AutoSurvey performs better than naive/zero-shot LLMs but is outperformed by HiReview (e.g., HiReview average 0.9358 vs AutoSurvey 0.8957).",
            "key_findings": "Outline-then-generation and careful prompt design improve quality over naive LLM pipelines, but taxonomy-guided generation (HiReview) yields superior structure and relevance.",
            "limitations_challenges": "Needs large retrieval sets (incurs noise and computation); less effective at leveraging graph-structure context than taxonomy-driven approaches.",
            "scaling_behavior": "Relies on scaling retrieval size (e.g., 1200) to maintain recall; this introduces computational cost and noise tradeoffs.",
            "uuid": "e4396.2",
            "source_info": {
                "paper_title": "Taxonomy Tree Generation from Citation Graph",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GPT-4o (as used)",
            "name_full": "GPT-4o (OpenAI generative pre-trained transformer variant used in experiments)",
            "brief_description": "A strong commercial LLM used both as a content generator for literature review generation and as a baseline for taxonomy generation in the paper's experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "GPT-4o (used as baseline/generator)",
            "system_description": "Used in experiments as a zero-shot/prompted generative backbone to (a) generate taxonomies directly in baseline comparisons, (b) serve as the content generator for review generation pipelines (HiReview uses GPT-4o for content generation in experiments), and (c) as an LLM-based clustering baseline in ablations.",
            "llm_model_used": "GPT-4o-2024-05-13 (content generator) per implementation details; in some baseline conditions provided with top-500 BM25 retrieved papers.",
            "extraction_technique": "Zero-shot generation from its internal knowledge; or used with BM25 retrieval (top-500) as naive RAG baseline; in clustering baseline used as a clustering LLM.",
            "synthesis_technique": "Direct generation or RAG-augmented generation; for clustering baseline used LLM-based clustering heuristics (details not elaborated).",
            "number_of_papers": "In baseline experiments some LLMs were given top-500 retrieved papers; HiReview uses GPT-4o with top-200 retrieved papers.",
            "domain_or_topic": "Used across computer-science literature-review experiments in the paper.",
            "output_type": "Generated taxonomy labels (in baselines) and literature-review content.",
            "evaluation_metrics": "LLM-Score (coverage/structure/relevance), human evaluation, BERTScore in comparisons.",
            "performance_results": "As baseline: GPT-4o taxonomy generation: Coverage 0.9132 ±0.015, Structure 0.8914 ±0.018, Relevance 0.8828 ±0.012, LLM Avg 0.8958; literature-review generation (pure LLM): average 0.8000 (Table 1 & 2).",
            "comparison_baseline": "Used as main strong LLM baseline versus HiGTL/HiReview and AutoSurvey.",
            "performance_vs_baseline": "GPT-4o is competitive in relevance but underperforms HiGTL/HiReview on structure and coverage, and on human evaluation metrics (see Tables 1–2).",
            "key_findings": "Powerful zero-shot generator, but taxonomy-guided and graph-aware retrieval approaches beat naive GPT-4o usage in structured tasks like literature-review generation and taxonomy construction.",
            "limitations_challenges": "Pure GPT-4o without graph-aware retrieval or taxonomy scaffolding is less stable and has lower structure/coverage; susceptible to noise when given large retrieval sets; clustering via LLM baseline underperforms graph-clustering.",
            "scaling_behavior": "Paper tests GPT-4o as-is and as RAG with different retrieval sizes (500 vs 200); no systematic analysis of model-size scaling beyond using GPT-4o as a strong baseline.",
            "uuid": "e4396.3",
            "source_info": {
                "paper_title": "Taxonomy Tree Generation from Citation Graph",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "Naive RAG-based LLMs",
            "name_full": "Naive Retrieval-Augmented Generation LLM pipelines (BM25 + LLM)",
            "brief_description": "Baseline approach that retrieves top documents with a simple retriever (BM25) and feeds them to an LLM to generate literature reviews or taxonomies; used for comparison.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "Naive RAG-based LLMs (BM25 retrieval + LLM generation)",
            "system_description": "Retrieve a large set of candidate documents using a simple retriever (BM25 or SentenceBert) and provide retrieved content to a large LLM (e.g., GPT-4o, Claude-3.5) to generate summaries or reviews without taxonomy scaffolding or hierarchical clustering.",
            "llm_model_used": "GPT-4o, Claude-3.5 used in naive RAG baselines in experiments.",
            "extraction_technique": "BM25 sparse retrieval (top-500 in some baselines) or SentenceBert dense retrieval in other ablations, optionally with neighbor information (graph-aware).",
            "synthesis_technique": "Flat LLM summarization of retrieved content (no taxonomy scaffold); multi-output & prompt engineering (as in AutoSurvey) sometimes applied to reduce instability.",
            "number_of_papers": "Baseline variants used top-500 retrieved papers; AutoSurvey uses larger retrieval (e.g., 1200) per discussion.",
            "domain_or_topic": "Computer science literature-review generation tasks.",
            "output_type": "Generated literature reviews (flat summaries), sometimes taxonomies in single-shot generation.",
            "evaluation_metrics": "LLM-Score (coverage/structure/relevance), BERTScore, human evaluation.",
            "performance_results": "Naive RAG-based LLMs (GPT-4o •) achieved average ~0.8495 (Coverage 0.8219, Structure 0.8293, Relevance 0.8972) and Claude-3.5 • average ~0.8535 in the paper's Table 2; lower than HiReview.",
            "comparison_baseline": "Compared to HiReview and AutoSurvey.",
            "performance_vs_baseline": "Outperformed pure zero-shot LLMs but underperformed HiReview and AutoSurvey on stability and structure metrics; HiReview improvements attributed to taxonomy scaffold and graph-aware retrieval.",
            "key_findings": "RAG helps over pure zero-shot generation, but naive retrieval with large sets introduces noise; graph-aware retrieval + taxonomy scaffolding (HiReview) reduces noise and improves quality.",
            "limitations_challenges": "Retrieval size trade-off (larger sets increase recall but also noise); no hierarchical scaffold leads to poorer structure and less stable outputs; susceptibility to irrelevant documents.",
            "scaling_behavior": "Performance depends heavily on retrieval size and retrieval quality; naive RAG often requires large retrieval sets (500–1200) to avoid missing content, increasing compute and noise.",
            "uuid": "e4396.4",
            "source_info": {
                "paper_title": "Taxonomy Tree Generation from Citation Graph",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "LLM clustering (baseline)",
            "name_full": "LLM-based clustering baseline (GPT-4o used to cluster papers)",
            "brief_description": "A baseline that uses an LLM (GPT-4o) to cluster papers into topics (as opposed to the paper's hierarchical GNN-based clustering), evaluated and found to underperform.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_name": "LLM clustering baseline (GPT-4o)",
            "system_description": "Use an LLM to assign or group papers into clusters (topic groups) directly, varying the number of clusters to represent levels, rather than using a hierarchical graph-clustering algorithm. The paper treats this as a baseline to compare clustering quality.",
            "llm_model_used": "GPT-4o (used to cluster papers in baseline experiments).",
            "extraction_technique": "Prompt-based grouping using LLM semantic understanding of paper text (details not fully elaborated in paper).",
            "synthesis_technique": "Direct LLM-driven grouping (no explicit hierarchical coarsening via graph operations); lacks joint trainability with graph clustering module.",
            "number_of_papers": "Applied per citation-graph instance (same datasets), statistics reported as clustering performance across levels.",
            "domain_or_topic": "Computer science citation graphs.",
            "output_type": "Clusters of papers (intended to map to taxonomy nodes), not full taxonomy trees in the baseline evaluation.",
            "evaluation_metrics": "Hierarchical clustering level scores (Level 1/Level 2 accuracy) used in Table 6-like ablation (clustering-specific metrics).",
            "performance_results": "LLM clustering baseline: Level 1 0.4296, Level 2 0.4518, Average 0.4407, underperforming the paper's hierarchical GAT-based clustering (GAT average 0.6761).",
            "comparison_baseline": "Compared with GAT-based hierarchical clustering and k-means baseline for clustering quality.",
            "performance_vs_baseline": "Substantially worse than the proposed hierarchical clustering (GAT) and even worse than simple k-means for this task (per clustering metrics in ablation).",
            "key_findings": "LLM-only clustering baseline fails to capture hierarchical structure and the graph connectivity signal required for robust taxonomy induction; not jointly trainable with taxonomy verbalizer.",
            "limitations_challenges": "Cannot be jointly trained with taxonomy verbalizer; fails to provide soft-to-hard clustering transition across levels; inferior clustering metrics.",
            "scaling_behavior": "Performance degrades relative to structured GNN clustering as hierarchical complexity increases; not discussed for very large datasets.",
            "uuid": "e4396.5",
            "source_info": {
                "paper_title": "Taxonomy Tree Generation from Citation Graph",
                "publication_date_yy_mm": "2024-10"
            }
        },
        {
            "name_short": "GraphLLM / Graph-aware LLM encoding",
            "name_full": "GraphLLM / graph-to-LLM encoding approaches (related prior work)",
            "brief_description": "Related works that adapt or boost LLMs to reason about graphs and structured data by encoding graph information into LLM-consumable formats; cited as background motivating HiGTL's graph-to-LLM projection.",
            "citation_title": "Graphllm: Boosting graph reasoning ability of large language model.",
            "mention_or_use": "mention",
            "system_name": "GraphLLM / 'Let your graph do the talking' style approaches",
            "system_description": "Research lines that transform structured graph signals into representations consumable by LLMs—examples include encoding nodes/edges as text tokens or projecting graph embeddings into LLM embedding spaces—enabling LLMs to perform graph reasoning. The paper cites these as related work that motivates projecting graph cluster embeddings into the LLM token space via MLP Φ.",
            "llm_model_used": "Varies by referenced work (general statement); referenced GraphLLM work and 'Let your graph do the talking' use standard LLMs augmented with graph encoders or tokenization strategies.",
            "extraction_technique": "Projecting graph embeddings to LLM-friendly formats, encoding structured data as tokens, or combining GNN-produced embeddings with LLM text embeddings.",
            "synthesis_technique": "Use of LLM reasoning over encoded graph tokens to perform higher-level abstraction or answer generation; informs HiGTL's approach of concatenating h_c, h_pi, and h_q before feeding to LLM.",
            "number_of_papers": "Not applicable (related-method mention).",
            "domain_or_topic": "Graph reasoning and structured-data augmentation of LLMs (general).",
            "output_type": "Graph-conditioned text outputs, improved graph reasoning capabilities, or structured summaries.",
            "evaluation_metrics": "Not detailed in this paper (referenced works use graph reasoning/QA metrics).",
            "performance_results": "Not reported in this paper; cited as motivating literature.",
            "comparison_baseline": "Referenced as alternative approaches to combine graph signals with LLMs; not experimentally compared here.",
            "performance_vs_baseline": "N/A within this paper (cited as related work).",
            "key_findings": "Inspiration for mapping graph embeddings into LLM token space and for combining structural and textual modalities for downstream generation tasks.",
            "limitations_challenges": "General challenges include how to faithfully encode graph structure without losing information, and how to train LLMs to use graph signals effectively; specifics are in referenced works.",
            "scaling_behavior": "Referenced works discuss scaling graph token encodings and the compute tradeoffs for larger LLMs; specifics are external to this paper.",
            "uuid": "e4396.6",
            "source_info": {
                "paper_title": "Taxonomy Tree Generation from Citation Graph",
                "publication_date_yy_mm": "2024-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AutoSurvey: Large Language Models Can Automatically Write Surveys.",
            "rating": 2,
            "sanitized_title": "autosurvey_large_language_models_can_automatically_write_surveys"
        },
        {
            "paper_title": "Graphllm: Boosting graph reasoning ability of large language model.",
            "rating": 2,
            "sanitized_title": "graphllm_boosting_graph_reasoning_ability_of_large_language_model"
        },
        {
            "paper_title": "Let your graph do the talking: Encoding structured data for llms.",
            "rating": 2,
            "sanitized_title": "let_your_graph_do_the_talking_encoding_structured_data_for_llms"
        },
        {
            "paper_title": "HiExpan",
            "rating": 1
        },
        {
            "paper_title": "NetTaxo",
            "rating": 1
        }
    ],
    "cost": 0.01887925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Taxonomy Tree Generation from Citation Graph
27 Feb 2025</p>
<p>Yuntong Hu yuntong.hu@emory.edu 
Zhuofeng Li zhuofengli12345@gmail.com 
Chen Ling chen.ling@emory.edu 
Raasikh Kanjiani raasikh.kanjiani@emory.edu 
Boxin Zhao 
Liang Zhao liang.zhao@emory.edu 
Zheng Zhang zheng.zhang@emory.edu 
Taxonomy </p>
<p>Emory University Atlanta
GAUSA</p>
<p>Shanghai University Shanghai
China</p>
<p>Zheng Zhang</p>
<p>Emory University Atlanta
GAUSA</p>
<p>Emory University Atlanta
GAUSA</p>
<p>Emory University Atlanta
GAUSA</p>
<p>Emory University Atlanta
GAUSA</p>
<p>Emory University Atlanta
GAUSA</p>
<p>Conference acronym 'XX
03-05, 2018June, WoodstockNY</p>
<p>Taxonomy Tree Generation from Citation Graph
27 Feb 2025068D7C9B5A2751AE5630639AC54C4F9CarXiv:2410.03761v2[cs.CL]Hierarchical taxonomy generationcitation graphgraph learningretrieval-augmented generationlarge language models
Constructing taxonomies from citation graphs is essential for organizing scientific knowledge, facilitating literature reviews, and identifying emerging research trends.However, manual taxonomy construction is labor-intensive, time-consuming, and prone to human biases, often overlooking pivotal but less-cited papers.In this paper, to enable automatic hierarchical taxonomy generation from citation graphs, we propose HiGTL (Hierarchical Graph Taxonomy Learning), a novel end-to-end framework guided by humanprovided instructions or preferred topics.Specifically, we propose a hierarchical citation graph clustering method that recursively groups related papers based on both textual content and citation structure, ensuring semantically meaningful and structurally coherent clusters.Additionally, we develop a novel taxonomy node verbalization strategy that iteratively generates central concepts for each cluster, leveraging a pre-trained large language model (LLM) to maintain semantic consistency across hierarchical levels.To further enhance performance, we design a joint optimization framework that fine-tunes both the clustering and concept generation modules, aligning structural accuracy with the quality of generated taxonomies.Extensive experiments demonstrate that HiGTL effectively produces coherent, high-quality taxonomies.</p>
<p>INTRODUCTION</p>
<p>Citation graphs represent the citation relationships within a collection of documents (e.g., research papers, patents, etc.), capturing the structure of scholarly communication and illustrating how ideas evolve and influence one another.A taxonomy of citation graphs organizes research topics hierarchically, grouping related papers into clusters that form broader categories (as shown in Figure 1).This tree-like structure highlights the connections between specific subtopics and overarching fields, offering an interpretable overview of the research landscape.Such taxonomies are typically substantial backbones of subsequent activities such as literature reviews [37], meta-analysis and knowledge discovery [11,21], and research trend identification [28], which are vital to the scientific community.However, manual taxonomy construction is labor-intensive, time-consuming, and prone to human bias and comprehensiveness, which hence struggle to keep pace with the rapid growth of publications.In natural language processing, there are techniques called existing taxonomy learning which focuses on deriving semantic relationships from linguistic patterns in texts [11,25,40,42].They are designed for plain text corpora rather than text graphs and hence cannot handle patterns like connectivities and communities of documents, which are crucial for taxonomy generation from citation graphs [3,5,21,49].In this paper, we focus on a compelling problem:</p>
<p>Can we automatically generate taxonomies from citation graphs?</p>
<p>Generating taxonomy trees from citation graphs, though important, encompasses significant and unique major challenges to be addressed: (1) How do graph topology and texts jointly decide taxonomy?Citation graphs contain both 1) the citing relationships among papers with similar topics of interest and 2) the rich textual content of each paper.Effectively integrating these two modalities is crucial because relying solely on graph topology may overlook semantic nuances, while focusing only on text may ignore the technique evolution and contextual relevance indicated through citation links.(2) How are the taxonomy nodes verbalized?Verbalizing taxonomy nodes involves generating concise, accurate labels that capture the essence of each cluster while maintaining semantic coherence across hierarchical levels.This is challenging because clusters often span diverse or overlapping topics, requiring the model to balance specificity and generalization to produce informative and contextually appropriate labels.(3) How to automatically learn the whole taxonomy generator from data?Transforming from citation graphs to taxonomy is a nontrivial tasks that require a highly expressive model for both taxonomy tree construction and taxonomy node verbalization.How to train such a model from limited data is challenging in both the optimization and learning aspects.</p>
<p>To pursue our goal and address these challenges, we propose Citation Graph to Taxonomy Transformation (CGT), a novel framework that automatically extracts taxonomies from citation graphs.First, to enable meaningful hierarchical citation graph clustering, we propose a novel method that recursively decomposes the citation graph into multiple levels by grouping papers based on both textual content and citation link topology.At each level, nodes of lower-level topics are clustered into "supernodes" of higher-level topics, while feature aggregation maintains semantic coherence, facilitating the extraction of structured taxonomies.Second, to verbalize the nodes in taxonomy, we introduce a hierarchical concept abstraction strategy that iteratively generates central concepts for each cluster.The method synergizes cluster-level graph embeddings and paper-level information to produce consistent, semantically rich taxonomies aligned with user-preferred topics by prompting.Finally, to ensure clusters are structurally meaningful and semantically coherent, we jointly optimize all the parameters of our CGT through a customized training strategy consisting of pre-training and fine-tuning.To thoroughly evaluate the effectiveness of our method, we collected 518 citation graphs, each corresponding to a citation graph of a high-quality, human-written literature review in the computer science domain.Extensive experiments demonstrate that our model effectively generates high-quality taxonomies.</p>
<p>In summary, our primary contributions are as follows:</p>
<p>• We introduce a novel end-to-end framework that automatically generates taxonomies from citation graphs, guided by humanprovided central topics to enhance relevance.• We propose a hierarchical citation graph clustering approach for citation graphs that integrates textual content and structural information, enabling the formation of semantically meaningful and structurally coherent clusters.• We develop a taxonomy node verbalization method that iteratively generates central concepts for clusters at each level, ensuring semantic consistency throughout the hierarchy.• We design a novel optimization framework that jointly fine-tunes hierarchical citation graph clustering and taxonomy node verbalizer, improving both structural accuracy and the coherence of the generated taxonomies.</p>
<p>RELATED WORK 2.1 Taxonomy Learning</p>
<p>Taxonomy learning [11,24,41] aims to construct hierarchical structures that capture semantic relationships within domain-specific corpora.Its objective is to identify and organize conceptual relationships while maximizing semantic coherence and relevance.Existing approaches can be categorized into four main paradigms: pattern-based methods [29,32,41], clustering-based techniques [5,21], statistical frameworks [4,46], and graph-based approaches [15,17,44].While these methods have advanced our understanding of taxonomy construction, they predominantly focus on unstructured database [14,30], limiting their applicability to more complex structured databases.Recent advances have extended taxonomy learning to graph-structured data, particularly knowledge graphs [23].However, these approaches are primarily designed for knowledge graphs where nodes represent atomic concepts or entities.This makes them insufficient for real-world graph databases like citation networks, where nodes contain rich textual content.</p>
<p>Hierarchical Graph Clustering</p>
<p>The primary goal of hierarchical graph clustering is to construct a hierarchy of clusters that effectively captures the underlying multi-scale structure of a graph.Flake et al. introduced a minimum s-t-cuts-based hierarchical graph clustering method, guaranteeing high-quality partitions.Doll et al. extended this approach by allowing arbitrary minimum s-t-cuts, enabling more flexible clustering hierarchies.While these methods provide strong theoretical guarantees, they struggle with scalability on large-scale graphs.</p>
<p>To improve efficiency, density-based methods have been explored [34,43], grouping nodes based on high local density, making them robust to variations in graph topology.Modularity-based methods have also been successful.Zeng and Yu proposed a parallel hierarchical graph clustering algorithm leveraging distributed memory architectures and a divide-and-conquer strategy for large-scale graphs.These methods remain widely used in unsupervised community detection [1,48].To improve scalability, minimum spanning tree (MST)-based clustering methods, such as Affinity Clustering [8] and Hierarchical Spectral Clustering [39], have been proposed to recursively partition graphs.</p>
<p>Parameter-Efficient Fine-Tuning (PEFT)</p>
<p>Full fine-tuning of domain-specific large pre-trained models is resource-intensive, while parameter-efficient fine-tuning (PEFT) improves performance in specific domains by updating only a subset of parameters [6,10].Key techniques in PEFT include Adapters, Low-Rank Adaptation (LoRA) [13], and Prompt Tuning.Adapters are small neural networks inserted into the layers of a pre-trained model to modify its behavior for specific tasks [12,27].Instead of updating the entire model, only the adapter parameters are updated, making the process computationally efficient.Adapters have also been explored in applying PEFT to graph-based language models [2,22,26].LoRA introduces low-rank matrices into transformer layers, enabling fine-tuning of only a small subset of the model's parameters while preserving overall performance.Prompt Tuning involves learning task-specific prompts that guide the model to perform various tasks without altering its underlying parameters [18][19][20].The pre-trained model remains fixed, with only the input prompts being optimized for the specific task.Taxonomy Tree Generation from Citation Graph.The goal of taxonomy generation is to learn a mapping function  that constructs an optimal taxonomy tree from the citation graph for a specific topic , i.e.,  : G,  → T * .To achieve this goal, three challenges must be addressed:</p>
<p>PROBLEM FORMALIZATION</p>
<p>• Graph-to-Taxonomy Transformation.Citation graphs exhibit complex citation relationships, often allowing papers to belong to multiple topics, whereas taxonomy trees impose a strict hierarchical structure with a single-parent constraint.• In Section 4.2, we propose a novel hierarchical graph clustering approach as the hierarchy induction function to address the Graph-to-Taxonomy Transformation challenge.We hierarchically categorize papers in a citation network, ensuring that each cluster at a given level corresponds to a topic in the taxonomy tree at the same level.• In Section 4.3, to achieve taxonomy node verbalization and address the challenge of Semantic Coherence in Taxonomy Generation, we propose an iterative graph-taxonomy generation approach-a bottom-up method that identifies the central topic of each cluster at every hierarchical level while ensuring conceptual alignment across levels.• In Section 4.4, we propose a novel two-phase optimization strategy for the Learning of Taxonomy Generation, enabling end-to-end joint training of graph clustering and taxonomy generation.The process begins with pretraining the hierarchical citation graph clustering module using discrete clustering labels, followed by fine-tuning both the clustering and generation modules with continuous labels representing topic content.</p>
<p>Hierarchical Citation Graph Clustering</p>
<p>Given a citation graph G, let {G ( ) = ( ( ) ,  ( ) ,  ( ) )}  =1 represent its hierarchical decomposition into a sequence of (hyper-)graphs across  levels, where  ( ) are the nodes,  ( ) are the edges, and  ( ) are the node representations at level .The recursive updates are defined as:</p>
<p>( (+1) ,  (+1) ) = CLU( ( ) ,  ( ) ,  ( ) ),</p>
<p>(+1) = AGG(CLU( ( ) ,  ( ) ,  ( ) )),</p>
<p>where CLU(•) clusters nodes in G ( ) into hyper-nodes for G (+1)  and forms edges between them (elaborated in Section 4.2.1), while AGG(•) computes features for hyper-nodes in  (+1) (elaborated in Section 4.2.2).The sequence is initialized from the citation graph G (1) = G, with node features  (1) = LM({  }  ∈ ) generated using pre-trained dense language models.The hierarchical citation graph clustering and aggregation operations collectively define the hierarchy induction function  : ( , ) → ( , ).This hierarchical citation graph clustering partitions the node set  ( ) into clusters C ( ) , with each hyper-node  ∈  (+1) corresponding to a unique topic  ∈  .The edge set  is directly induced from the evolving hierarchical aggregation across hierarchical levels.</p>
<p>Clustering Operator CLU(•).</p>
<p>The clustering function partitions nodes in  ( ) into clusters based on structural and feature similarities, treating each cluster as a hyper-node in  (+1) .Edges  (+1) are formed between hyper-nodes if any nodes in their corresponding clusters were connected in  ( ) :
𝐸 (𝑙+1) = {(𝑐 (𝑙 ) 𝑖 , 𝑐 (𝑙 ) 𝑗 ) | ∃(𝑢, 𝑣) ∈ 𝐸 (𝑙 ) , 𝑢 ∈ 𝑐 (𝑙 ) 𝑖 , 𝑣 ∈ 𝑐 (𝑙 ) 𝑗 }.
where  ( )  and  ( )  represent node clusters at level .We next introduce our strategy for extracting clusters at each hierarchical level.</p>
<p>Given G ( ) = ( ( ) ,  ( ) ,  ( ) ), we apply a graph encoder, such as GNN  , which aggregates information from neighboring nodes, updating node embeddings.We then compute the probability p ( )  of nodes ,  ∈  ( ) belonging to the same cluster using MLP  ( ) with a softmax transformation, taking the concatenated embeddings [ℎ  ; ℎ  ] as input.We then calculate the node density d to measure the similarity-weighted proportion of same-cluster nodes in its neighborhood:
d𝑢 = 1 |N (𝑢)| ∑︁ 𝑘 ∈ N (𝑢 ) p (𝑙 ) 𝑢𝑘 • ℎ 𝑢 • ℎ 𝑘 ∥ℎ 𝑢 ∥∥ℎ 𝑘 ∥ . (3)
where N () indicates the neighboring nodes of node .The density assesses how densely connected each node is within its local neighborhood.High-density nodes are more likely to be in the core of a cluster, whereas low-density nodes are more likely to be in ambiguous regions between clusters.d = 0 if  is isolated.</p>
<p>To balance fine-grained topic relationships with a well-structured taxonomy, we employ soft clustering at the first level, allowing topic overlap to reflect the interdisciplinary nature of research, and progressively transition to hard clustering at higher levels to enforce distinct, non-overlapping categories in the taxonomy hierarchy.</p>
<p>At the base level ( = 1), given p ( )  , d , d , and a pre-defined edge connection threshold   , we generate the candidate cluster as:
𝑐 (𝑙 ) 𝑖 = {𝑢} ∪ {𝑣 | d𝑢 &lt; d𝑣 and p (𝑙 ) 𝑢𝑣 &gt; 𝑝 𝜏 }.(4)
Nodes with higher density may belong to multiple clusters, reflecting the interdisciplinary nature of research.Final clusters are formed by merging candidate clusters:
C (𝑙 ) = {𝑐 (𝑙 ) 𝑖 | 𝑐 (𝑙 ) 𝑖 ⊄ 𝑐 (𝑙 ) 𝑗 , |𝑐 (𝑙 ) 𝑖 | ≠ 1}. (5)
At higher levels ( &gt; 1), clustering transitions to hard clustering to enforce distinct, non-overlapping categories.Edges between nodes are formed based on the strongest connection probabilities:
E = {(𝑢, 𝑣) | arg max 𝑣≠𝑢 ∈𝑉 p (𝑙 ) 𝑢𝑣 }, 𝑢 ∈ 𝑉 (𝑙 ) .(6)
After traversing all nodes, connected components defined by E form disjoint clusters C ( ) , ensuring clear topic boundaries within the taxonomy hierarchy.</p>
<p>Aggregation Operator AGG(•).</p>
<p>The aggregation function computes the feature representation for each hyper-node in  (+1) .Specifically, the feature of a hyper-node is derived from both the average and the most representative node features within its corresponding cluster:
𝑥 𝑢 = 1 |𝑐 (𝑙 ) 𝑖 | ∑︁ 𝑧 ∈𝑐 (𝑙 ) 𝑖 ℎ 𝑧 + ℎ 𝑘 , where 𝑘 = arg max 𝑧 ∈𝑐 (𝑙 ) 𝑖 d𝑧 .(7)
Here, ℎ  corresponds to the node with the highest density in cluster 
(𝑙 )
 , ensuring that the hyper-node captures both general and distinctive characteristics of the cluster.This aggregated feature set  (+1) serves as the input for the next hierarchical level.</p>
<p>Hierarchical Citation Graph Clustering</p>
<p>Objective.Since paper-level annotations are the most granular and reliable supervision, leveraging them indirectly optimizes higher-level clusters, preserving semantic integrity and aligning the taxonomy with real-world research structures without relying on hypothetical hypernode labels.Moreover, papers consistently clustered together across multiple hierarchical levels should be closer in the embedding space than those grouped only at lower levels or not at all, reflecting stronger and more persistent semantic relationships.Therefore, we design the objective function to operate on paper-level labels as:
L HiCluster = L cluster + 𝛼 • L HiMulCon ,(8)
where the first term, L cluster , optimizes clustering by grouping papers with structural and semantic similarities at each hierarchical level.The second term, L HiMulCon , is a hierarchical contrastive loss that enforces multi-level consistency by bringing together nodes that persist in the same cluster across levels and pushing apart those that diverge, ensuring the embeddings reflect the taxonomy's hierarchical structure. controls the contribution from the contrastive loss.Specifically, the clustering loss term is defined as follows,
L cluster = 𝐿 ∑︁ 𝑙=1 −1 |𝐸| ∑︁ (𝑢,𝑣) ∈𝐸 𝑞 (𝑙 ) 𝑢𝑣 log p (𝑙 ) 𝑢𝑣 + (1 −𝑞 (𝑙 ) 𝑢𝑣 ) log(1 − p (𝑙 ) 𝑢𝑣 ) (9)
where  ( )  = 1 if ,  ∈  belong to the same cluster at level , and  ( )  = 0 otherwise.This is computed over the edge set  rather than all node pairs, as we empirically found it reduces computational costs without significantly affecting performance.</p>
<p>To ensure that papers consistently assigned to the same cluster across multiple levels are positioned closer in the embedding space, the second term is formulated as a hierarchical contrastive loss as:
L HiMulCon = 1 |𝐿 | 𝐿 ∑︁ 𝑙 =1 ∑︁ 𝑢 ∈𝑉 −𝛿 𝑙 |𝑆 (𝑙 ) 𝑢 | ∑︁ 𝑘 ∈𝑆 (𝑙 ) 𝑢 log exp(sim(ℎ 𝑢 , ℎ 𝑘 )/𝜏 ) 𝑣 ∈𝑉 \𝑢 exp(sim(ℎ 𝑢 , ℎ 𝑣 )/𝜏 )(10)
where
𝑆 (𝑙 )
 represents the set of positive samples for node  at level , i.e., nodes that belong to the same cluster.  is a weighting factor that reflects the importance of each level for the loss, and  is the temperature parameter. represents the total number of hierarchical levels.</p>
<p>Hierarchical Taxonomy Node Verbalization</p>
<p>A citation graph G = ( , , {  }  ∈ ) reveals local research communities that expand into larger clusters through hierarchical analysis.Its structure supports a bottom-up taxonomy, where denser subgraphs form the basis for broader categories.After hierarchical citation graph clustering, each cluster  ( ) ∈ C ( ) corresponds to a topic  ∈  in the taxonomy T .We now introduce an iterative generation method to achieve the mapping ℎ : {  }  ∈ → {  }  ∈ for summarizing the central concept of each cluster.Specifically, we propose an iterative graph-taxonomy generation strategy that refines fine-grained topics and abstracts them into higher-level concepts, ensuring semantic consistency across hierarchy levels.</p>
<p>Iterative Generation.</p>
<p>Given the hierarchical decomposition G ( ) = ( ( ) ,  ( ) ,  ( ) ) of a citation graph G at level  and its corresponding clusters C ( ) , we generate the central concept for each cluster.To generate a meaningful taxonomy for a specific topic , we employ a pre-trained LLM Θ as the backbone for concept abstraction, leveraging its extensive text processing capabilities.For each cluster  ( ) ∈ C ( ) , the probability distribution of its generated central concept is defined as:
𝑝 Θ,Φ (𝑌 (𝑙+1) 𝑢 | 𝑐 (𝑙 ) , 𝑞) = 𝑟 𝑖=1 𝑝 Θ,Φ (𝑦 𝑖 | 𝑦 &lt;𝑖 , 𝑐 (𝑙 ) , 𝑞),(11)
where  (+1)  denotes the token sequence of central concept of cluster  ( ) ∈ C ( ) , corresponding to the hyper-node  ∈  (+1) . represents the instruction prompt reflecting user preferences, and  &lt; denotes the prefix tokens.Φ indicates the parameters of a projector that maps the graph embedding into the LLM text embedding space.</p>
<p>Each cluster at level  represents a subgraph of G ( ) .To capture interactions between hyper-nodes for central concept generation, we apply a projector MLP Φ to map graph embeddings into the LLM text space as graph tokens, where graph embeddings are produced using the same GNN  in the hierarchical citation graph clustering process.While hierarchical aggregation retains cluster features in hyper-nodes, information loss from individual papers is inevitable as the hierarchy deepens.To address this, we also incorporate paperlevel information from the base graph, as each hyper-node at higher levels can be coarsened to its corresponding set of base-level papers.Formally, the cluster  ( ) is encoded as:
h 𝑐 = MLP Θ (GNN 𝜃 (𝑐 (𝑙 ) )) ∈ R 𝑑 LLM (12)
where  LLM denotes the LLM text embedding dimension.The associated papers and instructions are encoded using the LLM text embedder as:
h 𝜋 = TextEmbedder({𝐷 𝑣 } 𝑣 ∈𝜋 (𝑙 ) 𝑐 ) ∈ R 𝑁 𝜋 ×𝑑 LLM ,(13)h 𝑞 = TextEmbedder(𝑞) ∈ R 𝑁 𝑞 ×𝑑 LLM ,(14)
where   ,   indicate the length of token sequences, </p>
<p>represents the set of base-level papers corresponding to the coarsened  ( ) .Finally, the probability distribution in Equation ( 11) is as,
𝑝 Θ,Φ (𝑌 (𝑙+1) 𝑢 | 𝑐 (𝑙 ) , 𝑞) = 𝑟 𝑖=1 𝑝 Θ,Φ (𝑦 𝑖 | 𝑦 &lt;𝑖 , [h 𝑐 ; h 𝜋 ; h 𝑞 ]),(15)
where [ ; ] indicates the concatenation of token embeddings before feeding them through transformer layers of LLM Θ .</p>
<p>Hierarchical Generation</p>
<p>Objective.The goal of taxonomy generation is to maximize the likelihood of generating coherent central concepts for all clusters across the hierarchical citation graph, thereby constructing the entire taxonomy tree.Formally, the objective is defined as:
L Gen = 𝐿 ∑︁ 𝑙=1 ∑︁ 𝑐 (𝑙 ) ∈ C (𝑙 ) log 𝑝 Θ,Φ (𝑌 (𝑙+1) 𝑢 | 𝑐 (𝑙 ) , 𝑞). (16)
By maximizing this hierarchical likelihood, the model generates semantically consistent and contextually relevant topics, ensuring coherence throughout the taxonomy tree.</p>
<p>Optimization for CGT</p>
<p>We jointly train the hierarchical citation graph clustering model and the taxonomy node verbalizer to ensure that the hierarchical clusters formed are meaningful from a textual perspective, and that the topics generated are coherent with the structural information embedded in the citation network.The final objective function is given as, min
Θ,Φ,𝜃,𝜙 L = L Gen (Θ, Φ) + 𝜆 • L HiCluster (𝜃, 𝜙), (17)
where  is a pre-defined parameter, L HiCluster is the loss function for hierarchical citation graph clustering, and L Gen is the loss function for taxonomy node verbalization.Since only a small portion of the citation graph has been labeled for hierarchical citation graphclustering (as it is challenging to collect labels that indicate which hierarchical citation graph cluster the conferences in a literature review belong to), and the learning dynamics of GNNs and LLMs differ significantly, directly training both models simultaneously can result in a complex and unstable optimization process.This leads to the GNN struggling to learn meaningful clusters early on, which subsequently hinders the LLM's ability to generate coherent topics.Therefore, we pre-train the hierarchical citation graph clustering module to simplify the optimization process for the LLM, allowing the LLM to focus solely on content generation.After pre-training the hierarchical clustering module (, ), we then jointly fine-tune them and the concept generator (Θ, Φ) to generate the central topic for each cluster.The fine-tuning is conducted using Low-Rank Adaptation (LoRA) [13], with MLP Φ serving as an adapter for graphs, enabling efficient adjustment of both models.</p>
<p>EXPERIMENT</p>
<p>To demonstrate the effectiveness of the proposed taxonomy learning method, we evaluate the quality of the generated taxonomies and further assess the model's ability to guide literature review generation.We first introduce the experimental settings in Section 5.1.Next, we present the performance of HiGTL on citation graphs in the computer science domain in Section 5.2.We then conduct ablation studies to demonstrate the contributions of Hierarchical Citation Graph Clustering and Taxonomy Node Verbalization in Section 5.3.Finally, we include a case study to highlight the strengths and limitations of our framework in Section 5.4.</p>
<p>Experiment Setting</p>
<p>5.1.1Datasets.We manually collected 518 high-quality literature review articles with clear taxonomies or well-defined structures from arXiv 1 across various computer science domains, with most published in the past three years.To evaluate taxonomy learning performance, we extracted taxonomy trees from these reviews and gathered the 1-hop citation graph for each review, representing citation relationships among its direct references.Additionally, we collected the 3-hop citation graph for each review to assess literature review generation based on HiGTL-generated taxonomies.Further details are provided in Appendix A.1.</p>
<p>1 https://arxiv.org/5.1.2Comparison methods.We compare our model with the following state-of-the-art taxonomy learning methods: HiExpan [36], which constructs taxonomies by recursively expanding seed sets using pattern-based and distributional embedding methods, followed by a global optimization module to refine the taxonomy structure for better coherence.TaxoGen [49], which generates topic taxonomies by embedding concept terms into a latent space and applying adaptive spherical clustering and local term embeddings to ensure fine-grained semantic distinctions at each hierarchical level.NetTaxo [35], which integrates text and network data for taxonomy construction by learning term embeddings from both contexts and employing instance-level motif selection to enhance hierarchical citation graph clustering and topic differentiation.Since our model leverages LLMs, we also include GPT-4o and Claude-3.5 as baselines for taxonomy generation.Implementation details are provided in Appendix A.2.</p>
<p>Evaluation Metrics.</p>
<p>To comprehensively evaluate the model's generation quality, we assess outputs from three perspectives: LLM-Score, Human Evaluation, and BertScore [50].Wang et al. demonstrated that LLM-based evaluations align closely with human preferences, so we leverage LLMs to rate the generated content.We also employ BertScore to measure the semantic similarity between generated texts and human-written references.However, human evaluation remains essential for accurately capturing human preferences and for cases without available human-written labels, ensuring a thorough assessment of the generated content, especially given LLM hallucinations.Details of the prompts used for LLM evaluation and the instructions for the human evaluation process are provided in Appendix A.3. 1 demonstrate that HiGTL outperforms all other models across multiple evaluation metrics, achieving the highest scores in Coverage (0.9357), Structure (0.9413), Relevance (0.8748), and the overall LLM Average (0.9173).It also leads in Human Evaluation metrics with the highest Adequacy (0.7150) and Validity (2.6700), reflecting its superior quality in both automated and human assessments.Additionally, HiGTL achieves the highest BertScore (0.8694), indicating strong alignment with reference taxonomies.Among large language models, GPT-4o shows competitive performance, particularly in Relevance (0.8828), where it slightly edges out HiGTL.However, it falls behind in other areas like Structure and Coverage, as well as human evaluations.Traditional taxonomy learning methods like NetTaxo and TaxoGen consistently lag behind HiGTL and LLM-based approaches due to their lack of ability to capture continuous, nuanced concepts.</p>
<p>Main Results</p>
<p>Taxonomy Learning Results. Results in Table</p>
<p>The potential of taxonomy-guided literature review generation led us to investigation an automatic literature review generation method based on our proposed HiGTL, called Hierarchical Taxonomy-Driven Automatic Literature Review Generation (HiReview).The next section presents the results of this literature review generation.</p>
<p>Review Generation Results</p>
<p>. We compare the reviews generated by our model with those written by human experts, zero-shot LLMs and naive RAG-based LLMs, i.e., GPT-4o and Claude-3.5.Zeroshot LLMs rely solely on their pretrained knowledge to generate literature review content, naive RAG-based LLMs utilize a simple BM25 retriever.The LLM backbone of AutoSurvey and HiReview are both GPT-4o for generating the literature review content.Additionally, we benchmark our model against the state-of-the-art review generator, AutoSurvey [47].Following AutoSurvey's scoring criteria, we evaluate the generated reviews based on coverage, structure, and relevance when selecting the best output and calculating the LLMScore.When evaluate the review content, instead of directly scoring the generation, we have LLMs compare the generated content with human-written reviews.</p>
<p>As shown in Table 2, Our method HiReview consistently outperforms the other review generation methods in all metrics.It excels across all LLMScore categories, with notably high structure (0.9484) and relevance (0.9428) scores.AutoSurvey employs a structured methodology that combines retrieval, outline generation, and section drafting, leading to superior content generation compared to naive systems (with average LLMScore of 0.8957).</p>
<p>Pure LLMs and naive RAG-based LLMs struggle with both stability and performance, which makes them unreliable for consistent literature review generation.AutoSurvey reduces this instability through prompt design and multi-output generation, achieving Structure ±0.05 and Relevance ±0.04-lower deviations than those of pure and naive RAG-based LLMs.HiReview, however, outperforms all other models across all metrics, with consistently low standard deviations.This demonstrates HiReview's superior stability and consistency in generating high-quality reviews.Its success can be attributed not only to HiReview's use of a graph-context-aware retrieval method but also to the taxonomy tree, which provides hierarchical context for domain-specific concerns within the large language model.An example of a generated literature review section is provided in Appendix B.</p>
<p>Ablation Study</p>
<p>Although we demonstrate the performance of HiReview (taxonomythen-generation) in terms of the quality of the literature review generated, we will assess the impact of various components on the performance of HiReview.</p>
<p>Impacts of Components.As shown in Table 4, we test three variants of HiReiview mode.HiReview w/o retrieval refers to the variant where the graph retrieval module is removed, and all papers in the citation network are used.A significant drop is observed across all metrics, particularly in Coverage (0.9163 → 0.6705) and Relevance (0.9428 → 0.7073).This indicates that the inclusion of unrelated papers introduces substantial noise, negatively impacting both the taxonomy tree generation (due to an excess of negative samples in hierarchical citation graph clustering) and content generation (where the noise hinders the creation of precise summaries).As a result, the quality opf generated summaries is even worse than those produced by zero-shot LLMs.</p>
<p>HiReview w/o clustering * bypasses the clustering process and directly uses the retrieved papers to generate the taxonomy.Instead of iteratively generating topics at each level, this variant creates the taxonomy in a single step.It is marked with * because the taxonomy node verbalizer in this case is an LLM i.e., GPT-4o, rather than a fine-tuned LLaMA, as the number of taxonomy trees is insufficient for effective fine-tuning.Although this variant performs worse than HiReview, it still delivers competitive performance, outperforming naive RAG-based LLMs and AutoSurvey.This suggests that the combination of the graph retrieval module and the taxonomy-thengeneration paradigm is more effective than naive retrieval-thengeneration and outline-then-generation approaches.</p>
<p>HiReview w/o taxonomy removes the hierarchical taxonomy tree generation module, instead using paper clusters to prompt the LLM for taxonomy node verbalization and review generation.The absence of a hierarchical taxonomy reduces the model's ability to leverage topic relations across different levels, leading to less organized and relevant content (Structure: 0.9484 → 0.8790 and Coverage: 0.9163 → 0.8612).Similar to HiReview w/o clustering * , HiReview w/o taxonomy does not use fine-tuned LLaMA, and its performance is more degraded.This indicates that when using pure LLM generation methods, generating a hierarchical taxonomy tree to guide content generation significantly enhances the quality of the output.Finally, we can answer remaining questions raised at the beginning of the Experiment Section.</p>
<p>We consider two baseline methods for the hierarchical citation graph clustering module: one that utilizes an LLM (i.e., GPT-4o) to cluster papers and another that applies -means, adjusting the number of clusters to represent different levels.However, neither method can be jointly trained with the taxonomy node verbalizer.Even disregarding the training requirement, the hierarchical nature of the literature review's taxonomy tree requires soft clustering at the initial layer and hard clustering at subsequent layers-an issue that no existing work addresses.As shown in Table ??, when  As shown in Table 2, HiReview, which incorporates a taxonomy tree, outperforms all other models, particularly in Structure, achieving a score of 0.9484.In contrast, AutoSurvey, which follows an outline-then-generation approach without hierarchical taxonomy, shows lower scores, such as a Structure score of 0.9122.The ablation study further supports this.When the taxonomy is removed, the structure score drops significantly (as in Table 4).This demonstrates that the taxonomy tree plays a critical role in organizing and guiding the content generation process, especially when maintaining a clear structure is crucial for a literature review.The taxonomy ensures more coherent and relevant summaries.Without providing the taxonomy tree, the generation loses its hierarchical guidance, leading to less structured and less comprehensive content.</p>
<p>Case Study</p>
<p>As shown in Figure 3, the taxonomy generated by HiGTL identifies Continual Text Classification as a key subtopic under Applications of Continual Learning, highlighting its relevance in areas like sentiment analysis, spam detection, and topic categorization.It emphasizes the need for models that can adapt incrementally as new data emerges.While HiGTL captures broad applications effectively, it may lack the finer distinctions found in human-crafted taxonomies (Figure 4), such as domain-specific classifications in legal, medical, or social media contexts.However, this generalized framework ensures semantic coherence and practical usability, providing a solid foundation that human experts can easily refine for tasks like literature review generation.</p>
<p>CONCLUSION</p>
<p>In this paper, we introduced HiGTL, a novel end-to-end framework for automatic taxonomy generation from citation graphs, guided by human-provided instructions or preferred topics.By integrating hierarchical citation graph clustering with taxonomy node verbalization, HiGTL effectively combines the structural relationships inherent in citation networks with the rich textual content of academic papers.Our joint optimization strategy ensures both structural coherence and semantic consistency across hierarchical levels.Extensive experiments on 518 citation graphs from high-quality literature reviews in computer science demonstrated that HiGTL generates meaningful taxonomies.Furthermore, we extended our framework to literature review generation with HiReview, showcasing its practical utility in guiding the creation of coherent, wellstructured reviews.Ablation studies confirmed the critical role of hierarchical clustering and taxonomy-guided generation in enhancing performance, while case studies illustrated the flexibility and usability of the generated taxonomies.HiGTL offers a scalable, robust solution for organizing scientific knowledge and supporting downstream tasks such as literature review generation, knowledge discovery, and trend identification.</p>
<p>A APPENDIX A.1 Dataset</p>
<p>We conduct experiments on 2-hop citation networks for each literature review paper rather than randomly collecting papers to construct a large citation network as a database.This is because using a large, random network complicates performance evaluation, making it difficult to assess both retrieval and clustering accuracy.Additionally, if related papers published after the literature review are present in the citation network, the retriever may include these newer papers in generating the review content.This would lead to an unfair comparison when evaluate the generated content against the human-written review.Citation Network Construction Process.For each literature review, we first extracted its references and constructed a citation tree, with the review paper as the root and its cited papers as the leaves.We then repeated this process for each cited paper, constructing a citation tree for each one.Next, we merged all these trees into a single, large citation network, consolidating any duplicate nodes.To automate this process, we used citation information from arXiv, which provides the LaTeX source code for each paper, including the bib or bbl files.If a paper was available on arXiv, we extracted its .texfile to obtain both the abstract and full text, using these as high-quality text features for the corresponding node.We used the arXiv API to automate this process.For papers not available on arXiv, we used the Google Scholar API to automatically retrieve the abstract, which we used as the text feature for the corresponding node in the citation network.Finally, we removed the node representing the original literature review, leaving 1-hop, 2-hop, and 3-hop citation networks for each review.The mutual citations among references form complex citation networks, averaging 6,658.4papers and 11,632.9edges, including isolated papers.</p>
<p>A.2 Implementation</p>
<p>All experiments were conducted on a Linux-based server equipped with 4 NVIDIA A10G GPUs.For 518 review papers, we successfully collected taxonomy trees with hierarchical citation graph clustering labels for 313 of the literature reviews.Of these, 200 reviews were used to train the hierarchical citation graph clustering and taxonomy generation module, while the remaining 118 were used to test the performance of the pre-trained hierarchical citation graph clustering model.318 reviews were reserved for a comprehensive evaluation of review content generation.The number of articles retrieved in retrieval phase was set to 200.The scaling factor  is set to 1. Pre-Train Hierarchical Citation Graph Clustering Module.GNN used in this paper is GAT [45] which has 2 layers with 4 heads per layer and a hidden dimension size of 1024.MLP  has 2 layers and a hidden dimension size of 1024.The edge connection threshold   is searched in [0.1, 0.2, 0.5, 0.8].The clustering model is trained for a maximum of 500 epochs using an early stop scheme with patience of 10.The learning rate is set to 0.001.The training batch is set to 512 and the test batch is 1024.Fine-Tuning.The LLM backbone is Llama-2-7b-hf.We adopt Low Rank Adaptation (LoRA) [13] for fine-tuning, and configure the LoRA parameters as follows: the dimension of the low-rank matrices is set to 8; the scaling factor is 16; the dropout rate is 0.05.For optimization, the AdamW optimizer is used.The initial learning rate is set to 1e-5 and the weight decay is 0.05.Each experiment is run for a maximum of 10 epochs, with a batch size of 4 for both training and testing.The MLP Φ has 2 layers and a hidden dimension size of 1024.LLMs.When calling the API, we set temperature as 1 and other parameters to default.The content generator is gpt-4o-2024-05-13 and the content judge is and claude-3-haiku-20240307.</p>
<p>A.3 Evaluation Metrics</p>
<p>We engaged two groups of human raters, each consisting of two professional PhD students in computer science, to evaluate the generated taxonomies and literature reviews using two scoring mechanisms.Each group scores 100 generated samples Given the need to compare model-generated outputs with those written by humans, we designed the evaluation criteria to be straightforward and focused.The two key metrics used are Adequacy and Validity: Adequacy is a binary metric where evaluators respond with "Yes" or "No" to the question: "Compared to the taxonomy written by humans, is this taxonomy suitable for learning this field?"This assesses whether the taxonomy is practically usable and meets the fundamental requirements for understanding the domain.Validity, on the other hand, is rated on a scale from 1 to 5, evaluating the degree to which the taxonomy accurately reflects factual information and represents the domain's conceptual structure.The scoring is defined as follows:</p>
<p>• 1 -Completely inaccurate, with significant factual errors or misrepresentations of the domain.This combination of metrics allows us to capture both the practical usability the factual correctness of the generated taxonomies, ensuring a comprehensive and nuanced evaluation from multiple perspectives.</p>
<p>For LLM evaluation, we assess the generated content from three perspectives: coverage, relevance, and structure, with each scored on a scale from 1 to 100.The specific prompts used for these evaluations are shown in Figure 5, Figure 6 and Figure 7.Under review as a conference paper at ICLR 2025 Prompt for evaluating the coverage of generated review.
918
Instruction: You are an expert in literature review evaluation, tasked with comparing a generated literature review to a human-written literature review on the topic of [TOPIC].</p>
<p>Human-Written Literature Review (Gold Standard):
[GROUND TRUTH REVIEW]
Generated Literature Review (To be evaluated):
[GENERATED REVIEW]
Evaluation Requirements: The human-written literature review serves as the gold standard.Your job is to assess how well the generated literature review compares in terms of coverage.Carefully analyze both reviews and provide a score.</p>
<p>Evaluate Coverage (Score out of 100).Assess how comprehensively the generated review covers the content from the human-written review.Consider:</p>
<p>• The percentage of key subtopics addressed from the human-written review.</p>
<p>• The depth of discussion for each subtopic compared to the human-written version.</p>
<p>• Balance between different areas within the topic as presented in the human-written review.</p>
<p>Only return only a numerical score out of 100, where 100 represents perfect alignment with the human-written literature review, without providing any additional information.Under review as a conference paper at ICLR 2025 Prompt for evaluating the relevance of generated review.</p>
<p>Instruction: You are an expert in literature review evaluation, tasked with comparing a generated literature review to a human-written literature review on the topic of [TOPIC].</p>
<p>Human-Written Literature Review (Gold Standard):
[GROUND TRUTH REVIEW]
Generated Literature Review (To be evaluated):
[GENERATED REVIEW]
Evaluation Requirements: The human-written literature review serves as the gold standard.Your job is to assess how well the generated literature review compares in terms of relevance.</p>
<p>Carefully analyze both reviews and provide a score.</p>
<p>Evaluate Relevance (Score out of 100).Evaluate how well the generated literature review aligns with the focus and content of the human-written literature review.Consider:</p>
<p>• Alignment with the core aspects of [TOPIC] as presented in the human-written literature review.</p>
<p>• Relevance of examples and case studies compared to those in the human-written literature review.</p>
<p>• Appropriateness for the target audience as demonstrated by the human-written literature review.</p>
<p>• Exclusion of tangential or unnecessary information not present in the human-written version.</p>
<p>Only return only a numerical score out of 100, where 100 represents perfect alignment with the human-written literature review, without providing any additional information.Under review as a conference paper at ICLR 2025
20
Prompt for evaluating the structure of generated review.</p>
<p>Instruction: You are an expert in literature review evaluation, tasked with comparing a generated literature review to a human-written literature review on the topic of [TOPIC].</p>
<p>Human-Written Literature Review (Gold Standard):
[GROUND TRUTH REVIEW]
Generated Literature Review (To be evaluated):
[GENERATED REVIEW]
Evaluation Requirements: The human-written literature review serves as the gold standard.Your job is to assess how well the generated literature review compares in terms of structure.Carefully analyze both reviews and provide a score.</p>
<p>Evaluate Structure (Score out of 100).Assess how well the generated literature review's organization and flow match that of the human-written literature review.Consider:</p>
<p>• Similarity in logical progression of ideas.</p>
<p>• Presence of a clear hierarchy of sections and subsections comparable to the humanwritten literature review.</p>
<p>• Appropriate use of headings and subheadings in line with the human-written version.</p>
<p>• Overall coherence within and between sections relative to the human-written literature review.</p>
<p>Only return only a numerical score out of 100, where 100 represents perfect alignment with the human-written literature review, without providing any additional information.</p>
<p>A.4 Investigation of Retrieval Models</p>
<p>We experimented with different retrieval models and strategies, testing two representative methods: the sparse retrieval model, BM25 [33], and the dense retrieval model, SentenceBert [31].In citation networks, neighbor information and the topological structure play a crucial role in retrieval, as papers on the same topic often cite each other.To assess the impact of using neighbor information, we applied two retrieval strategies for both models: one incorporating neighbor information as described in Section ?? (Retrieval w/ Neighbor) and the other excluding neighbor information (Retrieval w/o Neighbor).Given a topic (specifically, the title of a review paper), we retrieved papers related to this topic from the citation network and measured the accuracy by calculating how many of the retrieved papers appeared in the references of the corresponding literature review.The number of retrieved papers was not fixed, but matched the reference count for each review paper.As shown in Table 5, SentenceBert consistently outperforms BM25 across all scales when neighbor information is not used.For example, in the 1-hop merged case, SentenceBert achieves an accuracy of 0.5234, significantly higher than BM25's 0.3308.However, both methods show relatively low accuracy without neighbor information, and their performance declines as the size of citation networks increases, indicating that retrieving relevant papers becomes more challenging as the network expands.In contrast, BM25 significantly outperforms SentenceBert when neighbor information is utilized.For instance, in the 1-hop merged case, BM25 reaches an accuracy of 0.7445, while SentenceBert's accuracy drops sharply to 0.2602.BM25 maintains much higher accuracy across all scales with neighbor information.BM25, as a sparse retrieval model, relies on exact term matches, which is particularly advantageous in structured environments like citation networks, where specific terms (e.g., paper titles or keywords) are highly relevant.The inclusion of neighbor information allows BM25 to better capture relationships between papers by focusing on direct term matches in titles or citations.When neighbor information is introduced, the context around the target paper becomes more critical.BM25 effectively leverages this by prioritizing exact matches from neighboring papers, while SentenceBert, which focuses on semantic similarity, may lose precision when handling a broader context that includes less directly related papers.</p>
<p>Without graph-aware retrieval, methods like AutoSurvey must retrieve a large number of papers (e.g., 1200 in AutoSurvey) to avoid missing relevant ones.Retrieving fewer papers risks missing important content, while retrieving too many introduces noise from irrelevant papers.Graph-aware retrieval significantly alleviates this issue.The graph context-aware retrieval strategy we propose achieves more accurate results with fewer retrievals, i.e., 200, reducing irrelevant information and contributing to the superior generation performance of our model.Moreover, even when applied to large citation networks (such as 2-hop merged each containing over 200,000 papers), our method maintains stable retrieval accuracy, demonstrating HiReview's robustness across different citation network sizes.Additionally, we experimented with different retrieval strategies, such as retrieving based on both the title and abstract.We found that using only the title yielded the best results, while incorporating additional information like the abstract reduced retrieval performance.</p>
<p>A.5 The Choice of GNN</p>
<p>In addition to GAT [45], we also explored other GNNs as graph encoders, i.e., GCN [16] and Graph Transformer [38].The comparison results of these models on clustering are shown in Table 6.GAT achieves the highest performance across both levels, with an average score of 0.6761.It outperforms the other models at both Level 1 (0.7127) and Level 2 (0.6395), making it the most effective GNN for this task.This superior performance can likely be attributed to GAT's attention mechanism, which enables the model to assign varying importance weights to neighboring papers, allowing it to better capture the hierarchical structure of the graph.As a result, we selected GAT as the GNN backbone for HiReview.Distillation-based methods focus on transferring knowledge from previous models to new ones.Some approaches use contrastive ensemble distillation to enhance the discriminability between text categories [1], while others employ multi-strategy rebalancing to tackle class imbalance issues [2].Replay-based methods involve revisiting previously learned information to maintain performance on older tasks.These methods often integrate contrastive learning techniques to enhance the quality of replay samples [3] or use structured learning methods to improve the stability of the learning process [4].</p>
<p>B LITERATURE REVIEW GENERATION EXAMPLE</p>
<p>Regularization-based techniques aim to constrain the model's parameter updates to prevent forgetting.Some approaches focus on selectively updating a small subset of parameters to reduce computational burden [5], while others implement adaptive regularization processes that automatically adjust to balance retaining old knowledge with acquiring new information [6].Architecture-based methods modify the model structure to accommodate new tasks, with some proposing hierarchical overlay projections to optimize knowledge transfer across tasks [7] and others introducing mechanisms for parameter isolation to minimize interference between new and old tasks [8].</p>
<p>In addition to these primary approaches, researchers have explored data augmentation and generation techniques, using large language models to generate diverse and relevant samples for memory augmentation [9].Meta-learning frameworks, although less common in text classification, have also been employed to enable quick adaptation to new tasks while mitigating catastrophic forgetting [10].</p>
<p>The application of these techniques has been explored in specific domains, such as medical intent detection [11] and social media content analysis [12].As the field progresses, there is an increasing focus on developing methods that can handle few-shot scenarios [13] and multilingual settings [14].These advancements aim to make continual text classification more practical and adaptable in real-world applications where data may be scarce or linguistically diverse, ultimately contributing to the development of more robust and versatile large language models capable of continuous learning and adaptation.</p>
<p>Figure 1 :
1
Figure 1: The citation graph and its taxonomy exhibit a clear hierarchical mapping.</p>
<p>Figure 2 :
2
Figure 2: Overall framework of proposed hierarchical taxonomy generation.</p>
<p>Figure 4 :Figure 5 :
45
Figure 4: Taxonomy Tree Built by Human (Zheng et al., 2024).</p>
<p>16
16</p>
<p>Figure 3 :Figure 4 :
34
Figure 3: Taxonomy Tree Generated by HiGTL.</p>
<p>Figure 5 :
5
Figure 5: Taxonomy Tree Generated by HiReview.</p>
<p>Figure 4 :
4
Figure 4: Taxonomy Tree Built by Zheng et al..</p>
<p>Figure 5 :
5
Figure 5: Prompt used for evaluating coverage with LLMs.</p>
<p>Figure 6 :
6
Figure 6: Prompt used for evaluating relevance with LLMs.</p>
<p>Figure 7 :
7
Figure 7: Prompt used for evaluating structure with LLMs.</p>
<p>Table 1 :
1
Results of taxonomy generation.The best performance is in Bold.↑ indicates that a higher metric value corresponds to better model performance.
ModelCoverage ↑LLMScores↑ Structure ↑ Relevance ↑ Average ↑ Adequacy↑ Validity↑ Human Evaluation↑ BertScore ↑Large Language ModelsGPT-4o0.9132 ±0.015 0.8914 ±0.018 0.8828 ±0.0120.89580.64002.44000.8376 ±0.014Claude-3.5 0.8821 ±0.019 0.8734 ±0.016 0.8725 ±0.0140.87602.26002.60000.8319 ±0.013Taxonomy Learning MethodsHiExpan0.8427 ±0.021 0.8538 ±0.018 0.8219 ±0.0200.83950.46501.50000.8145 ±0.015TaxoGen0.8645 ±0.017 0.8812 ±0.014 0.8456 ±0.0190.86380.53001.56000.8249 ±0.012NetTaxo0.8734 ±0.016 0.8721 ±0.013 0.8508 ±0.0150.86540.51501.71000.8213 ±0.011HiGTL0.9357 ±0.012 0.9413 ±0.010 0.8748 ±0.0130.91730.71502.67000.8694 ±0.010</p>
<p>Table 2 :
2
Results of literature review generation by pure LLMs, naive RAG-based LLMs, AutoSurvey, and HiReview.The best performance is in Bold.LLM • indicates the LLM is provided with the top-500 relevant papers retrieved by BM25.↑ indicates that a higher metric value corresponds to better model performance.
ModelLLMScores↑ Coverage ↑ Structure ↑ Relevance ↑ Average ↑BertScore ↑Human-written1. 00001. 00001. 00001. 00001. 0000Pure LLMsGPT-4o0.7430 ±0.12 0.8346 ±0.11 0.8225 ±0.070.80000.8127 ±0.03Claude-3.50.7224 ±0.09 0.8116 ±0.14 0.7948 ±0.090.77630.8130 ±0.04Naive RAG-based LLMsGPT-4o •0.8219 ±0.13 0.8293 ±0.12 0.8972 ±0.060.84950.8094 ±0.03Claude-3.5 •0.8339 ±0.11 0.8215 ±0.13 0.9051 ±0.050.85350.8141 ±0.02Auto Review SystemAutoSurvey0.8646 ±0.07 0.9122 ±0.05 0.9093 ±0.040.89570.8256 ±0.02HiReview0.9163 ±0.03 0.9484 ±0.02 0.9428 ±0.010.93580.8449 ±0.02</p>
<p>Table 3 :
3
Ablation study results for HiReview.
ModelLLMScores↑ Coverage ↑ Structure ↑ Relevance ↑w/o retrieval0.67050.72160.7073w/o clustering  *0.88630.92610.9314w/o taxonomy0.86120.87900.9078</p>
<p>Table 4 :
4
Ablation study results for HiReview.
ModelLevel 1 Level 2 AverageHiCluserting0.7127 0.63950.6761K-means0.3723 0.42010.3962LLM clustering 0.4296 0.45180.4407considering the clustering task alone, both baselines underperformcompared to our hierarchical approach.</p>
<p>• 2 -Mostly inaccurate, capturing only a few correct facts but failing to represent the domain coherently.• 3 -Moderately accurate, containing some factual correctness but missing important concepts or relationships.• 4 -Mostly accurate, representing the domain well with minor factual inaccuracies or omissions.• 5 -Highly accurate, thoroughly reflecting the domain's factual structure with no noticeable errors.</p>
<p>Table 5 :
5
Results of retrieval on the citation network corresponding to 50 review papers.2-hop and 3-hop represent citation networks of review papers at different scales.1-hop (merged) refers to the 1-hop citation network of a review paper, merged with all other 1-hop citation networks, different review papers.Similarly, 2-hop (merged) is constructed by merging the 2-hop citation network of a review with all other 49 review papers.
Model1-hop (merged)Accuracy↑ 2-hop 2-hop (merged)3-hopRetrieval w/o NeighborBM250.33080.13750.09470.1014SentenceBert0.52340.17460.15210.1490Retrieval w NeighborBM250.74450.64350.59500.6179SentenceBert0.26020.27580.21810.2144</p>
<p>Table 6 :
6
Performance of different GNN on hierarchical citation graph clustering.
ModelLevel 1 Level 2 AverageGAT0.71270.63950.6761GCN0.67300.59630.6347Graph Transformer0.68110.60240.6418</p>
<p>Received 20 February 2007; revised 12 March 2009; accepted 5 June 2009Continual Text Classification is a crucial area of research in lifelong learning for large language models, addressing the challenge of adapting to new categories and evolving data distributions over time.This field is particularly relevant in real-world applications where new topics or intents may emerge, such as during global events that introduce novel categories.The techniques employed in continual text classification can be broadly categorized into several approaches, including distillation-based methods, replay-based methods, regularization-based methods, and architecture-based methods.
1080 1081A.7 EXAMPLE OF GENERATION1082 1083Example Generation for Continual Text Classification.1084108510861087108810891090109110921093109410951096109710981099110011011102110311041105110611071108110911101111111211131114111511161117111811191120112111221123112411251126
Acknowledgments.We thank all the researchers in the community for producing high-quality literature review papers.These articles are the basis of this study.
Hierarchical Graph Clustering using Node Pair Sampling. Thomas Bonald, Bertrand Charpentier, Alexis Galland, Alexandre Hollocou, MLG 2018-14th International Workshop on Mining and Learning with Graphs. 2018</p>
<p>Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen Huang, Yang Yang, arXiv:2310.05845Graphllm: Boosting graph reasoning ability of large language model. 2023. 2023arXiv preprint</p>
<p>Domain taxonomy learning from text: The subsumption method versus hierarchical clustering. Jeroen De Knijff, Flavius Frasincar, Frederik Hogenboom, Data &amp; Knowledge Engineering. 832013. 2013</p>
<p>The semantic growbag algorithm: Automatically deriving categorization systems. Jörg Diederich, Wolf-Tilo Balke, Research and Advanced Technology for Digital Libraries: 11th European Conference. Proceedings. Budapest, HungarySpringer2007. 2007. September 16-21, 200711</p>
<p>Taxolearn: A semantic approach to domain taxonomy learning. Emmanuelle-Anna Dietz, Damir Vandic, Flavius Frasincar, IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology. 12012. 2012IEEE</p>
<p>Parameterefficient fine-tuning of large-scale pre-trained language models. Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, Nature Machine Intelligence. 52023. 2023</p>
<p>Fully-dynamic hierarchical graph clustering using cut trees. Christof Doll, Tanja Hartmann, Dorothea Wagner, Workshop on Algorithms and Data Structures. Springer2011</p>
<p>Matrix completion with hierarchical graph side information. Adel Elmahdy, Junhyung Ahn, Changho Suh, Soheil Mohajer, Advances in neural information processing systems. 332020. 2020</p>
<p>Graph clustering and minimum cut trees. Gary William Flake, Robert E Tarjan, Kostas Tsioutsiouliklis, Internet Mathematics. 12004. 2004</p>
<p>Zeyu Han, Chao Gao, Jinyang Liu, Qian Sai, Zhang, arXiv:2403.14608Parameterefficient fine-tuning for large models: A comprehensive survey. 2024. 2024arXiv preprint</p>
<p>Automatic acquisition of hyponyms from large text corpora. A Marti, Hearst, The 14th international conference on computational linguistics. 19922COLING 1992</p>
<p>Parameter-efficient transfer learning for NLP. Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, International conference on machine learning. PMLR2019</p>
<p>J Edward, Yelong Hu, Phillip Shen, Zeyuan Wallis, Yuanzhi Allen-Zhu, Shean Li, Lu Wang, Weizhu Wang, Chen, arXiv:2106.09685Lora: Low-rank adaptation of large language models. 2021. 2021arXiv preprint</p>
<p>An unsupervised approach for learning a Chinese IS-A taxonomy from an unstructured corpus. Subin Huang, Xiangfeng Luo, Jing Huang, Yike Guo, Shengwei Gu, Knowledge-Based Systems. 1821048612019. 2019</p>
<p>Taxofinder: A graph-based approach for taxonomy learning. Yong-Bin Kang, Pari Delir Haghigh, Frada Burstein, IEEE Transactions on Knowledge and Data Engineering. 282015. 2015</p>
<p>Semi-supervised classification with graph convolutional networks. N Thomas, Max Kipf, Welling, arXiv:1609.029072016. 2016arXiv preprint</p>
<p>A semi-supervised method to learn and construct taxonomies using the web. Zornitsa Kozareva, Eduard Hovy, Proceedings of the 2010 conference on empirical methods in natural language processing. the 2010 conference on empirical methods in natural language processing2010</p>
<p>The Power of Scale for Parameter-Efficient Prompt Tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Prefix-Tuning: Optimizing Continuous Prompts for Generation. Lisa Xiang, Percy Li, Liang, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing. Long Papers. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing20211</p>
<p>Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, Graham Neubig, Comput. Surveys. 552023. 2023</p>
<p>Automatic taxonomy construction from keywords. Xueqing Liu, Yangqiu Song, Shixia Liu, Haixun Wang, Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. the 18th ACM SIGKDD international conference on Knowledge discovery and data mining2012</p>
<p>Can we soft prompt LLMs for graph learning tasks. Zheyuan Liu, Xiaoxin He, Yijun Tian, Nitesh V Chawla, Companion Proceedings of the ACM on Web Conference 2024. 2024</p>
<p>Taxonomy extraction using knowledge graph embeddings and hierarchical clustering. Félix Martel, Amal Zouaq, Proceedings of the 36th Annual ACM Symposium on Applied Computing. the 36th Annual ACM Symposium on Applied Computing2021</p>
<p>Espresso: Leveraging generic patterns for automatically harvesting semantic relations. Patrick Pantel, Marco Pennacchiotti, Proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the Association for Computational Linguistics. the 21st international conference on computational linguistics and 44th annual meeting of the Association for Computational Linguistics2006</p>
<p>Automatically labeling semantic classes. Patrick Pantel, Deepak Ravichandran, Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 2004. the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics: HLT-NAACL 20042004</p>
<p>Let your graph do the talking: Encoding structured data for llms. Bryan Perozzi, Bahare Fatemi, Dustin Zelle, Anton Tsitsulin, Mehran Kazemi, Rami Al-Rfou, Jonathan Halcrow, arXiv:2402.058622024. 2024arXiv preprint</p>
<p>Jonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé, Kyunghyun Cho, Iryna Gurevych, arXiv:2005.00247Adapterfusion: Non-destructive task composition for transfer learning. 2020. 2020arXiv preprint</p>
<p>Knowledge-rich word sense disambiguation rivaling supervised systems. Paolo Simone, Roberto Ponzetto, Navigli, Proceedings of the 48th annual meeting of the association for computational linguistics. the 48th annual meeting of the association for computational linguistics2010</p>
<p>Taxonomy induction based on a collaboratively built knowledge repository. Paolo Simone, Michael Ponzetto, Strube, Artificial Intelligence. 1752011. 2011</p>
<p>Automatic Taxonomy Construction for Eye Colors Data without Using Context Information. Jing Qiu, Yaqi Si, Zhihong Tian, 2018 IEEE Third International Conference on Data Science in Cyberspace (DSC). IEEE2018</p>
<p>Reimers, arXiv:1908.10084Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. 2019. 2019arXiv preprint</p>
<p>Learning concept hierarchies from textual resources for ontologies construction. Ana B Rios-Alvarado, Ivan Lopez-Arevalo, Victor J Sosa-Sosa, Expert Systems with Applications. 402013. 2013</p>
<p>The probabilistic relevance framework: BM25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 32009. 2009</p>
<p>DenGraph-HO: a densitybased hierarchical graph clustering algorithm. Nico Schlitter, Tanja Falkowski, Jörg Lässig, Expert Systems. 312014. 2014</p>
<p>Nettaxo: Automated topic taxonomy construction from text-rich network. Jingbo Shang, Xinyang Zhang, Liyuan Liu, Sha Li, Jiawei Han, Proceedings of the web conference 2020. the web conference 20202020</p>
<p>Hiexpan: Task-guided taxonomy construction by hierarchical tree expansion. Jiaming Shen, Zeqiu Wu, Dongming Lei, Chao Zhang, Xiang Ren, Michelle T Vanni, Brian M Sadler, Jiawei Han, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>A Web-scale system for scientific knowledge exploration. Zhihong Shen, Hao Ma, Kuansan Wang, Proceedings of ACL 2018, System Demonstrations. ACL 2018, System Demonstrations2018</p>
<p>Masked label prediction: Unified message passing model for semisupervised classification. Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjin Wang, Yu Sun, arXiv:2009.035092020. 2020arXiv preprint</p>
<p>End-to-End Supervised Hierarchical Graph Clustering for Speaker Diarization. Prachi Singh, Sriram Ganapathy, IEEE Transactions on Audio, Speech and Language Processing. 2025. 2025</p>
<p>Semantic taxonomy induction from heterogenous evidence. Rion Snow, Dan Jurafsky, Andrew Y Ng, Proceedings of the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics. the 21st international conference on computational linguistics and 44th annual meeting of the association for computational linguistics2006</p>
<p>Combining linguistic and statistical analysis to extract relations from web documents. Georgiana Fabian M Suchanek, Gerhard Ifrim, Weikum, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. the 12th ACM SIGKDD international conference on Knowledge discovery and data mining2006</p>
<p>Yago: a core of semantic knowledge. Gjergji Fabian M Suchanek, Gerhard Kasneci, Weikum, Proceedings of the 16th international conference on World Wide Web. the 16th international conference on World Wide Web2007</p>
<p>A fast and memory-efficient hierarchical graph clustering algorithm. László Szilágyi, Sándor Miklós Szilágyi, Béat Hirsbrunner, International Conference on Neural Information Processing. Springer2014</p>
<p>Ontolearn reloaded: A graph-based algorithm for taxonomy induction. Paola Velardi, Stefano Faralli, Roberto Navigli, Computational Linguistics. 392013. 2013</p>
<p>Graph attention networks. Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, Yoshua Bengio, arXiv:1710.109032017. 2017arXiv preprint</p>
<p>Probabilistic topic models for learning terminological ontologies. Wei Wang, Payam Mamaani Barnaghi, Andrzej Bargiela, IEEE Transactions on knowledge and Data engineering. 222009. 2009</p>
<p>Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, arXiv:2406.10252AutoSurvey: Large Language Models Can Automatically Write Surveys. 2024. 2024arXiv preprint</p>
<p>Parallel modularity-based community detection on large-scale graphs. Jianping Zeng, Hongfeng Yu, 2015 IEEE International Conference on Cluster Computing. IEEE. 2015</p>
<p>Taxogen: Unsupervised topic taxonomy construction by adaptive term embedding and clustering. Chao Zhang, Fangbo Tao, Xiusi Chen, Jiaming Shen, Meng Jiang, Brian Sadler, Michelle Vanni, Jiawei Han, Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining. the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining2018</p>
<p>BERTScore: Evaluating Text Generation with BERT. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, International Conference on Learning Representations. 2019</p>
<p>Junhao Zheng, Shengjie Qiu, Chengming Shi, Qianli Ma, arXiv:2406.06391Towards Lifelong Learning of Large Language Models: A Survey. 2024. 2024arXiv preprint</p>
<p>CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks. </p>
<p>Lifelong intent detection via multi-strategy rebalancing. </p>
<p>Alleviating Catastrophic Forgetting in Continual Text Classification from An Information Theoretic Perspective. Infocl, </p>
<p>Class Lifelong Learning for Intent Detection via Structure Consolidation Networks. </p>
<p>Parameter-efficient Continual Learning Framework in Industrial Real-time Text Classification System. </p>
<p>Continuous Learning for Domain Classification in Natural Language Understanding [7] HOP to the Next Tasks and Domains for Continual Learning in NLP. Hyperparameter-Free, </p>
<p>Prompts Can Play Lottery Tickets Well: Achieving Lifelong Information Extraction via Lottery Prompt Tuning. </p>
<p>Making Pre-trained Language Models Better Continual Few-Shot Relation Extractors. </p>
<p>Meta-Learning Improves Lifelong Relation Extraction. </p>
<p>Lifelong Learning of Hate Speech Classification on Social Media [13] Continual few-shot intent detection [14] Learning to solve NLP tasks in an incremental number of languages Figure 8: Example of a generated section on the topic of Continual Text Classification. </p>            </div>
        </div>

    </div>
</body>
</html>
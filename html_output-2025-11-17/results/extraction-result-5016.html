<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5016 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5016</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5016</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-107.html">extraction-schema-107</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <p><strong>Paper ID:</strong> paper-265150088</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2311.06736v3.pdf" target="_blank">Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning</a></p>
                <p><strong>Paper Abstract:</strong> Logical reasoning is a pivotal component in the field of artificial intelligence. Proof planning, particularly in contexts requiring the validation of explanation accuracy, continues to present challenges. The recent advancement of large language models (LLMs) has led to significant progress in natural language proof planning, evolving from one-stage generators to more complex three-stage systems that include additional searchers or verifiers. While these assisted methods improve the quality of generated results, they also introduce increased search efforts and computational costs. Furthermore, the generative process itself remains underexplored. In this study, we propose a stepwise decoding approach augmented by contrastive learning to address two common errors encountered during the LLM generator's decoding process. We fine-tune the language model using both vanilla and enhanced hard negatives to mitigate these decoding errors. Empirical results demonstrate the effectiveness of our strategy. Additionally, our further analysis reveals that even larger LLMs still struggle to generate rigorous logical chains.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5016.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5016.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ConDec</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Contrastive learning based stepwise Decoding (ConDec)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stepwise decoding method that fine-tunes a sequence-to-sequence LLM using contrastive learning over positive stepwise proofs and curated hard negatives (vanilla substitution negatives and enhanced negatives generated by a reasoner+checker) to reduce repetition and invalid-entailment decoding errors in natural-language proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large (finetuned with ConDec)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Flan-T5-Large encoder-decoder transformer (instruction-tuned T5 variant) fine-tuned further using stepwise MLE and a contrastive loss against hard negatives; training alternates MLE and contrastive objectives. Enhanced negatives are produced by an auxiliary reasoner (Flan-T5-Large) and filtered by Vera.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.8B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (EntailmentBank Tasks 1–3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate an entailment/proof tree that derives a hypothesis from a set of facts (leaf nodes). EntailmentBank has three settings: Task1 (no distractors), Task2 (15–20 distractor sentences added), Task3 (full-corpus retrieval from a 12K corpus). Evaluation measures correctness of leaves, steps, intermediate conclusions and overall AllCorrect.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stepwise fine-tuning with a combined MLE and contrastive loss (L = L_MLE + α L_contrastive-hard). Hard negatives: (1) vanilla substitution negatives (replace gold conclusion with a premise/context) and (2) enhanced negatives generated by a separate reasoner trained on proof steps and filtered by a plausibility checker (Vera). Contrastive projection pools encoder and decoder hidden states into embeddings and uses a temperature-scaled softmax contrastive loss.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Reported on EntailmentBank (Tables in paper). ConDec (vanilla hard negatives): Task1 Leaves F1 99.9 LeavesAllCorrect 98.2 Steps F1 55.7 StepsAllCorrect 42.1 Intermediates F1 72.3 IntermediatesAllCorrect 38.9 Overall AllCorrect 36.2. ConDec ⋆ (vanilla + enhanced negatives): Task1 Leaves F1 99.9 LeavesAllCorrect 98.2 Steps F1 57.3 StepsAllCorrect 43.2 Intermediates F1 72.9 IntermediatesAllCorrect 41.5 Overall AllCorrect 37.9. Task2 ConDec: Leaves F1 91.0 LeavesAllCorrect 59.1 Steps F1 50.2 StepsAllCorrect 36.5 Intermediates F1 70.3 IntermediatesAllCorrect 38.2 Overall AllCorrect 34.1. Task2 ConDec ⋆: Leaves F1 91.1 LeavesAllCorrect 60.6 Steps F1 50.7 StepsAllCorrect 37.4 Intermediates F1 70.7 IntermediatesAllCorrect 38.2 Overall AllCorrect 34.7. Task3 ConDec ⋆: Leaves F1 44.7 LeavesAllCorrect 9.4 Steps F1 11.7 StepsAllCorrect 7.1 Intermediates F1 42.3 IntermediatesAllCorrect 17.7 Overall AllCorrect 7.1.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>ConDec improves leaf and step accuracy more than intermediate-conclusion accuracy; performance on Task1 (no distractors) can degrade slightly when hard negatives are added (hard negatives can 'pull' the generator away from trivial correct steps); intermediate node generation (semantic inference) remains challenging. Overall AllCorrect rates remain modest (e.g., ~34% on Task2). Method depends on a curated human-annotated dataset (limited size) and on quality of reasoner/checker to produce useful enhanced negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>ConDec (one-stage, finetuning of generator) achieves comparable or better performance on Task2/Task3 than more complex two- or three-stage methods like MetGen* and NLProofs* while being more inference-efficient (no extra search/verifier at inference). ConDec outperforms zero-shot/k-shot prompting with closed-source LLMs (GPT-3/3.5/4 variants) on these metrics. However, some methods like MetGen/NLProofs that use search and verification are competitive and sometimes better on particular metrics; RLET (RL-based one-stage) can have advantages on Task3's long-path proofs.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Ablations show: (1) contrastive loss alone improves leaf accuracy but may reduce step accuracy; (2) adding vanilla hard negatives improves steps and overall; (3) adding enhanced hard negatives further improves coverage and performance across tasks; (4) selecting enhanced negatives randomly yields better intermediate accuracy than selecting via BM25 similarity; (5) model architecture matters: encoder-decoder (Flan-T5) outperforms decoder-only (LLaMA) in stepwise setting; (6) increasing model size (Flan-T5-XL 3B) yields similar performance to Flan-T5-Large, indicating size alone doesn't solve multi-hop proof planning; (7) ConDec reduces inference time significantly versus NLProofs (three-stage) on Task2 validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5016.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-Large (stepwise baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-Large (instruction-tuned T5, encoder-decoder)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder instruction-tuned T5 model used as the generator and finetuned for stepwise proof generation; serves as the backbone for ConDec experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-Large</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-decoder transformer (T5-Large) further instruction-finetuned (FLAN) and then finetuned on EntailmentBank proof steps; used both as generator and as the auxiliary reasoner for enhanced negatives.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>0.8B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Tasks 1–3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stepwise fine-tuning (MLE) over sampled subtrees of proof trees; used as baseline generator and starting point for ConDec contrastive training.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Baseline stepwise Flan-T5-Large (no contrastive loss) on Task2 (ablation Table 5): Leaves F1 90.7 LeavesAllCorrect 58.8 Steps F1 49.2 StepsAllCorrect 36.2 Intermediates F1 69.6 IntermediatesAllCorrect 36.8 Overall AllCorrect 33.5. Adding contrastive losses and hard negatives improved these numbers as reported in ConDec entries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Baseline generator often selects relevant premises but struggles to infer intermediate conclusions (semantic deduction), producing superfluous or imprecise intermediate steps; MLE-only stepwise training yields repetition and invalid-entailment errors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Outperforms decoder-only LLaMA variants in stepwise generation; when finetuned with ConDec, performance improves substantially versus MLE-only training; similar or better than larger Flan-T5-XL in these experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Contrastive loss and hard negatives provided incremental gains; MLE-only vs MLE+contrastive comparisons show improved discrimination between premises and conclusions when contrastive training is used.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5016.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Flan-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Flan-T5-XL (instruction-tuned T5 XL)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Larger encoder-decoder variant evaluated as a backbone for stepwise proof generation to test model-size effects.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Flan-T5-XL</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>A larger Flan-T5 encoder-decoder model (approx. 3B parameters in this work) used with stepwise training.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>3B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Task 2 ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stepwise MLE training (no contrastive in reported ablation) to evaluate model-size impact.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Flan-T5-XL (stepwise) achieved performance similar to Flan-T5-Large: Leaves F1 ~90.9 LeavesAllCorrect ~57.1 Steps F1 ~50.2 StepsAllCorrect ~36.5 Intermediates F1 ~68.8 IntermediatesAllCorrect ~35.9 Overall AllCorrect ~33.8 (ablation Table 5 indicates similar metrics).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Increasing size (to 3B) provided marginal gains; proof planning still challenging—indicates that model-size alone does not solve multi-hop deductive reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Similar performance to Flan-T5-Large when both trained stepwise; ConDec improvements primarily come from training strategy (contrastive + hard negatives) rather than sheer model size.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Model-size ablation shows limited benefit: Flan-T5-XL did not substantially outperform Flan-T5-Large on proof planning metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5016.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLaMA-3.2-1B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>LLaMA 3.2 (1.2B) decoder-only</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A decoder-only LLM evaluated for stepwise proof generation; found to struggle in distinguishing input premises from outputs and to produce shorter or confused outputs under stepwise training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>LLaMA-3.2-1B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Decoder-only architecture focused on next-token prediction; tested both without and with stepwise training on the proof generation task.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>1.2B</td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Task 2 ablation)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Stepwise MLE training and evaluation (decoder-only behavior examined).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Ablation Table 5 reports poor performance: LLaMA-3.2-1B (no stepwise) Leaves F1 19.5 LeavesAllCorrect 5.3 Steps F1 6.4 StepsAllCorrect 2.9 Intermediates F1 13.9 IntermediatesAllCorrect 5.3 Overall AllCorrect 2.7. With stepwise training, performance decreased further (Overall AllCorrect ~3.2).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Decoder-only architecture has trouble distinguishing inputs (premises) from outputs (generated steps) in stepwise setup, leading to short or truncated outputs and inability to reliably produce correct intermediate conclusions. Tends to imitate premises rather than deduce new conclusions.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Performs far worse than encoder-decoder Flan-T5 variants despite similar or larger parameter counts, indicating architecture matters for this sequence-to-sequence proof generation task.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Stepwise training hurt LLaMA's performance (produced shorter outputs). Case studies show LLaMA tends to output next-sentence imitation rather than proper deductions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e5016.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3 family (GPT-3 / GPT-3.5-turbo)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI GPT-3 and GPT-3.5-turbo (few-shot prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Closed-source GPT models evaluated via k-shot prompting (5-shot) and chain-of-thought paradigms for proof generation; showed substantially lower performance than finetuned ConDec.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-3, GPT-3.5-turbo</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only transformer LLMs accessed via prompting (5-shot); no finetuning reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Task 2 validation prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>5-shot in-context learning (vanilla prompting) and chain-of-thought (CoT) prompting; Select-Inference (SI) two-stage CoT variant also evaluated for GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Task2 validation (Table 6): GPT-3 (5-shot) Leaves F1 64.2 LeavesAllCorrect 15.3 Steps F1 17.6 StepsAllCorrect 12.3 Intermediates F1 53.6 IntermediatesAllCorrect 22.3 Overall AllCorrect 12.3. GPT-3.5-turbo (5-shot) had Leaves F1 61.9 LeavesAllCorrect 9.0 Steps F1 16.9 StepsAllCorrect 4.3 Intermediates F1 51.9 IntermediatesAllCorrect 15.1 Overall AllCorrect 3.7 (Table 6).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Prompted GPT-3/3.5 tends to produce irrelevant or inaccurate steps and often imitates premises rather than performing rigorous deduction; low AllCorrect metrics indicate poor full-tree correctness.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Finetuned ConDec models (Flan-T5 variants) substantially outperform these zero-/few-shot prompted GPT models on structured proof generation metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Case studies show GPT-3.5 often generates imitation-of-premises steps; GPT-4 (see separate entry) performs better under CoT but still lags finetuned methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e5016.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 (vanilla/CoT/SI)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 (evaluated with 5-shot, Chain-of-Thought, and Select-Inference)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-performing closed-source LLM evaluated via prompting strategies (5-shot, CoT, Select-Inference) for entailment-tree proof generation, performing best among prompted models but below finetuned ConDec.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large decoder-only LLM accessed via API; evaluated in 5-shot in-context setups with plain prompting, chain-of-thought (CoT), and Select-Inference (SI) two-stage CoT variant.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Task 2 validation prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Few-shot prompting with CoT and a Select-Inference variant that separates premise selection and conclusion inference.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>On Task2 validation (Table 6): GPT-4 (5-shot) Leaves F1 78.1 LeavesAllCorrect 32.6 Steps F1 30.2 StepsAllCorrect 22.5 Intermediates F1 63.9 IntermediatesAllCorrect 30.8 Overall AllCorrect 21.9. GPT-4 (CoT) improved slightly (Leaves F1 ~79.0, Overall ~22.5). GPT-4 (SI) had Leaves F1 ~79.1 and different step/selection behavior and an Overall ~13.9 in another reported setting.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Even GPT-4 makes premise-selection and step-generation errors; Select-Inference can introduce mistakes in the premise selection stage; overall AllCorrect remains well below finetuned ConDec.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>GPT-4 outperforms GPT-3/3.5 in prompting scenarios but is still outperformed by finetuned ConDec on full-tree correctness and step/intermediate accuracy. The paper reports ConDec ⋆ outperforms all closed-source LLM prompting variants across all listed metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Case studies: GPT-4 with CoT generates more accurate deductive steps than GPT-3.5; SI changes error modes (premise selection errors).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e5016.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Other closed LLMs (o1-mini, o1-preview, GPT4o-mini)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenAI variants and other closed-source LLMs (o1-mini, o1-preview, GPT‑4o‑mini)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Newer closed-source LLM variants tested with 5-shot prompting; show incremental improvements but still lag behind finetuned methods for proof planning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>o1-mini, o1-preview, GPT-4o-mini</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Variants of closed-source LLM APIs (smaller/preview versions) evaluated via few-shot prompting on EntailmentBank.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank Task2 prompting</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>5-shot prompting</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 6 reports: o1-preview (5-shot) Leaves F1 84.0 LeavesAllCorrect 43.3 Steps F1 38.7 StepsAllCorrect 28.0 Intermediates F1 67.8 IntermediatesAllCorrect 33.7 Overall AllCorrect 21.9. o1-mini and GPT-4o-mini show intermediate numbers (Overall AllCorrect in low teens).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Although improved relative to older GPT-3.5/GPT-3, still significantly worse than finetuned one-stage ConDec in terms of full-tree correctness and intermediate inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Some of these newer models close the gap on leaf/step F1 relative to GPT-4 but do not match the performance of finetuned ConDec.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Reported as validation comparisons; no internal ablation for these closed models beyond prompt strategy variants.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e5016.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EntailmentWriter</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>EntailmentWriter (Dalvi et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A one-stage finetuning approach previously used as a baseline for natural-language entailment-tree generation (proof generation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Explaining answers with entailment trees</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>EntailmentWriter</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>One-stage transformer-based model trained to produce full proof trees in a single generation pass (prior baseline).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Tasks 1–3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Single-shot finetuning to generate complete entailment trees (no stepwise correction); sometimes scaled to larger parameter sizes (11B) in comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3: Task1 EntailmentWriter: Leaves F1 98.7 LeavesAllCorrect 86.2 Steps F1 50.5 StepsAllCorrect 37.7 Intermediates F1 67.6 IntermediatesAllCorrect 36.2 Overall AllCorrect 33.5. EntailmentWriter (11B) improves on some numbers but remains below ConDec on several metrics for Task2/3.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Single-shot generation struggles with step-level correctness and intermediate inference compared to stepwise/training-with-verifier approaches; scaling parameters alone (11B) doesn't fully close the gap.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>ConDec (stepwise contrastive finetuning) outperforms EntailmentWriter on tasks with distractors (Task2) and full-corpus retrieval (Task3); stepwise correction matters.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper reports that stepwise methods (including ConDec) outperform single-shot EntailmentWriter even when EntailmentWriter is much larger (11B).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e5016.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MetGen</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MetGen (Hong et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A modular three-stage entailment-tree generation framework (module-based), using separate modules for single-step entailment and a controller to select promising steps, evaluated as a competitive multi-stage baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>METGEN: A module-based entailment tree generation framework for answer explanation</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>MetGen</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three-stage method combining modules for rule/fact selection and composition, with search/controller for selecting steps.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Tasks 1–3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Iterative single-step entailment with separate modules and a reasoning controller, plus search to compose trees.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3 reports mixed results: MetGen achieves perfect Leaf metrics on Task1 (100.0 Leaves F1/AllCorrect) and competitive step/intermediate numbers for some tasks, but is not uniformly superior across Tasks 1–3. On Task2 and Task3 MetGen is competitive but ConDec matches or exceeds it in many metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Multi-stage design increases search and computational costs at inference; complexity of pipeline may be a drawback compared to a single-stage ConDec at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>ConDec achieves comparable or better performance on Task2/Task3 while being more inference-efficient (one-stage).</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper positions MetGen as a representative three-stage baseline and reports runtime comparisons showing ConDec is faster at inference on Task2 validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e5016.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NLProofs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NLProofs (Yang et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A three-stage natural-language proof generation method that uses a trained verifier to guide search and select proof trees; used as a competitive baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Generating natural language proofs with verifier-guided search</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>NLProofs</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Three-stage pipeline: iterative generation of candidate steps, a verifier to score validity, and search to assemble a proof tree.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Tasks 1–3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Verifier-guided search: generate candidate steps and use a trained verifier to prune and score expansions, then search over expansions to select best proof tree.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Table 3: NLProofs achieves competitive performance (e.g., Task2 Leaves F1 ~90.3 LeavesAllCorrect ~58.8 Steps F1 ~47.2 StepsAllCorrect ~34.4 Intermediates F1 ~70.2 IntermediatesAllCorrect ~37.8 Overall AllCorrect ~33.3). ConDec matches or slightly exceeds NLProofs on several Task2 metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>Higher inference cost due to search and verification steps; computationally heavier than ConDec at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>ConDec attains comparable or better performance on Task2/Task3 while requiring less inference-time search/verification; NLProofs remains strong baseline especially when verifier quality is high.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Paper reports inference-time comparisons (Table 4) showing ConDec is faster than NLProofs on Task2 validation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5016.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e5016.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models being evaluated or improved for strict logical reasoning, including details of the models, logical reasoning tasks or benchmarks, methods or approaches used, performance results, limitations, and comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RLET</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RLET (Reinforcement Learning for Entailment Trees)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A one-stage reinforcement-learning based method that optimizes cumulative signals across entire entailment trees; discussed as alternative training signal for proof generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Rlet: A reinforcement learning based approach for explainable qa with entailment trees</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RLET</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>One-stage generator trained with cumulative (sequence-level) reward signals using reinforcement learning to optimize proof trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>logical_reasoning_task</strong></td>
                            <td>EntailmentBank proof generation (Tasks 1–3)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>See ConDec entry.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_approach</strong></td>
                            <td>Reinforcement learning with trajectory-level rewards rather than stepwise MLE; trains on full entailment trees.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Paper states RLET performs better on Task3 (full-corpus) relative to some finetuned methods because of its long-path training focus, but performs substantially lower than prior methods on Task1/Task2 under zero-shot retrieval settings.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_cases</strong></td>
                            <td>When supporting facts are noisy or differ from training distributions, RLET's advantage can diminish; overall performance lower than several finetuning-based baselines on Tasks 1 and 2 per the paper's discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison</strong></td>
                            <td>Contrasted with ConDec and other stepwise methods: RLET focuses on trajectory-level rewards and can be advantageous on long-path tasks (Task3) but underperforms on Task1/Task2 in the paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis_results</strong></td>
                            <td>Discussion in appendix compares training paradigms (one-stage RL vs stepwise MLE + contrastive) and highlights trade-offs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning', 'publication_date_yy_mm': '2023-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Explaining answers with entailment trees <em>(Rating: 2)</em></li>
                <li>ProofWriter: Generating implications, proofs, and abductive statements over natural language <em>(Rating: 2)</em></li>
                <li>METGEN: A module-based entailment tree generation framework for answer explanation <em>(Rating: 2)</em></li>
                <li>Generating natural language proofs with verifier-guided search <em>(Rating: 2)</em></li>
                <li>Rlet: A reinforcement learning based approach for explainable qa with entailment trees <em>(Rating: 2)</em></li>
                <li>Selection-inference: Exploiting large language models for interpretable logical reasoning <em>(Rating: 2)</em></li>
                <li>Vera: A general-purpose plausibility estimation model for commonsense statements <em>(Rating: 1)</em></li>
                <li>Contrastive neural text generation <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5016",
    "paper_id": "paper-265150088",
    "extraction_schema_id": "extraction-schema-107",
    "extracted_data": [
        {
            "name_short": "ConDec",
            "name_full": "Contrastive learning based stepwise Decoding (ConDec)",
            "brief_description": "A stepwise decoding method that fine-tunes a sequence-to-sequence LLM using contrastive learning over positive stepwise proofs and curated hard negatives (vanilla substitution negatives and enhanced negatives generated by a reasoner+checker) to reduce repetition and invalid-entailment decoding errors in natural-language proof generation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large (finetuned with ConDec)",
            "model_description": "Flan-T5-Large encoder-decoder transformer (instruction-tuned T5 variant) fine-tuned further using stepwise MLE and a contrastive loss against hard negatives; training alternates MLE and contrastive objectives. Enhanced negatives are produced by an auxiliary reasoner (Flan-T5-Large) and filtered by Vera.",
            "model_size": "0.8B",
            "logical_reasoning_task": "EntailmentBank proof generation (EntailmentBank Tasks 1–3)",
            "task_description": "Generate an entailment/proof tree that derives a hypothesis from a set of facts (leaf nodes). EntailmentBank has three settings: Task1 (no distractors), Task2 (15–20 distractor sentences added), Task3 (full-corpus retrieval from a 12K corpus). Evaluation measures correctness of leaves, steps, intermediate conclusions and overall AllCorrect.",
            "method_or_approach": "Stepwise fine-tuning with a combined MLE and contrastive loss (L = L_MLE + α L_contrastive-hard). Hard negatives: (1) vanilla substitution negatives (replace gold conclusion with a premise/context) and (2) enhanced negatives generated by a separate reasoner trained on proof steps and filtered by a plausibility checker (Vera). Contrastive projection pools encoder and decoder hidden states into embeddings and uses a temperature-scaled softmax contrastive loss.",
            "performance": "Reported on EntailmentBank (Tables in paper). ConDec (vanilla hard negatives): Task1 Leaves F1 99.9 LeavesAllCorrect 98.2 Steps F1 55.7 StepsAllCorrect 42.1 Intermediates F1 72.3 IntermediatesAllCorrect 38.9 Overall AllCorrect 36.2. ConDec ⋆ (vanilla + enhanced negatives): Task1 Leaves F1 99.9 LeavesAllCorrect 98.2 Steps F1 57.3 StepsAllCorrect 43.2 Intermediates F1 72.9 IntermediatesAllCorrect 41.5 Overall AllCorrect 37.9. Task2 ConDec: Leaves F1 91.0 LeavesAllCorrect 59.1 Steps F1 50.2 StepsAllCorrect 36.5 Intermediates F1 70.3 IntermediatesAllCorrect 38.2 Overall AllCorrect 34.1. Task2 ConDec ⋆: Leaves F1 91.1 LeavesAllCorrect 60.6 Steps F1 50.7 StepsAllCorrect 37.4 Intermediates F1 70.7 IntermediatesAllCorrect 38.2 Overall AllCorrect 34.7. Task3 ConDec ⋆: Leaves F1 44.7 LeavesAllCorrect 9.4 Steps F1 11.7 StepsAllCorrect 7.1 Intermediates F1 42.3 IntermediatesAllCorrect 17.7 Overall AllCorrect 7.1.",
            "limitations_or_failure_cases": "ConDec improves leaf and step accuracy more than intermediate-conclusion accuracy; performance on Task1 (no distractors) can degrade slightly when hard negatives are added (hard negatives can 'pull' the generator away from trivial correct steps); intermediate node generation (semantic inference) remains challenging. Overall AllCorrect rates remain modest (e.g., ~34% on Task2). Method depends on a curated human-annotated dataset (limited size) and on quality of reasoner/checker to produce useful enhanced negatives.",
            "comparison": "ConDec (one-stage, finetuning of generator) achieves comparable or better performance on Task2/Task3 than more complex two- or three-stage methods like MetGen* and NLProofs* while being more inference-efficient (no extra search/verifier at inference). ConDec outperforms zero-shot/k-shot prompting with closed-source LLMs (GPT-3/3.5/4 variants) on these metrics. However, some methods like MetGen/NLProofs that use search and verification are competitive and sometimes better on particular metrics; RLET (RL-based one-stage) can have advantages on Task3's long-path proofs.",
            "ablation_or_analysis_results": "Ablations show: (1) contrastive loss alone improves leaf accuracy but may reduce step accuracy; (2) adding vanilla hard negatives improves steps and overall; (3) adding enhanced hard negatives further improves coverage and performance across tasks; (4) selecting enhanced negatives randomly yields better intermediate accuracy than selecting via BM25 similarity; (5) model architecture matters: encoder-decoder (Flan-T5) outperforms decoder-only (LLaMA) in stepwise setting; (6) increasing model size (Flan-T5-XL 3B) yields similar performance to Flan-T5-Large, indicating size alone doesn't solve multi-hop proof planning; (7) ConDec reduces inference time significantly versus NLProofs (three-stage) on Task2 validation.",
            "uuid": "e5016.0",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Flan-T5-Large (stepwise baseline)",
            "name_full": "Flan-T5-Large (instruction-tuned T5, encoder-decoder)",
            "brief_description": "An encoder-decoder instruction-tuned T5 model used as the generator and finetuned for stepwise proof generation; serves as the backbone for ConDec experiments.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-Large",
            "model_description": "Encoder-decoder transformer (T5-Large) further instruction-finetuned (FLAN) and then finetuned on EntailmentBank proof steps; used both as generator and as the auxiliary reasoner for enhanced negatives.",
            "model_size": "0.8B",
            "logical_reasoning_task": "EntailmentBank proof generation (Tasks 1–3)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "Stepwise fine-tuning (MLE) over sampled subtrees of proof trees; used as baseline generator and starting point for ConDec contrastive training.",
            "performance": "Baseline stepwise Flan-T5-Large (no contrastive loss) on Task2 (ablation Table 5): Leaves F1 90.7 LeavesAllCorrect 58.8 Steps F1 49.2 StepsAllCorrect 36.2 Intermediates F1 69.6 IntermediatesAllCorrect 36.8 Overall AllCorrect 33.5. Adding contrastive losses and hard negatives improved these numbers as reported in ConDec entries.",
            "limitations_or_failure_cases": "Baseline generator often selects relevant premises but struggles to infer intermediate conclusions (semantic deduction), producing superfluous or imprecise intermediate steps; MLE-only stepwise training yields repetition and invalid-entailment errors.",
            "comparison": "Outperforms decoder-only LLaMA variants in stepwise generation; when finetuned with ConDec, performance improves substantially versus MLE-only training; similar or better than larger Flan-T5-XL in these experiments.",
            "ablation_or_analysis_results": "Contrastive loss and hard negatives provided incremental gains; MLE-only vs MLE+contrastive comparisons show improved discrimination between premises and conclusions when contrastive training is used.",
            "uuid": "e5016.1",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Flan-T5-XL",
            "name_full": "Flan-T5-XL (instruction-tuned T5 XL)",
            "brief_description": "Larger encoder-decoder variant evaluated as a backbone for stepwise proof generation to test model-size effects.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Flan-T5-XL",
            "model_description": "A larger Flan-T5 encoder-decoder model (approx. 3B parameters in this work) used with stepwise training.",
            "model_size": "3B",
            "logical_reasoning_task": "EntailmentBank proof generation (Task 2 ablation)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "Stepwise MLE training (no contrastive in reported ablation) to evaluate model-size impact.",
            "performance": "Flan-T5-XL (stepwise) achieved performance similar to Flan-T5-Large: Leaves F1 ~90.9 LeavesAllCorrect ~57.1 Steps F1 ~50.2 StepsAllCorrect ~36.5 Intermediates F1 ~68.8 IntermediatesAllCorrect ~35.9 Overall AllCorrect ~33.8 (ablation Table 5 indicates similar metrics).",
            "limitations_or_failure_cases": "Increasing size (to 3B) provided marginal gains; proof planning still challenging—indicates that model-size alone does not solve multi-hop deductive reasoning.",
            "comparison": "Similar performance to Flan-T5-Large when both trained stepwise; ConDec improvements primarily come from training strategy (contrastive + hard negatives) rather than sheer model size.",
            "ablation_or_analysis_results": "Model-size ablation shows limited benefit: Flan-T5-XL did not substantially outperform Flan-T5-Large on proof planning metrics.",
            "uuid": "e5016.2",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "LLaMA-3.2-1B",
            "name_full": "LLaMA 3.2 (1.2B) decoder-only",
            "brief_description": "A decoder-only LLM evaluated for stepwise proof generation; found to struggle in distinguishing input premises from outputs and to produce shorter or confused outputs under stepwise training.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "LLaMA-3.2-1B",
            "model_description": "Decoder-only architecture focused on next-token prediction; tested both without and with stepwise training on the proof generation task.",
            "model_size": "1.2B",
            "logical_reasoning_task": "EntailmentBank proof generation (Task 2 ablation)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "Stepwise MLE training and evaluation (decoder-only behavior examined).",
            "performance": "Ablation Table 5 reports poor performance: LLaMA-3.2-1B (no stepwise) Leaves F1 19.5 LeavesAllCorrect 5.3 Steps F1 6.4 StepsAllCorrect 2.9 Intermediates F1 13.9 IntermediatesAllCorrect 5.3 Overall AllCorrect 2.7. With stepwise training, performance decreased further (Overall AllCorrect ~3.2).",
            "limitations_or_failure_cases": "Decoder-only architecture has trouble distinguishing inputs (premises) from outputs (generated steps) in stepwise setup, leading to short or truncated outputs and inability to reliably produce correct intermediate conclusions. Tends to imitate premises rather than deduce new conclusions.",
            "comparison": "Performs far worse than encoder-decoder Flan-T5 variants despite similar or larger parameter counts, indicating architecture matters for this sequence-to-sequence proof generation task.",
            "ablation_or_analysis_results": "Stepwise training hurt LLaMA's performance (produced shorter outputs). Case studies show LLaMA tends to output next-sentence imitation rather than proper deductions.",
            "uuid": "e5016.3",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-3 family (GPT-3 / GPT-3.5-turbo)",
            "name_full": "OpenAI GPT-3 and GPT-3.5-turbo (few-shot prompting)",
            "brief_description": "Closed-source GPT models evaluated via k-shot prompting (5-shot) and chain-of-thought paradigms for proof generation; showed substantially lower performance than finetuned ConDec.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-3, GPT-3.5-turbo",
            "model_description": "Large decoder-only transformer LLMs accessed via prompting (5-shot); no finetuning reported in this paper.",
            "model_size": null,
            "logical_reasoning_task": "EntailmentBank proof generation (Task 2 validation prompting)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "5-shot in-context learning (vanilla prompting) and chain-of-thought (CoT) prompting; Select-Inference (SI) two-stage CoT variant also evaluated for GPT-4.",
            "performance": "On Task2 validation (Table 6): GPT-3 (5-shot) Leaves F1 64.2 LeavesAllCorrect 15.3 Steps F1 17.6 StepsAllCorrect 12.3 Intermediates F1 53.6 IntermediatesAllCorrect 22.3 Overall AllCorrect 12.3. GPT-3.5-turbo (5-shot) had Leaves F1 61.9 LeavesAllCorrect 9.0 Steps F1 16.9 StepsAllCorrect 4.3 Intermediates F1 51.9 IntermediatesAllCorrect 15.1 Overall AllCorrect 3.7 (Table 6).",
            "limitations_or_failure_cases": "Prompted GPT-3/3.5 tends to produce irrelevant or inaccurate steps and often imitates premises rather than performing rigorous deduction; low AllCorrect metrics indicate poor full-tree correctness.",
            "comparison": "Finetuned ConDec models (Flan-T5 variants) substantially outperform these zero-/few-shot prompted GPT models on structured proof generation metrics.",
            "ablation_or_analysis_results": "Case studies show GPT-3.5 often generates imitation-of-premises steps; GPT-4 (see separate entry) performs better under CoT but still lags finetuned methods.",
            "uuid": "e5016.4",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "GPT-4 (vanilla/CoT/SI)",
            "name_full": "GPT-4 (evaluated with 5-shot, Chain-of-Thought, and Select-Inference)",
            "brief_description": "A high-performing closed-source LLM evaluated via prompting strategies (5-shot, CoT, Select-Inference) for entailment-tree proof generation, performing best among prompted models but below finetuned ConDec.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "GPT-4",
            "model_description": "Large decoder-only LLM accessed via API; evaluated in 5-shot in-context setups with plain prompting, chain-of-thought (CoT), and Select-Inference (SI) two-stage CoT variant.",
            "model_size": null,
            "logical_reasoning_task": "EntailmentBank proof generation (Task 2 validation prompting)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "Few-shot prompting with CoT and a Select-Inference variant that separates premise selection and conclusion inference.",
            "performance": "On Task2 validation (Table 6): GPT-4 (5-shot) Leaves F1 78.1 LeavesAllCorrect 32.6 Steps F1 30.2 StepsAllCorrect 22.5 Intermediates F1 63.9 IntermediatesAllCorrect 30.8 Overall AllCorrect 21.9. GPT-4 (CoT) improved slightly (Leaves F1 ~79.0, Overall ~22.5). GPT-4 (SI) had Leaves F1 ~79.1 and different step/selection behavior and an Overall ~13.9 in another reported setting.",
            "limitations_or_failure_cases": "Even GPT-4 makes premise-selection and step-generation errors; Select-Inference can introduce mistakes in the premise selection stage; overall AllCorrect remains well below finetuned ConDec.",
            "comparison": "GPT-4 outperforms GPT-3/3.5 in prompting scenarios but is still outperformed by finetuned ConDec on full-tree correctness and step/intermediate accuracy. The paper reports ConDec ⋆ outperforms all closed-source LLM prompting variants across all listed metrics.",
            "ablation_or_analysis_results": "Case studies: GPT-4 with CoT generates more accurate deductive steps than GPT-3.5; SI changes error modes (premise selection errors).",
            "uuid": "e5016.5",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "Other closed LLMs (o1-mini, o1-preview, GPT4o-mini)",
            "name_full": "OpenAI variants and other closed-source LLMs (o1-mini, o1-preview, GPT‑4o‑mini)",
            "brief_description": "Newer closed-source LLM variants tested with 5-shot prompting; show incremental improvements but still lag behind finetuned methods for proof planning.",
            "citation_title": "",
            "mention_or_use": "mention",
            "model_name": "o1-mini, o1-preview, GPT-4o-mini",
            "model_description": "Variants of closed-source LLM APIs (smaller/preview versions) evaluated via few-shot prompting on EntailmentBank.",
            "model_size": null,
            "logical_reasoning_task": "EntailmentBank Task2 prompting",
            "task_description": "See ConDec entry.",
            "method_or_approach": "5-shot prompting",
            "performance": "Table 6 reports: o1-preview (5-shot) Leaves F1 84.0 LeavesAllCorrect 43.3 Steps F1 38.7 StepsAllCorrect 28.0 Intermediates F1 67.8 IntermediatesAllCorrect 33.7 Overall AllCorrect 21.9. o1-mini and GPT-4o-mini show intermediate numbers (Overall AllCorrect in low teens).",
            "limitations_or_failure_cases": "Although improved relative to older GPT-3.5/GPT-3, still significantly worse than finetuned one-stage ConDec in terms of full-tree correctness and intermediate inference.",
            "comparison": "Some of these newer models close the gap on leaf/step F1 relative to GPT-4 but do not match the performance of finetuned ConDec.",
            "ablation_or_analysis_results": "Reported as validation comparisons; no internal ablation for these closed models beyond prompt strategy variants.",
            "uuid": "e5016.6",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "EntailmentWriter",
            "name_full": "EntailmentWriter (Dalvi et al., 2021)",
            "brief_description": "A one-stage finetuning approach previously used as a baseline for natural-language entailment-tree generation (proof generation).",
            "citation_title": "Explaining answers with entailment trees",
            "mention_or_use": "mention",
            "model_name": "EntailmentWriter",
            "model_description": "One-stage transformer-based model trained to produce full proof trees in a single generation pass (prior baseline).",
            "model_size": null,
            "logical_reasoning_task": "EntailmentBank proof generation (Tasks 1–3)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "Single-shot finetuning to generate complete entailment trees (no stepwise correction); sometimes scaled to larger parameter sizes (11B) in comparisons.",
            "performance": "Table 3: Task1 EntailmentWriter: Leaves F1 98.7 LeavesAllCorrect 86.2 Steps F1 50.5 StepsAllCorrect 37.7 Intermediates F1 67.6 IntermediatesAllCorrect 36.2 Overall AllCorrect 33.5. EntailmentWriter (11B) improves on some numbers but remains below ConDec on several metrics for Task2/3.",
            "limitations_or_failure_cases": "Single-shot generation struggles with step-level correctness and intermediate inference compared to stepwise/training-with-verifier approaches; scaling parameters alone (11B) doesn't fully close the gap.",
            "comparison": "ConDec (stepwise contrastive finetuning) outperforms EntailmentWriter on tasks with distractors (Task2) and full-corpus retrieval (Task3); stepwise correction matters.",
            "ablation_or_analysis_results": "Paper reports that stepwise methods (including ConDec) outperform single-shot EntailmentWriter even when EntailmentWriter is much larger (11B).",
            "uuid": "e5016.7",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "MetGen",
            "name_full": "MetGen (Hong et al., 2022)",
            "brief_description": "A modular three-stage entailment-tree generation framework (module-based), using separate modules for single-step entailment and a controller to select promising steps, evaluated as a competitive multi-stage baseline.",
            "citation_title": "METGEN: A module-based entailment tree generation framework for answer explanation",
            "mention_or_use": "mention",
            "model_name": "MetGen",
            "model_description": "Three-stage method combining modules for rule/fact selection and composition, with search/controller for selecting steps.",
            "model_size": null,
            "logical_reasoning_task": "EntailmentBank proof generation (Tasks 1–3)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "Iterative single-step entailment with separate modules and a reasoning controller, plus search to compose trees.",
            "performance": "Table 3 reports mixed results: MetGen achieves perfect Leaf metrics on Task1 (100.0 Leaves F1/AllCorrect) and competitive step/intermediate numbers for some tasks, but is not uniformly superior across Tasks 1–3. On Task2 and Task3 MetGen is competitive but ConDec matches or exceeds it in many metrics.",
            "limitations_or_failure_cases": "Multi-stage design increases search and computational costs at inference; complexity of pipeline may be a drawback compared to a single-stage ConDec at inference time.",
            "comparison": "ConDec achieves comparable or better performance on Task2/Task3 while being more inference-efficient (one-stage).",
            "ablation_or_analysis_results": "Paper positions MetGen as a representative three-stage baseline and reports runtime comparisons showing ConDec is faster at inference on Task2 validation.",
            "uuid": "e5016.8",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "NLProofs",
            "name_full": "NLProofs (Yang et al., 2022)",
            "brief_description": "A three-stage natural-language proof generation method that uses a trained verifier to guide search and select proof trees; used as a competitive baseline.",
            "citation_title": "Generating natural language proofs with verifier-guided search",
            "mention_or_use": "mention",
            "model_name": "NLProofs",
            "model_description": "Three-stage pipeline: iterative generation of candidate steps, a verifier to score validity, and search to assemble a proof tree.",
            "model_size": null,
            "logical_reasoning_task": "EntailmentBank proof generation (Tasks 1–3)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "Verifier-guided search: generate candidate steps and use a trained verifier to prune and score expansions, then search over expansions to select best proof tree.",
            "performance": "Table 3: NLProofs achieves competitive performance (e.g., Task2 Leaves F1 ~90.3 LeavesAllCorrect ~58.8 Steps F1 ~47.2 StepsAllCorrect ~34.4 Intermediates F1 ~70.2 IntermediatesAllCorrect ~37.8 Overall AllCorrect ~33.3). ConDec matches or slightly exceeds NLProofs on several Task2 metrics.",
            "limitations_or_failure_cases": "Higher inference cost due to search and verification steps; computationally heavier than ConDec at inference.",
            "comparison": "ConDec attains comparable or better performance on Task2/Task3 while requiring less inference-time search/verification; NLProofs remains strong baseline especially when verifier quality is high.",
            "ablation_or_analysis_results": "Paper reports inference-time comparisons (Table 4) showing ConDec is faster than NLProofs on Task2 validation.",
            "uuid": "e5016.9",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        },
        {
            "name_short": "RLET",
            "name_full": "RLET (Reinforcement Learning for Entailment Trees)",
            "brief_description": "A one-stage reinforcement-learning based method that optimizes cumulative signals across entire entailment trees; discussed as alternative training signal for proof generation.",
            "citation_title": "Rlet: A reinforcement learning based approach for explainable qa with entailment trees",
            "mention_or_use": "mention",
            "model_name": "RLET",
            "model_description": "One-stage generator trained with cumulative (sequence-level) reward signals using reinforcement learning to optimize proof trajectories.",
            "model_size": null,
            "logical_reasoning_task": "EntailmentBank proof generation (Tasks 1–3)",
            "task_description": "See ConDec entry.",
            "method_or_approach": "Reinforcement learning with trajectory-level rewards rather than stepwise MLE; trains on full entailment trees.",
            "performance": "Paper states RLET performs better on Task3 (full-corpus) relative to some finetuned methods because of its long-path training focus, but performs substantially lower than prior methods on Task1/Task2 under zero-shot retrieval settings.",
            "limitations_or_failure_cases": "When supporting facts are noisy or differ from training distributions, RLET's advantage can diminish; overall performance lower than several finetuning-based baselines on Tasks 1 and 2 per the paper's discussion.",
            "comparison": "Contrasted with ConDec and other stepwise methods: RLET focuses on trajectory-level rewards and can be advantageous on long-path tasks (Task3) but underperforms on Task1/Task2 in the paper's experiments.",
            "ablation_or_analysis_results": "Discussion in appendix compares training paradigms (one-stage RL vs stepwise MLE + contrastive) and highlights trade-offs.",
            "uuid": "e5016.10",
            "source_info": {
                "paper_title": "Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning",
                "publication_date_yy_mm": "2023-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Explaining answers with entailment trees",
            "rating": 2,
            "sanitized_title": "explaining_answers_with_entailment_trees"
        },
        {
            "paper_title": "ProofWriter: Generating implications, proofs, and abductive statements over natural language",
            "rating": 2,
            "sanitized_title": "proofwriter_generating_implications_proofs_and_abductive_statements_over_natural_language"
        },
        {
            "paper_title": "METGEN: A module-based entailment tree generation framework for answer explanation",
            "rating": 2,
            "sanitized_title": "metgen_a_modulebased_entailment_tree_generation_framework_for_answer_explanation"
        },
        {
            "paper_title": "Generating natural language proofs with verifier-guided search",
            "rating": 2,
            "sanitized_title": "generating_natural_language_proofs_with_verifierguided_search"
        },
        {
            "paper_title": "Rlet: A reinforcement learning based approach for explainable qa with entailment trees",
            "rating": 2,
            "sanitized_title": "rlet_a_reinforcement_learning_based_approach_for_explainable_qa_with_entailment_trees"
        },
        {
            "paper_title": "Selection-inference: Exploiting large language models for interpretable logical reasoning",
            "rating": 2,
            "sanitized_title": "selectioninference_exploiting_large_language_models_for_interpretable_logical_reasoning"
        },
        {
            "paper_title": "Vera: A general-purpose plausibility estimation model for commonsense statements",
            "rating": 1,
            "sanitized_title": "vera_a_generalpurpose_plausibility_estimation_model_for_commonsense_statements"
        },
        {
            "paper_title": "Contrastive neural text generation",
            "rating": 1,
            "sanitized_title": "contrastive_neural_text_generation"
        }
    ],
    "cost": 0.020557,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning
30 Oct 2025</p>
<p>Ying Su yingsu@scut.edu.cn 
South China University of Technology</p>
<p>Mingwen Liu 
South China University of Technology</p>
<p>Zhijiang Guo zhijiangguo@hkust-gz.edu.cn 
South China University of Technology</p>
<p>Are LLMs Rigorous Logical Reasoners? Empowering Natural Language Proof Generation by Stepwise Decoding with Contrastive Learning
30 Oct 20258116F5616E4231EA222072000EF82C2BarXiv:2311.06736v3[cs.CL]
Logical reasoning is a pivotal component in the field of artificial intelligence.Proof planning, particularly in contexts requiring the validation of explanation accuracy, continues to present challenges.The recent advancement of large language models (LLMs) has led to significant progress in natural language proof planning, evolving from one-stage generators to more complex three-stage systems that include additional searchers or verifiers.While these assisted methods improve the quality of generated results, they also introduce increased search efforts and computational costs.Furthermore, the generative process itself remains underexplored.In this study, we propose a stepwise decoding approach augmented by contrastive learning to address two common errors encountered during the LLM generator's decoding process.We fine-tune the language model using both vanilla and enhanced hard negatives to mitigate these decoding errors.Empirical results demonstrate the effectiveness of our strategy.Additionally, our further analysis reveals that even larger LLMs still struggle to generate rigorous logical chains.</p>
<p>Introduction</p>
<p>Logical reasoning underpins the comprehension of human cognition and intelligence in machines (Goel et al., 2017).Large Language Models (LLMs) like GPT (Brown et al., 2020;Ouyang et al., 2022) and PaLM (Chowdhery et al., 2022;Anil et al., 2023) have pioneered using natural language as a platform for logical reasoning, complementing the traditional use of formal languages (Kazemi et al., 2022;Creswell et al., 2022).The incorporation of natural language broadens the scope of logical reasoning by allowing flexible querying and tapping into the extensive implicit knowledge encapsulated within LLMs.</p>
<p>In examining the capacity of LLMs for logical reasoning, it is crucial to consider not only the accuracy of their answers but also the correctness of their explanations (Xu et al., 2023).Utilizing prompting methods such as in-context learning (Brown et al., 2020) and chain-of-thought (Wei et al., 2022), LLMs have shown promising results across various deductive reasoning tasks in question-answering formats (Weston et al., 2015;Tafjord et al., 2021;Saparov and He, 2022;Han et al., 2022).These approaches decompose the final task goal by guiding the LLMs through intermediate reasoning steps in a carefully constructed context.However, providing correct explanations, which covers completeness, redundancy, correctness (Xu et al., 2023), emerges as a more daunting challenge.This is particularly evident in tasks that involve generating reasoning chains from premises leading to a conclusion, known as proof generation (Clark et al., 2020;Dalvi et al., 2021).Unfortunately, LLMs often fall short in creating concise and exact proof trees, commonly producing superfluous or imprecise intermediate steps.</p>
<p>Previous studies have utilized LLMs to generate proof trees, employing a range of techniques from holistic approaches (Qu et al., 2022) to incremental steps (Tafjord et al., 2021;Sanyal et al., 2022).Recent methods increasingly rely on postprocessing to enhance the quality of generated results, introducing verification-and search-based systems (Hong et al., 2022;Yang et al., 2022).However, as the methods become more complex, there is a corresponding increase in search efforts and computational costs.Conversely, there has been insufficient focus on refining the generative process itself.Current models exhibit proficiency in selecting relevant premises but struggle to deduce intermediary conclusions, highlighting a deficiency in their understanding of semantic nuances during stepwise deductive reasoning.</p>
<p>Addressing this issue, we introduce a novel strategy dubbed ConDec (Contrastive learning based stepwise Decoding), designed to enhance the generative aspect of LLMs for deductive reasoning tasks.ConDec leverages carefully constructed hard negatives -outputs that are deceivingly similar in form yet differ semantically -to refine generation precision.These hard negatives can be simple sequence alterations or products of an intricate sampling and reasoning process, aided by an external reasoner and checker.Intuitively, the hard negatives are designed to solve decoding errors analyzed in (Dalvi et al., 2021): repetition and invalid entailment.Finetuning with these hard negatives notably advances the LLMs' proficiency in intermediate step and conclusion generation, culminating in overall improved proof accuracy.The main contributions of this study are threefold:</p>
<p>• We introduce ConDec, a stepwise decoding with contrastive learning strategy that enhances stepwise generative quality in proof generation tasks, and devise an automatic method for hard negative generation involving a reasoner and a checker;</p>
<p>• We conduct an extensive empirical analysis on the Entailment benchmark, demonstrating the effectiveness of the proposed method;</p>
<p>• We reveal that LLMs even equipped with chain-of-thought strategies still struggle to perform rigorous logical reasoning in natural language proof generation tasks.</p>
<p>2 Related Work</p>
<p>Logical Reasoning with Natural Language</p>
<p>Logical reasoning is an important ability to realize human-level cognition and intelligence in AI (Nunes, 2012).Early research of logical reasoning uses formal language to represent knowledge and conducts symbolic reasoning (Muggleton and De Raedt, 1994).Recent research uses pretrained language models for logical reasoning in the form of natural language to alleviate the representation challenge with formal language (Musen and Van der Lei, 1988).Among the logical reasoning over natural language (Yang et al., 2023), deductive reasoning covers aspects including hypothesis classification, proof generation, proof generation with incomplete information, and implication enumeration.Several tasks have been proposed to evaluate these reasoning abilities.Specifically, hypothesis classification is conducted over RuleTaker with transformers (Clark et al., 2020).Proof generation providing rationals along with the predicted answer for emulating formal reasoning is further proposed to increase the explanability (Saha et al., 2020).ProofWriter (Tafjord et al., 2021) produces a deductive chain of reasoning over proof generation and implication enumeration with an iterative generating style.To enhance the chain of reasoning for multi-step premises, EntailmentWriter tests transformers' explainability in the form of entailment trees over EntailmentBank (Dalvi et al., 2021).</p>
<p>Proof Generation</p>
<p>Methods for finetuning language models for proof generation vary in the proof direction, inference with or without hypothesis, and whether search or verification is involved.One line of research is inference without a hypothesis available.FaiRR (Sanyal et al., 2022) breaks proof generation into three steps: rule selection, fact selection, and knowledge composition.MetGen (Hong et al., 2022) iteratively generates the entailment tree by conducting a single-step entailment with separate modules and a reasoning controller.SC-Search (Bostrom et al., 2022) decomposes the deductive reasoning task into separate steps coordinated by a search procedure, producing a tree of intermediate conclusions that faithfully reflects the system's reasoning process.ADGV (Sprague Another line of work is with the hypothesis available.IBR (Qu et al., 2022) enhances the interpretability of reasoning procedures by predicting nodes and edges in the proof tree iteratively backward from the question, as well as increasing efficiency and accuracy by simplifying the intermediate process of reasoning.IRGR (Ribeiro et al., 2022) explains a given hypothesis by iteratively searching for suitable premises, constructing a single entailment step at a time.NLProofs (Yang et al., 2022) trains an independent verifier to check the validity of the proof steps to improve decoding accuracy.Our work follows this line and we focus on stepwise decoding correction on the generator itself.A comparison of our method with other approaches with LLMs is presented in Table 1.</p>
<p>Problem Formulation</p>
<p>Following the task definition in Yang et al. (2022), the proof generation task is to derive a proof tree T given a hypothesis h and a set of supporting facts C = {sent 1 , sent 2 , ..., sent n }.The proof tree T is represented as a tuple T = (h, Ĉ, U, S).Ĉ is a subset of the facts {sent i } ∈ C, denoting leaf nodes on the tree T .U = {int 1 , int 2 , ..., int m } denotes the intermediate nodes on the tree.The intermediate nodes are deduced during the reasoning process.Each intermediate node represents an intermediate conclusion.The intermediate nodes are internal tree nodes.On top of the tree, h is the root node as well as the final conclusion needs to be proved.</p>
<p>The structure of the tree is denoted by reasoning steps S = {step 1 , step 2 , ..., step t }.Each internal tree node corresponds to a reasoning step step i ∈ S with int j ∈ U as the conclusion and its children as premises, i.e., sent
1 &amp; int 1 → int 2 , representing that intermediate node int 2 is the con- clusion of leaf</p>
<p>Approach</p>
<p>Stepwise Decoding with Contrastive Learning</p>
<p>With stepwise training, subtrees are sampled on the original entire proof graph.The decoding goal can be the intermediate node or the final hypothesis of the subtree, depending on the sampling strategy.As analyzed in (Dalvi et al., 2021), there are decoding errors leading to inaccurate proof step generation, finally leading to entire proof generation failure.</p>
<p>An overview is presented in Figure 1.</p>
<p>To address the problem, we adopt a contrastive learning technique to improve the stepwise decoding quality.Learning with contrasting positive and negative pairs can improve the generalization ability of conditional text generations (Lee et al., 2020;An et al., 2022).Inspired by this, we construct negative decoding samples to improve the reasoning ability of the generator.</p>
<p>The goal of the generator is to output a stepwise reasoning step step
(i) j with tokens (s (i) 1 , s(i) 2 , ..., s(i) L ) with length L conditioned on the input text sequence x (i) = (x (i) 1 ,
x(i) 2 , ...).The input text sequence is a concatenation of contexts from hypothesis h, facts C, and previous steps {step
(i) 1 , ..., step (i) j−1 }. i is the index of instances in a batch.
The finetuning loss is to maximize the conditional log-likelihood log p θ (step j |x) for a given
N observations {(x (i) , step (i) j ) N i=1 } as follows: L M LE (θ) = N i=1 log p θ (step (i) j |x (i) ), (1) p θ (s (i) 1 , ..., s(i) L |x (i) ) = L l=1 p θ (s (i) l |s (i) &lt;l , x (i) ),(2)h (i) l = g(s (i) l−1 , M (i) ; θ), M (i) = f (x (i) ; θ),(3)
where f, g denote the encoder and the decoder respectively.M (i) is the concatenation of the hidden representations of the source tokens x (i) .
H (i) is the concatenation of the hidden states [h (i) 1 , ..., h(i)
L ] at the decoder output.With a linear projection layer, the hidden states M (i) and H (i) of the encoder and decoder are mapped onto the latent embedding space:
z (i) x = AvgPool(ReLU(W proj M (i) + b proj )), (4) z (i) s = AvgPool(ReLU(W proj H (i) + b proj )).
(5) The semantic similarity sim between them can be calculated by distance with a dot or cosine function.A contrastive loss maximizes the similarity between the pair of source sequence and target sequence while minimizing the similarity between the negative pairs as follows:
L cont (θ) = N i=1 log exp(sim(z (i) x , z (i) s )/τ ) z (k)
s ∈S exp(sim(z
(i) x , z (k) s )/τ ) , (6
) where S is the set of negative samples in the same batch and τ is the temperature parameter.</p>
<p>Training with Hard Negative</p>
<p>Though stepwise generation improves over a singleshot generation (training over the entire proof tree and the decoding goal is a hypothesis), it still has typical errors (Dalvi et al., 2021): 1) repetition(the entailed conclusion simply repeats one of the input sentences); 2) invalid entailment(the entailed conclusion does not follow from input sentences).To improve the decoding quality, we design two types of hard negatives for these errors and finetune the model with these hard negatives.</p>
<p>The hard negative sequence step (i) j</p>
<p>is constructed based on the gold proof step sequence step (i) j .With the hard negative sequences, the decoding loss becomes:
L cont−hard (θ) = N i=1 log exp(sim(z (i)
x , z
(i) s )/τ ) z (k) s ∈S {z (i)
s } exp(sim(z
(i)
x , z
(i) s )/τ ) ,(7)
where z(i) s is the projected hidden state of hard negatives.The final loss for finetuning is:
L = L M LE + αL cont−hard ,(8)
α is a weighted parameter.</p>
<p>Vanilla Hard Negative</p>
<p>Vanilla hard negatives are constructed based on substitution, which mimics the form of gold stepwise proofs, to address the repetition error.we randomly select one of the premises in a proof step step j and replace the conclusion with its context.For example in Figure 1, the vanilla hard negative is constructed by replacing the gold standard conclusion with the context of input node sent11.</p>
<p>Enhanced Hard Negative</p>
<p>To increase the entailment quality over stepwise proofs, we also propose to construct enhanced hard negatives by exploring unseen proof steps with a reasoner and a checker.The reasoner is first trained with proof steps step j ∈ S in natural language and then utilized to generate a conclusion given an unseen combination of premises in the supporting facts C. Given proof step step j , the premises and conclusion in natural language are denoted as set {p 1 , p 2 , ...} and c.The premises are concatenated as input and the conclusion is output for training the reasoner.
L M LE (ϕ) = log p ϕ (c|p 1 , p 2 , ...),(9)
After training, the reasoner can generate a conclusion given an unseen combination of premises.The premise combinations are first sampled from supporting fact set.The sampled premises {p s 1 , p s 2 , ...} and generated conclusion c s constructs an enhanced hard negative step.Details are presented in Figure 2. In the example, for premises int1 and sent11 in gold proof, randomly sample one premise to substitute one of them and use the reasoner to generate a new conclusion given the recombined premises.</p>
<p>To improve the quality of the hard negatives generated from the reasoner, we further adopt the checker Vera (Liu et al., 2023) to score the hard negatives and filter those with low scores.Vera is finetuned over T5-11B with commonsense datasets.The score from the checker indicates the extent of the reasonableness of the deductive commonsense knowledge generated by the reasoner:
score = sigmoid([p s 1 , p s 2 , ..., c s ]; γ), (10)
where the score is calculated with a sigmoid function after hidden layers γ.</p>
<p>Experiment</p>
<p>Dataset</p>
<p>EntailmentBank (Dalvi et al., 2021): Entailment trees are made up of individual and multi-premise textual entailment steps.EntailmentBank contains 1,840 multi-step entailment trees for accompanying QA pairs.For the proof generation task, only the hypothesis H and context set C are used as inputs.</p>
<p>Each proof tree T contains an average of 6.6 nodes and 2.7 entailment steps.Train/Validation/Test splits are 1,313/187/340 respectively.Entailment-Bank consists of three tasks as follows:</p>
<p>(1) Task 1 (no-distractor): C consists of exactly the leaf nodes of the ground truth proof tree;</p>
<p>(2) Task 2 (distractor): C consists of 15-20 distractor sentences besides the leaf nodes on the ground truth proof tree;</p>
<p>(3) Task 3 (full-corpus): C is a large corpus of 12K sentences derived from WorldTree V2 (Xie et al., 2020), requiring the model to retrieve relevant supporting facts from the corpus.For each hypothesis, 25 supporting facts are retrieved.Following Dalvi et al. (2021), we evaluate the zeroshot performance of the model from Task 2.</p>
<p>Evaluation</p>
<p>Following Dalvi et al. (2021), we use the official tools1 to evaluate the generated entailment tree T = (h, L, E, S) with the golden entailment tree T * = (h, L * , E * , S * ).These metrics evaluate the correctness along 4 dimensions:</p>
<p>(1) Leaves (F1, AllCorrect): F1 measures the precision of leaf nodes of T comparing to gold tree T * .ALLCorrect=1 if F1=1, and ALLCorrect=1 if F1&lt;1.</p>
<p>(2) Steps (F1, AllCorrect): F1 measures the precision of proof steps structurally correctness.Each step contains an internal node u ∈ T * (aligned to v ∈ T ).The predicted step is correct if u and v are perfectly aligned.For each tree, ALLCorrect=1 if F1=1 otherwise 0.</p>
<p>(3) Intermediates (F1, AllCorrect): For the internal node u ∈ T * (aligned to v ∈ T ), the intermediate conclusion is correct if the BLEURT (Sellam et al., 2020) score between u and v is greater than 0.28 (Dalvi et al., 2021).F1 and AllCorrect from all intermediate conclusions in T * and T are calculated.ALLCorrect=1 if F1=1 otherwise 0. (4) Overall (AllCorrect): The metric evaluates whether leaves, steps, and intermediates are all correct, AllCorret = 1 if and only if all the leaves, steps, and intermediates are all correct.</p>
<p>Implementation Details</p>
<p>The optimizer is set as Adam (Kingma and Ba, 2014) for all the training.The average running time is 18 hours for Task 1 and 24 hours for Task 2. Experiments are conducted on A800.</p>
<p>Generator.We use Flan-T5-Large as the generator.Flan-T5-Large is a finetuned version of T5-Large (Raffel et al., 2020) over a collection of FLAN instructions (Chung et al., 2022;Longpre et al., 2023).The generator is trained for 500 epochs on Task1 and 600 epochs on Task2.The learning rate for the first-stage generator is 1e-4 and 5e-5 for Task 1 and Task 2 respectively.</p>
<p>ConDec.The vanilla hard negatives are constructed by substituting the conclusion in the gold proof step with a randomly selected premise node context.For example, for sent11 &amp; sent24 -&gt; int: neptune orbits the sun in the solar system, the hard negatives substitute conclusion neptune orbits the sun in the solar system with the context of sent11 or sent24.The temperature τ is set as 0.05 and α is 0.1.The learning rate of the stepwise decoding stage is the same as finetuning with original proof trees.The MLE loss and contrastive loss are alternatively trained for 10 epochs based on the finetuned generator with MLE loss only.</p>
<p>Enhanced Hard Negatives.We use Flan-T5-Large as an additional reasoner to generate unseen proof steps based on labeled proof trees as hard negatives.To train the reasoner, details of data collection are presented in Appendix A.1.The reasoner is trained for 30 epochs with a learning rate of 1e-4.We further apply Vera (Liu et al., 2023) to filter unreasonable generated proof steps.Details of stepwise proofs sampled for reasoner and enhanced new negatives filtered by the checker are presented in Table 2. To increase diversity, enhanced hard negatives and vanilla hard negatives are jointly sampled for training.</p>
<p>6 Result Analysis</p>
<p>Main Results</p>
<p>The main results are presented in Table 3.By analyzing the results, we can find that: Stepwise correction matters.Stepwise generation methods (MetGen, NLProofs, ConDec) outperform single-shot training (EntailmentWriter), even for EntailmentWriter with a much larger parameter size (11B).When comparing ConDec with threestage generation methods MetGen and NLProofs, ConDec achieves comparable or even better performance on Task 2 and Task 3.This shows that contrastive decoding with hard negatives can improve language models' reasoning ability, demonstrating our methods' effectiveness.While our research focuses on the generator, combining our method with theirs may still improve the final accuracy and it is worth exploring.Enhanced hard negatives facilitate reasoning.With enhanced hard negatives, we can find that the ability of proof planning over three tasks is all improved.Unlike vanilla negative construction, the enhanced hard negatives contain harder or more accurate conclusions given premises.Detailed evaluation of enhanced hard negatives is in Appendix A.1.It further improves the training coverage over reasoning steps, thus leading to better performance ConDec achieves the best performance mostly over leave or step accuracy.The contrastive loss helps the generator discriminate between premises and finds semantically correlated premises to deduce a conclusion.However, deductive reasoning is still challenging as the improvement over intermediates is not as obvious as that on leaves or steps.</p>
<p>Computational Cost Analysis</p>
<p>ConDec's design can reduce the inference cost with a one-stage process during inference as shown in Table 4.We provide a concrete comparison using the validation set for Task 2, with the same model size (Flan-T5-Large), on an A800 GPU.It clearly shows that ConDec can significantly improve time efficiency compared to two and three-stage methods (NLProofs).</p>
<p>Ablation Study</p>
<p>Results of the ablation study on Task 2 test split are presented in Table 5. Backbone model.The results indicate that LLaMA-3.2-1Bfails to adequately differentiate between input premises and output reasoning proof steps.As a decoder model focused on next-word prediction, LLaMA simply predicts proof steps as the next sentence following the premises.With stepwise training, the presence of intermediate or hypothesis nodes can confuse the model regarding when to stop generating output, as a subtree with an intermediate node is a subsequence of the entire tree that includes the hypothesis node.This is why LLaMA tends to produce shorter outputs when trained in a stepwise manner.Moreover, the results for LLaMA are significantly lower than those for Flan-T5-Large, despite the latter's smaller size.Natural proof generation is fundamentally a sequence-to-sequence task.Distinguishing between input and output is crucial, as the</p>
<p>Analysis of Closed-Source LLMs</p>
<p>In-context learning (ICL; Brown et al. 2020) and CoT (Wei et al., 2022) have been widely used in various reasoning tasks (Patel et al., 2021;Cobbe et al., 2021;Fu et al., 2022;Xiong et al., 2023).We apply 5-shot prompting and CoT to evaluate how LLMs perform on the proof generation task.The vanilla prompt method simply adopts a k-shot in-context learning strategy.The CoT decomposes the reasoning process into step-by-step generation.</p>
<p>Based on it, Select-inference(SI) (Creswell et al., 2022) is a two-stage COT that decomposes each reasoning step into a premise selection stage and a conclusion inference state.Details of the CoT is illustrated in Appendix A.2.2.As shown in Table 6.Comparing different LLMs, GPT4 generally outperforms GPT3 and GPT3.5-turbo on all the metrics.One detailed case study of GPT3.5-turbo and GPT4 with k-shot prompting results is in Appendix A.2.1.It shows that the GPT3.5-turbotends to generate more irrelevant and inaccurate steps with simple imitation of premises during inferencing new conclusions.In contrast, GPT4 conducts deductive reasoning and generates correct conclusions based on premises.When accompanied by CoT, GPT4 achieves better performance than the vanilla prompt.While with SI, GPT4 makes more mistakes in the premise selection stage.</p>
<p>Our ConDec ⋆ outperforms all closed-source LLMs in all metrics.Although recent LLMs such as o1-mini and o1-preview show improvements in several metrics compared to their predecessors (GPT-3.5-turboand GPT-4), a substantial performance gap remains when compared to finetuningbased methods.Through stepwise training with curated datasets, these language models can better capture the correlations between premises.</p>
<p>Conclusion</p>
<p>Logical reasoning is both challenging and fundamental in artificial intelligence.Proof generation serves as a measure of the explanatory capabilities of large language models in the context of logical reasoning.To enhance stepwise deductive reasoning, we propose a decoding strategy augmented by contrastive learning.The carefully designed hard negatives address the typical errors encountered during the decoding process.The experimental results across standard benchmarks demonstrate the effectiveness of our method.Additionally, our analysis of larger LLMs reveals that they continue to struggle with proof planning tasks.</p>
<p>Limitations</p>
<p>From the analysis of the paper, natural proof planning ability is still a challenging topic in evaluating LLMs' deductive reasoning ability.The current curated human-annotated dataset in our experiments is of limited size to improve LLMs' deductive reasoning ability.Knowledge from related corpora such as cause and effect, logic reasoning can be further applied to improve the proof generation ability with pre-training or transfer learning.</p>
<p>Threshold score Accuracy(%) 0.7 36 0.8 50 0.9 60</p>
<p>A Appendix</p>
<p>A.1 Details of Enhanced Hard Negatives</p>
<p>Training data of the reasoner is basically all the proof steps in the training set.Each proof step is converted into the form of natural language: 1) premises are concatenated with conjunction "and"; 2) premises and conclusion are linked with "Because" and "Therefore".The reasoner is trained with 8,819 proof steps for both Task 1 and Task 2.</p>
<p>For each proof step step j , substitute one premise from the the supporting fact set C except the ones in current proof step.Recombined proof premises {p s 1 , p s 2 , ...} are converted into natural language form and used as input to the reasoner.The reasoner generates a new conclusion c s based on the premises, forming a hard negative step (i) j .The constructed hard negative step (i) j is then filtered by the checker with a threshold score.To determine the threshold score, three PhD students from the CSE department are assigned with the task to check the quality of 100 randomly selected constructed hard negatives.Each hard negative is first judged by Vera.The score from Vera indicates the reasonableness of the hard negative.By filtering the score with a threshold, we choose the constructed negatives with high quality for further training.The result of human checking over the filtered negatives is shown in the table 7. We find that with a threshold score of 0.9, 60% of the filtered hard negatives are reasonable.</p>
<p>A.2 Prompting Methods</p>
<p>A.2.1 Case study for prompting</p>
<p>Result case from GPT3.5-turbo and GPT4 on Task2 dev split with vanilla prompts is shown in Table 8.</p>
<p>A.2.2 Template for prompting</p>
<p>Prompting template for GPT4 with CoT is in Table 9.</p>
<p>A.3 Baseline Details</p>
<p>MetGen and NLProofs are three-stage methods.MetGen divides single-step entailment into basic logical operations.It reasons in both forward deductive and backward abductive steps.A controller finally selects promising steps among the reasoning results.NLProofs uses a trained verifier to guide the search process of proof generation.The verifier scores the generation expansion steps and finally, a proof tree is selected according to the scores.</p>
<p>A.4 Discussion with RLET RLET (Liu et al., 2022) is trained using cumulative signals across the entire entailment tree, marking the first introduction of reinforcement learning into the entailment tree generation task.It employs a one-stage generation method that does not involve search or verification.RLET flexibly assigns rewards to each generated step and utilizes the overall cumulative reward to optimize training based on the full trajectory.In contrast, our approach and previous methods (as listed in Table 1) rely on exact matches of gold steps as training signals.</p>
<p>In Task 3, the retrieved supporting facts may contain noise and are longer than those in Task 1 and Task 2. RLET's focus on long-path proof training provides it with an advantage over our method (and previous methods listed in Table 1) in this task.However, since the supporting facts are sourced from a relevant corpus, they may deviate from the training setting, particularly under zero-shot performance conditions.As a result, RLET's performance is significantly lower than that of previous methods in Task 1 and Task 2.</p>
<p>A.5 Selection of Enhanced Hard Negatives</p>
<p>Besides randomly selecting premises to construct enhanced hard negatives, we also select according to the top similarity score calculated with BM25 (Robertson et al., 2009).Results are presented in Table 10.The results show that similar distracted premises do not contribute to intermediate node accuracy as much as random premises do.This is because randomly selected premises can introduce more diversity for the constructed hard negatives.</p>
<p>A.6 Case Study</p>
<p>We present a case study comparing the generation capabilities of LLaMA 3.2-1B and Flan-T5-Large to illustrate the differences between encoderdecoder and decoder-only LLMs, as shown in Table 11.The table indicates that LLaMA (without stepwise training) generates longer proofs because it is trained with complete proof chains.In contrast,LLaMA (with stepwise training) typically produces one-step proofs that conclude with either a "hypothesis" or an "intermediate node," reflecting the fact that both are endpoints in the sequences used for stepwise training.Flan-T5-Large (with stepwise training) strikes a balance between generating long and short proof chains due to its encoderdecoder architecture.However, it still faces challenges in accurately selecting premises.Hypothesis: as the distance of the star to earth decreases, the star will appear brighter.</p>
<p>Context: sent1: a star produces light and heat sent2: far is the opposite of close sent3: as the distance from an object increases , the force of gravity on that object will decrease sent4: brightness means amount of light sent5: if two or more things are in a relationship then those things impact each other sent6: feature is synonymous with characteristic ... sent25: moving away from the source increases the distance</p>
<p>Figure 2: Enhanced hard negative construction is implemented by exploring the unseen combination of premises, inferencing with the reasoner, and filtering with a score from the checker.</p>
<p>Gold</p>
<p>Proof: sent1 &amp; sent13 -&gt; int1: a star is a source of light; int1 &amp; sent21 &amp; sent23 -&gt; hypothesis; LLaMA-3.1-1B(w/ostepwise): sent13 &amp; sent1 -&gt; int1: stars are a source of light; int1 &amp; sent21 -&gt; int2: as the distance of a star to earth decreases, the star will appear brighter; int2 &amp; sent7 -&gt; hypothesis; LLaMA-3.1-1B(stepwise):sent21 &amp; sent3 -&gt; hypothesis; Flan-T5-Large(stepwise): sent1 &amp; sent13 -&gt; int1: a star is a source of light and produces light; sent17 &amp; int1 &amp; sent21 -&gt; hypothesis;</p>
<p>Table 1 :
1
Comparison of methods over natural language proof generation.Stepwise Correction means that if stepwise generation is enhanced in training.Stage calculates if the method contains generation, verification, and search.
MethodStepwiseStepwiseDirection Search Verifier Human-authoredStageGenerationCorrectionBenchmarkEntailmentWriter (Dalvi et al., 2021) ✗✗→✗✗✓1IRGR (Ribeiro et al., 2022)✓✗→✓✗✓2SCSearch (Bostrom et al., 2022)✓✗→✓✗✗2MetGen (Hong et al., 2022)✓✗both✓✗✓2ADGV (Sprague et al., 2022)✓✗both✓✓✓3NLProofs (Yang et al., 2022)✓✗→✓✓✓3ConDec✓✓→✗✗✓1</p>
<p>Enhanced hard negative 𝐬𝐭𝐞𝐩 𝟐 : Stepwise output 𝐬𝐭𝐞𝐩 𝟐 : Proof tree
Hypothesis 𝒉:The sun will be the star that appears the brightest to the earth;(𝐓):ℎ: the sun will be the star that appears the brightest to theearthsent1: the sun is the star that is closestFacts 𝑪:to earth sent2: the four planets farthest from the sent3: far means great in distance sun are made of gasint2: as the stars become closer, the light of the stars will appear brightersent1: the sun is the star that is closest to earthsent4: furthest / farthest means greatest / most / highest in distance sent5: to be in the sun means to be inint1: stars are a source of lightsent11: as a source of light becomes closer, the light will appear brighterthe sunlightsent6: appear is similar to apparentsent7: brightness means amount of lightsent13: a source ofsent16: a star……something produces lightproduces lightint1 &amp; sent11 -&gt; int2: as the stars become$hypothesis$=the sun will be thecloser, the light of the stars will appear brighter;star that appears the brightest tothe earth; $facts$=sent1: the sun is the starVanilla hard negative 𝐬𝐭𝐞𝐩 𝟐 :that is closest to earth sent2: thefour …;int1 &amp; sent11 -&gt; int2: as a source of light$partial_proof$=sent13 &amp; sent16becomes closer , the light will appear brighter;-&gt; int1: stars are a source of lightsent11 &amp; sent1 -&gt; int2: the sun will be the starthat appear the brightest to the earth;</p>
<p>Input Output Stepwise Decoding with Contrastive Learning (ConDec) Task Description Encoder Decoder
I𝐧𝐩𝐮𝐭 𝒙 𝐟𝐨𝐫 𝟐𝐧𝐝 𝐬𝐭𝐞𝐩 𝐠𝐞𝐧𝐞𝐫𝐚𝐭𝐢𝐨𝐧:𝐌𝐇Projection layer𝐳 !𝐙 " !𝐳 "𝐙 " !Figure 1: Architecture of the stepwise decoding with contrastive learning over hard negatives. The hard negativesare constructed by vanilla and enhanced strategies. Vanilla strategy means simple conclusion substitution. Theenhanced strategy uses a reasoner and a checker to generate hard negatives.et al., 2022) proposes to abductively infer a premisegiven another premise and a conclusion, as wellas to search over two fingers interleaving deduc-tive (forward-chaining) and abductive (backward-chaining) inferences.</p>
<p>node sent 1 and intermediate node int 1 .The premise of a reasoning step can be from leaf nodes or intermediate nodes.Under the stepwise generation setting, intermediate nodes and proof steps up to the current step are added to given facts as input to generate new intermediate nodes for the next step.</p>
<p>Table 2 :
2
Distribution of samples from training data, constructed enhanced hard negatives and filtered hard negatives on Task 1 and Task 2. In filtering, threshold score is 0.9.
Reasoner Enhanced Negative Filtered Negative8,819Task 1 47,386 104,665 16,371 21,948 Task 2 Task 1 Task 2</p>
<p>Table 3 :
3
Main results on EntailmentBank with finetuning method.Methods with * are three-stage.ConDec denotes finetuning with vanilla hard negatives and ConDec ⋆ denotes finetuning with combination of vanilla and enhanced hard negatives.Best results are boldface and second-best results are underlined.
TaskMethodF1Leaves AllCorrect F1 AllCorrect F1 AllCorrect AllCorrect Steps Intermediates OverallTask 1EntailmentWriter98.786.250.537.767.636.233.5(no-distractor) EntailmentWriter (11B) 99.089.451.538.271.238.535.3MetGen  <em>100.0100.057.741.970.839.236.5NLProofs  </em>97.890.155.642.372.440.638.9ConDec99.998.255.742.172.338.936.2ConDec ⋆99.998.257.343.272.941.537.9Task 2EntailmentWriter84.335.635.522.961.828.520.9(distractor)EntailmentWriter (11B) 89.148.841.427.766.231.525.6MetGen  <em>82.746.141.329.661.432.427.7NLProofs  </em>90.358.847.234.470.237.833.3ConDec91.059.150.236.570.338.234.1ConDec ⋆91.160.650.737.470.738.234.7Task 3EntailmentWriter35.72.96.12.433.47.72.4(full-corpus) EntailmentWriter (11B) 39.93.87.42.935.97.12.9MetGen  <em>34.88.79.88.636.620.48.6NLProofs  </em>43.28.211.26.942.917.36.9ConDec43.38.211.16.543.418.06.5ConDec ⋆44.79.411.77.142.317.77.1over proof planning.ConDec is more efficient with distractors. Specif-ically, contrastive learning with hard negativesachieves obvious performance gain over Task 2and Task 3 while deteriorating the performance onTask 1. For Task 2 and Task 3, there are distractorpremises or full-corpus, which increases the chal-lenges for rigorous reasoning. For Task 1, there isno distractor. Adding hard negatives deviates thegenerator from generating correct steps instead ofintermediate nodes.Predicting intermediate node is still challenging.</p>
<p>Table 4 :
4
Computation time comparison(percentage reduction).Time denotes the inference time.NLProofs requires additional search and verification for prediction.</p>
<p>Table 5 :
5
Ablation study results on EntailmentBank test set in Task 2.
MethodLeaves F1 AllCorrect F1 AllCorrect F1 AllCorrect AllCorrect Steps Intermediates OverallLLaMA-3.2-1B (1.2B, w/o stepwise)19.55.36.42.913.95.32.7LLaMA-3.2-1B (1.2B, stepwise)15.63.54.13.29.95.03.2Flan-T5-Large (0.8B, stepwise)90.758.849.236.269.636.833.5+ contrastive loss90.960.349.535.969.437.132.4+ contrastive loss, vanilla hard91.059.150.236.570.338.234.1+ contrastive loss, vanilla and enhanced hard 91.160.650.737.470.738.234.7Flan-T5-XL (3B, stepwise)90.957.150.236.568.835.933.8MethodF1Leaves AllCorrect F1Steps AllCorrect F1Intermediates AllCorrect AllCorrect OverallConDec ⋆92.463.155.345.572.843.941.2GPT3 (5-shot)64.215.317.612.353.622.312.3GPT3.5-turbo (5-shot)61.99.016.94.351.915.13.74GPT4 (5-shot)78.132.630.222.563.930.821.9GPT4 (SI)79.130.024.314.065.432.113.9GPT4 (CoT)79.033.730.922.564.731.622.5GPT4o-mini (5-shot)63.118.220.013.954.524.113.9o1-mini (5-shot)72.527.328.613.950.918.711.8o1-preview (5-shot)84.043.338.728.067.833.721.9</p>
<p>Table 6 :
6
Results on EntailmentBank validation set in Task 2 with finetuning and prompting methods.GPT3.5 is GPT3.5-turbo-0613.GPT4 is GPT4-0613.GPT4o-mini is gpt-4o-mini-2024-07-18.o1-mini is o1-mini-2024-09-12.o1-preview is o1-preview-2024-09-12.
encoder-decoder architecture of Flan-T5 processesdifferent texts. A case study comparing LLaMAand Flan-T5 is presented in the Appendix A.6.Model size. For the model size, we also try Flan-T5-XL (3B). Stepwise training over Flan-T5-Largeprovides a strong baseline. Though much largerin model size, Flan-T5-XL achieves similar per-formance over the metrics. It shows that proofplanning requires a higher level of understandingof deductive reasoning over multi-hops.Negative sample. ConDec without hard negativesimproves the leave accuracy while decreasing thestep accuracy, leading to a slight drop in the overallperformance. Adding vanilla or enhanced hard neg-atives can generally improve over all the metrics.</p>
<p>Table 7 :
7
Human check accuracy on enhanced hard negatives from reasoner and checker with different threshold scores.</p>
<p>Table 10 :
10
Ablation study of enhanced hard negative construction on EntailmentBank test set on Task 2.
MethodLeaves F1 AllCorrect F1 AllCorrect F1 AllCorrect AllCorrect Steps Intermediates OverallRandom 91.060.650.737.470.738.234.7BM2590.860.350.336.868.837.934.7</p>
<p>Table 11 :
11
Case study of LLaMA and Flan-T5.</p>
<p>https://github.com/allenai/entailment_bank</p>
<p>Cont: Contrastive neural text generation. Chenxin An, Jiangtao Feng, Kai Lv, Lingpeng Kong, Xipeng Qiu, Xuanjing Huang, Advances in Neural Information Processing Systems. 202235</p>
<p>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, arXiv:2305.10403Palm 2 technical report. 2023arXiv preprint</p>
<p>Natural language deduction through search over statement compositions. Kaj Bostrom, Zayne Sprague, Swarat Chaudhuri, Greg Durrett, Findings of the Association for Computational Linguistics: EMNLP 2022. 2022</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, arXiv:2204.02311Palm: Scaling language modeling with pathways. 2022arXiv preprint</p>
<p>Chung Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, arXiv:2210.11416Scaling instruction-finetuned language models. 2022arXiv preprint</p>
<p>Transformers as soft reasoners over language. Peter Clark, Oyvind Tafjord, Kyle Richardson, 10.24963/ijcai.2020/537Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence. the Twenty-Ninth International Joint Conference on Artificial Intelligence20202020</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Selection-inference: Exploiting large language models for interpretable logical reasoning. Antonia Creswell, Murray Shanahan, Irina Higgins, arXiv:2205.097122022arXiv preprint</p>
<p>Explaining answers with entailment trees. Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, Peter Clark, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language Processing2021</p>
<p>Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, Tushar Khot, arXiv:2210.00720Complexity-based prompting for multi-step reasoning. 2022arXiv preprint</p>
<p>Vinod Goel, Gorka Navarrete, Ira A Noveck, Jérôme Prado, The reasoning brain: the interplay between cognitive neuroscience and theories of reasoning. 2017</p>
<p>Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, arXiv:2209.00840Folio: Natural language reasoning with firstorder logic. 2022arXiv preprint</p>
<p>METGEN: A modulebased entailment tree generation framework for answer explanation. Ruixin Hong, Hongming Zhang, Xintong Yu, Changshui Zhang, 10.18653/v1/2022.findings-naacl.145Findings of the Association for Computational Linguistics: NAACL 2022. Seattle, United StatesAssociation for Computational Linguistics2022</p>
<p>Najoung Seyed Mehran Kazemi, Deepti Kim, Xin Bhatia, Deepak Xu, Ramachandran, arXiv:2212.13894Lambada: Backward chaining for automated reasoning in natural language. 2022arXiv preprint</p>
<p>P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980Adam: A method for stochastic optimization. 2014arXiv preprint</p>
<p>Contrastive learning with adversarial perturbations for conditional text generation. Seanie Lee, Dong Bok Lee, Sung Ju Hwang, International Conference on Learning Representations. 2020</p>
<p>Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A Smith, Yejin Choi, Hannaneh Hajishirzi, arXiv:2305.03695Vera: A general-purpose plausibility estimation model for commonsense statements. 2023arXiv preprint</p>
<p>Rlet: A reinforcement learning based approach for explainable qa with entailment trees. Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang, Xipeng Qiu, Zheng Zhang, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Barret Quoc V Le, Jason Zoph, Wei, arXiv:2301.13688The flan collection: Designing data and methods for effective instruction tuning. 2023arXiv preprint</p>
<p>Inductive logic programming: Theory and methods. Stephen Muggleton, Luc De, Raedt , The Journal of Logic Programming. 191994</p>
<p>Of brittleness and bottlenecks: Challenges in the creation of pattern-recognition and expert-system models. A Mark, Johan Musen, Van Der Lei, Machine intelligence and pattern recognition. Elsevier19887</p>
<p>Logical reasoning and learning. Terezinha Nunes, 2012</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in Neural Information Processing Systems. 202235</p>
<p>Arkil Patel, Satwik Bhattamishra, Navin Goyal, arXiv:2103.07191Are nlp models really able to solve simple math word problems?. 2021arXiv preprint</p>
<p>Interpretable proof generation via iterative backward reasoning. Hanhao Qu, Yu Cao, Jun Gao, Liang Ding, Ruifeng Xu, 10.18653/v1/2022.naacl-main.216Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United States2022Association for Computational Linguistics</p>
<p>Exploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020</p>
<p>Entailment tree explanations via iterative retrieval-generation reasoner. Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Rui Dong, Xiaokai Wei, Henghui Zhu, Xinchi Chen, Peng Xu, Zhiheng Huang, Andrew Arnold, Findings of the Association for Computational Linguistics: NAACL 2022. 2022</p>
<p>The probabilistic relevance framework: Bm25 and beyond. Stephen Robertson, Hugo Zaragoza, Foundations and Trends® in Information Retrieval. 342009</p>
<p>Prover: Proof generation for interpretable reasoning over rules. Swarnadeep Saha, Sayan Ghosh, Shashank Srivastava, Mohit Bansal, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>FaiRR: Faithful and robust deductive reasoning over natural language. Soumya Sanyal, Harman Singh, Xiang Ren, 10.18653/v1/2022.acl-long.77Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Long Papers. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland20221Association for Computational Linguistics</p>
<p>Language models are greedy reasoners: A systematic formal analysis of chain-of-thought. Abulhair Saparov, He He, arXiv:2210.012402022arXiv preprint</p>
<p>Bleurt: Learning robust metrics for text generation. Thibault Sellam, Dipanjan Das, Ankur Parikh, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>Natural language deduction with incomplete information. Zayne Sprague, Kaj Bostrom, Swarat Chaudhuri, Greg Durrett, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>Proofwriter: Generating implications, proofs, and abductive statements over natural language. Oyvind Tafjord, Bhavana Dalvi, Peter Clark, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2021</p>
<p>Chain-of-thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H Chi, V Quoc, Denny Le, Zhou, 2022In NeurIPS</p>
<p>Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, Tomas Mikolov, arXiv:1502.05698Towards ai-complete question answering: A set of prerequisite toy tasks. 2015arXiv preprint</p>
<p>Worldtree v2: A corpus of sciencedomain structured explanations and inference patterns supporting multi-hop inference. Zhengnan Xie, Sebastian Thiem, Jaycie Martin, Elizabeth Wainwright, Steven Marmorstein, Peter Jansen, Proceedings of the Twelfth Language Resources and Evaluation Conference. the Twelfth Language Resources and Evaluation Conference2020</p>
<p>Dq-lore: Dual queries with low rank approximation re-ranking for in-context learning. Jiong Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang, Chengming Li, Xiaodan Liang, ArXiv, abs/2310.029542023</p>
<p>Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, arXiv:2306.09841Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views. Jun Liu, and Erik Cambria. 2023arXiv preprint</p>
<p>Generating natural language proofs with verifier-guided search. Kaiyu Yang, Jia Deng, Danqi Chen, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. the 2022 Conference on Empirical Methods in Natural Language Processing2022</p>
<p>k / f / c sent5: united states is located in the northern hemisphere ... sent9: when the seasons change from summer to fall / from summer to winter , the amount of daylight will decrease ... sent14: new york / new york state is a state located in the united states of america sent15: winter has the least sunlight ... sent22: the earth being tilted on its rotating axis causes seasons sent23: december is during the winter in the northern hemisphere. Zonglin Yang, Xinya Du, Rui Mao, Jinjie Ni, Erik Cambria, arXiv:2303.12023373 / 212 / 100Logical reasoning over natural language as knowledge representation: A survey. 2023arXiv preprintGold Proof: sent14 &amp; sent5 -&gt; int1: new york state is located in the northern hemisphere. int1 &amp; sent23 -&gt; int2: december is during the winter for new york state; int2 &amp; sent15 -&gt; hypothesis</p>
<p>&amp; sent22 -&gt; int1: December is during the winter in the northern hemisphere; int1 &amp; sent9 -&gt; int2: the amount of daylight will decrease during December; int2 &amp; sent15 -&gt; int3: December has the least sunlight. GPT3.5-turbo: sent23int3 &amp; sent14 -&gt; hypothesis</p>
<p>Context: sent1: identifying is similar to determining. sent2: if a fossil is of an organism that cannot be identified then that organism is probably extinct. sent3: discovering something usually requires seeing that something. sent4: a dinosaur is a kind of extinct animal. sent5: fossils can be used as evidence for the ancient environment. sent6: dead means not alive. ... sent25: fossils can be used to study the history of organisms and environments on earth Reasoning step by step and finally output the proof: From sent13 &amp; sent24. GPT4: sent14 &amp; sent5 -&gt; int1: New York state is located in the northern hemisphere; int1 &amp; sent23 -&gt; int2: December is during the winter in New York state; int2 &amp; sent15 -&gt; hypothesis. Table 8: Case study of vanilla prompt with GPT3.5-turbo and GPT4. Hypothesis: if a fossil of a bird cannot be identified then that kind of bird is probably extinct. we can infer that a bird is a kind of organism. int1</p>
<p>From int1 &amp; sent2, we can infer that if a fossil of a bird cannot be identified then that kind of bird is probably extinct. hypothesis</p>
<p>Proof: sent13 &amp; sent24 -&gt; int1: a bird is a kind of organism; int1 &amp; sent2 -&gt; hypothesis; Table 9: Prompting template of stepwise proof generation for GPT4 with COT. </p>            </div>
        </div>

    </div>
</body>
</html>
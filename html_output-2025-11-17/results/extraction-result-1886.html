<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1886 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1886</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1886</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-37.html">extraction-schema-37</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <p><strong>Paper ID:</strong> paper-276482965</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.14297v3.pdf" target="_blank">Evaluating Sakana's AI Scientist: Bold Claims, Mixed Results, and a Promising Future?</a></p>
                <p><strong>Paper Abstract:</strong> Recently, Sakana.ai introduced the AI Scientist, a system claiming to automate the entire research lifecycle and conduct research autonomously, a concept we term Artificial Research Intelligence (ARI). Achieving ARI would be a major milestone toward Artificial General Intelligence (AGI) and a prerequisite to achieving Super Intelligence. The AI Scientist received much attention in the academic and broader AI community. A thorough evaluation of the AI Scientist, however, had not yet been conducted.1 We evaluated the AI Scientist and found several critical shortcomings. The system's literature review process is inadequate, relying on simplistic keyword searches rather than profound synthesis, which leads to poor novelty assessments. In our experiments, several generated research ideas were incorrectly classified as novel, including well-established concepts such as micro-batching for stochastic gradient descent (SGD). The AI Scientist also lacks robustness in experiment execution—five out of twelve proposed experiments (42%) failed due to coding errors, and those that did run often produced logically flawed or misleading results. In one case, an experiment designed to optimize energy efficiency reported improvements in accuracy while consuming more computational resources, contradicting its stated goal. Furthermore, the system modifies experimental code minimally, with each iteration adding only 8% more characters on average, suggesting limited adaptability. The generated manuscripts were poorly substantiated, with a median of just five citations per paper—most of which were outdated (only five out of 34 citations were from 2020 or later). Structural errors were frequent, including missing figures, repeated sections, and placeholder text such as "Conclusions Here". Hallucinated numerical results were contained in several manuscripts, undermining the reliability of its outputs. Despite its limitations, the AI Scientist represents a significant leap forward in research automation. It produces complete research manuscripts with minimal human intervention, challenging conventional expectations of AI-generated scientific work. Many reviewers or university instructors conducting only a superficial assessment may struggle to distinguish its output from that of human researchers, demonstrating how far AI has progressed in mimicking academic writing and structuring scientific arguments. While the quality of its manuscripts currently aligns with that of an unmotivated undergraduate student rushing to meet a deadline, this level of autonomy in research generation is remarkable. More strikingly, it achieves this at an unprecedented speed and cost efficiency—our analysis indicates that generating a full research paper costs only $6–$15, with just 3.5 hours of human involvement. This is significantly faster than traditional human researchers. Given that AI research automation was nearly nonexistent just a few years ago, the AI Scientist marks a substantial milestone toward Artificial Research Intelligence (ARI), signalling the acceleration of AI-driven scientific discovery. The AI Scientist also illustrates the urgent need for a discussion within the Information Retrieval (IR) and broader scientific communities. Whether and when ARI becomes a reality depends on how the academic and AI communities shape its development and governance. We propose concrete steps, including pilot projects and competitions, and standardized attribution frameworks such as research logs and markup languages.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1886.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1886.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-novelty-classifier</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Scientist novelty classification (Semantic Scholar keyword-based novelty detector)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The AI Scientist's automated novelty assessor queries the Semantic Scholar API (up to 10 results, up to 10 iterations) and assigns a binary 'novel' label based on keyword matching rather than deep synthesis, causing systematic misclassification of well-established ideas as novel.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>automated novelty classifier / literature-search-based novelty assessment</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>binary 'novel' label derived from Semantic Scholar keyword-match search results (up to 10 results, up to 10 iterations)</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>In the authors' test set the system labelled all 10 generated ideas plus both seed ideas as novel (12/12 = 100% labeled novel), including well-documented techniques (e.g., micro-batching, adaptive learning rates) — i.e., 100% false-positive rate for known items in this sample.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>systematic positive-label bias (near-constant), i.e., tendency to over-assign 'novel' regardless of true novelty</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>No longitudinal recognition: literature retrieval appears insensitive to recency; manuscripts produced by the system had few recent citations (only 5 of 34 citations were from 2020+), indicating a recency/temporal gap in retrieved references.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Recommender systems / Green Recommender Systems (applied domain used in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not evaluated across fields in this paper (single-domain test only).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>binary novelty label assigned by automated literature search</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Authors' domain knowledge / existence of prior published work (manual verification by authors that techniques were known)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>12/12 mismatch in the test sample (100% of items labelled novel despite prior literature), demonstrating a complete mismatch on the small test set.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>The classifier frequently labelled incremental applications of known techniques as 'novel'; paper reports many generated ideas were incremental despite being labelled novel, but no further numerical split provided between incremental vs transformational beyond examples.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>No — only the novelty-proxy was examined; compound proxy failures were not tested.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Poor for novelty-detection in this sample: complete failure to detect known prior work (100% false-positive novelty), due to reliance on shallow keyword matching of up to 10 search results.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not directly examined; system relies on external search API (Semantic Scholar) rather than an explicitly trained novelty classifier; authors note reliance on historical-indexed literature may bias results toward misses of established prior art or recent work.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>None — authors did not test corrective interventions for the novelty classifier in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Authors note that in prior informal tests the AI Scientist occasionally flagged some ideas as non-novel, indicating non-deterministic behavior, but no formal counter-examples of correct novelty detection are quantified.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Search depth and retrieval strategy (max 10 results, up to 10 iterations), keyword-based matching, and limits of the Semantic Scholar metadata extraction; dependence on user-provided seed ideas and templates can also steer outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Empirical test on 10 generated ideas + 2 seed ideas (12 items); novelty labels produced by the AI Scientist via Semantic Scholar queries; authors manually compared labels to known literature to identify misclassifications.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1886.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1886.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-reviewer-agent</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI Scientist reviewer agent (LLM-based automated peer review)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based reviewer agent that extracts text from PDFs (cannot interpret figures/tables) and produces structured reviews with numeric ratings and binary accept/reject recommendations; evaluated by authors on both AI-generated and human-written manuscripts, showing strong conservative bias and misalignment with human decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>automated peer review (LLM-based reviewer agent)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Numerical rating on 'originality' (and other dimensions) plus binary accept/reject; however, novelty assessment is based on textual analysis and limited metadata extraction (no figure/table understanding).</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>On a set of 10 human-written OpenReview papers (5 accepted by humans, 5 rejected), the AI reviewer rejected 9/10 papers (90% rejection). It rejected 4 of the 5 papers that had been accepted by human reviewers (80% false-negative rate for human-accepted papers). Overall agreement with human acceptance/rejection = 5/10 (50%). For AI-generated manuscripts, the agent recommended rejection for all 7 generated manuscripts (7/7 = 100% rejection).</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>conservative thresholding / strong negative bias (tendency toward rejection regardless of human acceptance), not a linear scaling with novelty but a majority-reject policy in tested sample</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>No temporal dynamics evaluated; limitations noted include inability to parse figures/tables and shallow depth of critique, which could persist across time but no time-course data provided.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Machine learning / information retrieval papers (OpenReview; domain of the tested manuscripts).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not analyzed across fields; tested only on small OpenReview set and AI-generated manuscripts in ML/IR domain.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>accept/reject recommendation and numeric rubric scores (originality, clarity, significance, soundness, presentation, contribution)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Human OpenReview acceptance/rejection outcomes used as the ground-truth comparator in the authors' test.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Agreement with human decisions was 50% (5/10); 4 of 5 human-accepted papers were marked rejected by the AI reviewer (80% false-negative among human-accepted), indicating large gap between AI reviewer recommendations and human editorial decisions.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Authors infer a conservative bias that could penalize novel/transformational work, but no direct stratified quantitative comparison by novelty level was provided beyond the observed high rejection of both AI-generated and some human-accepted papers.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not tested; only the automated reviewer performance vs human decisions was evaluated.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Structured and superficially plausible but lacking depth; fails to detect several serious methodological and structural issues; cannot interpret figures/tables; shows low alignment with human reviewers (50% agreement) and high rejection rates (90% on the tested human-paper sample).</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not measured directly; authors caution generally that LLMs inherit historical-data biases, but no experiments altering training data were reported.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>No corrective interventions (e.g., calibration, ensemble with humans) were tested in this study.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>One case where the AI reviewer recommended acceptance was a paper that had been rejected by human reviewers — an instance of AI/human divergence (AI accepted 1 paper that humans had rejected).</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>PDF-text-extraction limitations (no figure/table understanding), reliance on structured review templates (may favor surface-level critiques), and dependence on the LLM used (model choice affects outputs); quality of input PDF and completeness of manuscript strongly moderate outcome.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Evaluation on 7 AI-generated manuscripts (self-reviewed by the system) and 10 human-written OpenReview papers (5 accepted, 5 rejected). Comparison between AI reviewer recommendations and OpenReview human outcomes provided the quantitative measures.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1886.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1886.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-citation-patterns</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Citation and reference patterns in AI-generated manuscripts</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Empirical observations of the bibliographic support in AI-generated manuscripts: low citation counts and a paucity of recent references, indicating weak literature grounding and potential recency bias in automated literature retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>citation-count and recency as proxies for literature grounding / novelty support</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Not a formal novelty metric here; citations used as evidence of literature integration and novelty claims are cross-checked against presence/absence of prior art.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>For 7 AI-generated manuscripts the median number of citations was 5 per paper; only 5 out of 34 total citations (14.7%) were from 2020 or later, indicating strong skew toward older literature in these outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>observational skew: AI-generated manuscripts show low citation volume and low recency; no formal functional relationship between novelty and citation pattern was estimated.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Recency deficiency in cited literature (only ~15% of citations from 2020+); no longitudinal follow-up on how citations to these AI-generated works change over time.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Recommender systems / machine learning domain (domain of generated manuscripts).</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not evaluated across fields.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>short-term citation count per manuscript and recency distribution of cited works</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Manual verification by authors of whether cited literature covered relevant prior work; adequacy judged qualitatively by authors rather than a numeric gold standard.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not reported as a numeric correlation; qualitative gap: many AI-generated manuscripts claimed novelty while failing to cite relevant prior art, indicating mismatch between citation-based evidence produced and true novelty/support.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Authors found many AI-generated ideas were incremental and yet were presented as novel with sparse/dated citations — citation patterns did not reliably reflect true novelty vs incremental status.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not tested.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>Automated literature retrieval used by the AI Scientist produced few and often outdated citations, undermining the manuscripts' ability to substantiate novelty claims.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>Not explicitly measured, though reliance on particular indexing/search APIs (Semantic Scholar) may bias which works are returned and cited.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>No interventions; authors recommend better dataset/metadata integration and repository links (PapersWithCode, GitHub) but did not test fixes.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Sakana's preprint itself received rapid attention (>100 citations by early Feb 2025), showing that community attention (citation accumulation) can be high even for controversial or low-quality AI-generated claims; however this is a separate phenomenon from the AI-generated manuscripts analyzed.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Quality of the experimental pipeline, user-provided templates, and the retrieval strategy (API limits) moderate citation outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Quantitative analysis of citations in 7 AI-generated manuscripts (total 34 citations) — median citations reported and recency distribution (counts from 2020+).</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1886.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1886.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how evaluation systems (peer review, citation metrics, automated systems, journal decisions) perform on novel or transformational scientific work compared to incremental work, including quantitative measurements of bias, temporal patterns, and field differences.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>human-peer-review-variability</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human peer review variability and conservatism (qualitative observations)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper documents known features of human peer review—subjectivity, reviewer disagreements, superficial reviews, and resistance or unease toward AI-driven submissions—illustrated by an ICLR 2025 case with an intense review thread, and positions AI review evaluation against this noisy human baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_system_type</strong></td>
                            <td>human peer review (conference/journal review processes)</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_measure</strong></td>
                            <td>Human reviewer judgments of novelty expressed in qualitative reviews and acceptance decisions; no standardized numeric novelty index used in this paper for human reviews.</td>
                        </tr>
                        <tr>
                            <td><strong>bias_magnitude</strong></td>
                            <td>No quantitative general bias estimate provided in this paper; a cited example: an AI-driven idea-generation paper experienced a contentious ICLR review with 45 discussion posts before rejection (illustrative of high disagreement and contestation), but this is an anecdote rather than a measured bias magnitude.</td>
                        </tr>
                        <tr>
                            <td><strong>relationship_type</strong></td>
                            <td>heterogeneous / inconsistent: human reviewers vary widely in judgment, leading to disagreement and potential conservative outcomes for contentious/novel work, but no functional relationship quantified here.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_pattern</strong></td>
                            <td>Not quantified; authors note that peer review can be slow and variable but provide no time-lag metrics for recognition of transformational work.</td>
                        </tr>
                        <tr>
                            <td><strong>field_studied</strong></td>
                            <td>Machine learning / AI conferences (example from ICLR 2025) and general academic publishing context.</td>
                        </tr>
                        <tr>
                            <td><strong>field_differences</strong></td>
                            <td>Not systematically analyzed across fields in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_studied</strong></td>
                            <td>accept/reject decisions, reviewer discussion activity (qualitative), reviewer ratings where available.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_measure</strong></td>
                            <td>Not defined — human review outcome is treated as an imperfect benchmark in parts of the study (used as comparison for AI reviewer agent), and authors discuss the need for expert retrospective evaluations as better benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_truth_gap</strong></td>
                            <td>Not numerically specified for human reviews; the authors use human decisions as a comparison but also argue human review is inconsistent, raising doubt about its suitability as ground truth.</td>
                        </tr>
                        <tr>
                            <td><strong>incremental_vs_transformational</strong></td>
                            <td>Authors argue qualitatively that human peer review can be conservative and may resist transformational work (citing community unease around AI-authored research), but provide no numeric comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxy_failures</strong></td>
                            <td>Not addressed quantitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>automated_system_performance</strong></td>
                            <td>N/A (this entity concerns humans), but authors contrast human variability with AI reviewer shortcomings to argue neither is currently a reliable sole arbiter.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_bias</strong></td>
                            <td>N/A.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_tested</strong></td>
                            <td>No interventions tested here, though authors recommend pilot projects, benchmarks, and revised guidelines for AI use in authorship and review.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples</strong></td>
                            <td>Anecdotal: some reviewers or media high-profile positive commentary around AI Scientist exists (e.g., media/YouTube praise), showing occasional overvaluation or uncritical acceptance of AI claims; conversely, contentious community rejections (e.g., ICLR case) show rapid resistance — both are mentioned qualitatively.</td>
                        </tr>
                        <tr>
                            <td><strong>moderating_factors</strong></td>
                            <td>Reviewer expertise, review workload (superficial vs deep reviews), ability to inspect code/figures, and community attitudes toward AI all modulate human review outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size_and_methods</strong></td>
                            <td>Discussion and anecdotal examples drawn from community events (ICLR 2025 thread with 45 posts) and the authors' own comparison of AI reviewer recommendations to OpenReview outcomes (10 human papers), but no large-scale human-review statistical study performed in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews <em>(Rating: 2)</em></li>
                <li>Are We There Yet? Revealing the Risks of Utilizing Large Language Models in Scholarly Peer Review <em>(Rating: 2)</em></li>
                <li>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers <em>(Rating: 2)</em></li>
                <li>The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery <em>(Rating: 2)</em></li>
                <li>Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1886",
    "paper_id": "paper-276482965",
    "extraction_schema_id": "extraction-schema-37",
    "extracted_data": [
        {
            "name_short": "AI-novelty-classifier",
            "name_full": "AI Scientist novelty classification (Semantic Scholar keyword-based novelty detector)",
            "brief_description": "The AI Scientist's automated novelty assessor queries the Semantic Scholar API (up to 10 results, up to 10 iterations) and assigns a binary 'novel' label based on keyword matching rather than deep synthesis, causing systematic misclassification of well-established ideas as novel.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "automated novelty classifier / literature-search-based novelty assessment",
            "novelty_measure": "binary 'novel' label derived from Semantic Scholar keyword-match search results (up to 10 results, up to 10 iterations)",
            "bias_magnitude": "In the authors' test set the system labelled all 10 generated ideas plus both seed ideas as novel (12/12 = 100% labeled novel), including well-documented techniques (e.g., micro-batching, adaptive learning rates) — i.e., 100% false-positive rate for known items in this sample.",
            "relationship_type": "systematic positive-label bias (near-constant), i.e., tendency to over-assign 'novel' regardless of true novelty",
            "temporal_pattern": "No longitudinal recognition: literature retrieval appears insensitive to recency; manuscripts produced by the system had few recent citations (only 5 of 34 citations were from 2020+), indicating a recency/temporal gap in retrieved references.",
            "field_studied": "Recommender systems / Green Recommender Systems (applied domain used in experiments)",
            "field_differences": "Not evaluated across fields in this paper (single-domain test only).",
            "proxy_metric_studied": "binary novelty label assigned by automated literature search",
            "ground_truth_measure": "Authors' domain knowledge / existence of prior published work (manual verification by authors that techniques were known)",
            "proxy_truth_gap": "12/12 mismatch in the test sample (100% of items labelled novel despite prior literature), demonstrating a complete mismatch on the small test set.",
            "incremental_vs_transformational": "The classifier frequently labelled incremental applications of known techniques as 'novel'; paper reports many generated ideas were incremental despite being labelled novel, but no further numerical split provided between incremental vs transformational beyond examples.",
            "multiple_proxy_failures": "No — only the novelty-proxy was examined; compound proxy failures were not tested.",
            "automated_system_performance": "Poor for novelty-detection in this sample: complete failure to detect known prior work (100% false-positive novelty), due to reliance on shallow keyword matching of up to 10 search results.",
            "training_data_bias": "Not directly examined; system relies on external search API (Semantic Scholar) rather than an explicitly trained novelty classifier; authors note reliance on historical-indexed literature may bias results toward misses of established prior art or recent work.",
            "intervention_tested": "None — authors did not test corrective interventions for the novelty classifier in this study.",
            "counter_examples": "Authors note that in prior informal tests the AI Scientist occasionally flagged some ideas as non-novel, indicating non-deterministic behavior, but no formal counter-examples of correct novelty detection are quantified.",
            "moderating_factors": "Search depth and retrieval strategy (max 10 results, up to 10 iterations), keyword-based matching, and limits of the Semantic Scholar metadata extraction; dependence on user-provided seed ideas and templates can also steer outcomes.",
            "sample_size_and_methods": "Empirical test on 10 generated ideas + 2 seed ideas (12 items); novelty labels produced by the AI Scientist via Semantic Scholar queries; authors manually compared labels to known literature to identify misclassifications.",
            "uuid": "e1886.0"
        },
        {
            "name_short": "AI-reviewer-agent",
            "name_full": "AI Scientist reviewer agent (LLM-based automated peer review)",
            "brief_description": "An LLM-based reviewer agent that extracts text from PDFs (cannot interpret figures/tables) and produces structured reviews with numeric ratings and binary accept/reject recommendations; evaluated by authors on both AI-generated and human-written manuscripts, showing strong conservative bias and misalignment with human decisions.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "automated peer review (LLM-based reviewer agent)",
            "novelty_measure": "Numerical rating on 'originality' (and other dimensions) plus binary accept/reject; however, novelty assessment is based on textual analysis and limited metadata extraction (no figure/table understanding).",
            "bias_magnitude": "On a set of 10 human-written OpenReview papers (5 accepted by humans, 5 rejected), the AI reviewer rejected 9/10 papers (90% rejection). It rejected 4 of the 5 papers that had been accepted by human reviewers (80% false-negative rate for human-accepted papers). Overall agreement with human acceptance/rejection = 5/10 (50%). For AI-generated manuscripts, the agent recommended rejection for all 7 generated manuscripts (7/7 = 100% rejection).",
            "relationship_type": "conservative thresholding / strong negative bias (tendency toward rejection regardless of human acceptance), not a linear scaling with novelty but a majority-reject policy in tested sample",
            "temporal_pattern": "No temporal dynamics evaluated; limitations noted include inability to parse figures/tables and shallow depth of critique, which could persist across time but no time-course data provided.",
            "field_studied": "Machine learning / information retrieval papers (OpenReview; domain of the tested manuscripts).",
            "field_differences": "Not analyzed across fields; tested only on small OpenReview set and AI-generated manuscripts in ML/IR domain.",
            "proxy_metric_studied": "accept/reject recommendation and numeric rubric scores (originality, clarity, significance, soundness, presentation, contribution)",
            "ground_truth_measure": "Human OpenReview acceptance/rejection outcomes used as the ground-truth comparator in the authors' test.",
            "proxy_truth_gap": "Agreement with human decisions was 50% (5/10); 4 of 5 human-accepted papers were marked rejected by the AI reviewer (80% false-negative among human-accepted), indicating large gap between AI reviewer recommendations and human editorial decisions.",
            "incremental_vs_transformational": "Authors infer a conservative bias that could penalize novel/transformational work, but no direct stratified quantitative comparison by novelty level was provided beyond the observed high rejection of both AI-generated and some human-accepted papers.",
            "multiple_proxy_failures": "Not tested; only the automated reviewer performance vs human decisions was evaluated.",
            "automated_system_performance": "Structured and superficially plausible but lacking depth; fails to detect several serious methodological and structural issues; cannot interpret figures/tables; shows low alignment with human reviewers (50% agreement) and high rejection rates (90% on the tested human-paper sample).",
            "training_data_bias": "Not measured directly; authors caution generally that LLMs inherit historical-data biases, but no experiments altering training data were reported.",
            "intervention_tested": "No corrective interventions (e.g., calibration, ensemble with humans) were tested in this study.",
            "counter_examples": "One case where the AI reviewer recommended acceptance was a paper that had been rejected by human reviewers — an instance of AI/human divergence (AI accepted 1 paper that humans had rejected).",
            "moderating_factors": "PDF-text-extraction limitations (no figure/table understanding), reliance on structured review templates (may favor surface-level critiques), and dependence on the LLM used (model choice affects outputs); quality of input PDF and completeness of manuscript strongly moderate outcome.",
            "sample_size_and_methods": "Evaluation on 7 AI-generated manuscripts (self-reviewed by the system) and 10 human-written OpenReview papers (5 accepted, 5 rejected). Comparison between AI reviewer recommendations and OpenReview human outcomes provided the quantitative measures.",
            "uuid": "e1886.1"
        },
        {
            "name_short": "AI-citation-patterns",
            "name_full": "Citation and reference patterns in AI-generated manuscripts",
            "brief_description": "Empirical observations of the bibliographic support in AI-generated manuscripts: low citation counts and a paucity of recent references, indicating weak literature grounding and potential recency bias in automated literature retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "citation-count and recency as proxies for literature grounding / novelty support",
            "novelty_measure": "Not a formal novelty metric here; citations used as evidence of literature integration and novelty claims are cross-checked against presence/absence of prior art.",
            "bias_magnitude": "For 7 AI-generated manuscripts the median number of citations was 5 per paper; only 5 out of 34 total citations (14.7%) were from 2020 or later, indicating strong skew toward older literature in these outputs.",
            "relationship_type": "observational skew: AI-generated manuscripts show low citation volume and low recency; no formal functional relationship between novelty and citation pattern was estimated.",
            "temporal_pattern": "Recency deficiency in cited literature (only ~15% of citations from 2020+); no longitudinal follow-up on how citations to these AI-generated works change over time.",
            "field_studied": "Recommender systems / machine learning domain (domain of generated manuscripts).",
            "field_differences": "Not evaluated across fields.",
            "proxy_metric_studied": "short-term citation count per manuscript and recency distribution of cited works",
            "ground_truth_measure": "Manual verification by authors of whether cited literature covered relevant prior work; adequacy judged qualitatively by authors rather than a numeric gold standard.",
            "proxy_truth_gap": "Not reported as a numeric correlation; qualitative gap: many AI-generated manuscripts claimed novelty while failing to cite relevant prior art, indicating mismatch between citation-based evidence produced and true novelty/support.",
            "incremental_vs_transformational": "Authors found many AI-generated ideas were incremental and yet were presented as novel with sparse/dated citations — citation patterns did not reliably reflect true novelty vs incremental status.",
            "multiple_proxy_failures": "Not tested.",
            "automated_system_performance": "Automated literature retrieval used by the AI Scientist produced few and often outdated citations, undermining the manuscripts' ability to substantiate novelty claims.",
            "training_data_bias": "Not explicitly measured, though reliance on particular indexing/search APIs (Semantic Scholar) may bias which works are returned and cited.",
            "intervention_tested": "No interventions; authors recommend better dataset/metadata integration and repository links (PapersWithCode, GitHub) but did not test fixes.",
            "counter_examples": "Sakana's preprint itself received rapid attention (&gt;100 citations by early Feb 2025), showing that community attention (citation accumulation) can be high even for controversial or low-quality AI-generated claims; however this is a separate phenomenon from the AI-generated manuscripts analyzed.",
            "moderating_factors": "Quality of the experimental pipeline, user-provided templates, and the retrieval strategy (API limits) moderate citation outcomes.",
            "sample_size_and_methods": "Quantitative analysis of citations in 7 AI-generated manuscripts (total 34 citations) — median citations reported and recency distribution (counts from 2020+).",
            "uuid": "e1886.2"
        },
        {
            "name_short": "human-peer-review-variability",
            "name_full": "Human peer review variability and conservatism (qualitative observations)",
            "brief_description": "The paper documents known features of human peer review—subjectivity, reviewer disagreements, superficial reviews, and resistance or unease toward AI-driven submissions—illustrated by an ICLR 2025 case with an intense review thread, and positions AI review evaluation against this noisy human baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "evaluation_system_type": "human peer review (conference/journal review processes)",
            "novelty_measure": "Human reviewer judgments of novelty expressed in qualitative reviews and acceptance decisions; no standardized numeric novelty index used in this paper for human reviews.",
            "bias_magnitude": "No quantitative general bias estimate provided in this paper; a cited example: an AI-driven idea-generation paper experienced a contentious ICLR review with 45 discussion posts before rejection (illustrative of high disagreement and contestation), but this is an anecdote rather than a measured bias magnitude.",
            "relationship_type": "heterogeneous / inconsistent: human reviewers vary widely in judgment, leading to disagreement and potential conservative outcomes for contentious/novel work, but no functional relationship quantified here.",
            "temporal_pattern": "Not quantified; authors note that peer review can be slow and variable but provide no time-lag metrics for recognition of transformational work.",
            "field_studied": "Machine learning / AI conferences (example from ICLR 2025) and general academic publishing context.",
            "field_differences": "Not systematically analyzed across fields in this paper.",
            "proxy_metric_studied": "accept/reject decisions, reviewer discussion activity (qualitative), reviewer ratings where available.",
            "ground_truth_measure": "Not defined — human review outcome is treated as an imperfect benchmark in parts of the study (used as comparison for AI reviewer agent), and authors discuss the need for expert retrospective evaluations as better benchmarks.",
            "proxy_truth_gap": "Not numerically specified for human reviews; the authors use human decisions as a comparison but also argue human review is inconsistent, raising doubt about its suitability as ground truth.",
            "incremental_vs_transformational": "Authors argue qualitatively that human peer review can be conservative and may resist transformational work (citing community unease around AI-authored research), but provide no numeric comparisons.",
            "multiple_proxy_failures": "Not addressed quantitatively.",
            "automated_system_performance": "N/A (this entity concerns humans), but authors contrast human variability with AI reviewer shortcomings to argue neither is currently a reliable sole arbiter.",
            "training_data_bias": "N/A.",
            "intervention_tested": "No interventions tested here, though authors recommend pilot projects, benchmarks, and revised guidelines for AI use in authorship and review.",
            "counter_examples": "Anecdotal: some reviewers or media high-profile positive commentary around AI Scientist exists (e.g., media/YouTube praise), showing occasional overvaluation or uncritical acceptance of AI claims; conversely, contentious community rejections (e.g., ICLR case) show rapid resistance — both are mentioned qualitatively.",
            "moderating_factors": "Reviewer expertise, review workload (superficial vs deep reviews), ability to inspect code/figures, and community attitudes toward AI all modulate human review outcomes.",
            "sample_size_and_methods": "Discussion and anecdotal examples drawn from community events (ICLR 2025 thread with 45 posts) and the authors' own comparison of AI reviewer recommendations to OpenReview outcomes (10 human papers), but no large-scale human-review statistical study performed in this paper.",
            "uuid": "e1886.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews",
            "rating": 2
        },
        {
            "paper_title": "Are We There Yet? Revealing the Risks of Utilizing Large Language Models in Scholarly Peer Review",
            "rating": 2
        },
        {
            "paper_title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
            "rating": 2
        },
        {
            "paper_title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
            "rating": 2
        },
        {
            "paper_title": "Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents",
            "rating": 1
        }
    ],
    "cost": 0.01748225,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Evaluating Sakana's AI Scientist for Autonomous Research: Wishful Thinking or an Emerging Reality Towards 'Artificial Research Intelligence' (ARI)?
15 Oct 2025</p>
<p>Joeran Beel joeran.beel@uni-siegen.de 
Min-Yen Kan 
Moritz Baumgart moritz.baumgart@student.uni-siegen.de </p>
<p>Intelligent Systems Group &amp; Recommender-Systems.com
University of Siegen
Germany</p>
<p>Information Retrieval / Natural Language Processing Group (WING)
National University of Singapore -Web
Singapore</p>
<p>University of Siegen
Germany</p>
<p>Intelligent Systems Group &amp; Recommender-Systems.com
University of Siegen
Siegen, Min-Yen KanGermany</p>
<p>Information Retrieval / Natural Language Processing Group (WING)
National University of Singapore -Web
Singapore</p>
<p>Moritz Baumgart</p>
<p>University of Siegen
SiegenGermany</p>
<p>Evaluating Sakana's AI Scientist for Autonomous Research: Wishful Thinking or an Emerging Reality Towards 'Artificial Research Intelligence' (ARI)?
15 Oct 20257759ECDB6C688F0FA7427C472BD571C110.1145/nnnnnnnarXiv:2502.14297v3[cs.IR]2025-02. Manuscript submitted to ACM Manuscript submitted to ACMCCS Concepts:Computing methodologies → Machine learningArtificial intelligence• Information systems → Language models AI Science, AI Scientist, Sakana, Review
Recently, Sakana.aiintroduced the AI Scientist, a system claiming to automate the entire research lifecycle and conduct research autonomously, a concept we term Artificial Research Intelligence (ARI).Achieving ARI would be a major milestone toward Artificial General Intelligence (AGI) and a prerequisite to achieving Super Intelligence.The AI Scientist received much attention in the academic and broader AI community.A thorough evaluation of the AI Scientist, however, had not yet been conducted.We evaluated the AI Scientist and found several critical shortcomings.The system's literature review process is inadequate, relying on simplistic keyword searches rather than profound synthesis, which leads to poor novelty assessments.In our experiments, several generated research ideas were incorrectly classified as novel, including well-established concepts such as micro-batching for stochastic gradient descent (SGD).The AI Scientist also lacks robustness in experiment execution-five out of twelve proposed experiments (42%) failed due to coding errors, and those that did run often produced logically flawed or misleading results.In one case, an experiment designed to optimize energy efficiency reported improvements in accuracy while consuming more computational resources, contradicting its stated goal.Furthermore, the system modifies experimental code minimally, with each iteration adding only 8% more characters on average, suggesting limited adaptability.The generated manuscripts were poorly substantiated, with a median of just five citations per paper-most of which were outdated (only five out of 34 citations were from 2020 or later).Structural errors were frequent, including missing figures, repeated sections, and placeholder text such as "Conclusions Here".Hallucinated numerical results were contained in several manuscripts, undermining the reliability of its outputs.Despite its limitations, the AI Scientist represents a significant leap forward in research automation.It produces complete research manuscripts with minimal human intervention, challenging conventional expectations of AI-generated scientific work.Many reviewers or university instructors conducting only a superficial assessment may struggle to distinguish its output from that of human researchers, demonstrating how far AI has progressed in mimicking academic writing and structuring scientific arguments.While the quality of its manuscripts currently aligns with that of an unmotivated undergraduate student rushing to meet a deadline, this level of autonomy in research generation is remarkable.More strikingly, it achieves this at an unprecedented speed and cost efficiency-our analysis indicates that generating a full research paper costs only $6-$15, with just 3.5 hours of human involvement.This is significantly faster than traditional human researchers.Given that AI research automation was nearly nonexistent just a few years ago, the AI Scientist marks a substantial milestone toward Artificial Research Intelligence (ARI), signalling the acceleration of AI-driven scientific discovery.The AI Scientist also illustrates the urgent need for a discussion within the Information Retrieval (IR) and broader scientific communities.Whether and when ARI becomes a reality depends on how the academic and AI communities shape its development and governance.We propose concrete steps, including pilot projects and competitions, and standardized attribution frameworks such as research logs and markup languages.</p>
<p>Introduction</p>
<p>In autumn 2024, Sakana.ai, a Tokyo-based start-up that has raised $200 million in funding 1 , announced the "AI Scientist".</p>
<p>With the AI Scientist, Sakana boldly promised the "beginning of a new era in scientific discovery" [12].The open-source 2 AI Scientist is supposed to "automate the entire research lifecycle"; i.e. it generates research ideas, designs and conducts experiments, analyzes results, writes research papers, and finally even reviews them -essentially automating the daily work of millions of researchers worldwide.According to Sakana, the AI Scientist produces research papers for approximately $15 each.Sakana acknowledges "occasional flaws" and explains further limitations in a pre-print manuscript [8].Yet, based on Sakana's released information, most readers will understand that the AI Scientist appears to be a fully-functional system.Especially considering Sakana's claims that "It is worth noting that the [AI Scientist] can autonomously run the entire life cycle of machine learning research without any human intervention except for initial preparation" [13] and that the peer review system works "with near-human accuracy" [12].Sakana's AI Scientist is not the only AI tool that aims to support or even replace significant aspects of scientific work.A rapidly expanding body of research explores AI's role in scientific discovery, with recent preprints examining its impact on literature retrieval, idea generation, and automated experimental design.Several studies indicate that large language models (LLMs) can already generate research ideas comparable to those of human scientists [15, 18, 19],</p>
<p>with some findings suggesting AI may even surpass human creativity [11, 14].Google has framed AI as ushering in a "new era of scientific discovery" [10], while specialized workshops such as "Towards Agentic AI for Science" at ICLR 2025 highlight the field's growing momentum 3 .</p>
<p>Notably, just one day before the submission of our current manuscript for review to a major IR conference, and hours before uploading it to arXiv, Google announced the AI Co-Scientist 4 , indicating the emerging interest in autonomous research agents, or, as we term it 'Artificial Research Intelligence' (ARI), i.e. artificial intelligence that has not yet reached the level of general intelligence, but intelligence that is capable of conducting research that is indistinguishable from human research.In this context it is also interesting to note that [7] submitted a paper on AI-driven idea generation to ICLR 2025, which was rejected after an intense review process involving 45 discussion posts -described by the conference chair as "filled with drama" 5 .This case exemplifies the unease among researchers and reviewers who face the prospect of AI encroaching on their roles, raising questions about the future of human-led scientific inquiry.</p>
<p>Despite the large body of work, the AI Scientist is probably the tool that has made the boldest promises, yet.And, possibly because of this, it has gained the most attention, both within the academic community and among the broader AI enthusiast audience.As of early February 2025, Sakana's pre-print manuscript gained more than 100 citations 6 , their GitHub repository received 8.9k stars and it was forked over 1.3k times.Prominent AI content creators on YouTube and X, each with hundreds of thousands of followers [5, 16], enthusiastically discuss the AI Scientist.</p>
<p>Notably, the most positive reviewers of the AI Scientist, e.g., [5], have not tested the system themselves but rely solely on Sakana's information.Meanwhile, users in online forums and the GitHub issue trackers report difficulties in setting up the AI Scientist 7 or encountering technical issues 8 .Other discussions 9 use the AI Scientist's release as a broader reflection on AI's role in scientific discovery.Some reviews provide in-depth analyses 10 , examining its source code and generated papers.However, to the best of our knowledge, a comprehensive, independent assessment based on direct experimentation has yet to be conducted.</p>
<p>The AI Scientist is a machine learning driven system, making it highly relevant to the machine learning research community.However, its impact extends beyond machine learning, with profound implications for the Information Retrieval (IR) community, which motivates us to write this paper.While a recent SIGIR perspective paper [20] provided valuable insights into how LLMs are transforming traditional IR tasks, we must also consider a broader question: What happens when LLMs move beyond assisting IR research and begin to conduct it autonomously?This possibility is no longer theoretical -AI systems are increasingly capable of generating research ideas, designing experiments, and even evaluating scientific work.</p>
<p>In this paper, we take the next step in this discussion by providing an independent, systematic assessment of the AI Scientist's capabilities, limitations, and trajectory.Our goal is not only to evaluate its functionality but to highlight its potential role in reshaping scientific inquiry, particularly within information retrieval.By analyzing its performance across idea generation, experimentation, and manuscript production and review, we offer a comprehensive perspective on what AI-driven research tools can -and cannot -achieve today and may achieve in the future.</p>
<p>Our study focuses exclusively on Sakana's AI Scientist rather than other comparable tools due to its particularly bold claims and the significant attention it has garnered within the research community.While numerous AI-driven tools support various aspects of scientific research-including AI-assisted literature retrieval and summarization (Semantic Scholar, Elicit, Scite.ai),automated experiment design (IBM's Watson Discovery), and research paper generationthe AI Scientist stands out for its promise to fully automate the entire research lifecycle, from idea generation to peer review.This ambitious goal, combined with its open-source nature and extensive media coverage, makes it a uniquely compelling subject for independent evaluation.Other tools, while valuable in their respective domains, typically focus on specific tasks rather than claiming end-to-end automation of scientific inquiry.By concentrating on the AI Scientist, we aim to provide a detailed and critical analysis of its capabilities and limitations without diluting our focus across multiple tools with varying objectives and architectures.</p>
<p>Our findings reveal a system that, while far from replacing human researchers, demonstrates the potential to automate significant parts of the research process.The AI Scientist does not yet fulfill its promises, struggling with methodological soundness, experimental execution, and literature retrieval.However, these limitations are technical hurdles rather than fundamental barriers -challenges that will likely be addressed as AI-driven research systems evolve.</p>
<p>For the IR community, the emergence of such tools presents both an opportunity and a challenge.On one hand, they offer new ways to automate literature retrieval, citation analysis, and experimental design, potentially advancing the field.On the other, they raise critical questions about the role of human researchers, the nature of scientific discovery, and the reliability and generalizability of AI-generated knowledge.</p>
<p>The time to engage with these technologies is now.Whether by integrating them into research workflows, critically evaluating their outputs, or contributing to their development, IR researchers must take an active role in shaping the future of AI-driven science.These systems are poised to transform how knowledge is produced and disseminated, and the IR community is uniquely positioned to lead this transformation.</p>
<p>AI Scientist: Functionality and Evaluation</p>
<p>This section includes an overview of the features and architecture of the AI Scientist, case study snippets, critiques and recommendations for use.</p>
<p>Setup and Installation</p>
<p>All setup, coding and experiment execution were carried out by the third author of this manuscript, a third-year computer science Bachelor's student with basic machine learning knowledge but strong Python proficiency.A typical user of the AI Scientist is likely to have greater expertise, such as that of a trained researcher or machine learning engineer.Thus, our setup time and usability assessments should be considered lower bounds.We have made our experimental codebase that replicates our experimentation in this paper available on GitHub: https://code.isg.beel.org/ISG-Siegen/theai-scientist-reproduced.The repository includes a Singularity container definition file to facilitate full reproducibility and modifications in our experiments.</p>
<p>We installed the AI Scientist on two computers and found the setup to be relatively straightforward, contrary to some reports on the Web 11 .The entire process took approximately five hours, including minor troubleshooting.For more details, we refer the interested reader to our GitHub repository.</p>
<p>We first tested and prototyped the AI Scientist on a consumer laptop 12 .The main experiments were then conducted on our university's computing cluster 13 .Although the AI Scientist provides an experimental pipeline optimized for deep learning with PyTorch and an NVIDIA GPU (CUDA), our experiments used a simple recommendation algorithm, FunkSVD, that runs on a CPU.</p>
<p>The AI Scientist requires a foundation large language model (LLM) to function.At the time of our experiments, it supported OpenAI's gpt-4o-2024-05-13, multiple models from Anthropic's Claude series, DeepSeek Coder V2, and Meta's Llama 3.1.We opted for gpt-4o-2024-05-13.Since then, Sakana has expanded support to additional models.</p>
<p>Preparing for the experiments</p>
<p>We initially assumed the AI Scientist could autonomously conduct research based solely on a prompt.However, it requires a user-defined "template, " which significantly limits the autonomy of the AI Scientist.Such a template consist of several elements, described in the following.</p>
<ol>
<li>Goal and Research Direction.The research goal is specified via a prompt file (prompts.json).For our experiments, we focused on Green Recommender Systems [3], a research topic which aims to reduce the carbon footprint of recommender systems and its associated machine learning algorithms.We gave the prompt as: 14 :</li>
</ol>
<p>Listing 1.The general task provided as a JSON prompt { "task_description": "The attached code trains and evaluates the FunkSVD algorithm with stochastic ↩→ gradient descent for recommender systems.Your goal is to find novel ways to optimize energy-↩→ efficiency." } 2. Experimental Pipeline.The AI Scientist requires a Python-based experimental pipeline, consisting of an experiment.py,which defines dataset usage and model training as well as a plot.py,which handles result visualization.We implemented FunkSVD for collaborative filtering, training and evaluating it on an 80/10/10 split on MovieLens-100k (RMSE metric), with results visualized via line charts.This minimalist setup runs on a CPU, but larger datasets and complex models would likely increase compute costs.It is important to note that the AI Scientist requires this pipeline in a special format.Users cannot simply input any Python code but would have to adjust their existing code to be able to be used by the AI Scientist.</p>
<ol>
<li>Seed Ideas.The AI Scientist further requires user-provided seed ideas.We supplied two:</li>
</ol>
<p>(1) Adaptive Learning Rates for SGD.Aims to accelerate convergence, reducing computational cost and energy use.Though not novel, this ideas demonstrates our intended research direction.</p>
<p>(2) E-Fold Cross-Validation.An alternative to k-fold cross-validation that dynamically selects an optimal  to balance computational efficiency and reliability.This idea is also not entirely novel, but was proposed only recently [1, 4, 9].</p>
<p>The seed ideas must include values for "interestingness, " "feasibility, " and "novelty." However, we observed that these values had no apparent impact on the AI Scientist's processing.</p>
<p>Listing 2. e-fold Cross Validation; one of the two seed ideas { "Name": "e_fold_cv", "Title": "E-Fold Cross-Validation for Model Performance Estimation", "Experiment": "Develop an alternative to k-fold cross-validation named e-fold cross-validation.Instead ↩→ of a static k it uses an intelligently chosen or dynamically adjusted paramter e to optimize ↩→ the number of folds and to minimize computational energy while maintaining reliable model ↩→ performance estimates.","Interestingness": 8, "Feasibility": 7, "Novelty": 7 } 4. L A T E XTemplates.The AI Scientist generates research papers using predefined LaTeX templates.We used the default templates provided by Sakana without modification.</p>
<p>Idea Generation</p>
<p>Once provided with required input, the AI Scientist generated the following ten research ideas:</p>
<p>Manuscript submitted to ACM Feasibility, and Novelty, along with a binary "novel" label (True/False).However, while the novel label affects its workflow, the other scores appear arbitrarily assigned and are not used in further processing.To determine novelty, the AI Scientist queries the Semantic Scholar API, retrieving up to 10 search results per query.The system extracts search result metadata (title, authors, venue, year, citations, and abstracts) and can iterate up to 10 times before deciding.If no clear matches are found, it assigns novel=True; otherwise, it may classify the idea as non-novel.</p>
<p>Despite mimicking a literature review, this approach is unreliable.In our experiments, the AI Scientist classified all 10 generated ideas and both seed ideas as novel 15 , despite some being well-documented.For instance, micro-batching for SGD (Idea 7) is a known technique [6], as are adaptive learning rates (seed idea), hybrid matrix factorization (idea 10) and e-fold cross validation (seed idea).This misclassification likely results from the AI Scientist's reliance on keyword matching rather than any deeper synthesis of research.While some ideas may be novel within recommender systems, many simply apply existing techniques (e.g., pruning) to matrix factorization, offering incremental rather than groundbreaking improvements.</p>
<p>Conducting Experiments</p>
<p>The AI Scientist executes research ideas by modifying the user-provided experimental pipeline using Aider 16 , an LLM-driven coding assistant that iterates on experiment.py up to five times.This reliance on Aider adds complexity, as errors in one system propagate to the other.While the AI Scientist does introduce variation, it remains highly dependent on the provided framework, reworking the predefined templates rather than devising novel methodologies.</p>
<p>Our code analysis suggests minimal changes per iteration.Our template had 6,260 characters of code (255 lines), with Aider adding, on average, 529 characters (+8%) in the first iteration, followed by 118, 83, 66, and 21 characters in subsequent iterations (see Table 1).</p>
<p>The AI Scientist struggled with both implementation and methodological correctness.It failed to execute five of twelve ideas due to unresolved coding errors, often cycling through flawed versions of the same code.Even when producing executable code, outputs were frequently unreliable.For instance, in our e-fold cross-validation experiment, the AI Scientist intended to test  = {2, 3, 4, 5} but erroneously kept  fixed at 2, invalidating results.It also failed to re-run the baseline (k-fold), making e-fold cross validation appear superior -an impossible outcome because e-fold cross-validation by design cannot lead to better performance than k-fold cross validation but only energy savings (with, ideally, comparable performance) [1, 4, 9].There are many more similar examples that are available in our GitHub repository.</p>
<p>Coding Iteration</p>
<p>These issues reveal a fundamental limitation: the AI Scientist cannot critically assess its own results.It fails to detect methodological flaws or logical inconsistencies, making it unsuitable for autonomous scientific inquiry.Without human oversight, its findings could mislead researchers.</p>
<p>Manuscripts</p>
<p>The AI Scientist generated manuscripts for all seven successfully conducted experiments, with lengths ranging from six to eight pages (median: seven Given these flaws, these manuscripts would likely be rejected at reputable conferences or journals.Even at lower-tier venues, the lack of proper citations and unreliable results would be problematic.However, an undergraduate submitting such work for a course project might still pass if the instructor does not scrutinize the code or experiments.Therefore, in our opinion, the AI Scientist's output resembles that of a student rushing to complete an assignment -formally structured but careless in execution.</p>
<p>Review Functionality</p>
<p>The AI Scientist includes a reviewer agent for AI-assisted peer review, evaluated by us on both AI-generated and human-written manuscripts.To generate a review, the AI Scientist extracts text from a provided PDF but cannot interpret figures, tables, or supplementary material.It outputs a structured review (review.txt)containing a summary, identified strengths and weaknesses, improvement questions, numerical ratings for originality, clarity, significance, soundness, presentation, and contribution, plus a binary accept/reject recommendation.</p>
<p>Reviews of AI-Generated Manuscripts.For all seven manuscripts generated by the AI Scientist, its own review system recommended rejection.One common critique was that the research was based on a single, small dataset (MovieLens 100k).This criticism is justified, and the use of the single dataset not an intrinsic flaw of the AI Scientist itself, but rather a consequence of our experimental pipeline, which only included this dataset.Had our pipeline contained multiple datasets, the AI Scientist likely would have incorporated them.That being said, in an ideal scenario, the AI Scientist would be capable of retrieving and utilizing additional datasets autonomously, rather than being entirely dependent on user-supplied input.</p>
<p>The AI Scientist highlighted more valid concerns, such as weak theoretical justification and potential biases, but overlooked critical flaws, including redundant text, formatting errors, missing sections, and flawed experimental results.</p>
<p>Most notably, the reviewer agent did not identify any of the truly serious issues that we highlighted in the sections above.</p>
<p>While human reviews vary in quality -from superficial comments to detailed critiques -the AI Scientist's reviews were consistently structured, though lacking depth.Despite this, they appeared plausible enough that editors and conference chairs might not immediately recognize them as AI-generated.While less rigorous than strong human reviews, they were still more substantive than the brief, low-effort reviews that sometimes occur in the conference reviewing process.</p>
<p>Reviews of Human-Written Manuscripts.We tested 18 the AI Scientist's reviewing capabilities with ten human-written papers on OpenReview.net 19 , five that were accepted and five rejected.Reviews created by the AI Scientist followed the same structured format as for AI-generated manuscripts, including summaries, critiques, improvement suggestions, and numerical ratings.</p>
<p>While indicative, the results were striking: the AI Scientist rejected 9 out of 10 papers, including four that had been accepted by human reviewers, while recommending acceptance for only one, which had been rejected on OpenReview.</p>
<p>This suggests a strong conservative bias and misalignment with human judgment.Despite this, its reviews raised valid concerns about methodology and biases but lacked depth, often missing broader contributions and argumentation.</p>
<p>Our evaluation of the AI Scientist reviewer agent, illustrates a challenge: how to evaluate AI-generated reviews,</p>
<p>given the inconsistencies of human peer review?Should human judgment be the benchmark?Peer review is known for subjectivity, reviewer disagreements, and biases.An AI system that diverges from human reviewers may still highlight valid weaknesses or apply overly rigid criteria.</p>
<p>This raises another fundamental question: should AI peer review aim to mimic human reviewers or provide a more systematic alternative?If the former, the AI Scientist falls short.If the latter, human evaluation alone may be an insufficient benchmark.A better assessment would compare AI-generated reviews against expert evaluations that measure consistency and validity.</p>
<p>Ultimately, while structured and plausible, the AI Scientist's reviews lack the depth needed for academic evaluation.</p>
<p>It follows predefined templates but struggles with context-dependent judgment.Without improvements in assessing contributions, argumentation, and methodology, it remains better utilized as feedback tool, rather than a peer-review replacement.</p>
<p>Costs and Effort</p>
<p>When assessing the AI Scientist's potential, cost is a crucial factor -and surprisingly, it remains low.The total cost 20 for our main experiments, which included generating 10 ideas, running experiments for 7 ideas (from 12 total, including 2 seed ideas, of which 5 failed), and producing 7 manuscripts with reviews, amounted to only $42 USD -an average of $6 per manuscript.Given that our experiments were less complex than typical recommender systems research or those conducted by Sakana, their claim that a full research paper can be produced for around $15 seems realistic.</p>
<p>However, human labor is still required.Setting up the AI Scientist took approximately 5 hours, while implementing the experimental template required an additional 15 hours.It must be noted that if we were to re-implement similar pipelines, the required time would decrease due to the learning curve.If we were to implement more complex experiments, however, time would increase.Beyond this, we spent about 10 more hours brainstorming initial seed ideas, reviewing outputs, and refining results.Excluding the initial setup time, this totals 25 hours of human effort -approximately 3.5 hours per manuscript -mainly from an undergraduate student.</p>
<p>In summary, producing a research paper -even of relatively low quality -at an average cost of $6 to $15 and 3.5 hours of human effort, is remarkable.By comparison, we estimate that an undergraduate student would require at least 20 to 40 hours, while an experienced researcher -who would likely be reluctant to produce work of similar qualitymight take 10 to 20 hours.Thus, the AI Scientist operates approximately 3 to 11 times faster than a human researcher at negligible costs.</p>
<p>Limitations</p>
<p>Our own evaluation had limitations: we used a single dataset (MovieLens-100k) and one research domain (Green Recommender Systems), relied on fixed seed ideas and one template.While these constraints ensured a controlled assessment, they may affect generalizability and reproducibility.Despite this, we believe our findings remain meaningful.</p>
<p>Future Work and Recommendations</p>
<p>We outline several actionable recommendations for the Information Retrieval (IR) community, emphasizing practical steps for researchers, conference organizers, journal editors, and institutions.While these recommendations are primarily targeted at IR, they also reflect broader best practices in academia.</p>
<p>Researchers should familiarize themselves with research-assistive AI tools</p>
<p>At present, tools like the AI Scientist hold little practical value.In contrast, more general AI tools, such as ChatGPT for text generation and GitHub Copilot for programming, already provide substantial benefits in research, improving writing, coding, and brainstorming efficiency.Researchers who ignore these readily available AI-powered assistants are already at a disadvantage compared to their peers who integrate them into their workflows.</p>
<p>Looking ahead, tools like the AI Scientist will become vastly more powerful and their use may be ubiquitous.Unlike today's prototypes, we forecast that future iterations will reliably generate research ideas, execute complex experiments, and draft high-quality manuscripts with minimal human oversight.Notably, such research-assistive AI capitalize on the underlying emergent capabilities and rapid performance progress of their underlying foundation models.</p>
<p>Early-career researchers, in particular, must closely monitor these developments.The use of AI in research will soon become indispensable.Those who fail to adopt AI tools risk falling behind their peers, akin to a researcher today relying on a typewriter and calculator instead of adopting a computer with text editor and programming language support.The future of research belongs to those who embrace AI-enhanced workflows and integrate them into their daily scientific practice.</p>
<p>The academic community must prepare for this shift and ideally lead the discussion on how AI should be integrated into scientific research.To do so, researchers should actively engage with tools like the AI Scientist, follow advancements in this domain, and, where possible, contribute to their development and define their appropriate use.By understanding and shaping these tools, researchers ensure that AI development aligns with rigorous scientific principles rather than being dictated solely by industry interests.</p>
<p>Updated Guidelines for Authors and Researchers on the Use of AI</p>
<p>The ACM and other publishers provide guidelines on using generative AI in manuscript preparation.Currently, ACM permits AI to assist in manuscript writing, including generating text, images, tables, and code21 .However, the potential for AI to conduct research and autonomously generate entire manuscripts and experimental workflows has not yet been addressed by these policies.</p>
<p>We urge the IR community and publishers, such as ACM, to initiate discussions on how AI-driven research tools, like the AI Scientist, should best be integrated into scientific workflows.Specifically, the community should establish policies as to what extent AI-generated research can be submitted to conferences such as SIGIR.</p>
<p>At present, our assessment suggests that tools like the AI Scientist are not yet reliable enough to contribute meaningfully to research.Consequently, manuscripts generated entirely by AI should not be accepted for publication at this time.However, this stance will need to be revisited regularly as AI capabilities evolve rapidly.</p>
<p>We recommend that the academic community, particularly editors of journals and members of steering or ethics committees, take an active role in revising guidelines regarding the use of AI tools in research.These revisions should be developed in collaboration with experts who possess in-depth knowledge of the capabilities and limitations of AI systems, such as the AI Scientist.Additionally, input from stakeholders like peer reviewers, program chairs of conferences, and other members of the academic publishing ecosystem will be essential to ensure that these tools are integrated effectively while maintaining academic integrity and quality control.</p>
<p>Addressing the Risks of AI-Generated Assignments and Homeworks</p>
<p>The ability of AI tools such as the AI Scientist to generate well-structured academic papers that are hard to distinguish from student submissions poses an immediate challenge for universities.These systems can produce coherent, plausible reports that, especially under time constraints, may not be easily recognized as AI-generated by instructors.This presents a fundamental risk to academic integrity, as students can now generate entire assignments without engaging in the learning process.The issue is no longer speculative-AI-generated work is already at a level where instructors may fail to detect it without in-depth scrutiny.It is also difficult to prosecute, as the stochastic nature of such tools produce diverse output that is difficult to attribute back to a plagiarised source.Universities must therefore act urgently to adapt their assessment methodologies and reinforce academic standards.</p>
<p>A first step is to rethink assignment design, prioritizing evaluations that require learners to demonstrate personal mastery of the subject matter.This can be through iterative work, personal reflection, or in-person verification.</p>
<p>Traditional take-home essays should be supplemented with oral examinations, in-class assignments, and project-based assessments where students demonstrate an evolving understanding of the subject, which adds value beyond what AI tools yield as a first draft.Additionally, universities should establish clear policies on AI usage, distinguishing acceptable from unacceptable applications of these tools and incorporating AI literacy into curricula.Educating students on the ethical implications and potential academic consequences of AI misuse can encourage responsible engagement.</p>
<p>Furthermore, while AI-detection tools remain unreliable, institutions should implement verification mechanisms such as requiring students to submit intermediate drafts, video explanations, or interactive discussions about their work.</p>
<p>These measures, when combined with updated academic integrity policies and structured instructor training, can help mitigate the risks posed by AI-generated assignments and uphold the value of university education.</p>
<p>Benchmarking AI-Driven Research Agents in Information Retrieval</p>
<p>We propose developing structured benchmarks to evaluate AI-driven research agents, recognizing the need for broader community discussion to establish consensus on assessment criteria.Such benchmarks are essential to determine when AI agents achieve "Scientific AGI, " the point at which they can autonomously conduct research at human-level quality.</p>
<p>A critical area is AI-powered peer review.These systems generate structured reviews, yet their reliability remains untested.Benchmarks could include expert annotations, inter-reviewer agreement analyses, and alignment with evaluation criteria to compare AI and human reviews.</p>
<p>AI-driven idea generation also requires assessment.Evaluations could compare AI-generated ideas with expert proposals, analyzing acceptance rates and citation impact.Experimental AI agents should be benchmarked for their ability to implement methodologies, configure parameters, and ensure reproducibility.Failure analysis is crucial to detect methodological flaws.</p>
<p>Manuscript generation benchmarks should go beyond fluency, assessing argument structuring, literature integration, and empirical accuracy.Automated inconsistency detection, including hallucinated citations, is essential.Blinded peer review comparisons and authorship attribution frameworks may help distinguish AI contributions.</p>
<p>Other AI-assisted tasks, including literature review and dataset selection, require evaluation.AI benchmarks could include retrieval relevance metrics, citation analysis, and bias detection to meet expert standards.</p>
<p>Manuscript submitted to ACM Developing a comprehensive benchmarking suite requires collaboration across academia, industry, and publishers.</p>
<p>Standardized datasets, evaluation protocols, and performance metrics would help ensure scientific rigor and meaningful AI integration into research.Establishing these benchmarks will provide clear indicators of AI progress toward autonomous, human-level scientific inquiry.</p>
<p>Release of AI Research Logs for Transparency</p>
<p>A key advantage of AI-generated research is its ability to systematically log every step of the process.Unlike human researchers, AI systems can capture all experimental iterations, hyperparameter adjustments, and decision rationales in real time.To maximize transparency and reproducibility, AI-generated studies should include execution logs detailing raw input prompts, intermediate code modifications, and abandoned experimental runs.These logs would enable human and AI researchers to trace the AI's reasoning process, identify potential errors, and refine methodologies.</p>
<p>Comprehensive versioning of all research artifacts should be mandatory.Every change to code and experimental setups should be stored in structured repositories, such as Git with detailed commit histories.Timestamped metadata should accompany these changes, documenting the AI model, system configuration, and key decision points.This will allow researchers to track the evolution of AI-generated research and compare different experimental approaches systematically.</p>
<p>To further improve reproducibility, AI-generated research should be accompanied by containerized environments encapsulating dependencies and configurations.Using tools like Docker or Singularity, researchers should provide prebuilt containers that replicate the AI's computational environment.Additionally, automated reports summarizing why specific methodologies were selected or discarded would provide insight into the AI's decision-making process, helping both human researchers and future AI agents build on existing work more effectively.</p>
<p>Pilot Projects and Competitions</p>
<p>The IR community can learn from initiatives such as ICLR 2025 and NeurIPS, which have explored generative AI's role in peer review and manuscript submission through pilot projects.Expanding on these efforts, targeted pilot projects can further investigate AI's impact on scientific research.</p>
<p>One promising initiative is a competition assessing AI-generated research.We propose integrating such a competition into a major conference like SIGIR, where AI-generated papers would be evaluated alongside human-authored research through a blind peer-review process.This would provide an unbiased comparison of AI's ability to conduct rigorous scientific inquiry.</p>
<p>While current AI research tools remain limited, a competition of this nature could become viable within the next few years.Early planning and discussion within the community would ensure that appropriate evaluation frameworks and ethical considerations are established ahead of time, enabling a structured and fair assessment of AI-driven scientific contributions.</p>
<p>AI-Generated Content Tagging: Research Attribution Markup Language (RAML)</p>
<p>As AI-generated research becomes more prevalent, distinguishing between human-authored and AI-assisted content is critical for transparency, academic integrity, and reproducibility.Existing solutions for image watermarking and document metadata do not adequately address the need for systematic attribution in text-based research outputs.</p>
<p>Therefore, we propose the Research Attribution Markup Language (RAML), a standardized method to tag AI-generated content within academic manuscripts.RAML provides a structured approach to tracking AI contributions in research manuscripts, promoting transparency and accountability.As AI plays an increasing role in scientific discovery, standardized attribution mechanisms will be essential to maintaining the credibility and integrity of academic publishing.</p>
<p>Research Process Markup Language (RPML)</p>
<p>The AI Scientist faces difficulties in tracking detailed experimental metadata.Modifications made by its LLM-based coding assistant (Aider) are often undocumented, leading to reproducibility issues and making it hard to trace experimental changes.</p>
<p>We propose developing a standardized Research Process Markup Language (RPML) that captures all aspects of the research workflow in a structured format.RPML-based on a schema such as JSON or XML-should record experiment setups, code versions, container images, dataset versions, hyperparameters, and citation contexts automatically, ensuring full traceability and reproducibility.</p>
<p>Standardized Dataset Annotation and Integration</p>
<p>Tools like the AI Scientist rely on user-supplied datasets that often lack detailed, standardized metadata, limiting its ability to autonomously select high-quality relevant datasets for experiments.</p>
<p>We propose to encourage dataset providers (e.g., Hugging Face, Kaggle, UCI Machine Learning Repository) to adopt a standardized annotation schema.This schema should include comprehensive metadata such as domain applicability, data quality scores, versioning, benchmark results, and recommended use cases.With uniform metadata, the AI Scientist can automatically query and integrate datasets that best fit the experimental requirements.</p>
<p>Code Repositories Integration</p>
<p>The AI Scientist lacks access to recent baseline implementations from the state-of-the-art literature, making it challenging to benchmark its innovations against current best practices.</p>
<p>We propose to establish direct integrations with platforms such as PapersWithCode, GitHub, and Code Ocean.</p>
<p>Standardized APIs and metadata from these platforms can be used to retrieve relevant source code repositories and performance metrics, allowing the AI Scientist to automatically download, run, and compare these baselines against its own results.</p>
<p>Participatory Strategic Retreat</p>
<p>The ideas presented in our paper should be seen as an initial stimulus for discussion.To explore the future of generative AI in IR research, key stakeholders in the community must be involved-particularly leading experts in the IR field, as well as young researchers whose future will be significantly shaped by generative AI.We believe that a strategic, multi-day workshop or retreat -such as ones held by the Dagstuhl or NII Shonan seminar series -would be the right setting for engendering an initial, larger-scale discussion where such in-depth discussions can inform the writing of white papers that codify opinions, similar to our opinions here.Such papers can then help catalyse the community to comment on and organize significant IR efforts, in practice and research.</p>
<p>Discussion and Conclusion</p>
<p>Our study confirms that the AI Scientist represents a significant step toward automating scientific research, yet our evaluation reveals that its current capabilities remain far from fulfilling its ambitious promises.While the AI Scientist can generate research ideas, it only sometimes successfully conducts experiments, and many of its generated ideas are not truly novel.It heavily depends on user input, struggles with methodological soundness, and lacks the ability to critically assess its own results.In its present form, the AI Scientist is, at best, an advanced research assistant that requires a lot of supervision, rather than an independent scientific agent.</p>
<p>A key takeaway from our study is that we believe AI systems like the AI Scientist will meaningfully contribute to the scientific critique process in the near future, benefiting both authors and reviewers.Researchers could leverage AI-generated feedback to iteratively refine their work through adversarial loops in which an AI challenges their hypotheses, methodologies, and analyses.Similarly, peer reviewers could use AI-assisted prompts to enhance their critiques, ensuring consistency and depth.Some academic conferences, such as ICLR, have already incorporated AIgenerated review suggestions into their workflows.However, our findings indicate that AI-generated reviews by the AI Scientist often focus on surface-level critiques, while failing to detect deeper methodological flaws.This may make the job of second-level reviewers (such as area chairs or meta-reviewers) more important, as AI-generated reviews may lack depth but be well-formatted and directly address the review form's criteria.Such consolidators will have to look beyond surface-level analyses to judge whether a work holds promise.Despite these potential benefits, integrating AI into scientific research presents ethical and practical challenges.</p>
<p>AI models inherit biases from historical data and cannot independently distinguish between scientific quality and consensus.This limitation raises concerns that AI tools might reinforce outdated methodologies or amplify biases in peer review and publishing.Additionally, junior researchers may develop an overreliance on AI-generated suggestions, leading to automation bias that stifles innovation and independent thought.Research further indicates that AI can enhance scientific productivity, as shown in [17], yet its impact on researcher satisfaction remains complex.In one Manuscript submitted to ACM study, while AI-assisted workflows increased efficiency, 82% of scientists reported lower job satisfaction afterwards.</p>
<p>This paradox exemplifies a broader dilemma-if AI can outperform humans in key research tasks, what remains for human scientists to do? Addressing these risks requires AI-assisted workflows that actively encourage users to critically reassess assumptions and explore alternative perspectives.</p>
<p>One pressing concern is the potential for mass AI-generated paper submissions.Tools like the AI Scientist could enable researchers to generate large volumes of seemingly novel but low-value papers by tweaking existing codebases and experimental parameters.Such submissions could overwhelm academic venues, burdening already-overworked reviewers.The precedent set by AI-generated nonsense papers being accepted into conferences, such as those produced by SciGen, demonstrates the urgency of this issue.Unlike SciGen, modern AI-generated papers are often indistinguishable from human writing, making them harder to detect.Without proactive measures, the academic publishing ecosystem risks being inundated with research that adds quantity but not quality.To mitigate this risk, improved authorship verification, AI disclosure policies, and systematic detection methods must be developed.</p>
<p>The same risks extend to students using the AI Scientist to generate academic assignments.AI-generated essays, reports, and research papers could bypass the learning process, allowing students to submit work that appears wellstructured but lacks genuine understanding or critical engagement with the subject matter.This raises significant concerns about academic integrity, as automated content generation makes plagiarism detection more difficult and blurs the line between acceptable assistance and unethical practices.Moreover, reliance on AI for coursework could hinder students' ability to develop essential skills in reasoning, analysis, and problem-solving.To address this, educators must adapt assessment strategies, incorporating oral defenses, iterative feedback loops, and AI-detection tools to ensure that students engage meaningfully with their work rather than merely outsourcing it to AI.Finally, the Information Retrieval (IR) community must actively engage with these developments.The AI Scientist relies on IR techniques for literature search, data retrieval, citation analysis, and experimental design.As AI-driven research automation grows, the IR community has the opportunity to shape its methodologies.Addressing limitations in retrieval quality, document ranking, and automated citation analysis will be critical to ensuring AI-generated research is both credible and impactful.</p>
<p>In summary, while the AI Scientist falls short of its grand claims, it offers a glimpse into the future of AI-driven
scientific
1 .
1
Factor pruning for energy-efficient matrix factorization 2. Dynamic factor adjustment in matrix factorization 3. Green metrics for sustainable recommender systems 4. Quantization techniques for energy-efficient SGD 5. Time-aware early stopping 6. Interaction-priority SGD for energy efficiency 7. Micro-batching for energy-efficient SGD 8. Sparse-aware SGD for matrix factorization 9. Cluster-aware SGD for matrix factorization 10.Hybrid matrix factorization Each generated idea includes a brief description and three AI Scientist judged numerical scores (1-10) for Interestingness,</p>
<p>Looking ahead, the AI Scientist and similar tools represent an extraordinary technological leap.AI systems that once struggled to generate coherent text can now propose research ideas, design experiments, and draft manuscripts.While the AI Scientist currently operates at the level of an unmotivated undergraduate student, continued advancements could soon enable AI to produce research indistinguishable from that of dedicated Master's or PhD students.Future iterations may integrate real-time literature retrieval, deeper theoretical reasoning, and more rigorous experimental validation, progressively narrowing the gap between AI-driven and human-led scientific discovery.Recent developments such as Google's 22 and OpenAI's 23 Deep Research indicate that AI-driven research automation is rapidly advancing in this direction.</p>
<p>Table 1 .
1
Net change in character count (sum of additions and deletions) compared to the previous iteration.The baseline implementation contained 6,260 characters across 225 lines of code.
IDIdea12345Seed 1e-fold+ 818 (13.1%)0 (0%)0 (0%)0 (0%)0 (0%)Seed 2Adaptive Learning Rate+ 95 (1.5%)+ 766 (12.1%) + 407 (5.7%) + 28 (0.4%) + 1 (0%)1Factor PruningFailed to run2Dynamic Factor Adjustment+ 538 (8.6%)0 (0%)0 (0%)0 (0%)0 (0%)3Green Metrics+ 168 (2.7%)+ 97 (1.5%)+ 162 (2.5%) + 176 (2.6%) -18 (0.3%)4Quantized SGD+ 1173 (18.7%)-36 (0.5%)+ 17 (0.2%) + 258 (3.5%) + 169 (2.2%)5Time Aware Early Stopping+ 392 (6.3%)+ 4 (0.1%)0 (0%)0 (0%)0 (0%)6Interaction PriorityFailed to run7Micro BatchingFailed to run8Sparse Aware SGD+ 524 (8.4%)+ 1 (0%)0 (0%)0 (0%)0 (0%)9Cluster Aware SGDFailed to run10Hybrid MFFailed to run</p>
<p>Manuscript submitted to ACMRAML is a JSON-based schema embedded within research papers, either as inline annotations or in a separate metadata file.It enables authors and reviewers to track which sections of a manuscript were generated, modified, or influenced by AI.RAML defines various levels of AI involvement: Generated, meaning the content was fully created by AI; Edited, where the content was generated by AI but reviewed and revised by a human; and Suggested, in which the content was suggested by AI but primarily authored by a human.RAML also includes metadata on the AI model used, such as GPT-4, Claude, or Gemini, along with version and parameters.Additionally, it supports version control by storing AI-generated drafts, change logs, and prompts used.To encourage adoption, RAML should be incorporated into academic publishing standards such as ACM, IEEE, and Springer, which should establish guidelines requiring RAML annotations in AI-assisted papers.Preprint servers like arXiv and OpenReview should support RAML metadata for AI disclosure.Journals and conferences should require RAML compliance for submissions involving AI.Digital Object Identifiers (DOI) should include RAML information for research reproducibility.</p>
<p>Dublin Core, Open Data Initiative) and facilitate structured data deposits in version-controlled repositories, providing verifiable provenance for scientific claims.These mechanisms incentivize transparent and reproducible research, potentially leading to new prestige metrics or blockchain-based certification systems for replication integrity.</p>
<p>Another promising role for the AI Scientist lies in replication and validation.Reproducibility is a cornerstone of scientific credibility, as highlighted by recent initiatives in recommendation systems (e.g., the Best Paper Award at RecSys) and NLP (e.g., ReproHum initiatives).AI could be employed to establish proof of work, ensuring that findings are replicable and that experimental procedures are verifiable.To achieve this, AI-driven replication efforts must prioritize transparency through open-weight and open-data models.Additionally, AI can support automated metadata creation and tagging (e.g.,</p>
<p>discovery and towards 'Artificial Research Intelligence' (ARI).With responsible development and integration, AI could enhance knowledge generation, improve reproducibility, and streamline the research process in ways we are only beginning to explore.However, realizing this potential requires careful consideration of AI's limitations, ethical challenges, and long-term implications for the scientific community.The time to act and to participate in evaluating, exploring, discussing and developing AI-driven research agents is now! 22https://blog.google/products/gemini/google-gemini-deep-research/ 23https://openai.com/index/introducing-deep-research/</p>
<p>https://sakana.ai/series-a/
https://github.com/SakanaAI/AI-Scientist
https://blog.iclr.cc/2025/02/03/workshops-at-iclr-2025
https://blog.google/feed/google-research-ai-co-scientist/
https://openreview.net/forum?id=GHJzxPgFa6&amp;noteId=tN3UypVtcw
https://scholar.google.com/scholar?cites=3682611450429091902 Manuscript submitted to ACM
https://medium.com/@nimritakoul01/evaluating-the-ai-scientist-63e419e575b8 Manuscript submitted to ACM
Beel &amp; Kan et al.
https://www.reddit.com/r/learnmachinelearning/comments/1fuw2yb/has_anybody_gotten_sakana_ai_scientist_to_work/
HUAWEI MateBook D 16, Windows 11, Intel Core i5-12450H with integrated graphics, 16GB RAM (3733 MT/s)
Single node with 2 AMD EPYC 7452 CPUs (32 cores, 2.35-3.35 GHz, 128 MB cache) and 256 GB DDR4 RAM (3200 MHz), running Rocky Linux 8.8
All prompts are shortened and rephrased for brevity; the originals are available in our GitHub repository.Manuscript submitted to ACM
In prior tests, the AI Scientist occasionally flagged some ideas as non-novel, showing that it does not always default to a "novel" classification.
https://Aider.chat/ Manuscript submitted to ACM
To accomplish this, we modified the AI Scientist; this modification is available on our GitHub account.
https://openreview.net/ Manuscript submitted to ACM
We report only costs of OpenAI's API; other costs such as electricity for our laptop or university cluster are ignored Manuscript submitted to ACM
https://www.acm.org/publications/policies/frequently-asked-questions#h-can-i-use-generative-ai-software-tools-to-prepare-my-manuscript Manuscript submitted to ACM
Manuscript submitted to ACM
AcknowledgementsWe used generative AI (ChatGPT) to improve this manuscript, namely for thorough proofreading and brainstorming[2].
2024. e-Fold Cross-Validation for Recommender-System Evaluation. Moritz Baumgart, Lukas Wegmeth, Tobias Vente, Joeran Beel, International Workshop on Recommender Systems for Sustainability and Social Good (RecSoGood) at the 18th ACM Conference on Recommender Systems (ACM RecSys. </p>
<p>Our use of AI-tools for writing research papers. Joeran Beel, Intelligent Systems Group. 2024</p>
<p>Green Recommender Systems: A Call for Attention. Joeran Beel, Alan Said, Tobias Vente, Lukas Wegmeth, 10.31219/osf.io/5ru2gACM SIGIR Forum. 582024. Dec. 2024</p>
<p>Joeran Beel, Lukas Wegmeth, Tobias Vente, E-fold Cross-validation: A Computing and Energy-efficient Alternative to K-fold Crossvalidation with Adaptive Folds. 2024Proposal</p>
<p>. Osf Preprints, 10.31219/osf.io/exw3j2024</p>
<p>Matthew Berman, AI Scientist" Can Discover New Science! (Self-Improving AI = AGI). YouTube2024. 2024</p>
<p>Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification. Prateek Jain, Rahul Sham M Kakade, Praneeth Kidambi, Aaron Netrapalli, Sidford, Journal of machine learning research. 182232018. 2018</p>
<p>Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xingxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, Deli Zhao, Yu Rong, Tian Feng, Lidong Bing, arXiv:2410.13185Chain of Ideas: Revolutionizing Research Via Novel Idea Development with LLM Agents. 2024. 2024</p>
<p>Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, David Ha, arXiv:2408.06292[cs.AIThe AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. 2024</p>
<p>From Theory to Practice: Implementing and Evaluating e-Fold Cross-Validation. Christopher Mahlich, Tobias Vente, Joeran Beel, International Conference on Artificial Intelligence and Machine Learning Research (CAIMLR). 2024</p>
<p>A new era of discovery. James Manyika, Google Blog. 2024. 2024</p>
<p>Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S Weld, arXiv:2409.14634Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination. 2025. 2025</p>
<p>Sakana, The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. Sakan, Blog, 2024. 2024</p>
<p>Sakana "AI Scientist. Ai Sakana, Sakana.ai Website2024. 2024</p>
<p>Chenglei Si, Diyi Yang, Tatsunori Hashimoto, arXiv:2409.04109Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers. 2024. 2024</p>
<p>Two Heads Are Better Than One: A Multi-Agent System Has the Potential to Improve Scientific Idea Generation. Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, Nanqing Dong, arxiv:2410.094032024. 2024</p>
<p>TheAIGRID. 2024SakanaAI New AI AGENT Researcher STUNS The ENITRE INDUSTRY! YouTube. 2024</p>
<p>Aidan Toner-Rodgers, arXiv:2412.17866Artificial Intelligence, Scientific Discovery, and Product Innovation. 2024. 2024</p>
<p>AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews. Keith Tyser, Ben Segev, Gaston Longhitano, Xin-Yu Zhang, Zachary Meeks, Jason Lee, Uday Garg, Nicholas Belsten, Avi Shporer, Madeleine Udell, Dov Te'eni, Iddo Drori, 2024. 2408. 202410365</p>
<p>Are We There Yet? Revealing the Risks of Utilizing Large Language Models in Scholarly Peer Review. Rui Ye, Xianghe Pang, Jingyi Chai, Jiaao Chen, Zhenfei Yin, Zhen Xiang, Xiaowen Dong, Jing Shao, Siheng Chen, arXiv:2412.017082024. 2024</p>
<p>Large Language Models and Future of Information Retrieval: Opportunities and Challenges. Chengxiang Zhai, 10.1145/3626772.3657848Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. the 47th International ACM SIGIR Conference on Research and Development in Information RetrievalWashington DC, USA; New York, NY, USAAssociation for Computing Machinery2024SIGIR '24)</p>            </div>
        </div>

    </div>
</body>
</html>
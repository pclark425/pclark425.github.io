<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6045 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6045</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6045</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-272635554</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2305.17819v3.pdf" target="_blank">Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation</a></p>
                <p><strong>Paper Abstract:</strong> The paper introduces a framework for the evaluation of the encoding of factual scientific knowledge, designed to streamline the manual evaluation process typically conducted by domain experts. Inferring over and extracting information from Large Language Models (LLMs) trained on a large corpus of scientific literature can potentially define a step change in biomedical discovery, reducing the barriers for accessing and integrating existing medical evidence. This work explores the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery. The framework involves of three evaluation steps, each assessing different aspects sequentially: fluency, prompt alignment, semantic coherence, factual knowledge, and specificity of the generated responses. By splitting these tasks between non-experts and experts, the framework reduces the effort required from the latter. The work provides a systematic assessment on the ability of eleven state-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two prompting-based tasks: chemical compound definition generation and chemical compound-fungus relation determination. Although recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities. The ability of LLMs to serve as biomedical knowledge bases is questioned, and the need for additional systematic evaluation frameworks is highlighted. While LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases in a zero-shot setting, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6045.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6045.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM-as-evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Large Language Models used as evaluators (LLM-as-a-judge)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The notion of using LLMs to automatically evaluate generated text (e.g., factuality, fluency, faithfulness), reported in related work as sometimes aligning with human judgments but with notable exceptions and domain-dependent failures.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Automatic evaluation of NLG outputs / factuality assessment (general; applied discussion in biomedical text generation in this paper)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Various (examples in literature: ChatGPT, GPT-4, GPT-3 and other GPT-family models used as automatic evaluators)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Not directly executed in this paper for LLM-as-evaluator; cited studies vary. In this paper human evaluation (used as ground truth) consisted of two domain experts (PhDs) independently labeling outputs across three binary steps (STEP1 non-expert filter; STEP2 expert factuality; STEP3 specificity), resolving disagreements by discussion after a calibration pilot.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Reported metrics in literature include agreement/correlation with human judgments, alignment rates, accuracy/consistency; in this paper the human-evaluation criteria used as gold standard were fluency, prompt-alignment, semantic coherence, factuality, and specificity (binary scores).</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Literature cited reports mixed outcomes: many studies find LLM evaluators often align well with human assessments on some NLG tasks, while other studies report notable deficiencies; the paper highlights that some works claim good alignment but others find failures, and that automated/token-based metrics (BLEU/ROUGE) correlate poorly with factual consistency. The authors emphasize that LLM-based evaluation cannot yet replace human judges in biomedical factuality tasks and that human verification remains necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Reported limitations include domain sensitivity (worse in biomedical/clinical use), co-occurrence and prevalence biases (over-represent common entities), hallucinations that are fluent and hard to detect, sensitivity to prompt design, lack of situational awareness and reasoning control, overconfidence or failure to express epistemic uncertainty, repetition/looping, and poorer performance on rare/long-tail knowledge.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Cited and observed failure modes: LLM evaluators or generators produce plausible-sounding but false statements (high-quality hallucinations) that are difficult for non-experts to spot; confusion of rare entities with prevalent related processes (e.g., Conidiogenone vs conidiation), generation of incorrect fungus names derived from compound tokens, Bloom and some Llama 2 variants producing incoherent or repetitive outputs, and cases where LLMs omit requested relation despite fluent text (e.g., GPT-4 or ChatGPT focusing on biosynthesis rather than naming producing fungi).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Proposed or cited mitigations include keeping human evaluation in the loop (not replacing judges), combining non-expert filters with expert checks (the paper's three-step framework), prompt engineering and prompt optimization, domain specialization/fine-tuning, scaling/model selection and human-feedback alignment (RLHF), retrieval-augmented generation (RAG) or high-recall retrieval augmentation, explicit epistemic calibration (models that admit uncertainty or ask for more context), and integration with symbolic reasoning or verification pipelines.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6045.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6045.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3-step human evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Three-step framework to streamline human expert evaluation of LLM factuality (non-expert filter + expert factuality + specificity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A practical evaluation pipeline introduced in this paper that uses non-experts to triage outputs by fluency, prompt-alignment and semantic coherence (STEP1), then domain experts to verify factuality (STEP2) and specificity (STEP3), reducing expert workload while maintaining rigorous human oversight.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Biomedical factual knowledge extraction and relation verification (chemical compound definitions and chemical-fungus relation determination)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Not an LLM judge — the framework is human-centric; experiments applied it to outputs from 11 LLMs including GPT-3, BioGPT(-large), ChatGPT, GPT-4, Llama 2 variants, BLOOM, GPT-2, GPT-neo.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Two experienced domain experts (PhD microbiology/chemistry and PhD biochemical pharmacology) independently scored outputs at each step using binary criteria; discrepancies resolved by discussion; experts piloted criteria before annotation. Non-experts performed STEP1 triage using fluency/prompt-alignment/semantic coherence.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Framework uses qualitative binary metrics per step: STEP1 (fluency, prompt-alignment, semantic coherence), STEP2 (factuality: STEP2A entity validity and STEP2B full factual description), STEP3 (specificity vs generic). The paper reports reductions in expert review workload (33% reduction for Task1; 46% for Task2) and per-model factuality/specificity rates.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Using this human-led framework contrasts with proposals to use LLMs as automatic judges: the paper reports that (a) many LLM-generated high-fluency outputs still contain factual errors/hallucinations that are difficult to detect without domain experts, and (b) the framework materially reduces expert time while preserving accuracy, implying current LLM-as-judge approaches are insufficient in this biomedical context.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>When used to generate candidate outputs (not as judges), LLMs displayed hallucinations (including realistic but false scientific-sounding text), prevalence bias (overproducing common entities like Aspergillus), sensitivity to prompt wording, and large variance across models and prompts; the framework acknowledges these limitations and treats human experts as necessary verifiers.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Concrete examples motivating the framework include: BioGPT-large producing high-quality hallucinations that imitate correct definitions (hard to spot); GPT-3 and others producing incorrect fungus names that mirror compound tokens; Llama 2 generating implausible comparative phrases; GPT-4/ChatGPT producing fluent text that omits the prompted relation or states incorrect auxiliary facts (e.g., year or discoverer), all demonstrating need for expert verification.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>The framework itself is a mitigation: (1) non-expert triage to discard obviously bad outputs, (2) focused expert factuality checks only on filtered outputs, (3) binary, narrow evaluation criteria to speed expert work, (4) use of prompt engineering to improve generation, (5) recommend future use of retrieval-augmentation (RAG/high-recall retrieval) and symbolic reasoning, and (6) favoring domain-specialized and human-feedback-aligned models (and selecting optimal prompts per chemical) to reduce hallucinations.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation', 'publication_date_yy_mm': '2023-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. <em>(Rating: 2)</em></li>
                <li>G-eval: NLG evaluation using gpt-4 with better human alignment <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Is ChatGPT a good NLG evaluator? a preliminary study <em>(Rating: 2)</em></li>
                <li>Benchmarking cognitive biases in large language models as evaluators <em>(Rating: 2)</em></li>
                <li>Are large language model-based evaluators the solution to scaling up multilingual evaluation? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6045",
    "paper_id": "paper-272635554",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "LLM-as-evaluator",
            "name_full": "Large Language Models used as evaluators (LLM-as-a-judge)",
            "brief_description": "The notion of using LLMs to automatically evaluate generated text (e.g., factuality, fluency, faithfulness), reported in related work as sometimes aligning with human judgments but with notable exceptions and domain-dependent failures.",
            "citation_title": "",
            "mention_or_use": "mention",
            "task_domain": "Automatic evaluation of NLG outputs / factuality assessment (general; applied discussion in biomedical text generation in this paper)",
            "llm_judge_model": "Various (examples in literature: ChatGPT, GPT-4, GPT-3 and other GPT-family models used as automatic evaluators)",
            "human_evaluation_setup": "Not directly executed in this paper for LLM-as-evaluator; cited studies vary. In this paper human evaluation (used as ground truth) consisted of two domain experts (PhDs) independently labeling outputs across three binary steps (STEP1 non-expert filter; STEP2 expert factuality; STEP3 specificity), resolving disagreements by discussion after a calibration pilot.",
            "metrics_compared": "Reported metrics in literature include agreement/correlation with human judgments, alignment rates, accuracy/consistency; in this paper the human-evaluation criteria used as gold standard were fluency, prompt-alignment, semantic coherence, factuality, and specificity (binary scores).",
            "reported_differences": "Literature cited reports mixed outcomes: many studies find LLM evaluators often align well with human assessments on some NLG tasks, while other studies report notable deficiencies; the paper highlights that some works claim good alignment but others find failures, and that automated/token-based metrics (BLEU/ROUGE) correlate poorly with factual consistency. The authors emphasize that LLM-based evaluation cannot yet replace human judges in biomedical factuality tasks and that human verification remains necessary.",
            "llm_specific_limitations": "Reported limitations include domain sensitivity (worse in biomedical/clinical use), co-occurrence and prevalence biases (over-represent common entities), hallucinations that are fluent and hard to detect, sensitivity to prompt design, lack of situational awareness and reasoning control, overconfidence or failure to express epistemic uncertainty, repetition/looping, and poorer performance on rare/long-tail knowledge.",
            "notable_failure_cases": "Cited and observed failure modes: LLM evaluators or generators produce plausible-sounding but false statements (high-quality hallucinations) that are difficult for non-experts to spot; confusion of rare entities with prevalent related processes (e.g., Conidiogenone vs conidiation), generation of incorrect fungus names derived from compound tokens, Bloom and some Llama 2 variants producing incoherent or repetitive outputs, and cases where LLMs omit requested relation despite fluent text (e.g., GPT-4 or ChatGPT focusing on biosynthesis rather than naming producing fungi).",
            "mitigation_strategies": "Proposed or cited mitigations include keeping human evaluation in the loop (not replacing judges), combining non-expert filters with expert checks (the paper's three-step framework), prompt engineering and prompt optimization, domain specialization/fine-tuning, scaling/model selection and human-feedback alignment (RLHF), retrieval-augmented generation (RAG) or high-recall retrieval augmentation, explicit epistemic calibration (models that admit uncertainty or ask for more context), and integration with symbolic reasoning or verification pipelines.",
            "uuid": "e6045.0",
            "source_info": {
                "paper_title": "Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation",
                "publication_date_yy_mm": "2023-05"
            }
        },
        {
            "name_short": "3-step human evaluation framework",
            "name_full": "Three-step framework to streamline human expert evaluation of LLM factuality (non-expert filter + expert factuality + specificity)",
            "brief_description": "A practical evaluation pipeline introduced in this paper that uses non-experts to triage outputs by fluency, prompt-alignment and semantic coherence (STEP1), then domain experts to verify factuality (STEP2) and specificity (STEP3), reducing expert workload while maintaining rigorous human oversight.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Biomedical factual knowledge extraction and relation verification (chemical compound definitions and chemical-fungus relation determination)",
            "llm_judge_model": "Not an LLM judge — the framework is human-centric; experiments applied it to outputs from 11 LLMs including GPT-3, BioGPT(-large), ChatGPT, GPT-4, Llama 2 variants, BLOOM, GPT-2, GPT-neo.",
            "human_evaluation_setup": "Two experienced domain experts (PhD microbiology/chemistry and PhD biochemical pharmacology) independently scored outputs at each step using binary criteria; discrepancies resolved by discussion; experts piloted criteria before annotation. Non-experts performed STEP1 triage using fluency/prompt-alignment/semantic coherence.",
            "metrics_compared": "Framework uses qualitative binary metrics per step: STEP1 (fluency, prompt-alignment, semantic coherence), STEP2 (factuality: STEP2A entity validity and STEP2B full factual description), STEP3 (specificity vs generic). The paper reports reductions in expert review workload (33% reduction for Task1; 46% for Task2) and per-model factuality/specificity rates.",
            "reported_differences": "Using this human-led framework contrasts with proposals to use LLMs as automatic judges: the paper reports that (a) many LLM-generated high-fluency outputs still contain factual errors/hallucinations that are difficult to detect without domain experts, and (b) the framework materially reduces expert time while preserving accuracy, implying current LLM-as-judge approaches are insufficient in this biomedical context.",
            "llm_specific_limitations": "When used to generate candidate outputs (not as judges), LLMs displayed hallucinations (including realistic but false scientific-sounding text), prevalence bias (overproducing common entities like Aspergillus), sensitivity to prompt wording, and large variance across models and prompts; the framework acknowledges these limitations and treats human experts as necessary verifiers.",
            "notable_failure_cases": "Concrete examples motivating the framework include: BioGPT-large producing high-quality hallucinations that imitate correct definitions (hard to spot); GPT-3 and others producing incorrect fungus names that mirror compound tokens; Llama 2 generating implausible comparative phrases; GPT-4/ChatGPT producing fluent text that omits the prompted relation or states incorrect auxiliary facts (e.g., year or discoverer), all demonstrating need for expert verification.",
            "mitigation_strategies": "The framework itself is a mitigation: (1) non-expert triage to discard obviously bad outputs, (2) focused expert factuality checks only on filtered outputs, (3) binary, narrow evaluation criteria to speed expert work, (4) use of prompt engineering to improve generation, (5) recommend future use of retrieval-augmentation (RAG/high-recall retrieval) and symbolic reasoning, and (6) favoring domain-specialized and human-feedback-aligned models (and selecting optimal prompts per chemical) to reduce hallucinations.",
            "uuid": "e6045.1",
            "source_info": {
                "paper_title": "Large Language Models, scientific knowledge and factuality: A framework to streamline human expert evaluation",
                "publication_date_yy_mm": "2023-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks.",
            "rating": 2,
            "sanitized_title": "llms_instead_of_human_judges_a_large_scale_empirical_study_across_20_nlp_evaluation_tasks"
        },
        {
            "paper_title": "G-eval: NLG evaluation using gpt-4 with better human alignment",
            "rating": 2,
            "sanitized_title": "geval_nlg_evaluation_using_gpt4_with_better_human_alignment"
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2,
            "sanitized_title": "judging_llmasajudge_with_mtbench_and_chatbot_arena"
        },
        {
            "paper_title": "Is ChatGPT a good NLG evaluator? a preliminary study",
            "rating": 2,
            "sanitized_title": "is_chatgpt_a_good_nlg_evaluator_a_preliminary_study"
        },
        {
            "paper_title": "Benchmarking cognitive biases in large language models as evaluators",
            "rating": 2,
            "sanitized_title": "benchmarking_cognitive_biases_in_large_language_models_as_evaluators"
        },
        {
            "paper_title": "Are large language model-based evaluators the solution to scaling up multilingual evaluation?",
            "rating": 1,
            "sanitized_title": "are_large_language_modelbased_evaluators_the_solution_to_scaling_up_multilingual_evaluation"
        }
    ],
    "cost": 0.01373125,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>October 24, 2024</p>
<p>Magdalena Wysocka magdalena.wysocka@cruk.manchester.ac.uk 
CRUK-MI
National Biomarker Centre
Univ. of Manchester
United Kingdom</p>
<p>Oskar Wysocki 
CRUK-MI
National Biomarker Centre
Univ. of Manchester
United Kingdom</p>
<p>Idiap Research Institute
Switzerland</p>
<p>Maxime Delmas 
Idiap Research Institute
Switzerland</p>
<p>Vincent Mutel 
Inflamalps SA
MontheySwitzerland</p>
<p>André Freitas 
CRUK-MI
National Biomarker Centre
Univ. of Manchester
United Kingdom</p>
<p>Idiap Research Institute
Switzerland</p>
<p>Department of Computer Science
Univ. of Manchester
United Kingdom
October 24, 2024EC8DA88071E0C087B7181E0E60D3BBFBarXiv:2305.17819v3[cs.CL]
The paper introduces a framework for the evaluation of the encoding of factual scientific knowledge, designed to streamline the manual evaluation process typically conducted by domain experts.Inferring over and extracting information from Large Language Models (LLMs) trained on a large corpus of scientific literature can potentially define a step change in biomedical discovery, reducing the barriers for accessing and integrating existing medical evidence.This work explores the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery.The framework involves of three evaluation steps, each assessing different aspects sequentially: fluency, prompt alignment, semantic coherence, factual knowledge, and specificity of the generated responses.By splitting these tasks between non-experts and experts, the framework reduces the effort required from the latter.The work provides a systematic assessment on the ability of eleven state-of-the-art models LLMs, including ChatGPT, GPT-4 and Llama 2, in two prompting-based tasks: chemical compound definition generation and chemical compound-fungus relation determination.Although recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities.The ability of LLMs to serve as biomedical knowledge bases is questioned, and the need for additional systematic evaluation frameworks is highlighted.While LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases in a zero-shot setting, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback.</p>
<p>Introduction</p>
<p>Despite comprehensive linguistic interpretation properties, LLMs have inherent operational limitations, most notably hallucinations Bender et al. [2021], Ji et al. [2023a], i.e. divergence from factual knowledge and spurious inference, despite its fluency and syntactic correctness Mahowald et al. [2023], Weidinger et al. [2022].This elicits the need to establish the ability of these models to encode and preserve factual knowledge as well as to define methodologies for the critical evaluation of its representation properties Wysocki et al. [2023a], Jullien et al. [2022], Rozanova et al. [2023].</p>
<p>In this work we introduce an evaluation framework, which is designed to streamline the manual evaluation process typically conducted by domain experts.Despite the advancement and implementation of automated evaluation methods Li et al. [2024], the necessity for manual, human-led verification persists Bavaresco et al. [2024].Users engaged in the biomedical field are likely to continue performing assessments of models within the context of new tasks, using examples they are well-acquainted with to ensure the model's accuracy and reliability.The framework, therefore, is not meant to replace human evaluation but to guide users on key aspects to consider when conducting their assessments, acting as a roadmap for thorough and effective manual evaluation.Being transferable to other biomedical domains, it enables the recognition of, i.a., biases in LLM outputs towards prevalent topics, challenges in identifying rare entities, deviations from intended contexts, and significant performance variability influenced by prompt design.</p>
<p>Using the framework, we systematically assess the biological relational knowledge encoded in a broad spectrum of state-of-the-art LLMs, namely: GPT-2, GPT-3, GPT-neo, Bloom, BioGPT, BioGPT-Large, ChatGPT, GPT-4 and variants of Llama 2 with 7B, 13B, and 70B parameters.Recently, artificial intelligence (AI) algorithms have significantly facilitated the acceleration of antibiotic discovery through high-throughput drug screening, with a particular emphasis on the identification of novel antibiotic compounds Jablonka et al. [2024], Guo et al. [2023], Torres and de la Fuente-Nunez [2019], Lluka and Stokes [2023], Ruiz Puentes et al. [2022], David et al. [2021], Melo et al. [2021].Motivated by this progress, we focus on systematically determining the ability of LLMs to capture fundamental entities (fungi, chemicals and antibiotic properties), their relations and supporting facts, which are fundamental for delivering inference in the context of downstream biomedical inference.For instance, we aim to answer the question on whether LLMs can faithfully capture biological domain-specific factual knowledge such as: "Aspergillus fumigatus produces festuclavine"; "Cycloclavine is biosynthesised by Aspergillus japonicus".This overarching question is translated into the following specific research questions (RQs): RQ1: Do large language models encode biomedical domain knowledge at entity level (e.g.fungi, chemical, antibiotic properties) and at a relational level?RQ2: Are there significant differences between different models in their ability of encoding domain knowledge?RQ3: Which model performs better in faithfully encoding biological relations?RQ4: How do we evaluate the limitations of LLMs in factual knowledge extraction?</p>
<p>This study is organised as follows: first, we define our contribution and link it with the existing frameworks and other investigations within the biomedical domain.Then, within the Materials and methods section, we present a framework for the qualitative evaluation of the biomedical knowledge embedded in LLMs.In the Results section, we apply the proposed framework to eleven LLMs and evaluate them for ten chemical compounds with a detailed description of the generated responses.We then discuss our findings in the broader context of reported LLMs' limitations.Finally, we conclude the paper with a summary of the contributions and with future perspectives on the role of LLMs in biomedical inference.</p>
<p>Related Work</p>
<p>LLMs achieve state-of-the-art performance in multiple Natural Language Inference and Understanding tasks, frequently outperforming previous baselines in few-shot settings Brown et al. [2020].The knowledge extracted by prompting these models has been widely discussed in the context of its factuality Ji et al. [2023b], Bavaresco et al. [2024].Since the first demonstration by Petroni et al. Petroni et al. [2019] on Masked Language Models, several studies have examined the mechanisms and biases underlying these outputs, questioning their applicability beyond their potential as knowledge bases.</p>
<p>Howard et al.Howard et al. [2023] examined the implications of generative AI for antimicrobial advisory for addressing infection questions and outlined the limitations of implementing ChatGPT for interventions due to deficits in situational awareness, inference, and consistency.Li et al. Li et al. [2023] reported that ChatGPT performs moderately or poorly in several biomedical tests and is unreliable in actual clinical use.Furthermore, integrating LLMs into clinical practice is associated with numerous obstacles, such as deficits in situational awareness, lack of reasoning control mechanisms, and consistency Wang et al. [2020].While the use of natural language processing in healthcare is not novel, LLMs have elicited intense debate regarding its potential opportunities and challenges in healthcare Nori et al. [2023].This paper builds upon findings from several key studies to address the limitations and potentials of LLMs in encoding scientific domain knowledge.Zhao et al. Zhao et al. [2021], Kassner, Krojer, and Schutze Kassner et al. [2020] and Wysocki et al. Wysocki et al. [2023a] emphasise the inconsistency of language models and their tendency to replicate prevalent responses, a form of co-occurrence bias also noted in LLMs by Kandpal et al. Kandpal et al. [2023] and Kang and Choi Kang and Choi [2023].This tendency undermines the models' ability to generalise, a concern supported by Razeghi et al. Razeghi et al. [2022], who report an accuracy disparity tied to the frequency of terms in training data.Biderman et al. Biderman et al. [2023], Power et al. Power et al. [2022] and Tirumala et al. Tirumala et al. [2022] explore delicate balance between memorisation's beneficial role in preventing the hallucinations, versus outputting entire sequences from their training data verbatim.This is a key safety concern, particularly in preventing the unintended disclosure of sensitive or proprietary information from the training data Carlini et al. [2021].Wang et al. Wang et al. and Delmas, Wysocka, and Freitas Delmas et al. [2023] remark on LLMs' underperformance when compared to supervised settings, positing retrieval-augmentation as a potential remedy for knowledge-dense tasks.Together, these studies highlight the need for strategies to refine LLMs' bias and factuality, enhancing their application in biomedical domain.</p>
<p>The essential requirement for the biomedical utility of LLMs is the factual accuracy of the generated text.Standard N-Gram matching metrics, including BLEU Papineni et al. [2002], ROUGE Lin [2004], and token-level F1, have been found to poorly correlate with factual consistency Maynez et al. [2020a], Honovich et al. [2021].Given the cumbersome nature of human evaluation for factuality, there is a rising inclination towards employing LLMs for automating the evaluation of model output.Several studies suggest that LLMs can effectively serve as evaluators, often aligning well with human assessments Liu et al. [2023], Zheng et al. [2023], Chen et al. [2023], Törnberg [2023], Huang et al. [2024], Naismith et al. [2023], Gilardi et al. [2023], Kocmi and Federmann [2023], Verga et al. [2024], though there are some exceptions Wang et al. [2023], Wu and Aji [2023], Hada et al. [2024], Pavlovic and Poesio [2024].In contrast, other research highlights notable deficiencies in the performance of LLMs as evaluators Koo et al. [2023], Zeng et al. [2024], Baris Schlicht et al. [2024], and some studies fail to benchmark LLMs against human judgments Jiang et al. [2023], Landwehr et al. [2023].For summary LLM-based evaluation methodologies, we refer to Li et al. [2024].Bavaresco et al. conclude that LLMs are not yet equipped to systematically replace human judges in NLP, reinforcing our argument for the need for human evaluation Bavaresco et al. [2024].</p>
<p>Existing frameworks for evaluating the factuality of LLMs highlight various approaches and their effectiveness.Luo et al. proposes a systematic framework using diverse, high-coverage questions generated from Knowledge Graphs Luo et al. [2023].Hendrycks et al. measures how well text models learn and apply knowledge encountered during pretraining, assessing language understanding across 57 subjects of varying difficulty Hendrycks et al. [2020].The FRANK survey evaluates the faithfulness metrics for summarisation and compares the correlations of these metrics with human judgments, introducing a typology of errors for factual consistency Pagnoni et al. [2021].Rashkin et al. suggests that binary labeling is more beneficial for practical applications where filtering out unfaithful predictions is necessary, aligning with recommendations for human evaluation of attribution in text generation Rashkin et al. [2021].Sun et al. provides a comprehensive study on the trustworthiness of LLMs, establishing benchmarks and principles for different dimensions of trustworthiness Sun et al. [2024].</p>
<p>In the bioinformatics domain, Yin et al. provides a thorough analysis of the limitations of LLMs in complex bioinformatics tasks Yin et al. [2024].Piccolo et al. evaluates LLMs in bioinformatics programming, focusing on their ability to handle domain-specific coding tasks Piccolo et al. [2023].Park et al. conducts a comparative evaluation of LLMs for automatically extracting knowledge from scientific literature to understand protein interactions and pathway knowledge Park et al. [2023].Our work introduces a unique evaluation framework that not only rigorously tests LLMs' ability to encode and apply biomedical knowledge in antibiotic discovery but also significantly reduces the effort required from human experts.This focus on minimizing human evaluation sets our approach apart from previous studies, providing new insights and actionable recommendations to improve factual accuracy and domain-specific reliability.</p>
<p>Materials and methods</p>
<p>A framework to streamline human expert evaluation of LLMs on the encoding of factual scientific knowledge</p>
<p>Our evaluation framework, depicted in Fig. 1, is designed to pragmatically assess the biomedical utility of LLMs by closely mirroring the human evaluation process from which it is directly derived.We propose a methodology that substantially reduces the time commitment required from experts, acknowledging the high value and limited availability of their expertise.Our approach formalises a system where non-experts, guided by specific criteria, can filter out outputs that do not require specialised biomedical knowledge, relying instead on their language proficiency.As a result, the volume of outputs that need expert evaluation is significantly reduced.</p>
<p>Existing human evaluation frameworks highlight distinct dimensions of this process: emphasising the impact of cognitive and utility biases on evaluation, particularly with perception-based metrics for truthfulness Elangovan et al. [2024]; examining whether LLMs can effectively assist bioinformatics experts Chen and Deng [2023]; proposing a framework for model responses along multiple dimensions, including factuality, comprehension, reasoning, potential harm, and bias Singhal et al. [2023].</p>
<p>Our framework complements these by aiming to reduce the effort required from human experts when their involvement is necessary, an area which is still unexplored by existing frameworks.To the best of our knowledge, no existing Figure 1: The framework to streamline human expert evaluation of LLMs and the encoding of factual scientific knowledge in the Large Language Models, in the context of extracting biological relations.Entity 1 stands for chemical compound name.The entity 2 stands for fungus name.Fluency, prompt-alignment, and semantic coherence are assessed in STEP 1 by a non-expert (within the target domain).Then the domain expert evaluates the factuality in STEP 2 for Task 1.For Task 2, the factuality of the generated entity 2 (STEP 2A) is verified before the entire description is assessed (STEP 2B).Outputs that do not pass STEP 2 are classified as hallucinations.The specificity is evaluated in STEP 3.</p>
<p>framework optimises the evaluation process by composing non-expert and expert assessment phases.We argue that as LLMs are increasingly used in biomedical research, manual human evaluation will always be an integral part of model assessment.</p>
<p>The framework consists of three steps: 1) a non-expert evaluates whether the LLM produces text that is fluent, aligned with the prompt, and semantically coherent.If these criteria are met, the output is then passed on to an expert for further evaluation; 2) the expert verifies the factual accuracy of the output; 3) the expert assesses whether the output is generic or specific to the prompt.Each criterion is assessed qualitatively, receiving a binary score of either 0 or 1.A detailed description of the metrics is provided below.</p>
<p>Fluency evaluates the quality of the generated text, considering two sub-criteria: syntactic correctness and style Kann et al. [2018], Mutton et al. [2007].Syntactic correctness assesses whether a sentence conforms to grammar.Fluency is assessed first due to its simplicity, allowing non-experts to perform the evaluation effectively.</p>
<p>Prompt-alignment refers to the relation of relevance and pragmatic coherence between the prompt and the generated text, whether the generated text is consistent with the input prompt and is related to the entities and context of the prompt Webson and Pavlick [2021], Raj et al. [2022].In other words, it checks whether the model accurately answers the question without drifting into irrelevant responses -an aspect that non-experts can effectively identify.</p>
<p>Semantic coherence consists of two levels of analysis: (i) intra-sentence assesses whether the sentence is relating surface terms to expected argument types for a given predicate and (ii) inter-sentence: assessing whether adjacent sentences have a coherent discourse relation Ke et al. [2022].</p>
<p>Factuality is distinct from the meaning of a sentence which is conveyed by its semantic coherence.A sentence generated by a model is true if it aligns with a factual statement in the scientific literature, a database, or any other trusted source Maynez et al. [2020a].</p>
<p>Specificity evaluates the alignment between the prompt and the answer regarding their level of abstraction.While some answers can be factually correct, they may not answer to the level of abstraction required by the prompt Maynez et al. [2020a].</p>
<p>While auto-regressive language models are designed to deliver fluency, prompt-alignment and semantic coherence, text generation during decoding can lead to repetitive, incoherent or meaningless outputs Holtzman et al. [2020].As they are complementary, the fluency, prompt-alignment and semantic coherence were assessed in one step of the evaluation process (Fig. 1, STEP 1).Moreover, the probabilistic nature of the next token prediction task also contrasts with the notion of factuality and large language models are prone to hallucination Maynez et al. [2020b], Ji et al. [2023b].Hallucinations are factually incorrect statements which can be difficult to be localised by non-experts due to their expression in a fluent and semantically coherent text Varshney et al. [2023], Curran et al. [2023].In the context of this study, major hallucinations can be associated with the compound description or relationships with fungi, while minor hallucinations may be associated with the date of first isolation for instance.In our pragmatic approach, any output that reaches the factuality check (fulfills STEP 1) but fails to pass is considered a hallucination.This framework encompasses prompt definition, prompt engineering, and evaluation metrics.Then the analysis includes the extraction and curation of a dataset with biological relations, the definition of two text generation tasks for LLMs, and the selection of the target LLMs for evaluation (Fig. 2).These steps allowed us to provide the first comparative analysis of LLMs in the context of tasks related to fungi and antibiotics and is transportable to other relations in bioscience, such as "A is produced by B," "A inhibits B","A activates B," etc.The code and dataset used in the study are available on GitHub2 .</p>
<p>Dataset</p>
<p>Investigating the biological knowledge stored in LLMs requires a collection of biological relations that are elicited and referred to in the scientific literature.Research has shown that language models (LM) exhibit bias due to imbalanced distributions in their training corpora, with performance correlated to the frequency of occurrences in the data Henning et al. [2022], Wysocki et al. [2023b], Jung and van der Plas [2024].Scientific publications are part of the training corpus for most LLMs, thus higher prevalence in literature should lead to better performance in factual knowledge extraction.In our study, we created a dataset of biological relations accounting for the amount of supporting evidence (number of associated papers).</p>
<p>Chemical compound-fungus pairs were extracted from the Wikidata Knowledge Graph using the predicate found in taxon (p:P703), along with the number of bibliographic references supporting each relationship (via API).The classification and nomenclature information on the fungi was extracted from the Mycobank database3 .Information about antibiotic activity of chemicals were extracted from the ChEBI ontology4 .</p>
<p>A subset containing only fungal species producing chemicals with antibiotic activity was first created.The pairs with the most bibliographic references were progressively integrated until obtaining 100 distinct subject fungi, which corresponds to 123 pairs.Then, a complementary subset (with an equal number of pairs) involving produced chemicals but without antibiotic activity was added from the same list of fungi.Again, the pairs with the most bibliographic references were selected.Only molecules with a 3-STAR review in ChEBI (Chemical Entities of Biological Interest) database, indicating the highest level of manual curation and verification by ChEBI curators, were considered and only those that have an annotated role in the antimicrobial agent class (CHEBI:33281) were selected for positive examples of antibiotics.As a result, a dataset with 246 chemical compound-fungus pairs (see Fig. 2, N ) equally divided between chemicals with and without antibiotic activity was obtained.For the experiments, we randomly selected ten chemicals distributed by antibiotic activity and number of references (five chemicals with and five without antibiotic activity) (Table A.1).These ten chemicals paired with one or more fungi.In total, there were 23 chemical compound-fungus pairs (see Fig. 2, n relations ) in the analysed subset.The selected ten chemical compounds were defined as entity 1 for two tasks.The fungi were defined as entity 2 in the second task.Of note, the selected 23 chemical-fungus relations are described by multiple scientific publications in PubMed, all published before 2019 (for exact numbers see Table A.1).This ensures these relations are contained in the LLM training corpus, provided that the corpus contains PubMed articles or abstracts.</p>
<p>Large Language Models</p>
<p>In this study, we evaluated eleven LLMs (summarised in Table A.2), where six of them were released since July 2022.The models differ in terms of training corpus, vocabulary size, number of parameters, layers and maximum input sequence length.</p>
<p>The final baseline set consists of: GPT-2, GPT-neo (1.3B parameters), BLOOM (3B parameters), BioGPT and BioGPTlarge via huggingface API5 ; Llama 2 using the llama.cpp6(accessed in October 2023); all above with the default sampling procedure for text generation, i.e. greedy decoding and temperature equal 0; GPT-3 via the OpenAI API; and for ChatGPT and GPT-4 a default OpenAI user interface was used to generate outputs, each prompt in a separate session (accessed in March 2023).The maximum numbers of tokens to generate, excluding the number of tokens in the prompt is 30, except for ChatGPT, GPT-4 and Llama 2.</p>
<p>Text generation tasks</p>
<p>We investigate the ability of LLMs to act as knowledge bases and establish the relation between fungi and antibiotics.Following the framework (3.1), we define two text generation tasks (Fig. 2).</p>
<p>Task 1: Chemical compound definition generation Task 1 uses prompts for chemical compound definition generation, shown in the Table 1 as P1-P4.For the same task we define simple prompts (P1, P2) and context-based prompts, elucidating the type of the subject entity: 'a compound' or 'a substance' (P3 and P4) Petroni et al. [2020].The entity 1 represents the selected chemical compounds.We investigated ten examples of chemical compounds for the qualitative evaluation according to the previously described criteria (see section 3.1, Table A.1). Performance in Task 1 in each STEP (Fig. 1) is shown in Table 2. Performance in Task 1 for optimal prompt in each STEP is shown in Table 3. Examples of generated answers with the qualitative evaluation are presented in Table A.3.</p>
<p>Task 2: Chemical-fungus relation determination via entity generation Task 2 uses prompts for relation determination via entity generation: {entity 1}-{relation} -{entity 2}, where entity 1 is a chemical compound name and entity 2 is a fungus name (P5-P15, Table 1).The prompts P10-P15 contain the context at the end, guiding the model towards a relation to a fungus, rather than to another organism capable of producing the compound.We investigated ten compounds, the same as in Task 1 described in section 3.2.Evaluation was performed according to the framework (Fig. 1) for the entire task, both with and without context.Performance in Task 2 is each STEP is shown in Table 4. Performance in Task 2 for optimal prompt in each STEP is shown in Table 5.</p>
<p>Expert evaluation</p>
<p>The outputs were evaluated and labeled according to the proposed framework by two experienced reviewers independently of each other, on each of steps 1-3.They are experts in the field, one with a PhD in microbiology and chemistry and the other with a PhD in biochemical pharmacology, both having prior experience in the manual assessment and curation of datasets.Any discrepancies were resolved by discussion.Both reviewers participated in the study design and the definition of criteria.The reviewers went through a pilot process with the criteria prior to starting the study to ensure they had a similar understanding of the criteria.</p>
<p>Results</p>
<p>of outputs requiring expert evaluation, decreasing by 33% in Task 1 and 46% in Task 2 (see Fig. 3), by discarding incorrect outputs in STEP 1.For Task 1, only 67% of the outputs proceeded to the factuality check, and ultimately, only 21% required a detailed assessment of specificity.In Task 2, 54% of the outputs needed factuality verification for entity 2, and subsequently, only 21% required comprehensive factuality evaluation of the entire output.</p>
<p>The responses that meet the criteria for each model and for each STEP are reported in Table 2 and 4 A .3 and A.4, and optimal prompt with the best performance in Table 3 and 5, and Supp  GPT-3: Semantically coherent answers less sensitive to prompt design.40% of factual answers for the best prompt (Table 3).Hallucinates fungi names from compounds.GPT-3 generated semantically coherent text regardless of the prompt design for all chemical compounds, both with and without context (see example of Atromentin in Table A.3).It generated a similar number of factual answers regardless of the context (30% vs 35%).GTP-3 generated incorrect fungi names (in 39.3% of incorrect answers) that consists of the compound name in the prompt, e.g.Myriocin -Streptomyces myriocin, Chloromonicilin -Streptomyces chloromyceticus (Table A.4).</p>
<p>Bloom: no factual knowledge, low semantic coherence: biased towards treatments, repeats the same associated terms or statements.For prompts without context, Bloom fails to generate a syntactically correct output.The model tends to repeat the same words or phrases for a given entity, or generates names that does not exist in the literature.</p>
<p>When adding the context to the prompts, Bloom is biased towards considering the compounds as a treatment for a variety of diseases.Frequently the answer contains repeated statements and facts, e.g.'Fumitremorgin C is a substance that is used in the treatment of cancer.It is a natural product that is used in the treatment of cancer.'.</p>
<p>BioGPT: low specificity, adding context leads to even lower specificity, leading to a definitional generation for the compound.BioGPT generates fluent and syntactically correct text and tends to correspond to a more general discourse type (when contrasted to scientific discourse).When more context is provided, BioGPT provides a non-specific, universal definition of a compound, which is referred to regardless of the compound name (Table A.3).After adding the prompt context 'is a substance', BioGPT recognises that 5/10 compounds were produced by a fungi, but all incorrect.</p>
<p>BioGPT-large: Semantically very coherent.40% of factual answers for the best prompt (Table 3).Hallucinations under high quality, scientific generated text are difficult to spot.Similarly to GPT-3, for 28% of answers with no factuality, BioGPT-large assigned a wrong producer to the compound, e.g.'Chloromonilicin is a new antibiotic produced by a strain of Streptomyces.It is active against Gram-positive bacteria and fungi.' (true answer: is produced by Monilinia fructicola).This is of particular concern, as the generated text imitates correct definitions very well, disguising the lack of factual knowledge.Such hallucination is very difficult to spot and a vigilant expert check is required.</p>
<p>ChatGPT: Fluent, prompt-alignment and semantically coherent descriptions for all prompts.Hallucinations are difficult to spot.70% of factual answers for the best prompt (Table 3).Interestingly, the answers often closely resembled Wikipedia-style descriptions (7/10 compounds -Wikipedia, 2/10 -Wikidata).For prompts with no context, the statements were longer and divided into paragraphs (mostly three; note, that a limit was not set on the length of generated answer for ChatGPT).Comparing to BioGPT-large, ChatGPT produces even more fluent, prompt-aligned and semantically coherent answers.Due to its semantic fluency, evaluation of the factual knowledge requires both expert knowledge and the time-consuming fact verification.In other words, generated answers can be easily regarded as true and factual for a non-expert, creating a real risk of deriving false facts from the model.</p>
<p>GPT-4: Top performer from the list and epistemically-aware (admits to lack of knowledge).80% of factual answers for the best prompt (Table 3).The model did not guess the definition if it did not recognise the name.For two chemical compounds, the model generated a request for more context or referred to the knowledge cutoff date of 2021-09.In general, adding context did not improve neither the factuality nor specificity metrics, which remained at 70% (for two prompts in total, P1-P2 vs P3-P4).The quoted names of the fungus that produces the chemical were incorrect.In general, the model generated fluent, semantically coherent, highly specific text (Table A.4).We observed less hallucinations compared to ChatGPT.</p>
<p>Llama 2: Exhibits a proclivity for generating inaccurate comparisons, particularly evident in chemical contexts.</p>
<p>Llama 2-70B model produced 20% (4/20) responses that were both specific and factual (20% of factual answers for each prompt, P1 and P2).Smaller versions of Llama2 demonstrated inferior performance.Adding context failed to enhance either factuality or specificity metrics; instead, such augmentation resulted in a noteworthy decline, approaching zero, across all tested Llama 2 models.The Llama2 models displayed a noticeable tendency to generate comparative phrases, particularly evident when tasked with chemical comparisons, frequently resorting to statements like '100 times more toxic than'.In summary, the Llama 2 models failed to generate text characterized by fluency, semantic coherence, and high specificity.).Alternariol turned out to be a chemical with the most factual relations (8 out of 10 tested prompts), which aligns with the results from section 3.1 for GPT-3 (Table A.5).</p>
<p>BioGPT biased towards Aspergillus, limited proportion of factual relations.BioGPT generated factual relations with fungi names for only 13.6% (15/110) of the cases, of which only ten answers had factual knowledge.Out of 62 generated answers that contained a fungus, 45 were Aspergillus, of which only 11 answers were factual.</p>
<p>BioGPT-large: at least one factual chemical-fungus relation for 9/10 compounds.This implies that it is possible to achieve 90% of factual relations if picking the right prompt for each chemical, showing the dependency on prompt optimisation.The model gave the most factual answers for Ergosterol (8 out of 10 tested prompts).Contrary to previous models, we observed that BioGPT-large did not generate additional context, when unclear (e.g. the names of the discoverers or the year of discovery of the compound if not known) reducing the overall number of hallucinations.</p>
<p>ChatGPT: coherent answers, regardless of the prompt, but only 31.8% (35/110) of them were fully factual for the whole generated description (STEP 2B).The lack of factuality was mainly due to the incorrect statement of the year of discovery/isolation of a given compound, the name of the discoverer or the class of the compound.The model provided all 11 fungal-name responses for eight compounds.The exception was one prompt for Conidiogenone and seven prompts for Ergosterol.The rest of the answers given for Ergosterol (4) were 100% factual with fungus relation.For three compounds all answers were factual (100% for Fumitremorgin C, Alternariol, Verrucarin A (Table A.6)) and for four compounds all prompts led to incorrect relations.</p>
<p>GPT-4 gives either the factual chemical-fungus relations or no answer at all.For four compounds, all answers were factual (11/11, 100% for Fumitremorgin C, Alternariol, Verrucarin A, Myriocin).The model for all prompts for Ergosterol did not generate any relations with the fungus and factually recognized the compound by citing its characteristics.In the case of 13.6% (15/110) responses, the model asked for more context and/or indicated that the question is incomplete.The hallucination (lack of factual knowledge) was observed for 56% of outputs, mainly as an incorrect statement of the year of discovery/isolation of a given compound or the incorrect citation of the name of the synonym of the fungus.</p>
<p>Llama 2: the factuality increases with the model size.As the model size increases, the Llama 2 model exhibits an improved capacity to generate fungal names and factual chemical compound-fungus relations, constituting 28.2% (31/110) of correct relations for Llama 2-70B.Notably, the Llama 2 70B accurately generated complete definitions, inclusive of the fungus name, with a success rate of 12.7% (14/110).Irrespective of the model size, Llama 2 exhibited a proclivity for response repetition and looping.Concerning the chemical compound Verrucarin A, all Llama 2 models erroneously postulated the name of a fungus (Verrucaria) unrelated to the actual producer of this compound (Albifimbria verrucaria or Myrothecium verrucaria), with instances recorded at 36.4% (4/11) for Llama 2-7B, 45.5% (5/11) for Llama 2-13B, and 36.4% (4/11) for Llama 2-70B.</p>
<p>Discussion</p>
<p>Our observations, initially rooted in the analysis of fungus-chemical interactions, hold relevance for a range of specific domains within the biomedical field.In this section, we detail these broadly applicable insights, directing the evaluator's attention to essential concerns.Key observations include biases in LLM outputs towards more prevalent topics, difficulties in accurately identifying rare entities, a tendency for outputs to stray from the intended biomedical context, and significant variability in performance influenced by the design of the prompts.</p>
<p>LLMs' biases towards compounds and fungi</p>
<p>5.1.1Correctly generated relations are homogeneous and prevalent.</p>
<p>Considering the models with outputs that contain the fungus name, in majority they recognise one specific factual chemical-fungus relation, although the chemical can be produced by several fungi (RQ1).For instance, in all generated relations which were factual, for Fumitremorgin C the fungus was either Aspergillus fumigatus or Aspergillus (39 out of Table 5: Performance for optimal prompts in Task 2: chemical-fungus relation determination via entity generation.For each model an optimal prompt (highest score in STEP 2B) was selected.Evaluation according to the framework (Fig. 1).X/10 is the number of correct responses vs total number of chemicals.39 correct fungi name, from 66 generated fungi in total).Factually generated relations for Ergosterol are with two fungi: Aspergillus (11/23) and Ganoderma lucidum (7/23).Interestingly, Aspergillus is output by BioGPT and Ganoderma lucidum by BioGPT-Large.Although trained on the same corpus, we hypothesise that the higher number of parameters in BioGPT-Large (347M vs 1.5B) led to a more specific relation (RQ2).</p>
<p>Bias towards</p>
<p>Aspergillus -the most cited fungus in PubMed.</p>
<p>We observe that the models are biased towards certain compounds (RQ1).For instance, BioGPT generates the fungus Aspergillus as the answer in most of the prompts (79%, 45/57), which is rarely correct (only 11 times; 19%) (Table A.7).We attribute this bias to the imbalance in the training corpus, as the Aspergillus is a ubiquitous fungus in home and hospital environments.The PubMed search outputs almost 60k references related to Aspergillus.The most cited compound from our analysis Ergosterol outputs ∼6000, second Aphidicolin ∼2500.Thus, we argue Aspergillus is over-represented in the training corpus and occurs as a statistically most probable fungus to be related to chemicals.</p>
<p>5.1.3Rare chemical compound confused with an overrepresented biological process.</p>
<p>Another example, Conidiogenone was consistently not recognised by the best performing models (GPT-4, ChatGPT, BioGPT-large).We argue that the models confuse Conidiogenone with conidiogenesis or conidiation (RQ1).Conidiogenesis is a widespread morphogenetic process that filamentous fungi undertake for dispersion.Conidiation is also important for pathogenicity of phytopathogens.Understanding the cellular mechanism of conidiation is a highly relevant research topic, thus it has been extensively studied.Conidiogenone is associated with nine results in a PubMed search, conidiogenesis with 346 and conidiation with 6047.The strongest and widespread stimulus for conidiation among filamentous fungi is the exposure of hyphae to the air.Often the generated texts focused on this aspect of the process, suggesting that the model encoded the signal for the representation of the process instead of the chemical compound.</p>
<p>5.1.4</p>
<p>Output not focused on fungi despite the prompt context.</p>
<p>In the case of Ergosterol, which is a compound widely known for its significance in the scientific community, the LLM specifically trained for the bio-domain (BioGPT-Large) outperforms the rest (RQ3).It generated seven factual Ergosterol-fungus relations from 11 prompts (Table A.8).For the other models, the relation to fungus was either minimally addressed or not considered at all, particularly for the models that have generally demonstrated superior efficacy (i.e., GPT-4 with 0/11 and ChatGPT with 4/11 factual responses).We observe that GPT-4 and ChatGPT focus on Ergosterol's biosynthesis in fungi and relevance as an antifungal target, rather than on the producing fungi, despite the prompt that explicitly defines such context.Importantly, these two models do not generate text as a continuation of the prompt, but they generate new paragraphs.</p>
<p>Limited ability of LLMs as Knowledge Bases</p>
<p>In this study, we provide evidence of the limitations in the determination of factual knowledge from LLMs in the text generation tasks.Best models were able to provide only 6/10 of the factual description of chemical entities, and only 5/10 of the factual description of chemical-fungus relations.Despite small sample size in our evaluation, it clearly shows that the performance is insufficient for a systematic and reliable application (RQ1, RQ4).</p>
<p>High dependency and variability due to prompt design choices</p>
<p>A desirable property of a LLM is to a consistent answer regardless of the prompts design.For instance, the prompt '{entity 1} is produced by fungi, such as' should be equivalent to '{entity 1} is isolated from fungi, such as' because it refers to the same biological relation.However, we observe a variation in the output as the evidence of sensitivity for prompt design (RQ1).Similarly to Masked Language Models Zhou et al. [2023], in order to fully exploit GPTs capabilities, prompt engineering is required.In our study, we observe that providing context in the prompt improves the performance.Of note, ChatGPT and GPT-4 are the least sensitive and produce new paragraphs instead of finishing the prompt (RQ3).</p>
<p>Llama 2, despite large size, responses lack factuality and specificity</p>
<p>The analysis of Llama 2's capabilities shows that factuality improves with increased model size.An example of this correlation is Ergosterol, not recognised in Task1 by 7B and 13B versions, but correctly defined by Llama 2 70B.Yet, even the biggest Llama2 variant, did not match the factual strength of GPT-4, ChatGPT, or BioGPT-large, delivering longer, less fluent, and semantically weaker responses.In generating chemical definitions, GPT models surpassed Llama 2 with more nuanced outputs.</p>
<p>Best overall performance was on large corpora, large parameters and most extensive human feedback alignment</p>
<p>GPT-4 outperforms ChatGPT (as well the rest tested models) in terms of factuality of the answer (RQ3).It scores 70% (7/10) and 43.6% (48/110) for chemical entity recognition and chemical-fungi relation recognition, respectively.In general, the factual knowledge of the entire generated text was higher (still significantly higher in GPT-4), but the output did not contain the relation that was prompted for, so they were considered not specific.The hallucinations in GPT-4 are the lowest (54.4%).Importantly, GPT-4 produced only names of fungi which actually produce chemical compounds.</p>
<p>Limitations</p>
<p>We recognise the following limitations of this investigation.First, LLMs are evaluated on a specific biological dataset related to chemical compounds, fungi and antibiotic activity.The models can perform differently for other biological relations.The sample size of 10 compounds is relatively small and increasing the size would lead to more representative results.</p>
<p>Although we evaluated 15 prompt designs, the prompt engineering could be improved with e.g. a systematic search or optimal prompt algorithms.No fine-tuning, nor few-shot learning was performed.The analysis used of-the-shelf LLMs, focusing on their factuality in a specific scientific task.Despite the models' varying corpora, parameters, and design intents, the study did not aim to equalise these factors but rather to explore which features contribute to factual knowledge-based inference in LLMs.</p>
<p>A total of &gt; 150 000 fungi species have been described in the literature Wang et al. [2022].The naming of fungal species is subject to change over time Wang et al. [2022], Richards et al. [2017], and moreover, the number of fungal names continue to increase over time.Despite the improvements in information sharing, the changing fungal nomenclature rules Aime et al. [2021], Turland et al. [2018] and providing a tool for name standardisation, the existing literature still contains plenty of synonyms, homonyms, orthographic variants and misused names that are not in accordance with the standard nomenclature system Lücking et al. [2020].In addition, some taxon names in the databases may be variants, synonyms or invalidly published names for various reasons.This may lead to inaccurate search results and renders the assessment of factual knowledge in generated relations challenging even for a domain expert.</p>
<p>Conclusions</p>
<p>This work explores the potential of LLMs to encode domain-specific factual knowledge, with an emphasis on the biomedical field.To assess this, we introduced a framework to streamline the labor-intensive process of human expert evaluation, using non-experts for preliminary assessments to optimise the time and expertise of specialists, without sacrificing accuracy and efficiency.In our study, this approach resulted in a 33% and 46% reduction in experts' effort for two different tasks.The framework replicates the process of human evaluation, which remains an indispensable step regardless of the presence of automated evaluation systems Bavaresco et al. [2024].This is due to the fact that every user in the biomedical field will invariably wish to conduct manual checks of the model using a variety of familiar examples.Essentially, this framework serves as a directive on the aspects to focus on.</p>
<p>The results reveal that, although recent advancements have enhanced fluency and semantic coherence in LLMs, a reliable performance of the models to operate as a domain-specific knowledge base remains elusive.The factuality of the generated content, especially in biological contexts, is still lacking, with models showing a pronounced bias towards entities that are more prevalent in the training corpus.Our findings align with previous studies across various domains Razeghi et al. [2022], Wang et al., Zhao et al. [2021], Kassner et al. [2020], Kandpal et al. [2023], confirming co-occurrence biases irrespective of model size.Notably, GPT-4, while outperforming others by demonstrating mechanisms of epistemic awareness, acknowledging its knowledge limitations, thereby preventing certain hallucinations.It recognised 70% of chemical compounds and produced less than half of the factual and specific relations to fungi.This compares to the Llama 2 model, which recognized 20% of compounds and 12.7% of relations to fungi.Our study addresses the increasing demand for thorough assessments of LLMs in various fields, advancing beyond the standard benchmarks that are commonly employed.This research complements existing works investigating LLM performance within the biomedical domain, specifically probing whether these models simply recall information or truly understand the symbolic connections between chemicals and fungi when generating definitions.</p>
<p>Future work.As a continuation of this study, we plan to further assess LLMs for multi-hop biomedical inference.We plan to extend our proposed framework addressing the need for efficient qualitative and quantitative evaluation of responses generated by LLMs.Such frameworks should go beyond the well-known benchmarks and be transferable to other biomedical domains, such as the one exemplified in this study (chemical-fungus relation).A promising future direction is the use of robust retrieval augmented generation (RAG) mechanisms integration with robust, high-recall retrieval augmentation mechanisms, e.g.Delmas et al. [2023], Wysocki et al. [2024] in coordination with symbolic reasoning methods.</p>
<p>CRediT authorship contribution statement</p>
<p>Magdalena Wysocka: Writing -review and editing, Writing -original draft, Software, Methodology, Formal analysis, Data curation, Visualization, Conceptualization.Oskar Wysocki: Supervision, Writing -review and editing, Formal analysis, Methodology, Conceptualization.Maxime Delmas: Writing -review and editing, Writing -original draft, Software.Vincent Mutel: Project administration, Methodology.André Freitas: Writing -review and editing, Supervision, Investigation.</p>
<p>Declaration of competing interest</p>
<p>The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.Myriocin , a specific inhibitor of serine palmitoyltransferase, was used to inhibit de novo sphingolipid synthesis.The results showed that myriocin treatment significantly reduced the levels of 1</p>
<p>Nonexpert</p>
<p>ChatGPT</p>
<p>Myriocin is a natural product and a type of mycotoxin that is produced by several species of fungi, including Isaria sinclairii and Myriococcum albomyces.It was first isolated from the fungus Myriococcum albomyces in 1994.Myriocin is also known as thermozymocidin, ISP-1 or Penicillium mycotoxin, and it has been shown to have a range of biological activities.Myriocin is a potent inhibitor of serine palmitoyltransferase, which is an enzyme involved in the synthesis of sphingolipids.Sphingolipids are important structural components of cell membranes and play a role in signaling pathways.By inhibiting serine palmitoyltransferase, Myriocin disrupts sphingolipid synthesis, which can lead to apoptosis (programmed cell death) in certain cells.Myriocin has been studied for its potential use as a therapeutic agent for a variety of conditions, including cancer, inflammation, and autoimmune diseases.It has also been investigated for its potential as an immunosuppressive agent in the treatment of organ transplantation and autoimmune diseases.Additionally, Myriocin has been found to exhibit antifungal and antiparasitic activity.</p>
<p>GPT-4</p>
<p>Myriocin, also known as ISP-1 or thermozymocidin, is a naturally occurring compound with potent immunosuppressive and antifungal properties.It is derived from the fungus Mycelia sterilia, specifically the Isaria sinclairii species.Myriocin is a structural analog of sphingosine, a component of sphingolipids, which are essential components of cell membranes in eukaryotes.Myriocin acts by inhibiting serine palmitoyltransferase (SPT), a key enzyme in the de novo biosynthesis of sphingolipids.This inhibition disrupts the synthesis of sphingolipids, leading to alterations in cell membrane structure and function.Consequently, myriocin has the potential to suppress various immune responses and limit the growth of certain fungal pathogens.</p>
<p>1 Llama 2-7B</p>
<p>is a new antibiotic that has been developed to treat bacterial infections.It is effective against many types of bacteria, including those that are resistant to other antibiotics.Myriocin works by interfering with the bacteria‚ Äôs ability to make proteins.This prevents the bacteria from growing and multiplying, which eventually kills them.Myriocin is available in both oral and intravenous forms.(CAS 10238-79-6) World Market Report 2019 aims at providing comprehensive data on myriocin market globally and regionally (Europe, Asia, North America, Latin America etc.).The report includes myriocin description, covers its application areas, manufacturing methods, patterns.It captures myriocin market trends, pays close attention to myriocin manufacturers and names suppliers.Furthermore, myriocin prices in regional markets can be found in the report with regards to countries and companies.In addition to the above the report determines myriocin consumers.</p>
<p>Cordyceps sinensis</p>
<p>Ergosterol is a natural product found in Ergosterol is a natural chemical compound found in</p>
<p>&lt;ContextFigure 2 :
2
Figure 2: The workflow of the performed analysis.N -whole dataset with 246 selected biological relations of chemical compound-fungus pairs; n relations -23 selected chemical compound-fungus pairs; n entity1 -10 unique chemical compounds included in those pairs; In prompt engineering: blue -entity 1, orange -added context.Qualitative evaluation performed according the framework depicted in Fig. 1.</p>
<p>, and Supp.Fig.A.1, A.2, A.3.Examples of responses generated with qualitative evaluation are presented in Table</p>
<p>Figure 3 :
3
Figure 3: The number of outputs evaluated at each STEP of the framework, showing the percentage reduction compared to the initial number of outputs evaluated in STEP 1 by non-experts.Results are aggregated for 11 models and prompts from the given task: A) Task 1, B) Task 2.</p>
<p>Figure A. 1 :Figure A. 2 :Figure A. 3 :
123
Figure A.1: The number of outputs evaluated at each STEP of the framework in Task 1 for simple prompts, showing the percentage of results evaluated by non-expert and expert.Results are aggregated for prompts P1-P2 from the given model.</p>
<p>product isolated from the marine sponge Mycale hentscheli.It has been shown to have antitumor activity in vitro and in vivo, but its mechanism of action remains unknown.In this study, we investigated the effect of myriocin on cell cycle progression and apoptosis in human breast cancer cells.0Llama 2-70B</p>
<ol>
<li>MYRIOCIN MARKET WORLDWIDE.MANUFACTURERS AND SUPPLIERS OF MYRIOCIN (INCLUDING CONTACT DETAILS).0 Table A.8. Generated fungus name in Task 2: chemical-fungus relation determination via entity generation task for one chemical compound Ergosterol for 11 prompts (P5-P15).Fungus names that are generated correctly in bold.</li>
</ol>
<p>Table 1 :
1
Prompts used in Task 1 and Task 2. Task 1: Chemical compound definition generation GPT-2 and GPT-neo: no semantic coherence, no factuality.GPT-2 failed to generate any semantically coherent text related to the entity and the target domain (biomedicine in general).Adding context leads to more fluent outputs but still loses semantic coherence and there is no prompt-alignment.GPT-neo, similarly to GPT-2, failed to generate any text or any fluent text.After adding context, the model asserts two compounds (Fumitremorgin C, Conidiogenone) as being produced by fungi (Fusarium sphaerospermum, Penicillium notatum, respectively), however, under incorrect associations.The model failed to generate factual knowledge.
TaskType of prompt No. PromptP1{entity 1}simple promptsP2{entity 1} isTask 1: Chemical compoundcontext-basedP3{entity 1} is a compounddefinition generationpromptsP4{entity 1} is a substanceP5{entity 1} is isolated fromP6{entity 1} originally isolated fromsimple promptsP7{entity 1} is produced byP8{entity 1} is a natural chemical compound found inTask 2: Chemical-fungusP9{entity 1} is a natural product found inrelation determinationP10 {entity 1} is isolated from the fungusvia entity generationP11 {entity 1} is isolated from fungicontext-based promptsP12 {entity 1} is isolated from fungi, such as P13 {entity 1} is produced by the fungus P14 {entity 1} is produced by fungiP15 {entity 1} is produced by fungi, such as</p>
<p>Table 2 :
2
Performance in Task1: chemical compound definition generation.Each step from the framework (see Section 3.1) evaluated separately.X/20 represents the number of correct responses versus the total number of outputs for a given type of prompt (each prompt has 10 outputs).The best results are highlighted in bold.
Task 1: Chemical compound definition generationsimple prompts P1-P2context-based prompts P3-P4ModelSTEP 1 STEP 2 STEP 3 STEP 1 STEP 2 STEP 3GPT-22/200/200/209/200/200/20GPT-320/206/206/2020/207/207/20GPT-neo8/200/200/2011/200/200/20Bloom4/200/200/206/200/200/20BioGPT14/201/200/2019/207/202/20BioGPT-large 16/206/205/2020/207/206/20ChatGPT20/2012/2012/2020/2011/2010/20GPT-420/2014/2014/2020/2014/2014/20Llama 2-7B16/202/202/204/200/200/20Llama 2-13B 12/202/202/209/200/200/20Llama 2-70B 13/204/204/2012/201/201/20</p>
<p>Table 3 :
3
Performance for optimal prompts in Task 1: chemical compound definition generation.For each model an optimal prompt (highest score in STEP 2) was selected.Evaluation according to the framework (Fig.1).X/10 is the number of correct responses vs total number of chemicals.
ModelOptimal PromptSTEP1 STEP2 STEP 3GPT-3{entity 1} is a substance ...10/104/104/10BioGPT{entity 1} is a compound ... 9/105/101/10BioGPT-large {entity 1} is a compound ... 10/104/104/10ChatGPT{entity 1} is a substance ...10/107/106/10GPT-4{entity 1} ...10/108/108/10Llama 2-7B{entity 1} ...9/102/102/10Llama 2-13B{entity 1} ... {entity 1} is ...7/10 5/101/10 1/101/10 1/10Llama 2-70B{entity 1} ... {entity 1} is ...7/10 6/102/10 2/102/10 2/104.2 Task 2: Chemical-fungus relation determination via entity generationGPT-3 produces at least one factual relation for 6/10 compounds. GPT-3 generated 51% (56/110) answers containinga fungus name, where 23.6% (26/110) were factual compound-fungus relations, but only in 13.6% (15/110) the wholerelation description was factual (STEP 2B</p>
<p>Table 4 :
4
Performance in Task 2: chemical-fungus relation determination via entity generation.Each step from the framework (see Section 3.1) evaluated separately.X/110 represents the number of correct responses versus the total number of responses for a given model, with 110 derived from 11 prompts each applied to 10 chemicals.The best results are highlighted in bold.
Task 2: Chemical-fungus relation determination via entity generationsimple prompts + context-based prompts P5-P15STEP 1STEP 2ASTEP 2B STEP 3occurrence offactual occurrenceModelfungus name inof fungus name inthe generated textthe generated textGPT-20/1100/1100/1100/1100/110GPT-357/11056/11026/11015/11015/110GPT-neo29/11028/1104/1102/1100/110Bloom13/11012/1103/1102/1102/110BioGPT62/11057/11015/11010/1102/110BioGPT-large 84/11074/11044/11018/11014/110ChatGPT103/110 101/11049/11035/11035/110GPT-487/11077/11056/11048/11048/110Llama 2-7B69/11036/1107/1102/1102/110Llama 2-13B 70/11049/11014/1108/1104/110Llama 2-70B 76/11062/11031/11014/11014/110</p>
<p>Table A .
A
1.The dataset for the experiments contains ten chemical compounds (chem name): five with (antibiotic activity = 1) and five without antibiotic activity (antibiotic activity = 0).For each relation of a selected chemical compound with a fungus (fungi name) there is a number of references (nb ref) describing the relation based on PubChem.
fungi idfungi name entity 2family namepubchem idchem name entity 1antibiotic activity 0/1nb refPublication year of the latest reference211776Aspergillus fumigatusAspergillaceae403923Fumitremorgin C0102015119834Alternaria alternataPleosporaceae5359485Alternariol092012148413Ganoderma lucidumPolyporaceae444679Ergosterol062009100745Ganoderma australePolyporaceae444679Ergosterol032004119872Ganoderma applanatumPolyporaceae444679Ergosterol032014504340Ophiocordyceps sinensisOphiocordycipitaceae 444679Ergosterol032018182069Aspergillus nidulansAspergillaceae444679Ergosterol032009298052Gymnopilus spectabilisStrophariaceae444679Ergosterol022005250886Phaeolepiota aureaAgaricaceae444679Ergosterol021991315778Hypsizygus marmoreusAgaricaceae444679Ergosterol022006236989Monilinia fructicolaSclerotiniaceae121225493(+)-Chloromonilicin 022011247956Penicillium aurantiogriseum Aspergillaceae10892066Conidiogenone022003257047Cephalosporium aphidicolaCordycipitaceae457964Aphidicolin1102004303853Pleospora betaePleosporaceae457964Aphidicolin152018337192Pleospora bjoerlingiiNeocamarosporiaceae 457964Aphidicolin152016284309Aspergillus nigerAspergillaceae5748546Flavasperone162004815927Albifimbria verrucariaStachybotryaceae6326658Verrucarin A152012815989Paramyrothecium roridumStachybotryaceae6326658Verrucarin A132019245274Myrothecium verrucariaStachybotryaceae6326658Verrucarin A122012283055Dendrodochium toxicumBionectriaceae6326658Verrucarin A122004360246Tapinella atrotomentosaTapinellaceae99148Atromentin122007340026Thelephora aurantiotinctaThelephoraceae99148Atromentin122005504278Ophiocordyceps heteropoda Ophiocordycipitaceae 6438394Myriocin122009</p>
<p>Table A .
A
2. Summary of the evaluated Large Language Models ordered by the release date.
ReferenceRadford et al. [2019]Brown et al. [2020]Gao et al. [2020]BigScience [2022]Luo et al. [2022]Luo et al. [2022]Touvron et al. [2023]Date of themodel release<em>Feb 2019June 2020March 2021July 2022Oct 2022Oct 2022Nov 2022Feb 2023July 2023Maximum inputsequencelength (tokens)102420482048204810242048409681924096Layers (L),hidden size (H),heads (A)L=24, H=1024, A=16L=96, H=12288, A=96L=24, H=2048, A=16L=24, H=1024, A=16L=24, H=1024, A=16L=24, H=1024, A=16L=48, H=12288, A=96L=32, H=4096, A=32L=40, H=5120, A=40L=80, H=8192, A=64Model size(parameters)used/available124M/1.5B175B1.3B/2.7B560M-3B/176B347M/347M1.5B/1.5B20B (175B)7B13B70BVocabulary size50,25750,257250,68042,38442,384Training CorpusWebText: 40 GB of text, 8M documents,from 45M webpages upvoted on Reddit(all Wikipedia pages removed)570 GB plaintext, 0.4 trillion tokens.(410B CommonCrawl, 19B WebText2,3B English Wikipedia, and two bookscorpora (12B Books1 and 55B Books2))The Pile -a large scale curateddataset created by EleutherAI825GB, 380 billion tokens45 natural languages12 programming languagesIn 1.5TB of pre-processed text,converted into 350B unique tokens15M PubMed abstracts (trained from scratch)15M PubMed abstracts (trained from scratch)unavailableunavailableunavailableLabOpenAIOpenAIEleutherAIBigScienceMicrosoftMicrosoftOpenAIOpenAIMeta AIModelGPT-2GPT-3GPT-neoBloomBioGPTBioGPT-largeChatGPTGPT-4Llama 2
</em>The 'Date of the model release' is based on its first arXiv version (if exists) or official public release.</p>
<p>Table A .
A
3. Examples of generated text for selected models, selected compounds as input, and selected prompts.In the 'Comment' column there are correct answers or comments by the authors referring to semantic coherence or factuality of generated text.
ModelInputPromptGenerated TextCommentSTEP 2 FactualityAphidicolin is a potent inhibitor ofboth host cell DNA polymerase alphaAphidicolin{entity 1} isAphidicolin is a potent inhibitor of DNA polymerase alpha and beta, but not of DNA polymerase gamma.and herpes simplex virus (HSV)-induced DNA polymerase but has no effect on0DNA polymerases beta and gammaof host cells. [ref]Ergosterol{entity 1} isErgosterol is a natural product that has been shown to have a wide range of biological activities.General description which was repeated by the model several times.1BioGPTAphidicolin{entity 1} is a compoundAphidicolin is a compound that inhibits DNA polymerase alpha. replication by inhibiting the enzyme DNA1Aphidicolin{entity 1} is a substanceAphidicolin is a substance that inhibits DNA polymerase. replication by blocking the replicative DNA1Flavasperone{entity 1} is a compoundFlavasperone is a compound with a wide range of pharmacological properties.General description which was repeated by the model several times.1Verrucarin A{entity 1} is a compoundVerrucarin A is a compound with a unique structure and a unique mechanism of action.General description which was repeated by the model several times.0Fumitremorgin C{entity 1} is a compoundFumitremorgin C is a compound with a unique structure and a unique mode of action.General description which was repeated by the model several times.0Alternariol{entity 1} is a compoundAlternariol is a compound with a wide range of biological activities.General description which was repeated by the model several times.1Fumitremorgin C{entity 1} is a substanceFumitremorgin C is a substance that is produced by the fungus Aspergillus terreus.Fumitremorgin C is not produced by the fungus Aspergillus terreus.0Ergosterol{entity 1} is a substanceErgosterol is a substance that is produced by the fungus Ergoderma lucidum.Model generates new name of fungus. in the literature. [ref] There is no Ganoderma lucidum0Conidiogenone{entity 1} is a substanceConidiogenone is a substance isolated from the fungus Aspergillus versicolor.Conidiogenone is not isolated from the fungus Aspergillus versicolor.0(+)-Chloromonilicin{entity 1} is a substance(+)-Chloromonilicin is a substance that inhibits the growth of the fungus Fusarium oxysporum f.(+)-Chloromonilicin doesn't inhibit the oxysporum f. growth of the fungus Fusarium0Verrucarin A{entity 1} is a substanceVerrucarin A is a substance that is produced by the fungus Aspergillus versicolor.Verrucarin A is not produced by the fungus Aspergillus versicolor.0BioGPT-Large (+)-Chloromonilicin {entity 1} is(+)-Chloromonilicin is a new antibiotic produced by a strain of Streptomyces. It is active against of (+) -chloromon Gram-positive bacteria and fungi. The structureChloromonilicin is a natural sonchi and Monilinia fructicola . product found in Alternaria0Aphidicolin is a naturally occurring compound thatwas originally isolated from the fungus Cephalosporiumaphidicola. It is a potent inhibitor of DNA synthesis andis commonly used as a research tool to study DNA replication.Aphidicolin inhibits the activity of DNA polymerases,which are the enzymes responsible for synthesizingnew DNA strands during DNA replication. By blockingthis process, aphidicolin prevents cells from dividing andproliferating. This makes it useful for studying the effectsof DNA replication inhibition on cellular processes suchAphidicolin{entity 1}as DNA repair, cell cycle progression, and gene expression.1ChatGPTAphidicolin has also been investigated as a potentialanti-cancer agent, as many cancer cells rely on rapidDNA replication to support their growth and survival.However, its clinical use has been limited by its toxicityand side effects.Overall, aphidicolin is a valuable research tool for studyingDNA replication and has potential applications in cancertreatment, but further research is needed to better understandits mechanisms of action and potential therapeutic uses.Aphidicolin is a naturally occurring compound that iscommonly used as a research tool to study DNA replication.It is a potent inhibitor of DNA synthesis and works byblocking the activity of DNA polymerases, which arethe enzymes responsible for synthesizing new DNA strandsAphidicolin{entity 1} isduring DNA replication. By inhibiting DNA replication, aphidicolin prevents cells from dividing and proliferating,1making it useful for studying the effects of DNA replicationinhibition on cellular processes such as DNA repair, cell cycleprogression, and gene expression. Aphidicolin has also beeninvestigated as a potential anti-cancer agent, but its clinicaluse has been limited by its toxicity and side effects.Yes, aphidicolin is a compound. It is a naturallyoccurring tetracyclic diterpene, which means it isAphidicolin{entity 1} is a compounda type of organic molecule made up of four fused rings and derived from the isoprene unit. The chemicalThe chemical formula was incorrect0structure of aphidicolin consists of 18 carbon atoms,28 hydrogen atoms, and 2 oxygen atoms.Aphidicolin{entity 1} is a substance
Yes, aphidicolin is a substance.It is a naturally occurring organic compound that is found in certain fungi, including Cephalosporium aphidicola.It can also be synthesized chemically in the laboratory.As a substance, aphidicolin is used as a research tool to study DNA replication and as a potential anti-cancer agent, although its clinical use has been limited due to its toxicity and side effects.</p>
<p>Table A .
A
4. Examples of generated text for selected input: chemical compound Myriocin and a simple prompt: '{entity 1}' for all eleven models analyzed in the paper.Factuality of generated text are provided for all generated responses.Myriocin is an antifungal drug used to treat fungal infections.It is derived from the fungus Streptomyces myriocin, and is used to treat a variety of fungal infections, including Candida 0
ModelGenerated TextSTEP 2 FactualityGPT-2Myriocin ¬ †(Kaihuil) In this series, we see the relationship between three main protagonists, Kaihua, Kihua and I0GPT-3gpt-neoMyriocin0Myriocin ÊäóËÇøÁò §ËçØ ÊäóËÇøÁò §ËçØ ÊäóËÇøÁò §ËçØ ÊäóËÇøÁò §ËçØ ÊäóËÇøÁò §ËçØ ÊäóËÇøÁò §ËçØbloomÊäóËÇøÁò §ËçØ ÊäóËÇøÁò §0BioGPTMyriocin , a novel peptide from the venom of the scorpion Leiurus quinquestriatus hebraeus.0BioGPT-large
https://github.com/digital-ECMT/LLM_factuality_evaluation_framework.git
https://www.mycobank.org
https://www.ebi.ac.uk/chebi/downloadsForward.do
https://huggingface.co
https://github.com/ggerganov/llama.cpp
AcknowledgmentsJ. Dumoulin, J. Rossier and C. Verzat for their support.FundingThis project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 965397.This work was supported by the IDIAP Research Institute and has been done in collaboration with the company Inflamalps SA and is supported by the Ark Foundation.The funding bodies played no role in the design of the study, research, writing and publication of the paper.A Supplementary dataThe following is the Supplementary material related to this article.
On the dangers of stochastic parrots: Can language models be too big?. Emily M Bender, Timnit Gebru, Angelina Mcmillan-Major, Shmargaret Shmitchell, 10.1145/3442188.3445922Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21. the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21New York, NY, USAAssociation for Computing Machinery2021</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Comput. Surv. 0360-03005512mar 2023a</p>
<p>Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, Evelina Fedorenko, Dissociating language and thought in large language models: a cognitive perspective. 2023</p>
<p>Taxonomy of Risks posed by Language Models. Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, Courtney Biles, Sasha Brown, Zac Kenton, Will Hawkins, Tom Stepleton, Abeba Birhane, Lisa Anne Hendricks, Laura Rimell, William Isaac, Julia Haas, Sean Legassick, Geoffrey Irving, Iason Gabriel, 10.1145/3531146.35330882022 ACM Conference on Fairness, Accountability, and Transparency. Seoul Republic of KoreaACMJune 2022</p>
<p>. Oskar Wysocki, Zili Zhou, Paul O 'regan, Deborah Ferreira, Magdalena Wysocka, Dónal Landers, André Freitas, 10.1162/coli_a_00462Transformers and the Representation of Biomedical Background Knowledge. Computational Linguistics. 0891-201749103 2023a</p>
<p>Do transformers encode a foundational ontology? probing abstract classes in natural language. Mael Jullien, Marco Valentino, Andre Freitas, arXiv:2201.102622022arXiv preprint</p>
<p>Interventional probing in high dimensions: An nli case study. Julia Rozanova, Marco Valentino, Lucas Cordeiro, Andre Freitas, arXiv:2304.103462023arXiv preprint</p>
<p>Leveraging large language models for nlg evaluation: A survey. Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao, arXiv:2401.071032024arXiv preprint</p>
<p>Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks. Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, F T André, Philipp Martins, Vera Mondorf, Sandro Neplenbroek, Pezzelle, Aditya K Suglia, Ece Surikuchi, Alberto Takmaz, Testoni, 2024Barbara Plank, David Schlangen, Alessandro</p>
<p>Leveraging large language models for predictive chemistry. Kevin Maik, Jablonka , Philippe Schwaller, Andres Ortega-Guerrero, Berend Smit, Nature Machine Intelligence. 622024</p>
<p>What can large language models do in chemistry? a comprehensive benchmark on eight tasks. Taicheng Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh Chawla, Olaf Wiest, Xiangliang Zhang, Advances in Neural Information Processing Systems. 202336</p>
<p>Toward computer-made artificial antibiotics. Current Opinion in Microbiology. 10.1016/j.mib.2019.03.004Marcelo Der Torossian Torres and Cesar de la Fuente-Nunez201951</p>
<p>Antibiotic discovery in the artificial intelligence era. Telmah Lluka, Jonathan M Stokes, Annals of the New York Academy of Sciences. 151912023</p>
<p>Rational discovery of antimicrobial peptides by means of artificial intelligence. Paola Ruiz Puentes, Maria C Henao, Javier Cifuentes, Carolina Muñoz-Camargo, Luis H Reyes, Juan C Cruz, Pablo Arbeláez, Membranes. 1277082022</p>
<p>Abdulrahman Ismaiel, Dinu Iuliu Dumitrascu, Daniel Corneliu Leucuta, Mihaela Fadygas Stanculete, et al. Artificial intelligence and antibiotic discovery. Liliana David, Monica Anca, Cristina Brata, Cristina Mogosan, Zoltan Pop, Lucian Czako, Muresan, Antibiotics. 101113762021</p>
<p>Accelerating antibiotic discovery through artificial intelligence. Jacqueline Rma Marcelo Cr Melo, Cesar Maasch, De La Fuente-Nunez, Communications biology. 4110502021</p>
<p>Language models are few-shot learners. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Ilya Sutskever, and Dario Amodei. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford2020</p>
<p>Survey of hallucination in natural language generation. Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye , Jin Bang, Andrea Madotto, Pascale Fung, 10.1145/3571730ACM Computing Surveys. 0360-03005512 2023b</p>
<p>Language Models as Knowledge Bases?. Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:1909.01066September 2019</p>
<p>ChatGPT and antimicrobial advice: the end of the consulting infection doctor? The Lancet Infectious Diseases. Alex Howard, William Hope, Alessandro Gerada, 10.1016/S1473-3099(23)00113-5April 202323</p>
<p>Chatgpt in healthcare: A taxonomy and systematic review. Jianning Li, Amin Dada, Jens Kleesiek, Jan Egger, 10.1101/2023.03.30.232878992023</p>
<p>Systematic evaluation of research progress on natural language processing in medicine over the past 20 years: Bibliometric study on pubmed. Jing Wang, Huan Deng, Bangtao Liu, Anbin Hu, Jun Liang, Lingye Fan, Xu Zheng, Tong Wang, Jianbo Lei, 10.2196/16816J Med Internet Res. 1438-8871221e16816Jan 2020</p>
<p>Capabilities of gpt-4 on medical challenge problems. Harsha Nori, Nicholas King, Scott Mayer Mckinney, Dean Carignan, Eric Horvitz, 2023</p>
<p>Calibrate Before Use: Improving Few-Shot Performance of Language Models. Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, Sameer Singh, June 2021</p>
<p>Are Pretrained Language Models Symbolic Reasoners over Knowledge?. Nora Kassner, Benno Krojer, Hinrich Schütze, 10.18653/v1/2020.conll-1.45Proceedings of the 24th Conference on Computational Natural Language Learning. the 24th Conference on Computational Natural Language LearningAssociation for Computational Linguistics2020</p>
<p>Large Language Models Struggle to Learn Long-Tail Knowledge. Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, Colin Raffel, July 2023</p>
<p>Impact of Co-occurrence on Factual Knowledge of Large Language Models. Cheongwoong Kang, Jaesik Choi, October 2023</p>
<p>Impact of Pretraining Term Frequencies on Few-Shot Numerical Reasoning. Yasaman Razeghi, Robert L Logan, Matt Iv, Sameer Gardner, Singh, 10.18653/v1/2022.findings-emnlp.59Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics2022</p>
<p>Emergent and Predictable Memorization in Large Language Models. Stella Biderman, Lintang Usvsn Sai Prashanth, Hailey Sutawika, Quentin Schoelkopf, Shivanshu Anthony, Edward Purohit, Raff, May 2023</p>
<p>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets. Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra, January 2022</p>
<p>Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models. Kushal Tirumala, Aram H Markosyan, Luke Zettlemoyer, Armen Aghajanyan, November 2022</p>
<p>Extracting training data from large language models. Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, 30th USENIX Security Symposium (USENIX Security 21). 2021</p>
<p>Retrieve What You Need: A Mutual Learning Framework for Open-domain Question Answering. Dingmin Wang, Qiuyuan Huang, Matthew Jackson, Jianfeng Gao, </p>
<p>Relation Extraction in underexplored biomedical domains: A diversity-optimised sampling and synthetic data generation approach. Maxime Delmas, Magdalena Wysocka, André Freitas, November 2023</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Pierre Isabelle, Eugene Charniak, Dekang Lin, the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsJuly 2002</p>
<p>ROUGE: A package for automatic evaluation of summaries. Chin-Yew Lin, Text Summarization Branches Out. Barcelona, SpainAssociation for Computational LinguisticsJuly 2004</p>
<p>On faithfulness and factuality in abstractive summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, 10.18653/v1/2020.acl-main.173Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, Joel Tetreault, the 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 2020a</p>
<p>Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. Or Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, Omri Abend, 10.18653/v1/2021.emnlp-main.619Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Marie-Francine Moens, Xuanjing Huang, Lucia Specia, Scott , Wen-Tau Yih, the 2021 Conference on Empirical Methods in Natural Language ProcessingDominican RepublicAssociation for Computational LinguisticsNovember 20212Online and Punta Cana</p>
<p>G-eval: NLG evaluation using gpt-4 with better human alignment. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu, 10.18653/v1/2023.emnlp-main.153Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Houda Bouamor, Juan Pino, Kalika Bali, the 2023 Conference on Empirical Methods in Natural Language ProcessingSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Judging llm-as-a-judge with mt-bench and chatbot arena. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P Xing, Hao Zhang, Joseph E Gonzalez, Ion Stoica, 2023</p>
<p>Exploring the use of large language models for reference-free text quality evaluation: An empirical study. Yi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, Ruifeng Xu, 10.18653/v1/2023.findings-ijcnlp.32Findings of the Association for Computational Linguistics: IJCNLP-AACL 2023 (Findings). Jong C Park, Yuki Arase, Baotian Hu, Wei Lu, Derry Wijaya, Ayu Purwarianti, Adila Alfa Krisnadhi, BaliAssociation for Computational LinguisticsNovember 2023</p>
<p>Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning. Petter Törnberg, 2023</p>
<p>ChatGPT rates natural language explanation quality like humans: But on which scales?. Fan Huang, Haewoon Kwak, Kunwoo Park, Jisun An, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, Nianwen Xue, the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)Torino, ItaliaELRA and ICCLMay 2024</p>
<p>Automated evaluation of written discourse coherence using GPT-4. Ben Naismith, Phoebe Mulcaire, Jill Burstein, 10.18653/v1/2023.bea-1.32Proceedings of the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023). Ekaterina Kochmar, Jill Burstein, Andrea Horbach, Ronja Laarmann-Quante, Nitin Madnani, Anaïs Tack, Victoria Yaneva, Zheng Yuan, Torsten Zesch, the 18th Workshop on Innovative Use of NLP for Building Educational Applications (BEA 2023)Toronto, CanadaAssociation for Computational LinguisticsJuly 2023</p>
<p>Chatgpt outperforms crowd workers for text-annotation tasks. Fabrizio Gilardi, Meysam Alizadeh, Maël Kubli, 10.1073/pnas.2305016120Proceedings of the National Academy of Sciences. 1091-649012030July 2023</p>
<p>Large language models are state-of-the-art evaluators of translation quality. Tom Kocmi, Christian Federmann, Proceedings of the 24th Annual Conference of the European Association for Machine Translation. Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Alvarez Sergi, Nora Vidal, Mara Aranberri, Carla Nunziatini, Mikel Parra Escartín, Maja Forcada, Carolina Popovic, Helena Scarton, Moniz, the 24th Annual Conference of the European Association for Machine TranslationTampere, FinlandEuropean Association for Machine TranslationJune 2023</p>
<p>Replacing judges with juries: Evaluating llm generations with a panel of diverse models. Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, Patrick Lewis, May 2024</p>
<p>Is ChatGPT a good NLG evaluator? a preliminary study. Jiaan Wang, Yunlong Liang, Fandong Meng, Zengkui Sun, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, Jie Zhou, 10.18653/v1/2023.newsum-1.1Proceedings of the 4th New Frontiers in Summarization Workshop. Yue Dong, Wen Xiao, Lu Wang, Fei Liu, Giuseppe Carenini, the 4th New Frontiers in Summarization WorkshopSingaporeAssociation for Computational LinguisticsDecember 2023</p>
<p>Style over substance: Evaluation biases for large language models. Minghao Wu, Alham Fikri, Aji , 2023</p>
<p>Are large language model-based evaluators the solution to scaling up multilingual evaluation?. Rishav Hada, Varun Gumma, Adrian Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, Sunayana Sitaram, Findings of the Association for Computational Linguistics: EACL 2024. Yvette Graham, Matthew Purver, St. Julian's, MaltaAssociation for Computational LinguisticsMarch 2024</p>
<p>The effectiveness of LLMs as annotators: A comparative overview and empirical analysis of direct representation. Maja Pavlovic, Massimo Poesio, Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024. Gavin Abercrombie, Valerio Basile, Davide Bernadi, Shiran Dudy, Simona Frenda, Lucy Havens, Sara Tonelli, the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024Torino, ItaliaMay 2024ELRA and ICCL</p>
<p>Benchmarking cognitive biases in large language models as evaluators. Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae , Myung Kim, Dongyeop Kang, 2023</p>
<p>Evaluating large language models at evaluating instruction following. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, Danqi Chen, The Twelfth International Conference on Learning Representations. 2024</p>
<p>Pitfalls of conversational LLMs on news debiasing. Ipek Baris Schlicht, Defne Altiok, Maryanne Taouk, Lucie Flek, Proceedings of the First Workshop on Language-driven Deliberation Technology (DELITE) @ LREC-COLING 2024. Annette Hautli-Janisz, Gabriella Lapesa, Lucas Anastasiou, Valentin Gold, Anna De Liddo, Chris Reed, the First Workshop on Language-driven Deliberation Technology (DELITE) @ LREC-COLING 2024Torino, ItaliaMay 2024ELRA and ICCL</p>
<p>LLM-blender: Ensembling large language models with pairwise ranking and generative fusion. Dongfu Jiang, Xiang Ren, Bill Yuchen, Lin , 10.18653/v1/2023.acl-long.792Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. Anna Rogers, Jordan Boyd-Graber, Naoaki Okazaki, the 61st Annual Meeting of the Association for Computational LinguisticsToronto, CanadaAssociation for Computational LinguisticsJuly 20231</p>
<p>Memories for virtual AI characters. Fabian Landwehr, Erika Varis Doggett, Romann M Weber, 10.18653/v1/2023.inlg-main.17Proceedings of the 16th International Natural Language Generation Conference. C , Maria Keet, Hung-Yi Lee, Sina Zarrieß, the 16th International Natural Language Generation ConferencePrague, CzechiaAssociation for Computational LinguisticsSeptember 2023</p>
<p>Linhao Luo, Thuy-Trang Vu, Dinh Phung, Gholamreza Haffari, arXiv:2310.11638Systematic assessment of factual knowledge in large language models. 2023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. Artidoro Pagnoni, Vidhisha Balachandran, Yulia Tsvetkov ; Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, Yichao Zhou, 10.18653/v1/2021.naacl-main.383Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Kristina Toutanova, Anna Rumshisky, the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesAssociation for Computational LinguisticsJune 2021</p>
<p>Measuring attribution in natural language generation models. Vitaly Hannah Rashkin, Matthew Nikolaev, Lora Lamm, Michael Aroyo, Dipanjan Collins, Slav Das, Gaurav Petrov, Iulia Singh Tomar, David Turc, Reitter, 202112</p>
<p>Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, arXiv:2401.05561Trustworthiness in large language models. 2024arXiv preprint</p>
<p>Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun, arXiv:2402.13714An evaluation of large language models in bioinformatics research. 2024arXiv preprint</p>
<p>Many bioinformatics programming tasks can be automated with chatgpt. Paul Stephen R Piccolo, Andrew Denny, Samuel Luxton-Reilly, Perry G Payne, Ridge, arXiv:2303.135282023arXiv preprint</p>
<p>Comparative performance evaluation of large language models for extracting molecular interactions and pathway knowledge. Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero, Patrick Johnstone, Shinjae Yoo, Francis J Alexander, arXiv:2307.088132023arXiv preprint</p>
<p>Considers-the-human evaluation framework: Rethinking human evaluation for generative large language models. Aparna Elangovan, Ling Liu, Lei Xu, Sravan Bodapati, Dan Roth, arXiv:2405.186382024arXiv preprint</p>
<p>Bioinfo-bench: A simple benchmark framework for llm bioinformatics skills evaluation. Qiyuan Chen, Cheng Deng, bioRxiv. 2023</p>
<p>Large language models encode clinical knowledge. Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Nature. 62079722023</p>
<p>Katharina Kann, Sascha Rothe, Katja Filippova, arXiv:1809.08731Sentence-level fluency evaluation: References help, but can be spared!. 2018arXiv preprint</p>
<p>Gleu: Automatic evaluation of sentence-level fluency. Andrew Mutton, Mark Dras, Stephen Wan, Robert Dale, Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics. the 45th Annual Meeting of the Association of Computational Linguistics2007</p>
<p>Do prompt-based models really understand the meaning of their prompts?. Albert Webson, Ellie Pavlick, arXiv:2109.012472021arXiv preprint</p>
<p>Measuring reliability of large language models through semantic consistency. Harsh Raj, Domenic Rosati, Subhabrata Majumdar, arXiv:2211.058532022arXiv preprint</p>
<p>CTRLEval: An Unsupervised Reference-Free Metric for Evaluating Controlled Text Generation. Pei Ke, Hao Zhou, Yankai Lin, Peng Li, Jie Zhou, Xiaoyan Zhu, Minlie Huang, arXiv:2204.00862December 2022</p>
<p>The Curious Case of Neural Text Degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, arXiv:1904.09751February 2020</p>
<p>On Faithfulness and Factuality in Abstractive Summarization. Joshua Maynez, Shashi Narayan, Bernd Bohnet, Ryan Mcdonald, arXiv:2005.00661May 2020b</p>
<p>A stitch in time saves nine: Detecting and mitigating hallucinations of llms by validating low-confidence generation. Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, Dong Yu, arXiv:2307.039872023arXiv preprint</p>
<p>Hallucination is the last thing you need. Shawn Curran, Sam Lansley, Oliver Bethell, arXiv:2306.115202023arXiv preprint</p>
<p>A survey of methods for addressing class imbalance in deep-learning based natural language processing. Sophie Henning, William Beluch, Alexander Fraser, Annemarie Friedrich, arXiv:2210.046752022arXiv preprint</p>
<p>. Oskar Wysocki, Zili Zhou, Paul O 'regan, Deborah Ferreira, Magdalena Wysocka, Dónal Landers, André Freitas, 10.1162/coli_a_00462Transformers and the Representation of Biomedical Background Knowledge. Computational Linguistics. 0891-201749103 2023b</p>
<p>Understanding the effects of language-specific class imbalance in multilingual fine-tuning. Vincent Jung, Lonneke Van Der Plas, arXiv:2402.130162024arXiv preprint</p>
<p>How context affects language models' factual predictions. Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, 2020</p>
<p>Large language models are human-level prompt engineers. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, Jimmy Ba, 2023</p>
<p>Fungal names: a comprehensive nomenclatural repository and knowledge base for fungal taxonomy. Fang Wang, Ke Wang, Lei Cai, Mingjun Zhao, Paul M Kirk, Guomei Fan, Qinglan Sun, Bo Li, Shuai Wang, Zhengfei Yu, Dong Han, Juncai Ma, Linhuan Wu, Yijian Yao, 10.1093/nar/gkac926Nucleic Acids Research. 0305-104851D110 2022</p>
<p>What defines the "kingdom" fungi?. Thomas A Richards, Guy Leonard, Jeremy G Wideman, 10.1128/microbiolspec.FUNK-0044-2017Microbiology Spectrum. 5320175.3.23</p>
<p>How to publish a new fungal species, or name. Aime Catherine, Andrew N Miller, Takayuki Aoki, Konstanze Bensch, Lei Cai, Pedro W Crous, David L Hawksworth, Kevin D Hyde, Paul M Kirk, Robert Lücking, 202112version 3.0. IMA fungus</p>
<p>International Code of Nomenclature for algae, fungi, and plants (Shenzhen Code) adopted by the Nineteenth International Botanical Congress Shenzhen. Nick J Turland, John Harry Wiersema, Fred R Barrie, Werner Greuter, David L Hawksworth, Patrick Stephen Herendeen, Sandra Knapp, Wolf-Henning Kusber, De-Zhu Li, Karol Marhold, July 2017. 2018ChinaKoeltz botanical books</p>
<p>Unambiguous identification of fungi: where do we stand and how accurate and precise is fungal dna barcoding? IMA fungus. Robert Lücking, Catherine Aime, Barbara Robbertse, Andrew N Miller, Hiran A Ariyawansa, Takayuki Aoki, Gianluigi Cardinali, Pedro W Crous, Irina S Druzhinina, David M Geiser, 20201114</p>
<p>An llm-based knowledge synthesis and scientific reasoning framework for biomedical discovery. Oskar Wysocki, Magdalena Wysocka, Danilo Carvalho, Alex Teodor Bogatu, Danilo Miranda Gusicuma, Maxime Delmas, Harriet Unsworth, Andre Freitas, arXiv:2406.186262024arXiv preprint</p>
<p>Language models are unsupervised multitask learners. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, 2019</p>
<p>The pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.000272020arXiv preprint</p>
<p>BigScience Language Open-science Open-access Multilingual (BLOOM) Language Model. Bigscience, 2022</p>
<p>BioGPT: generative pre-trained transformer for biomedical text generation and mining. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, Tie-Yan Liu, 10.1093/bib/bbac409Briefings in Bioinformatics. 1477-4054236</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1993 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1993</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1993</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-46.html">extraction-schema-46</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <p><strong>Paper ID:</strong> paper-276408396</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2502.10802v2.pdf" target="_blank">CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation</a></p>
                <p><strong>Paper Abstract:</strong> Large Language Models (LLMs) have shown remarkable performance in automated code generation. However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable. While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases. To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases. CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers. The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with an additional test case generation operator for test case evolution. Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multi-objective optimization method for test case selection. Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art performance in automated code generation and testing. These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1993.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1993.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoCoEvo</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoCoEvo: Co-Evolution of Programs and Test Cases</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based co-evolutionary framework that alternates program evolution (with LLM-based crossover and mutation operators) and test-case evolution (with LLM-based test generation and Pareto selection) to generate and validate code without predefined tests.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CoCoEvo</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based (hybrid evolutionary framework)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Program operators: LLM-based crossover and LLM-based mutation. Crossover: two parent programs are concatenated into a prompt and the LLM is asked to synthesise useful fragments into a child program; parents selected by binary tournament on fitness. Mutation: a single program is included in a prompt and the LLM is instructed to rewrite it using an alternative implementation. Test-case operator: LLM-based generation of additional test cases guided by coverage feedback from the best program. No online fine‑tuning is performed; operators are LLM inference calls driven by task-specific prompts. A cosine-annealing crossover-rate scheduler adjusts ratio of crossover vs mutation over iterations. Test-case selection uses multi-objective (Pareto) selection on Confidence and Discrimination metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td>Not reported for the operators themselves. Operators are implemented by calling pretrained LLMs (GPT-4o-mini, Qwen2.5-Coder-32B, Llama-3.1-70B, DeepSeek-V3); the paper does not provide the training corpora or sizes for these models.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Program synthesis / automated code generation on LeetCode-Contest (80 contest problems, ~644 ground-truth tests per problem) and a small DevEval real-world subset (10 problems from real projects).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Sampling, Sampling+Filtering, Self-Repair, Reflexion, INTERVENOR, CodeCoT, AgentCoder, MBR-Exec, CodeT (all adapted to use LLM-generated tests where applicable).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>pass@1 on LeetCode-Contest (CoCoEvo final submitted program): GPT-4o-mini 49.75%, Qwen2.5-Coder-32B 55.75%, Llama-3.1-70B 45.00%, DeepSeek-V3 76.25% (Table II). On DevEval real-world subset with Qwen2.5-Coder-32B: CoCoEvo 44.00% (Table XI).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td>CoCoEvo is itself hybrid (evolutionary algorithm control + LLM-based operators). Its empirical hybrid performance is reported above (see performance_learned_operator) and consistently outperforms all listed baselines across the four tested LLMs (Table II).</td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td>Not reported as a single % for programs; evaluation uses ground-truth test suites (program considered correct if it passes all ground-truth tests). For test cases the paper reports that the evolved test-case population has consistently higher accuracy than randomly generated test cases (qualitative and plotted in Fig.10), but exact numeric rates are not tabulated in the main text.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td>For test-cases: 'Discrimination' measured as binary-pass entropy (Disc_T = -p log2 p - (1-p) log2 (1-p)) where p is pass rate across programs; Pareto selection preserves high-confidence and/or high-discrimination tests. For program diversity/novelty no explicit numeric diversity metric provided.</td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>Partially evaluated: dataset selection aimed to avoid training-set leakage (problems released after March 2024). On a small real-world DevEval subset (more complex dependencies), performance degraded for all methods but CoCoEvo still outperformed baselines (CoCoEvo 44% vs Sampling 38%, Sampling+Filtering 40%, CodeT 40% — Table XI).</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>The paper intentionally selected recent contest problems to reduce overlap with LLM pretraining corpora, but acknowledges no definitive proof training-data leakage is absent. It reports that methods relying on predefined (trusted) test suites degrade substantially when switched to LLM-self-generated tests (faulty LLM tests mislead repair methods), indicating susceptibility of pipeline methods to noise in LLM outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Token-usage analysis: CoCoEvo requires more input tokens (prompts) than CodeT because crossover/mutation/test-generation modules include existing programs/tests in prompts, but CoCoEvo's output (completion) tokens are lower than repair-based methods. An early-stopping mechanism reduces CoCoEvo overall token usage by ~30–50% (authors report 30%–50% reduction with early-stop parameter n=4) while only slightly reducing accuracy. Exact per-method token counts are in Table VIII (tables omitted here), but the qualitative conclusion is: LLM-based evolutionary operators incur significant token/inference cost; repair-oriented baselines consume the most tokens due to verbose reflective prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td>Multiple general-purpose code-capable LLMs were compared (GPT-4o-mini, Qwen2.5-Coder-32B, Llama-3.1-70B, DeepSeek-V3); results vary by model (see performance_learned_operator), but the paper does not systematically separate 'domain‑specific' vs 'general' pretraining effects beyond reporting per-model performance.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td>Removing the test-case evolution module (so only program evolution with raw LLM-generated tests) reduces pass@1: GPT-4o-mini CoCoEvo 49.75% → w/o test evolution 48.00%; Qwen2.5 55.75% → 51.75%; Llama-3.1 45.00% → 43.00%; DeepSeek-V3 76.25% → 74.50% (Table VII). This shows test-case evolution provides consistent, modest gains.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td>Not explicitly characterized for programs. For test-cases, the paper computes coverage feedback against the best program and uses that to guide generation; no explicit measure of hypothesis-space coverage for operators is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Adaptation occurs via the evolutionary loop and a dynamic crossover-rate scheduler (cosine annealing) that changes the ratio of crossover vs mutation across generations; LLM parameters are not fine-tuned online — operators adapt only via selection and prompt-driven generation, not gradient updates. The scheduler improves long-term convergence relative to a fixed-rate baseline (cosine beats constant after ~5 iterations in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>1) High token/inference cost due to many LLM calls; 2) Dependence on the LLM's code-generation ability — operators inherit LLM limitations; 3) LLM-generated test cases can be incorrect, and some high-discrimination but low-confidence tests may slip through Pareto selection and slightly reduce test-population accuracy; 4) Challenges handling dependencies and long contexts in real-world code; 5) No online retraining of operators, so systematic biases in the pretrained LLMs persist.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>LLM-driven evolutionary operators (crossover, mutation, and test generation) integrated in a co-evolutionary loop can outperform multiple prior LLM-only and test‑agreement baselines on contest-style program synthesis benchmarks, provided (1) test cases are co-evolved (improves accuracy vs using raw LLM tests), (2) a dynamic scheduler controls exploration vs exploitation (cosine annealing avoids premature convergence), and (3) multi-objective selection of tests (confidence + discrimination via Pareto) preserves useful tests while filtering faulty ones. However, such learned operators are more costly (token/inference) and remain sensitive to noisy LLM outputs; they generalize better than baselines on held-out contest problems and still lead on a small real-world benchmark, but absolute performance degrades when moving to complex, dependency-heavy real code. Overall, learned (LLM) operators provide stronger directed search than naive random mutation, but at higher inference cost and with failure modes tied to the underlying LLM's reliability.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1993.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1993.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TraditionalRandomMutation</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Traditional random mutation operator (evolutionary/genetic programming literature)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Classical evolutionary mutation that applies random, syntactic or structural edits to programs (e.g., subtree substitution, token edits) without semantic guidance from a learned model.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Traditional random mutation</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>traditional GP / random mutation (hand-designed)</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Random, typically syntactic edits or pre-specified mutation operators (e.g., subtree replacement, random token edits). In the paper this is described only conceptually as the conventional alternative to LLM-based mutation.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Referenced conceptually in the context of program evolution (CoCoEvo compares LLM-based mutation vs 'traditional random mutation methods').</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>CoCoEvo's LLM-based mutation is contrasted qualitatively with traditional random mutation methods.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Not numerically compared in the paper. Authors argue LLM-based mutation can retain key features of high-quality programs allowing higher mutation rates without large loss, implying different exploration-exploitation tradeoffs; but no wall-clock or token-cost comparison vs classical random mutation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Traditional mutation does not adapt via prompts; adaptation arises via evolutionary selection only. In contrast, CoCoEvo's LLM-based mutation is prompt-driven and can be semantically guided.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Classical random mutation may introduce syntax/semantics-breaking edits and tends to require careful operator design to preserve useful building blocks; the paper motivates LLM-based mutation to avoid some destructive random edits but provides no numeric failure-mode study.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>The paper provides a qualitative claim: LLM-based mutation can preserve desirable program features better than random mutation enabling higher mutation rates and more effective exploration when combined with selection, but lacks controlled numeric comparisons vs classical GP mutation operators.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1993.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1993.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CodeT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CodeT (Code generation with generated tests)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-based method that generates code and test cases and uses a clustering/agreement metric (product of cluster size and number of test cases passed) to select better programs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>CodeT: Code generation with generated tests.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>CodeT</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based test-case generation + agreement-based selection</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Generates code samples and test cases using LLMs (zero-shot test generation), then clusters programs based on their performance on generated tests; scoring = (#programs in cluster) * (#tests passed). No GP-style learned operators; selection based on agreement among generated programs and tests.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Program synthesis on LeetCode-Contest dataset (as baseline in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared to Sampling, Sampling+Filtering, repair-based methods, MBR-Exec, and CoCoEvo.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td>pass@1 (Table II): GPT-4o-mini 46.25%, Qwen2.5-Coder-32B 47.50%, Llama-3.1-70B 41.25%, DeepSeek-V3 72.50%.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td>When augmented with test coverage information, CodeT's performance sometimes degrades depending on the LLM (table VI notes mixed effects), indicating sensitivity to coverage-guided test generation outside its original configuration.</td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td>Not directly reported; CodeT relies on LLM-generated tests and thus inherits any biases/noise in those tests. The paper notes CodeT generally outperforms sampling and repair-based methods in the self-generated-test setting.</td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Shares program and test generation costs with MBR-Exec and Sampling+Filtering; token consumption similar to those methods (Table VIII qualitative discussion).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Reliant on correctness of LLM-generated tests; if tests are faulty performance can be affected. In general CodeT is more robust than naive pass-rate scoring but still inferior to CoCoEvo in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>Agreement-based selection (CodeT score) provides better robustness and higher pass@1 than raw pass-rate selection and many repair-based methods when using self-generated tests, but co-evolutionary refinement of both tests and programs (CoCoEvo) produces further improvements.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1993.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1993.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that compare learned operators (LLM-based, neural, or data-driven) with traditional genetic programming operators, including performance comparisons, training data effects, generalization, computational costs, and hybrid approaches.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenELM_and_related</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenELM / LLM-as-operator prior works (OpenELM, Language-model-crossover, Romera et al., ReEvo, AutoTest, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Prior works that integrate LLMs as adaptive evolutionary operators or use few-shot prompting to create crossover/mutation operators; cited in related work as antecedents to LLM-based evolutionary operators.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>The openelm library: Leveraging progress in language models for novel evolutionary algorithms.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenELM and related LLM-driven evolutionary operator works</td>
                        </tr>
                        <tr>
                            <td><strong>operator_type</strong></td>
                            <td>LLM-based / learned-from-data (prompted) operators</td>
                        </tr>
                        <tr>
                            <td><strong>operator_description</strong></td>
                            <td>Examples: OpenELM provides library support and designs specialized evolutionary operators; Meyerson et al. propose few-shot prompting to implement a domain-agnostic crossover by concatenating parents; Romera et al. propose context-aware program mutations via LLM prompts; ReEvo and others combine reflective LLM steps with evolutionary search. These works use prompts and few-shot examples rather than gradient-based learning of operators.</td>
                        </tr>
                        <tr>
                            <td><strong>training_data_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_benchmark</strong></td>
                            <td>Mostly applied to optimization problems, program search, and algorithm/heuristic design in prior works; cited generally in context of code/program evolution.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Generally compared to classical evolutionary operators or used as new operator designs; this paper cites them as prior art but does not re-run those exact systems.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_learned_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_traditional_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_hybrid_operator</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validity_or_executability_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_or_diversity_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>out_of_distribution_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_bias_evidence</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>computational_cost_comparison</strong></td>
                            <td>Not quantified here; these references motivate the idea that LLM-operators incur inference cost and can increase semantic-quality of offspring but trade off with invocation cost.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_learning_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_vs_general_pretraining</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>ablation_study_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_space_characterization</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>adaptation_during_evolution</strong></td>
                            <td>Typically adaptation is via selection and via prompt engineering (few-shot examples) rather than online gradient updates; some works explore reflective loops to refine operators.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_modes</strong></td>
                            <td>Noted generally in the literature: LLM operators may be expensive, can hallucinate, and depend on model capability; this paper references these limitations but does not provide new quantitative measures for these prior works.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings_for_theory</strong></td>
                            <td>These prior works collectively indicate that LLMs can be effective as semantic-aware evolutionary operators (crossover/mutation) and inspire CoCoEvo's LLM-based operator design; however, direct numeric comparisons between these LLM-driven operators and classic GP operators remain sparse and are not provided in this paper.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>The openelm library: Leveraging progress in language models for novel evolutionary algorithms. <em>(Rating: 2)</em></li>
                <li>Language model crossover: Variation through few-shot prompting. <em>(Rating: 2)</em></li>
                <li>ReEvo: Large language models as hyper-heuristics with reflective evolution. <em>(Rating: 2)</em></li>
                <li>Autotest: Evolutionary code solution selection with test cases. <em>(Rating: 1)</em></li>
                <li>CodeT: Code generation with generated tests. <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1993",
    "paper_id": "paper-276408396",
    "extraction_schema_id": "extraction-schema-46",
    "extracted_data": [
        {
            "name_short": "CoCoEvo",
            "name_full": "CoCoEvo: Co-Evolution of Programs and Test Cases",
            "brief_description": "An LLM-based co-evolutionary framework that alternates program evolution (with LLM-based crossover and mutation operators) and test-case evolution (with LLM-based test generation and Pareto selection) to generate and validate code without predefined tests.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "CoCoEvo",
            "operator_type": "LLM-based (hybrid evolutionary framework)",
            "operator_description": "Program operators: LLM-based crossover and LLM-based mutation. Crossover: two parent programs are concatenated into a prompt and the LLM is asked to synthesise useful fragments into a child program; parents selected by binary tournament on fitness. Mutation: a single program is included in a prompt and the LLM is instructed to rewrite it using an alternative implementation. Test-case operator: LLM-based generation of additional test cases guided by coverage feedback from the best program. No online fine‑tuning is performed; operators are LLM inference calls driven by task-specific prompts. A cosine-annealing crossover-rate scheduler adjusts ratio of crossover vs mutation over iterations. Test-case selection uses multi-objective (Pareto) selection on Confidence and Discrimination metrics.",
            "training_data_description": "Not reported for the operators themselves. Operators are implemented by calling pretrained LLMs (GPT-4o-mini, Qwen2.5-Coder-32B, Llama-3.1-70B, DeepSeek-V3); the paper does not provide the training corpora or sizes for these models.",
            "domain_or_benchmark": "Program synthesis / automated code generation on LeetCode-Contest (80 contest problems, ~644 ground-truth tests per problem) and a small DevEval real-world subset (10 problems from real projects).",
            "comparison_baseline": "Sampling, Sampling+Filtering, Self-Repair, Reflexion, INTERVENOR, CodeCoT, AgentCoder, MBR-Exec, CodeT (all adapted to use LLM-generated tests where applicable).",
            "performance_learned_operator": "pass@1 on LeetCode-Contest (CoCoEvo final submitted program): GPT-4o-mini 49.75%, Qwen2.5-Coder-32B 55.75%, Llama-3.1-70B 45.00%, DeepSeek-V3 76.25% (Table II). On DevEval real-world subset with Qwen2.5-Coder-32B: CoCoEvo 44.00% (Table XI).",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": "CoCoEvo is itself hybrid (evolutionary algorithm control + LLM-based operators). Its empirical hybrid performance is reported above (see performance_learned_operator) and consistently outperforms all listed baselines across the four tested LLMs (Table II).",
            "validity_or_executability_rate": "Not reported as a single % for programs; evaluation uses ground-truth test suites (program considered correct if it passes all ground-truth tests). For test cases the paper reports that the evolved test-case population has consistently higher accuracy than randomly generated test cases (qualitative and plotted in Fig.10), but exact numeric rates are not tabulated in the main text.",
            "novelty_or_diversity_metric": "For test-cases: 'Discrimination' measured as binary-pass entropy (Disc_T = -p log2 p - (1-p) log2 (1-p)) where p is pass rate across programs; Pareto selection preserves high-confidence and/or high-discrimination tests. For program diversity/novelty no explicit numeric diversity metric provided.",
            "out_of_distribution_performance": "Partially evaluated: dataset selection aimed to avoid training-set leakage (problems released after March 2024). On a small real-world DevEval subset (more complex dependencies), performance degraded for all methods but CoCoEvo still outperformed baselines (CoCoEvo 44% vs Sampling 38%, Sampling+Filtering 40%, CodeT 40% — Table XI).",
            "training_bias_evidence": "The paper intentionally selected recent contest problems to reduce overlap with LLM pretraining corpora, but acknowledges no definitive proof training-data leakage is absent. It reports that methods relying on predefined (trusted) test suites degrade substantially when switched to LLM-self-generated tests (faulty LLM tests mislead repair methods), indicating susceptibility of pipeline methods to noise in LLM outputs.",
            "computational_cost_comparison": "Token-usage analysis: CoCoEvo requires more input tokens (prompts) than CodeT because crossover/mutation/test-generation modules include existing programs/tests in prompts, but CoCoEvo's output (completion) tokens are lower than repair-based methods. An early-stopping mechanism reduces CoCoEvo overall token usage by ~30–50% (authors report 30%–50% reduction with early-stop parameter n=4) while only slightly reducing accuracy. Exact per-method token counts are in Table VIII (tables omitted here), but the qualitative conclusion is: LLM-based evolutionary operators incur significant token/inference cost; repair-oriented baselines consume the most tokens due to verbose reflective prompts.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": "Multiple general-purpose code-capable LLMs were compared (GPT-4o-mini, Qwen2.5-Coder-32B, Llama-3.1-70B, DeepSeek-V3); results vary by model (see performance_learned_operator), but the paper does not systematically separate 'domain‑specific' vs 'general' pretraining effects beyond reporting per-model performance.",
            "ablation_study_results": "Removing the test-case evolution module (so only program evolution with raw LLM-generated tests) reduces pass@1: GPT-4o-mini CoCoEvo 49.75% → w/o test evolution 48.00%; Qwen2.5 55.75% → 51.75%; Llama-3.1 45.00% → 43.00%; DeepSeek-V3 76.25% → 74.50% (Table VII). This shows test-case evolution provides consistent, modest gains.",
            "hypothesis_space_characterization": "Not explicitly characterized for programs. For test-cases, the paper computes coverage feedback against the best program and uses that to guide generation; no explicit measure of hypothesis-space coverage for operators is provided.",
            "adaptation_during_evolution": "Adaptation occurs via the evolutionary loop and a dynamic crossover-rate scheduler (cosine annealing) that changes the ratio of crossover vs mutation across generations; LLM parameters are not fine-tuned online — operators adapt only via selection and prompt-driven generation, not gradient updates. The scheduler improves long-term convergence relative to a fixed-rate baseline (cosine beats constant after ~5 iterations in experiments).",
            "failure_modes": "1) High token/inference cost due to many LLM calls; 2) Dependence on the LLM's code-generation ability — operators inherit LLM limitations; 3) LLM-generated test cases can be incorrect, and some high-discrimination but low-confidence tests may slip through Pareto selection and slightly reduce test-population accuracy; 4) Challenges handling dependencies and long contexts in real-world code; 5) No online retraining of operators, so systematic biases in the pretrained LLMs persist.",
            "key_findings_for_theory": "LLM-driven evolutionary operators (crossover, mutation, and test generation) integrated in a co-evolutionary loop can outperform multiple prior LLM-only and test‑agreement baselines on contest-style program synthesis benchmarks, provided (1) test cases are co-evolved (improves accuracy vs using raw LLM tests), (2) a dynamic scheduler controls exploration vs exploitation (cosine annealing avoids premature convergence), and (3) multi-objective selection of tests (confidence + discrimination via Pareto) preserves useful tests while filtering faulty ones. However, such learned operators are more costly (token/inference) and remain sensitive to noisy LLM outputs; they generalize better than baselines on held-out contest problems and still lead on a small real-world benchmark, but absolute performance degrades when moving to complex, dependency-heavy real code. Overall, learned (LLM) operators provide stronger directed search than naive random mutation, but at higher inference cost and with failure modes tied to the underlying LLM's reliability.",
            "uuid": "e1993.0"
        },
        {
            "name_short": "TraditionalRandomMutation",
            "name_full": "Traditional random mutation operator (evolutionary/genetic programming literature)",
            "brief_description": "Classical evolutionary mutation that applies random, syntactic or structural edits to programs (e.g., subtree substitution, token edits) without semantic guidance from a learned model.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Traditional random mutation",
            "operator_type": "traditional GP / random mutation (hand-designed)",
            "operator_description": "Random, typically syntactic edits or pre-specified mutation operators (e.g., subtree replacement, random token edits). In the paper this is described only conceptually as the conventional alternative to LLM-based mutation.",
            "training_data_description": null,
            "domain_or_benchmark": "Referenced conceptually in the context of program evolution (CoCoEvo compares LLM-based mutation vs 'traditional random mutation methods').",
            "comparison_baseline": "CoCoEvo's LLM-based mutation is contrasted qualitatively with traditional random mutation methods.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": "Not numerically compared in the paper. Authors argue LLM-based mutation can retain key features of high-quality programs allowing higher mutation rates without large loss, implying different exploration-exploitation tradeoffs; but no wall-clock or token-cost comparison vs classical random mutation is provided.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Traditional mutation does not adapt via prompts; adaptation arises via evolutionary selection only. In contrast, CoCoEvo's LLM-based mutation is prompt-driven and can be semantically guided.",
            "failure_modes": "Classical random mutation may introduce syntax/semantics-breaking edits and tends to require careful operator design to preserve useful building blocks; the paper motivates LLM-based mutation to avoid some destructive random edits but provides no numeric failure-mode study.",
            "key_findings_for_theory": "The paper provides a qualitative claim: LLM-based mutation can preserve desirable program features better than random mutation enabling higher mutation rates and more effective exploration when combined with selection, but lacks controlled numeric comparisons vs classical GP mutation operators.",
            "uuid": "e1993.1"
        },
        {
            "name_short": "CodeT",
            "name_full": "CodeT (Code generation with generated tests)",
            "brief_description": "An LLM-based method that generates code and test cases and uses a clustering/agreement metric (product of cluster size and number of test cases passed) to select better programs.",
            "citation_title": "CodeT: Code generation with generated tests.",
            "mention_or_use": "mention",
            "system_name": "CodeT",
            "operator_type": "LLM-based test-case generation + agreement-based selection",
            "operator_description": "Generates code samples and test cases using LLMs (zero-shot test generation), then clusters programs based on their performance on generated tests; scoring = (#programs in cluster) * (#tests passed). No GP-style learned operators; selection based on agreement among generated programs and tests.",
            "training_data_description": null,
            "domain_or_benchmark": "Program synthesis on LeetCode-Contest dataset (as baseline in this paper).",
            "comparison_baseline": "Compared to Sampling, Sampling+Filtering, repair-based methods, MBR-Exec, and CoCoEvo.",
            "performance_learned_operator": "pass@1 (Table II): GPT-4o-mini 46.25%, Qwen2.5-Coder-32B 47.50%, Llama-3.1-70B 41.25%, DeepSeek-V3 72.50%.",
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": "When augmented with test coverage information, CodeT's performance sometimes degrades depending on the LLM (table VI notes mixed effects), indicating sensitivity to coverage-guided test generation outside its original configuration.",
            "training_bias_evidence": "Not directly reported; CodeT relies on LLM-generated tests and thus inherits any biases/noise in those tests. The paper notes CodeT generally outperforms sampling and repair-based methods in the self-generated-test setting.",
            "computational_cost_comparison": "Shares program and test generation costs with MBR-Exec and Sampling+Filtering; token consumption similar to those methods (Table VIII qualitative discussion).",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": null,
            "failure_modes": "Reliant on correctness of LLM-generated tests; if tests are faulty performance can be affected. In general CodeT is more robust than naive pass-rate scoring but still inferior to CoCoEvo in experiments.",
            "key_findings_for_theory": "Agreement-based selection (CodeT score) provides better robustness and higher pass@1 than raw pass-rate selection and many repair-based methods when using self-generated tests, but co-evolutionary refinement of both tests and programs (CoCoEvo) produces further improvements.",
            "uuid": "e1993.2"
        },
        {
            "name_short": "OpenELM_and_related",
            "name_full": "OpenELM / LLM-as-operator prior works (OpenELM, Language-model-crossover, Romera et al., ReEvo, AutoTest, etc.)",
            "brief_description": "Prior works that integrate LLMs as adaptive evolutionary operators or use few-shot prompting to create crossover/mutation operators; cited in related work as antecedents to LLM-based evolutionary operators.",
            "citation_title": "The openelm library: Leveraging progress in language models for novel evolutionary algorithms.",
            "mention_or_use": "mention",
            "system_name": "OpenELM and related LLM-driven evolutionary operator works",
            "operator_type": "LLM-based / learned-from-data (prompted) operators",
            "operator_description": "Examples: OpenELM provides library support and designs specialized evolutionary operators; Meyerson et al. propose few-shot prompting to implement a domain-agnostic crossover by concatenating parents; Romera et al. propose context-aware program mutations via LLM prompts; ReEvo and others combine reflective LLM steps with evolutionary search. These works use prompts and few-shot examples rather than gradient-based learning of operators.",
            "training_data_description": null,
            "domain_or_benchmark": "Mostly applied to optimization problems, program search, and algorithm/heuristic design in prior works; cited generally in context of code/program evolution.",
            "comparison_baseline": "Generally compared to classical evolutionary operators or used as new operator designs; this paper cites them as prior art but does not re-run those exact systems.",
            "performance_learned_operator": null,
            "performance_traditional_operator": null,
            "performance_hybrid_operator": null,
            "validity_or_executability_rate": null,
            "novelty_or_diversity_metric": null,
            "out_of_distribution_performance": null,
            "training_bias_evidence": null,
            "computational_cost_comparison": "Not quantified here; these references motivate the idea that LLM-operators incur inference cost and can increase semantic-quality of offspring but trade off with invocation cost.",
            "transfer_learning_results": null,
            "domain_specific_vs_general_pretraining": null,
            "ablation_study_results": null,
            "hypothesis_space_characterization": null,
            "adaptation_during_evolution": "Typically adaptation is via selection and via prompt engineering (few-shot examples) rather than online gradient updates; some works explore reflective loops to refine operators.",
            "failure_modes": "Noted generally in the literature: LLM operators may be expensive, can hallucinate, and depend on model capability; this paper references these limitations but does not provide new quantitative measures for these prior works.",
            "key_findings_for_theory": "These prior works collectively indicate that LLMs can be effective as semantic-aware evolutionary operators (crossover/mutation) and inspire CoCoEvo's LLM-based operator design; however, direct numeric comparisons between these LLM-driven operators and classic GP operators remain sparse and are not provided in this paper.",
            "uuid": "e1993.3"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "The openelm library: Leveraging progress in language models for novel evolutionary algorithms.",
            "rating": 2
        },
        {
            "paper_title": "Language model crossover: Variation through few-shot prompting.",
            "rating": 2
        },
        {
            "paper_title": "ReEvo: Large language models as hyper-heuristics with reflective evolution.",
            "rating": 2
        },
        {
            "paper_title": "Autotest: Evolutionary code solution selection with test cases.",
            "rating": 1
        },
        {
            "paper_title": "CodeT: Code generation with generated tests.",
            "rating": 2
        }
    ],
    "cost": 0.0172195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation
25 Jul 2025</p>
<p>Kefan Li kefanli@buaa.edu.cn 
Yuan Yuan yuan21@buaa.edu.cn 
Hongyue Yu 
Tingyu Guo tingyuguo@buaa.edu.cn 
Shijie Cao </p>
<p>School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>Qingdao Research Institute
Beihang University
Hangzhou Innovation Institute</p>
<p>Beihang University</p>
<p>School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>School of Computer Science and Engineering
Beihang University
100191BeijingChina</p>
<p>CoCoEvo: Co-Evolution of Programs and Test Cases to Enhance Code Generation
25 Jul 2025B14CC1307D5B1000B3EA9F624B94D26AarXiv:2502.10802v2[cs.SE]received 31 January 2025; revised 7 June 2025, accepted 24 July 2025.Large Language ModelsCode GenerationTest Case GenerationCo-Evolution
Large Language Models (LLMs) have shown remarkable performance in automated code generation.However, existing approaches often rely heavily on pre-defined test cases, which become impractical in scenarios where such cases are unavailable.While prior works explore filtering techniques between programs and test cases, they overlook the refinement of test cases.To address this limitation, we introduce CoCoEvo, a novel LLM-based co-evolution framework that simultaneously evolves programs and test cases.CoCoEvo eliminates the dependency on pre-defined test cases by generating both programs and test cases directly from natural language problem descriptions and function headers.The framework employs specialized evolutionary operators, including LLM-based crossover and mutation operators for program evolution, along with an additional test case generation operator for test case evolution.Additionally, we propose optimization strategies such as a crossover rate scheduler to balance exploration and convergence, and a multiobjective optimization method for test case selection.Experimental results on multiple state-of-the-art LLMs demonstrate that CoCoEvo surpasses existing methods, achieving state-of-the-art performance in automated code generation and testing.These results underscore the potential of co-evolutionary techniques in advancing the field of automated programming.</p>
<p>approaches frequently rely on pre-defined test cases for evaluation.For instance, Sampling+Filtering [5] first generates a batch of candidate codes, evaluates them using pre-defined test cases, and selects the code with the highest score as the final answer.Other methods, such as Reflexion [6], Self-Repair [7], INTERVENOR [8], LDB [9], and PairCoder [10], utilize execution feedback on pre-defined test cases to refine or repair generated code.These methods generally assume that predefined test cases are accurate and can be fully trusted.</p>
<p>However, pre-defined test cases are not always available.In test-driven development (TDD) [11], test cases need to be defined before program implementation.In real-world software development, teams often rely solely on natural language requirements, as there are no pre-existing test cases or example programs.Furthermore, creating pre-defined test cases can be labor-intensive, requiring significant manual effort.In situations where LLMs must generate test cases independently, the effectiveness of existing methods remains uncertain.</p>
<p>Some approaches do not rely on pre-defined test cases.For instance, methods like CodeT [12] and MBR-Exec [13] generate both code and test cases using LLMs and filter the code based on the execution results of these test cases.Similarly, CodeCoT [14], which utilizes chain-of-thought prompting [15], enables LLMs to generate code and test cases simultaneously and evaluate and refine the code based on these cases.Additionally, AgentCoder [16] introduces a multiagent system with a dedicated Tester agent to generate test cases for evaluating and repairing code.However, studies such as [12], [17] highlight that LLM-generated test cases can contain significant errors, potentially leading to unreliable evaluations and incorrect feedback.Moreover, many of these methods fail to verify the quality of the generated test cases.While approaches implement basic filtering mechanisms for test cases, other methods often use generated test cases directly for code evaluation without ensuring their correctness.</p>
<p>There are also previous co-evolution methods based on code and test cases, which have primarily focused on code maintenance and program repair.For example, [18] recommends new test cases based on source code similarity, [19] identifies outdated test cases, and [20] constructs test oracles from bug reports using predefined templates.Similarly, [21] generates test cases based on predefined specifications to drive software evolution.However, these approaches depend heavily on preexisting programs, test specifications, or templates, limiting their applicability in scenarios lacking such resources.</p>
<p>To address these limitations, we propose CoCoEvo, an LLM-based co-evolution framework that enables the simultaneous evolution of programs and test cases to produce more accurate outcomes (illustrated in Fig. 1).In CoCoEvo, both programs and test cases are generated by LLMs based on natural language problem descriptions.No pre-defined programs, test cases, or templates are needed.The CoCoEvo framework consists of two alternating steps: program evolution and test case evolution.In program evolution, we introduce LLMbased crossover and mutation operators to generate program offspring.The current test case population is used to calculate program fitness.In test case evolution, we design an additional test case generation operator to produce test case offspring.Conversely, the current program population is used to calculate test case fitness.For programs, we implement a fitness calculation method inspired by CodeT [12], which assesses agreement between programs and test cases.For test cases, we adopt a multi-objective optimization approach to select those with high accuracy and strong discriminatory power, using the Pareto method [22].Additionally, we introduce a dynamic crossover rate scheduling mechanism, enabling broader exploration during early evolution stages and faster convergence later.</p>
<p>To evaluate the effectiveness of CoCoEvo, we collected a LeetCode-Contest dataset comprising relatively new programming problems that are unlikely to have been included in the training data of the evaluated LLMs.We conducted experiments on four state-of-the-art models: GPT-4o-mini [1], Qwen2.5-Coder-32B[3], Llama-3.1-70B[2], and DeepSeek-V3 [4].We compared with both basic code generation methods, repair-based methods, and agreement-based methods.The results demonstrate that CoCoEvo outperforms all existing methods.Additionally, the experiments revealed that methods relying on pre-defined test cases for feedback experience significant performance degradation when adapted to selfgenerated test cases.This highlights the limitations of such methods in scenarios lacking pre-defined test cases.Furthermore, we conducted in-depth comparative experiments and discussions, which validated the effectiveness of the optimization strategies employed in CoCoEvo.</p>
<p>In summary, our main contributions are as follows:</p>
<p>1) Co-evolution framework.We propose a novel LLMbased co-evolution framework for programs and test cases, including specialized LLM-based operators for offspring generation.2) Optimized Evolution Techniques.For program evolution, we design a dynamic crossover rate scheduler that enhances exploration early on and accelerates convergence later.For test cases, we develop a multi-objective optimization method for fitness calculation and new population selection.3) Experimental Validation.We introduce the LeetCode-Contest dataset and conduct experiments using four state-of-the-art LLMs: GPT-4o-mini, Qwen2.5-Coder-32b,Llama-3.1-70B, and DeepSeek-V3.The experimental results demonstrate that CoCoEvo achieves state-ofthe-art performance, surpassing existing methods.Additionally, we conduct comparative experiments to evaluate the utility of each module within the framework, further highlighting the effectiveness of our approach.The remainder of this article is organized as follows.Section II discusses the related work.In Section III, the design details and implementation methods of the CoCoEvo are described.Sections IV and V present the experimental settings and discussions, respectively.Finally, Section VI concludes this paper and discusses the limitations and future work.</p>
<p>II. RELATED WORK</p>
<p>A. Enhancing Code Generation Accuracy with Test Case</p>
<p>Integrating test cases is now crucial for improving LLMgenerated code.Key methodologies use them to filter, evaluate, and refine programs, enhancing performance and reliability.</p>
<p>Li et al. [5]'s Sampling+Filtering approach samples numerous candidate programs and scores them against a given test set.Chen et al. [12] use LLMs to co-generate code and tests, selecting superior programs via a dual agreement metric.Shi et al. [13] use test execution results to assess program similarity, guiding program selection.</p>
<p>Beyond evaluation, test-case results drive iterative repair and refinement of generated code.Reflexion [6] converts test failures into verbal feedback for program repair.Similarly, Olausson et al. [7] evaluate LLMs' capabilities to repair programs using failed test feedback.Zhang et al. [23] allow LLMs to analyze test execution results to revise erroneous code.Zhong et al. [9] extend this idea by enabling step-by-step debugging.CodeCoT [14] uses chain-of-thought prompting [15] to sequentially generate code and tests, iteratively refining the code with test feedback.Zhang et al. [10] introduce a pair programming framework where test feedback informs repair strategies and planning.</p>
<p>The generation of test cases differs across these methodologies.While CodeT [12] and CodeCoT [14] use LLMs exclusively for test generation, Reflexion [6] employs them mainly for simple programming tasks.However, most methods rely on predefined test cases to filter, refine, and select programs.</p>
<p>B. Automated Test Case Generation</p>
<p>Early works, such as [24], leveraged search-based methods to generate test cases.More recently, research has shifted toward using LLMs for this task.Studies such as [25], [26], and [27] have explored the effectiveness of LLMs in automating program testing.Similarly, [28] investigates methods for enhancing existing test cases using LLMs.Others, like TOGA [29], TOGLL [30], and [31], generate test oracles from predefined templates.Meanwhile, [32] trained a deep reinforcement learning model for text-to-test case generation.The application of LLMs in software development extends to generating test cases for various scenarios.For instance, [33] employs GPT-4 to generate security tests for identifying vulnerabilities, while [27] uses LLMs to assist in generating unit tests for automatic software testing.Similarly, [34] demonstrates the use of LLMs to create test cases aimed at reproducing general software bugs.LLMs are also employed to improve the code accuracy through test case generation.CodeT [12], for example, utilizes LLMs with zero-shot prompts to directly generate test cases.Reflexion [6] introduces a Test Agent powered by LLMs for test case generation.</p>
<p>C. Evolution with LLMs for Code Generation</p>
<p>The integration of LLMs with evolutionary algorithms has been extensively explored to enhance code generation.Bradley et al. [35] introduced OpenELM, an open-source Python library that designs specialized evolutionary operators for code generation.Chen et al. [36] employed LLMs as adaptive operators for evolutionary neural architecture search to optimize network designs.Liu et al. [37] proposed using LLMs to evolve optimization algorithms, automating the processes of initialization, selection, and mutation.Liu et al. [38] introduced a framework integrating LLMs with evolutionary computation to co-evolve heuristic descriptions and executable code.Ma et al. [39] combined LLMs with evolutionary search to generate and refine RL reward functions via a reward-reflection mechanism.Meyerson et al. [40] used few-shot prompting to create a domain-agnostic crossover operator by concatenating parent solutions into one prompt.Similarly, Romera et al. [41] integrated LLMs with evolutionary algorithms to create contextaware program mutations.Ye et al. [42]'s ReEvo combines evolutionary search and LLM-driven reflections to generate heuristics via verbal optimization gradients.AutoTest [43] embeds LLM co-generation of code and test cases in an evolutionary loop for improved optimization.</p>
<p>Additionally, various LLM-based co-evolution methods have been proposed.For instance, Chi et al. [19] identifies outdated test cases by co-evolving production and test code.Ruan et al. [20] constructs test oracles from bug reports using predefined templates.Chi et al. [19] used LLMs with dynamic validation to automate production and test code co-evolution, identifying obsolete tests to enhance software quality and lower maintenance overhead.TestART [44]  For programs, we introduce LLM-based crossover and mutation operators for offspring generation.For test cases, we implement an LLM-based additional test case generation operator.Once the preset number of iterations is reached, the co-evolution loop terminates, and the program with the highest fitness score is returned as the final answer.Algorithm 1 provides the pseudo-code detailing the CoCoEvo process.Further implementation details for each algorithm step are elaborated in subsequent sections.</p>
<p>B. Program Evolution</p>
<p>Program evaluation.During evaluation, programs and test cases are cross-evaluated, as illustrated in Fig. 3.The outcomes are represented by a matrix M :
M i,j = 1, p i passed t j 0, p i failed t j(1)
where p i and t j represent the i-th program and the j-th test case, respectively.</p>
<p>Crossover Evaluation We design a Confidence metric to evaluate program fitness, drawing inspiration from CodeT [12].Programs that pass the same set of test cases are grouped into a set Z P , while the corresponding test cases are grouped into a set Z T .The Confidence of program i is defined as
Conf P (p i ) = |Z P | × |Z T | (2)
where the size of Z P and Z T are denoted by |Z P | and |Z T |, respectively.This Confidence score evaluates the likelihood of a program being correct.The fitness of program i is defined as
F P (p i ) = Conf P (p i )(3)
where a higher fitness score indicates a higher probability of correctness for the program.Crossover Rate Scheduler.When leveraging LLMs to perform program crossover and mutation, the crossover operation typically synthesizes the strengths of disparate programs but may drive the population toward premature convergence, whereas the mutation operation often introduces novel features yet cannot guarantee evolution in a more desirable direction.Therefore, we propose a dynamic crossover rate scheduler that adaptively adjusts the number of crossover and mutation operations.We define the crossover rate as the proportion of offspring programs generated through the crossover operator.In the early stages, a lower crossover rate is applied to promote exploration, and the crossover rate increases in later stages to refine convergence.This enables extensive exploration of the search space during the early stages of evolution, while facilitating rapid convergence toward superior solutions in later generations.</p>
<p>Unlike traditional random mutation methods, the LLMbased program mutation operator leverages LLMs for code generation, which could retain key features of high-quality programs.This allows for a higher mutation rate without significant loss of desirable traits.Thus, we adopt the cosine annealing method [45] for scheduling crossover rate:
x r = x f inal + 1 2 (x init −x f inal )(1+cos( π(r − 1) max iter − 1 )) (4)
where x r denotes the crossover rate at the r-th iteration, x init and x f inal represent the initial crossover rate and the final crossover rate, which are set to 0.0 and 1.0, respectively.During the iteration process, r starts at 1 and ends at max iter.</p>
<p>In the formula, both r and max iter are decremented by 1, since in the first iteration (r = 1) the program population is doing initialization and no crossover or mutation operations are performed.This formulation ensures that the crossover rate increases smoothly from 0 to 1.In each generation, N P offspring programs are produced.The number of crossover operations is determined by multiplying the crossover rate by N P and taking the floor of the result.The number of mutation operations is equal to N P minus the number of crossover operations, as calculated by the following equations:
N c P r = ⌊x r N P ⌋ N m P r = N P − N c P r(5)
where N P is the program population size, N c P r and N m P r represent the number of program crossover and mutation operations in the r-th iteration, respectively.Fig. 4 presents an example of the variation in crossover rate over 10 iterations.Since when r = 1, the population is randomly initialized, the figure illustrates changes in the crossover rate from r = 2 to 10, with the parameter max iter set to 10.This approach ensures a smooth transition between exploration (early stages) and convergence (later stages), enabling effective optimization of the solution space.</p>
<p>C. Test Case Evolution</p>
<p>Test case evaluation.The evaluation of test cases involves two key metrics: Confidence and Discrimination.Confidence evaluates the probability that a test case is correct, and discriminative power evaluates a test case's ability to distinguish between programs.The Confidence of test case j is calculated as
Conf T (t j ) = 1 N P N P i=1 M i,j F P (p i )(6)
where N P represents the size of the program population, F P (p i ) represents the fitness of program i, and M i,j is the evaluation matrix indicating whether p i passed t j .Intuitively, Confidence measures the degree of agreement between the program population and the test cases, with higher program fitness contributing more weight to the degree of agreement.</p>
<p>Simple test cases may fail to differentiate between correct and erroneous programs, as both types may pass them.To address this, we introduce a Discrimination metric.First, the pass rate of test case j across the program population is defined as
pr T j = 1 N P N P j=1 M i,j(7)
then the Discrimination of test case j is represented by the entropy of the pass rate:
Disc T (t j ) = −pr T j log 2 pr T j − (1 − pr T j )log 2 (1 − pr T j ). (8)
Test cases achieve higher Discrimination when their pass rate approaches 0.5, as they better differentiate among the program population.This indicator assesses whether the test cases can exert evolutionary pressure on the program population.Test case selection.The selection of test cases is performed using the Pareto approach for multi-objective optimization [22].The Pareto frontier, also known as the Pareto front or Pareto boundary, represents the set of non-dominated solutions in a multi-objective optimization problem.A solution is considered Pareto optimal if no other feasible solution improves one objective without causing a deterioration in at least one other.Formally, given a vector-valued objective function f
(x) = [f 1 (x), f 2 (x), ..., f n (x)]
, a solution x is Pareto optimal if there does not exist another solution y such that f i (x) ≤ f i (y) for all i = 1, ..., n, with at least one strict inequality.</p>
<p>In test case selection, the Pareto front is constructed based on the two metrics, Conf T and Disc T , which represent the Confidence of test cases and the Discrimination of test cases.The Pareto front is illustrated in Fig. 5. Test cases with high Conf T and low Disc T represent basic test cases that cover common scenarios, with a high probability of being correct.Test cases with high Disc T represent more challenging cases that are effective at exposing errors.During the selection process, preference is given to individuals that perform well on both indicators.This strategy aims to: (1) preserve the correctness of the test case population, ensuring accurate evaluation of the program, and (2) retain test cases with high discriminative power to maintain selection pressure on the program population.We also noticed that there are some test cases with very low Conf T that sometimes have a relatively high Disc T .To avoid adverse effects of these test cases on program evolution, the test cases with Conf T lower than the average in the selection results will be filtered out.</p>
<p>D. LLM-based Evolution Operators</p>
<p>Program crossover operator.We utilize LLMs to perform program crossover, as illustrated in Fig. 6.The process involves the following steps:</p>
<p>1) Selection.Two programs are selected as crossover parents using the binary tournament selection method based on their fitness scores.2) Crossover.The selected programs are combined into a prompt.The LLM is instructed to analyze their similarities and differences and generate a new program that merges useful elements from both.For example, in Fig. 6, two incorrect programs are sent to the LLM.By analyzing their structures, the LLM combines useful fragments from each and generates a new program that resolves the errors.</p>
<p>Program mutation operator.The LLM-based mutation operator intelligently rewrites a program to achieve the same functionality using a different approach.The steps are as follows:</p>
<p>1) Selection.A program is randomly selected from the current population as the mutation parent.2) Mutation.The parent program is included in a prompt, and the LLM is instructed to rewrite it using an alternative implementation method.For example, Fig. 7 demonstrates a mutation scenario where the task is to find the maximum difference between adjacent elements in a 2D grid.The parent program has a flaw-it only considers elements in the same row or column.The LLM analyzes the problem and rewrites the program to include diagonal relationships, successfully resolving the flaw.</p>
<p>IV. EXPERIMENTS A. Dataset</p>
<p>We collected a dataset of 80 programming problems from the weekly and biweekly contests hosted on the LeetCode platform 1 , forming the LeetCode-Contest dataset.In the dataset, each problem includes a prompt comprising the function header and a natural language description in the form of a docstring, along with program solutions and approximately 644 ground truth test cases per problem.These comprehensive ground truth test cases can closely simulate the real-world scenario of submissions on LeetCode, making our evaluation more convincing.To ensure data integrity and avoid potential leakage, all selected problems were released after March 2024.To more accurately assess the ability of LLMs to solve programming challenges, we took measures to prevent plagiarism Rethinking: The current implementation assumes that the difference between the first elements of <code>nums1</code> and <code>nums2</code> will correctly determine <code>x</code> for all elements in the arrays.This assumption may not hold if the arrays are not uniformly shifted by <code>x</code> due to some unexpected input or a bug.Additionally, ......</p>
<p>Additional Test Cases</p>
<p>This test case revealed an existing error in the program.Fig. 8. Illustration of LLM-based rethinking and additional test case generation.The LLM identified scenarios that were not covered by the current test case population and generated an edge test case (the test case with the green background).</p>
<p>B. Models</p>
<p>We utilized four widely adopted and powerful LLMs for our experiments: GPT-4o-mini [1], Qwen2.5-Coder-32B[3], Llama-3.1-70B[2], and DeepSeek-V3 [4].For GPT-4o-mini, we accessed the API via the OpenAI platform2 .For Qwen2.5-Coder-32B and Llama-3.1-70B,we used the APIs available on the DeepInfra platform 3 .For DeepSeek-V3, we utilized the API provided by the DeepSeek platform 4 .</p>
<p>C. Compared Baselines</p>
<p>We selected several of the most prominent and effective methods in the field of code generation with LLMs as our comparison baselines: 1) Sampling.A basic method that generates a large number of code samples and randomly selects one as the final answer.2) Sampling+Filtering [5].An enhancement of the Sampling method, where the generated codes are evaluated using test cases.The code with the highest score is selected as the final answer.3) Self-Repair [7].This method generates code randomly, and then feeds execution results on test cases back to the LLM, which repairs the code accordingly.4) Reflexion [6].Error information from test case executions is used to guide the LLM in providing reflection and repair suggestions.The code is then repaired based on these suggestions.5) INTERVENOR [8].Utilizes a Teacher-Student multiagent framework.The Teacher agent analyzes error information from test case execution and provides repair suggestions, while the Student agent applies the fixes.6) CodeCoT [14].Based on the chain-of-thought paradigm [15], this method requires the LLM to generate both code and test cases simultaneously.Feedback from execution errors is then used to repair the code.7) AgentCoder [16].Implements two LLM-based agents: a Programming agent for code generation and repair, and a Testing agent for test case generation.8) MBR-Exec [13].Generates both code and test cases using LLMs.A loss function evaluates execution similarity based on the outputs of the generated code when run on test inputs.Codes with higher execution divergence receive higher losses, and the code with the lowest loss is selected as the final answer.9) CodeT [12].Generates code and test cases with LLMs, then performs clustering based on how the codes perform on the test cases.The clustering score is calculated as the product of the number of codes in a cluster and the number of test cases passed.For the baselines (Sampling+Filtering, Self-Repair, Reflexion, INTERVENOR) that originally relied on pre-defined test cases for program evaluation, we adapt them to our setting by replacing the pre-defined test cases with LLM self-generated test cases.</p>
<p>D. Performance Indicators</p>
<p>The ground truth test cases in the dataset are used to evaluate the accuracy of the generated program.A code is considered correct if it passes all the ground truth test cases.</p>
<p>The performance of the approaches on the dataset is measured using the pass@1 metric [46].To calculate the pass@1 metric, each method submits a single program as the final answer for each problem.The pass@1 metric represents the percentage of problems successfully solved.Additionally, for further analysis of the CoCoEvo approach, the ground truth program solutions are used to evaluate the correctness of the generated test cases.A generated test case is considered correct if it executes successfully on the ground truth solution.</p>
<p>E. Experimental Settings</p>
<p>Following the comparison settings used in prior studies [7], [12], [13], we compare the performance of different methods by constraining the number of code generation attempts made by the LLMs to be equal.Based on the common limitations of these approaches and taking into account the cost of invoking LLMs, we limit the number of LLM code generation calls to 100.For methods that require test case generation, we impose a similar constraint by limiting the number of test case generation calls made to the LLMs to 10, with a maximum of 10 test cases extracted per generation.For CoCoEvo, we adopt a balanced configuration by setting both the population size N P and the number of iterations max iter to 10. Comparative experiments with alternative population sizes and detailed parameter settings of baselines and CoCoEvo can be found in the supplementary material.Since different code generation strategies may require varying numbers of tokens, we provide a detailed analysis of token consumption in Section V-G.We observed that when calculating the pass@1 metric, if multiple codes achieve the same score, one code will be randomly selected as the final result.This introduces a certain degree of randomness to the evaluation process.To mitigate this, for all methods, we independently and randomly repeated the "selection → evaluation" process 5 times and reported the averaged results.</p>
<p>V. RESULTS AND DISCUSSION</p>
<p>A. Comparison of Accuracy</p>
<p>In this section, we present the results of the accuracy comparison for the methods.Table II reports the pass@1 metric for each method across different LLMs on the LeetCode-Contest dataset.As shown in Table II, our proposed CoCoEvo method achieves the highest performance across all LLMs, demonstrating its superior ability to generate accurate code.This success can be attributed to the co-evolution framework employed by CoCoEvo, which facilitates a more effective iterative refinement between programs and test cases.</p>
<p>An interesting observation concerns the code repair based methods such as Self-Repair, Reflexion, and INTERVENOR.When these methods utilize test cases generated by LLMs instead of pre-defined ones, their effectiveness diminishes significantly.In some instances, their performance is even worse than that of the basic Sampling approach.This is primarily due to the faulty LLM-generated tests mislead the repair process, leading to suboptimal code modifications and degraded overall performance.Similarly, Sampling+Filtering yields only a slight improvement over plain Sampling under these conditions, highlighting these approaches' dependence on reliable test suites.</p>
<p>Furthermore, methods that leverage the agreement between programs and test cases, such as MBR-Exec and CodeT, outperform both Sampling and repair-based methods.Among these, CodeT stands out, delivering consistent improvements in pass@1 across all LLMs, suggesting that mutual validation of code and test cases is a promising strategy for improving generation accuracy.</p>
<p>Fig. 9 illustrates the changes of pass@1 during the evolution process.While fluctuations in performance are observed, the method demonstrates consistently strong performance on average.The figure shows that although the CoCoEvo method starts with a relatively low pass@1, its performance steadily improves over the course of evolution iterations, eventually surpassing that of the CodeT method.This demonstrates the effectiveness of the CoCoEvo method.By alternating the evolution of programs and test cases, the program population progressively converges toward the correct solution.</p>
<p>Fig. 10 presents the accuracy trends of the test case population and offspring over the evolutionary iterations, as well as the accuracy of the randomly generated test cases.As shown in the figure, the accuracy of the test case population consistently exceeds that of randomly generated test cases, demonstrating  Test Population Test Offspring Random Fig. 10.Illustrates the accuracy of the test case population and offspring during the evolutionary process, with the accuracy of the randomly generated test cases as a baseline.The variable r refers to the iteration round.The red line illustrates the accuracy of the CoCoEvo test case population, the blue line corresponds to the accuracy of its offspring during evolution, and the gray dashed line reflects the accuracy achieved by the random test case generation approach.</p>
<p>the effectiveness of the alternating selection strategy in the coevolutionary framework.This indicates that selecting test cases based on their Confidence is beneficial for identifying faulty test cases.Moreover, the strategy for generating additional test cases exhibits a modest upward trend in the accuracy of offspring test cases, which also contributes positively to maintaining the overall accuracy of the test case population.</p>
<p>Interestingly, the peak test accuracy of the test population is observed in the second iteration.In subsequent iterations, the accuracy declines slightly.The slight decline can be attributed to the presence of a few incorrect test cases on the Pareto front that exhibit high discriminative capability but low Confidence, thus affecting overall accuracy.Nevertheless, the Pareto selection approach preserves a substantial number of accurate and highly discriminative test cases, thereby offering greater benefits to the co-evolutionary process.Consequently, the marginal reduction in accuracy remains within an acceptable range.</p>
<p>B. Comparison of Program Evolution Scheduler</p>
<p>To assess the effectiveness of the crossover rate scheduler, we compare the cosine scheduler with a constant scheduler in our experiments, using a mutation rate of 0.2 and a crossover rate of 0.8.These experiments were performed on the four LLMs and the LeetCode-Contest dataset.The results, summarized in Table III and Figure 11, highlight key differences between the cosine scheduler and the constant rate approach.</p>
<p>At the early stages of the evolution process, the constant method, characterized by a fixed crossover and mutation rate, demonstrates faster convergence and initially outperforms the cosine scheduler on Qwen2.5-Coder-32B,Llama-3.1-70B, and DeepSeek-V3.However, this rapid convergence causes the Constant method to stagnate and become trapped in a local optimum, ultimately limiting its overall performance.In contrast, the cosine scheduler, despite its lower initial performance, maintains consistent improvement throughout the evolution process.On GPT-4o-mini, the cosine scheduler consistently outperforms the method with a constant crossover rate.</p>
<p>By adapting the crossover rate dynamically, the scheduler avoids premature convergence and achieves superior results in later stages.Notably, it surpasses the performance of the Constant method after the fifth iteration, ultimately delivering a higher pass@1 rate.This comparison underscores the importance of dynamic scheduling in optimizing program evolution, particularly in scenarios prone to local optima.</p>
<p>C. Comparison of Program Fitness Function</p>
<p>To evaluate the impact of different program fitness functions, we conducted a comparative analysis using the CodeT score and the simple pass rate as fitness functions.These experiments are performed on the four LLMs and the LeetCode-Contest dataset.The results are summarized in Table IV and illustrated in Figure 12.</p>
<p>The results indicate that using the raw pass rate as the fitness function yields a lower pass@1 than the CodeT score.This is because CodeT better captures the alignment between programs and test cases, reducing the impact of flawed or poorly designed tests on fitness.Consequently, CodeT delivers more stable performance and faster convergence.These findings highlight CodeT's superiority as a fitness function, particularly when robustness and convergence are essential.</p>
<p>D. Comparison of Test Fitness Function</p>
<p>In this section, we present the results of comparing various test fitness functions.Specifically, we evaluate Failure Rate,  mance.In contrast, using Failure Rate as the fitness function results in a sharp decline in pass@1 performance.Previous studies on co-evolution have generally argued that the strength of a test case increases as more programs fail on it.However, we discovered that if the test cases contain errors, this method can lead to a population dominated by faulty test cases, which fail to provide accurate feedback to the programs.Furthermore, Fig. 13 reveals that employing Confidence as the test fitness function slightly outperforms using Pass Rate in most cases.By weighting test cases according to program performance, higher-performing programs contribute more to the Confidence score, ensuring that the most reliable programs have the greatest influence on evaluation.</p>
<p>E. Analysis of Test Coverage Impact</p>
<p>During offspring test case generation, we incorporate coverage information of the current test case population on the best-performing program individual to guide the generation of more targeted test cases.To explore whether this strategy could improve baseline performance, we applied it to several compatible baselines.Since this strategy requires an initial set of programs and then iteratively generates test cases based on their coverage, we adopt this method for Sampling, Sampling+Filtering [5], MBR-Exec [13], and CodeT [12].We also conducted experiments to evaluate the performance of CoCoEvo without the coverage information.The results are shown in Table VI.</p>
<p>The experiments reveal that simply adding test coverage information to baseline methods can sometimes negatively affect performance, as observed in GPT-4o-mini with Sam-pling+Filtering, Llama-3.1-70B with MBR-Exec and CodeT, and DeepSeek-V3 with CodeT.In contrast, CoCoEvo benefited consistently from coverage information across all LLMs, underscoring that such data is most effective when used in conjunction with the co-evolutionary framework.</p>
<p>F. Ablation Study</p>
<p>We conduct ablation experiments across the four LLMs on the LeetCode-Contest dataset to evaluate the effectiveness of the test case evolution module.In these experiments, we removed the test case evolution module and directly utilized the test cases generated by the LLMs to evaluate the code.The comparison results are presented in Table VII and Fig. 15.</p>
<p>The experimental results demonstrate that using only the program evolution module, without incorporating the test case evolution module, still outperforms the Sampling baseline.However, after integrating the test case evolution module, the performance of CoCoEvo is further enhanced.This highlights the critical role of the test case evolution module in the coevolution process.</p>
<p>Through the co-evolution of test cases, two key benefits are achieved.First, the accuracy of test cases within the population improves, as they are refined based on the agreement among programs.Second, by applying the Pareto method, discriminative and challenging test cases are preserved.These refined test cases provide more accurate and meaningful feedback to the program evolution process, ultimately improving the program's pass rate in the final results.</p>
<p>G. Analysis of Token Usage</p>
<p>Some strategies, such as reflection and repair employed in some baseline methods, as well as the crossover, mutation, and additional test case generation strategies used in CoCoEvo, may consume more tokens than others.We therefore report each method's token consumption to compare their resource efficiency.Table VIII shows the average token consumption per problem on the LeetCode-Contest dataset.We report input (Prompt) and output (Completion) tokens separately, as output tokens are generally more costly.Notably, we designed an early stopping strategy to investigate whether the token consumption of the CoCoEvo method could be optimized.Since problem difficulty varies, simpler problems often require fewer iterations, whereas more complex problems tend to require more.Thus, the early stopping mechanism is triggered when the code population maintains the same fitness value and passes the same test cases for n consecutive generations, indicating convergence.In Table VIII, we report token consumption for different values of n (ranging from 1 to 5), denoted as CoCoEvo Stop-1 to CoCoEvo Stop-5, respectively.</p>
<p>As shown in Table VIII, Sampling consumes the fewest tokens because it only generates code, not test cases.Meanwhile, Sampling+Filtering [5], MBR-Exec [13], and CodeT [12] show identical token consumption as they share the same generated programs and test cases, differing only in their final program selection logic.Code repair methods (e.g., Self-Repair [7], Reflexion [6], INTERVENOR [8], CodeCoT [12], and AgentCoder [16]) consume significantly more tokens because their prompts must include the original code and test execution results.Reflexion consumes the most input tokens because it first generates a reflective analysis before rewriting the code.CoCoEvo's input token usage is higher than CodeT's because its crossover, mutation, and test generation modules require existing programs and test cases as input prompts.However, its output token usage is lower than that of repairbased methods.</p>
<p>Notably, we observe that with the integration of the early stopping strategy, the token consumption of CoCoEvo is significantly reduced.The required input tokens become comparable to those of CodeT, while the output token usage is lower.To examine the impact of the early stopping strategy on accuracy, we report the pass@1 performance of CoCoEvo on the LeetCode-Contest dataset under different values of n.The results are presented in Table IX.We find that, when jointly considering token consumption and accuracy, setting the early stopping parameter n to 4 leads to a slight decrease in accuracy compared to configurations without early stopping, but achieves a 30% to 50% reduction in overall token usage, This section evaluates CoCoEvo's performance on realworld software projects.We selected problems from the DevEval dataset [47], where each problem consists of a function and its corresponding unit tests extracted from a real-world project.To ensure realistic and manageable problems, we filtered for functions with: (1) at least one external dependency, (2) five or more unit tests, and (3) 10 to 100 lines of code.In total, we selected 10 problems from 7 software projects, as summarized in Table X.Unlike LeetCode-Contest, these real-world problems have complex dependencies and require full unit tests, not just single assertions.To address this, we manually added dependencies to the input prompts, adapted the test generation process to produce full unit tests, and refined the problem descriptions for clarity.Using the same setup as Section IV-E, we ran experiments with Qwen2.5-Coder-32B[3] and compared Co-CoEvo against three baselines: Sampling, Sampling+Filtering [5], and CodeT [12].As shown in Table XI, while all methods' performance degraded on real-world problems, CoCoEvo still consistently outperformed the baselines.A case study in the supplementary material presents a problem solved only by CoCoEvo, where every baseline fails.</p>
<p>However, our method still exhibits some limitations: The limitations of CoCoEvo are primarily reflected in its relatively high token consumption.However, this can be significantly mitigated by early stopping strategies, which reduce output tokens, though input prompts may remain long.Future work could explore how to compress the length of input prompts.Moreover, CoCoEvo faces several challenges when applied to real-world software programming tasks.Specifically, it lacks precise dependency extraction and memory management capabilities, and it relies heavily on detailed problem descriptions and docstrings.Future research may focus on integrating CoCoEvo with techniques such as dependency analysis, LLM-based agents, and RAG, as well as incorporating effective memory management mechanisms, to better adapt the method for real-world software development scenarios.In addition, all the datasets used in the experiments of this paper are based on the Python programming language.However, CoCoEvo is designed language-agnostic.Future research may investigate the effectiveness of CoCoEvo across multiple programming languages.Furthermore, the evolutionary operators in CoCoEvo rely to some extent on the programming capabilities of LLMs.Subsequent work could explore how to adapt CoCoEvo for use with smaller or less capable code language models.</p>
<p>Fig. 1 .
1
Fig. 1.An overview of CoCoEvo.Function implementation is considered a program individual, and each assert statement is considered a test case individual.These two entities undergo an alternating co-evolutionary process through crossover evaluation.</p>
<p>Fig. 3 .
3
Fig. 3. Illustration of the cross-evaluation process across programs and test cases.Each program and each test case are executed independently, producing either a pass or fail outcome.In the figure, P denotes programs, and T denotes test cases.Moreover, p 0 , p 1 , p 2 , ... represent each program, while t 0 , t 1 , t 2 , ... represent each test case.</p>
<p>Fig. 4 .
4
Fig.4.Illustration of the variation of the crossover rate scheduled based on the cosine annealing method.The crossover rate denotes the proportion of offspring generated through crossover operations during the reproduction process.r denote the r-th iteration, and xr represent the crossover rate at that iteration.In the figure, r starts from 2 because when r = 1 only population initialization takes place and no crossover operations are required.</p>
<p>Fig. 5 .
5
Fig. 5. Illustration of the Pareto front constructed based on the Conf T and Disc T .The Conf T and Disc T represent the Confidence of the test cases and the Discrimination of the test cases.The Pareto front is computed from the two indicators, and these test cases are selected as the next-generation population.</p>
<p>Fig. 6 .
6
Fig. 6.Illustration of LLM-based program crossover.The code blocks with the same background color indicate the segments of the parent program that are referenced by the LLM during crossover.</p>
<p>Fig. 7 .
7
Fig. 7. Illustration of LLM-based program mutation.During mutation, the LLM rewrote certain program segments, replacing the red background part of the program on the left with the green background part on the right, thereby correcting errors in the parent program.</p>
<h1>1 # 5 # 5</h1>
<p>155
Test case: Arrays with large numbers, x should be 1 assert addedInteger([999, 998, 997], [1000, 999, 998]) == Test case: Arrays with all zeros, x should be 0 assert addedInteger([0, 0, 0], [0, 0, 0]) == 0 # Test case: nums1 is larger than nums2 by 5, x should be -5 assert addedInteger([6, 7, 8], [1, 2, 3]) == -Test case: Arrays with mixed positive and negative numbers assert addedInteger([3, -1, 4], [1, -3, 2]) == -2 ...... Existing Test Cases [+] from typing import List [+] [+] def addedInteger(nums1: List[int], nums2: List[int]) -&gt; int: [+] return nums2[0] -nums1[0] Example Program # Test case: Arrays with all elements the same except one assert addedInteger([1, 1, 1, 1, 2], [1, 1, 1, 1, 2]) == 0 # Test case: Arrays with the same elements but different orders assert addedInteger([1, 2, 3], [3, 2, 1]) == 0 # Test case: Arrays with a single element assert addedInteger([5], [10]) == Test case: Arrays with all elements being the maximum value assert addedInteger([1000, 1000, 1000], [1000, 1000, 1000]) == 0 ......</p>
<p>Fig. 9 .
9
Fig.9.Illustrates the pass@1 performance during the evolutionary process.The shaded region in the graph indicates the data distribution, expressed as the standard deviation.The gray dashed line represents the final performance achieved by the CodeT method.In the figures, r represents the iteration round.</p>
<p>Fig. 11 .
11
Fig. 11.Program pass@1 performance during the evolutionary process for different crossover rate schedulers, where r represents the iteration round.</p>
<p>Fig. 12 .
12
Fig.12.Program pass@1 performance during the evolutionary process for different program fitness functions, while r refers to the iteration round.</p>
<p>Fig. 15 .
15
Fig.15.Illustrates the evolutionary process in the ablation study.The variable r refers to the iteration round.The red line shows the performance of CoCoEvo, whereas the green line represents CoCoEvo with the test case evolution module removed.</p>
<p>Co-Evolution Loop Program Evolution Test Case Evolution</p>
<p>An overview of the co-evolution loop.The workflow consists of alternating phases of program evolution and test case evolution.In the program evolution phase, the test case population is used to evaluate the fitness of programs, and the offspring programs are generated through crossover and mutation operators.In the test case evolution phase, the program population is used to evaluate the fitness of test cases, and the offspring test cases are produced by generating additional test cases.Rectangles with an orange background indicate the steps involving the use of LLMs.evolution, each of which includes a Generation-Evaluation-Selection sequence.During program evolution, the test case population is used to evaluate programs and calculate their fitness.Conversely, during test case evolution, the program population is utilized to evaluate test cases.
using template-based fixes and coverage feedback to improveaccuracy.NoReached max_iterYesFinishedIII. METHODSSelectA. Framework OverviewInitialize and Evaluate Program Population and test Case PopulationPrograms for Crossover Select ProgramsEvaluate Programs on Test Case PopulationSelect New Program PopulationAlgorithm 1 Pseudocode of CoCoEvo Require: An LLM , program population size N P , max iter-Evaluate Test Cases on Program Population Select New Generate Additional Test Case Population Test Cases with LLM Rethinkingfor Mutationation rounds max iterEnsure: Program p best with the highest fitnessFig. 2.1: // Initialization 2: Initialize program population P and test case populationT with the LLM ;3: Cross evaluate P and T to obtain F P ;4: Obtain the program with the highest fitness as p best ;5: // Co-evolution loop6: for r = 2 to max iter do7:// Program evolution8:Calculate x r via the cosine scheduler;9: 10: 11:Crossover operations N c P r ← ⌊x r × N P ⌋; Mutation operations N m P r ← N P − N c P r ; Program offspring P ′ ← ∅;12:for i = 1 to N c P r do13:Select programs p 1 , p 2 from P with the binarytournament algorithm;14:Perform crossover on p 1 and p 2 with the LLM toobtain p ′ ;15:P ′ ← P ′ ∪ {p ′ };16:end for17:for i = 1 to N m P r do18:Randomly select program p from P ;19:Perform mutation on p with the LLM to obtain p ′ ;20:P ′ ← P ′ ∪ {p ′ };21:end for22:Evaluate P ′ on T , calculate F P ;23:P ← P ∪ P ′ ;24:Select N P programs from P with the highest fitness;25:Update the program p best with the highest fitness;26:// Test case evolution27:Generate line coverage information f eedback based onT and p best ;28:Generate additional test cases T ′ based on T andf eedback with the LLM ;29:T ← T ∪ T ′ ;30:Cross evaluate P and T to obtain F P , Conf T , andDisc T ;31:Select the new test case population T based on Conf Tand Disc T with the Pareto method;32:Update the program p best with the highest fitness;33: end for34: return p best .An overview of CoCoEvo is presented in Fig. 2. The processbegins by randomly initializing both the program and test-casepopulations, using LLMs to generate the initial programs andenhances LLMtest cases. The co-evolution process follows a loop consistingunit testing by automatically generating and repairing tests,of two alternating steps: program evolution and test case</p>
<p>TABLE I
IILLUSTRATION OF THE DETAILED INFORMATION OF THELEETCODE-CONTEST DATASET.Problems80Average Test Cases644.40weekly-contest-402, weekly-contest-401,biweekly-contest-132, weekly-contest-400,weekly-contest-399, biweekly-contest-131,weekly-contest-398, weekly-contest-397,Selected Contestsbiweekly-contest-130, weekly-contest-396, weekly-contest-395, biweekly-contest-129,weekly-contest-394, weekly-contest-393,biweekly-contest-128, weekly-contest-392,weekly-contest-391, biweekly-contest-127,weekly-contest-390, weekly-contest-389of example test inputs and outputs in test case generation.Specifically, we removed all test input-output samples fromthe problem descriptions, retaining only the natural languageproblem statements, the function header, and the associated
constraints.The detailed information of the LeetCode-Contest dataset is summarized in TableI.</p>
<p>TABLE II ILLUSTRATION
II
OF PASS@1 PERFORMANCE ACROSS ALL METHODS ON THE LEETCODE-CONTEST DATASET.Denotes that the methods were originally designed to operate with pre-defined test cases.For this work, we adapted these methods to utilize test cases generated by the LLMs.
MethodGPT-4o-mini Qwen2.5-Coder-32B Llama-3.1-70B DeepSeek-V3Sampling35.2544.0032.0069.00Sampling+Filtering <em>37.5044.5031.0068.25Self-Repair </em>33.0042.0029.2560.75Reflexion <em>37.5048.7525.0062.50INTERVENOR </em>27.7520.2520.2547.50CodeCoT31.2536.5019.5063.50AgentCoder31.2541.7522.0060.25MBR-Exec33.7545.0038.7570.00CodeT46.2547.5041.2572.50CoCoEvo49.7555.7545.0076.25
*</p>
<p>Confidence represents a weighted pass rate, as described in Section III-C.Similarly, Failure Rate measures the failure rate of a program on the test cases.For these three fitness calculation methods, greedy selection is employed as the offspring selection method.Experimental results indicate that employing the Pareto method as the selection mechanism yields the best perfor-Fig.13.Program pass@1 performance during the evolutionary process for different test case fitness functions, while r refers to the iteration round.Fig.14.Illustrates the test case accuracy during the evolutionary process for different test case fitness functions, while r refers to the iteration round.
Pass@1 Tests Acc (%)25.00 30.00 35.00 40.00 45.00 50.00 20.00 40.00 60.00 80.00TABLE V COMPARISON OF PASS@1 PERFORMANCE ACROSS DIFFERENT TEST CASE FITNESS FUNCTIONS. Model Test Fitness Function Pass@1 GPT-4o-mini Failure Rate 37.50 Pass Rate 47.00 Confidence 45.50 Pareto 49.75 Qwen2.5-Coder-32B Failure Rate 37.00 Pass Rate 51.50 Confidence 51.50 Pareto 55.75 Llama-3.1-70B Failure Rate 26.00 Pass Rate 37.50 Confidence 36.75 Pareto 45.00 DeepSeek-V3 Failure Rate 66.75 Pass Rate 71.75 Confidence 72.25 Pareto 76.25 Pass Rate, Confidence, and Pareto as methods for test fitness calculation. These experiments are performed on the four LLMs and the LeetCode-Contest dataset. The detailed results are depicted in Fig. 13, and additional information on test case accuracy is provided in Fig. 14. Among the test fitness functions, Pass Rate directly cal-culates the proportion of test cases successfully passed by a Llama-3.1-70B DeepSeek-V3 1 4 7 10 r 10.00 20.00 30.00 75.00 40.00 Pass@1 Failure Rate Pass Rate Confidence Pareto 1 4 7 10 r 60.00 65.00 70.00 Pass@1 Failure Rate Pass Rate Confidence Pareto 55.00 Llama-3.1-70B DeepSeek-V3 program, while 1 4 7 10 r GPT-4o-mini Qwen2.5-Coder-32B Failure Rate Pass Rate Confidence Pareto 1 4 7 10 r 30.00 40.00 50.00 Pass@1 Failure Rate Pass Rate Confidence Pareto GPT-4o-mini Qwen2.5-Coder-32B 1 4 7 10 r Pareto Pass Rate Confidence Failure Rate Random 1 4 7 10 r 20.00 40.00 60.00 80.00 Tests Acc (%) Pareto Pass Rate Confidence Failure Rate Random 1 4 7 10 r 1 4 7 r 10 0.00 20.00 40.00 60.00 80.00 Tests Acc (%) Pareto Pass Rate Confidence Failure Rate Random 20.00 40.00 60.00 80.00 Tests Acc (%) Pareto Pass Rate Confidence Failure Rate Random</p>
<p>TABLE VI ILLUSTRATES
VI
THE IMPACT OF TEST CASE COVERAGE INFORMATION ON THE PASS@1 PERFORMANCE OF BASELINE METHODS AND COCOEVO.THE SUFFIX "W/ COV" IN BASELINE METHOD NAMES INDICATES THE INCLUSION OF TEST COVERAGE INFORMATION, WHILE "COCOEVO W/O COV" REPRESENTS COCOEVO WITHOUT TEST COVERAGE INFORMATION.
MethodGPT-4o-mini Qwen2.5-Coder-32B Llama-3.1-70B DeepSeek-V3Sampling+Filtering37.5044.5031.0068.25Sampling+Filtering w/ cov36.2547.7533.2568.50MBR-Exec33.7545.0038.7570.00MBR-Exec w/ cov35.0045.0033.7572.50CodeT46.2547.5041.2572.50CodeT w/ cov47.7550.2539.0071.50CoCoEvo w/o cov48.2553.0042.7576.00CoCoEvo49.7555.7545.0076.25</p>
<p>TABLE VII ILLUSTRATES
VII
THE RESULTS OF THE ABLATION EXPERIMENTS.WHERE "W/O TEST EVOLUTION" REPRESENTS THE METHOD OF REMOVING THE TEST CASE EVOLUTION MODULE OF COCOEVO.
ModelMethodPass@1Sampling35.25GPT-4o-miniw/o test evolution48.00CoCoEvo49.75Sampling44.00Qwen2.5-Coder-32Bw/o test evolution51.75CoCoEvo55.75Sampling32.00Llama-3.1-70Bw/o test evolution43.00CoCoEvo45.00Sampling69.00DeepSeek-V3w/o test evolution74.50CoCoEvo76.25</p>
<p>TABLE VIII ILLUSTRATES
VIII
THE TOKEN CONSUMPTION ACROSS THE APPROACHES.THE AVERAGE NUMBER OF INPUT TOKENS (PROMPT) AND OUTPUT TOKENS (COMPLETION) REQUIRED FOR EACH PROBLEM ON THE LEETCODE-CONTEST DATASET ARE REPORTED.THE "STOP-N" SUFFIX INDICATES THAT IF, FOR n CONSECUTIVE GENERATIONS, THE PROGRAM POPULATION MAINTAINS THE SAME FITNESS AND PASSES THE IDENTICAL SET OF TEST CASES, THE EVOLUTIONARY PROCESS IS TERMINATED.
MethodGPT-4o-miniQwen2.5-Coder-32BLlama-3.1-70BDeepSeek-V3PromptCompletionPromptCompletionPromptCompletionPromptCompletion</p>
<p>TABLE XI ILLUSTRATES
XI
THE COMPARISON OF PASS@1 BETWEEN BASELINES AND COCOEVO ON THE SELECTED REAL-WORLD PROJECT PROGRAMMING PROBLEMS.
MethodPass@1Sampling38.00Sampling+Filtering40.00CodeT40.00CoCoEvo44.00</p>
<p>•</p>
<p>Dependency Extraction: Dependencies must be manually supplied as the model cannot automatically identify external modules.Future work could automate this with dependency analysis, LLM agents, or RAG.• Context Limitations: Numerous dependencies can exceed model token limits, requiring memory management or larger context windows.• Quality of Problem Descriptions: Real-world projects often have unclear descriptions, challenging the LLM's intent-understanding capabilities.VI.CONCLUSION This paper introduces CoCoEvo, an LLM-based framework that simultaneously evolves programs and test cases for automated code generation.By eliminating the reliance on pre-defined test cases, CoCoEvo addresses a critical limitation in existing approaches, particularly in scenarios lacking comprehensive testing resources.The dynamic crossover rate scheduling mechanism and the multi-objective optimization for test case selection are key to the framework's success, enabling effective exploration and refinement throughout the evolution process.Experimental results on the LeetCode-Contest dataset, utilizing four leading LLMs, demonstrate that CoCoEvo achieves superior performance compared to traditional methods.Beyond its practical implications in software development, CoCoEvo opens new avenues for integrating evolutionary algorithms with LLMs.</p>
<p>https://leetcode.com/
https://platform.openai.com/
https://deepinfra.com/
https://platform.deepseek.com/
This work was supported in part by the National Natural Science Foundation of China under Grant 62202023.MethodGPT-4o-mini Qwen2.
TABLE IX ILLUSTRATES THE PASS@1 PERFORMANCE OF THE COCOEVO APPROACH ON THE LEETCODE-CONTEST DATASET UNDER DIFFERENT EARLY STOPPING STRATEGIES. THE "STOP-N" SUFFIX MEANS THAT IF THE POPULATION'S FITNESS AND PASSED TEST SET REMAIN UNCHANGED FOR n CONSECUTIVE GENERATIONS, THE EVOLUTIONARY PROCESS ENDS. </p>
<p>A Hurst, A Lerer, A P Goucher, A Perelman, A Ramesh, A Clark, A Ostrow, A Welihinda, A Hayes, A Radford, arXiv:2410.21276Gpt-4o system card. 2024arXiv preprint</p>
<p>The llama 3 herd of models. A Dubey, A Jauhri, A Pandey, A Kadian, A Al-Dahle, A Letman, A Mathur, A Schelten, A Yang, A Fan, arXiv:2407.217832024arXiv preprint</p>
<p>B Hui, J Yang, Z Cui, J Yang, D Liu, L Zhang, T Liu, J Zhang, B Yu, K Dang, arXiv:2409.12186Qwen2.5-coder technical report. 2024arXiv preprint</p>
<p>A Liu, B Feng, B Xue, B Wang, B Wu, C Lu, C Zhao, C Deng, C Zhang, C Ruan, arXiv:2412.19437Deepseek-v3 technical report. 2024arXiv preprint</p>
<p>Competitionlevel code generation with alphacode. Y Li, D Choi, J Chung, N Kushman, J Schrittwieser, R Leblond, T Eccles, J Keeling, F Gimeno, A Lago, Science. 37866242022</p>
<p>Reflexion: Language agents with verbal reinforcement learning. N Shinn, F Cassano, A Gopinath, K Narasimhan, S Yao, Advances in Neural Information Processing Systems. 202436</p>
<p>Is self-repair a silver bullet for code generation?. T X Olausson, J P Inala, C Wang, J Gao, A Solar-Lezama, The Twelfth International Conference on Learning Representations. 2023</p>
<p>Intervenor: Prompt the coding ability of large language models with the interactive chain of repairing. H Wang, Z Liu, S Wang, G Cui, N Ding, Z Liu, G Yu, arXiv:2311.098682023arXiv preprint</p>
<p>Debug like a human: A large language model debugger via verifying runtime execution step by step. L Zhong, Z Wang, J Shang, 10.18653/v1/2024.findings-acl.49Findings of the Association for Computational Linguistics, ACL 2024. L Ku, A Martins, V Srikumar, Bangkok, ThailandAssociation for Computational LinguisticsAugust 11-16, 2024. 2024and virtual meeting</p>
<p>A pair programming framework for code generation via multi-plan exploration and feedbackdriven refinement. H Zhang, W Cheng, Y Wu, W Hu, Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering. the 39th IEEE/ACM International Conference on Automated Software Engineering2024</p>
<p>Test driven development: By example addison-wesley. K Beck, The Addison-Wesley Signature Series. 2002</p>
<p>Codet: Code generation with generated tests. B Chen, F Zhang, A Nguyen, D Zan, Z Lin, J.-G Lou, W Chen, arXiv:2207.103972022arXiv preprint</p>
<p>Natural language to code translation with execution. F Shi, D Fried, M Ghazvininejad, L Zettlemoyer, S I Wang, arXiv:2204.114542022arXiv preprint</p>
<p>Codecot: Tackling code syntax errors in cot reasoning for code generation. D Huang, Q Bu, Y Qing, H Cui, CoRR. 23082023</p>
<p>Language models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, Advances in neural information processing systems. 202033</p>
<p>Agentcoder: Multi-agent-based code generation with iterative testing and optimisation. D Huang, Q Bu, J M Zhang, M Luck, H Cui, arXiv:2312.130102023arXiv preprint</p>
<p>Large language models as test case generators: Performance evaluation and enhancement. K Li, Y Yuan, arXiv:2404.133402024arXiv preprint</p>
<p>Leveraging code-test co-evolution patterns for automated test case recommendation. S Shimmi, M Rahimi, Proceedings of the 3rd ACM/IEEE International Conference on Automation of Software Test. the 3rd ACM/IEEE International Conference on Automation of Software Test2022</p>
<p>Reaccept: Automated co-evolution of production and test code based on dynamic validation and large language models. J Chi, X Wang, Y Huang, L Yu, D Cui, J Sun, J Sun, arXiv:2411.110332024arXiv preprint</p>
<p>Evolutionary testing for program repair. H Ruan, H L Nguyen, R Shariffdeen, Y Noller, A Roychoudhury, 2024 IEEE Conference on Software Testing, Verification and Validation (ICST). IEEE2024</p>
<p>Coevolving programs and unit tests from their specification. A Arcuri, X Yao, Proceedings of the 22nd IEEE/ACM International Conference on Automated Software Engineering. the 22nd IEEE/ACM International Conference on Automated Software Engineering2007</p>
<p>A fast elitist nondominated sorting genetic algorithm for multi-objective optimization: Nsga-ii. K Deb, S Agrawal, A Pratap, T Meyarivan, Parallel Problem Solving from Nature PPSN VI: 6th International Conference. Paris, FranceSeptember 18-20, 2000 Proceedings 6. Springer, 2000</p>
<p>Self-edit: Fault-aware code editor for code generation. K Zhang, Z Li, J Li, G Li, Z Jin, arXiv:2305.040872023arXiv preprint</p>
<p>Evosuite: automatic test suite generation for object-oriented software. G Fraser, A Arcuri, Proceedings of the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering. the 19th ACM SIGSOFT symposium and the 13th European conference on Foundations of software engineering2011</p>
<p>Using large language models to generate junit tests: An empirical study. M L Siddiq, J C Da Silva Santos, R H Tanvir, N Ulfat, F Al Rifat, V Carvalho Lopes, Proceedings of the 28th International Conference on Evaluation and Assessment in Software Engineering. the 28th International Conference on Evaluation and Assessment in Software Engineering2024</p>
<p>An initial investigation of chatgpt unit test generation capability. V Guilherme, A Vincenzi, Proceedings of the 8th Brazilian Symposium on Systematic and Automated Software Testing. the 8th Brazilian Symposium on Systematic and Automated Software Testing2023</p>
<p>An empirical evaluation of using large language models for automated unit test generation. M Schäfer, S Nadi, A Eghbali, F Tip, IEEE Transactions on Software Engineering. 5012024</p>
<p>Automated unit test improvement using large language models at meta. N Alshahwan, J Chheda, A Finogenova, B Gokkaya, M Harman, I Harper, A Marginean, S Sengupta, E Wang, Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering. 2024</p>
<p>Toga: A neural method for test oracle generation. E Dinella, G Ryan, T Mytkowicz, S K Lahiri, Proceedings of the 44th International Conference on Software Engineering. the 44th International Conference on Software Engineering2022</p>
<p>Togll: Correct and strong test oracle generation with llms. S B Hossain, M Dwyer, arXiv:2405.037862024arXiv preprint</p>
<p>Neural-based test oracle generation: A large-scale evaluation and lessons learned. S B Hossain, A Filieri, M B Dwyer, S Elbaum, W Visser, Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium the Foundations of Software Engineering. the 31st ACM Joint European Software Engineering Conference and Symposium the Foundations of Software Engineering2023</p>
<p>Tdd without tears: Towards test case generation from requirements through deep reinforcement learning. W Takerngsaksiri, R Charakorn, C Tantithamthavorn, Y.-F Li, arXiv:2401.075762024arXiv preprint</p>
<p>How well does llm generate security tests. Y Zhang, W Song, Z Ji, N Meng, arXiv:2310.007102023arXiv preprint</p>
<p>Large language models are fewshot testers: Exploring llm-based general bug reproduction. S Kang, J Yoon, S Yoo, 2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE). IEEE2023</p>
<p>The openelm library: Leveraging progress in language models for novel evolutionary algorithms. H Bradley, H Fan, T Galanos, R Zhou, D Scott, J Lehman, Genetic Programming Theory and Practice XX. Springer2024</p>
<p>Evoprompting: Language models for code-level neural architecture search. A Chen, D Dohan, D So, Advances in neural information processing systems. 202336</p>
<p>Algorithm evolution using large language model. F Liu, X Tong, M Yuan, Q Zhang, arXiv:2311.152492023arXiv preprint</p>
<p>Evolution of heuristics: Towards efficient automatic algorithm design using large language model. F Liu, T Xialiang, M Yuan, X Lin, F Luo, Z Wang, Z Lu, Q Zhang, Forty-first International Conference on Machine Learning. 2024</p>
<p>Eureka: Humanlevel reward design via coding large language models. Y J Ma, W Liang, G Wang, D.-A Huang, O Bastani, D Jayaraman, Y Zhu, L Fan, A Anandkumar, arXiv:2310.129312023arXiv preprint</p>
<p>Language model crossover: Variation through few-shot prompting. E Meyerson, M J Nelson, H Bradley, A Gaier, A Moradi, A K Hoover, J Lehman, 10.1145/3694791ACM Trans. Evol. Learn. Optim. 44Nov. 2024</p>
<p>Mathematical discoveries from program search with large language models. B Romera-Paredes, M Barekatain, A Novikov, M Balog, M P Kumar, E Dupont, F J Ruiz, J S Ellenberg, P Wang, O Fawzi, Nature. 62579952024</p>
<p>Reevo: Large language models as hyper-heuristics with reflective evolution. H Ye, J Wang, Z Cao, F Berto, C Hua, H Kim, J Park, G Song, arXiv:2402.011452024arXiv preprint</p>
<p>Autotest: Evolutionary code solution selection with test cases. Z Duan, J Wang, 2024 IEEE 11th International Conference on Cyber Security and Cloud Computing (CSCloud). IEEE2024</p>
<p>Testart: Improving llm-based unit test via co-evolution of automated generation and repair iteration. S Gu, C Fang, Q Zhang, F Tian, Z Chen, 20242408arXiv e-prints</p>
<p>SGDR: stochastic gradient descent with warm restarts. I Loshchilov, F Hutter, 5th International Conference on Learning Representations, ICLR 2017. Conference Track Proceedings. OpenReview.net. Toulon, FranceApril 24-26, 2017. 2017</p>
<p>Spoc: Search-based pseudocode to code. S Kulal, P Pasupat, K Chandra, M Lee, O Padon, A Aiken, P S Liang, Advances in Neural Information Processing Systems. H Wallach, H Larochelle, A Beygelzimer, F Alché-Buc, E Fox, R Garnett, Curran Associates, Inc201932</p>
<p>Deveval: A manually-annotated code generation benchmark aligned with real-world code repositories. J Li, G Li, Y Zhao, Y Li, H Liu, H Zhu, L Wang, K Liu, Z Fang, L Wang, arXiv:2405.198562024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
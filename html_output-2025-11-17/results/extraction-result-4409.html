<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4409 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4409</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4409</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-100.html">extraction-schema-100</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <p><strong>Paper ID:</strong> paper-271859627</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2408.06574v1.pdf" target="_blank">SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) have shown remarkable achievements across various language tasks. To enhance the performance of LLMs in scientific literature services, we developed the scientific literature LLM (SciLit-LLM) through pre-training and supervised fine-tuning on scientific literature, building upon the iFLYTEK Spark LLM. Furthermore, we present a knowledge service system Spark Research Assistant (SparkRA) based on our SciLit-LLM. SparkRA is accessible online and provides three primary functions: literature investigation, paper reading, and academic writing. As of July 30, 2024, SparkRA has garnered over 50,000 registered users, with a total usage count exceeding 1.3 million.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4409.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4409.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SparkRA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Spark Research Assistant</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A retrieval-augmented, LLM-driven knowledge service system for scientific literature providing literature investigation, paper reading, and academic writing features (including review generation, multi-paper comparison, Q&A, polishing, and translation).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SparkRA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>An end-to-end literature service platform built around a scientific-domain LLM (SciLit-LLM). Key components: (1) Literature investigation subsystem (investigation copilot, topic search engine, review generation), (2) Paper reading subsystem (RAG-based pipeline with text splitting, chapter recognition, cross-lingual retrieval embeddings, and long-context answer generation), and (3) Academic writing subsystem (polishing, translation with dynamic perception prompts). The system uses query rewriting, information extraction (NER) to drive precise retrieval, an embedding-based retriever, and the SciLit-LLM to synthesize retrieved evidence into summaries, reviews, comparative tables, and answers.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SciLit-LLM (13B) — a continually pre-trained and supervised-fine-tuned variant of the iFLYTEK Spark LLM (iFlytekSpark-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Retrieval-augmented extraction: text splitting + chapter recognition to segment papers; cross-lingual embedding-based retrieval trained with (question, positive, negative) pairs; query rewriting using SciLit-LLM; NER and information extraction to populate search plugins; retrieved segments are fed into the SciLit-LLM for question-answering.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Clustering of retrieved documents/segments and inductive summarization by the SciLit-LLM to produce structured reviews (introduction/body/conclusion and headings), multi-document comparison tables, and retrieval-augmented generation (RAG) for answers that combine evidence from multiple sources.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Review generation: up to 30 papers; Multi-document comparison: 2–5 papers; Retrieval and topic-level synthesis draw from SparkRA's large academic library (corpus used in pretraining >10M papers).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature across domains; supports English and Chinese bilingual processing.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Literature reviews (structured), concise paper summaries, comparative analysis tables, question-answer pairs grounded in paper content, polished/translated text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human Mean Opinion Score (MOS) 1–5 on task-specific dimensions (Factuality, Informativeness for paper reading; Fluency, Fidelity, Academic for polishing/translation); BLEU for machine translation evaluation (automatic).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Paper reading (SparkRA): Factuality 4.68, Informativeness 4.45, Avg 4.57 (highest among compared models). Paper polishing (SparkRA): Fluency 4.41, Fidelity 4.45, Academic 4.61, Avg 4.49 (highest). Academic translation (SparkRA): Fluency 4.34, Fidelity 4.91, Academic 4.75, Avg 4.67, BLEU 0.198.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Llama2-7B, Llama2-13B, Llama3-8B, GPT-3.5 (ChatGPT), GPT-4 (Turbo).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>SparkRA (SciLit-LLM 13B) achieved the highest average scores in paper reading (4.57 vs GPT-4 4.55) and paper polishing (4.49 vs GPT-4 4.32). In academic translation human scores GPT-4 scored higher (avg 4.74) than SparkRA (avg 4.67), but SparkRA had the highest fidelity (4.91) and BLEU (0.198) among compared models.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Domain continual pretraining on large scientific corpora combined with supervised fine-tuning and retrieval augmentation yields substantial gains in factuality and informativeness for paper-reading tasks; clustering + inductive summarization enables automatic review generation from up to 30 papers; query rewriting and targeted information extraction improve retrieval precision; a 13B domain-adapted LLM can outperform larger general-purpose models on many literature-centric tasks while being more tractable to train/deploy.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Long full-article contexts require splitting and retrieval (training long-context LLMs is expensive); potential for hallucination remains, though reduced by RAG and higher rejection tendency after domain pretraining; review generation limited to maximum 30 papers in current implementation; cross-language retrieval requires specialized embedding training; computational and engineering costs associated with indexing, retrieval, and upkeep of a large academic library.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>The paper reports improved performance after continual pretraining on a very large scientific corpus (>10M papers) and shows that a 13B SciLit-LLM outperforms smaller or similarly sized open models; no detailed quantitative scaling curve provided, but authors note better rejection behavior and reduced hallucination after domain continued pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4409.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4409.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SciLit-LLM</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Scientific Literature Large Language Model (SciLit-LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A scientific-domain-adapted LLM created by continual pretraining of iFLYTEK Spark-13B on scientific literature and further supervised fine-tuning for instruction-following and literature tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SciLit-LLM (domain-adapted LLM)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructed by continual pretraining the iFLYTEK Spark 13B model on an extensive cleaned corpus of scientific documents (papers, patents; ~>10M papers) using standard autoregressive next-token prediction, then applying supervised fine-tuning (SFT) with instruction-response pairs (self-instruct + human-written examples) tailored to scientific tasks (query rewriting, summarization, Q&A, polishing, translation).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Based on iFLYTEK Spark LLM (iFlytekSpark-13B); final SciLit-LLM at 13B parameters.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Used in pipelines for query rewriting, generating questions from segments for embedding training, and performing retrieval-augmented generation by conditioning on retrieved text segments; not a pure extractor but used to drive extraction via prompts and to synthesize retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Performs inductive summarization, clustering-based review generation, and retrieval-augmented answer generation; trained via SFT to follow instructions and produce expert-like outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used to synthesize sets up to 30 papers for review generation; pretraining data included >10M papers.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature across disciplines; bilingual (Chinese and English) support emphasized.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Summaries, literature reviews, scholarly Q&A, polished text, translations.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Same MOS human evaluations and BLEU for translation as used for SparkRA experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>As the core model behind SparkRA, SciLit-LLM (13B) produced the SparkRA scores: paper reading avg 4.57, paper polishing avg 4.49, translation BLEU 0.198 and fidelity 4.91.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Compared indirectly via SparkRA against Llama2 variants, Llama3-8B, GPT-3.5, GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>The domain-adapted 13B SciLit-LLM demonstrates superior performance on literature-centric tasks compared to Llama3-8B and GPT-3.5 and achieves parity or slight advantage against GPT-4 on specific metrics (e.g., factuality in paper reading 4.68 vs GPT-4 4.67).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Continual pretraining on in-domain scientific text improves factuality and increases the model's tendency to reject out-of-context prompts, reducing hallucinations; combined SFT with expert-crafted instruction-response pairs yields better downstream literature-service performance.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Maintaining general capabilities requires mixing in general corpora during continual pretraining; domain-pretraining alone may bias responses; model size (13B) is a tradeoff between cost and capability; paper does not report extreme-scale size comparisons beyond provided baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Authors note gains from large-scale domain pretraining (>10M papers) and that a 13B domain-adapted model yields strong performance, but no explicit monotonic scaling curve across many sizes is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4409.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4409.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RAG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-Augmented Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A method that augments LLM outputs with evidence retrieved from external textual sources to reduce hallucination and ground responses in factual data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Retrieval-augmented generation for knowledge-intensive nlp tasks</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Retrieval-Augmented Generation (RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Integrates an information retrieval component (embedding-based retriever and search index) with a generative LLM: queries or question-like prompts retrieve relevant passages, which are then provided as conditioning context to the LLM to produce grounded answers. In SparkRA this is applied to paper reading and out-of-paper questions, and used alongside query rewriting and a contrastively trained cross-lingual embedding encoder.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>Applied with SciLit-LLM (13B) in this work; RAG as a paradigm is model-agnostic (originally shown with BART/BERT-style encoders and sequence decoders).</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Embedding-based retrieval of document segments; segmentation and chapter recognition to preserve semantics; retrieval conditioned generation (RAG).</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Combines retrieved evidence at generation time to produce answers; for multi-paper outputs, synthesized via clustering and inductive summarization by the LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Used to retrieve and condition on multiple segments/papers as needed; review generation limited to 30 papers in the SparkRA pipeline.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Applied here to scientific literature (general-science, bilingual).</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Grounded Q&A, summaries, literature-informed answers, review text.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Human MOS for factuality and informativeness; RAG-related references also benchmark retrieval impact in dedicated studies.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>When used in SparkRA, the RAG pipeline contributes to paper reading Factuality 4.68 and Informativeness 4.45 (SparkRA overall). The paper does not isolate numeric ablation for RAG alone.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Generative-only LLMs without retrieval (e.g., vanilla Llama, GPT variants operate without external retrieval in these comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>SparkRA using RAG outperforms baselines on factuality and informativeness; exact ablation numbers isolating RAG are not provided in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>RAG mitigates hallucination and improves groundedness for literature QA and reading tasks; complementary techniques (query rewriting, contrastive retrieval training) further improve retrieval relevance and downstream answers.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Quality of retrieved passages critical; retrieval training and index maintenance are engineering challenges; paper notes the necessity of careful segmentation and chapter recognition to preserve semantics; no fine-grained error analysis isolating retrieval failures provided.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Paper cites related work that explores RAG scaling (benchmarks and data importance), but does not present new systematic scaling curves; authors report effectiveness when used with a domain-pretrained 13B LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4409.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4409.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Query Rewriting</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Query Rewriting for Retrieval-Augmented Pipelines</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A preprocessing step that cleans, normalizes, and reformulates user retrieval queries into concise, retrieval-friendly queries (used to improve search precision in a RAG pipeline).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Query rewriting for retrieval-augmented large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Query Rewriting (in SparkRA topic search engine)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>User queries (which may contain noise or be verbose) are reformulated by the SciLit-LLM into targeted retrieval queries (e.g., transforming conversational phrasing into concise topic keywords). The rewritten query is then used to perform entity extraction (NER for scholars, institutions, dates, domains) and drive the precise retrieval plugins.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SciLit-LLM (13B) used for rewriting; method references Ma et al. (2023) for the technique.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Rewritten queries are processed via NER and information extraction to collect structured retrieval parameters (authors, dates, keywords) for plugin-based search.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Not a synthesis technique per se; improves upstream retrieval quality which enables later synthesis by the SciLit-LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Applies to retrieval across the system's full indexed library; no fixed number.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature queries in English and Chinese.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Rewritten retrieval queries and subsequently improved retrieved paper lists and summaries.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Indirectly evaluated through downstream retrieval quality and SparkRA end-task MOS scores; no standalone numeric metric provided for rewriting in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Authors state query rewriting 'significantly enhance[s] the system's ability to locate the desired literature' but provide no isolated quantitative results.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Raw user queries without rewriting; related work by Ma et al. benchmarks rewriting improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Qualitatively reported to improve retrieval precision; no direct numeric comparison in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Query rewriting by a domain-adapted LLM improves retrieval relevance in literature search pipelines and enables more precise downstream summarization and review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Effectiveness depends on quality of prompt and rewriting heuristics; no quantitative ablation shown in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not explicitly discussed; applies at scale across the indexed library.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4409.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4409.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Cross-lingual Retrieval Embedding</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cross-lingual retrieval embedding model (XLM-RoBERTa fine-tuned via contrastive learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An embedding-based retriever trained for cross-language retrieval: LLM-generated question/segment pairs are used to construct training triplets and XLM-RoBERTa is fine-tuned with contrastive loss to produce cross-lingual dense vectors for retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Cross-lingual retrieval embedding model (SparkRA retriever)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Pipeline: use an LLM to generate question candidates from paper segments, build (question, positive sample, negative samples) triplets; fine-tune XLM-RoBERTa as the encoder using contrastive learning to create cross-lingual embeddings; at runtime use these embeddings to retrieve semantically relevant segments across languages for RAG.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>XLM-RoBERTa (as encoder base) fine-tuned; question-generation for training pairs done with SciLit-LLM (or another LLM); system used in SparkRA.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Contrastive-learning-based dense retrieval leveraging LLM-generated Q/A-style training data; embedding-based nearest-neighbor retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Supports synthesis by supplying relevant multilingual segments to the SciLit-LLM for retrieval-augmented generation and summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Trained on large collection of paper segments (scale not numerically specified); used to retrieve across SparkRA's indexed library.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>Scientific literature, cross-lingual (Chinese/English) retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Retrieved document segments used as conditioning context for Q&A, summarization, and review-generation.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported separately; contributes to downstream MOS metrics for SparkRA tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No isolated numeric retrieval metrics reported in the paper; described as part of the effective pipeline enabling high MOS scores for SparkRA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Not directly compared to other retriever training strategies in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>No direct numeric comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Training the retriever with LLM-generated (question, positive, negative) triplets and contrastive learning supports cross-language retrieval for RAG pipelines, improving the system's ability to answer cross-lingual queries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Paper does not provide retrieval-level ablations; building high-quality triplets depends on LLM-generated question quality; engineering overhead for contrastive training and index construction.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not quantitatively discussed; intended to operate over the large indexed academic library used by SparkRA.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4409.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e4409.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Clustering+InductiveSumm</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Clustering and Inductive Summarization (Review generation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A synthesis approach that clusters a set of papers and uses an LLM to inductively summarize clusters into sections (introduction, body, conclusion) and generate a structured review with headings and citation hyperlinks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Clustering + Inductive Summarization (SparkRA review generator)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>SparkRA clusters dozens of input papers (up to 30) by topical similarity and then prompts the SciLit-LLM to produce a hierarchical review: it organizes content into introduction, thematic bodies with headings, and conclusions, annotates analytical text with hyperlinks serving as citations, and produces an end-of-review reference validation section.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SciLit-LLM (13B) performs the inductive summarization and review writing.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Clustering of document embeddings (retrieved segments/papers) to group related works; extraction of key themes and contributions per cluster via LLM-driven summarization.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Hierarchical/inductive summarization that synthesizes multiple papers into a structured literature review with headings and linked citations.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Up to 30 papers per generated review.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature; user-specified area or author-centric collections.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Structured literature review (multi-section text) with annotated citations and links.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not separately quantified; effectiveness reflected in downstream user-facing system performance and qualitative claims of expedited comprehension.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No isolated numerical metrics for review quality reported; the system-level improvements in SparkRA's MOS indicate effective synthesis capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No direct baseline for automated review generation presented in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not directly quantified; authors claim the approach allowed rapid comprehension and generation of reviews from up to 30 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Combining clustering with inductive summarization by a domain-adapted LLM enables automated construction of coherent, structured reviews and helps in surfacing topical trends and potential future directions; hyperlink annotations aid traceability.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Scale cap of 30 papers per review; no quantitative assessment of citation coverage or fidelity of synthesized claims compared to human-written reviews; potential for omission or misattribution if retrieval/clustering misses relevant works.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Cap imposed (30 papers) in current system; authors do not present scaling experiments for larger sets.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4409.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e4409.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-Doc-Compare</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Document Comparison Module</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A module that compares 2–5 selected papers by extracting abstracts, contributions, and producing a comparative analysis table highlighting proposed approaches, advantages, similarities, and differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Multi-Document Comparison (SparkRA feature)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Given 2–5 user-selected papers, SparkRA extracts structured information (abstracts, contributions) and uses the SciLit-LLM to synthesize a comparison table that summarizes the methods, proposed approaches, claimed advantages, and enumerates similarities and differences across the set.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td>SciLit-LLM (13B) performs the comparison synthesis.</td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Segment extraction from papers (abstract and contribution sections), possibly using NER and section recognition, followed by LLM-based QA to produce comparative fields.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Tabular aggregation and contrastive summarization to highlight per-paper contributions and cross-paper similarities/differences.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>2–5 papers (as specified in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>General scientific literature.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Comparative analysis table and narrative summaries enumerating similarities and differences.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not separately measured; contributes to the overall user-evaluated MOS for paper-reading tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>No separate quantitative metrics; system-level paper-reading scores (Factuality 4.68, Informativeness 4.45) indicate utility.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>No explicit baseline for multi-document comparison provided.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not quantified; presented as a capability that aids users in quickly comparing multiple papers.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Automated extraction plus LLM synthesis enables efficient multi-paper comparisons; useful for surfacing relative contributions and tradeoffs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Limited to small sets of papers (2–5); fidelity of comparisons depends on extraction accuracy and available content in extracted sections.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Designed for small-batch comparisons; not evaluated for scaling beyond 5 papers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4409.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e4409.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems, methods, or approaches that use large language models (LLMs) to extract information from, synthesize, or generate theories from multiple scientific papers.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>AI-for-Science method (Wang2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>AI-based scientific research method (referenced by Wang et al., 2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A referenced approach that automatically extracts useful information from large datasets and uses that information to drive scientific discovery and research processes (mentioned in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Scientific discovery in the age of artificial intelligence</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>AI-based scientific research method (Wang et al., 2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Mentioned in related work as a methodology that leverages AI to extract information from large-scale data and to support automated scientific research and discovery workflows; details are cited from the referenced paper rather than implemented in SparkRA.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_model_used</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>extraction_technique</strong></td>
                            <td>Described broadly as automatic information extraction from data; precise extraction techniques not detailed within this SparkRA paper.</td>
                        </tr>
                        <tr>
                            <td><strong>synthesis_technique</strong></td>
                            <td>Described as using extracted information to conduct research and discovery; specific synthesis mechanisms reside in the cited work.</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_papers</strong></td>
                            <td>Not specified in this paper (referenced as a general AI-for-science approach).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_or_topic</strong></td>
                            <td>AI for scientific discovery across domains.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type</strong></td>
                            <td>Scientific discoveries, automated research outputs (as claimed by referenced work).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_metrics</strong></td>
                            <td>Not reported in this paper (refer to the cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_results</strong></td>
                            <td>Not reported here.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_baseline</strong></td>
                            <td>Mentioned as part of the broader AI-for-science literature; comparisons would be in the original cited paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_vs_baseline</strong></td>
                            <td>Not reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as evidence that AI methods can extract and utilize information from large datasets to accelerate scientific research; used to motivate SparkRA's goals.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>This SparkRA paper does not enumerate limitations of the referenced AI-for-science methods; refer to the cited work for details.</td>
                        </tr>
                        <tr>
                            <td><strong>scaling_behavior</strong></td>
                            <td>Not discussed here; likely discussed in the original reference.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model', 'publication_date_yy_mm': '2024-08'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 2)</em></li>
                <li>Query rewriting for retrieval-augmented large language models <em>(Rating: 2)</em></li>
                <li>Benchmarking large language models in retrieval-augmented generation <em>(Rating: 2)</em></li>
                <li>Improving retrieval-augmented large language models via data importance learning <em>(Rating: 2)</em></li>
                <li>Galactica: A large language model for science <em>(Rating: 2)</em></li>
                <li>Scibert: A pretrained language model for scientific text <em>(Rating: 1)</em></li>
                <li>Scientific discovery in the age of artificial intelligence <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4409",
    "paper_id": "paper-271859627",
    "extraction_schema_id": "extraction-schema-100",
    "extracted_data": [
        {
            "name_short": "SparkRA",
            "name_full": "Spark Research Assistant",
            "brief_description": "A retrieval-augmented, LLM-driven knowledge service system for scientific literature providing literature investigation, paper reading, and academic writing features (including review generation, multi-paper comparison, Q&A, polishing, and translation).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SparkRA",
            "system_description": "An end-to-end literature service platform built around a scientific-domain LLM (SciLit-LLM). Key components: (1) Literature investigation subsystem (investigation copilot, topic search engine, review generation), (2) Paper reading subsystem (RAG-based pipeline with text splitting, chapter recognition, cross-lingual retrieval embeddings, and long-context answer generation), and (3) Academic writing subsystem (polishing, translation with dynamic perception prompts). The system uses query rewriting, information extraction (NER) to drive precise retrieval, an embedding-based retriever, and the SciLit-LLM to synthesize retrieved evidence into summaries, reviews, comparative tables, and answers.",
            "llm_model_used": "SciLit-LLM (13B) — a continually pre-trained and supervised-fine-tuned variant of the iFLYTEK Spark LLM (iFlytekSpark-13B)",
            "extraction_technique": "Retrieval-augmented extraction: text splitting + chapter recognition to segment papers; cross-lingual embedding-based retrieval trained with (question, positive, negative) pairs; query rewriting using SciLit-LLM; NER and information extraction to populate search plugins; retrieved segments are fed into the SciLit-LLM for question-answering.",
            "synthesis_technique": "Clustering of retrieved documents/segments and inductive summarization by the SciLit-LLM to produce structured reviews (introduction/body/conclusion and headings), multi-document comparison tables, and retrieval-augmented generation (RAG) for answers that combine evidence from multiple sources.",
            "number_of_papers": "Review generation: up to 30 papers; Multi-document comparison: 2–5 papers; Retrieval and topic-level synthesis draw from SparkRA's large academic library (corpus used in pretraining &gt;10M papers).",
            "domain_or_topic": "General scientific literature across domains; supports English and Chinese bilingual processing.",
            "output_type": "Literature reviews (structured), concise paper summaries, comparative analysis tables, question-answer pairs grounded in paper content, polished/translated text.",
            "evaluation_metrics": "Human Mean Opinion Score (MOS) 1–5 on task-specific dimensions (Factuality, Informativeness for paper reading; Fluency, Fidelity, Academic for polishing/translation); BLEU for machine translation evaluation (automatic).",
            "performance_results": "Paper reading (SparkRA): Factuality 4.68, Informativeness 4.45, Avg 4.57 (highest among compared models). Paper polishing (SparkRA): Fluency 4.41, Fidelity 4.45, Academic 4.61, Avg 4.49 (highest). Academic translation (SparkRA): Fluency 4.34, Fidelity 4.91, Academic 4.75, Avg 4.67, BLEU 0.198.",
            "comparison_baseline": "Llama2-7B, Llama2-13B, Llama3-8B, GPT-3.5 (ChatGPT), GPT-4 (Turbo).",
            "performance_vs_baseline": "SparkRA (SciLit-LLM 13B) achieved the highest average scores in paper reading (4.57 vs GPT-4 4.55) and paper polishing (4.49 vs GPT-4 4.32). In academic translation human scores GPT-4 scored higher (avg 4.74) than SparkRA (avg 4.67), but SparkRA had the highest fidelity (4.91) and BLEU (0.198) among compared models.",
            "key_findings": "Domain continual pretraining on large scientific corpora combined with supervised fine-tuning and retrieval augmentation yields substantial gains in factuality and informativeness for paper-reading tasks; clustering + inductive summarization enables automatic review generation from up to 30 papers; query rewriting and targeted information extraction improve retrieval precision; a 13B domain-adapted LLM can outperform larger general-purpose models on many literature-centric tasks while being more tractable to train/deploy.",
            "limitations_challenges": "Long full-article contexts require splitting and retrieval (training long-context LLMs is expensive); potential for hallucination remains, though reduced by RAG and higher rejection tendency after domain pretraining; review generation limited to maximum 30 papers in current implementation; cross-language retrieval requires specialized embedding training; computational and engineering costs associated with indexing, retrieval, and upkeep of a large academic library.",
            "scaling_behavior": "The paper reports improved performance after continual pretraining on a very large scientific corpus (&gt;10M papers) and shows that a 13B SciLit-LLM outperforms smaller or similarly sized open models; no detailed quantitative scaling curve provided, but authors note better rejection behavior and reduced hallucination after domain continued pretraining.",
            "uuid": "e4409.0",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "SciLit-LLM",
            "name_full": "Scientific Literature Large Language Model (SciLit-LLM)",
            "brief_description": "A scientific-domain-adapted LLM created by continual pretraining of iFLYTEK Spark-13B on scientific literature and further supervised fine-tuning for instruction-following and literature tasks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SciLit-LLM (domain-adapted LLM)",
            "system_description": "Constructed by continual pretraining the iFLYTEK Spark 13B model on an extensive cleaned corpus of scientific documents (papers, patents; ~&gt;10M papers) using standard autoregressive next-token prediction, then applying supervised fine-tuning (SFT) with instruction-response pairs (self-instruct + human-written examples) tailored to scientific tasks (query rewriting, summarization, Q&A, polishing, translation).",
            "llm_model_used": "Based on iFLYTEK Spark LLM (iFlytekSpark-13B); final SciLit-LLM at 13B parameters.",
            "extraction_technique": "Used in pipelines for query rewriting, generating questions from segments for embedding training, and performing retrieval-augmented generation by conditioning on retrieved text segments; not a pure extractor but used to drive extraction via prompts and to synthesize retrieved evidence.",
            "synthesis_technique": "Performs inductive summarization, clustering-based review generation, and retrieval-augmented answer generation; trained via SFT to follow instructions and produce expert-like outputs.",
            "number_of_papers": "Used to synthesize sets up to 30 papers for review generation; pretraining data included &gt;10M papers.",
            "domain_or_topic": "Scientific literature across disciplines; bilingual (Chinese and English) support emphasized.",
            "output_type": "Summaries, literature reviews, scholarly Q&A, polished text, translations.",
            "evaluation_metrics": "Same MOS human evaluations and BLEU for translation as used for SparkRA experiments.",
            "performance_results": "As the core model behind SparkRA, SciLit-LLM (13B) produced the SparkRA scores: paper reading avg 4.57, paper polishing avg 4.49, translation BLEU 0.198 and fidelity 4.91.",
            "comparison_baseline": "Compared indirectly via SparkRA against Llama2 variants, Llama3-8B, GPT-3.5, GPT-4.",
            "performance_vs_baseline": "The domain-adapted 13B SciLit-LLM demonstrates superior performance on literature-centric tasks compared to Llama3-8B and GPT-3.5 and achieves parity or slight advantage against GPT-4 on specific metrics (e.g., factuality in paper reading 4.68 vs GPT-4 4.67).",
            "key_findings": "Continual pretraining on in-domain scientific text improves factuality and increases the model's tendency to reject out-of-context prompts, reducing hallucinations; combined SFT with expert-crafted instruction-response pairs yields better downstream literature-service performance.",
            "limitations_challenges": "Maintaining general capabilities requires mixing in general corpora during continual pretraining; domain-pretraining alone may bias responses; model size (13B) is a tradeoff between cost and capability; paper does not report extreme-scale size comparisons beyond provided baselines.",
            "scaling_behavior": "Authors note gains from large-scale domain pretraining (&gt;10M papers) and that a 13B domain-adapted model yields strong performance, but no explicit monotonic scaling curve across many sizes is provided.",
            "uuid": "e4409.1",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "RAG",
            "name_full": "Retrieval-Augmented Generation",
            "brief_description": "A method that augments LLM outputs with evidence retrieved from external textual sources to reduce hallucination and ground responses in factual data.",
            "citation_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "mention_or_use": "use",
            "system_name": "Retrieval-Augmented Generation (RAG)",
            "system_description": "Integrates an information retrieval component (embedding-based retriever and search index) with a generative LLM: queries or question-like prompts retrieve relevant passages, which are then provided as conditioning context to the LLM to produce grounded answers. In SparkRA this is applied to paper reading and out-of-paper questions, and used alongside query rewriting and a contrastively trained cross-lingual embedding encoder.",
            "llm_model_used": "Applied with SciLit-LLM (13B) in this work; RAG as a paradigm is model-agnostic (originally shown with BART/BERT-style encoders and sequence decoders).",
            "extraction_technique": "Embedding-based retrieval of document segments; segmentation and chapter recognition to preserve semantics; retrieval conditioned generation (RAG).",
            "synthesis_technique": "Combines retrieved evidence at generation time to produce answers; for multi-paper outputs, synthesized via clustering and inductive summarization by the LLM.",
            "number_of_papers": "Used to retrieve and condition on multiple segments/papers as needed; review generation limited to 30 papers in the SparkRA pipeline.",
            "domain_or_topic": "Applied here to scientific literature (general-science, bilingual).",
            "output_type": "Grounded Q&A, summaries, literature-informed answers, review text.",
            "evaluation_metrics": "Human MOS for factuality and informativeness; RAG-related references also benchmark retrieval impact in dedicated studies.",
            "performance_results": "When used in SparkRA, the RAG pipeline contributes to paper reading Factuality 4.68 and Informativeness 4.45 (SparkRA overall). The paper does not isolate numeric ablation for RAG alone.",
            "comparison_baseline": "Generative-only LLMs without retrieval (e.g., vanilla Llama, GPT variants operate without external retrieval in these comparisons).",
            "performance_vs_baseline": "SparkRA using RAG outperforms baselines on factuality and informativeness; exact ablation numbers isolating RAG are not provided in the paper.",
            "key_findings": "RAG mitigates hallucination and improves groundedness for literature QA and reading tasks; complementary techniques (query rewriting, contrastive retrieval training) further improve retrieval relevance and downstream answers.",
            "limitations_challenges": "Quality of retrieved passages critical; retrieval training and index maintenance are engineering challenges; paper notes the necessity of careful segmentation and chapter recognition to preserve semantics; no fine-grained error analysis isolating retrieval failures provided.",
            "scaling_behavior": "Paper cites related work that explores RAG scaling (benchmarks and data importance), but does not present new systematic scaling curves; authors report effectiveness when used with a domain-pretrained 13B LLM.",
            "uuid": "e4409.2",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Query Rewriting",
            "name_full": "Query Rewriting for Retrieval-Augmented Pipelines",
            "brief_description": "A preprocessing step that cleans, normalizes, and reformulates user retrieval queries into concise, retrieval-friendly queries (used to improve search precision in a RAG pipeline).",
            "citation_title": "Query rewriting for retrieval-augmented large language models",
            "mention_or_use": "use",
            "system_name": "Query Rewriting (in SparkRA topic search engine)",
            "system_description": "User queries (which may contain noise or be verbose) are reformulated by the SciLit-LLM into targeted retrieval queries (e.g., transforming conversational phrasing into concise topic keywords). The rewritten query is then used to perform entity extraction (NER for scholars, institutions, dates, domains) and drive the precise retrieval plugins.",
            "llm_model_used": "SciLit-LLM (13B) used for rewriting; method references Ma et al. (2023) for the technique.",
            "extraction_technique": "Rewritten queries are processed via NER and information extraction to collect structured retrieval parameters (authors, dates, keywords) for plugin-based search.",
            "synthesis_technique": "Not a synthesis technique per se; improves upstream retrieval quality which enables later synthesis by the SciLit-LLM.",
            "number_of_papers": "Applies to retrieval across the system's full indexed library; no fixed number.",
            "domain_or_topic": "General scientific literature queries in English and Chinese.",
            "output_type": "Rewritten retrieval queries and subsequently improved retrieved paper lists and summaries.",
            "evaluation_metrics": "Indirectly evaluated through downstream retrieval quality and SparkRA end-task MOS scores; no standalone numeric metric provided for rewriting in this paper.",
            "performance_results": "Authors state query rewriting 'significantly enhance[s] the system's ability to locate the desired literature' but provide no isolated quantitative results.",
            "comparison_baseline": "Raw user queries without rewriting; related work by Ma et al. benchmarks rewriting improvements.",
            "performance_vs_baseline": "Qualitatively reported to improve retrieval precision; no direct numeric comparison in the paper.",
            "key_findings": "Query rewriting by a domain-adapted LLM improves retrieval relevance in literature search pipelines and enables more precise downstream summarization and review generation.",
            "limitations_challenges": "Effectiveness depends on quality of prompt and rewriting heuristics; no quantitative ablation shown in this paper.",
            "scaling_behavior": "Not explicitly discussed; applies at scale across the indexed library.",
            "uuid": "e4409.3",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Cross-lingual Retrieval Embedding",
            "name_full": "Cross-lingual retrieval embedding model (XLM-RoBERTa fine-tuned via contrastive learning)",
            "brief_description": "An embedding-based retriever trained for cross-language retrieval: LLM-generated question/segment pairs are used to construct training triplets and XLM-RoBERTa is fine-tuned with contrastive loss to produce cross-lingual dense vectors for retrieval.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Cross-lingual retrieval embedding model (SparkRA retriever)",
            "system_description": "Pipeline: use an LLM to generate question candidates from paper segments, build (question, positive sample, negative samples) triplets; fine-tune XLM-RoBERTa as the encoder using contrastive learning to create cross-lingual embeddings; at runtime use these embeddings to retrieve semantically relevant segments across languages for RAG.",
            "llm_model_used": "XLM-RoBERTa (as encoder base) fine-tuned; question-generation for training pairs done with SciLit-LLM (or another LLM); system used in SparkRA.",
            "extraction_technique": "Contrastive-learning-based dense retrieval leveraging LLM-generated Q/A-style training data; embedding-based nearest-neighbor retrieval.",
            "synthesis_technique": "Supports synthesis by supplying relevant multilingual segments to the SciLit-LLM for retrieval-augmented generation and summarization.",
            "number_of_papers": "Trained on large collection of paper segments (scale not numerically specified); used to retrieve across SparkRA's indexed library.",
            "domain_or_topic": "Scientific literature, cross-lingual (Chinese/English) retrieval.",
            "output_type": "Retrieved document segments used as conditioning context for Q&A, summarization, and review-generation.",
            "evaluation_metrics": "Not reported separately; contributes to downstream MOS metrics for SparkRA tasks.",
            "performance_results": "No isolated numeric retrieval metrics reported in the paper; described as part of the effective pipeline enabling high MOS scores for SparkRA.",
            "comparison_baseline": "Not directly compared to other retriever training strategies in this paper.",
            "performance_vs_baseline": "No direct numeric comparison provided.",
            "key_findings": "Training the retriever with LLM-generated (question, positive, negative) triplets and contrastive learning supports cross-language retrieval for RAG pipelines, improving the system's ability to answer cross-lingual queries.",
            "limitations_challenges": "Paper does not provide retrieval-level ablations; building high-quality triplets depends on LLM-generated question quality; engineering overhead for contrastive training and index construction.",
            "scaling_behavior": "Not quantitatively discussed; intended to operate over the large indexed academic library used by SparkRA.",
            "uuid": "e4409.4",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Clustering+InductiveSumm",
            "name_full": "Clustering and Inductive Summarization (Review generation)",
            "brief_description": "A synthesis approach that clusters a set of papers and uses an LLM to inductively summarize clusters into sections (introduction, body, conclusion) and generate a structured review with headings and citation hyperlinks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Clustering + Inductive Summarization (SparkRA review generator)",
            "system_description": "SparkRA clusters dozens of input papers (up to 30) by topical similarity and then prompts the SciLit-LLM to produce a hierarchical review: it organizes content into introduction, thematic bodies with headings, and conclusions, annotates analytical text with hyperlinks serving as citations, and produces an end-of-review reference validation section.",
            "llm_model_used": "SciLit-LLM (13B) performs the inductive summarization and review writing.",
            "extraction_technique": "Clustering of document embeddings (retrieved segments/papers) to group related works; extraction of key themes and contributions per cluster via LLM-driven summarization.",
            "synthesis_technique": "Hierarchical/inductive summarization that synthesizes multiple papers into a structured literature review with headings and linked citations.",
            "number_of_papers": "Up to 30 papers per generated review.",
            "domain_or_topic": "General scientific literature; user-specified area or author-centric collections.",
            "output_type": "Structured literature review (multi-section text) with annotated citations and links.",
            "evaluation_metrics": "Not separately quantified; effectiveness reflected in downstream user-facing system performance and qualitative claims of expedited comprehension.",
            "performance_results": "No isolated numerical metrics for review quality reported; the system-level improvements in SparkRA's MOS indicate effective synthesis capabilities.",
            "comparison_baseline": "No direct baseline for automated review generation presented in the paper.",
            "performance_vs_baseline": "Not directly quantified; authors claim the approach allowed rapid comprehension and generation of reviews from up to 30 papers.",
            "key_findings": "Combining clustering with inductive summarization by a domain-adapted LLM enables automated construction of coherent, structured reviews and helps in surfacing topical trends and potential future directions; hyperlink annotations aid traceability.",
            "limitations_challenges": "Scale cap of 30 papers per review; no quantitative assessment of citation coverage or fidelity of synthesized claims compared to human-written reviews; potential for omission or misattribution if retrieval/clustering misses relevant works.",
            "scaling_behavior": "Cap imposed (30 papers) in current system; authors do not present scaling experiments for larger sets.",
            "uuid": "e4409.5",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "Multi-Doc-Compare",
            "name_full": "Multi-Document Comparison Module",
            "brief_description": "A module that compares 2–5 selected papers by extracting abstracts, contributions, and producing a comparative analysis table highlighting proposed approaches, advantages, similarities, and differences.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Multi-Document Comparison (SparkRA feature)",
            "system_description": "Given 2–5 user-selected papers, SparkRA extracts structured information (abstracts, contributions) and uses the SciLit-LLM to synthesize a comparison table that summarizes the methods, proposed approaches, claimed advantages, and enumerates similarities and differences across the set.",
            "llm_model_used": "SciLit-LLM (13B) performs the comparison synthesis.",
            "extraction_technique": "Segment extraction from papers (abstract and contribution sections), possibly using NER and section recognition, followed by LLM-based QA to produce comparative fields.",
            "synthesis_technique": "Tabular aggregation and contrastive summarization to highlight per-paper contributions and cross-paper similarities/differences.",
            "number_of_papers": "2–5 papers (as specified in the paper).",
            "domain_or_topic": "General scientific literature.",
            "output_type": "Comparative analysis table and narrative summaries enumerating similarities and differences.",
            "evaluation_metrics": "Not separately measured; contributes to the overall user-evaluated MOS for paper-reading tasks.",
            "performance_results": "No separate quantitative metrics; system-level paper-reading scores (Factuality 4.68, Informativeness 4.45) indicate utility.",
            "comparison_baseline": "No explicit baseline for multi-document comparison provided.",
            "performance_vs_baseline": "Not quantified; presented as a capability that aids users in quickly comparing multiple papers.",
            "key_findings": "Automated extraction plus LLM synthesis enables efficient multi-paper comparisons; useful for surfacing relative contributions and tradeoffs.",
            "limitations_challenges": "Limited to small sets of papers (2–5); fidelity of comparisons depends on extraction accuracy and available content in extracted sections.",
            "scaling_behavior": "Designed for small-batch comparisons; not evaluated for scaling beyond 5 papers.",
            "uuid": "e4409.6",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        },
        {
            "name_short": "AI-for-Science method (Wang2023a)",
            "name_full": "AI-based scientific research method (referenced by Wang et al., 2023a)",
            "brief_description": "A referenced approach that automatically extracts useful information from large datasets and uses that information to drive scientific discovery and research processes (mentioned in related work).",
            "citation_title": "Scientific discovery in the age of artificial intelligence",
            "mention_or_use": "mention",
            "system_name": "AI-based scientific research method (Wang et al., 2023a)",
            "system_description": "Mentioned in related work as a methodology that leverages AI to extract information from large-scale data and to support automated scientific research and discovery workflows; details are cited from the referenced paper rather than implemented in SparkRA.",
            "llm_model_used": "",
            "extraction_technique": "Described broadly as automatic information extraction from data; precise extraction techniques not detailed within this SparkRA paper.",
            "synthesis_technique": "Described as using extracted information to conduct research and discovery; specific synthesis mechanisms reside in the cited work.",
            "number_of_papers": "Not specified in this paper (referenced as a general AI-for-science approach).",
            "domain_or_topic": "AI for scientific discovery across domains.",
            "output_type": "Scientific discoveries, automated research outputs (as claimed by referenced work).",
            "evaluation_metrics": "Not reported in this paper (refer to the cited work).",
            "performance_results": "Not reported here.",
            "comparison_baseline": "Mentioned as part of the broader AI-for-science literature; comparisons would be in the original cited paper.",
            "performance_vs_baseline": "Not reported in this paper.",
            "key_findings": "Cited as evidence that AI methods can extract and utilize information from large datasets to accelerate scientific research; used to motivate SparkRA's goals.",
            "limitations_challenges": "This SparkRA paper does not enumerate limitations of the referenced AI-for-science methods; refer to the cited work for details.",
            "scaling_behavior": "Not discussed here; likely discussed in the original reference.",
            "uuid": "e4409.7",
            "source_info": {
                "paper_title": "SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model",
                "publication_date_yy_mm": "2024-08"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Query rewriting for retrieval-augmented large language models",
            "rating": 2,
            "sanitized_title": "query_rewriting_for_retrievalaugmented_large_language_models"
        },
        {
            "paper_title": "Benchmarking large language models in retrieval-augmented generation",
            "rating": 2,
            "sanitized_title": "benchmarking_large_language_models_in_retrievalaugmented_generation"
        },
        {
            "paper_title": "Improving retrieval-augmented large language models via data importance learning",
            "rating": 2,
            "sanitized_title": "improving_retrievalaugmented_large_language_models_via_data_importance_learning"
        },
        {
            "paper_title": "Galactica: A large language model for science",
            "rating": 2,
            "sanitized_title": "galactica_a_large_language_model_for_science"
        },
        {
            "paper_title": "Scibert: A pretrained language model for scientific text",
            "rating": 1,
            "sanitized_title": "scibert_a_pretrained_language_model_for_scientific_text"
        },
        {
            "paper_title": "Scientific discovery in the age of artificial intelligence",
            "rating": 1,
            "sanitized_title": "scientific_discovery_in_the_age_of_artificial_intelligence"
        }
    ],
    "cost": 0.018784,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model</p>
<p>Dayong Wu 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Jiaqi Li 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>University of Science and Technology of China
China</p>
<p>Baoxin Wang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Harbin Institute of Technology
China</p>
<p>Honghong Zhao 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Siyuan Xue 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Yanjie Yang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Zhijun Chang 
National Science Library
Chinese Academy of Sciences
China</p>
<p>Rui Zhang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Li Qian 
National Science Library
Chinese Academy of Sciences
China</p>
<p>Bo Wang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Shijin Wang 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>Zhixiong Zhang 
National Science Library
Chinese Academy of Sciences
China</p>
<p>Guoping Hu 
iFLYTEK Research
State Key Laboratory of Cognitive Intelligence
China</p>
<p>SparkRA: A Retrieval-Augmented Knowledge Service System Based on Spark Large Language Model
241B7D3B9B5DA10EF36BEDE25F9E6A8D
Large language models (LLMs) have shown remarkable achievements across various language tasks.To enhance the performance of LLMs in scientific literature services, we developed the scientific literature LLM (SciLit-LLM) through pre-training and supervised finetuning on scientific literature, building upon the iFLYTEK Spark LLM.Furthermore, we present a knowledge service system Spark Research Assistant (SparkRA) based on our SciLit-LLM.SparkRA is accessible online 1 and provides three primary functions: literature investigation, paper reading, and academic writing.As of July 30, 2024, SparkRA has garnered over 50,000 registered users, with a total usage count exceeding 1.3 million.</p>
<p>Introduction</p>
<p>Large language models (LLMs) have achieved significant success in natural language processing, including text generation and language understanding (Brown et al., 2020;Chowdhery et al., 2023).Owing to their strong capabilities, LLMs have shown immense potential across many downstream fields, such as education, medicine, and finance (Kasneci et al., 2023;Thirunavukarasu et al., 2023;Clusmann et al., 2023;Shah et al., 2023).</p>
<p>As the performance of LLMs in scientific literature does not fully meet the needs of scholars, we developed a Scientific Literature LLM (SciLit-LLM).We began by collecting a large dataset of scientific literature, including academic papers and patents, and performed data cleaning to ensure high-quality academic text.We then continued pre-training the open-source iFLYTEK Spark LLM (13B) 2 using an autoregressive training task, followed by supervised fine-tuning, to create our SciLit-LLM.Traditional knowledge service systems generally provide limited functionalities, such as the retrieval of scholarly articles and assistive reading services.In this paper, we introduce the Spark Research Assistant (SparkRA), a knowledge service system based on our scientific literature LLM.SparkRA offers a comprehensive, one-stop solution for scientific literature services.Figure 1 depicts the process of constructing the SparkRA system.The features of SparkRA are as follows:</p>
<p>• Literature investigation: this sub-system can automatically analyze and summarize research areas, and generate research reviews.</p>
<p>• Paper reading: this sub-system can intelligently interpret papers and quickly answer questions.</p>
<p>• Academic writing: this sub-system can provide the functions for writing academic papers including one-click translation, polishing, and automatic error detection.</p>
<p>Experimental evaluation demonstrates that SparkRA outperforms existing models, including GPT-3.5 and Llama3-8B, across all tasks, establishing its efficacy in enhancing the productivity and accuracy of academic research activities.</p>
<p>arXiv:2408.06574v1 [cs.CL] 13 Aug 2024</p>
<p>2 Scientific Literature LLM</p>
<p>Base model</p>
<p>To build the LLM for scientific literature services, we selected the Spark LLM as the foundation model for building our scientific literature LLM (ScLlit-LLM).The Spark LLM, developed by iFLY-TEK Research, demonstrates impressive performance in processing both English and Chinese languages.iFlytekSpark-13B has consistently ranked among the top in numerous well-known public benchmarks, demonstrating its superiority.Its performance is notably superior to other open-source models of equivalent size.</p>
<p>Continual pre-training</p>
<p>While the Spark LLM exhibits strong capabilities in language comprehension and text generation, it may struggle to directly provide accurate responses to scholarly inquiries without targeted training in the scientific domain.Consequently, we have designed a Scientific literature LLM that is specifically oriented towards parsing and understanding scientific literature.</p>
<p>Inspired by the existing research (Beltagy et al., 2019;Hong et al., 2022), we have further pretrained the spark model on an extensive corpus of academic texts to enhance the model's performance in processing and generating scientific literature Data preparation.To enhance the foundational large language model (LLM), it is imperative to amass a vast corpus of high-quality data, which includes kinds of scholarly literature like papers and patents.We collected a vast number of academic papers from various publicly accessible websites, such as arXiv 3 .</p>
<p>Given that academic documents are predominantly archived in PDF format, it is crucial to convert these PDFs into text while meticulously eliminating any extraneous elements.For this purpose, we employed a sophisticated PDF parsing tool developed by iFLYTEK.In the process of advancing our scientific literature LLM, we have incorporated a dataset comprising over 10M academic papers.</p>
<p>To prevent LLM from losing its general capabilities, we also incorporated a significant amount of general corpora.This strategy ensures that after continual pre-training, the scientific literature LLM performs better in the field of science while maintaining the general capabilities.</p>
<p>3 https://arxiv.org/Pre-training.Similar to the traditional LLM pretraining process, the scientific literature LLM employs the same next-word prediction task for its continual pre-training on a corpus of scientific literature comprising billions of tokens.</p>
<p>Upon evaluation, the scientific literature LLM, continual pre-training, exhibits improved performance on general scholarly inquiries.Moreover, for specialized academic queries without provided context, the scientific literature LLM demonstrates a higher rejection tendency, effectively reducing instances of hallucination.</p>
<p>Supervised fine-tuning</p>
<p>Supervised fine-tuning (SFT) is a technique used to enhance large language models (LLMs) by further training a pre-trained model to improve its accuracy and relevance for specific tasks or domains.The efficacy of SFT in refining LLMs is well-documented (Wei et al., 2022;Ouyang et al., 2022).This process involves utilizing a carefully curated dataset with labeled examples that illustrate the desired output.During SFT, the model learns from these examples to comprehend the intricacies of the task more thoroughly.Consequently, SFT enables the model to retain its broad knowledge base while acquiring specialization in targeted areas, resulting in enhanced user experiences and more precise information delivery.</p>
<p>Data preparation.In the construction of our datasets for supervised fine-tuning, each instance within datasets is composed of three elements: an instruction, an input, and an output.We utilize a dual approach in formulating instructions, leveraging both Self-instruct (Wang et al., 2023b) and human writing.</p>
<p>To exemplify, consider the instruction: "Please translate the input English sentence into Chinese"; here, the input component would be an English sentence.For the generation of outputs corresponding to given instructions and inputs, we employ meticulously devised manual methods to craft expert responses.</p>
<p>Training.Upon completing the construction of SFT datasets, we commenced the Supervised Fine-Tuning (SFT) of scientific literature LLM.The instances within the dataset serve as labeled data for the SFT of the model.Since each instance is meticulously crafted by experts, they are of higher quality compared to the generic data used during the 3 SparkRA</p>
<p>Based on our SciLit-LLM, we developed a literature services system SparkRA.This platform is comprised of three functions: literature investigation, paper reading, and academic writing.Notably, SparkRA is equipped to process inputs in both Chinese and English, thereby catering to a diverse linguistic user base.The architecture of SparkRA is shown in Figure 2 and the demonstration video has been published on YouTube 4 .</p>
<p>Literature investigation</p>
<p>This function is designed to facilitate the exploration of academic literature and is comprised of three integral components: an investigation copilot, a research topic search engine, and a review generation module.The architecture and screenshot of the literature investigation function are respectively shown in Figure 3 and Figure 4.</p>
<p>Investigation copilot.This copilot assists users in deepening their understanding of specific research domains and various scholars through interactive natural language dialogue.(1) Area-based survey.Users can easily obtain the summarization and papers of a specific research area.For example, the user can send the query "What are the recent papers of fake news section in 2023".SparkRA will show the papers and give a summary.</p>
<p>(2) Scholar-based survey.This function can output the papers of the input scholar and divide the papers into different research areas.For example, the user can send the query "What research has Chris Manning from Stanford University conducted".</p>
<p>Topic search engine.The search interface accommodates queries pertaining to research topics in both Chinese and English.Upon receiving a specified topic, SparkRA retrieves relevant papers from an extensive academic library and provides concise summaries of their content.</p>
<p>(1) Query rewriting.There is a diversity of user retrieval query formats and the occasional inclusion of noise, such as "In the library, what LLM technologies can assist users in improving the efficiency of finding books?".Upon receiving a user's query, scientific literature LLM is used to revise the query into a format more suited for retrieval, like "Applications of large models in library search domain".This strategy can significantly enhance the system's ability to locate the desired literature.</p>
<p>(2) Precise Retrieval.Upon completion of the rewriting process, the revised query is subjected to information extraction through natural language understanding technologies, such as Named Entity Recognition (NER).The extracted information encompasses scholars, institutions, dates, domains, and keywords, among others.Based on the extracted content, the corresponding search plugin interfaces are invoked to obtain precise search results.</p>
<p>(3) Literature-based summary.Building on the retrieval outcomes, the scientific literature LLM synthesizes findings, encompassing the distribution of publication years, trends in literature popularity, recent focal topics, and potential future directions of development.</p>
<p>Review generation.This function enables the generation of a report based on a selection of papers, with a maximum limit of 30 papers.The generated report facilitates an expedited comprehension of a substantial volume of literature within a specific domain or authored by an individual.</p>
<p>In this function, we leveraged the clustering capabilities and inductive summarization prowess of LLM.Through the clustering of dozens of literature papers, the model structured the introduction, body, and conclusion of a comprehensive review, including the formulation of pertinent headings.Subsequently, the model demonstrated its robust capacity for inductive reasoning and summarization.It also featured the capability to annotate the analytical text with hyperlinks, serving as citations that facilitate reference validation at the end of the review and enable user verification.</p>
<p>Paper reading</p>
<p>This function can assist scholars and students in reading academic papers.With the rapid development of artificial intelligence technology, a large number of cutting-edge papers emerge every day.It is necessary to develop an intelligent system to help people understand papers.</p>
<p>For paper reading, LLMs with longer context windows are required because the full article of paper is usually long.However, training an LLM with long context windows from scratch requires significantly larger investments.To facilitate this, we employ a retrieval-augmented approach to enhance the effectiveness of the large model's answers.We initiate text splitting as a primary step and engage in chapter recognition to preserve the semantic integrity of segments.For the cross-language retrieval embedding model, firstly, we generate questions from paper segments using an LLM and con-struct a large set of (question, positive sample, negative samples) pairs for training.Subsequently, we use XLM-RoBERTa (Conneau et al., 2020) as the language encoder and fine-tune the model via contrastive learning.The input question and retrieved segments are finally fed into the SciLit-LLM to generate answers.</p>
<p>Reading Copilot enhances paper comprehension through natural language interactions.Questions fall into two categories: those within the paper, which SciLit-LLM answers using the input paper alone, and those outside the paper, which require a search engine plugin to retrieve relevant information.For the latter, answers are generated through retrieval-augmented generation using SciLit-LLM.</p>
<p>Multi-Document Comparison allows for the comparison of two to five papers.For each selected paper, SparkRA provides the abstract and contributions separately.It also generates a comparative analysis table that highlights the proposed approaches and advantages of each paper.SparkRA can identify and output both the similarities and differences among the selected papers.</p>
<p>Academic writing</p>
<p>This function is directly powered by SciLit-LLM and includes polishing and translation.</p>
<p>Paper polishing.This function is used to assist the scholar and students in polishing the academic paper draft.We construct a large corpus of texts requiring polishing based on a multitude of wellwritten academic papers, utilizing few-shot learning and chain-of-thought (COT) prompting methodologies, followed by supervised learning for instruction fine-tuning.</p>
<p>Academic translation.In order to accurately translate domain-specific terminology, we have implemented a dynamic perception prompts approach to guide the model in completing translation tasks.Based on the user's input prompts, we obtain prompts with professional terminology translations from a terminology translation lexicon in the knowledge base, which are then fed into the large language model.</p>
<p>Experiments</p>
<p>Experiment setting</p>
<p>To validate the results of SparkRA, we adopt the following LLMs as the baseline models: • Llama: a large-scale language model developed and open-sourced by Meta, was compared to SciLit-LLM using three versions: Llama2-7B, Llama2-13B, and Llama3-8B.</p>
<p>• ChatGPT (GPT-3.5): it is a large-scale language model in the field of artificial intelligence developed by OpenAI.</p>
<p>• GPT-4: GPT-4 Turbo serves as our baseline model, consistently outperforming in a range of NLP tasks.</p>
<p>We evaluate the performance of models using the mean opinion score (MOS) on a scale of 1 (poorest) to 5 (optimal), with evaluations conducted by more than five individuals per task.For the machine translation task, we also use the BLEU metric (Papineni et al., 2002) for model evaluation.We gathered 100 academic parallel paragraphs from public Chinese journals with Chinese and English abstracts to serve as test sets.</p>
<p>To assess paper reading performance, we employ following two measures:</p>
<p>• Factuality: evaluates the accuracy of the system's response to factual information;</p>
<p>• Informativeness: assesses the completeness of the system's response.</p>
<p>To evaluate paper polishing and academic translation performance, we use three criteria:</p>
<p>• Fluency: assesses the language coherence of model's outputs;</p>
<p>• Fidelity: measures content faithfulness to the original text;</p>
<p>• Academic: evaluates adherence to academic language standards.</p>
<p>Results</p>
<p>The results of the paper reading are shown in Table 2 shows the results of the paper polishing task.While Llama2-13B generates coherent text, it struggles with fidelity due to non-existent elements.Although Llama3-8B performs well across tasks, our SparkRA model, pre-trained on scientific literature and fine-tuned with 13 billion parameters, shows even greater improvement.SparkRA achieves state-of-the-art results compared to widely used LLMs like GPT-3.5 and GPT-4 across all evaluation metrics, excelling particularly in academic relevance.</p>
<p>Table 3 presents the academic translation results.SparkRA excels with the highest fidelity score (4.91) and the second-highest academic quality (4.75), showcasing its superior ability to preserve meaning and produce contextually appropriate translations.Additionally, SparkRA's BLEU score of 0.198 reflects its robustness in both human and automatic evaluations.Despite lower human evaluation scores than GPT-4, SparkRA's 13B parameter size offers flexibility, ease of training, and cost-effectiveness.</p>
<p>Related Work</p>
<p>Scientific literature pre-trained language model Since the release of the pre-trained models  (Vaswani et al., 2017;Radford et al., 2018;Devlin et al., 2019), the language models for scientific literature have attracted the attention of scholars.These models are trained on various scientific datasets, with SciBERT on PubMed Central (Beltagy et al., 2019), BioBERT and BioMegatron on biomedical literature (Lee et al., 2020;Shin et al., 2020), Galactica on multilingual articles (Taylor et al., 2022), and ScholarBERT on ACL Anthology Corpus (Hong et al., 2022).</p>
<p>Retrieval augmented generation to LLM Retrieval-Augmented Generation (RAG), introduced by Lewis et al. (2020), mitigates hallucinations in Large Language Models (LLMs) by integrating external data.Ma et al. (2023) advanced RAG with query rewriting, while Chen et al. ( 2023) benchmarked its effects, creating the RGB.Lyu et al. (2023) developed an algorithm for assessing retrieved data significance.</p>
<p>AI for science Artificial intelligence has significantly impacted scientific research, enhancing efficiency and literature growth (Merchant et al., 2023;Szymanski et al., 2023).Wang et al. (2023a) proposed an AI-based scientific research method that can automatically extract useful information from a large amount of data and then use this information to conduct scientific research and discovery.Artificial intelligence technology has great potential in scientific research and discovery.</p>
<p>Conclusion</p>
<p>The SparkRA system, built on the SciLit-LLM, provides a comprehensive solution for academic tasks, including literature investigation, paper reading, and academic writing.Through extensive experiments, SparkRA demonstrated superior performance compared to existing models like ChatGPT, and even surpassed GPT-4 in specific tasks such as paper polishing, demonstrating its potential to enhance productivity for researchers and students with its precise and context-aware support for academic activities.</p>
<p>Figure 1 :
1
Figure 1: The process of building SparkRA system.</p>
<p>Figure 2 :
2
Figure 2: The system architecture of SparkRA integrates iFLYTEK Spark LLM and Scientific Literature LLM to facilitate literature investigation, paper reading, and academic writing.</p>
<p>Figure 3 :
3
Figure 3: The architecture of RAG-based literature investigation.</p>
<p>Figure 4 :
4
Figure 4: Literature investigation page.</p>
<p>Table 1 :
1
Results of paper reading task.
Factuality Informativeness Avg.Llama2-7B3.983.503.74Llama2-13B4.473.724.10Llama3-8B4.634.194.41GPT-3.54.203.974.09GPT-44.674.434.55SparkRA4.684.454.57</p>
<p>Table 2 :
2
Results of paper polishing task.
Fluency Fidelity Academic Avg.Llama2-7B4.593.944.444.32Llama2-13B4.593.534.064.06Llama3-8B4.563.974.474.33GPT-3.54.264.234.384.29GPT-44.264.294.414.32SparkRA4.414.454.614.49</p>
<p>Table 1 .
1
The highest results in the table are highlighted in bold, and the second-highest results are underlined.
SparkRA outperforms other models across all met-rics. It achieves the highest score in Factuality witha score of 4.68, surpassing the closest competitor,GPT-4, which scores 4.67. In terms of Informative-ness, SparkRA attains a score of 4.45, again leadingover GPT-4, which scores 4.43. Overall, SparkRAachieves the highest average score of 4.57, demon-strating superior performance compared to othermodels like Llama3-8B. These results underscoreSparkRA's effectiveness in producing factually ac-curate and informative text, establishing it as astate-of-the-art model in the paper reading task.</p>
<p>Table 3 :
3
Results of academic translation task.
Fluency Fidelity Academic Avg. BLEULlama2-7B4.533.934.134.200.104Llama2-13B4.734.034.334.360.116Llama3-8B4.644.464.434.510.168GPT-3.54.414.754.544.570.193GPT-44.504.884.844.740.180SparkRA4.344.914.754.670.198</p>
<p>Scibert: A pretrained language model for scientific text. Iz Beltagy, Kyle Lo, Arman Cohan, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language ProcessingEMNLP-IJCNLP2019</p>
<p>Alec Radford, Ilya Sutskever, and Dario Amodei. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems. Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishCurran Associates, Inc202033Language models are few-shot learners</p>
<p>Benchmarking large language models in retrieval-augmented generation. Jiawei Chen, Hongyu Lin, Xianpei Han, Le Sun, arXiv:2309.014312023arXiv preprint</p>
<p>Palm: Scaling language modeling with pathways. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Journal of Machine Learning Research. 242402023</p>
<p>Gregory P Veldhuizen, et al. 2023. The future landscape of large language models in medicine. Jan Clusmann, Fiona R Kolbinger, Sophie Hannah, Muti, Jan-Niklas Zunamys I Carrero, Narmin Eckardt, Chiara Ghaffari Laleh, Lavinia Maria, Sophie-Caroline Löffler, Michaela Schwarzkopf, Unger, Communications Medicine. 31141</p>
<p>Unsupervised cross-lingual representation learning at scale. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational Linguistics2020</p>
<p>BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Long and Short Papers. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesMinneapolis, MinnesotaAssociation for Computational Linguistics20191</p>
<p>Zhi Hong, Aswathy Ajith, Gregory Pauloski, Eamon Duede, Carl Malamud, Roger Magoulas, Kyle Chard, Ian Foster, arXiv:2205.11342Scholarbert: Bigger is not always better. 2022arXiv preprint</p>
<p>Chatgpt for good? on opportunities and challenges of large language models for education. Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Learning and individual differences. 1031022742023</p>
<p>Biobert: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Bioinformatics. 3642020</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-Tau Yih, Tim Rocktäschel, Advances in Neural Information Processing Systems. 202033</p>
<p>Improving retrieval-augmented large language models via data importance learning. Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, Ce Zhang, arXiv:2307.030272023arXiv preprint</p>
<p>Query rewriting for retrievalaugmented large language models. Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, Nan Duan, arXiv:2305.142832023arXiv preprint</p>
<p>Gowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for materials discovery. Amil Merchant, Simon Batzner, Muratahan Samuel S Schoenholz, Aykol, Nature. </p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>Improving language understanding by generative pre-training. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, 2018</p>
<p>Creation and adoption of large language models in medicine. H Nigam, David Shah, Michael A Entwistle, Pfeffer, Jama. 33092023</p>
<p>Biomegatron: Larger biomedical domain language model. Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina, Raul Puri, Mostofa Patwary, Mohammad Shoeybi, Raghav Mani, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)2020</p>
<p>Ekin Dogus Cubuk, Amil Merchant, et al. 2023. An autonomous laboratory for the accelerated synthesis of novel materials. Nathan J Szymanski, Bernardus Rendy, Yuxing Fei, Rishi E Kumar, Tanjin He, David Milsted, Matthew J Mcdermott, Max Gallant, Nature. </p>
<p>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, Robert Stojnic, arXiv:2211.09085Galactica: A large language model for science. 2022arXiv preprint</p>
<p>Large language models in medicine. Arun James Thirunavukarasu, Darren Shu, Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, Daniel Shu, Wei Ting, Nature medicine. 2982023</p>
<p>Attention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. 2017</p>
<p>Scientific discovery in the age of artificial intelligence. Hanchen Wang, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Nature. 62079722023a</p>
<p>Self-instruct: Aligning language models with self-generated instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, Hannaneh Hajishirzi, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Long Papers. the 61st Annual Meeting of the Association for Computational Linguistics2023b1</p>
<p>Finetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations. 2022</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-918 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-918</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-918</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-22.html">extraction-schema-22</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <p><strong>Paper ID:</strong> paper-455866ca838f356b53a7e3e5b344834f9e93dbbc</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/455866ca838f356b53a7e3e5b344834f9e93dbbc" target="_blank">ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalizedtool-use ability is feasible for compact language models.</p>
                <p><strong>Paper Abstract:</strong> Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e918.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e918.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-3.5 (OpenAI)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A family of large transformer-based language models from OpenAI that demonstrate strong zero-shot and few-shot generalization; used in this paper both as an assistant agent in data generation and as a baseline for tool-use evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large pre-trained transformer LLM used as (1) an assistant agent in the multi-agent simulation/data generation and (2) a strong baseline evaluator for tool-use generalization; no architectural modifications described in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Unseen simulated tools / Real-world APIs / GPT4Tools test set</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (multi-step tool invocation / multi-turn interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Simulated tools: Overall 75.0% (Procedure 77.0%, Response 85.0%); Real-world APIs: Overall 72.8% (Procedure 75.4%, Response 80.7%); GPT4Tools test set: SR (overall success) 91.5%</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e918.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e918.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source instruction-tuned conversational LLaMA-based model (representative compact LLM) used as the compact-model baseline in this paper; evaluated before and after fine-tuning on ToolAlpaca.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Vicuna-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compact instruction-following LLM (7B) used as a baseline; standard transformer LLM architecture, no architecture changes in paper; used for supervised fine-tuning experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Unseen simulated tools / Real-world APIs</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (multi-step tool invocation / multi-turn interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Simulated tools: Overall 17.0% (Procedure 19.0%, Response 21.0%); Human accept rate 16.0%. Real-world APIs: Overall 7.9% (Procedure 7.9%, Response 11.4%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>baseline (no ToolAlpaca fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Compact models without tool-specific or diverse tool-use training lack generalized tool-use ability compared to larger models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e918.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e918.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Vicuna (13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A larger variant of the Vicuna open-source conversational model (13B), used as a compact-model baseline and for ablations in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Vicuna-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Compact instruction-following LLM (13B) with transformer architecture; used as a baseline and fine-tuning target in experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Unseen simulated tools / Real-world APIs / GPT4Tools test set</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (multi-step tool invocation / multi-turn interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Simulated tools: Overall 16.0% (Procedure 17.0%, Response 31.0%); Human accept rate 25.0%. Real-world APIs: Overall 12.3% (Procedure 13.2%, Response 16.7%). GPT4Tools test set: SR (overall success) 26.2% (SR_t 84.4%, SR_act 43.7%, SR_args 46.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>baseline (no ToolAlpaca fine-tuning)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Same as Vicuna-7B: lack of exposure to diverse tool-use multi-turn instances leads to poor generalized tool-use despite being instruction-tuned.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e918.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e918.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolAlpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolAlpaca (fine-tuned Vicuna-7B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna-7B fine-tuned on the ToolAlpaca corpus (3.9k simulated tool-use instances) to acquire generalized tool-use ability; demonstrates large improvements on unseen tools compared to the unfine-tuned Vicuna baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolAlpaca-7B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-7B model fine-tuned (supervised) on a 3.9k-instance, multi-agent-simulated tool-use corpus representing 426 tools across 50 categories; retains the base transformer architecture but learns to produce ReAct-style thought/action sequences and to use tools described by OpenAPI-like specs.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>7B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Unseen simulated tools / Real-world APIs</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (multi-step tool invocation / multi-turn interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Simulated tools: Overall 60.0% (Procedure 63.0%, Response 69.0%); Human accept rate 73.0%. Real-world APIs: Overall 55.3% (Procedure 63.2%, Response 57.9%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No architectural change to core model; uses tool-use interface representation (OpenAPI specs) and ReAct-style prompting during generation/fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised fine-tuning on simulated multi-agent-generated tool-use corpus (ToolAlpaca); 3 epochs, batch size 128, learning rate 2e-5.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method / data augmentation (simulated multi-agent corpus)</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fine-tune compact Vicuna-7B on 3.9k synthesized multi-turn tool-use instances generated by a multi-agent simulation (user, assistant, tool-executor), where tools are represented by generated OpenAPI specs and the assistant uses a ReAct (thought/action/observation) format. Dataset spans 426 tools in 50 categories; diversity-promoting prompts were used.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Massive improvement vs unfine-tuned baseline: simulated overall accuracy from 17.0% -> 60.0%; real-world overall from 7.9% -> 55.3% (approximate increases of +43 and +47 percentage points respectively).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Authors attribute the baseline deficiency to lack of exposure to diverse tool-use instances; increasing diversity and providing explicit tool documentation during fine-tuning enables generalization to unseen tools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e918.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e918.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ToolAlpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ToolAlpaca (fine-tuned Vicuna-13B)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Vicuna-13B fine-tuned on the ToolAlpaca corpus; achieves performance on unseen tool-use tasks close to much larger models and comparable to GPT-3.5 on several benchmarks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>ToolAlpaca-13B</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Vicuna-13B model fine-tuned (supervised) on the ToolAlpaca 3.9k simulated tool-use corpus; model architecture unchanged, trained to produce tool-invocation procedures and final responses.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>13B</td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>Unseen simulated tools / Real-world APIs / GPT4Tools test set</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (multi-step tool invocation / multi-turn interaction)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Simulated tools: Overall 70.0% (Procedure 70.0%, Response 73.0%); Human accept rate 75.0%. Real-world APIs: Overall 61.4% (Procedure 66.7%, Response 67.5%). GPT4Tools test set: SR (overall success) 83.7% (SR_act 95.5%, SR_args 85.3%).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td>No core architectural modification; leverages tool description/OpenAPI and ReAct-style supervised examples during fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised fine-tuning on ToolAlpaca corpus (3 epochs, batch size 128, lr 2e-5).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method / data augmentation (simulated multi-agent corpus); dataset-diversity intervention</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Fine-tuning on 3.9k multi-agent-simulated tool-use instances spanning hundreds of tools; authors also ablate dataset diversity (keeping instance count constant while varying number of unique tools) and show diversity-driven gains.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>Large improvement from baseline Vicuna-13B (overall 16.0%) to ToolAlpaca-13B (overall 70.0%) on simulated tools; on GPT4Tools test set overall SR improved from 26.2% (Vicuna-13B) to 83.7% (ToolAlpaca-13B), approaching GPT-3.5/GPT4Tools (~90%+).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td>Lack of diverse, structured tool-use training data prevents compact models from generalizing to unseen tools; simulated diverse training mitigates this gap.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e918.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e918.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM or agent performance on question-answering tasks versus interactive/procedural tasks (such as tool use, planning, multi-step reasoning, sequential decision-making), including any architectural or training interventions that affect this performance gap.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT4Tools</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT4Tools (Yang et al. 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A concurrent work that uses self-instruction to generate large synthetic tool-use datasets (71k instances) and fine-tunes models for tool usage; used in this paper as an external testbed to evaluate out-of-distribution generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>GPT4Tools (trained models / dataset)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Framework / dataset that generates large numbers of synthetic tool-use instances (71k in cited work) and fine-tunes models for tool use; the test set from GPT4Tools was used in cross-evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_task_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>qa_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_name</strong></td>
                            <td>GPT4Tools test set (multi-modal tools included)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_task_type</strong></td>
                            <td>tool use (multi-step / multi-modal)</td>
                        </tr>
                        <tr>
                            <td><strong>interactive_performance</strong></td>
                            <td>Reported in this paper: GPT4Tools-trained model SR (overall success) 90.6% on its test set (as cited in Table 4 of this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>reports_both_qa_and_interactive</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_gap_observed</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>architectural_features</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Supervised fine-tuning on a much larger self-instructed synthetic tool-use dataset (71k instances in cited work).</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_type</strong></td>
                            <td>training method / large-scale synthetic data</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_description</strong></td>
                            <td>Large-scale self-instructed synthetic tool-use dataset and fine-tuning to teach models tool-use behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>intervention_effect</strong></td>
                            <td>High success rates on the GPT4Tools testbed (~90% SR), and ToolAlpaca-13B (trained on only 3.9k cases) achieves 83.7% SR on the same test set, suggesting data diversity and quality can reduce required scale.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesized_cause_of_gap</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction <em>(Rating: 2)</em></li>
                <li>Toolformer: Language Models Can Teach Themselves to Use Tools <em>(Rating: 2)</em></li>
                <li>ReAct: Synergizing Reasoning and Acting in Language Models <em>(Rating: 2)</em></li>
                <li>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs <em>(Rating: 2)</em></li>
                <li>Tool Learning with Foundation Models <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-918",
    "paper_id": "paper-455866ca838f356b53a7e3e5b344834f9e93dbbc",
    "extraction_schema_id": "extraction-schema-22",
    "extracted_data": [
        {
            "name_short": "GPT-3.5",
            "name_full": "GPT-3.5 (OpenAI)",
            "brief_description": "A family of large transformer-based language models from OpenAI that demonstrate strong zero-shot and few-shot generalization; used in this paper both as an assistant agent in data generation and as a baseline for tool-use evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "GPT-3.5",
            "model_description": "Large pre-trained transformer LLM used as (1) an assistant agent in the multi-agent simulation/data generation and (2) a strong baseline evaluator for tool-use generalization; no architectural modifications described in this paper.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Unseen simulated tools / Real-world APIs / GPT4Tools test set",
            "interactive_task_type": "tool use (multi-step tool invocation / multi-turn interaction)",
            "interactive_performance": "Simulated tools: Overall 75.0% (Procedure 77.0%, Response 85.0%); Real-world APIs: Overall 72.8% (Procedure 75.4%, Response 80.7%); GPT4Tools test set: SR (overall success) 91.5%",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": null,
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": null,
            "uuid": "e918.0",
            "source_info": {
                "paper_title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Vicuna-7B",
            "name_full": "Vicuna (7B)",
            "brief_description": "An open-source instruction-tuned conversational LLaMA-based model (representative compact LLM) used as the compact-model baseline in this paper; evaluated before and after fine-tuning on ToolAlpaca.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Vicuna-7B",
            "model_description": "Compact instruction-following LLM (7B) used as a baseline; standard transformer LLM architecture, no architecture changes in paper; used for supervised fine-tuning experiments.",
            "model_size": "7B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Unseen simulated tools / Real-world APIs",
            "interactive_task_type": "tool use (multi-step tool invocation / multi-turn interaction)",
            "interactive_performance": "Simulated tools: Overall 17.0% (Procedure 19.0%, Response 21.0%); Human accept rate 16.0%. Real-world APIs: Overall 7.9% (Procedure 7.9%, Response 11.4%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "baseline (no ToolAlpaca fine-tuning)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Compact models without tool-specific or diverse tool-use training lack generalized tool-use ability compared to larger models.",
            "uuid": "e918.1",
            "source_info": {
                "paper_title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Vicuna-13B",
            "name_full": "Vicuna (13B)",
            "brief_description": "A larger variant of the Vicuna open-source conversational model (13B), used as a compact-model baseline and for ablations in the paper.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_or_agent_name": "Vicuna-13B",
            "model_description": "Compact instruction-following LLM (13B) with transformer architecture; used as a baseline and fine-tuning target in experiments.",
            "model_size": "13B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Unseen simulated tools / Real-world APIs / GPT4Tools test set",
            "interactive_task_type": "tool use (multi-step tool invocation / multi-turn interaction)",
            "interactive_performance": "Simulated tools: Overall 16.0% (Procedure 17.0%, Response 31.0%); Human accept rate 25.0%. Real-world APIs: Overall 12.3% (Procedure 13.2%, Response 16.7%). GPT4Tools test set: SR (overall success) 26.2% (SR_t 84.4%, SR_act 43.7%, SR_args 46.7%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": null,
            "training_method": "baseline (no ToolAlpaca fine-tuning)",
            "intervention_type": null,
            "intervention_description": null,
            "intervention_effect": null,
            "hypothesized_cause_of_gap": "Same as Vicuna-7B: lack of exposure to diverse tool-use multi-turn instances leads to poor generalized tool-use despite being instruction-tuned.",
            "uuid": "e918.2",
            "source_info": {
                "paper_title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ToolAlpaca-7B",
            "name_full": "ToolAlpaca (fine-tuned Vicuna-7B)",
            "brief_description": "Vicuna-7B fine-tuned on the ToolAlpaca corpus (3.9k simulated tool-use instances) to acquire generalized tool-use ability; demonstrates large improvements on unseen tools compared to the unfine-tuned Vicuna baseline.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ToolAlpaca-7B",
            "model_description": "Vicuna-7B model fine-tuned (supervised) on a 3.9k-instance, multi-agent-simulated tool-use corpus representing 426 tools across 50 categories; retains the base transformer architecture but learns to produce ReAct-style thought/action sequences and to use tools described by OpenAPI-like specs.",
            "model_size": "7B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Unseen simulated tools / Real-world APIs",
            "interactive_task_type": "tool use (multi-step tool invocation / multi-turn interaction)",
            "interactive_performance": "Simulated tools: Overall 60.0% (Procedure 63.0%, Response 69.0%); Human accept rate 73.0%. Real-world APIs: Overall 55.3% (Procedure 63.2%, Response 57.9%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "No architectural change to core model; uses tool-use interface representation (OpenAPI specs) and ReAct-style prompting during generation/fine-tuning.",
            "training_method": "Supervised fine-tuning on simulated multi-agent-generated tool-use corpus (ToolAlpaca); 3 epochs, batch size 128, learning rate 2e-5.",
            "intervention_type": "training method / data augmentation (simulated multi-agent corpus)",
            "intervention_description": "Fine-tune compact Vicuna-7B on 3.9k synthesized multi-turn tool-use instances generated by a multi-agent simulation (user, assistant, tool-executor), where tools are represented by generated OpenAPI specs and the assistant uses a ReAct (thought/action/observation) format. Dataset spans 426 tools in 50 categories; diversity-promoting prompts were used.",
            "intervention_effect": "Massive improvement vs unfine-tuned baseline: simulated overall accuracy from 17.0% -&gt; 60.0%; real-world overall from 7.9% -&gt; 55.3% (approximate increases of +43 and +47 percentage points respectively).",
            "hypothesized_cause_of_gap": "Authors attribute the baseline deficiency to lack of exposure to diverse tool-use instances; increasing diversity and providing explicit tool documentation during fine-tuning enables generalization to unseen tools.",
            "uuid": "e918.3",
            "source_info": {
                "paper_title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "ToolAlpaca-13B",
            "name_full": "ToolAlpaca (fine-tuned Vicuna-13B)",
            "brief_description": "Vicuna-13B fine-tuned on the ToolAlpaca corpus; achieves performance on unseen tool-use tasks close to much larger models and comparable to GPT-3.5 on several benchmarks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_or_agent_name": "ToolAlpaca-13B",
            "model_description": "Vicuna-13B model fine-tuned (supervised) on the ToolAlpaca 3.9k simulated tool-use corpus; model architecture unchanged, trained to produce tool-invocation procedures and final responses.",
            "model_size": "13B",
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "Unseen simulated tools / Real-world APIs / GPT4Tools test set",
            "interactive_task_type": "tool use (multi-step tool invocation / multi-turn interaction)",
            "interactive_performance": "Simulated tools: Overall 70.0% (Procedure 70.0%, Response 73.0%); Human accept rate 75.0%. Real-world APIs: Overall 61.4% (Procedure 66.7%, Response 67.5%). GPT4Tools test set: SR (overall success) 83.7% (SR_act 95.5%, SR_args 85.3%).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": true,
            "architectural_features": "No core architectural modification; leverages tool description/OpenAPI and ReAct-style supervised examples during fine-tuning.",
            "training_method": "Supervised fine-tuning on ToolAlpaca corpus (3 epochs, batch size 128, lr 2e-5).",
            "intervention_type": "training method / data augmentation (simulated multi-agent corpus); dataset-diversity intervention",
            "intervention_description": "Fine-tuning on 3.9k multi-agent-simulated tool-use instances spanning hundreds of tools; authors also ablate dataset diversity (keeping instance count constant while varying number of unique tools) and show diversity-driven gains.",
            "intervention_effect": "Large improvement from baseline Vicuna-13B (overall 16.0%) to ToolAlpaca-13B (overall 70.0%) on simulated tools; on GPT4Tools test set overall SR improved from 26.2% (Vicuna-13B) to 83.7% (ToolAlpaca-13B), approaching GPT-3.5/GPT4Tools (~90%+).",
            "hypothesized_cause_of_gap": "Lack of diverse, structured tool-use training data prevents compact models from generalizing to unseen tools; simulated diverse training mitigates this gap.",
            "uuid": "e918.4",
            "source_info": {
                "paper_title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GPT4Tools",
            "name_full": "GPT4Tools (Yang et al. 2023)",
            "brief_description": "A concurrent work that uses self-instruction to generate large synthetic tool-use datasets (71k instances) and fine-tunes models for tool usage; used in this paper as an external testbed to evaluate out-of-distribution generalization.",
            "citation_title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
            "mention_or_use": "mention",
            "model_or_agent_name": "GPT4Tools (trained models / dataset)",
            "model_description": "Framework / dataset that generates large numbers of synthetic tool-use instances (71k in cited work) and fine-tunes models for tool use; the test set from GPT4Tools was used in cross-evaluation.",
            "model_size": null,
            "qa_task_name": null,
            "qa_performance": null,
            "interactive_task_name": "GPT4Tools test set (multi-modal tools included)",
            "interactive_task_type": "tool use (multi-step / multi-modal)",
            "interactive_performance": "Reported in this paper: GPT4Tools-trained model SR (overall success) 90.6% on its test set (as cited in Table 4 of this paper).",
            "reports_both_qa_and_interactive": false,
            "performance_gap_observed": null,
            "architectural_features": null,
            "training_method": "Supervised fine-tuning on a much larger self-instructed synthetic tool-use dataset (71k instances in cited work).",
            "intervention_type": "training method / large-scale synthetic data",
            "intervention_description": "Large-scale self-instructed synthetic tool-use dataset and fine-tuning to teach models tool-use behavior.",
            "intervention_effect": "High success rates on the GPT4Tools testbed (~90% SR), and ToolAlpaca-13B (trained on only 3.9k cases) achieves 83.7% SR on the same test set, suggesting data diversity and quality can reduce required scale.",
            "hypothesized_cause_of_gap": null,
            "uuid": "e918.5",
            "source_info": {
                "paper_title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
            "rating": 2
        },
        {
            "paper_title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "rating": 2
        },
        {
            "paper_title": "ReAct: Synergizing Reasoning and Acting in Language Models",
            "rating": 2
        },
        {
            "paper_title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "rating": 2
        },
        {
            "paper_title": "Tool Learning with Foundation Models",
            "rating": 1
        }
    ],
    "cost": 0.014435,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases</h1>
<p>Qiaoyu Tang ${ }^{1,3}$, Ziliang Deng ${ }^{1,3}$, Hongyu Lin ${ }^{1 <em>}$, Xianpei Han ${ }^{1,2 </em>}$, Qiao Liang ${ }^{1,3}$, Boxi Cao ${ }^{1,3}$, Le Sun ${ }^{1,2}$<br>${ }^{1}$ Chinese Information Processing Laboratory ${ }^{2}$ State Key Laboratory of Computer Science Institute of Software, Chinese Academy of Sciences, Beijing, China<br>${ }^{3}$ University of Chinese Academy of Sciences, Beijing, China<br>{tangqiaoyu2020,dengziliang2021,hongyu,xianpei}@iscas.ac.cn<br>{liangqiao2022,boxi2020,sunle}@iscas.ac.cn</p>
<h4>Abstract</h4>
<p>Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models. ${ }^{1}$</p>
<h2>1 Introduction</h2>
<p>Embodied intelligence, the ability to meaningfully interact with the environment, stands as a core attribute of advanced cognitive systems and a crucial advancement in artificial intelligence. The ability to create and use tools has expanded human beings' physical capabilities to interact with environments and augmented cognitive functions. Such evolutionary milestone has not only broadened our range of physical actions, but also brought about transformative changes in our problem-solving abilities and innovative thinking. The pursuit of incorporating tool-use capabilities into artificial</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>intelligence holds great significance in advancing the development of general intelligent systems.</p>
<p>Recent advancements in enhancing large language models (LLMs) such as GPT-4 (OpenAI 2023) with tool-use abilities have made significant progress in this area. These models have shown their ability to effectively employ external tools through integrated plugins, thereby expanding their versatility and enhancing the precision and quality of their outputs. Unfortunately, due to a lack of understanding of how existing large language models acquire the general tool-use capability, currently compact language models still do not possess such general ability. Consequently, substantial research efforts are dedicated to finetuning smaller language models to acquire the capacity for tool usage (Komeili, Shuster, and Weston 2022; Parisi, Zhao, and Fiedel 2022; Schick et al. 2023) on a limited range of tools, which lacks the ability to generalize to unseen tools. This discrepancy between the generalized tool-use abilities of larger models and the more constrained capabilities of compact models presents an intriguing question: Can these compact language models learn to generalize their tool-use abilities, thus enabling interaction with a broader spectrum of tools?</p>
<p>In this paper, we explore whether it is feasible for compact language models to learn generalized tool-use abilities. Intuitively, previous studies have demonstrated the possibility of equipping compact language models with generalized instruction-following abilities by fine-tuning them on diversified instruction datasets (Taori et al. 2023; Zhou et al. 2023). Hence, a promising strategy for equipping language models with generalized tool-use abilities would involve fine-tuning them on a corpus containing highly-diversified tool-use instances. Unfortunately, such a diversified corpus is currently unavailable. This absence can be attributed to several crucial factors. First, the absence of a set of available tool APIs that can accommodate various tool usage scenarios for language models presents a considerable challenge in assembling a diverse collection of tools. Second, real-world tool-use instances often entail complex, intricate, and multiturn interactions between the language model, users, and tools. This complexity significantly heightens the difficulty and manual effort involved in creating instances encompassing a wide array of tools on a large scale. Consequently,</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" />Figure 1: A high-level overview of ToolAlpaca, consisting of three components: (1)Toolset construction, where structured documentation for each tool is generated based on the brief introductions provided by public-apis. (2) Tool-use instance generation via multi-agent simulation. (3) ToolAlpaca model training, which involves fine-tuning language models on generated tool-use corpus to get ToolAlpaca.</p>
<p>these factors significantly restrict the efforts to construct a diversified tool-use corpus for language model training efficiently.</p>
<p>To this end, we propose a framework named ToolAlpaca, which is designed to automatically create a diverse and well-structured toolset for LLMs and generate multi-turn complex tool-use instances for generalized tool learning. The overall structure of ToolAlpaca is shown in Figure 1. Specifically, to ensure the diversity and comprehensiveness of the toolset, ToolAlpaca leverages LLMs text generation capability to construct a comprehensive toolset. ToolAlpaca gathers a substantial amount of brief introductions of potentially valuable tools from the internet. Its important to note that there is no requirement for these tools APIs to be functional or for them to possess structured documentation directly usable by LLMs. Building on this foundation, ToolAlpaca employs the generative capacity of LLMs by taking the brief introduction of relevant tools as input and prompts the model to produce detailed, structured documentation for each tool. By employing this methodology, ToolAlpaca has collected more than 400 tool descriptions spanning 50 categories. Each tool is uniformly represented using a standardized documentation format. Subsequently, in order to acquire tool-use instances involving the aforementioned tools, we have designed a simulation environment aimed at emulating the multi-step interactions among language models, users, and tools. Specifically, we utilize LLMs to simulate the interactions between the model, users, and the APIs of the tools by leveraging LLMs to serve as different kinds of agents. In this way, our simulation environment can generate a substantial volume of tool-use instances without any manual intervention. Consequently, we have crafted an inclusive tool-use dataset that comprises 3938 instances, effectively showcasing the practical application of over 400 distinct tools.</p>
<p>To verify whether our corpus can empower compact language models with the generalized tool-use ability, we conduct experiments to train ToolAlpaca model on Vicuna (Chiang et al. 2023), a representative compact language model, and subsequently evaluate its performance on various unseen tools. Through machine evaluation with GPT-4, we find that ToolAlpaca can effectively equip numerous unseen tools, ranging from real-world APIs to multi-modal tools, and it exhibits competitive performance with GPT-3.5. Furthermore, we investigate the effect of diversity. It is observed that even with the same number of instances, the model trained on more varied toolsets will achieve better performance. This underscores that diversity is a pivotal factor for ToolAlpaca to generalize tool learning with 3000 simulated cases.</p>
<p>In summary, the main contributions of this paper are:</p>
<ul>
<li>To the best of our knowledge, this paper is the first work that verifies the feasibility of equipping compact language models with generalized tool-use capacities as extremely large language models.</li>
<li>This paper presents ToolAlpaca, a simple framework for the automated generation of tool-use corpus and the enhancement of the compact language models generalized tool-use ability.</li>
<li>We create a diverse tool-use corpus containing 3.9k tool-use instances from more than 400 tools across 50 distinct categories. It serves as a solid foundation for compact language models to acquire generalized tool-use ability.</li>
</ul>
<h2>2 Related Work</h2>
<p>Tool Use The utilization of external tools in LLMs has emerged as a rapidly growing research area <em>Mialon et al. (2023); Qin et al. (2023a)</em>. Current approaches can be divided into two distinct categories. The first category leverages the capabilities of LLMs, prompting them to interact with various tools, ranging from highly specialized ones such as code interpreters <em>Gao et al. (2022); Chen et al. (2022)</em>, search engines <em>Yao et al. (2022)</em>, retrieval models <em>Khattab et al. (2023)</em> and AI models <em>Shen et al. (2023); Lu et al. (2023)</em>, to more versatile toolsets <em>Qin et al. (2023a); Li et al. (2023); Song et al. (2023)</em>. Large language models have already demonstrated robust generalization capabilities in tool usage and enable to equip numerous unseen tools via prompting. In contrast, the second category concentrates on enhancing the tool-specific usage capabilities of compact language models through fine-tuning with datasets specifically designed for the specialized tools <em>Parisi et al. (2022); Schick et al. (2023); Xu et al. (2023)</em>. Concurrent with our work, GPT4Tools <em>Yang et al. (2023)</em> fine-tuning compact models to incorporate multi-modal tools, which concentrates on a</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: An instance of a tool documentation, composed of five essential parts: name, introduction, description, function documentation, OpenAPI specification.
set of quite similar multi-modal tools. ToolLLM (Qin et al. 2023b) facilitates language models to master massive APIs. However, their data collection strategy requires the prior accumulation of massive authentic APIs, which requires manual efforts to obtain and verify. Despite their effectiveness, the domain of generalized tool-use abilities in compact language models remains largely unexplored upon the accomplishment of this paper. This study aims to bridge this research gap by automatically constructing a diverse dataset on tool utilization that encompasses various tool-use scenarios.</p>
<p>LLMs for Data Generation Many research studies have employed LLMs for data generation, focusing on various tasks such as question answering (Wang et al. 2021; Agrawal et al. 2022; Chen, Chen, and tau Yih 2023), semantic similarity predictions (Schick and Schtze 2021), and instruction tuning (Honovich et al. 2022; Wang et al. 2023). Furthermore, in the context of tool use, several works (Schick et al. 2023; Patil et al. 2023; Yang et al. 2023) have already employed model-synthesized data to enhance specific tool-use capabilities. However, the generation of generalized tool-use data poses more significant challenges, as it involves extensive and diverse tools and more intricate multi-turn interactions.</p>
<h2>3 Diversified Tool-use Corpus Generation via Multi-agent Simulation</h2>
<p>In this section, we introduce ToolAlpaca, a multi-agent simulation framework designed to generate a diversified tooluse corpus with minimal human intervention. As shown in Figure 1, our framework consists of two stages:</p>
<ol>
<li>Toolset Construction. This step aims to construct a collection of tools and represent them using a standardized format as {name, introduction, description, function documentation, OpenAPI specification}. Specifically, we initiate the process by sourcing tool names and introductions from the internet and then utilize LLMs to enrich
them with structured documentation that thoroughly delineates the functionality and usage of each tool. In this way, we can construct a diverse and structured toolset that closely resembles real-world scenarios.</li>
<li>Tool-use Instance Generation. Given the toolset, this phase's objective is to generate tool-use instances within a simulation environment automatically. This environment is engineered through the orchestration of three distinct virtual agents, each embodied by a large language model: the user, the tool executor, and the assistant. Through the multi-turn interplay among these agents, we can generate tool-use instances that reflect real-world tool utilization scenarios. Each tool-use instance consists of three key elements: {the user's instructions, the actions and their corresponding tool outputs, final response}.</li>
</ol>
<h3>3.1 Diverse Toolset Construction</h3>
<p>This section describes how to construct a diverse toolset and represent them in a uniform format. The process initiates with the accumulation of an extensive API collection from the internet, reflecting real-world tool usage scenarios. Given the rudimentary descriptions and lack of uniform representation in these APIs, we further leverage the generative capabilities of LLM to create comprehensive documentation for each tool. This documentation assists language models in understanding the functionality and usage of each tool. Subsequently, we adhere to OpenAPI standards to generate a uniform specification for each API, enabling automated computer invocation and facilitating subsequent tool execution simulation. In this way, each tool can be represented as a quintuple ${$ name, introduction, description, function documentation, OpenAPI specification}. Figure 2 provides an example, where the name, description, and introduction offer basic information and the purpose of the public holiday tool, the function documentation provides the functionality, inputs and outputs of various functions (getHolidays, search-</p>
<p>Holidays, getHolidayDetails) contained within the tool, and the OpenAPI Specification provides a more comprehensive and structured document. The detailed construction steps are elaborated as follows.</p>
<p>Tool Collection. Various tools are commonly utilized by human beings, typically manifested in the form of webbased APIs. To facilitate the utilization and discovery of these APIs, a plethora of repositories exist on the Internet, aggregating a vast collection of practical and commonly used APIs. Consequently, this step leverages the representative API repository, public-apis , as our target toolset. This repository encompasses over 1400 APIs spanning more than 50 distinct categories. From this, we collect the name and introduction of each tool.</p>
<p>Documentation Generation. To enhance the language models comprehension of tools functionalities and usage, this step employs LLMs to automatically generate documentation for each tool, including its description and function documentation. Specifically, given the tools name and introduction, we leverage the powerful generative capabilities of LLMs, such as ChatGPT, and construct corresponding prompts to generate the description and function documentation for each tool. As illustrated in Figure 2, for the tool description, we expand the introduction to provide a general overview of the tools purpose and functionality, enabling the language model to understand the appropriate task scenarios for using the tool. For the function documentation, we prompt LLMs to generate more specific functions within the scope described in the description, including their respective input and output parameters. It ensures that the LLM comprehends how to use the tools different functionalities. By employing this approach, we lay a solid foundation for subsequent user instruction generation and the creation of the simulated tool-use environment.</p>
<p>OpenAPI Specification Generation. Finally, we prompt LLM to generate a more formalized and structured representation for each tool in the form of OpenAPI Specification. OpenAPI Specification (OAS) defines a standard and language-agnostic interface for describing APIs, including information about endpoints, expected input/output types, and possible error responses. OAS provides consistency and readability for both humans and machines, making it an ideal choice for our tool-use corpus. This comprehensive documentation serves as the foundation for simulating tool execution in the subsequent stages of our framework.</p>
<p>In this way, we construct a diverse, uniformly represented toolset, which provides a solid foundation for the multiagent simulation environment building and further tool-use corpus generation.</p>
<h3>3.2 Automatic Tool-use Instances Generation</h3>
<p>Given the toolset, this section describes how to automatically construct a tool-use corpus, so that language models can be trained to acquire generalized tool-use ability. Specifically, as depicted in Figure 3, each tool-use instance can be represent as a triple {Instruction, Actions, Response}:</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: An illustration of the tool-use instance generation process within the simulation environment. The user agent initiates the sequence by providing an instruction. The assistant agent then interprets the instruction and engages in a multi-turn interaction with the user and the tool executor until a suitable response is generated.</p>
<ul>
<li>Instruction: A user query that requires tool assistance for resolution. " so can you tell me the list of holidays in Japan next year?" serves as an instruction in our example.</li>
<li>Actions: The process of resolving an instruction may involve executing multiple actions in a specific order. Following React (Yao et al. 2022), each action is represented by a tuple that includes the thought, the function name, the input parameters, and the corresponding tool response. For example, as shown in Figure 3, the tuple ("I need to get the list of holidays in Japan for 2024.", "getHolidays", {"country": "Japan", "year": 2024}, "Status Code: 200 Response:") represents an action.</li>
<li>Response: This refers to the models conclusive response after the multi-turn interaction, integrating the tool responses to provide a comprehensive solution to the original instruction. For instance, the response in our example</li>
</ul>
<p>is: "The list of holidays in Japan for 2024 is ...".
However, constructing a diverse and authentic tool-use dataset is a challenging task. Firstly, the wide variety within our toolset makes it impracticable to manually draft instructions for each tool. Given the vast array of tools, spanning from recreational to professional domains, and the fact that the construction of instructions relies on understanding the functionality and potential use cases of the tools, the burden of manual annotation becomes overwhelming. Secondly, tool usage in real-world scenarios often involves a multi-round iterative process of trial and error, making the automated construction of tool-use instances that reflect real situations highly challenging.</p>
<p>To this end, we design a simulation environment to generate a tool-use corpus, encompassing three virtual agents: the user, the assistant, and the tool executor. Tool-use instances are generated through the interplay among these agents. Specifically, each agent is simulated by a large language model with a specific prompt. The distinct roles of each agent are detailed as follows:</p>
<ul>
<li>User Agent is designed to mimic the tool user, with its functionalities encompassing: (1) drafting task instructions for the current tool based on its function documentation; (2) responding to the assistant's queries based on the current interaction context, providing essential information that might be missing from the initial instruction. For each functionality, we construct corresponding prompt templates to guide LLMs to generate appropriate outputs. Moreover, to ensure diversity in task instructions, we have employed various prompts to generate instructions of different formats, including commands, questions, and others. Leveraging the large model's proficiency across virtually all domains, this approach enables the generation of high-quality and diversified instructions based on tool documentation. This effectively addresses the previously mentioned issues with manual annotation.</li>
<li>Assistant Agent is designed to simulate an assistant with tool utilization capabilities. It receives instructions from the user agent and determines the subsequent actions. This involves choosing the appropriate tools and functions, generating commands for the tool executor, and summarizing the interaction to generate the final response. As shown in Figure 3, following ReAct (Yao et al. 2022), we employ a (thought, action, observation) format template to guide LLM in accomplishing these tasks.</li>
<li>Tool Executor Agent is constructed to emulate the execution of tools, receiving requests from the assistant agent and generating responses based on the tool's predefined functionalities. Specifically, after conducting format and parameter checks on the assistant's requests, these requests are converted into network request formats. Then the tool executor prompts LLM with the tool's OpenAPI specification and the assistant's requests to generate simulated execution results. Leveraging LLMs' robust simulation and generation capabilities, we mitigate the intricacies involved in constructing actual API calls. This method has been empirically validated as
both accurate and effective, as evidenced in the following section.</li>
</ul>
<p>Given the above agents, tool-use cases are generated through multiple rounds of interaction between them. Initially, the user agent generates instructions based on the tool information. Subsequently, the assistant agent selects an appropriate action and its corresponding input and awaits simulation execution and response from the tool executor. This iterative procedure of action selection and tool response collection continues until the assistant agent deems it has gathered sufficient information to respond to the user's instructions. Through this multi-agent interaction, we can simulate realistic tool-use scenarios and generate comprehensive and diversified tool-use instances.</p>
<h2>4 ToolAlpaca Corpus</h2>
<h3>4.1 Construction Details</h3>
<p>Leveraging the aforementioned multi-agent simulation framework, we have constructed the ToolAlpaca corpus. Specifically, the process begins with randomly selecting 500 APIs from the public-apis repository. Subsequently, we utilize ChatGPT to generate more comprehensive documentation, resulting in a varied and well-structured toolset. Within our simulation environment, we use ChatGPT as the user agent to generate ten instructions for each tool, and the tool executor to simulate tool execution. We appoint GPT-3.5 as the assistant agent due to its superior performance in structured output generation. ${ }^{3}$</p>
<p>To guarantee the quality of the data, we implement a simple yet effective filtering process on the generated corpus. We systematically exclude tools that might incorporate nontextual content within their inputs or outputs. In terms of instances, we discard those that exceed five interaction steps, lack relevant function calls, or exhibit parsing errors in their output.</p>
<p>Finally, we automatically construct an extensive and diversified tool-use corpus. As shown in Table 1, it encompasses 426 distinctive tools from 50 categories, totaling 3938 instances. In the following sections, we will analyze the diversity and quality of our corpus.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">statistics</th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"># of Tool Categories</td>
<td style="text-align: right;">50</td>
</tr>
<tr>
<td style="text-align: left;"># of Tools</td>
<td style="text-align: right;">426</td>
</tr>
<tr>
<td style="text-align: left;"># of Instance</td>
<td style="text-align: right;">3,938</td>
</tr>
<tr>
<td style="text-align: left;"># of single function call</td>
<td style="text-align: right;">2,512</td>
</tr>
<tr>
<td style="text-align: left;"># of multiple function calls</td>
<td style="text-align: right;">1,426</td>
</tr>
<tr>
<td style="text-align: left;">avg. functions per tool</td>
<td style="text-align: right;">4.85</td>
</tr>
<tr>
<td style="text-align: left;">avg. steps</td>
<td style="text-align: right;">1.66</td>
</tr>
<tr>
<td style="text-align: left;">avg. instruction length</td>
<td style="text-align: right;">23.42</td>
</tr>
<tr>
<td style="text-align: left;">avg. output length</td>
<td style="text-align: right;">36.19</td>
</tr>
</tbody>
</table>
<p>Table 1: Statistics of ToolAlpaca corpus.</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Length distribution of generated instructions and final outputs.</p>
<h2>4.2 Diversity</h2>
<p>As previously underscored, diversity is pivotal for large models to acquire generalized capabilities and adapt to a multitude of unseen scenarios (Wang et al. 2023). ToolAlpaca corpus demonstrates diversity in two aspects:</p>
<ul>
<li><strong>Toolset.</strong> As outlined in Table 1, our toolset demonstrates diversity in multiple aspects: (1) The toolset encompasses 50 categories of tools, ranging from common categories, such as jobs and news, to specialized categories like blockchain and finance. (2) Each tool provides an average of five functions, highlighting the diversity and comprehensiveness of its capabilities. (3) The range of function inputs varies from simple to complex scenarios, including arrays and objects, further enhancing the richness and complexity of our toolset.</li>
<li><strong>Instances.</strong> The instances within the ToolAlpaca corpus demonstrate diversity in terms of instruction, function calls, and error handling. Specifically, we employ a variety of prompts during instruction generation to stimulate the language model in producing diverse instructions. The wide-ranging distribution of instruction length, as illustrated in Figure 4, partly substantiates this point. Additionally, our dataset contains about 1.5k instances that require multiple function invocations for resolution, further underscoring the comprehensiveness of our dataset. Furthermore, our data adequately reflects the potential errors that may be encountered in authentic tool usage scenarios, encompassing instances that involve various types of errors, such as invalid actions, parsing errors, and incorrect parameters.</li>
</ul>
<h2>4.3 Quality</h2>
<p>To evaluate the quality of ToolAlpaca corpus, we randomly sample 100 instances and engage a human annotator for assessment. The evaluation tests the solvability of the instructions generated by the user agent, the precision of the output from the tool executor agent, and the accuracy of the assistant agent's actions and responses. As illustrated in Table 2, we observe that the metrics for assessing the capabilities of the three agents all exceed 80%. This substantiates that each agent is proficient in their respective roles, demonstrating the reliability of data constructed based on simulation and affirming the decent quality of our dataset.</p>
<table>
<thead>
<tr>
<th>Quality</th>
<th>Yes%</th>
</tr>
</thead>
<tbody>
<tr>
<td>solvability of instructions</td>
<td>88%</td>
</tr>
<tr>
<td>effectiveness of Tool agent's response</td>
<td>92%</td>
</tr>
<tr>
<td>accuracy of action sequences and final output</td>
<td>80%</td>
</tr>
</tbody>
</table>
<p>Table 2: Data quality review for ToolAlpaca corpus.</p>
<h2>5 Experiment</h2>
<p>In this section, we investigate whether a set of simulated data can empower compact language models to acquire generalized tool-use capabilities. To verify this, we conduct zero-shot experiments on various tools that have not appeared in the training set, ranging from simulated tools, real-world tools, to out-of-dataset multi-modal tools. Furthermore, we investigate how the diversity of the toolset impacts the generalized tool-use ability of language models.</p>
<h2>5.1 Experimental Settings</h2>
<p><strong>Training</strong> We fine-tune Vicuna models (Vicuna-7B and Vicuna-13B) on ToolAlpaca corpus. The fine-tuning process consists of three epochs, with a batch size of 128 and a learning rate of 2e-5.</p>
<p><strong>Evaluation</strong> To measure the generalized tool-use ability of the language model, we create an evaluation dataset through our data generation framework and manually annotate the data. This evaluation dataset consists of two subsets: (1) a simulated subset that includes 10 simulated tools, which were not part of the training toolset; (2) a real-world subset comprising 11 real-world APIs from various domains, designed to assess the divergence between our simulated data and real-world data.</p>
<p>To evaluate the models, we utilize GPT-4 for machine evaluation across all experiments, with an additional manual evaluation conducted specifically for the simulated subset. We prompt GPT-4 with the tool documentation and the standard answer from the human annotator and expect it to evaluate the performance in the following aspects:</p>
<ul>
<li><strong>Procedure:</strong> This metric evaluates the model's proficiency in accurately selecting suitable actions, utilizing correct parameters, and avoiding redundant actions.</li>
<li><strong>Response:</strong> This criterion measures whether the final response can satisfy the user's instruction.</li>
<li><strong>Overall:</strong> This metric evaluates the whole process, requiring the correctness of procedure and response.</li>
</ul>
<h2>5.2 Results</h2>
<p><strong>Effectiveness of ToolAlpaca corpus.</strong> Table 3 presents the main results from the simulated set, evidencing that fine-tuning on ToolAlpaca corpus can foster generalized tool learning for compact models. Without fine-tuning on our</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Simulated Tools</th>
<th></th>
<th></th>
<th></th>
<th>Real-world APIs</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Procedure</td>
<td>Response</td>
<td>Overall</td>
<td>Human</td>
<td>Procedure</td>
<td>Response</td>
<td>Overall</td>
</tr>
<tr>
<td>GPT-3.5</td>
<td>77.0</td>
<td>85.0</td>
<td>75.0</td>
<td>79.0</td>
<td>75.4</td>
<td>80.7</td>
<td>72.8</td>
</tr>
<tr>
<td>Vicuna-7B</td>
<td>19.0</td>
<td>21.0</td>
<td>17.0</td>
<td>16.0</td>
<td>7.9</td>
<td>11.4</td>
<td>7.9</td>
</tr>
<tr>
<td>ToolAlpaca-7B</td>
<td>63.0</td>
<td>69.0</td>
<td>60.0</td>
<td>73.0</td>
<td>63.2</td>
<td>57.9</td>
<td>55.3</td>
</tr>
<tr>
<td>Vicuna-13B</td>
<td>17.0</td>
<td>31.0</td>
<td>16.0</td>
<td>25.0</td>
<td>13.2</td>
<td>16.7</td>
<td>12.3</td>
</tr>
<tr>
<td>ToolAlpaca-13B</td>
<td>70.0</td>
<td>73.0</td>
<td>70.0</td>
<td>75.0</td>
<td>66.7</td>
<td>67.5</td>
<td>61.4</td>
</tr>
</tbody>
</table>
<p>Table 3: Evaluation results on unseen simulated tools and real-world APIs. We can observe that after training on our corpus, ToolAlpacas performance significantly surpasses that of the Vicuna model, reaching comparable performance with GPT-3.5.
corpus, Vicuna models demonstrate constrained tool-use capabilities, with the human accept rate of 16 and 25, respectively. These statistics emphasize the existing compact models insufficiency in achieving the generalized tool-use capacity like larger models. Nevertheless, our ToolAlpaca models can attain 73 (+57) and 75 (+50) accept rates, respectively. ToolAlpaca-13B even achieves comparable performance to GPT-3.5. This evidences the feasibility of instilling generalized tool-use capabilities into compact language models by only training on 3000 instances generated by our framework. Furthermore, the consistency between the human accept rate and the overall accuracy proves that machine evaluation can serve as a suitable evaluation method for this task.</p>
<p>Generalization on real-world tools. The effectiveness of our corpus is further validated through testing on real-world APIs, demonstrating that simulation serves as an exceptionally efficient data collection method. Table 3 exhibits the performance of ToolAlpaca on the real-world test set, where it achieves an overall accuracy of 55.3 and 61.4, respectively, significantly surpassing the performance of Vicuna models. This suggests that training on simulated data can indeed adapt to real-world tool usage scenarios. We attribute this to the current LLMs robust simulation capabilities, which provide compelling evidence for future simulation-based data construction.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>$SR_{t}$</th>
<th>$SR_{act}$</th>
<th>$SR_{args}$</th>
<th>$SR$</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-3.5</td>
<td>99.5</td>
<td>99.5</td>
<td>91.5</td>
<td>91.5</td>
</tr>
<tr>
<td>Vicuna-13B</td>
<td>84.4</td>
<td>43.7</td>
<td>46.7</td>
<td>26.2</td>
</tr>
<tr>
<td>GPT4Tools</td>
<td>98.2</td>
<td>97.0</td>
<td>92.2</td>
<td>90.6</td>
</tr>
<tr>
<td>ToolAlpaca-13B*</td>
<td>-</td>
<td>95.5</td>
<td>85.3</td>
<td>83.7</td>
</tr>
</tbody>
</table>
<p>Table 4: Evaluation results on unseen tools from GPT4Tools test set. Metrics: successful rate of thought, action, arguments, and the entire instance. We can observe that ToolAlpaca, with 3.9k cases, reaches the same performance level as GPT4Tools, which has been trained on 71k instances generated by the same process with the test set. *: As our training set does not include data not involving tool use, we exclude 50 out of 652 test cases that do not involve tool usage.</p>
<p>Moreover, to evaluate ToolAlpacas generalization on out-of-dataset scenarios, we conduct experiments on GPT4Tools<em>Yang et al. (2023)</em> test set, which encompasses 8 multi-modal tools. As shown in Table 4, ToolAlpaca, trained on merely 3.9k cases, demonstrate 83.7 success rate on out-</p>
<p>Figure 5: Performance variation with the increasing toolset diversity.
of-dataset evaluation, which is close to GPT4Tools, trained on 71k instances constructed with the same process. This observation indicates that the language model can invoke out-of-dataset tools after training on ToolAlpaca corpus. We speculate that the performance may be attributed to the diversity of instances and toolset, and we delve into it in the subsequent experiment.</p>
<p>Impact of diversity. The diversity of the dataset is crucial for the generalization of tool learning. To investigate this, we maintain the number of instances and construct datasets on 10, 40, 100, and 400 tools, respectively. Subsequently, we fine-tune Vicuna-13B on these datasets with the same experimental settings and utilize GPT-4 to evaluate the validation set. As shown in Figure 5, as the diversity of the toolset increases, the performance on the validation set gradually improves. Specifically, training with a dataset of 10 different tools resulted in a mere 51 overall accuarcy. In contrast, when the variety of tools increases to 400 and keeps the number of instances, the performance escalates to 70. This finding highlights the significant role of toolset diversity in generalizing tool learning. This provides valuable insight for the construction of datasets for generalized ability learning.</p>
<h2>6 Conclusion</h2>
<p>In this paper, we introduce ToolAlpaca, an automated framework designed to improve the generalized tool-use capability of language models. Specifically, we first create a comprehensive corpus spanning a broad range of tools with various usage instances. Subsequently, this corpus serves as the basis for fine-tuning compact language models, leading to the generation of the ToolAlpaca models. Experimental re-</p>
<p>sults indicate that ToolAlpaca performs comparably to GPT3.5 in generalized tool-use scenarios. This finding not only substantiates the potential of our data generation framework but also highlights the feasibility of mastering generalized tool use in compact-size models.</p>
<h2>References</h2>
<p>Agrawal, P.; Alberti, C.; Huot, F.; Maynez, J.; Ma, J.; Ruder, S.; Ganchev, K.; Das, D.; and Lapata, M. 2022. QAmeleon: Multilingual QA with Only 5 Examples. arXiv:2211.08264.
Chen, M.; Chen, X.; and tau Yih, W. 2023. Efficient Open Domain Multi-Hop Question Answering with FewShot Data Synthesis. arXiv:2305.13691.
Chen, W.; Ma, X.; Wang, X.; and Cohen, W. W. 2022. Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. arXiv:2211.12588.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y.; Wu, Z.; Zhang, H.; Zheng, L.; Zhuang, S.; Zhuang, Y.; Gonzalez, J. E.; Stoica, I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with $90 \%$ ChatGPT Quality.
Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Y.; Callan, J.; and Neubig, G. 2022. PAL: Program-aided Language Models. arXiv e-prints, arXiv:2211.10435.
Honovich, O.; Scialom, T.; Levy, O.; and Schick, T. 2022. Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor. arXiv:2212.09689.
Khattab, O.; Santhanam, K.; Li, X. L.; Hall, D.; Liang, P.; Potts, C.; and Zaharia, M. 2023. Demonstrate-SearchPredict: Composing retrieval and language models for knowledge-intensive NLP. arXiv:2212.14024.
Komeili, M.; Shuster, K.; and Weston, J. 2022. InternetAugmented Dialogue Generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 8460-8478. Dublin, Ireland: Association for Computational Linguistics.
Li, M.; Song, F.; Yu, B.; Yu, H.; Li, Z.; Huang, F.; and Li, Y. 2023. API-Bank: A Benchmark for Tool-Augmented LLMs. arXiv:2304.08244.
Lu, P.; Peng, B.; Cheng, H.; Galley, M.; Chang, K.-W.; Wu, Y. N.; Zhu, S.-C.; and Gao, J. 2023. Chameleon: Plug-andPlay Compositional Reasoning with Large Language Models. arXiv:2304.09842.
Mialon, G.; Dess, R.; Lomeli, M.; Nalmpantis, C.; Pasunuru, R.; Raileanu, R.; Rozire, B.; Schick, T.; DwivediYu, J.; Celikyilmaz, A.; Grave, E.; LeCun, Y.; and Scialom, T. 2023. Augmented Language Models: a Survey. arXiv:2302.07842.
OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
Parisi, A.; Zhao, Y.; and Fiedel, N. 2022. TALM: Tool Augmented Language Models. CoRR, abs/2205.12255.
Patil, S. G.; Zhang, T.; Wang, X.; and Gonzalez, J. E. 2023. Gorilla: Large Language Model Connected with Massive APIs. arXiv:2305.15334.</p>
<p>Qin, Y.; Hu, S.; Lin, Y.; Chen, W.; Ding, N.; Cui, G.; Zeng, Z.; Huang, Y.; Xiao, C.; Han, C.; Fung, Y. R.; Su, Y.; Wang, H.; Qian, C.; Tian, R.; Zhu, K.; Liang, S.; Shen, X.; Xu, B.; Zhang, Z.; Ye, Y.; Li, B.; Tang, Z.; Yi, J.; Zhu, Y.; Dai, Z.; Yan, L.; Cong, X.; Lu, Y.; Zhao, W.; Huang, Y.; Yan, J.; Han, X.; Sun, X.; Li, D.; Phang, J.; Yang, C.; Wu, T.; Ji, H.; Liu, Z.; and Sun, M. 2023a. Tool Learning with Foundation Models. arXiv:2304.08354.
Qin, Y.; Liang, S.; Ye, Y.; Zhu, K.; Yan, L.; Lu, Y.; Lin, Y.; Cong, X.; Tang, X.; Qian, B.; Zhao, S.; Tian, R.; Xie, R.; Zhou, J.; Gerstein, M.; Li, D.; Liu, Z.; and Sun, M. 2023b. ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs. arXiv:2307.16789.
Schick, T.; Dwivedi-Yu, J.; Dess, R.; Raileanu, R.; Lomeli, M.; Zettlemoyer, L.; Cancedda, N.; and Scialom, T. 2023. Toolformer: Language Models Can Teach Themselves to Use Tools. CoRR, abs/2302.04761.
Schick, T.; and Schtze, H. 2021. Generating Datasets with Pretrained Language Models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 6943-6951. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.
Shen, Y.; Song, K.; Tan, X.; Li, D.; Lu, W.; and Zhuang, Y. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face. arXiv:2303.17580.
Song, Y.; Xiong, W.; Zhu, D.; Li, C.; Wang, K.; Tian, Y.; and Li, S. 2023. RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs. arXiv:2306.06624.
Taori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y.; Li, X.; Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https: //github.com/tatsu-lab/stanford_alpaca.
Wang, S.; Liu, Y.; Xu, Y.; Zhu, C.; and Zeng, M. 2021. Want To Reduce Labeling Cost? GPT-3 Can Help. In Findings of the Association for Computational Linguistics: EMNLP 2021, 4195-4205. Punta Cana, Dominican Republic: Association for Computational Linguistics.
Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N. A.; Khashabi, D.; and Hajishirzi, H. 2023. Self-Instruct: Aligning Language Models with Self-Generated Instructions. arXiv:2212.10560.
Xu, Q.; Hong, F.; Li, B.; Hu, C.; Chen, Z.; and Zhang, J. 2023. On the Tool Manipulation Capability of Open-source Large Language Models. arXiv:2305.16504.
Yang, R.; Song, L.; Li, Y.; Zhao, S.; Ge, Y.; Li, X.; and Shan, Y. 2023. GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction. arXiv:2305.18752.
Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.; Narasimhan, K.; and Cao, Y. 2022. ReAct: Synergizing Reasoning and Acting in Language Models. arXiv preprint arXiv:2210.03629.
Zhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y.; Ma, X.; Efrat, A.; Yu, P.; Yu, L.; Zhang, S.; Ghosh, G.; Lewis, M.; Zettlemoyer, L.; and Levy, O. 2023. LIMA: Less Is More for Alignment. arXiv:2305.11206.</p>
<h1>A Implementation Details</h1>
<p>In this section, we show the details of prompt templates in ToolAlpaca. Figure 6, Figure 7, and Figure 8 delineate the prompts employed during toolset construction. Figure 9 and Figure 10 illustrate the corresponding prompts for user agent's two responsibility, generating user instructions and providing missing information. The prompts designed for to the assistant agent and the tool executor agent are detailed in Figure 11 and Figure 12. "\$ {...} " within the prompts are placeholders, will be replaced by real variables during the generation process.</p>
<h2>B Experiment Details</h2>
<h2>B. 1 Hyperparameters</h2>
<p>The fine-tuning configuration for ToolAlpaca is recorded in Table 5.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Hyperparameters</th>
<th style="text-align: center;">Value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">optimizer</td>
<td style="text-align: center;">AdamW</td>
</tr>
<tr>
<td style="text-align: left;">learning rate</td>
<td style="text-align: center;">$2 \mathrm{e}-5$</td>
</tr>
<tr>
<td style="text-align: left;">weight decay</td>
<td style="text-align: center;">0.0</td>
</tr>
<tr>
<td style="text-align: left;">warmup ratio</td>
<td style="text-align: center;">0.03</td>
</tr>
<tr>
<td style="text-align: left;">lr scheduler type</td>
<td style="text-align: center;">cosine</td>
</tr>
<tr>
<td style="text-align: left;">num train epochs</td>
<td style="text-align: center;">3</td>
</tr>
<tr>
<td style="text-align: left;">batch size</td>
<td style="text-align: center;">128</td>
</tr>
<tr>
<td style="text-align: left;">max length</td>
<td style="text-align: center;">2048</td>
</tr>
</tbody>
</table>
<p>Table 5: The fine-tuning configuration for ToolAlpaca.</p>
<h2>B. 2 Evaluation Dataset Details</h2>
<p>To evaluate the generalized tool-use ability, we construct the evaluation dataset via our framework, which consists two subsets: a simulated subset with 10 simulated tools and 100 instances, a real-world subset with 11 real-world APIs and 114 instances. The toolset used in the evaluation datset is detailed in Table 6.</p>
<h2>B. 3 Evaluation Prompt</h2>
<p>Following the evaluation method used by Vicuna (Chiang et al. 2023), we use GPT-4 as our evaluator. The evaluation prompt is shown in Figure 13.</p>
<h2>B. 4 Case Study</h2>
<p>Through training on a set of diverse simulated tool-use instances, ToolAlpaca can equip various tools, even real-world APIs, some selected cases are shown in Figure 14, Figure 15 and Figure 16.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Name</th>
<th style="text-align: center;">Category</th>
<th style="text-align: left;">Introduction</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Simulated Tools</td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Axolotl</td>
<td style="text-align: center;">Animals</td>
<td style="text-align: left;">Collection of axolotl pictures and facts</td>
</tr>
<tr>
<td style="text-align: left;">AniAPI</td>
<td style="text-align: center;">Anime</td>
<td style="text-align: left;">Anime discovery, streaming \&amp; syncing with trackers</td>
</tr>
<tr>
<td style="text-align: left;">AbuselPDB</td>
<td style="text-align: center;">Anti-Malware</td>
<td style="text-align: left;">IP/domain/URL reputation</td>
</tr>
<tr>
<td style="text-align: left;">Amthyste</td>
<td style="text-align: center;">Art \&amp; Design</td>
<td style="text-align: left;">Generate images for Discord users</td>
</tr>
<tr>
<td style="text-align: left;">Auth0</td>
<td style="text-align: center;">Authentication \&amp; Authorization</td>
<td style="text-align: left;">Easy to implement, adaptable authentication and authorization platform</td>
</tr>
<tr>
<td style="text-align: left;">Abstract Public Holidays</td>
<td style="text-align: center;">Calendar</td>
<td style="text-align: left;">Data on national, regional, and religious holidays via API</td>
</tr>
<tr>
<td style="text-align: left;">1Forge</td>
<td style="text-align: center;">Currency Exchange</td>
<td style="text-align: left;">Forex currency market data</td>
</tr>
<tr>
<td style="text-align: left;">A Bblia Digital</td>
<td style="text-align: center;">Books</td>
<td style="text-align: left;">Do not worry about managing the multiple versions of the Bible</td>
</tr>
<tr>
<td style="text-align: left;">Apache Superset</td>
<td style="text-align: center;">Business</td>
<td style="text-align: left;">API to manage your BI dashboards and data sources on Superset</td>
</tr>
<tr>
<td style="text-align: left;">Lob.com</td>
<td style="text-align: center;">Data Validation</td>
<td style="text-align: left;">US Address Verification</td>
</tr>
<tr>
<td style="text-align: left;">Real-world APIs</td>
<td style="text-align: center;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;">Nager.Date</td>
<td style="text-align: center;">Calendar</td>
<td style="text-align: left;">Public holidays for more than 90 countries</td>
</tr>
<tr>
<td style="text-align: left;">airportsapi</td>
<td style="text-align: center;">Transportation</td>
<td style="text-align: left;">Get name and website-URL for airports by ICAO code</td>
</tr>
<tr>
<td style="text-align: left;">AviationAPI</td>
<td style="text-align: center;">Transportation</td>
<td style="text-align: left;">FAA Aeronautical Charts and Publications, Airport Information, and Airport Weather</td>
</tr>
<tr>
<td style="text-align: left;">chucknorris.io</td>
<td style="text-align: center;">Entertainment</td>
<td style="text-align: left;">JSON API for hand curated Chuck Norris jokes</td>
</tr>
<tr>
<td style="text-align: left;">Random Useless Facts</td>
<td style="text-align: center;">Entertainment</td>
<td style="text-align: left;">Get useless, but true facts</td>
</tr>
<tr>
<td style="text-align: left;">apilayer weatherstack</td>
<td style="text-align: center;">Weather</td>
<td style="text-align: left;">Real-Time \&amp; Historical World Weather Data API</td>
</tr>
<tr>
<td style="text-align: left;">Free Dictionary</td>
<td style="text-align: center;">Dictionaries</td>
<td style="text-align: left;">Definitions, phonetics, pronounciations, parts of speech, examples, synonyms</td>
</tr>
<tr>
<td style="text-align: left;">WolframAlpha</td>
<td style="text-align: center;">Machine Learning</td>
<td style="text-align: left;">Provides specific answers to questions using data and algorithms</td>
</tr>
<tr>
<td style="text-align: left;">Fruityvice</td>
<td style="text-align: center;">Food \&amp; Drink</td>
<td style="text-align: left;">Data about all kinds of fruit</td>
</tr>
<tr>
<td style="text-align: left;">Cataas</td>
<td style="text-align: center;">Animals</td>
<td style="text-align: left;">Cat as a service (cats pictures and gifs)</td>
</tr>
<tr>
<td style="text-align: left;">CurrencyBeacon</td>
<td style="text-align: center;">Currency Exchange</td>
<td style="text-align: left;">Real-time and historical currency rates JSON API</td>
</tr>
</tbody>
</table>
<p>Table 6: Tools used in our evaluation dataset.</p>
<h1>Toolset Construction - Description Prompt</h1>
<p>I will provide the API's name, link, and brief introduction. You need to generate a detailed description for the API.</p>
<h2>Guidelines:</h2>
<ol>
<li>Write a general overview of the API's purpose and functionality.</li>
<li>List and briefly describe all features provided by the API, ensuring each feature has a clear and distinct purpose with low coupling between them.</li>
<li>Use clear, concise language and avoid jargon, keeping the description under 300 tokens in length.
<API>
Name: AdoptAPet
Link: https://www.adoptapet.com/public/apis/pet_list.html
Introduction: Resource to help get pets adopted
Description: The Adopt-a-Pet.com API (Application Programming Interface) is a series of tools that allows partners to use Adopt-a-Pet.com's pet adoption features and pet data in other applications. It provides the following features: 1) Retrieve the list of available pets for the shelter or rescue. 2) Retrieve the details for a specific pet.
</API>
<API>
Name: \${name}
Link: \${link}
Introduction: \${introduction}
Description:</li>
</ol>
<p>Figure 6: Description generation prompt.</p>
<h1>Toolset Construction - Function Documentation Prompt</h1>
<p>You are given the name, link, and description of an API. Your task is to create a comprehensive introduction for this API.</p>
<h2>Guidelines:</h2>
<ol>
<li>For each function of the API, detail its purpose, input requirements, and output results.</li>
<li>For function input, present it in JSON format. Each key should be the input parameter's name, and its value should be a string indicating whether it's required or not, its type, and a brief description, such as "Required/Optional. Integer. {some description}".</li>
<li>Do not design functions that return excessive data, such as 'getAllXxx'. If such a function is necessary, incorporate input parameters to limit, filter, or paginate the results.</li>
<li>Limit the number of functions generated. Only generate functions based on the API Description. Do not create unnecessary functions that overcomplicate the API.</li>
<li>If any API function requires fields that are not directly accessible to the users (like IDs, internal codes, etc.) as inputs, there must be corresponding methods for users to retrieve these values, such as through 'search' or 'list' functions.</li>
<li>Output with the following format:
{index}. Name: {function name, follow the camel case naming convention.}
Description: {function short description}
Input: {function input, presented as a single line without any formatting}
Output: {function output, describe all the information that this function will return}</li>
</ol>
<p>Begin!
Name: \${name}
Link: \${link}
Description: \${description}
Functions:</p>
<p>Figure 7: Function documentation generation prompt.</p>
<h1>Toolset Construction - OpenAPI Specification Prompt</h1>
<div class="codehilite"><pre><span></span><code><span class="n">Please</span><span class="w"> </span><span class="n">generate</span><span class="w"> </span><span class="n">API</span><span class="w"> </span><span class="n">documentation</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">conforms</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">OpenAPI</span>
<span class="n">Specification</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">provided</span><span class="w"> </span><span class="n">API</span><span class="p">,</span><span class="w"> </span><span class="n">following</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">guidelines</span><span class="p">:</span>
<span class="mf">1.</span><span class="w"> </span><span class="n">Name</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">API</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="s1">&#39;title&#39;</span><span class="w"> </span><span class="n">field</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="s1">&#39;info&#39;</span><span class="w"> </span><span class="n">section</span><span class="p">,</span><span class="w"> </span><span class="ow">and</span>
<span class="n">include</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="s1">&#39;version&#39;</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="s1">&#39;description&#39;</span><span class="w"> </span><span class="n">field</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">describe</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">API</span><span class="s1">&#39;s</span>
<span class="n">purpose</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">functionality</span><span class="w"> </span><span class="n">succinctly</span><span class="o">.</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">Exclude</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="s1">&#39;tags&#39;</span><span class="w"> </span><span class="n">field</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">specification</span><span class="o">.</span>
<span class="mf">3.</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">function</span><span class="p">:</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">Design</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">endpoint</span><span class="p">,</span><span class="w"> </span><span class="n">adhering</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">its</span><span class="w"> </span><span class="n">definition</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">input</span><span class="o">/</span><span class="n">output</span>
<span class="w">    </span><span class="n">requirements</span><span class="o">.</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">Use</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">function</span><span class="s1">&#39;s name in the &#39;</span><span class="n">operationId</span><span class="s1">&#39; field. Decompose the</span>
<span class="w">    </span><span class="n">description</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">function</span><span class="w"> </span><span class="n">into</span><span class="w"> </span><span class="n">appropriate</span><span class="w"> </span><span class="n">fields</span><span class="o">.</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">endpoint</span><span class="s1">&#39;s input, provide additional details in the</span>
<span class="w">    </span><span class="s1">&#39;parameters&#39;</span><span class="w"> </span><span class="n">section</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">complement</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">function</span><span class="s1">&#39;s input</span>
<span class="w">    </span><span class="n">requirements</span><span class="o">.</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">instance</span><span class="p">,</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="s1">&#39;enum&#39;</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">specify</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">parameter</span>
<span class="w">    </span><span class="n">values</span><span class="o">.</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">Generate</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">detailed</span><span class="w"> </span><span class="n">model</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">endpoint</span><span class="s1">&#39;s response, including</span>
<span class="w">    </span><span class="n">status</span><span class="w"> </span><span class="n">codes</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">structured</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="n">values</span><span class="o">.</span><span class="w"> </span><span class="n">This</span><span class="w"> </span><span class="n">should</span><span class="w"> </span><span class="n">base</span><span class="w"> </span><span class="n">on</span><span class="w"> </span><span class="n">the</span>
<span class="w">    </span><span class="n">function</span><span class="s1">&#39;s output description, detailing each field whenever</span>
<span class="w">    </span><span class="n">possible</span><span class="o">.</span>
<span class="w">    </span><span class="o">-</span><span class="w"> </span><span class="n">If</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">endpoint</span><span class="s1">&#39;s input includes fields unknown to the user, like</span>
<span class="w">    </span><span class="n">IDs</span><span class="p">,</span><span class="w"> </span><span class="n">these</span><span class="w"> </span><span class="n">fields</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">be</span><span class="w"> </span><span class="n">included</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">responses</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">relevant</span>
<span class="w">    </span><span class="s1">&#39;search&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;list&#39;</span><span class="p">,</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">similar</span><span class="w"> </span><span class="n">endpoints</span><span class="o">.</span>
<span class="mf">4.</span><span class="w"> </span><span class="n">Include</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="s1">&#39;description&#39;</span><span class="w"> </span><span class="n">field</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">input</span><span class="w"> </span><span class="n">parameter</span><span class="w"> </span><span class="ow">and</span>
<span class="s1">&#39;requestBody&#39;</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">operation</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="n">explain</span><span class="w"> </span><span class="n">their</span><span class="w"> </span><span class="n">purpose</span><span class="w"> </span><span class="ow">and</span>
<span class="n">usage</span><span class="o">.</span>
<span class="mf">5.</span><span class="w"> </span><span class="n">Ensure</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">OpenAPI</span><span class="w"> </span><span class="n">Specification</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">comprehensive</span><span class="p">,</span><span class="w"> </span><span class="n">capturing</span><span class="w"> </span><span class="n">all</span>
<span class="n">functions</span><span class="w"> </span><span class="n">mentioned</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">API</span><span class="w"> </span><span class="n">Introduction</span><span class="o">.</span>
<span class="mf">6.</span><span class="w"> </span><span class="n">For</span><span class="w"> </span><span class="n">parameters</span><span class="o">/</span><span class="n">schemas</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="s1">&#39;type&#39;</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="s1">&#39;object&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">you</span><span class="w"> </span><span class="n">must</span><span class="w"> </span><span class="n">include</span>
<span class="n">their</span><span class="w"> </span><span class="n">properties</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">specification</span><span class="o">.</span>
<span class="n">Name</span><span class="p">:</span><span class="w"> </span><span class="o">$</span><span class="p">{</span><span class="n">name</span><span class="p">}</span>
<span class="n">Link</span><span class="p">:</span><span class="w"> </span><span class="o">$</span><span class="p">{</span><span class="n">link</span><span class="p">}</span>
<span class="n">Description</span><span class="p">:</span><span class="w"> </span><span class="o">$</span><span class="p">{</span><span class="n">description</span><span class="p">}</span>
<span class="n">Functions</span><span class="p">:</span><span class="w"> </span><span class="o">$</span><span class="p">{</span><span class="n">functions</span><span class="p">}</span>
<span class="n">OpenAPI</span><span class="w"> </span><span class="n">Spec</span><span class="p">(</span><span class="n">Format</span><span class="w"> </span><span class="n">with</span><span class="w"> </span><span class="n">JSON</span><span class="p">,</span><span class="w"> </span><span class="n">indent</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
</code></pre></div>

<p>Figure 8: Openapi specification generation prompt.</p>
<h1>User Agent - Instruction Prompt</h1>
<p>Imagine that you are a user who wants to utilize the features provided by various APIs in your daily life. Your task is to come up with realistic scenarios for using these APIs and express them as natural language instructions, as if you were asking a friend or assistant for help.</p>
<p>Please follow these guidelines:</p>
<ol>
<li>The instructions should be 1 to 2 sentences long. Use a mix of interrogative sentences, first-person statements, imperative sentences, and other structures that convey a request. Aim for diversity in your instructions.</li>
<li>Do not mention the API's name in your instructions.</li>
<li>Your instructions should only involve the features provided by these APIs. The instructions that need multiple times of API call is better.</li>
<li>Generate 10 diverse instructions.</li>
<li>Use specific nouns and real-world examples from various domains, such as entertainment, sports, or technology. Avoid using any form of placeholder or generic phrases, such as "this xxx", "a xxx" or "a specific xxx", and provide concrete details instead.</li>
<li>Try not to repeat the verb for each instruction to maximize diversity.</li>
<li>Ensure diversity in language by combining questions with imperative statements and other structures that convey a request.
<API>
Name: \${name}
Description: \${description}
API Functions: \${functions}
</API>
Based on the API provided above, generate 10 natural language instructions with specific examples and diverse language, following the guidelines.</li>
</ol>
<p>Figure 9: User agent prompt 1 for instruction generation.</p>
<h2>User Agent - Additional Information Prompt</h2>
<p>As a user, you ask the AI assistant some questions, but the assistant believes you have missed crucial information. Please respond to the AI assistant's inquiries with specific and direct answers using text or formatted text. Avoid using placeholders. If a file is necessary, please provide the contents of the file in your response. your response should startswith "[User]:".
\${interaction_history}</p>
<p>Figure 10: User agent prompt 2 for providing missing information.</p>
<h1>Assistant Agent</h1>
<p>Your task is to answer the user's question using available tools. The user cannot see or use the tools themselves, nor can they know the process of your tool usage. Provide all necessary information in the "Final Answer" field. Do not make up any information. If required parameters are missing, use the "getDetails" tool to ask the user for them.
You have access to the following tools:
\${tool_list}
Use the following format:
Question: the input question you must answer
Thought: you should always think about what to do
Action: the action to take, should be one of [\${tool_names}].
Action Input: the input to the action, must be in JSON format. All of the action input must be realistic and from the user.
Observation: the result of the action
... (this Thought/Action/Action Input/Observation can repeat N times)
Thought: Summarize the information gathered and the reasoning behind your final answer.
Final Answer: Provide a user-friendly and detailed answer to the original input question that summarizes all relevant information from the Thought/Action/Action Input/Observation sequences.</p>
<p>Begin!
Question: \${instruction}
Thought: \${agent_scratchpad}</p>
<p>Figure 11: Assistant agent prompt.</p>
<h1>Tool Executor Agent</h1>
<p>As an API simulator, your task is to process API requests and generate appropriate responses based on the provided API documentation. Please adhere to the following guidelines:</p>
<ol>
<li>Validate the HTTP method and parameters in the request according to the OpenAPI Spec.</li>
<li>Generate a response that strictly adheres to the specified format in the OpenAPI Spec, and ensure it's in JSON format.</li>
<li>Responses should contain realistic data. Avoid using placeholders.</li>
<li>Handle edge cases by providing appropriate error responses.</li>
<li>For requests without any length limit, ensure to return at least 3 samples in the response.</li>
</ol>
<p>Output Format:
Status Code: Include the HTTP response status code.
Response: Ensure your response is in JSON format, contains realistic data, and aligns with the OpenAPI Spec format.
Explanation: Provide a brief explanation for the given response.
Avoid any extraneous content in your output.
API Details:
Name: \${name}
OpenAPI Spec: \${openapi_spec}
Request Details:
\${request}</p>
<p>Figure 12: Tool executor agent prompt.</p>
<p>Given the documentation of a REST API and a task instruction, I need you to evaluate whether the solution provided by my AI assistant aligns with the standard answer.</p>
<p>Follow these guidelines:</p>
<ol>
<li>You need to assess both the process and final response of the AI assistant's solution.</li>
<li>
<p>For the process, refer to the standard answer:</p>
</li>
<li>
<p>The standard answer only includes function names and parameters, while the AI assistant's solution also includes function returns. Therefore, it is acceptable to adjust the call situation based on the function return, such as retrying when the function errors, calling function <code>getDetails</code> for more information, and calling function <code>retrievalDataFromFile</code> when the function's return is too long.</p>
</li>
<li>Random calls to unrelated functions are not allowed.</li>
<li>The solution must contain all the steps in the standard answer.</li>
<li>
<p>The necessary parameters of the function need to be consistent with the standard answer. Parameters not mentioned in the instruction can be inconsistent.</p>
</li>
<li>
<p>You need to comprehensively judge whether the final response of the solution accurately summarizes the entire call process and provides a reasonable response to the initial instruction.</p>
</li>
<li>You need to first analyze the entire solution according to the guidelines, then give your answer. Your output should adhere to the format:
## Analysis
{some analysis}
## Results
Process Correctness: one of [Yes, No, Uncertain]
Final Response Correctness: one of [Yes, No, Uncertain]
## Documentation
\${openapi_spec}
## Task Instruction
\${instruction}
## Standard Answer
\${standard_answer}
## AI Assistant's Solution
\${solution}
## Analysis</li>
</ol>
<p>Figure 13: Evaluation prompt for GPT-4.</p>
<h1>Case For Real-world API: Nager.Date</h1>
<p>[User]: I'm curious about the public holidays in Australia in 2023. Can you fetch the list for me?
[Assistant]:
Thought: I need to use the PublicHolidayPublicHolidaysV3 tool to get the list of public holidays in Australia in 2023.
Action: PublicHolidayPublicHolidaysV3
Action Input: {"countryCode": "AU", "year": 2023}
[Tool]: The output is too long. You need to use the
'retrievalDataFromFile' function to retrieve the output from the file: ./tmp/retrieval_1692114481.txt.
[Assistant]:
Thought: I need to use the retrievalDataFromFile tool to retrieve the output from the file.
Action: retrievalDataFromFile
Action Input: {"file_path": "./tmp/retrieval_1692114481.txt", "query": "public holidays in Australia in 2023"}
[Tool]: {"retrieved_info": "Public Holidays in Australia in 2023:\n1. New Year's Day - January 1st\n2. Australia Day - January 26th\n3. Labour Day - March 6th (observed in AU-WA) \n4. Canberra Day - March 13th (observed in AU-ACT) \n5. March Public Holiday - March 13th (observed in AU-SA) \n6. Eight Hours Day - March 13th (observed in AU-TAS) \n7. Labour Day - March 13th (observed in AU-VIC) \n8. Good Friday - April 7th\n9. Holy Saturday (Easter Eve) - April 8th (observed in AU-ACT, AU-NSW, AU-NT, AU-QLD, AU-SA, AU-VIC) \n10. Easter Sunday - April 9th (observed in AU-ACT, AU-NSW, AU-VIC, AU-WA) \n11. Easter Monday - April 10th\n12. Anzac Day - April 25th\n13. May Day - May 1st (observed in AU-NT) \n14. Labour Day - May 1st (observed in AU-QLD) \n15. Reconciliation Day - May 29th (observed in AU-ACT) \n16. Western Australia Day - June 5th (observed in AU-WA) \n17. King's Birthday - June 12th (observed in AU-ACT, AU-NSW, AU-NT, AU-SA, AU-TAS, AU-VIC) \n18. Picnic Day - August 7th (observed in AU-NT) \n19. Labour Day - October 2nd (observed in AU-ACT, AU-NSW, AU-SA) \n20. Christmas Day - December 25th\n21. Boxing Day - December 26th"}
[Assistant]:
Thought: The output from the file contains the list of public holidays in Australia in 2023.
Response: The public holidays in Australia in 2023 are New Year's Day (January 1st), Australia Day (January 26th), Labour Day (March 6th), Canberra Day (March 13th), March Public Holiday (March 13th), Eight Hours Day (March 13th), Labour Day (March 13th), Good Friday (April 7th), Holy Saturday (Easter Eve) (April 8th), Easter Sunday (April 9th), Easter Monday (April 10th), Anzac Day (April 25th), May Day (May 1st), Labour Day (May 1st), Reconciliation Day (May 29th), Western Australia Day (June 5th), King's Birthday (June 12th), Picnic Day (August 7th), Labour Day (October 2nd), Christmas Day (December 25th), and Boxing Day (December 26th).</p>
<p>Figure 14: Cases for real-world API: Nager.Date.</p>
<h1>Case for Real-world API: Currency Beacon</h1>
<p>[User]: I'm a business owner and I need to invoice a client in Europe. Can you provide me with the current exchange rate from US dollars to Euros, and also the average rate for the July so I can set a fair price? Authentication information: api_key $=<em> * </em>$</p>
<h2>[Assistant]:</h2>
<p>Thought: I need to get the current exchange rate and the average rate for July.
Action: latest_get
Action Input: {"api_key": "<strong><em>", "base": "USD", "symbols": "EUR"}
[Tool]: Status Code: 200. Response: {"meta": {"code": 200,
"disclaimer": "Usage subject to terms:
https://currencybeacon.com/terms"}, "response": {"date":
"2023-08-15T16:46:58Z", "base": "USD", "rates": {"EUR": 0.91572671}},
"date": "2023-08-15T16:46:58Z", "base": "USD", "rates": {"EUR":
$0.91572671}}$
[Assistant]:
Thought: I have the current exchange rate from US dollars to Euros. Action: historical_get
Action Input: {"api_key": "</em></strong>", "base": "USD", "date": "2023-07-01", "symbols": "EUR"}
[Tool]: Status Code: 200. Response: {"meta": {"code": 200,
"disclaimer": "Usage subject to terms:
https://currencybeacon.com/terms"}, "response": {"date": "2023-07-01", "base": "USD", "rates": {"EUR": 0.91706262}}, "date": "2023-07-01", "base": "USD", "rates": {"EUR": 0.91706262}}
[Assistant]:
Thought: I have the average rate for July.
Response: The current exchange rate from US dollars to Euros is 0.91572671 , and the average rate for July was 0.91706262 .</p>
<p>Figure 15: Cases for real-world API: Currency Beacon.</p>
<h1>Case for Real-world API: apilayer weatherstack</h1>
<p>[User]: I'm planning a road trip from New York to Los Angeles next week. Can you help me check the weather conditions for the major cities along the route, like Chicago, Denver, and Las Vegas? I'd like to know if there will be any severe weather conditions that might affect my trip. Authentication information: access_key<strong><em>*
[Assistant]:
Thought: I need to get the weather conditions for the major cities along the route.
Action: current_get
Action Input: {"access_key": "</em></strong>", "query": "New York, Chicago, Denver, Las Vegas", "units": "m", "callback": "weatherResponse"}
[Tool]: Status Code: 200. Response: weatherResponse({"request": {"type": "City", "query": "Denver, United States of America", "language": "en", "unit": "m" }, "location": {"name": "Denver", "country": "United States of America", "region": "New York", "lat": "42.213", "lon": "-74.570", "timezone_id": "America/New_York", "localtime": "2023-08-15 12:18", "localtime_epoch": 1692101880, "utc_offset": "-4.0" }, "current": {"observation_time": "04:18 PM", "temperature": 17, "weather_code": 143, "weather_icons": ["https://cdn.worldweatheronline.com/images/wsymbols01_png_64/ wsymbol_0006_mist.png"], "weather_descriptions": ["Mist"], "wind_speed": 4, "wind_degree": 24, "wind_dir": "NNE", "pressure": 1009, "precip": 0, "humidity": 96, "cloudcover": 100, "feelslike": 17, "uv_index": 4, "visibility": 0, "is_day": "yes"}})
[Assistant]:
Thought: The response from the current_get tool indicates that the weather in Denver is currently misty with a temperature of $17^{\circ} \mathrm{C}$ Action: current_get
Action Input: {"access_key": "<strong><em>", "query": "Chicago, United States of America", "units": "m", "callback": "weatherResponse"}
[Tool]: Status Code: 200. Response: ...
[Assistant]:
Thought: The response from the current_get tool indicates that the weather in Chicago is currently lightly raining with a temperature of $19^{\circ} \mathrm{C}$.
Action: current_get
Action Input: {"access_key": "</em></strong>", "query": "Las Vegas, United States of America", "units": "m", "callback": "weatherResponse"}
[Tool]: Status Code: 200. Response: ...
[Assistant]:
Thought: The response from the current_get tool indicates that the weather in Las Vegas is currently sunny with a temperature of $34^{\circ} \mathrm{C}$. Response: The weather conditions for the major cities along the route from New York to Los Angeles are as follows: Denver: currently misty with a temperature of $17^{\circ} \mathrm{C}$, Chicago: currently lightly raining with a temperature of $19^{\circ} \mathrm{C}$, and Las Vegas: currently sunny with a temperature of $34^{\circ} \mathrm{C}$.</p>
<p>Figure 16: Cases for real-world API: apilayer weatherstack.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{3}$ Preliminary experiments demonstrated an occasional inability of ChatGPT to strictly adhere to the prescribed output formats.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
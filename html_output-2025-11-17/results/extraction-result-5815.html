<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-5815 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-5815</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-5815</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-116.html">extraction-schema-116</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <p><strong>Paper ID:</strong> paper-bc6ad001c395e92920839e45dfd7e05ce69405d2</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/bc6ad001c395e92920839e45dfd7e05ce69405d2" target="_blank">Machine Comprehension by Text-to-Text Neural Question Generation</a></p>
                <p><strong>Paper Venue:</strong> Rep4NLP@ACL</p>
                <p><strong>Paper TL;DR:</strong> A recurrent neural model is proposed that generates natural-language questions from documents, conditioned on answers, and fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality.</p>
                <p><strong>Paper Abstract:</strong> We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a question-answering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset SQuAD.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e5815.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e5815.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BeamSearch_vs_Sampling</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Beam-search generation vs random sampling during policy-gradient fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Using deterministic beam search to generate candidate sequences for policy-gradient (PG) updates (beam size 32) versus sampling from the model distribution; the paper finds beam search necessary to reliably improve rewards and stabilize RL fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-Decoder Question Generator (attention + pointer-softmax)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question generation (SQuAD) with policy-gradient fine-tuning</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate natural-language questions conditioned on a document and an answer; fine-tune the generator with REINFORCE using sequence-level rewards (QA F1, LM PPL, or combinations).</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Decoder output sequences are produced with beam search (beam size 32) and the resulting high-probability complete sequences are used as samples for the policy-gradient update (the model then 'replays' the beam sequence with teacher-forcing to reconstruct states for gradient computation).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Random (stochastic) sampling from the model distribution to produce sequences for policy-gradient updates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Qualitative: Beam-search enabled meaningful improvement of scalar rewards during policy-gradient fine-tuning; without beam-search the authors report they could not improve rewards through training (no numerical reward gains when using random sampling).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Sampling: no consistent improvement in rewards; Beam-search: enabled reward optimization and stabilized PG training (authors found beam-search required empirically).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved (beam search improved ability to optimize sequence-level rewards vs sampling)</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Beam search better approximates high-probability modes of the model distribution and reduces variance in reward estimates; using beam outputs and then teacher-forcing to recreate decoder states yields more stable and higher initial reward signals for PG updates.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Comprehension by Text-to-Text Neural Question Generation', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5815.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e5815.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Reward_Objective_Format</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Choice of sequence-level reward (QA F1, LM PPL, or combined) as an effective 'format' that shapes generated question presentation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Different scalar rewards used to fine-tune the generator (QA performance measured by a pretrained MPCM model, fluency measured by an LSTM language model perplexity, or combinations) produce markedly different question styles and downstream QA performance, trading off fluency and answerability and sometimes encouraging exploitative phrasing.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-Decoder Question Generator (pointer-softmax); evaluated with MPCM QA model and an LSTM Question LM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question generation (SQuAD) evaluated by QA F1 (MPCM) and fluency (LM PPL)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate questions conditioned on (document, answer). Evaluate question quality by (a) how well a pretrained QA model (MPCM) can recover the answer (token F1), and (b) language-model perplexity as a proxy for fluency.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Policy-gradient fine-tuning with reward = MPCM F1 (R_QA), or reward = negative LM perplexity (R_PPL), or a weighted linear combination R_PPL+QA.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Maximum-likelihood pretraining (teacher forcing) without PG; PG with R_PPL; PG with R_QA; PG with R_PPL+R_QA.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Key reported dev-set numbers (Table 2):
- Our System (pre-PG): MPCM F1 (QA) = 65.3; LM PPL = 175.7; BLEU = 10.2; question-ground-truth F1 = 39.5.
- + PG (R_PPL): QA = 61.1; PPL = 155.6; BLEU = 9.2; F1 = 38.2.
- + PG (R_QA): QA = 74.2; PPL = 300.9; BLEU = 10.5; F1 = 40.1.
- + PG (R_PPL+QA): QA = 70.2; PPL = 183.1; BLEU = 9.2; F1 = 37.8.
- MPCM on ground-truth questions: QA = 70.5; Question LM on ground-truth PPL = 87.7.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to pre-PG Our System, +PG(R_QA) raised MPCM F1 from 65.3 to 74.2 (+8.9 absolute points), but dramatically worsened LM PPL (175.7 -> 300.9). +PG(R_PPL) reduced QA (65.3 -> 61.1) while improving PPL (175.7 -> 155.6). Combined reward yields a compromise (QA 70.2, PPL 183.1).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+8.9 absolute F1 points (MPCM F1) for +PG(R_QA) over pre-PG Our System; +3.7 absolute points higher than MPCM on ground-truth (74.2 vs 70.5). PPL increased by ~125 points under +PG(R_QA) compared to pre-PG.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>Mixed: optimizing QA reward improved downstream QA performance but reduced fluency; optimizing PPL improved fluency but reduced QA performance; combined reward traded off both objectives.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Optimizing directly for MPCM F1 steers the generator to produce forms that better match the QA model's retrieval/matching heuristics, possibly exploiting model quirks (e.g., phrasing that aligns tokens for pointer-based answer extraction), at the cost of producing less natural/frugal language (higher LM perplexity). Optimizing LM PPL places a prior toward natural-sounding questions, which can make them less tightly tied to recoverability by the QA model.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td>The authors report that optimizing BLEU or skip-thought similarity as rewards did not yield consistent improvements (no reliable gains were found), suggesting not all reward-based 'formats' improve generation.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Comprehension by Text-to-Text Neural Question Generation', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5815.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e5815.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Answer_Conditioning_and_Suppression</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Conditioning on extractive answer spans combined with answer-suppression/masking</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Encoding the answer as part of the model input (answer-span flags and an extractive encoding) increases specificity and QA performance; concurrently applying answer-suppression and hard masking prevents trivial 'cheating' by copying the answer into the question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-Decoder Question Generator (answer-conditioned; pointer-softmax with answer masking)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Answer-conditioned question generation (SQuAD)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate questions conditioned on a document and a specified answer span; enforce constraints to avoid generating the answer inside the question and to encourage diversity.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Input presentation includes per-token binary feature indicating whether a document token is part of the answer, an extractive answer encoding (biLSTM over answer token annotations), and an answer-suppression loss plus hard masking of answer tokens in the location softmax.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Seq2Seq baseline: weaker answer-conditioning (average of answer encodings), separate encoder/decoder vocabs, no answer-suppression constraint, no entropy maximization regularizer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Dev-set metrics (Table 2): Seq2Seq baseline: MPCM QA = 45.6; BLEU = 4.9; PPL = 153.2; NLL = 45.8. Our System (with stronger answer-conditioning and suppression): MPCM QA = 65.3; BLEU = 10.2; PPL = 175.7; NLL = 35.3.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Absolute improvement in MPCM F1: +19.7 points (65.3 vs 45.6) and BLEU +5.3 points, but PPL increased (worse) by ~22.5 (175.7 vs 153.2). NLL decreased (better likelihood fit).</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>+19.7 absolute MPCM F1 points when using stronger answer-conditioning and suppression vs Seq2Seq baseline; BLEU +5.3.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved answerability and n-gram overlap (BLEU), but reduced marginal fluency (increased PPL).</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Explicit answer-conditioning biases the generator toward more specific, answer-focused question formulations (helping QA recoverability), but focusing probability mass on such specific phrasings harms modelling of the marginal question distribution and increases perplexity. Answer-suppression/masking is needed to prevent trivial solutions where the generator simply includes the answer token(s) in the question to boost QA scores.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Comprehension by Text-to-Text Neural Question Generation', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e5815.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e5815.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of how the format or presentation of problems (e.g., prompt wording, structure, context, formatting) affects the performance of large language models (LLMs), including details of the formats used, tasks evaluated, models tested, performance results, and any explanations or comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Phrasing_Wh_at_End</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Exploitative question phrasing (placing interrogative 'wh' word at the end) learned under QA reward</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>When fine-tuned to maximize a QA model's F1, the generator sometimes adopts atypical phrasing (e.g., moving the interrogative 'wh' token to the end) that increases QA recoverability while substantially increasing language-model perplexity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Encoder-Decoder Question Generator evaluated with MPCM QA model and LM</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>Question generation (SQuAD) evaluated by MPCM F1 and LM PPL</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Generate questions whose surface form affects the downstream QA model's ability to locate the answer in the source text.</td>
                        </tr>
                        <tr>
                            <td><strong>problem_format</strong></td>
                            <td>Generator is free to choose word order and phrasing; under R_QA optimization it sometimes produces questions with the interrogative token placed at the end (e.g., '... by what?'), aligning token order to source phrases that the pointer-based QA model matches.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_format</strong></td>
                            <td>Standard, more natural interrogative phrasing (e.g., 'By what was X listed in 1954?') seen under ML training or PPL reward.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Observed phenomenon: +PG(R_QA) achieves MPCM F1 = 74.2 (higher than MPCM on ground-truth 70.5) while producing very high LM PPL = 300.9 (ground-truth PPL 87.7). Example-generated questions in Table 4 show many instances where the 'wh' word is placed at the end; such sequences yield high QA but poor LM scores.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_comparison</strong></td>
                            <td>Compared to ML-trained or PPL-optimized generators, the R_QA-optimized generator trades lower fluency (much higher PPL) for higher MPCM F1.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_size</strong></td>
                            <td>MPCM F1 improved to 74.2 ( +3.7 points over MPCM on ground-truth; +8.9 over pre-PG Our System) while PPL increased by ~125 points relative to pre-PG.</td>
                        </tr>
                        <tr>
                            <td><strong>format_effect_direction</strong></td>
                            <td>improved QA performance but reduced fluency (higher perplexity); format change exploited the QA model's matching behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>explanation_or_hypothesis</strong></td>
                            <td>Reordering or unusual phrasing can make the generated question's surface tokens align more directly with spans in the source text or with the QA model's matching heuristics (especially pointer/attention mechanisms), thereby improving extractive QA performance even if language naturalness suffers.</td>
                        </tr>
                        <tr>
                            <td><strong>counterexample_or_null_result</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Machine Comprehension by Text-to-Text Neural Question Generation', 'publication_date_yy_mm': '2017-05'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>An actor-critic algorithm for sequence prediction <em>(Rating: 2)</em></li>
                <li>Sequence level training with recurrent neural networks <em>(Rating: 2)</em></li>
                <li>Reward augmented maximum likelihood for neural structured prediction <em>(Rating: 2)</em></li>
                <li>Deep reinforcement learning for dialogue generation <em>(Rating: 1)</em></li>
                <li>Sentence simplification with deep reinforcement learning <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-5815",
    "paper_id": "paper-bc6ad001c395e92920839e45dfd7e05ce69405d2",
    "extraction_schema_id": "extraction-schema-116",
    "extracted_data": [
        {
            "name_short": "BeamSearch_vs_Sampling",
            "name_full": "Beam-search generation vs random sampling during policy-gradient fine-tuning",
            "brief_description": "Using deterministic beam search to generate candidate sequences for policy-gradient (PG) updates (beam size 32) versus sampling from the model distribution; the paper finds beam search necessary to reliably improve rewards and stabilize RL fine-tuning.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-Decoder Question Generator (attention + pointer-softmax)",
            "model_size": null,
            "task_name": "Question generation (SQuAD) with policy-gradient fine-tuning",
            "task_description": "Generate natural-language questions conditioned on a document and an answer; fine-tune the generator with REINFORCE using sequence-level rewards (QA F1, LM PPL, or combinations).",
            "problem_format": "Decoder output sequences are produced with beam search (beam size 32) and the resulting high-probability complete sequences are used as samples for the policy-gradient update (the model then 'replays' the beam sequence with teacher-forcing to reconstruct states for gradient computation).",
            "comparison_format": "Random (stochastic) sampling from the model distribution to produce sequences for policy-gradient updates.",
            "performance": "Qualitative: Beam-search enabled meaningful improvement of scalar rewards during policy-gradient fine-tuning; without beam-search the authors report they could not improve rewards through training (no numerical reward gains when using random sampling).",
            "performance_comparison": "Sampling: no consistent improvement in rewards; Beam-search: enabled reward optimization and stabilized PG training (authors found beam-search required empirically).",
            "format_effect_size": null,
            "format_effect_direction": "improved (beam search improved ability to optimize sequence-level rewards vs sampling)",
            "explanation_or_hypothesis": "Beam search better approximates high-probability modes of the model distribution and reduces variance in reward estimates; using beam outputs and then teacher-forcing to recreate decoder states yields more stable and higher initial reward signals for PG updates.",
            "counterexample_or_null_result": null,
            "uuid": "e5815.0",
            "source_info": {
                "paper_title": "Machine Comprehension by Text-to-Text Neural Question Generation",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Reward_Objective_Format",
            "name_full": "Choice of sequence-level reward (QA F1, LM PPL, or combined) as an effective 'format' that shapes generated question presentation",
            "brief_description": "Different scalar rewards used to fine-tune the generator (QA performance measured by a pretrained MPCM model, fluency measured by an LSTM language model perplexity, or combinations) produce markedly different question styles and downstream QA performance, trading off fluency and answerability and sometimes encouraging exploitative phrasing.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-Decoder Question Generator (pointer-softmax); evaluated with MPCM QA model and an LSTM Question LM",
            "model_size": null,
            "task_name": "Question generation (SQuAD) evaluated by QA F1 (MPCM) and fluency (LM PPL)",
            "task_description": "Generate questions conditioned on (document, answer). Evaluate question quality by (a) how well a pretrained QA model (MPCM) can recover the answer (token F1), and (b) language-model perplexity as a proxy for fluency.",
            "problem_format": "Policy-gradient fine-tuning with reward = MPCM F1 (R_QA), or reward = negative LM perplexity (R_PPL), or a weighted linear combination R_PPL+QA.",
            "comparison_format": "Maximum-likelihood pretraining (teacher forcing) without PG; PG with R_PPL; PG with R_QA; PG with R_PPL+R_QA.",
            "performance": "Key reported dev-set numbers (Table 2):\n- Our System (pre-PG): MPCM F1 (QA) = 65.3; LM PPL = 175.7; BLEU = 10.2; question-ground-truth F1 = 39.5.\n- + PG (R_PPL): QA = 61.1; PPL = 155.6; BLEU = 9.2; F1 = 38.2.\n- + PG (R_QA): QA = 74.2; PPL = 300.9; BLEU = 10.5; F1 = 40.1.\n- + PG (R_PPL+QA): QA = 70.2; PPL = 183.1; BLEU = 9.2; F1 = 37.8.\n- MPCM on ground-truth questions: QA = 70.5; Question LM on ground-truth PPL = 87.7.",
            "performance_comparison": "Compared to pre-PG Our System, +PG(R_QA) raised MPCM F1 from 65.3 to 74.2 (+8.9 absolute points), but dramatically worsened LM PPL (175.7 -&gt; 300.9). +PG(R_PPL) reduced QA (65.3 -&gt; 61.1) while improving PPL (175.7 -&gt; 155.6). Combined reward yields a compromise (QA 70.2, PPL 183.1).",
            "format_effect_size": "+8.9 absolute F1 points (MPCM F1) for +PG(R_QA) over pre-PG Our System; +3.7 absolute points higher than MPCM on ground-truth (74.2 vs 70.5). PPL increased by ~125 points under +PG(R_QA) compared to pre-PG.",
            "format_effect_direction": "Mixed: optimizing QA reward improved downstream QA performance but reduced fluency; optimizing PPL improved fluency but reduced QA performance; combined reward traded off both objectives.",
            "explanation_or_hypothesis": "Optimizing directly for MPCM F1 steers the generator to produce forms that better match the QA model's retrieval/matching heuristics, possibly exploiting model quirks (e.g., phrasing that aligns tokens for pointer-based answer extraction), at the cost of producing less natural/frugal language (higher LM perplexity). Optimizing LM PPL places a prior toward natural-sounding questions, which can make them less tightly tied to recoverability by the QA model.",
            "counterexample_or_null_result": "The authors report that optimizing BLEU or skip-thought similarity as rewards did not yield consistent improvements (no reliable gains were found), suggesting not all reward-based 'formats' improve generation.",
            "uuid": "e5815.1",
            "source_info": {
                "paper_title": "Machine Comprehension by Text-to-Text Neural Question Generation",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Answer_Conditioning_and_Suppression",
            "name_full": "Conditioning on extractive answer spans combined with answer-suppression/masking",
            "brief_description": "Encoding the answer as part of the model input (answer-span flags and an extractive encoding) increases specificity and QA performance; concurrently applying answer-suppression and hard masking prevents trivial 'cheating' by copying the answer into the question.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-Decoder Question Generator (answer-conditioned; pointer-softmax with answer masking)",
            "model_size": null,
            "task_name": "Answer-conditioned question generation (SQuAD)",
            "task_description": "Generate questions conditioned on a document and a specified answer span; enforce constraints to avoid generating the answer inside the question and to encourage diversity.",
            "problem_format": "Input presentation includes per-token binary feature indicating whether a document token is part of the answer, an extractive answer encoding (biLSTM over answer token annotations), and an answer-suppression loss plus hard masking of answer tokens in the location softmax.",
            "comparison_format": "Seq2Seq baseline: weaker answer-conditioning (average of answer encodings), separate encoder/decoder vocabs, no answer-suppression constraint, no entropy maximization regularizer.",
            "performance": "Dev-set metrics (Table 2): Seq2Seq baseline: MPCM QA = 45.6; BLEU = 4.9; PPL = 153.2; NLL = 45.8. Our System (with stronger answer-conditioning and suppression): MPCM QA = 65.3; BLEU = 10.2; PPL = 175.7; NLL = 35.3.",
            "performance_comparison": "Absolute improvement in MPCM F1: +19.7 points (65.3 vs 45.6) and BLEU +5.3 points, but PPL increased (worse) by ~22.5 (175.7 vs 153.2). NLL decreased (better likelihood fit).",
            "format_effect_size": "+19.7 absolute MPCM F1 points when using stronger answer-conditioning and suppression vs Seq2Seq baseline; BLEU +5.3.",
            "format_effect_direction": "improved answerability and n-gram overlap (BLEU), but reduced marginal fluency (increased PPL).",
            "explanation_or_hypothesis": "Explicit answer-conditioning biases the generator toward more specific, answer-focused question formulations (helping QA recoverability), but focusing probability mass on such specific phrasings harms modelling of the marginal question distribution and increases perplexity. Answer-suppression/masking is needed to prevent trivial solutions where the generator simply includes the answer token(s) in the question to boost QA scores.",
            "counterexample_or_null_result": null,
            "uuid": "e5815.2",
            "source_info": {
                "paper_title": "Machine Comprehension by Text-to-Text Neural Question Generation",
                "publication_date_yy_mm": "2017-05"
            }
        },
        {
            "name_short": "Phrasing_Wh_at_End",
            "name_full": "Exploitative question phrasing (placing interrogative 'wh' word at the end) learned under QA reward",
            "brief_description": "When fine-tuned to maximize a QA model's F1, the generator sometimes adopts atypical phrasing (e.g., moving the interrogative 'wh' token to the end) that increases QA recoverability while substantially increasing language-model perplexity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Encoder-Decoder Question Generator evaluated with MPCM QA model and LM",
            "model_size": null,
            "task_name": "Question generation (SQuAD) evaluated by MPCM F1 and LM PPL",
            "task_description": "Generate questions whose surface form affects the downstream QA model's ability to locate the answer in the source text.",
            "problem_format": "Generator is free to choose word order and phrasing; under R_QA optimization it sometimes produces questions with the interrogative token placed at the end (e.g., '... by what?'), aligning token order to source phrases that the pointer-based QA model matches.",
            "comparison_format": "Standard, more natural interrogative phrasing (e.g., 'By what was X listed in 1954?') seen under ML training or PPL reward.",
            "performance": "Observed phenomenon: +PG(R_QA) achieves MPCM F1 = 74.2 (higher than MPCM on ground-truth 70.5) while producing very high LM PPL = 300.9 (ground-truth PPL 87.7). Example-generated questions in Table 4 show many instances where the 'wh' word is placed at the end; such sequences yield high QA but poor LM scores.",
            "performance_comparison": "Compared to ML-trained or PPL-optimized generators, the R_QA-optimized generator trades lower fluency (much higher PPL) for higher MPCM F1.",
            "format_effect_size": "MPCM F1 improved to 74.2 ( +3.7 points over MPCM on ground-truth; +8.9 over pre-PG Our System) while PPL increased by ~125 points relative to pre-PG.",
            "format_effect_direction": "improved QA performance but reduced fluency (higher perplexity); format change exploited the QA model's matching behavior.",
            "explanation_or_hypothesis": "Reordering or unusual phrasing can make the generated question's surface tokens align more directly with spans in the source text or with the QA model's matching heuristics (especially pointer/attention mechanisms), thereby improving extractive QA performance even if language naturalness suffers.",
            "counterexample_or_null_result": null,
            "uuid": "e5815.3",
            "source_info": {
                "paper_title": "Machine Comprehension by Text-to-Text Neural Question Generation",
                "publication_date_yy_mm": "2017-05"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "An actor-critic algorithm for sequence prediction",
            "rating": 2
        },
        {
            "paper_title": "Sequence level training with recurrent neural networks",
            "rating": 2
        },
        {
            "paper_title": "Reward augmented maximum likelihood for neural structured prediction",
            "rating": 2
        },
        {
            "paper_title": "Deep reinforcement learning for dialogue generation",
            "rating": 1
        },
        {
            "paper_title": "Sentence simplification with deep reinforcement learning",
            "rating": 1
        }
    ],
    "cost": 0.01604025,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Machine Comprehension by Text-to-Text Neural Question Generation</h1>
<p>Xingdi Yuan ${ }^{1 <em>}$ Tong Wang ${ }^{1, </em>}$ Caglar Gulcehre ${ }^{2, * \dagger}$ Alessandro Sordoni ${ }^{1, *}$ Philip Bachman ${ }^{1}$ Sandeep Subramanian ${ }^{2, \dagger}$ Saizheng Zhang ${ }^{2, \dagger}$ Adam Trischler ${ }^{1}$<br>${ }^{1}$ Microsoft Maluuba, ${ }^{2}$ Montreal Institute for Learning Algorithms, Université de Montréal<br>{eric.yuan,tong.wang, alsordon, phbachma, adam.trischler}@microsoft.com<br>gulcehrc@iro.umontreal.ca, sandeep.subramanian@gmail.com, saizheng.zhang@umontreal.ca</p>
<h4>Abstract</h4>
<p>We propose a recurrent neural model that generates natural-language questions from documents, conditioned on answers. We show how to train the model using a combination of supervised and reinforcement learning. After teacher forcing for standard maximum likelihood training, we fine-tune the model using policy gradient techniques to maximize several rewards that measure question quality. Most notably, one of these rewards is the performance of a questionanswering system. We motivate question generation as a means to improve the performance of question answering systems. Our model is trained and evaluated on the recent question-answering dataset $S Q u A D$.</p>
<h2>1 Introduction</h2>
<p>People ask questions to improve their knowledge and understanding of the world. Questions can be used to access the knowledge of others or to direct one's own information-seeking behavior. Here we study the generation of natural-language questions by machines, based on text passages. This task is synergistic with machine comprehension (MC), which pursues the understanding of written language by machines at a near-human level. Because most human knowledge is recorded in text, this would enable transformative applications.</p>
<p>Many machine comprehension datasets have been released recently. These generally comprise (document, question, answer) triples (Hermann et al., 2015; Hill et al., 2015; Rajpurkar et al., 2016; Trischler et al., 2016a; Nguyen et al., 2016), where the goal is to predict an answer, conditioned on a document and question. The availability of large</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Text Passage
in $\mathbf{1 0 6 6}{ }^{1,2}$, duke william $\mathbf{i i}^{3}$ of normandy conquered england killing king harold ii at the battle of hastings. the invading normans and their descendants ${ }^{4}$ replaced the anglo-saxons as the ruling class of england.</p>
<h2>Questions Generated by our System</h2>
<p>1) when did the battle of hastings take place?
2) in what year was the battle of hastings fought?
3) who conquered king harold ii at the battle of hastings?
4) who became the ruling class of england?</p>
<p>Table 1: Examples of conditional question generation given a context and an answer from the $S Q u A D$ dataset, using the scheme referred to as $R_{\mathrm{PPL}+\mathrm{QA}}$ below. Bold text in the passage indicates the answers used to generate the numbered questions.
labeled datasets has spurred development of increasingly advanced models for question answering (QA) from text (Kadlec et al., 2016; Trischler et al., 2016b; Seo et al., 2016; Wang et al., 2016; Shen et al., 2016).</p>
<p>In this paper we reframe the standard MC task: rather than answering questions about a document, we teach machines to ask questions. Our work has several motivations. First, we believe that posing appropriate questions is an important aspect of information acquisition in intelligent systems. Second, learning to ask questions may improve the ability to answer them. Singer and Donlan (1982) demonstrated that having students devise questions before reading can increase scores on subsequent comprehension tests. Third, answering the questions in most existing QA datasets is an extractive task - it requires selecting some span of text within the document - while question asking is comparatively abstractive - it requires generation of text that may not appear in the document. Fourth, asking good questions involves skills beyond those used to answer them. For instance, in existing QA</p>
<p>datasets, a typical (document, question) pair specifies a unique answer. Conversely, a typical (document, answer) pair may be associated with multiple questions, since a valid question can be formed from any information or relations which uniquely specify the given answer. Finally, a mechanism to ask informative questions about documents (and eventually answer them) has many practical applications, e.g.: generating training data for question answering (Serban et al., 2016; Yang et al., 2017), synthesising frequently asked question (FAQ) documentation, and automatic tutoring systems (Lindberg et al., 2013).</p>
<p>We adapt the sequence-to-sequence approach of Cho et al. (2014) for generating questions, conditioned on a document and answer: first we encode the document and answer, then output question words sequentially with a decoder that conditions on the document and answer encodings. We augment the standard encoder-decoder approach with several modifications geared towards the question generation task. During training, in addition to maximum likelihood for predicting questions from (document, answer) tuples, we use policy gradient optimization to maximize several auxiliary rewards. These include a language-modelbased score for fluency and the performance of a pretrained question-answering model on generated questions. We show quantitatively that policy gradient increases the rewards earned by generated questions at test time, and provide examples to illustrate the qualitative effects of different training schemes. To our knowledge, we present the first end-to-end, text-to-text model for question generation.</p>
<h2>2 Related Work</h2>
<p>Recently, automatic question generation has received increased attention from the research community. It has been harnessed, for example, as a means to build automatic tutoring systems (Heilman and Smith, 2010; Ali et al., 2010; Lindberg et al., 2013; Labutov et al., 2015; Mazidi and Nielsen, 2015), to reroute queries to community question-answering systems (Zhao et al., 2011), and to enrich training data for question-answering systems (Serban et al., 2016; Yang et al., 2017).</p>
<p>Several earlier works process documents as individual sentences using syntactic (Heilman and Smith, 2010; Ali et al., 2010; Kumar et al., 2015) or semantic-based parsing (Mannem et al., 2010;</p>
<p>Lindberg et al., 2013), then reformulate questions using hand-crafted rules acting on parse trees. These traditional approaches generate questions with a high word overlap with the original text that pertain specifically to the given sentence by re-arranging the sentence parse tree. An alternative approach is to use generic question templates whose slots can be filled with entities from the document (Lindberg et al., 2013; Chali and Golestanirad, 2016). Labutov et al. (2015), for example, use ontology-derived templates to generate high-level questions related to larger portions of the document. These approaches comprise pipelines of independent components that are difficult to tune for final performance measures.</p>
<p>More recently, neural networks have enabled end-to-end training of question generation systems. Serban et al. (2016) train a neural system to convert knowledge base (KB) triples into naturallanguage questions. The head and the relation form a context for the question and the tail serves as the answer. Similarly, we assume that the answer is known a priori, but we extend the context to encompass a span of unstructured text. Mostafazadeh et al. (2016) use a neural architecture to generate questions from images rather than text. Contemporaneously with this work, Yang et al. (2017) developed generative domain-adaptive networks, which perform question generation as an auxiliary task in training a QA system. The main goal of their question generation is data augmentation, thus questions themselves are not evaluated. In contrast, our work focuses primarily on developing a neural model for question generation that could be applied to a variety of downstream tasks that includes question answering.</p>
<p>Our model shares similarities with recent end-to-end neural QA systems, e.g. Seo et al. (2016); Wang et al. (2016). I.e., we use an encoder-decoder structure, where the encoder processes answer and document (instead of question and document) and our decoder generates a question (instead of an answer). While existing question answering systems typically extract the answer from the document, our decoder is a fully generative model.</p>
<p>Finally, we relate the recent body of works that apply reinforcement learning to natural language generation, such as Li et al. (2016); Ranzato et al. (2016); Kandasamy and Bachrach (2017); Zhang and Lapata (2017). We similarly apply a REINFORCE-style (Williams, 1992) algorithm</p>
<p>to maximize various rewards earned by generated questions.</p>
<h2>3 Encoder-Decoder Model for Question Generation</h2>
<p>We adapt the simple encoder-decoder architecture first outlined by Cho et al. (2014) to the question generation problem. Particularly, we base our model on the attention mechanism of Bahdanau et al. (2015) and the pointer-softmax copying mechanism of Gulcehre et al. (2016). In question generation, we can condition our encoder on two different sources of information (compared to the single source in neural machine translation (NMT)): a document that the question should be about and an answer that should fit the generated question. Next, we describe how we adapt the encoder and decoder architectures in detail.</p>
<h3>3.1 Encoder</h3>
<p>Our encoder is a neural model acting on two input sequences: the document, $D=\left(d_{1}, \ldots, d_{n}\right)$ and the answer, $A=\left(a_{1}, \ldots, a_{m}\right)$. Sequence elements $d_{i}, a_{j} \in \mathbb{R}^{D_{e}}$ are given by embedding vectors (Bengio et al., 2001).</p>
<p>In the first stage of encoding, similar to current question answering systems, e.g. (Seo et al., 2016), we augment each document word embedding with a binary feature that indicates if the document word belongs to the answer. Then, we run a bidirectional long short-term memory (Hochreiter and Schmidhuber, 1997) (LSTM) network on the augmented document sequence, producing annotation vectors $\mathbf{h}^{d}=\left(\mathbf{h}<em e="e">{1}^{d}, \ldots, \mathbf{h}</em>}^{d}\right)$. Here, $\mathbf{h<em h="h">{i}^{d} \in \mathbb{R}^{D</em>}}$ is the concatenation of the network's forward $\left(\overrightarrow{\mathbf{h}<em i="i">{i}^{d}\right)$ and backward hidden states $\left(\overrightarrow{\mathbf{h}}</em>}^{d}\right)$ for input token $i$, i.e., $\mathbf{h<em i="i">{i}^{d}=\left[\overrightarrow{\mathbf{h}}</em>$}^{d} ; \overrightarrow{\mathbf{h}}_{i}^{d}\right] .{ }^{1</p>
<p>Our model operates on QA datasets where the answer is extractive; thus, we encode the answer $A$ using the annotation vectors corresponding to the answer word positions in the document. We assume that, without loss of generality, $A$ consists of the sequence of words $\left(d_{s}, \ldots, d_{e}\right)$ in the document, s.t. $1 \leq s \leq e \leq n$. We concatenate the annotation sequence $\left(\mathbf{h}<em e="e">{s}^{d}, \ldots, \mathbf{h}</em>}^{d}\right)$ with the corresponding answer word embeddings $\left(a_{s}, \ldots, a_{e}\right)$, i.e., $\left[\mathbf{h<em j="j">{j}^{d} ; a</em>\right], s \leq j \leq e$, then apply a second bidirectional LSTM (biLSTM) over the resulting sequence of vectors to obtain the extractive condition</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>encoding $\mathbf{h}^{a} \in \mathbb{R}^{D_{h}}$. We form $\mathbf{h}^{a}$ by concatenating the final hidden states from each direction of the biLSTM.</p>
<p>We also compute an initial state $\mathbf{s}<em s="s">{0} \in \mathbb{R}^{D</em>$ for the decoder using the annotation vectors and the extractive condition encoding:}</p>
<p>$$
\mathbf{r}=\mathbf{L} \mathbf{h}^{a}+\frac{1}{n} \sum_{i}^{|D|} \mathbf{h}<em 0="0">{i}^{d}, \quad \mathbf{s}</em>}=\tanh \left(\mathbf{W<em 0="0">{0} \mathbf{r}+\mathbf{b}</em>\right)
$$</p>
<p>where $\mathbf{L} \in \mathbb{R}^{D_{h} \times D_{h}}, \mathbf{W}<em s="s">{0} \in \mathbb{R}^{D</em>} \times D_{h}}$, and $\mathbf{b<em s="s">{0} \in$ $\mathbb{R}^{D</em>$}}$ are model parameters. ${ }^{2</p>
<h3>3.2 Decoder</h3>
<p>Our decoder is a neural model that generates outputs $y_{t}$ sequentially to yield the question sequence $Q=\left{y_{t}\right}$. At each time-step $t$, the decoder models a conditional distribution parametrized by $\theta$,</p>
<p>$$
p_{\theta}\left(y_{t} \mid y_{&lt;t}, D, A\right)
$$</p>
<p>where $y_{&lt;t}$ represents the outputs at earlier timesteps. In question generation, output $y_{t}$ is a word sampled according to (1).</p>
<p>When formulating questions based on documents, it is common to refer to phrases and entities that appear directly in the text. We therefore incorporate into our decoder a mechanism for copying relevant words from $D$. We use the pointer-softmax formulation (Gulcehre et al., 2016), which has two output layers: the shortlist softmax and the location softmax. The shortlist softmax places a distribution over words in a predefined output vocabulary. The location softmax is a pointer network (Vinyals et al., 2015) that places a distribution over document tokens to be copied. A source switching network enables the model to interpolate between these distributions.</p>
<p>In more detail, the decoder is a recurrent neural network. Its internal state, $\mathbf{s}<em s="s">{t} \in \mathbb{R}^{D</em>$, evolves according to the long short-term memory update (Hochreiter and Schmidhuber, 1997), i.e.,}</p>
<p>$$
\mathbf{s}<em t-1="t-1">{t}=\operatorname{LSTM}\left(\mathbf{s}</em>\right)
$$}, y_{t-1}, \mathbf{v}_{t</p>
<p>where $\mathbf{v}_{t}$ is a the context vector computed from the document and answer encodings.</p>
<p>At every time-step $t$, the model computes a softalignment score over the document to decide which words are more relevant to the question being generated. As in a traditional NMT architecture, the decoder computes a relevance weight $\alpha_{t j}$ for the $j$ th</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>word in the document when generating the $t$ th word in the question. Alignment score vector $\boldsymbol{\alpha}<em t="t">{t} \in \mathbb{R}^{|D|}$ is computed with a single layer feedforward neural network $f(\cdot)$ using the $\tanh (\cdot)$ activation function. The scores $\boldsymbol{\alpha}</em>$ are computed as in (4):}$ are also used as the location softmax distribution. The network defined by $f(\cdot)$ computes energies according to (3) for the alignments, and the normalized alignments $\alpha_{t j</p>
<p>$$
\begin{aligned}
e_{t j} &amp; =\exp \left(f\left(\mathbf{h}<em t="t">{j}^{d}, \mathbf{h}^{a}, y</em>}, \mathbf{s<em j="j" t="t">{t-1}\right)\right) \
\alpha</em>
\end{aligned}
$$} &amp; =\frac{\exp \left(e_{t j}\right)}{\sum_{i=1}^{T} \exp \left(e_{i j}\right)</p>
<p>To compute the context vector $\mathbf{v}<em t="t">{t}$ used in (2), we first construct context vector $\mathbf{c}</em>$ :}$ for the document and then concatenate it with $\mathbf{h}^{a</p>
<p>$$
\begin{aligned}
\mathbf{c}<em i="1">{t} &amp; =\sum</em>}^{|D|} \alpha_{t i} \mathbf{h<em t="t">{i}^{d} \
\mathbf{v}</em>\right]
\end{aligned}
$$} &amp; =\left[\mathbf{c}_{t} ; \mathbf{h}^{a</p>
<p>We use a deep output layer (Pascanu et al., 2013) at each time-step for the shortlist softmax vector $\mathbf{o}<em t="t">{t}$. This layer fuses the information coming from $\mathbf{s}</em>}$, $\mathbf{v<em t-1="t-1">{t}$ and $y</em>}$ through a simple MLP to predict the word logits for the softmax as in (7). Parameters of the softmax layer are denoted as $\mathbf{W<em h="h">{o} \in \mathbb{R}^{|V| \times D</em>$, where $|V|$ is the size of the shortlist vocabulary (we used 2000 words).}}$ and $\mathbf{b}_{o} \in \mathbb{R}^{|V|</p>
<p>$$
\begin{aligned}
\mathbf{e}<em t="t">{t} &amp; =g\left(\mathbf{s}</em>}, \mathbf{v<em t-1="t-1">{t}, y</em>\right) \
\mathbf{o}<em o="o">{t} &amp; =\operatorname{softmax}\left(\mathbf{W}</em>} \mathbf{e<em o="o">{t}+\mathbf{b}</em>\right)
\end{aligned}
$$</p>
<p>A source switching variable $z_{t}$ enables the model to interpolate between document copying and generation from shortlist. It is computed by an MLP with two hidden layers using tanh units (Gulcehre et al., 2016). Similarly to the computation of the shortlist softmax, the switching network takes $\mathbf{s}<em t="t">{t}$, $\mathbf{v}</em>$ through the logistic sigmoid activation function.}$ and $y_{t-1}$ as inputs. Its output layer generates the scalar $z_{t</p>
<p>Finally, $p_{\theta}\left(y_{t} \mid y_{&lt;t}, D, A\right)$ is approximated by the full pointer-softmax $\mathbf{p}<em t="t">{t} \in \mathbb{R}^{|V|+|D|}$ by concatenating $\mathbf{o}</em>}$ and $\boldsymbol{\alpha<em t="t">{t}$ after both are weighted by $z</em>$ :</p>
<p>$$
\mathbf{p}<em t="t">{t}=\left[z</em>} \mathbf{o<em t="t">{t} ;\left(1-z</em>\right]
$$}\right) \boldsymbol{\alpha}_{t</p>
<p>As is standard in NMT, during decoding we use a beam search (Graves, 2012) to maximize (approximately) the conditional probability of an output sequence. We discuss this in more detail in the following section.</p>
<h3>3.3 Training</h3>
<p>The model is trained initially to minimize the negative log-likelihood of the training data under the model distribution,</p>
<p>$$
\mathcal{L}=-\sum_{t} \log p_{\theta}\left(y_{t} \mid y_{&lt;t}, D, A\right)
$$</p>
<p>where, in the decoder as defined in (2), the previous token $y_{t-1}$ comes from the source sequence rather than the model output (this is called teacher forcing).</p>
<p>Based on our knowledge of the task, we introduce additional training signals to aid the model's learning. First, we encourage the model not to generate answer words in the question. We use the soft answer-suppression constraint given in (10) with the penalty hyperparameter $\lambda_{s} ; \hat{\mathcal{A}}$ denotes the set of words that appear in the answer but not in the ground-truth question:</p>
<p>$$
\mathcal{L}<em s="s">{s}=\lambda</em>, D, A\right)
$$} \sum_{t} \sum_{\bar{a} \in \hat{\mathcal{A}}} p_{\theta}\left(y_{t}=\bar{a} \mid y_{&lt;t</p>
<p>We also encourage variety in the output words to counteract the degeneracy often observed in NLG systems towards common outputs (Sordoni et al., 2015). This is achieved with a loss term that maximizes entropy in the output softmax (8), i.e.,</p>
<p>$$
\mathcal{L}<em e="e">{e}=\lambda</em>} \sum_{t} \mathbf{p<em t="t">{t}^{T} \log \mathbf{p}</em>
$$</p>
<h2>4 Policy Gradient Optimization</h2>
<p>As described above, we use teacher forcing to train our model to generate text by maximizing groundtruth likelihood. Teacher forcing introduces critical differences between the training phase (in which the model is driven by ground-truth sequences) and the testing phase (in which the model is driven by its own outputs) (Bahdanau et al., 2016). Significantly, teacher forcing prevents the model from making and learning from mistakes during training. This is related to the observation that maximizing ground-truth likelihood does not teach the model how to distribute probability mass among examples other than the ground-truth, some of which may be valid questions and some of which may be completely incoherent. This is especially problematic in language, where there are often many ways to say the same thing. A reinforcement learning (RL) approach, by which a model is rewarded or penalized for its own actions, could mitigate these</p>
<p>issues - though likely at the expense of reduced stability during training. A properly designed reward, maximized via RL, could provide a model with more information about how to distribute probability mass among sequences that do not occur in the training set (Norouzi et al., 2016).</p>
<p>We investigate the use of RL to fine-tune our question generation model. Specifically, we perform policy gradient optimization following a period of "pretraining" on maximum likelihood, using a combination of scalar rewards correlated to question quality. We detail this process below. To make clear that the model is acting freely without teacher forcing, we indicate model-generated tokens with $\hat{y}_{t}$ and sequences with $\hat{Y}$.</p>
<h3>4.1 Rewards</h3>
<p>Question answering (QA) One obvious measure of a question's quality is whether it can be answered correctly given the context document $D$. We therefore feed model-generated questions into a pretrained question-answering system and use that system's accuracy as a reward. We use the recently proposed Multi-Perspective Context Matching (MPCM) (Wang et al., 2016) model as our reference QA system, sans character-level encoding. Broadly, that model takes in a generated question $\hat{Y}$ and a document $D$, processes them through bidirectional recurrent neural networks, applies an attention mechanism, and points to the start and end tokens of the answer in $D$. After training a MPCM model on the $S Q u A D$ dataset, the reward $R_{\mathrm{QA}}(\hat{Y})$ is given by MPCM's answer accuracy on $\hat{Y}$ in terms of the F1 score, a token-based measure proposed by Rajpurkar et al. (2016) that accounts for partial word matches:</p>
<p>$$
R_{\mathrm{QA}}(\hat{Y})=\mathrm{F} 1(\hat{A}, A)
$$</p>
<p>where $\hat{A}=\operatorname{MPCM}(\hat{Y})$ is the answer to the generated question by the MPCM model. Optimizing the QA reward could lead to 'friendly' questions that are either overly simplistic or that somehow cheat by exploiting quirks in the MPCM model. One obvious way to cheat would be to inject answer words into the question. We prevented this by masking these out in the location softmax, a hard version of the answer suppression loss (10).</p>
<p>Fluency (PPL) Another measure of quality is a question's fluency - i.e., is it stated in proper, grammatical English? As simultaneously proposed in Zhang and Lapata (2017), we use a language
model to measure and reward the fluency of generated questions. In particular, we use the perplexity assigned to $\hat{Y}$ by an LSTM language model:</p>
<p>$$
R_{\mathrm{PPL}}(\hat{Y})=-2^{-\frac{1}{T} \sum_{t=1}^{T} \log <em _mathrm_LM="\mathrm{LM">{2} p</em>}}\left(\hat{y<em _t="&lt;t">{t} \mid \hat{y}</em>
$$}\right)</p>
<p>where the negation is to reward the model for minimizing perplexity. The language model is trained through maximum likelihood estimation on over 80,000 human-generated questions from $S Q u A D$ (the training set).</p>
<p>Combination For the total scalar reward earned by the word sequence $\hat{Y}$, we also test a weighted combination of the individual rewards:</p>
<p>$$
R_{\mathrm{PPL}+\mathrm{QA}}(\hat{Y})=\lambda_{\mathrm{QA}} R_{\mathrm{QA}}(\hat{Y})+\lambda_{\mathrm{PPL}} R_{\mathrm{PPL}}(\hat{Y})
$$</p>
<p>where $\lambda_{\mathrm{QA}}$ and $\lambda_{\mathrm{PPL}}$ are hyperparameters. The individual reward functions use neural models to tune the neural question generator. This is reminiscent of recent work on GANs (Goodfellow et al., 2014) and actor-critic methods (Bahdanau et al., 2016). We treat the reward models as black boxes, rather than attempting to optimize them jointly or backpropagate error signals through them. We leave these directions for future work.</p>
<p>We also experimented with several other rewards, most notably the BLEU score (Papineni et al., 2002) between $\hat{Y}$ and the ground-truth question for the given document and answer, and a softer measure of similarity between output and ground-truth based on skip-thought vectors (Kiros et al., 2015). Empirically, we were unable to obtain consistent improvements on these rewards through training, though this may be an issue with hyperparameter settings.</p>
<h3>4.2 REINFORCE</h3>
<p>We use the REINFORCE algorithm (Williams, 1992) to maximize the model's expected reward. For each generated question $\hat{Y}$, we define the loss</p>
<p>$$
\mathcal{L}<em _hat_Y="\hat{Y">{\mathrm{RL}}=-\mathbb{E}</em>)]
$$} \sim \pi(\hat{Y} \mid D, A)}[R(\hat{Y</p>
<p>where $\pi$ is the policy to be trained. The policy is a distribution over discrete actions, i.e. words $\hat{y}_{t}$ that make up the sequence $\hat{Y}$. It is the distribution induced at the output layer of the encoder-decoder model (8), initialized with the parameters determined through likelihood optimization. ${ }^{3}$</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p>REINFORCE approximates the expectation in (14) with independent samples from the policy distribution, yielding the policy gradient</p>
<p>$$
\nabla \mathcal{L}<em t="1">{\mathrm{RL}} \approx \sum</em>} \nabla \log \pi\left(\hat{y<em _t="&lt;t">{t} \mid \hat{y}</em>
$$}, D, A\right) \frac{R(\hat{Y})-\mu_{R}}{\sigma_{R}</p>
<p>The optional $\mu_{R}$ and $\sigma_{R}$ are the running mean and standard deviation of the reward, which push $R(\hat{Y})$ toward zero mean and unit variance. This "whitening" of rewards is a simple version of PopArt (van Hasselt et al., 2016), and we found empirically that it stabilized learning.</p>
<p>It is straightforward to combine policy gradient with maximum likelihood, as both gradients can be computed by backpropagating through a properly reweighted sequence-level log-likelihood. The sequences for policy gradient are sampled from the model and weighted by a whitened reward, and the likelihood sequences are sampled from the training set and weighted by 1 .</p>
<h3>4.3 Training Scheme</h3>
<p>Instead of sampling from the model's output distribution, we use beam-search to generate questions from the model and approximate the expectation in Eq. 14. Empirically we found that rewards could not be improved through training without this approach. Randomly sampling from the model's distribution may not be as effective for estimating the modes of the generation policy and it may introduce more variance into the policy gradient.</p>
<p>Beam search keeps a running set of candidates that expands and contracts adaptively. At each time-step $t, k$ output words that maximize the probabilities of their respective paths are selected and added to the candidate sequences, where $k$ is the beam size. The probabilities of these candidates are given by their accumulated log-likelihood up to $t .^{4}$</p>
<p>Given a complete sample from the beam search and its accumulated log-likelihood, the gradient in (15) can be estimated as follows. After calculating the reward with a sequence generated by beam search, we use the sample to teacher-force the decoder so as to recreate exactly the model states from which the sequence was generated. The model can then be accurately updated by cou-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>pling the parameter-independent reward with the log-likelihood of the generated sequence. This approach adds a computational overhead but it significantly increases the initial reward values earned by the model and stabilizes policy gradient training.</p>
<p>We also further tune the likelihood during policy gradient optimization to prevent the model from overwriting its earlier training. We combine the policy gradient update to the model parameters, $\nabla \mathcal{L}_{\mathrm{RL}}$, with an update from $\nabla \mathcal{L}$ based on teacher forcing on the ground-truth signal.</p>
<h2>5 Experiments</h2>
<h3>5.1 Dataset</h3>
<p>We conducted our experiments on the $S Q u A D$ dataset for machine comprehension (Rajpurkar et al., 2016), a large-scale, human-generated corpus of (document, question, answer) triples. Documents are paragraphs from 536 high-PageRank Wikipedia articles covering a variety of subjects. Questions are posed by crowdworkers in natural language and answers are spans of text in the related paragraph highlighted by the same crowdworkers. There are 107,785 question-answer pairs in total, including 87,599 training instances and 10,570 development instances.</p>
<h3>5.2 Baseline Seq2Seq System</h3>
<p>Our baseline system, denoted "Seq2Seq," is based on the encoder-decoder architecture with attention and pointer-softmax outlined in Bahdanau et al. (2015) and Gulcehre et al. (2016). This is essentially the model outlined in Section 3, with a few key differences: (i) since the baseline was originally designed for translation, its encoder and decoder vocabularies are separate; (ii) the baseline conditions question generation on the answer simply by setting $\mathbf{h}^{a}$ as the average of the document encodings corresponding to the answer positions in $D$; (iii) the baseline has no constraint on generating answer words in the question (Equation (10)); and (iv) the baseline does not include the entropy-based loss defined in (11).</p>
<h3>5.3 Quantitative Evaluation</h3>
<p>We use several automatic evaluation metrics to judge the quality of generated questions with respect to the ground-truth questions from the dataset. We are undertaking a large-scale human evaluation to determine how these metrics align with human judgments. The first metric is BLEU (Papineni</p>
<table>
<thead>
<tr>
<th></th>
<th>NLL</th>
<th>BLEU</th>
<th>F1</th>
<th>QA</th>
<th>PPL</th>
</tr>
</thead>
<tbody>
<tr>
<td>Seq2Seq</td>
<td>45.8</td>
<td>4.9</td>
<td>31.2</td>
<td>45.6</td>
<td>153.2</td>
</tr>
<tr>
<td>Our System</td>
<td>$\mathbf{3 5 . 3}$</td>
<td>10.2</td>
<td>39.5</td>
<td>65.3</td>
<td>175.7</td>
</tr>
<tr>
<td>+ PG $\left(R_{\mathrm{PPL}}\right)$</td>
<td>35.7</td>
<td>9.2</td>
<td>38.2</td>
<td>61.1</td>
<td>$\mathbf{1 5 5 . 6}$</td>
</tr>
<tr>
<td>+ PG $\left(R_{\mathrm{QA}}\right)$</td>
<td>39.8</td>
<td>$\mathbf{1 0 . 5}$</td>
<td>$\mathbf{4 0 . 1}$</td>
<td>$\mathbf{7 4 . 2}$</td>
<td>300.9</td>
</tr>
<tr>
<td>+ PG $\left(R_{\mathrm{PPL}+\mathrm{QA}}\right)$</td>
<td>39.0</td>
<td>9.2</td>
<td>37.8</td>
<td>70.2</td>
<td>183.1</td>
</tr>
<tr>
<td>Question LM</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>87.7</td>
</tr>
<tr>
<td>MPCM</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>70.5</td>
<td>-</td>
</tr>
</tbody>
</table>
<p>Table 2: Automatic metrics on $S Q u A D$ 's dev set. NLL is the negative log-likelihood. BLEU and F1 are computed with respect to the ground-truth questions. QA is the F1 obtained by the MPCM model answers to generated questions and PPL is the perplexity computed with the question language model (LM) (lower is better). PG denotes policy gradient training. The bottom two lines report performance on ground-truth questions.</p>
<h2>Text Passage</h2>
<p>...the court of justice accepted that a requirement to speak gaelic to teach in a dublin design college could be justified as part of the public policy of promoting the irish language.</p>
<h2>Generated Questions</h2>
<p>1) what did the court of justice not claim to do?
2) what language did the court of justice say should be justified as part of the public language?
3) what language did the court of justice decide to speak?
4) what language did the court of justice adopt a requirement to speak?
5) what language did the court of justice say should be justified as part of?</p>
<p>Table 3: Examples of generated questions given a context and an answer. Questions are generated by the five systems in Table 2, in order.
et al., 2002), a standard in machine translation, which computes ${1,2,3,4}$-gram matches between generated and ground-truth questions. Next we use F1, which focuses on unigram matches (Rajpurkar et al., 2016). We also report fluency and QA performance metrics used in our reward computation. Fluency is measured by the perplexity (PPL) of the generated question computed by the pretrained question language model. The PPL score is proportional to the marginal probability $p(\hat{Y})$ estimated from the corpus. The QA performance is measured by running the pretrained MPCM model on the generated questions and measuring F1 between the predicted answer and the conditioning answer.</p>
<h3>5.4 Results and qualitative analysis</h3>
<p>Our results for automatic evaluation on $S Q u A D$ 's development set are presented in Table 2. Implementation details for all models are given in the supplementary material. One striking feature is that BLEU scores are quite low for all systems tested, which relates to our earlier argument that a typical (document, answer) pair may be associated with multiple semantically-distinct questions. This seems to be born out by the result since most generated samples look reasonable despite low BLEU scores (see Tables 1, 3).</p>
<p>Our system vs. Seq2Seq Comparing our model to the Seq2Seq baseline, we see that all metrics improve notably with the exception of PPL. Interestingly, our system performs worse in terms of PPL despite achieving lower negative log-likelihood. This, along with the improvements in BLEU, F1 and QA, suggests that our system learns a more powerful conditional model at the expense of accurately modelling the marginal distribution over questions. It is likely challenging for the model to allocate probability mass to rarer keywords that are helpful to recover the desired answer while also minimizing perplexity. We illustrate with samples from both models, specifically the first two samples in Table 3. The Seq2Seq baseline generated a wellformed English question, which is also quite vague - it is only weakly conditioned on the answer. On the other hand, our system's generated question is more specific, but still not correct given the context and perhaps less fluent given the repetition of the word language. We found that our proposed entropy regularization helped to avoid over-fitting and worked nicely in tandem with dropout: the training loss for our regularized model was 26.6 compared to 22.0 for the Seq2Seq baseline that used only dropout regularization.</p>
<p>Policy gradient ( $R_{\mathrm{PPL}}: \lambda_{\mathrm{PPL}}=0.1$ ) Policy gradient training with the negative perplexity of the pretrained language model improves the generator's PPL score as desired, which approaches that of the baseline Seq2Seq model. However, QA, F1, and BLEU scores decrease. This aligns with the above observation that fluency and answerability (as measured by the automatic scores) may be in competition. As an example, the third sample in Table 3 is more fluent than the previous examples but does not refer to the desired answer.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Training</th>
<th style="text-align: center;">Generated Questions</th>
<th style="text-align: center;">QA</th>
<th style="text-align: center;">PPL</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$R_{\mathrm{PPL}}$</td>
<td style="text-align: center;">what was the name of the library that was listed on the grainger market?</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">73.2</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{QA}}$</td>
<td style="text-align: center;">the grainger market architecture was listed in 1954 by what?</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">775</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{QA}+\mathrm{PPL}}$</td>
<td style="text-align: center;">what language did the grainger market architecture belong to?</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">257</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{PPL}}$</td>
<td style="text-align: center;">what are the main areas of southern california?</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">114</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{QA}}$</td>
<td style="text-align: center;">southern california is famous for what?</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">269</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{QA}+\mathrm{PPL}}$</td>
<td style="text-align: center;">what is southern california known for?</td>
<td style="text-align: center;">16.6</td>
<td style="text-align: center;">179</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{PPL}}$</td>
<td style="text-align: center;">what was the goal of the imperial academy of medicine?</td>
<td style="text-align: center;">19.1</td>
<td style="text-align: center;">44.3</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{QA}}$</td>
<td style="text-align: center;">why were confucian scholars attracted to the medical profession?</td>
<td style="text-align: center;">73.7</td>
<td style="text-align: center;">405</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{QA}+\mathrm{PPL}}$</td>
<td style="text-align: center;">what did the confucian scholars believe were attracted to the medical schools?</td>
<td style="text-align: center;">90.9</td>
<td style="text-align: center;">135</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{PPL}}$</td>
<td style="text-align: center;">what is an example of a theory that can be solved in theory?</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">38</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{QA}}$</td>
<td style="text-align: center;">in complexity theory, it is known as what?</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">194</td>
</tr>
<tr>
<td style="text-align: center;">$R_{\mathrm{QA}+\mathrm{PPL}}$</td>
<td style="text-align: center;">what is an example of a theory that can cause polynomial-time solutions to be useful?</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">37</td>
</tr>
</tbody>
</table>
<p>Table 4: Comparison of questions from different reward combinations on the same text and answer.</p>
<p>Policy gradient ( $R_{\mathrm{QA}}: \lambda_{\mathrm{QA}}=1.0$ ) Policy gradient is very effective at maximizing the QA reward, gaining $8.9 \%$ in accuracy over the improved Seq2Seq model and improving most other metrics as well. The fact that QA score is $3.7 \%$ higher than that obtained on the ground-truth questions suggests that the question generator may have learned to exploit MPCM's answering mechanism, and the higher reported perplexity suggests questions under this scheme may be less fluent. We explore this in more detail below. The fourth sample in Table 3, in contrast to the others, is clearly answered by the context word gaelic as desired.</p>
<p>Policy gradient ( $R_{\mathrm{PPL}+\mathrm{QA}}: \lambda_{\mathrm{PPL}}=0.25, \lambda_{\mathrm{QA}}=$ $0.5)$ We attempted to improve fluency and answerability in tandem by combining QA and PPL rewards. The PPL reward adds a prior towards questions that look natural. According to Table 2, this optimization scheme yields a good balance of performance, improving over the maximum-likelihood model by a large margin in terms of QA performance and gaining back some PPL. In the sample shown in Table 3, however, the question is specific to the answer but ends prematurely.</p>
<p>In Table 4 we provide additional generated samples from the different PG rewards. This table reveals one of the 'tricks' encouraged by the QA reward for improving MPCM performance: questions are often phrased with the interrogative 'wh' word at the end. This gives the language high perplexity, since such questions are rarer in the training data, but brings the question form closer to the form of the source text for answer matching.</p>
<h3>5.5 Discussion</h3>
<p>Looking through examples revealed certain difficulties in the task and some pathologies in the model that should be rectified through future work.</p>
<p>Entities and Verbs Similar entities and related verbs are often swapped, e.g., miami for jacksonville in a question about population. This issue could be mitigated by biasing the pointer softmax towards the document for certain word types.</p>
<p>Abstraction We desire a system that generates interesting questions, which are not limited to reordering words from the context but exhibit some abstraction. Rewards from existing QA systems do not seem beneficial for this purpose. Questions generated through NLL training show more abstraction at the expense of decreased specificity.</p>
<p>Commonsense and Reasoning Commonsense understanding appears critical for generating questions that are well-posed and show abstraction from the original text. Likewise, the ability to reason about and compose relations between entities could lead to more abstract and interesting questions. The existing model has no such capacities.</p>
<p>Evaluation Due to the large number of possible questions given a predefined answer, it is challenging to evaluate the outputs using standard overlap-based metrics such as BLEU. In this sense, question generation from text is similar to other tasks with large output spaces (Galley et al., 2015) and may benefit from corpora with multiple ground-truth questions associated to a quality rating (Mostafazadeh et al., 2016).</p>
<h2>6 Conclusion and Future Work</h2>
<p>We proposed a recurrent neural model that generates natural-language questions conditioned on text passages and predefined answers. We showed how to train this model using a combination of maximum likelihood and policy gradient optimization, and demonstrated both quantitatively and qualitatively how several reward combinations affect the generated outputs. We are now undertaking a human evaluation to determine the correlation between rewards and human judgments, improving our model, and testing on additional datasets.</p>
<p>One of our interests is to build models that seek information autonomously through question asking, as people do. This would entail, among other things, the direct sampling of interesting, informative questions from documents, i.e., modelling distribution $p(Q \mid D)$ rather than the distribution conditioned on the answer, $p(Q \mid D, A)$, as in this work. The present work may serve as a useful first step toward this goal, since the larger problem can be tackled by factorizing $p(Q \mid D)=$ $\sum_{A} p(Q \mid D, A) p(A \mid D)$ and first sampling a document's likely answers according to modelled distribution $p(A \mid D)$.</p>
<h2>References</h2>
<p>Husam Ali, Yllias Chali, and Sadid A Hasan. 2010. Automation of question generation from sentences. Proc. of QG2010: The Third Workshop on Question Generation .</p>
<p>Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2016. An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086 .</p>
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. International Conference on Learning Representations (ICLR) .</p>
<p>Yoshua Bengio, Réjean Ducharme, and Pascal Vincent. 2001. A neural probabilistic language model. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, NIPS'2000. MIT Press, pages 932938 .</p>
<p>Yllias Chali and Sina Golestanirad. 2016. Ranking automatically generated questions using common human queries. Proc. of INLG .</p>
<p>Kyunghyun Cho, Bart Van Merriënboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning
phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078 .</p>
<p>François Chollet. 2015. keras. https://github. com/fchollet/keras.</p>
<p>Michel Galley, Chris Brockett, Alessandro Sordoni, Yangfeng Ji, Michael Auli, Chris Quirk, Margaret Mitchell, Jianfeng Gao, and Bill Dolan. 2015. deltableu: A discriminative metric for generation tasks with intrinsically diverse targets. arXiv preprint arXiv:1506.06863 .</p>
<p>Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial nets. In Advances in neural information processing systems. pages 2672-2680.</p>
<p>Alex Graves. 2012. Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711 .</p>
<p>Caglar Gulcehre, Sungjin Ahn, Ramesh Nallapati, Bowen Zhou, and Yoshua Bengio. 2016. Pointing the unknown words. arXiv preprint arXiv:1603.08148 .</p>
<p>Michael Heilman and Noah A Smith. 2010. Good question! statistical ranking for question generation. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, pages 609-617.</p>
<p>Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil Blunsom. 2015. Teaching machines to read and comprehend. In Advances in Neural Information Processing Systems. pages 1693-1701.</p>
<p>Felix Hill, Antoine Bordes, Sumit Chopra, and Jason Weston. 2015. The goldilocks principle: Reading children's books with explicit memory representations. arXiv preprint arXiv:1511.02301 .</p>
<p>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory. Neural Computation 9.8:17351780 .</p>
<p>Rudolf Kadlec, Martin Schmid, Ondrej Bajgar, and Jan Kleindienst. 2016. Text understanding with the attention sum reader network. arXiv preprint arXiv:1603.01547 .</p>
<p>Kirthevasan Kandasamy and Yoram Bachrach. 2017. Batch policy gradient methods for improving neural conversation models. Proc. of ICLR .</p>
<p>Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR).</p>
<p>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems. pages 3294-3302.</p>
<p>Girish Kumar, Rafael E Banchs, and Luis Fernando D'Haro Enriquez. 2015. Revup: Automatic gap-fill question generation from educational texts. Proc. of ACL .</p>
<p>Igor Labutov, Sumit Basu, and Lucy Vanderwende. 2015. Deep questions without deep understanding. Proc. of ACL .</p>
<p>Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. 2016. Deep reinforcement learning for dialogue generation. Proc. of EMNLP .</p>
<p>David Lindberg, Fred Popowich, and John Nesbit Phil Winne. 2013. Generating natural language questions to support learning on-line. Proc. of ENLG .</p>
<p>Prashanth Mannem, Rashmi Prasad, and Aravind Joshi. 2010. Question generation from paragraphs at upenn: Qgstec system description. In Proceedings of QG2010: The Third Workshop on Question Generation. pages 84-91.</p>
<p>Karen Mazidi and Rodney D Nielsen. 2015. Leveraging multiple views of text for automatic question generation. In International Conference on Artificial Intelligence in Education. Springer, pages 257266.</p>
<p>Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Margaret Mitchell, Xiaodong He, and Lucy Vanderwende. 2016. Generating natural questions about an image. arXiv preprint arXiv:1603.06059 .</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: A human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268 .</p>
<p>Mohammad Norouzi, Samy Bengio, Zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Wu, and Dale Schuurmans. 2016. Reward augmented maximum likelihood for neural structured prediction. In Advances In Neural Information Processing Systems.</p>
<p>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, pages 311-318.</p>
<p>Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. 2013. How to construct deep recurrent neural networks. arXiv preprint arXiv:1312.6026 .</p>
<p>Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP). pages 1532-1543. http://www.aclweb.org/anthology/D14-1162.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250 .</p>
<p>Marc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2016. Sequence level training with recurrent neural networks. Proc. of ICLR .</p>
<p>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention flow for machine comprehension. arXiv preprint arXiv:1611.01603 .</p>
<p>Iulian Vlad Serban, Alberto García-Durán, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, and Yoshua Bengio. 2016. Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. Proc. of ACL .</p>
<p>Yelong Shen, Po-Sen Huang, Jianfeng Gao, and Weizhu Chen. 2016. Reasonet: Learning to stop reading in machine comprehension. arXiv preprint arXiv:1609.05284 .</p>
<p>Harry Singer and Dan Donlan. 1982. Active comprehension: Problem-solving schema with question generation for comprehension of complex short stories. Reading Research Quarterly pages 166-186.</p>
<p>Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015. A neural network approach to context-sensitive generation of conversational responses. arXiv preprint arXiv:1506.06714 .</p>
<p>Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. J. Mach. Learn. Res. 15(1):1929-1958. http://dl.acm.org/citation.cfm?id=2627435.2670313.</p>
<p>Theano Development Team. 2016. Theano: A Python framework for fast computation of mathematical expressions. arXiv e-prints abs/1605.02688. http://arxiv.org/abs/1605.02688.</p>
<p>Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2016a. Newsqa: A machine comprehension dataset. arXiv preprint arXiv:1611.09830 .</p>
<p>Adam Trischler, Zheng Ye, Xingdi Yuan, Phil Bachman, Alessandro Sordoni, and Kaheer Suleman. 2016b. Natural language comprehension with the epireader. In Empirical Methods on Natural Language Processing (EMNLP).</p>
<p>Hado P van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr Mnih, and David Silver. 2016. Learning values across many orders of magnitude. In $A d$ vances in Neural Information Processing Systems. pages 4287-4295.</p>
<p>Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural Information Processing Systems. pages 2692-2700.</p>
<p>Zhiguo Wang, Haitao Mi, Wael Hamza, and Radu Florian. 2016. Multi-perspective context matching for machine comprehension. arXiv preprint arXiv:1612.04211 .</p>
<p>Ronald J Williams. 1992. Simple statistical gradientfollowing algorithms for connectionist reinforcement learning. Machine learning 8(3-4):229-256.</p>
<p>Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and William W Cohen. 2017. Semi-supervised qa with generative domain-adaptive nets. arXiv preprint arXiv:1702.02206 .</p>
<p>Xingxing Zhang and Mirella Lapata. 2017. Sentence simplification with deep reinforcement learning. arXiv preprint arXiv:1703.10931 .</p>
<p>Shiqi Zhao, Haifeng Wang, Chao Li, Ting Liu, and Yi Guan. 2011. Automatically generating questions from queries for community-based question answering. Proc. of IJCNLP .</p>
<h2>Supplementary Material</h2>
<h2>A Implementation details</h2>
<p>All models are implemented using Keras (Chollet, 2015) with Theano (Theano Development Team, 2016) backend. We used Adam (Kingma and Ba, 2014) with an initial learning rate $2 \mathrm{e}-4$ for both maximum likelihood and policy gradient updates. Word embeddings were initialized with the GloVe vectors (Pennington et al., 2014) and updated during training. The hidden size for all RNNs is 768.</p>
<p>Dropout (Srivastava et al., 2014) is applied with a rate of 0.3 to the embedding layers as well as all the RNNs (between both input-hidden and hiddenhidden connections).</p>
<p>Both $\lambda_{s}$ for answer-suppression and $\lambda_{e}$ for entropy maximization are set to 0.01 . We used beam search with a beam size of 32 in all experiments. The reward weights used in policy gradient training are listed in Table 5. These parameters are selected using grid search based on validation QA reward.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\mathrm{QA}_{\text {MPCM }}$</th>
<th style="text-align: center;">$\mathrm{PPL}_{\text {Quest. } \mathrm{LM}}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\lambda_{\mathrm{QA}}$</td>
<td style="text-align: center;">1.0</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">$\lambda_{\mathrm{PPL}}$</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">0.1</td>
</tr>
<tr>
<td style="text-align: center;">$\lambda_{\mathrm{PPL}+\mathrm{QA}}$</td>
<td style="text-align: center;">0.5</td>
<td style="text-align: center;">0.25</td>
</tr>
</tbody>
</table>
<p>Table 5: Hyperparameter settings for policy gradient training.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ We also experimented with a stochastic version of beam search by randomly sampling $k$ words from top- $2 k$ predictions sorted by candidate sequence probability at each time step. No performance improvement was observed.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ Let $|X|$ denote the length of sequence $X$.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
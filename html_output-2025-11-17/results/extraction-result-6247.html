<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-6247 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-6247</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-6247</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-122.html">extraction-schema-122</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <p><strong>Paper ID:</strong> paper-777d4ec0148c34b0bfab91e9ac3a902e420b891e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/777d4ec0148c34b0bfab91e9ac3a902e420b891e" target="_blank">Verbosity Bias in Preference Labeling by Large Language Models</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This work examines the biases that come along with evaluating LLMs with other LLMs and takes a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities.</p>
                <p><strong>Paper Abstract:</strong> In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias -- a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.</p>
                <p><strong>Cost:</strong> 0.011</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e6247.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e6247.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4 verbosity preference (Section 4)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4 preference for longer answers in creative writing tasks (experiment in Section 4)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An experiment showing that GPT-4, when used as a judge, tends to prefer longer/more verbose answers produced by the same LLM (Vicuna-7b-v1.5) for creative-writing prompts; preference strength increases with word-count difference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Creative writing / open-ended question answering</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>No human evaluators in this experiment; 100 sample answers per prompt were generated by Vicuna-7b-v1.5 (temperature=0.7) for 3 creative prompts. GPT-4 judged pairwise comparisons using a fixed template; each pair was evaluated twice with positions swapped and counted as a draw if the two permutations disagreed.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Pairwise preference outcome (first option selected / draw / second option selected) binned and plotted vs. relative word-count difference between the pair.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>GPT-4 shows a positive correlation between preference for an option and its word count: when the length difference is large GPT-4 almost always prefers the longer answer; when differences are small evaluation variance is high. The experiment cannot by itself prove bias because there is no ground-truth human oracle in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Preference magnitude depends on prompt (shape varies by question), so verbosity effect is not solely a function of raw word count; results may reflect heuristics learned by GPT-4 from its training data rather than objective quality. The experiment also did not use human labels, so longer answers could theoretically be higher quality.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>High variance and contradictions when word-count differences are small; near-deterministic preference for longer answers when length differences are large (risk of rewarding verbosity even if content quality is equal or worse).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors accounted for position bias by swapping positions and treating inconsistent permutations as draws. They discuss, elsewhere in the paper, general mitigations (chain-of-thought, few-shot prompting) though not applied in this specific experiment.</td>
                        </tr>
                        <tr>
                            <td><strong>confidence</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbosity Bias in Preference Labeling by Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6247.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e6247.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LLM vs Human alignment on verbosity (Section 5, HH-RLHF)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of GPT-4/GPT-3.5 judgments to human labels in HH-RLHF (human alignment vs. verbosity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An analysis using the HH-RLHF human-preference dataset showing that LLM judges (GPT-4 and GPT-3.5) have higher alignment with humans when humans preferred longer answers, but lower alignment when humans preferred shorter answers; quantified by a signed verbosity-bias metric.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Dialogue / helpfulness evaluation (conversational assistants)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 and GPT-3.5</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human labels taken from the HH-RLHF dataset (Bai et al., 2022a); each conversation pair has a single human preference label indicating which assistant was more helpful. GPT judges were asked to evaluate the whole conversation pair using the same template; agreement with the single human label was measured.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Human alignment (agreement rate between LLM judge decision and human label) as a function of relative word-count difference between chosen and rejected answers; also the authors' verbosity-bias scalar metric (see Section 6) computed for each LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>When humans preferred longer answers, LLM alignment was high (LLMs tended to agree). When humans preferred shorter answers, LLM alignment dropped substantially because GPT-4 and GPT-3.5 still tended to choose the longer answer. Quantitatively, the paper reports verbosity-bias values of 0.328 for GPT-4 and 0.428 for GPT-3.5 (positive values indicate preference toward verbosity).</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>LLMs show a persistent tendency to favor verbosity that lowers agreement with humans specifically in cases where humans prefer concision; this suggests a learned heuristic favoring longer responses. The HH-RLHF dataset provides only a single human label per pair, limiting per-prompt human preference estimation.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Cases where human label selects the more concise answer but LLM judge selects the longer one, producing low human alignment for those instances (documented as a general pattern rather than isolated examples).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors recommend plotting human-alignment vs. length-difference alongside any scalar metric, calling for further experiments across more LLMs and prompts; general mitigations mentioned in the paper (see separate entry) include position-swapping, chain-of-thought prompting, few-shot examples, and calibration techniques from prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>confidence</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbosity Bias in Preference Labeling by Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6247.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e6247.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Verbosity-bias metric (accuracy parity formulation)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Signed verbosity bias based on accuracy parity / difference in inaccuracy conditioned on which response is longer</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A quantitative metric introduced in this paper that measures the degree and direction of verbosity bias of an LLM judge, defined as the difference in error rates between groups where the chosen option is the longer vs. shorter response.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>Evaluation bias quantification for preference-labeling / RLAIF</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Applied to GPT-4 and GPT-3.5 in the paper; metric is model-agnostic.</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Requires human-labeled preference pairs (used HH-RLHF in the paper) to compute accuracy parity conditioned on the sensitive attribute S (which option is longer).</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Metric = P(Y' = 1 - Y | S = 1 - Y) - P(Y' = 1 - Y | S = Y), where Y is human-chosen option, Y' is LLM decision, and S indicates which option is longer; positive indicates preference for verbose answers.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Applied to the HH-RLHF experiment, yields positive values (GPT-4: 0.328, GPT-3.5: 0.428), indicating both LLMs biased toward longer answers, with GPT-3.5 showing a larger bias than GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>The metric only partitions pairs by which answer is longer (binary S) and is agnostic to magnitude of length difference; it cannot detect within-group effects (e.g., different behavior for slightly vs. vastly longer answers). It can therefore understate bias in cases where bias varies with magnitude of length difference.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>A model that is highly aligned when large length differences exist but misaligned when differences are small could produce a near-zero scalar under this metric; authors caution against relying solely on the scalar and recommend plotting alignment curves alongside the metric.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Authors recommend reporting the human-alignment plot (alignment vs. relative length difference) in addition to the scalar metric to expose within-group effects.</td>
                        </tr>
                        <tr>
                            <td><strong>confidence</strong></td>
                            <td>high</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbosity Bias in Preference Labeling by Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e6247.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e6247.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between LLM-as-a-judge and human evaluations, including reported differences, limitations, failure cases, and mitigation strategies.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Mitigations & calibration methods (mentioned)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Mitigation strategies for biases in automated LLM evaluation (chain-of-thought, few-shot, position-swap, multiple-evidence calibration, human-in-the-loop calibration)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A set of mitigation techniques discussed in the paper (some proposed by prior work) to reduce biases when using LLMs as judges: request chain-of-thought, provide few-shot examples, swap positions and treat contradictions as draws, and apply calibration methods like multiple-evidence and human-in-the-loop.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>task_domain</strong></td>
                            <td>General automated evaluation / RLAIF</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>General (techniques applicable to GPT-family and other LLM judges)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Not a direct human-evaluation protocol in the experiments here; some methods (human-in-the-loop calibration) explicitly require human intervention when LLM judgments are uncertain or misaligned.</td>
                        </tr>
                        <tr>
                            <td><strong>metrics_compared</strong></td>
                            <td>Not a metric per se; these are procedural mitigations aimed at improving alignment, consistency, and reducing biases like position and verbosity in pairwise judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>reported_differences</strong></td>
                            <td>Paper notes prior work (Wang et al., 2023) observed position biases (GPT-4 preferring first option, ChatGPT preferring second) and proposed multiple-evidence calibration and human-in-the-loop calibration; chain-of-thought is suggested to encourage more reasoned judgments and few-shot prompting to bias the judge toward desired patterns.</td>
                        </tr>
                        <tr>
                            <td><strong>llm_specific_limitations</strong></td>
                            <td>Authors do not empirically evaluate all mitigations in this paper; effectiveness may vary by task and LLM. Some mitigations (e.g., chain-of-thought) may expose internal reasoning and be sensitive to prompt formulation and may not always be reliable.</td>
                        </tr>
                        <tr>
                            <td><strong>notable_failure_cases</strong></td>
                            <td>Not evaluated exhaustively in this paper; prior work cited shows varying success (e.g., Zheng et al., 2023 found GPT-4 less susceptible to repetitive-list verbosity attacks than GPT-3.5/Claude-v1).</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_strategies</strong></td>
                            <td>Enumerated mitigations: position-swapping with permutation-checking, chain-of-thought prompting, one-shot/few-shot prompting, multiple-evidence calibration, human-in-the-loop calibration; authors recommend combining scalar metrics with alignment plots and further cross-model experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>confidence</strong></td>
                            <td>medium</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Verbosity Bias in Preference Labeling by Large Language Models', 'publication_date_yy_mm': '2023-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Training a helpful and harmless assistant with reinforcement learning from human feedback <em>(Rating: 2)</em></li>
                <li>Judging llm-as-a-judge with mt-bench and chatbot arena <em>(Rating: 2)</em></li>
                <li>Large language models are not fair evaluators <em>(Rating: 2)</em></li>
                <li>Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles <em>(Rating: 2)</em></li>
                <li>Rlaif: Scaling reinforcement learning from human feedback with ai feedback <em>(Rating: 1)</em></li>
                <li>Constitutional ai: Harmlessness from ai feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-6247",
    "paper_id": "paper-777d4ec0148c34b0bfab91e9ac3a902e420b891e",
    "extraction_schema_id": "extraction-schema-122",
    "extracted_data": [
        {
            "name_short": "GPT-4 verbosity preference (Section 4)",
            "name_full": "GPT-4 preference for longer answers in creative writing tasks (experiment in Section 4)",
            "brief_description": "An experiment showing that GPT-4, when used as a judge, tends to prefer longer/more verbose answers produced by the same LLM (Vicuna-7b-v1.5) for creative-writing prompts; preference strength increases with word-count difference.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Creative writing / open-ended question answering",
            "llm_judge_model": "GPT-4",
            "human_evaluation_setup": "No human evaluators in this experiment; 100 sample answers per prompt were generated by Vicuna-7b-v1.5 (temperature=0.7) for 3 creative prompts. GPT-4 judged pairwise comparisons using a fixed template; each pair was evaluated twice with positions swapped and counted as a draw if the two permutations disagreed.",
            "metrics_compared": "Pairwise preference outcome (first option selected / draw / second option selected) binned and plotted vs. relative word-count difference between the pair.",
            "reported_differences": "GPT-4 shows a positive correlation between preference for an option and its word count: when the length difference is large GPT-4 almost always prefers the longer answer; when differences are small evaluation variance is high. The experiment cannot by itself prove bias because there is no ground-truth human oracle in this setup.",
            "llm_specific_limitations": "Preference magnitude depends on prompt (shape varies by question), so verbosity effect is not solely a function of raw word count; results may reflect heuristics learned by GPT-4 from its training data rather than objective quality. The experiment also did not use human labels, so longer answers could theoretically be higher quality.",
            "notable_failure_cases": "High variance and contradictions when word-count differences are small; near-deterministic preference for longer answers when length differences are large (risk of rewarding verbosity even if content quality is equal or worse).",
            "mitigation_strategies": "Authors accounted for position bias by swapping positions and treating inconsistent permutations as draws. They discuss, elsewhere in the paper, general mitigations (chain-of-thought, few-shot prompting) though not applied in this specific experiment.",
            "confidence": "high",
            "uuid": "e6247.0",
            "source_info": {
                "paper_title": "Verbosity Bias in Preference Labeling by Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "LLM vs Human alignment on verbosity (Section 5, HH-RLHF)",
            "name_full": "Comparison of GPT-4/GPT-3.5 judgments to human labels in HH-RLHF (human alignment vs. verbosity)",
            "brief_description": "An analysis using the HH-RLHF human-preference dataset showing that LLM judges (GPT-4 and GPT-3.5) have higher alignment with humans when humans preferred longer answers, but lower alignment when humans preferred shorter answers; quantified by a signed verbosity-bias metric.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Dialogue / helpfulness evaluation (conversational assistants)",
            "llm_judge_model": "GPT-4 and GPT-3.5",
            "human_evaluation_setup": "Human labels taken from the HH-RLHF dataset (Bai et al., 2022a); each conversation pair has a single human preference label indicating which assistant was more helpful. GPT judges were asked to evaluate the whole conversation pair using the same template; agreement with the single human label was measured.",
            "metrics_compared": "Human alignment (agreement rate between LLM judge decision and human label) as a function of relative word-count difference between chosen and rejected answers; also the authors' verbosity-bias scalar metric (see Section 6) computed for each LLM.",
            "reported_differences": "When humans preferred longer answers, LLM alignment was high (LLMs tended to agree). When humans preferred shorter answers, LLM alignment dropped substantially because GPT-4 and GPT-3.5 still tended to choose the longer answer. Quantitatively, the paper reports verbosity-bias values of 0.328 for GPT-4 and 0.428 for GPT-3.5 (positive values indicate preference toward verbosity).",
            "llm_specific_limitations": "LLMs show a persistent tendency to favor verbosity that lowers agreement with humans specifically in cases where humans prefer concision; this suggests a learned heuristic favoring longer responses. The HH-RLHF dataset provides only a single human label per pair, limiting per-prompt human preference estimation.",
            "notable_failure_cases": "Cases where human label selects the more concise answer but LLM judge selects the longer one, producing low human alignment for those instances (documented as a general pattern rather than isolated examples).",
            "mitigation_strategies": "Authors recommend plotting human-alignment vs. length-difference alongside any scalar metric, calling for further experiments across more LLMs and prompts; general mitigations mentioned in the paper (see separate entry) include position-swapping, chain-of-thought prompting, few-shot examples, and calibration techniques from prior work.",
            "confidence": "high",
            "uuid": "e6247.1",
            "source_info": {
                "paper_title": "Verbosity Bias in Preference Labeling by Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Verbosity-bias metric (accuracy parity formulation)",
            "name_full": "Signed verbosity bias based on accuracy parity / difference in inaccuracy conditioned on which response is longer",
            "brief_description": "A quantitative metric introduced in this paper that measures the degree and direction of verbosity bias of an LLM judge, defined as the difference in error rates between groups where the chosen option is the longer vs. shorter response.",
            "citation_title": "here",
            "mention_or_use": "use",
            "task_domain": "Evaluation bias quantification for preference-labeling / RLAIF",
            "llm_judge_model": "Applied to GPT-4 and GPT-3.5 in the paper; metric is model-agnostic.",
            "human_evaluation_setup": "Requires human-labeled preference pairs (used HH-RLHF in the paper) to compute accuracy parity conditioned on the sensitive attribute S (which option is longer).",
            "metrics_compared": "Metric = P(Y' = 1 - Y | S = 1 - Y) - P(Y' = 1 - Y | S = Y), where Y is human-chosen option, Y' is LLM decision, and S indicates which option is longer; positive indicates preference for verbose answers.",
            "reported_differences": "Applied to the HH-RLHF experiment, yields positive values (GPT-4: 0.328, GPT-3.5: 0.428), indicating both LLMs biased toward longer answers, with GPT-3.5 showing a larger bias than GPT-4.",
            "llm_specific_limitations": "The metric only partitions pairs by which answer is longer (binary S) and is agnostic to magnitude of length difference; it cannot detect within-group effects (e.g., different behavior for slightly vs. vastly longer answers). It can therefore understate bias in cases where bias varies with magnitude of length difference.",
            "notable_failure_cases": "A model that is highly aligned when large length differences exist but misaligned when differences are small could produce a near-zero scalar under this metric; authors caution against relying solely on the scalar and recommend plotting alignment curves alongside the metric.",
            "mitigation_strategies": "Authors recommend reporting the human-alignment plot (alignment vs. relative length difference) in addition to the scalar metric to expose within-group effects.",
            "confidence": "high",
            "uuid": "e6247.2",
            "source_info": {
                "paper_title": "Verbosity Bias in Preference Labeling by Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        },
        {
            "name_short": "Mitigations & calibration methods (mentioned)",
            "name_full": "Mitigation strategies for biases in automated LLM evaluation (chain-of-thought, few-shot, position-swap, multiple-evidence calibration, human-in-the-loop calibration)",
            "brief_description": "A set of mitigation techniques discussed in the paper (some proposed by prior work) to reduce biases when using LLMs as judges: request chain-of-thought, provide few-shot examples, swap positions and treat contradictions as draws, and apply calibration methods like multiple-evidence and human-in-the-loop.",
            "citation_title": "",
            "mention_or_use": "mention",
            "task_domain": "General automated evaluation / RLAIF",
            "llm_judge_model": "General (techniques applicable to GPT-family and other LLM judges)",
            "human_evaluation_setup": "Not a direct human-evaluation protocol in the experiments here; some methods (human-in-the-loop calibration) explicitly require human intervention when LLM judgments are uncertain or misaligned.",
            "metrics_compared": "Not a metric per se; these are procedural mitigations aimed at improving alignment, consistency, and reducing biases like position and verbosity in pairwise judgments.",
            "reported_differences": "Paper notes prior work (Wang et al., 2023) observed position biases (GPT-4 preferring first option, ChatGPT preferring second) and proposed multiple-evidence calibration and human-in-the-loop calibration; chain-of-thought is suggested to encourage more reasoned judgments and few-shot prompting to bias the judge toward desired patterns.",
            "llm_specific_limitations": "Authors do not empirically evaluate all mitigations in this paper; effectiveness may vary by task and LLM. Some mitigations (e.g., chain-of-thought) may expose internal reasoning and be sensitive to prompt formulation and may not always be reliable.",
            "notable_failure_cases": "Not evaluated exhaustively in this paper; prior work cited shows varying success (e.g., Zheng et al., 2023 found GPT-4 less susceptible to repetitive-list verbosity attacks than GPT-3.5/Claude-v1).",
            "mitigation_strategies": "Enumerated mitigations: position-swapping with permutation-checking, chain-of-thought prompting, one-shot/few-shot prompting, multiple-evidence calibration, human-in-the-loop calibration; authors recommend combining scalar metrics with alignment plots and further cross-model experiments.",
            "confidence": "medium",
            "uuid": "e6247.3",
            "source_info": {
                "paper_title": "Verbosity Bias in Preference Labeling by Large Language Models",
                "publication_date_yy_mm": "2023-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
            "rating": 2
        },
        {
            "paper_title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
            "rating": 2
        },
        {
            "paper_title": "Large language models are not fair evaluators",
            "rating": 2
        },
        {
            "paper_title": "Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles",
            "rating": 2
        },
        {
            "paper_title": "Rlaif: Scaling reinforcement learning from human feedback with ai feedback",
            "rating": 1
        },
        {
            "paper_title": "Constitutional ai: Harmlessness from ai feedback",
            "rating": 1
        }
    ],
    "cost": 0.0109665,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Verbosity Bias in Preference Labeling by Large Language Models</h1>
<p>Keita Saito*<br>University of Tsukuba \&amp; RIKEN AIP<br>Tsukuba, Ibaraki 305-8573, Japan<br>keita.saito@bbo.cs.tsukuba.ac.jp</p>
<p>Koki Wataoka<br>LY Corporation<br>Chiyoda-ku, Tokyo 102-8282, Japan<br>koki.wataoka@lycorp.co.jp</p>
<p>Akifumi Wachi<br>LY Corporation<br>Chiyoda-ku, Tokyo 102-8282, Japan<br>akifumi.wachi@lycorp.co.jp</p>
<p>Youhei Akimoto<br>University of Tsukuba \&amp; RIKEN AIP<br>Tsukuba, Ibaraki 305-8573, Japan<br>akimoto@cs.tsukuba.ac.jp</p>
<h4>Abstract</h4>
<p>In recent years, Large Language Models (LLMs) have witnessed a remarkable surge in prevalence, altering the landscape of natural language processing and machine learning. One key factor in improving the performance of LLMs is alignment with humans achieved with Reinforcement Learning from Human Feedback (RLHF), as for many LLMs such as GPT-4, Bard, etc. In addition, recent studies are investigating the replacement of human feedback with feedback from other LLMs named Reinforcement Learning from AI Feedback (RLAIF). We examine the biases that come along with evaluating LLMs with other LLMs and take a closer look into verbosity bias - a bias where LLMs sometimes prefer more verbose answers even if they have similar qualities. We see that in our problem setting, GPT-4 prefers longer answers more than humans. We also propose a metric to measure this bias.</p>
<h2>1 Introduction</h2>
<p>Large Language Models (LLMs) have made tremendous strides in recent years and continue to gain popularity (Zhao et al., 2023). With its growing size in network parameters, its wide application ranges from conventional natural language processing tasks such as chat-bots, summarization, and translation, to other applications beyond its original intended use such as search engines, programming assistance, and foundation models (Zhao et al., 2023; Brants et al., 2007; Katz, 1987).</p>
<p>After pretraining for general purposes, LLMs are fine-tuned to further better their performance for specific tasks with supervised learning and RLHF - reinforcement learning from preference labeling feedback from humans (Stiennon et al., 2020; Ouyang et al., 2022). However, issues arise with RLHF where human feedback can become costly. To work around this problem, Reinforcement Learning from AI Feedback (RLAIF) was proposed (Bai et al., 2022b; Lee et al., 2023), which replaces human feedback with inexpensive feedback from other LLMs.</p>
<p>In many cases, the question lack a clear-cut "correct answer" and require creativity and imagination. As evident in an example of feedback by an LLM provided in Figure 1, when LLMs are tasked to assess responses to such prompts, the evaluation process can become arbitrary and introduce various biases. One prominent bias is the verbosity bias, which occurs when LLMs are influenced</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: An example of a prompt to an LLM to judge two texts and the verdict. There is no one correct answer, and a comprehensive judgement is required.
by verbosity, favoring longer and more verbose texts, even if they appear wordy or of lower quality. Without accounting for this bias, LLM agents may learn to generate unnecessarily long texts. This may result in failures in downstream tasks such as lengthy summarizations or chatbots that return verbose responses to simple questions.
While previous studies have explored the concept of verbosity bias, they have tended to focus on specific cases. Zheng et al. (2023) limits their problem setting to questions answered with lists in their experiment on verbosity bias, and Huang et al. (2023) conducted experiments on summarization tasks. Moreover, these do not compare the preferences of LLMs to those of humans. We believe that such a comparison is crucial in challenging the conjecture that longer answers are inherently better and that LLMs are actually correct in their preferences.</p>
<p>Our contributions. In this paper, we conduct experiments on verbosity bias and saw that 1) LLMs exhibit a preference for longer answers in creative writing tasks, and 2) there is a discrepancy between of LLMs and those of humans in verbosity preference. Additionally, we formulate a quantification for measuring verbosity bias based on accuracy parity. This can be used to compare LLMs on their degree of verbosity bias.</p>
<h1>2 Preliminaries</h1>
<p>After undergoing pretraining for general purposes, LLMs are fine-tuned to further improve their performance in specific tasks. Pretraining is accomplished through self-supervised learning, where the model is trained to predict the next token in a sentence. Once the LLM is able to generate cohesive sentences, we proceed to fine-tune the model to solve specific tasks. One approach to fine-tuning involves supervised learning using expert data. This method relies on examples where experts have solved the task at hand. An example of a conversational LLM trained solely using this approach is Vicuna (Chiang et al., 2023). Vicuna achieved performance comparable to ChatGPT by utilizing user-shared conversations with ChatGPT as expert data. However, it is worth noting that obtaining expert data is often challenging.
RLHF addresses the challenge of limited training data in supervised learning by leveraging human feedback (Stiennon et al., 2020; Ouyang et al., 2022). This approach not only mitigates data scarcity but also significantly enhances alignment with human preferences, a critical factor in applications such as question answering. In RLHF, a reward model is trained to closely match human feedback data, which acts as the reward signal in the subsequent RL phase. Prominent LLMs like ChatGPT and Bard adopt a hybrid approach, combining both supervised learning and RLHF techniques to further refine their alignment with human preferences.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Two phases in RLHF. First the reward model is trained to align with human preference by matching with human feedback. In the second RL phase, the trained reward model provides reward signals to the language model.</p>
<h1>2.1 RLHF</h1>
<p>The first step of RLHF is to fit the reward function to align with human feedback. RL directly from human feedback as reward signal is unstable and requires volume. Therefore, a reward model that acts as the reward signal later in the process is trained to be consistant with human preference. Given a dataset $\mathcal{D}$ consisting of the original question, a pair of generated text, and the human preference label on which is chosen and rejected, the reward model is trained by minimizing</p>
<p>$$
\mathcal{L}(\phi)=-\underset{\left(x, y_{\text {chosen }} ; y_{\text {rejected }}\right) \sim \mathcal{D}}{\mathbb{E}}\left[\log \sigma\left(r_{\phi}\left(y_{\text {chosen }} \mid x\right)-r_{\phi}\left(y_{\text {rejected }} \mid x\right)\right)\right]
$$</p>
<p>where $x$ is the prompt to the LLM, $y_{\text {chosen }}$ is the preferred text, $y_{\text {rejected }}$ is the rejected text, and $r_{\phi}$ parametrised with $\phi$ is the reward model that takes text as input and outputs the rating score.</p>
<p>In the second step of RLHF, now that we have a reward model to evaluate a generated text without human interaction, regular RL can take place. In this context, the state is the question and the generated text so far, action is the next token to generate, and the reward is $r_{\phi}(y)$ given after the full text is generated. This is equivalent to a task where a sparse reward is given only at episode termination. The LLM maximizes the signal from the reward model with the following:</p>
<p>$$
\max <em _phi="\phi">{\theta} \mathbb{E}\left[r</em>(x) \mid x\right)\right]
$$}\left(\pi_{\theta</p>
<p>where $\pi_{\theta}$ is a policy parameterized by $\theta$. An optional KL divergence term is added to penalize the policy from deviating from the original policy.</p>
<h3>2.2 RLAIF</h3>
<p>While RLHF brings down the cost of human labor compared to generating an expert data from scratch, human feedback is still costly. For example, in Wang et al. (2023), it cost around 3 minutes ( $\$ 0.75$ if $\$ 15.00$ per hour) per evaluation. In one occasion, OpenAI worked around this by employing people in Kenya on less than $\$ 2$ per hour pay in the process of labeling violent or innappropriate texts. They were under scrutiny for the unideal working conditions.</p>
<p>To combat these promblems, RLAIF was proposed. This method replaces human feedback with feedback from other LLMs. This brings down the cost significantly; in our case, evaluation cost around $\$ 0.05$ each, which is $1 / 15$ th compared to human feedback in the previously cited paper (Wang et al., 2023).</p>
<h1>2.3 Biases in Automated LLM Evaluation</h1>
<p>When LLMs evaluate generated texts, various biases are introduced. We provide below a list of biases discussed in various papers (Zheng et al., 2023; Bai et al., 2022a; Wang et al., 2023).
Position Bias: Position bias occurs when, in comparing generated texts, LLMs prefer the answer given in certain positions. If we define the ground truth probability of $a$ (the first parameter) preferred over $b$ (the second parameter) to be $P(a, b)$, it should be that $P\left(y_{0}, y_{1}\right)=1-P\left(y_{1}, y_{0}\right)$, meaning the position should have no affect on the judgement. Position bias is when the comparison by model is $\hat{P}\left(y_{0}, y_{1}\right) \neq\left(1-\hat{P}\left(y_{1}, y_{0}\right)\right)$. For example, GPT-4 tends to prefer the first option given to it, while ChatGPT prefers the second option (Wang et al., 2023). To account for this bias, we can simply swap the positions and evaluate the options twice. If the model gives contradicting results between permutations, we count it as a draw.
Wang et al. (2023) has proposed several methods to calibrate this bias further: Multiple Evidence Calibration asks the LLM to provide evidence before making judgement, and Human-in-the-Loop Calibration involves human adjustment when deemed neccessary.
Self-enhancement Bias: LLMs tend to prefer answers generated by itself compared to answers generated by other models. This becomes a problem when benchmarking LLMs by evaluating them with LLMs (Zheng et al., 2023), but not so much in the context of RLAIF, as the comparisons are always between answers generated by the same model.
Verbosity Bias: Verbosity bias refers to the bias where LLMs prefer longer, more verbose answers even if there are no difference in quality. Training with RLAIF with verbosity bias present can lead to LLMs generating excessively long responses, when in reality a much more concise response would suffice. In tasks such as question answering, a verbose response can be critical to its usefulness, but there aren't enough researches that look into this. For these reasons we take a closer look into this.
There are several proposed methods to mitigate the effect of biases.
Chain-of-thought Prompting is a prompting technique where the LLM is asked to provide the thought process before generating the actual evaluation. This way, at the time when the LLM generates the actual evaluation, it has its chain-of-thought to base its evaluation from. This encourages human alignment and more accurate evaluations, rather than arbitrary evaluations without thought.
One-shot/Few-shot Prompting is another prompting technique which gives one example/several examples of a prompt and its corresponding correct answer when promting the LLM. When generating the response, the LLM can continue the pattern from the examples to better align with the intended response.</p>
<h2>3 Related Works</h2>
<h3>3.1 RLAIF Advancements</h3>
<p>There have been several recent advancements in the field of RLAIF. Bai et al. (2022b) trained an LLM via RLAIF with limited human feedback. In this work, they claim that helpfulness and harmfulness have a trade-off relationship, and aim to train an LLM that keeps a balance between those two. Their method only requires human feedback in the helpfulness aspect, and harmless behavior is achieved purely from RLAIF. The LLMs trained in the work by Lee et al. (2023) achieved near-human performance in summarization tasks with RLAIF without any human feedback. While not a study on RLAIF itself, Zheng et al. (2023) evaluate LLMs with other LLMs as a judge and show that GPT-4 has a high human alignment and agrees with humans on over $80 \%$ of evaluations.</p>
<h3>3.2 On Verbosity Bias in Evaluations by LLMs</h3>
<p>Zheng et al. (2023) also provides lists of biases and methods to overcome them. Alongside their experiment on position bias, they experimented on verbosity bias by attempting a "repetitive list attack" on several LLMs. This attack pertains to "listing" tasks, in which the prompt asks to list several items (e.g. "What are examples of fruits that are round?"). The "repetitive list attack" is done by making the answers verbose by repeating items multiple times, and then asking the LLMs to evaluate these augmented answers. If the LLM evaluates these "repetitive lists" to be better than the</p>
<p>original, the attack is considered a success. Their results show GPT-4 is significantly less prone to this attack with below $10 \%$ success rate, while GPT-3.5 and Claude-v1 both suffer over $90 \%$ success rate. Compared to this research, we expand the problem setting to general question-answering tasks. Huang et al. (2023) tackle verbosity bias in summarization tasks. They found that GPT-4 actually prefers short responses in faithfulness and coverage when it comes to summarization, although this is seen strongly only in single-answer grading, and not in comparison grading. This suggests that verbosity bias can be different between different tasks.</p>
<p>Compared to these studies, our problem setting is more general and we compare the verbosity preference between humans and LLMS. The experiments conducted in these papers measure the difference in evaluations when the texts are artificially made verbose while maintaining the same content. The assumption is that elongating the texts would have no effect on a true evaluator, so the difference in evaluation indicates verbosity bias. In our attempt to broaden the problem setting, we make use of human feedback as the oracle instead of making this assumption.</p>
<h1>4 Verbosity Preference of LLMs</h1>
<p>First, we experiment to see how much LLMs actually prefer longer answers. We ask GPT-4 to choose between pairs of responses and examine if it prefers longer responses or not. We did not limit our scope to prompts answered with a specific format (like lists in Zheng et al. (2023)) in order to observe the LLMs' general tendency to prefer longer answers.</p>
<p>We generated 100 sample answers each to 3 prompts, all from the same model (Vicuna-7b-v1.5) generated with the temperature parameter set to 0.7 . One of the questions and two examples of the answers are as follows.</p>
<ul>
<li>
<p>Question: Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions</p>
</li>
<li>
<p>I recently had the opportunity to travel to Hawaii, and it was an experience that I will never forget. From stunning beaches to vibrant culture, there was so much to explore and enjoy during my time on the islands...</p>
</li>
<li>Hawaii is a tropical paradise, and I recently had the opportunity to experience it for myself. This archipelago is made up of eight main islands, each with its unique culture, traditions, and breathtaking landscapes. During my trip, I had the chance to visit several cultural sites, such as the Polynesian Cultural Center on Oahu...</li>
<li>...</li>
</ul>
<p>The prompts are taken from the library introduced by Zheng et al. (2023), all from the "creative" category because 1) answers generated to other categories didn't vary in word count enough to see verbosity bias and 2) GPT-4 was not good at judging answers in those categories. We then take answers from these generated samples and insert them into the template shown in Figure 1. With the template complete, we asked GPT-4 to evaluate preferences between pairs of answers with the template. The outcome is either the first option selected, a draw, or the second option selected. In order to account for position bias, GPT-4 evaluated the pair twice with the position swapped the second time. It was considered a draw unless it gave the same result on both permutations.</p>
<p>The results are shown in Figure 3a for the overall result, and Figures 3b to 3d for results from each prompt. Both in the overall result and the individual results, there is a tendency for GPT-4 to prefer longer answers. When the word count difference is large enough, GPT-4 almost always prefers the longer answer. For question 1 and 2, the preference is smooth and clear, while for question 3, when the word count difference is small, there is high variance in evaluation. As we can see the shape varies between questions, and therefore we can deduce that verbosity does not rely entirely on word count and is different for each question. This makes adjusting for verbosity post-evaluation hard unless we know the verbosity preference shape for the prompt in question.</p>
<p>From this experiment, we can draw the conclusion that GPT-4 generally prefers longer answers among those that are generated by the same LLM with the same prompt. However, this experiment by itself does not indicate that GPT-4 suffers from verbosity bias; it could be that the longer answers generated by vicuna are actually higher in quality and helpfulness. In order to truly measure verbosity bias, we would need the ground truth of each comparison which we do not have. Instead, we next utilize a dataset of human evaluations as the baseline.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Example experimental results for three questions (Question 1: Blog about trip to Hawaii, Question 2: Email to professor about paper, Question 3: Blog comparing smartphones). Figure 3a combines results from all three questions and Figures 3b to 3d show results for each indivisual quesion. X -axis is the percentage of the difference between the first and the second option compared to the length of the second option. Y-axis is the actual score, with 1.0 meaning the first option was selected, 0 meaning it was a draw, and -1.0 meaning the second option was selected. There is an positive correlation between word count difference between the two options and the resulting evaluation. The data points are binned with each ranging $20 \%$. The circles represent the average in each range and the errorbars show the standard deviation.</p>
<h1>5 Is There A Difference in Verbosity Preference Between LLMs and Humans?</h1>
<p>Considering that LLMs replace humans as annotators in RLAIF, it is sufficient if LLMs could replicate human feedback and it does not necessarily have to be aligned with the ground truth. As seen in Figure 4 which plots verbosity preference of humans in the HH-RLHF dataset described later, humans seem to prefer longer answers too. Whether or not the longer answers are actually helpful is irrelevant as long as the LLM and the human come to the same conclusion. In light of this, we compare the difference in verbosity preference between LLMs and humans. We can view this as verbosity bias since the aim of LLM judgment in RLAIF is human alignment and not the eradication of biased preference in verbosity.</p>
<p>We use the HH-RLHF dataset (Bai et al., 2022a) which contains human feedback data comparing pairs of answers to a prompt. It only has one feedback data per prompt, so we cannot plot the verbosity preference of humans like in the experiment in the previous chapter. Instead we can see the dissimilarity between LLMs and humans in verbosity preference in general across various questions. Precisely, this experiment looks into the relationship between the difference in a number of words in the pair of responses and the human alignment of LLMs, meaning how often LLMs give the same judgment as humans.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: X-axis is the percentage of the difference between the first and the second option compared to the length of the second option. Y-axis is the resulting score by human judgement.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: X-axis is the percentage of the difference between the chosen and the rejected option, compared to the length of the rejected option. Y-axis is the human alignment measured by the rate of LLM's decision agreeing with humans. The numbers in brackets indicate the sample size in each bracket.</p>
<p>We used the same prompt template as the previous experiment but asked GPT-4 to evaluate the whole conversation. Unlike the previous experiment which evaluated answers to a single question, HH-RLHF contains conversations between a human and an assistant. Therefore we asked GPT-4 to evaluate the pair of whole conversations and answer which assistant was more helpful.
In cases where human feedback preferred the longer answer, human alignment was high for the LLMs, meaning the LLMs preferred the longer answers as well. However, when human feedback chose the answer with fewer words, human alignment was low, because the LLMs still chose the longer answers regardless of the helpfulness of the shorter answer.
One possible explanation for this is that LLMs learned to mimic human behavior heuristically by choosing longer answers - in this dataset, human feedback did tend to favor longer responses as seen in Figure 4, and it is possible the dataset used to train GPT-3.5/GPT-4 had the same tendency. Nevertheless, a closer look into the cause is up for debate.</p>
<p>Table 1: Verbosity bias values calculated with (6) for GPT-4 and GPT-3.5 with data from experiment.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>GPT-4</th>
<th>GPT-3.5</th>
</tr>
</thead>
<tbody>
<tr>
<td>Verbosity Bias</td>
<td>0.328</td>
<td>0.428</td>
</tr>
</tbody>
</table>
<p>6 Formulation of Verbosity Bias</p>
<p>In the second experiment, we observed the tendency of LLMs to have low human alignment for cases where human feedback preferred shorter answers. In this section, we formulate verbosity bias to allow for quantative comparison between models.</p>
<p>In our problem setting, we define the given pair of text inputs as $y_{0}$ and $y_{1}$, the LLM outputs decision as $Y^{\prime}\in{0,1}$, and the more helpful option labeled by humans as $Y \in{0,1}$. We define the sensitive attribute $S \in{0,1}$ which equals 0 when $y_{0}$ has more words than $y_{1}$, and 1 when $y_{1}$ has more words than $y_{0}$.</p>
<p>With these definitions, equal opportunity (Hardt et al., 2016) with respect to sensitive attribute $S$ is satisfied if</p>
<p>$$
P\left(Y^{\prime}=0 \mid S=0, Y=0\right)=P\left(Y^{\prime}=0 \mid S=1, Y=0\right)
$$</p>
<p>This only accounts for cases where human feedback prefers $y_{0}$. Although this can be attained by sorting the inputs beforehand, the equation can be generalized with accuracy parity instead of equal opportunity. Accuracy parity is satisfied if the accuracy of prediction is equal among both demographics:</p>
<p>$$
P\left(Y^{\prime}=Y \mid S=Y\right)=P\left(Y^{\prime}=Y \mid S=1-Y\right)
$$</p>
<p>The deviance from accuracy parity can be calculated with the following equation:</p>
<p>$$
\left|P\left(Y^{\prime}=Y \mid S=Y\right)-P\left(Y^{\prime}=Y \mid S=1-Y\right)\right|
$$</p>
<p>Even though this is how the deviance is calculated in general, we thought it important that the directional information of the bias isn't lost. With the formulation below (6), a positive value indicates that the LLM prefers verbose answers, and a negative value indicates it prefers shorter answers. This distinction is crucial as some tasks may have a negative bias, for example in summarization tasks as shown in Huang et al. (2023). We also opted for the difference in inaccuracy between demographics</p>
<p>$$
P\left(Y^{\prime}=1-Y \mid S=1-Y\right)-P\left(Y^{\prime}=1-Y \mid S=Y\right)
$$</p>
<p>because verbosity bias refers to the inacuracy influenced by verbosity.
Table 1 shows the verbosity bias values of GPT-3.5 and GPT-4 calculated with data from Section 5. From these numbers, we can conclude that GPT-4 has improved in verbosity bias. Compared to Wang et al. (2023), which had a limited problem setting and gave the impression that GPT-4 is significantly less prone to verbosity bias, we see that the verbosity bias still exists for GPT-4. A further experiment on other LLMs for comparison is required.</p>
<h1>7 Discussion</h1>
<h3>7.1 Other Metrics of Equality</h3>
<p>In the context of our study, we treat the verbosity of the response pair as the sensitive attribute in our formulation of verbosity bias in Section 6. What verbosity differs from sensitive attributes generally discussed in other cases of biases is the fact that verbosity should actually be taken into consideration when evaluating the responses, whereas attributes like gender or race shouldn't be a factor in the outcome in other cases. This is why employing other metrics of equality like demographic parity doesn't make sense here, and therefore we base the measurement of verbosity bias on equal opportunity and accuracy parity.</p>
<h1>7.2 Limitations of Our Experiments</h1>
<p>In the experiment in Section 4, we generate the sample responses from the same questions (before concatenation of results from all three questions). However, in our experiment in Section 5, we mix together results from various questions. This has led us to only attain the result across many kinds of questions, not the result on any specific question like in the experiment in Section 4. It is debatable which of these results is preferable.</p>
<h3>7.3 Limitation of Our Metric of Verbosity Bias</h3>
<p>Our formulation of verbosity bias only accounts for bias between two groups divided by whether $y_{0}$ is longer than $y_{1}$. What it cannot detect is the bias within each of these groups; it is agnostic to the bias between cases where $y_{0}$ is barely longer than $y_{1}$ and cases where $y_{0}$ is significantly longer than $y_{0}$. Hence, if there were to be an instance where the model has high human alignment when there is a large difference in length between the pair of responses - the plot would have a concave shape symmetrical around the vertical line down the middle - our metric would suggest that the model has close to zero verbosity bias. To avoid such a situation, showing the human alignment plot alongside the metric is recommended.</p>
<h2>8 Conclusion</h2>
<p>In this paper, we conducted experiments on the verbosity bias seen in LLMs' judgment by LLMs. In previous works, the problem settings were limited and did not compare the verbosity preference to humans. With our experiments, we saw that 1) LLMs tend to favor longer answers for creative writing tasks, and 2) alignment with humans varies on verbosity with lower human alignment in cases where humans preferred shorter answers. We then formulated verbosity bias based on accuracy parity that can be used to quantitatively compare verbosity biases among models.</p>
<h2>References</h2>
<p>Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., et al. (2022a). Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.</p>
<p>Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., et al. (2022b). Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073.</p>
<p>Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J. (2007). Large language models in machine translation.
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. (2023). Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality.</p>
<p>Hardt, M., Price, E., and Srebro, N. (2016). Equality of opportunity in supervised learning. Advances in neural information processing systems, 29.</p>
<p>Huang, K.-H., Laban, P., Fabbri, A. R., Choubey, P. K., Joty, S., Xiong, C., and Wu, C.-S. (2023). Embrace divergence for richer insights: A multi-document summarization benchmark and a case study on summarizing diverse information from news articles. arXiv preprint arXiv:2309.09369.</p>
<p>Katz, S. (1987). Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE transactions on acoustics, speech, and signal processing, 35(3):400-401.</p>
<p>Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T., Bishop, C., Carbune, V., and Rastogi, A. (2023). Rlaif: Scaling reinforcement learning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267.</p>
<p>Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.</p>
<p>Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and Christiano, P. F. (2020). Learning to summarize with human feedback. Advances in Neural Information Processing Systems, $33: 3008-3021$.</p>
<p>Wang, P., Li, L., Chen, L., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., and Sui, Z. (2023). Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926.</p>
<p>Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., et al. (2023). A survey of large language models. arXiv preprint arXiv:2303.18223.</p>
<p>Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. (2023). Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Research done during internship at LY Corp.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
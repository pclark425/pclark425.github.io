<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2225 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2225</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2225</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-60.html">extraction-schema-60</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <p><strong>Paper ID:</strong> paper-278481240</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2505.05540v2.pdf" target="_blank">Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open Ended Action Environments</a></p>
                <p><strong>Paper Abstract:</strong> —Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution. However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in procedurally generated, out-of-distribution (OOD) environments, remains limited. In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLM and VLA models—including GPT-4o, GPT-4.1, OpenVLA, Pi0 Base, and Pi0 FAST—on diverse procedural tasks from the Procgen benchmark. Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexity; (2) OpenVLA generally outperforms other models due to its robust architectural design; and (3) GPT variants demonstrate substantial improvements when constrained appropriately, high-lighting the sensitivity of model performance to precise prompt engineering.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2225.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2225.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>OpenVLA (openvla/openvla-7b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source vision-language-action model fine-tuned on large robotics demonstrations that outputs normalized continuous actions which are unnormalized and clamped to discrete Procgen actions at inference time.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>OpenVLA</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A vision-language-action model trained on ~970k robotics demonstrations (Open X-Embodiment). At inference it takes images + short prompts, predicts normalized continuous actions (in [-1,1]) which are unnormalized using dataset statistics and clamped/rounded to the Procgen discrete action set. Outputs logits are extracted from a Llama 2 backbone for probability estimates.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied AI / simulated game agent benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Brier Mean Absolute Error (Brier MAE); micro/macro precision and recall; Percentage Invalids; normalized/quantile-filtered Brier MAE</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Brier MAE measures the absolute difference between the model's predicted probabilities (grouped from Llama token logits into Procgen discrete actions) and one-hot ground-truth action labels per timestep. Complementary metrics include micro/macro precision and recall computed against expert RL action labels, and percentage invalids measuring predictions outside the valid action space. Normalized and quantile-filtered variants assess outlier influence.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (probabilistic calibration metrics / surrogate objective comparing model probabilities to expert action labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Expert RL trajectory actions from Procgen (offline expert labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Ground truth consists of discrete action labels from expert reinforcement-learning agent trajectories on Procgen subdatasets (sourced from Facebook public repository). These are treated as the oracle actions for each timestep in evaluation/test splits.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Brier MAE: generally >1.50 (scale 0 best to 2 worst; paper reports OpenVLA generally exceeded 1.50 while other models often exceeded 1.70); Macro recall typically 9–12.5% (e.g., up to ~12.5% on Maze); micro precision up to ~27% on Coinrun; Percentage invalids = 0% (by design). Normalized and quantile-filtered Brier MAE follow same high-error trend (max relative Brier MAE ≈1.0).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Poor calibration: Brier MAE >1.50; macro recall ≈9–12.5% across datasets; micro precision peaked at ≈27% on Coinrun; zero invalid predictions (0%).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Out-of-distribution (zero-shot) — evaluation on Procgen is far from OpenVLA's continuous-robotics training distribution; performance characterized as poor on OOD tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Gap widens for large domain shifts (continuous robotics → discrete Procgen actions). OpenVLA's clamping mitigates some degradation (fewer invalids) but calibration errors (Brier MAE) remain high; best macro-recall appears on simpler tasks (e.g., Maze) and drops on complex/special-action datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Output clamping (constraint of unnormalized predictions to valid action ranges) and careful unnormalization/rounding. No experimental (wet-lab) calibration methods used.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Clamping eliminated invalid outputs (0% invalids) and improved practical action-space validity; OpenVLA nevertheless still exhibits high Brier MAE (>1.5) and limited macro recall, so clamping reduces one failure mode but does not restore calibration or accuracy fully. Quantitatively: OpenVLA Brier MAE ~1.5 vs others >1.7; macro recall ~9–12% vs others lower.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not discussed in monetary terms; evaluation is entirely computational. Inference run on an NVIDIA L4 instance for OpenVLA; no comparison to any experimental/real-world validation cost provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging: VLA models are active research systems with known domain-transfer limitations from robotics to simulated discrete-action games.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Poorly calibrated: Brier MAE values close to the maximum (1.5+ out of 2) indicate over/under-confident, inaccurate probabilities despite clamping. Normalized Brier MAE and quantile-filtered analyses confirm systemic miscalibration.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Metrics are consistent: normalized and quantile-filtered Brier MAE follow the same high-error trends, indicating proxies are correlated and not dominated by outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Single-stage computational cascade: model probabilistic predictions → comparison to offline expert RL action labels (no intermediate or experimental validation stages). Errors therefore measured directly against expert labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Major limitations include domain gap (robotics continuous actions vs Procgen discrete actions), low-resolution Procgen images (64x64 vs training 224x224), missing proprioceptive inputs, rounding/unnormalization artifacts, and poor probability calibration (high Brier MAE).</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Action-space mismatch (continuous → discrete), image resolution and view differences, dataset statistics used to unnormalize outputs, and decoding/tokenization effects that influence mapping to discrete actions.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2225.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2225.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pi0 Base</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pi0 Base (diffusion-based Pi0)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A diffusion-based vision-language-action model trained on robotics trajectories that reconstructs actions via denoising diffusion; produces distributed (uncertain) predictions on OOD inputs and lacks accessible logits for Brier MAE computation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pi0 Base</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Diffusion-based VLA trained on robotics trajectory data (OpenX-Embodiment plus private data). At inference the model uses flow-matching denoising across ten steps to predict actions (default action dimension 32 reduced to first dimension for Procgen). Produces action reconstructions that are 'close' to training distributions and tends to distribute probability mass across multiple classes on OOD inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied AI / simulated game agent benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Macro/micro precision & recall; percentage invalids; class-wise recall; normalized Brier MAE not computed (no logits available)</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Proxy evaluation compares Pi0 Base action outputs (decoded and unnormalized) to expert RL discrete actions using classification metrics (precision/recall/F1) and distributional comparisons (confusion matrices, action distributions). Brier MAE could not be computed due to absence of logits in its inference architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (diffusion generative surrogate compared to discrete expert labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Expert RL trajectory actions from Procgen (offline expert labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Discrete action labels from Procgen expert agent trajectories used as gold-standard per-timestep actions for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Brier MAE not available (no logits); macro recall typically ~6–10% (best ≈10% on simpler datasets); micro precision and class-wise recall show concentration on central actions (classes 4–7) with poor recall on peripheral actions (~0.02). Normalized/quantile Brier trends unavailable.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Macro recall ~6–10% across datasets; tendency to output distributed predictions (diffusion behavior) with moderate invalid output rates (fewer invalids than autoregressive Pi0 FAST but more than OpenVLA).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Out-of-distribution zero-shot (simulated game environments are far from robotics training data); performance is weaker on more novel/sparse-action datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Diffusion model tends to distribute predictions when inputs are OOD, producing uncertainty rather than collapsing to a single wrong class; this means larger practical error (low recall) on OOD, especially for sparse/timed special actions, but less catastrophic invalid-output rates than AR models.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>None explicitly beyond standard decoding; diffusion architecture itself is relied upon to provide proximity to training distributions. No clamping was applied as in OpenVLA.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Not reported; absence of logits prevents Brier-based calibration corrections; performance remains moderate but inferior to OpenVLA on macro recall.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Not discussed; inference used an NVIDIA A100 (40GB) for Pi0 Base.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging; diffusion-based VLA approaches are newer compared to AR methods for action decoding and have distinct OOD behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Not reported / not computable via Brier MAE because logits/probabilities are not exposed in the model inference path.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Where available, proxy metrics (macro recall, class-wise recall) are consistent with diffusion-style distributed predictions; normalized Brier MAE trends reported for other models are not directly comparable.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Single-stage: diffusion model outputs → direct comparison to offline expert actions.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>No accessible logits for standard probabilistic calibration metrics; diffusion reconstructions degrade on OOD inputs, leading to distributed but inaccurate predictions. Domain gap issues (continuous training → discrete Procgen), missing proprioception, and image resolution mismatches also degrade performance.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Diffusion generative inductive bias (reconstructs near-training manifold), action representation normalization, and missing training-view proprioceptive data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2225.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2225.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Pi0 FAST</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Pi0 FAST (autoregressive Pi0 Fast)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An autoregressive variant of Pi0 (Fast) that uses specialized tokenization and DCT+BPE token schemes to produce discrete action tokens; shows collapse and high invalid-output rates on OOD Procgen tasks and is slow at inference.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Pi0 FAST</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Autoregressive VLA variant using Paligemma backbone with DCT + BPE tokenization and short decoding token sequences; decoding constrained to four tokens and mapped to Procgen integer actions via static BPE token mappings. Predictions are autoregressive probabilities aggregated to action classes.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied AI / simulated game agent benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Brier MAE (computed via aggregated token probabilities), micro/macro precision & recall, percentage invalids, class-wise recall</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Brier MAE computed by mapping Paligemma token logits to Procgen integer actions and aggregating per-action probabilities; other metrics are standard classification metrics vs expert RL labels. Percentage invalids tracks tokens that don't correspond to valid actions or outputs outside action range.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (autoregressive token-probability surrogate compared to expert labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Expert RL trajectory actions from Procgen (offline expert labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Procgen expert RL discrete actions used as per-timestep gold labels.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Brier MAE: Pi0 FAST consistently >1.70 (paper reports >1.70 for GPT-4o/GPT-4.1/Pi0 FAST); macro recall frequently <6% and sometimes as low as ~2% on many datasets; invalid prediction rates high (some datasets >80% invalids); on certain datasets with frequent special actions (Starpilot, Bossfight) Pi0 FAST anomalously had recalls ≈10%.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Very poor overall: Brier MAE >1.70; macro recall often below 6%; minimum precision ~1% on some datasets; high invalid rates (>80% on some datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Zero-shot OOD evaluation; often far-from-training data causing severe collapse or invalid outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Pi0 FAST shows better recall on datasets where special actions are frequent (less sparse) and worse on sparse/timed-action datasets; invalid-output and collapse behaviors worsen with greater domain shift. Tokenization and DCT-based low-frequency prioritization cause concentration on certain global patterns and class biases.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Token mapping caching and input embedding caching were used to speed inference; no explicit clamping. The Genesis-style prompt adaptations were applied for Pi0 variants (zero arrays for missing views), but no effective calibration described.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Limited: caching improved inference speed but did not rectify high invalid rates or low recall. On some datasets Pi0 FAST had anomalous higher recall (~10%) where the training distribution aligned better with frequent special actions.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Pi0 FAST required 4x A100 GPUs due to heavy inference cost and was approximately ten times slower than OpenVLA; no monetary cost breakdown is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Emerging; autoregressive tokenization approaches for action prediction are actively developed but show brittle OOD behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Poor calibration (Brier MAE >1.70), and high invalid-output penalty contributes to worst-case Brier values; normalization/quantile-filtered analyses indicate little reliance on outliers (errors are uniformly high).</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Proxy metrics (Brier MAE, normalized variants, precision/recall) are consistent in indicating poor performance; high invalid rates dominate some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Computational-only: autoregressive predictions → mapping to Procgen actions → comparison to expert labels.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>High inference cost and latency, brittle autoregressive decoding leading to collapse to a few classes, very high invalid prediction rates when not clamped, DCT+BPE tokenization biases, and strong sensitivity to image complexity and missing proprioceptive inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Autoregressive overconfidence in OOD, tokenization-induced class biases, and mismatch between frequency of special actions in Procgen vs training data.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2225.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2225.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GPT-4o / GPT-4.1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>GPT-4o and GPT-4.1 (VLM-based agents via Genesis prompting)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Large vision-language models (GPT-4o and GPT-4.1) used as VLM agents via a prompt-engineering intermediary (Genesis) to convert Procgen image/state info into textual structured prompts; their outputs are parsed to probabilities over discrete actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GPT-4o and GPT-4.1 (via Genesis prompt engineering)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>General-purpose vision-language models interfaced through the Genesis framework that translates Procgen imagery and metadata into structured text prompts specifying action space and required output format. GPT-4.1 used a more carefully curated prompt to reduce invalid outputs. Outputs are expected as probability maps over action indices and then compared to expert labels.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied AI / simulated game agent benchmarking</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Brier MAE (for models where logits/probabilities were extracted), micro/macro precision & recall, percentage invalids; normalized/quantile-filtered Brier MAE</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Brier MAE and classification metrics quantify how the models' output probabilities (or structured outputs parsed into probabilities) align with expert RL action labels. Percentage invalids counts outputs not conforming to required vector-of-probabilities format or outside valid action space.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>data-driven ML prediction (VLM probabilistic outputs compared to expert action labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Expert RL trajectory actions from Procgen (offline expert labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Discrete actions recorded from offline expert RL agents on Procgen subdatasets serve as per-timestep ground-truth actions.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Brier MAE: GPT-4o and GPT-4.1 consistently scored >1.70 (close to max 2) indicating strong miscalibration; micro precision very low (GPT-4o as low as 1% on Starpilot); macro precision generally <15%; macro-recall for GPT-4.1 ~6–11% (≈11% on simple datasets), GPT-4o worse on special-action datasets (recall <2.5% on Fruitbot/Starpilot). Percentage invalids: GPT-4o and Pi0 FAST had some datasets with invalid rates >80%; GPT-4.1 improved due to prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Poor: Brier MAE >1.70; GPT-4.1 showed better macro-recall on simpler datasets (≈11%) and fewer invalid outputs after prompt refinement; GPT-4o showed low recall and high invalid rates on several datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Zero-shot, out-of-distribution (procedural games differ from web-scale vision-text training data), especially difficult for tasks requiring precise timed special actions.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Gap larger for tasks requiring precise timing or sparse special actions (e.g., 'fire' actions) — GPT-4o struggled particularly on such datasets (very low recall). Prompt engineering (Genesis) reduces invalids and can improve precision/recall somewhat (GPT-4.1 better than GPT-4o). Correlations with image complexity show moderate-to-weak negative correlations for GPT-4o/GPT-4.1 (Shannon entropy: -0.409 and -0.183 respectively), implying worse performance with increasing image complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Prompt engineering via the Genesis framework (more rigorous prompt for GPT-4.1 specifying simulated/hypothetical context, stricter format constraints) to reduce invalid outputs; also constraining outputs parsing and instructing valid action space.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Effective in reducing invalid outputs and improving per-dataset performance: GPT-4.1 outperformed GPT-4o on many subdatasets and produced fewer invalids; quantified improvements include higher macro-recall (~11% on simple datasets for GPT-4.1) and fewer invalid predictions compared to GPT-4o (GPT-4o had invalids >80% on some datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>Inference for GPT-4x family was run through OpenAI's Batch API and external cloud resources; no explicit cost comparison to experimental validation is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Mature in vision-language text tasks but emerging/underdeveloped for direct action prediction and zero-shot embodied control.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Poor calibration indicated by Brier MAE near worst possible values (>1.70); however, calibration improved modestly with targeted prompting (GPT-4.1 vs GPT-4o).</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Proxy metrics were consistent: normalized and quantile-filtered Brier MAE followed the same trends, indicating little sensitivity to outliers.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Computational-only cascade: structured prompt → VLM probabilistic output → parsing into action probabilities → comparison to offline expert labels. No intermediate experimental validation.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Format/validity of outputs (invalids), severe domain shift from web-scale vision-language to discrete action prediction, sensitivity to image complexity, bias to default actions (e.g., 'Do Nothing' or 'Right'), and strong class imbalance effects.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Prompt sensitivity, image complexity (entropy correlations), action space specification in prompt, and lack of proprioceptive state information for this domain.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2225.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2225.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of systems that use proxy metrics, computational predictions, or surrogate objectives for scientific discovery, and how these compare to experimental or ground-truth validation, including quantitative measures of agreement or disagreement.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Benchmarking suite / metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MultiNet v0.2 benchmarking suite (metrics: Brier MAE, macro/micro precision/recall, percentage invalids, normalized/quantile-filtered Brier)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The benchmark and metric suite (MultiNet v0.2) used to evaluate VLMs and VLAs on Procgen tasks using probabilistic calibration (Brier MAE), classification metrics, and invalid-output penalties to measure proxy predictive performance against expert RL ground-truth.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>MultiNet v0.2 evaluation framework</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>A unified evaluation and profiling framework that standardizes Procgen expert trajectories, ingest formats (TFDS), and a suite of proxy metrics including Brier MAE (with normalized and quantile variants), micro/macro precision/recall/F1, percentage invalids, class-wise metrics, and entropy-based image complexity analyses; used to compare several VLA/VLMs in zero-shot OOD settings.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>benchmarking / model evaluation for embodied AI</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_name</strong></td>
                            <td>Brier Mean Absolute Error (primary), normalized Brier MAE, normalized quantile-filtered Brier MAE, micro/macro precision & recall, percentage invalids, class-wise metrics</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_description</strong></td>
                            <td>Brier MAE computes per-timestep absolute error between model-assigned action probabilities and the one-hot expert action; invalid predictions receive maximum penalty (Brier score = 2). Normalized and quantile-filtered variants assess outlier influence. Precision/recall/F1 and class-wise analyses reveal bias and class-imbalance effects.</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_metric_type</strong></td>
                            <td>empirical surrogate (statistical/probabilistic metrics computed over model predictions vs offline expert labels)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_metric</strong></td>
                            <td>Procgen expert RL agent actions (offline trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_description</strong></td>
                            <td>Test splits of Procgen expert trajectories (10% random episodes per subdataset) are used as gold-standard labels for evaluation; no experimental wet-lab ground truth exists in this domain.</td>
                        </tr>
                        <tr>
                            <td><strong>has_both_proxy_and_ground_truth</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>quantitative_gap_measure</strong></td>
                            <td>Aggregate findings: Brier MAE values for evaluated models approached the maximum (2) with typical values >1.50–1.70; normalized/quantile-filtered variants track these trends. Macro metrics rarely exceeded ~15% and macro recall frequently in single digits to low tens of percent (OpenVLA ~9–12.5%). Percentage invalids varied widely (0% for OpenVLA, up to >80% for GPT-4o/Pi0 FAST on some datasets).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_performance</strong></td>
                            <td>Across models, proxy metrics indicate poor zero-shot generalization: high Brier MAE (~1.5–2.0), low macro/micro precision and recall, and high invalid rates for some models.</td>
                        </tr>
                        <tr>
                            <td><strong>ground_truth_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_positive_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>false_negative_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_characterization</strong></td>
                            <td>Designed to probe out-of-distribution (zero-shot) generalization; clearly differentiates in-distribution vs far OOD performance.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_varies_with_novelty</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>gap_variation_details</strong></td>
                            <td>Benchmark analyses show larger proxy-to-ground-truth mismatch for datasets that are visually complex or require sparse/timed special actions; Shannon entropy and Delentropy correlations (example: Shannon entropy correlation for GPT-4o = -0.409) quantify sensitivity to image complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_method</strong></td>
                            <td>Metric-side approaches include normalized and quantile-filtered Brier to assess outlier impacts; dataset-side mitigation via more careful prompt engineering (Genesis) and output clamping for some models were analyzed as strategies to reduce observed gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>gap_reduction_effectiveness</strong></td>
                            <td>Quantitative: normalized and quantile-filtered metrics confirmed minimal outlier reliance; Genesis prompt refinements (used for GPT-4.1) reduced invalid outputs and improved some macro-recall/precision numbers; OpenVLA clamping eliminated invalids and improved practical mapping to action space though did not fix calibration.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cost_comparison</strong></td>
                            <td>All evaluations are computational; infrastructure outlined (L4, A100s, OpenAI Batch API) shows variable compute costs but no explicit cost-per-evaluation numbers or comparison to experimental validation given the digital-only domain.</td>
                        </tr>
                        <tr>
                            <td><strong>temporal_validation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_maturity</strong></td>
                            <td>Benchmarking methods are mature for classification/regression metrics but application to VLA zero-shot transfer is revealing limitations and is an active research area.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_calibration</strong></td>
                            <td>Metrics permit calibration analysis: results indicate poor calibration across models (high Brier MAE); normalized/quantile variants used to assess robustness to outliers and show system-wide miscalibration.</td>
                        </tr>
                        <tr>
                            <td><strong>multiple_proxies</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_correlation</strong></td>
                            <td>Proxy measures are internally consistent (normalized and quantile-filtered Brier follow same trends as raw Brier MAE), indicating correlated failure modes rather than orthogonal signals.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_cascade</strong></td>
                            <td>Single-stage computational validation (predictions vs offline expert labels); no physical experiments reported.</td>
                        </tr>
                        <tr>
                            <td><strong>publication_bias_discussion</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_challenges</strong></td>
                            <td>Proxy metrics reflect disagreement even when models produce valid action outputs; lack of experimental ground-truth beyond expert labels limits interpretation of practical performance for real robots or humans; proxies penalize invalid outputs heavily which dominates some metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>domain_specific_factors</strong></td>
                            <td>Class imbalance in Procgen expert data, discrete action rounding/unnormalization effects, image complexity (entropy), and differences between continuous action training regimes and discrete action evaluation.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Openvla: An open-source vision-language-action model <em>(Rating: 2)</em></li>
                <li>pi0: A vision-language-action flow model for general robot control <em>(Rating: 2)</em></li>
                <li>Open x-embodiment: Robotic learning datasets and rt-x models <em>(Rating: 2)</em></li>
                <li>Denoising diffusion probabilistic models <em>(Rating: 1)</em></li>
                <li>Rt-2: Vision-language-action models transfer web knowledge to robotic control <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2225",
    "paper_id": "paper-278481240",
    "extraction_schema_id": "extraction-schema-60",
    "extracted_data": [
        {
            "name_short": "OpenVLA",
            "name_full": "OpenVLA (openvla/openvla-7b)",
            "brief_description": "An open-source vision-language-action model fine-tuned on large robotics demonstrations that outputs normalized continuous actions which are unnormalized and clamped to discrete Procgen actions at inference time.",
            "citation_title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments",
            "mention_or_use": "use",
            "system_name": "OpenVLA",
            "system_description": "A vision-language-action model trained on ~970k robotics demonstrations (Open X-Embodiment). At inference it takes images + short prompts, predicts normalized continuous actions (in [-1,1]) which are unnormalized using dataset statistics and clamped/rounded to the Procgen discrete action set. Outputs logits are extracted from a Llama 2 backbone for probability estimates.",
            "domain": "embodied AI / simulated game agent benchmarking",
            "proxy_metric_name": "Brier Mean Absolute Error (Brier MAE); micro/macro precision and recall; Percentage Invalids; normalized/quantile-filtered Brier MAE",
            "proxy_metric_description": "Brier MAE measures the absolute difference between the model's predicted probabilities (grouped from Llama token logits into Procgen discrete actions) and one-hot ground-truth action labels per timestep. Complementary metrics include micro/macro precision and recall computed against expert RL action labels, and percentage invalids measuring predictions outside the valid action space. Normalized and quantile-filtered variants assess outlier influence.",
            "proxy_metric_type": "data-driven ML prediction (probabilistic calibration metrics / surrogate objective comparing model probabilities to expert action labels)",
            "ground_truth_metric": "Expert RL trajectory actions from Procgen (offline expert labels)",
            "ground_truth_description": "Ground truth consists of discrete action labels from expert reinforcement-learning agent trajectories on Procgen subdatasets (sourced from Facebook public repository). These are treated as the oracle actions for each timestep in evaluation/test splits.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Brier MAE: generally &gt;1.50 (scale 0 best to 2 worst; paper reports OpenVLA generally exceeded 1.50 while other models often exceeded 1.70); Macro recall typically 9–12.5% (e.g., up to ~12.5% on Maze); micro precision up to ~27% on Coinrun; Percentage invalids = 0% (by design). Normalized and quantile-filtered Brier MAE follow same high-error trend (max relative Brier MAE ≈1.0).",
            "proxy_performance": "Poor calibration: Brier MAE &gt;1.50; macro recall ≈9–12.5% across datasets; micro precision peaked at ≈27% on Coinrun; zero invalid predictions (0%).",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Out-of-distribution (zero-shot) — evaluation on Procgen is far from OpenVLA's continuous-robotics training distribution; performance characterized as poor on OOD tasks.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Gap widens for large domain shifts (continuous robotics → discrete Procgen actions). OpenVLA's clamping mitigates some degradation (fewer invalids) but calibration errors (Brier MAE) remain high; best macro-recall appears on simpler tasks (e.g., Maze) and drops on complex/special-action datasets.",
            "gap_reduction_method": "Output clamping (constraint of unnormalized predictions to valid action ranges) and careful unnormalization/rounding. No experimental (wet-lab) calibration methods used.",
            "gap_reduction_effectiveness": "Clamping eliminated invalid outputs (0% invalids) and improved practical action-space validity; OpenVLA nevertheless still exhibits high Brier MAE (&gt;1.5) and limited macro recall, so clamping reduces one failure mode but does not restore calibration or accuracy fully. Quantitatively: OpenVLA Brier MAE ~1.5 vs others &gt;1.7; macro recall ~9–12% vs others lower.",
            "validation_cost_comparison": "Not discussed in monetary terms; evaluation is entirely computational. Inference run on an NVIDIA L4 instance for OpenVLA; no comparison to any experimental/real-world validation cost provided.",
            "temporal_validation": null,
            "domain_maturity": "Emerging: VLA models are active research systems with known domain-transfer limitations from robotics to simulated discrete-action games.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Poorly calibrated: Brier MAE values close to the maximum (1.5+ out of 2) indicate over/under-confident, inaccurate probabilities despite clamping. Normalized Brier MAE and quantile-filtered analyses confirm systemic miscalibration.",
            "multiple_proxies": true,
            "proxy_correlation": "Metrics are consistent: normalized and quantile-filtered Brier MAE follow the same high-error trends, indicating proxies are correlated and not dominated by outliers.",
            "validation_cascade": "Single-stage computational cascade: model probabilistic predictions → comparison to offline expert RL action labels (no intermediate or experimental validation stages). Errors therefore measured directly against expert labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Major limitations include domain gap (robotics continuous actions vs Procgen discrete actions), low-resolution Procgen images (64x64 vs training 224x224), missing proprioceptive inputs, rounding/unnormalization artifacts, and poor probability calibration (high Brier MAE).",
            "domain_specific_factors": "Action-space mismatch (continuous → discrete), image resolution and view differences, dataset statistics used to unnormalize outputs, and decoding/tokenization effects that influence mapping to discrete actions.",
            "uuid": "e2225.0"
        },
        {
            "name_short": "Pi0 Base",
            "name_full": "Pi0 Base (diffusion-based Pi0)",
            "brief_description": "A diffusion-based vision-language-action model trained on robotics trajectories that reconstructs actions via denoising diffusion; produces distributed (uncertain) predictions on OOD inputs and lacks accessible logits for Brier MAE computation.",
            "citation_title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments",
            "mention_or_use": "use",
            "system_name": "Pi0 Base",
            "system_description": "Diffusion-based VLA trained on robotics trajectory data (OpenX-Embodiment plus private data). At inference the model uses flow-matching denoising across ten steps to predict actions (default action dimension 32 reduced to first dimension for Procgen). Produces action reconstructions that are 'close' to training distributions and tends to distribute probability mass across multiple classes on OOD inputs.",
            "domain": "embodied AI / simulated game agent benchmarking",
            "proxy_metric_name": "Macro/micro precision & recall; percentage invalids; class-wise recall; normalized Brier MAE not computed (no logits available)",
            "proxy_metric_description": "Proxy evaluation compares Pi0 Base action outputs (decoded and unnormalized) to expert RL discrete actions using classification metrics (precision/recall/F1) and distributional comparisons (confusion matrices, action distributions). Brier MAE could not be computed due to absence of logits in its inference architecture.",
            "proxy_metric_type": "data-driven ML prediction (diffusion generative surrogate compared to discrete expert labels)",
            "ground_truth_metric": "Expert RL trajectory actions from Procgen (offline expert labels)",
            "ground_truth_description": "Discrete action labels from Procgen expert agent trajectories used as gold-standard per-timestep actions for evaluation.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Brier MAE not available (no logits); macro recall typically ~6–10% (best ≈10% on simpler datasets); micro precision and class-wise recall show concentration on central actions (classes 4–7) with poor recall on peripheral actions (~0.02). Normalized/quantile Brier trends unavailable.",
            "proxy_performance": "Macro recall ~6–10% across datasets; tendency to output distributed predictions (diffusion behavior) with moderate invalid output rates (fewer invalids than autoregressive Pi0 FAST but more than OpenVLA).",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Out-of-distribution zero-shot (simulated game environments are far from robotics training data); performance is weaker on more novel/sparse-action datasets.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Diffusion model tends to distribute predictions when inputs are OOD, producing uncertainty rather than collapsing to a single wrong class; this means larger practical error (low recall) on OOD, especially for sparse/timed special actions, but less catastrophic invalid-output rates than AR models.",
            "gap_reduction_method": "None explicitly beyond standard decoding; diffusion architecture itself is relied upon to provide proximity to training distributions. No clamping was applied as in OpenVLA.",
            "gap_reduction_effectiveness": "Not reported; absence of logits prevents Brier-based calibration corrections; performance remains moderate but inferior to OpenVLA on macro recall.",
            "validation_cost_comparison": "Not discussed; inference used an NVIDIA A100 (40GB) for Pi0 Base.",
            "temporal_validation": null,
            "domain_maturity": "Emerging; diffusion-based VLA approaches are newer compared to AR methods for action decoding and have distinct OOD behaviors.",
            "uncertainty_quantification": null,
            "uncertainty_calibration": "Not reported / not computable via Brier MAE because logits/probabilities are not exposed in the model inference path.",
            "multiple_proxies": true,
            "proxy_correlation": "Where available, proxy metrics (macro recall, class-wise recall) are consistent with diffusion-style distributed predictions; normalized Brier MAE trends reported for other models are not directly comparable.",
            "validation_cascade": "Single-stage: diffusion model outputs → direct comparison to offline expert actions.",
            "publication_bias_discussion": false,
            "limitations_challenges": "No accessible logits for standard probabilistic calibration metrics; diffusion reconstructions degrade on OOD inputs, leading to distributed but inaccurate predictions. Domain gap issues (continuous training → discrete Procgen), missing proprioception, and image resolution mismatches also degrade performance.",
            "domain_specific_factors": "Diffusion generative inductive bias (reconstructs near-training manifold), action representation normalization, and missing training-view proprioceptive data.",
            "uuid": "e2225.1"
        },
        {
            "name_short": "Pi0 FAST",
            "name_full": "Pi0 FAST (autoregressive Pi0 Fast)",
            "brief_description": "An autoregressive variant of Pi0 (Fast) that uses specialized tokenization and DCT+BPE token schemes to produce discrete action tokens; shows collapse and high invalid-output rates on OOD Procgen tasks and is slow at inference.",
            "citation_title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments",
            "mention_or_use": "use",
            "system_name": "Pi0 FAST",
            "system_description": "Autoregressive VLA variant using Paligemma backbone with DCT + BPE tokenization and short decoding token sequences; decoding constrained to four tokens and mapped to Procgen integer actions via static BPE token mappings. Predictions are autoregressive probabilities aggregated to action classes.",
            "domain": "embodied AI / simulated game agent benchmarking",
            "proxy_metric_name": "Brier MAE (computed via aggregated token probabilities), micro/macro precision & recall, percentage invalids, class-wise recall",
            "proxy_metric_description": "Brier MAE computed by mapping Paligemma token logits to Procgen integer actions and aggregating per-action probabilities; other metrics are standard classification metrics vs expert RL labels. Percentage invalids tracks tokens that don't correspond to valid actions or outputs outside action range.",
            "proxy_metric_type": "data-driven ML prediction (autoregressive token-probability surrogate compared to expert labels)",
            "ground_truth_metric": "Expert RL trajectory actions from Procgen (offline expert labels)",
            "ground_truth_description": "Procgen expert RL discrete actions used as per-timestep gold labels.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Brier MAE: Pi0 FAST consistently &gt;1.70 (paper reports &gt;1.70 for GPT-4o/GPT-4.1/Pi0 FAST); macro recall frequently &lt;6% and sometimes as low as ~2% on many datasets; invalid prediction rates high (some datasets &gt;80% invalids); on certain datasets with frequent special actions (Starpilot, Bossfight) Pi0 FAST anomalously had recalls ≈10%.",
            "proxy_performance": "Very poor overall: Brier MAE &gt;1.70; macro recall often below 6%; minimum precision ~1% on some datasets; high invalid rates (&gt;80% on some datasets).",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Zero-shot OOD evaluation; often far-from-training data causing severe collapse or invalid outputs.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Pi0 FAST shows better recall on datasets where special actions are frequent (less sparse) and worse on sparse/timed-action datasets; invalid-output and collapse behaviors worsen with greater domain shift. Tokenization and DCT-based low-frequency prioritization cause concentration on certain global patterns and class biases.",
            "gap_reduction_method": "Token mapping caching and input embedding caching were used to speed inference; no explicit clamping. The Genesis-style prompt adaptations were applied for Pi0 variants (zero arrays for missing views), but no effective calibration described.",
            "gap_reduction_effectiveness": "Limited: caching improved inference speed but did not rectify high invalid rates or low recall. On some datasets Pi0 FAST had anomalous higher recall (~10%) where the training distribution aligned better with frequent special actions.",
            "validation_cost_comparison": "Pi0 FAST required 4x A100 GPUs due to heavy inference cost and was approximately ten times slower than OpenVLA; no monetary cost breakdown is provided.",
            "temporal_validation": null,
            "domain_maturity": "Emerging; autoregressive tokenization approaches for action prediction are actively developed but show brittle OOD behavior.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Poor calibration (Brier MAE &gt;1.70), and high invalid-output penalty contributes to worst-case Brier values; normalization/quantile-filtered analyses indicate little reliance on outliers (errors are uniformly high).",
            "multiple_proxies": true,
            "proxy_correlation": "Proxy metrics (Brier MAE, normalized variants, precision/recall) are consistent in indicating poor performance; high invalid rates dominate some metrics.",
            "validation_cascade": "Computational-only: autoregressive predictions → mapping to Procgen actions → comparison to expert labels.",
            "publication_bias_discussion": false,
            "limitations_challenges": "High inference cost and latency, brittle autoregressive decoding leading to collapse to a few classes, very high invalid prediction rates when not clamped, DCT+BPE tokenization biases, and strong sensitivity to image complexity and missing proprioceptive inputs.",
            "domain_specific_factors": "Autoregressive overconfidence in OOD, tokenization-induced class biases, and mismatch between frequency of special actions in Procgen vs training data.",
            "uuid": "e2225.2"
        },
        {
            "name_short": "GPT-4o / GPT-4.1",
            "name_full": "GPT-4o and GPT-4.1 (VLM-based agents via Genesis prompting)",
            "brief_description": "Large vision-language models (GPT-4o and GPT-4.1) used as VLM agents via a prompt-engineering intermediary (Genesis) to convert Procgen image/state info into textual structured prompts; their outputs are parsed to probabilities over discrete actions.",
            "citation_title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments",
            "mention_or_use": "use",
            "system_name": "GPT-4o and GPT-4.1 (via Genesis prompt engineering)",
            "system_description": "General-purpose vision-language models interfaced through the Genesis framework that translates Procgen imagery and metadata into structured text prompts specifying action space and required output format. GPT-4.1 used a more carefully curated prompt to reduce invalid outputs. Outputs are expected as probability maps over action indices and then compared to expert labels.",
            "domain": "embodied AI / simulated game agent benchmarking",
            "proxy_metric_name": "Brier MAE (for models where logits/probabilities were extracted), micro/macro precision & recall, percentage invalids; normalized/quantile-filtered Brier MAE",
            "proxy_metric_description": "Brier MAE and classification metrics quantify how the models' output probabilities (or structured outputs parsed into probabilities) align with expert RL action labels. Percentage invalids counts outputs not conforming to required vector-of-probabilities format or outside valid action space.",
            "proxy_metric_type": "data-driven ML prediction (VLM probabilistic outputs compared to expert action labels)",
            "ground_truth_metric": "Expert RL trajectory actions from Procgen (offline expert labels)",
            "ground_truth_description": "Discrete actions recorded from offline expert RL agents on Procgen subdatasets serve as per-timestep ground-truth actions.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Brier MAE: GPT-4o and GPT-4.1 consistently scored &gt;1.70 (close to max 2) indicating strong miscalibration; micro precision very low (GPT-4o as low as 1% on Starpilot); macro precision generally &lt;15%; macro-recall for GPT-4.1 ~6–11% (≈11% on simple datasets), GPT-4o worse on special-action datasets (recall &lt;2.5% on Fruitbot/Starpilot). Percentage invalids: GPT-4o and Pi0 FAST had some datasets with invalid rates &gt;80%; GPT-4.1 improved due to prompt engineering.",
            "proxy_performance": "Poor: Brier MAE &gt;1.70; GPT-4.1 showed better macro-recall on simpler datasets (≈11%) and fewer invalid outputs after prompt refinement; GPT-4o showed low recall and high invalid rates on several datasets.",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Zero-shot, out-of-distribution (procedural games differ from web-scale vision-text training data), especially difficult for tasks requiring precise timed special actions.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Gap larger for tasks requiring precise timing or sparse special actions (e.g., 'fire' actions) — GPT-4o struggled particularly on such datasets (very low recall). Prompt engineering (Genesis) reduces invalids and can improve precision/recall somewhat (GPT-4.1 better than GPT-4o). Correlations with image complexity show moderate-to-weak negative correlations for GPT-4o/GPT-4.1 (Shannon entropy: -0.409 and -0.183 respectively), implying worse performance with increasing image complexity.",
            "gap_reduction_method": "Prompt engineering via the Genesis framework (more rigorous prompt for GPT-4.1 specifying simulated/hypothetical context, stricter format constraints) to reduce invalid outputs; also constraining outputs parsing and instructing valid action space.",
            "gap_reduction_effectiveness": "Effective in reducing invalid outputs and improving per-dataset performance: GPT-4.1 outperformed GPT-4o on many subdatasets and produced fewer invalids; quantified improvements include higher macro-recall (~11% on simple datasets for GPT-4.1) and fewer invalid predictions compared to GPT-4o (GPT-4o had invalids &gt;80% on some datasets).",
            "validation_cost_comparison": "Inference for GPT-4x family was run through OpenAI's Batch API and external cloud resources; no explicit cost comparison to experimental validation is provided.",
            "temporal_validation": null,
            "domain_maturity": "Mature in vision-language text tasks but emerging/underdeveloped for direct action prediction and zero-shot embodied control.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Poor calibration indicated by Brier MAE near worst possible values (&gt;1.70); however, calibration improved modestly with targeted prompting (GPT-4.1 vs GPT-4o).",
            "multiple_proxies": true,
            "proxy_correlation": "Proxy metrics were consistent: normalized and quantile-filtered Brier MAE followed the same trends, indicating little sensitivity to outliers.",
            "validation_cascade": "Computational-only cascade: structured prompt → VLM probabilistic output → parsing into action probabilities → comparison to offline expert labels. No intermediate experimental validation.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Format/validity of outputs (invalids), severe domain shift from web-scale vision-language to discrete action prediction, sensitivity to image complexity, bias to default actions (e.g., 'Do Nothing' or 'Right'), and strong class imbalance effects.",
            "domain_specific_factors": "Prompt sensitivity, image complexity (entropy correlations), action space specification in prompt, and lack of proprioceptive state information for this domain.",
            "uuid": "e2225.3"
        },
        {
            "name_short": "Benchmarking suite / metrics",
            "name_full": "MultiNet v0.2 benchmarking suite (metrics: Brier MAE, macro/micro precision/recall, percentage invalids, normalized/quantile-filtered Brier)",
            "brief_description": "The benchmark and metric suite (MultiNet v0.2) used to evaluate VLMs and VLAs on Procgen tasks using probabilistic calibration (Brier MAE), classification metrics, and invalid-output penalties to measure proxy predictive performance against expert RL ground-truth.",
            "citation_title": "Benchmarking Vision, Language, & Action Models in Procedurally Generated, Open-Ended Action Environments",
            "mention_or_use": "use",
            "system_name": "MultiNet v0.2 evaluation framework",
            "system_description": "A unified evaluation and profiling framework that standardizes Procgen expert trajectories, ingest formats (TFDS), and a suite of proxy metrics including Brier MAE (with normalized and quantile variants), micro/macro precision/recall/F1, percentage invalids, class-wise metrics, and entropy-based image complexity analyses; used to compare several VLA/VLMs in zero-shot OOD settings.",
            "domain": "benchmarking / model evaluation for embodied AI",
            "proxy_metric_name": "Brier Mean Absolute Error (primary), normalized Brier MAE, normalized quantile-filtered Brier MAE, micro/macro precision & recall, percentage invalids, class-wise metrics",
            "proxy_metric_description": "Brier MAE computes per-timestep absolute error between model-assigned action probabilities and the one-hot expert action; invalid predictions receive maximum penalty (Brier score = 2). Normalized and quantile-filtered variants assess outlier influence. Precision/recall/F1 and class-wise analyses reveal bias and class-imbalance effects.",
            "proxy_metric_type": "empirical surrogate (statistical/probabilistic metrics computed over model predictions vs offline expert labels)",
            "ground_truth_metric": "Procgen expert RL agent actions (offline trajectories)",
            "ground_truth_description": "Test splits of Procgen expert trajectories (10% random episodes per subdataset) are used as gold-standard labels for evaluation; no experimental wet-lab ground truth exists in this domain.",
            "has_both_proxy_and_ground_truth": true,
            "quantitative_gap_measure": "Aggregate findings: Brier MAE values for evaluated models approached the maximum (2) with typical values &gt;1.50–1.70; normalized/quantile-filtered variants track these trends. Macro metrics rarely exceeded ~15% and macro recall frequently in single digits to low tens of percent (OpenVLA ~9–12.5%). Percentage invalids varied widely (0% for OpenVLA, up to &gt;80% for GPT-4o/Pi0 FAST on some datasets).",
            "proxy_performance": "Across models, proxy metrics indicate poor zero-shot generalization: high Brier MAE (~1.5–2.0), low macro/micro precision and recall, and high invalid rates for some models.",
            "ground_truth_performance": null,
            "false_positive_rate": null,
            "false_negative_rate": null,
            "novelty_characterization": "Designed to probe out-of-distribution (zero-shot) generalization; clearly differentiates in-distribution vs far OOD performance.",
            "gap_varies_with_novelty": true,
            "gap_variation_details": "Benchmark analyses show larger proxy-to-ground-truth mismatch for datasets that are visually complex or require sparse/timed special actions; Shannon entropy and Delentropy correlations (example: Shannon entropy correlation for GPT-4o = -0.409) quantify sensitivity to image complexity.",
            "gap_reduction_method": "Metric-side approaches include normalized and quantile-filtered Brier to assess outlier impacts; dataset-side mitigation via more careful prompt engineering (Genesis) and output clamping for some models were analyzed as strategies to reduce observed gaps.",
            "gap_reduction_effectiveness": "Quantitative: normalized and quantile-filtered metrics confirmed minimal outlier reliance; Genesis prompt refinements (used for GPT-4.1) reduced invalid outputs and improved some macro-recall/precision numbers; OpenVLA clamping eliminated invalids and improved practical mapping to action space though did not fix calibration.",
            "validation_cost_comparison": "All evaluations are computational; infrastructure outlined (L4, A100s, OpenAI Batch API) shows variable compute costs but no explicit cost-per-evaluation numbers or comparison to experimental validation given the digital-only domain.",
            "temporal_validation": null,
            "domain_maturity": "Benchmarking methods are mature for classification/regression metrics but application to VLA zero-shot transfer is revealing limitations and is an active research area.",
            "uncertainty_quantification": true,
            "uncertainty_calibration": "Metrics permit calibration analysis: results indicate poor calibration across models (high Brier MAE); normalized/quantile variants used to assess robustness to outliers and show system-wide miscalibration.",
            "multiple_proxies": true,
            "proxy_correlation": "Proxy measures are internally consistent (normalized and quantile-filtered Brier follow same trends as raw Brier MAE), indicating correlated failure modes rather than orthogonal signals.",
            "validation_cascade": "Single-stage computational validation (predictions vs offline expert labels); no physical experiments reported.",
            "publication_bias_discussion": false,
            "limitations_challenges": "Proxy metrics reflect disagreement even when models produce valid action outputs; lack of experimental ground-truth beyond expert labels limits interpretation of practical performance for real robots or humans; proxies penalize invalid outputs heavily which dominates some metrics.",
            "domain_specific_factors": "Class imbalance in Procgen expert data, discrete action rounding/unnormalization effects, image complexity (entropy), and differences between continuous action training regimes and discrete action evaluation.",
            "uuid": "e2225.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Openvla: An open-source vision-language-action model",
            "rating": 2
        },
        {
            "paper_title": "pi0: A vision-language-action flow model for general robot control",
            "rating": 2
        },
        {
            "paper_title": "Open x-embodiment: Robotic learning datasets and rt-x models",
            "rating": 2
        },
        {
            "paper_title": "Denoising diffusion probabilistic models",
            "rating": 1
        },
        {
            "paper_title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
            "rating": 1
        }
    ],
    "cost": 0.0193755,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Benchmarking Vision, Language, &amp; Action Models in Procedurally Generated, Open-Ended Action Environments
17 Jun 2025</p>
<p>Pranav Guruprasad 
Yangyue +12 
Wang 
Sudipta Chowdhury 
Harshvardhan Sikka 
Paul Pu Liang 
Manifold Research 
Metarch Ai 
Georgia Tech 
Mit 
Benchmarking Vision, Language, &amp; Action Models in Procedurally Generated, Open-Ended Action Environments
17 Jun 20250675D566601BF314599057E27A8A2A85arXiv:2505.05540v2[cs.CV]
Vision-language-action (VLA) models represent an important step toward general-purpose robotic systems by integrating visual perception, language understanding, and action execution.However, systematic evaluation of these models, particularly their zero-shot generalization capabilities in procedurally out-of-distribution (OOD) environments, remains limited.In this paper, we introduce MultiNet v0.2, a comprehensive benchmark designed to evaluate and analyze the generalization performance of state-of-the-art VLMs and VLAs-including GPT-4o, GPT-4.1,OpenVLA, Pi0 Base, and Pi0 FAST-on diverse procedural tasks from the Procgen benchmark.Our analysis reveals several critical insights: (1) all evaluated models exhibit significant limitations in zero-shot generalization to OOD tasks, with performance heavily influenced by factors such as action representation and task complexity; (2) VLAs generally outperforms other models due to their robust architectural design; and (3) VLM variants demonstrate substantial improvements when constrained appropriately, highlighting the sensitivity of model performance to precise prompt engineering.We release our benchmark, evaluation framework, and findings to enable the assessment of future VLA models and identify critical areas for improvement in their application to out-of-distribution digital tasks.</p>
<p>I. INTRODUCTION</p>
<p>Recent advancements in large-scale vision-language models (VLMs) and vision-language-action models (VLAs) have demonstrated remarkable capabilities across various domains, including image recognition, natural language understanding, multimodal association, and preliminary robotics applications [5], [18], [27].These models promise a future where generalpurpose AI systems can interpret visual inputs, comprehend language commands, and execute appropriate actions in diverse scenarios.</p>
<p>However, a significant challenge remains: ensuring these models can generalize effectively to out-of-distribution (OOD) tasks.Current models often struggle with zero-shot transfer capabilities, particularly when confronted with novel scenarios or tasks that differ substantially from their training data [13].This limitation is especially evident in procedurally generated environments, such as those provided by the Procgen benchmark [7], which are designed to test visual understanding, decision-making, and action generation capabilities in varied and unpredictable settings.* equal contribution, alphabetical order.+ Corresponding Author: harshsikka@gatech.edu</p>
<p>The architecture and training methodologies of these models play a crucial role in their generalization abilities.Factors such as the nature of the action space (continuous vs. discrete), the domain of training data (real-world robotics vs. simulated environments), and the methods of input-output processing can significantly impact performance.For instance, models trained predominantly on robotics data may not perform well in simulated game environments due to differences in action representation and environmental complexity.</p>
<p>To address these challenges, we introduce MultiNet v0.2, a comprehensive benchmarking effort aimed at evaluating the generalist capabilities of VLMs and VLAs on procedurally generated tasks.Our study encompasses a diverse set of models, including GPT-4o, GPT-4.1,OpenVLA, Pi0 Base, and Pi0 FAST, assessed across multiple Procgen datasets [3], [7], [17], [18], [23], [24].</p>
<p>Our primary contributions in this paper are:</p>
<p>• A systematic benchmarking framework that evaluates the performance of state-of-the-art models on a variety of procedurally generated game environments.• Detailed profiling results for an initial set of SoTA VLMs and VLAs, along with analysis of their performance.• Analysis of the impact of architectural choices, training data, and output processing techniques on model generalization, highlighting the limitations of current approaches.• Insights into how factors like action space representation and image complexity influence model performance.The remainder of this paper is structured as follows: Section 2 reviews related work in the field; Section 3 details our experimental setup and datasets; Section 4 presents our empirical results and analysis; and Section 5 discusses the implications of our findings and outlines directions for future research.</p>
<p>II. RELATED WORK</p>
<p>Our work builds on the rapid advancement in visionlanguage-action (VLA) models for embodied AI, both in physical robots and digital environments, that enable agents to perceive, reason, and act across diverse tasks and embodiments.In this section, we review the landscape of related work, organized into two complementary areas: evaluation in digital environments and evaluation of generalist VLAs.For comprehensive surveys on VLAs and computer use agents, see [16], [19], [21], [28].Evaluation in digital environments.Digital benchmarks play a crucial role in measuring agents' decision-making, perception, and planning skills in simulated settings.We categorize prominent digital benchmarks into three groups:</p>
<p>• Gameplay Benchmarks.Benchmarks such as Gymnasium, which wraps the Arcade Learning Environment (ALE) for Atari games and provides continuous-control tasks, have become standard testbeds for reinforcementlearning agents [2].More recent procedurally generated game benchmarks such as Procgen, MineDojo and competition and crafting game benchmarks such as AlphaStar, Crafter introduce a family of tasks that probe models' ability in 2D/3D games with more complex objectives and environments to further assess agents' ability to adapt and generalize [7], [11], [14], [34].• Interface and Web-Interaction Benchmarks.As agent evaluation transitions from controlled gameplay simulations to more general computer use tasks, benchmarks such as Mind2Web, AITW, and OSWorld aim to evaluate proficiency in tasks such as web browsering, menu navigation, and form completion across environments such as web applications, Windows, MacOS, Android, and Linux.</p>
<p>[6], [9], [20], [26], [35].In recent years, agents driven by large language models have been evaluated in subsets of these benchmarks [12], [31].</p>
<p>We extend digital evaluation by systematically including state-of-the-art VLAs alongside VLM-based agents, applying a unified metric suite across all procedurally generated and discrete-control environments.</p>
<p>Evaluation of generalist VLAs.Generalist VLAs are commonly evaluated using in-domain robotics benchmarks such as OpenX, VLABench, and LIBERO, consisting of common physical manipulation tasks and robot embodiments [8], [10], [22], [32], [38].These assessments focus on metrics such as task completion rates, sim-to-real transfer fidelity, and robustness to sensor noise.Some recent efforts such as Magma [36] have attempted to narrow cross-domain transfer, but none systematically unify digital and physical benchmarks under consistent metrics and datasets.</p>
<p>In MultiNet v0.2, we repurpose robotics-trained VLAs and general-purpose VLMs as digital agents and benchmark them in procedurally generated 2D environments.</p>
<p>III. EXPERIMENTAL SETUP</p>
<p>A. Dataset</p>
<p>We utilized offline trajectories from expert reinforcement learning (RL) agents trained on the Procgen dataset, which were sourced from Facebook's publicly available repository1 [7].To facilitate consistent and straightforward data handling, we translated these trajectories into the TensorFlow Datasets (TFDS) format 2 .This standardized format ensured streamlined loading and usage across all subsequent profiling experiments.</p>
<p>For evaluation purposes, we established test splits comprising 10% of the episodes randomly sampled from each subdataset.Given that the Procgen dataset consists of 16 distinct subdatasets, this approach resulted in balanced and representative test splits for comprehensive evaluation.Importantly, all profiling and performance assessment experiments reported in this study were conducted exclusively on these designated test splits, ensuring that our results reflect the models' genuine generalization capabilities without contamination from training data.</p>
<p>B. Models</p>
<p>In this study, we leveraged state-of-the-art Vision-Language-Action (VLA) models with predefined full-sized weights to evaluate performance comprehensively.The models and their respective sources were:</p>
<p>OpenVLA: The weights were obtained from the Hugging-Face repository under the identifier openvla/openvla-7b. 3i0 Base: We used weights stored at s3://openpiassets/checkpoints/pi0 base/params.We obtained this path from the Openpi open-source codebase 4Pi0 Fast: We used weights accessible via s3://openpiassets/checkpoints/pi0 fast base/params.We obtained this path from the Openpi open-source codebase.</p>
<p>All models were employed in their complete forms, without any quantization techniques, to ensure accurate and undiluted performance evaluation.</p>
<p>To align the models with the Procgen task context and dataset specifics, we performed targeted adaptations during ingestion and inference.</p>
<p>1) Data Ingestion: GPT 4o and GPT 4.1: Employed the Genesis prompt engineering framework, which acts as a crucial intermediary between the data and the model, by translating the raw Procgen trajectory data (images and associated metadata) into a rich, structured textual representation that primes the VLM for analyzable outputs.Further information about the Genesis framework is detailed in Appendix Section VII-C.</p>
<p>OpenVLA: Received visual observations per timestep as direct image inputs, accompanied by succinct single-line text prompts describing the corresponding subdataset task.</p>
<p>Pi0: Incorporated the first image view as the primary visual observation, supplemented with zero arrays for the second and third image views, with image masks indicating True for the primary view and False for others.Proprioceptive states were also set as zero arrays.Each timestep included a concise single-line task description prompt.</p>
<p>Pi0 with Fast: Similar to Pi0 Base, Pi0 Fast utilized the first image view as its primary visual observation but employed zero arrays for the second and third image views with image masks set to True for all three views.Zero arrays represented the proprioceptive states, and each timestep included a singleline task description prompt.</p>
<p>2) Model Adaptation: OpenVLA: Adaptation involved restricting the autoregressive step to one, ensuring singledimensional action vector predictions.Generated actions were unnormalized using statistics computed across the entire Procgen subdataset and subsequently rounded to discrete actions.To compute evaluation metrics such as Brier MAE 1, we first extracted logits from the Llama 2 backbone and computed the corresponding probabilities [33].We then initialized and cached mappings from Llama vocabulary tokens to Procgen integer actions.During inference, this approach allowed us to group the probabilities of each vocabulary token according to their associated integer action classes after unnormalization, while maintaining efficient inference throughout the process.</p>
<p>Pi0 Base: The action horizon was constrained to one timestep, with flow matching denoising executed over ten steps (default configuration).Actions were predicted with a default dimension of 32.The first dimension of the predicted action was picked to ensure a final single-dimensional action vector prediction, which was then unnormalized based on Procgen dataset-wide statistics (same dataset statistics used for OpenVLA), and discretized by rounding.</p>
<p>Pi0 Fast: Adaptation entailed adjusting the action horizon to one timestep and setting the action dimension to one, compatible with Procgen's single-dimensional action space.To explicitly generate the correct action value, the decoding steps were limited to four tokens: "Action", ":", a space character, and the Paligemma (VLM backbone of Pi0 models) location token.To compute probabilities for Brier MAE, leveraging the static nature of the BPE tokenizer, token mappings between the Paligemma token IDs and Procgen integer actions were established.During inference, probabilities extracted from the Paligemma backbone logits were aggregated according to this mapping.Recognizing Pi0 Fast's significantly slower inference speed (approximately ten times slower than OpenVLA), we optimized runtime by caching embeddings for the two static zero-images after initial processing by the SigLIP embedding [37], effectively doubling inference speed.</p>
<p>C. Inference Infrastructure</p>
<p>For systematic performance evaluation and model profiling, we utilized dedicated hardware resources optimized for each model's computational requirements:</p>
<p>• OpenVLA inference was executed on an NVIDIA L4 GPU instance, selected for its balance of computational efficiency and memory capacity, ideal for models of intermediate complexity.</p>
<p>• Pi0 Base utilized a single NVIDIA A100 GPU instance equipped with 40 GB of memory, providing ample computational power and memory bandwidth for accurate and efficient inference.• Pi0 Fast, due to its greater computational demands and inference complexity, was allocated four NVIDIA A100 GPU instances, each with 40 GB of memory, facilitating parallel processing to substantially enhance inference throughput.</p>
<p>• In contrast, GPT 4x inference was managed externally through OpenAI's Batch API, eliminating local processing overhead and leveraging scalable cloud resources to efficiently handle its computational requirements.</p>
<p>D. Evaluation metrics</p>
<p>In order to capture the ability of state-of-the-art VLAs and VLMs to generalize to completely unseen, OOD action data, we carefully curated a list of metrics to evaluate the models as fairly as possible.These metrics address diverse aspects of model performance, such as how well calibrated they are in their estimates, reliance on outliers, how they are affected by class imbalance, how well they are able to restrict their predictions to the valid action space, whether the models are biased towards certain kinds of subdatasets or actions, etc.</p>
<p>• Brier Mean Absolute Error
Brier MAE = 1 N N ∑ t=1 R ∑ i=1 | f ti − o ti |(1)
where f ti is the probability of the model prediction for class i at timestep t, o ti is the i th dimension of the one hot representation of the ground truth action for the same timestep, R is the possible number of classes, and N is the total number of timesteps.</p>
<p>Brier MAE is a variation of the original Brier score [4], which is a useful method to measure the accuracy of probabilistic predictions.In our case, for a given timestep and the associated action space, each model computes a probability for each action that represents how likely it is to be equal to the truth label.This measures how well the predictions of the models are calibrated.</p>
<p>This metric quantifies how much the worst-case error deviates from the typical (median) error produced by the model for a given subdataset of Procgen.A value significantly greater than 1 indicates the presence of some extremely bad predictions that can skew the results.For our case, where it is a multi-class, single correct prediction task and the classes (actions) are mutually exclusive, false positives are equal to false negatives.Simply, an incorrectly predicted action is a false positive for its class, while also a false negative for the ground truth action class.Due to this, precision = recall = f1 score = exact match rate when calculating micro metrics in our case.The micro precision/recall/f1 is calculated over all timesteps of the test split of a given subdataset.This metric measures how precise the models are -high precision indicates that the model gets a high number of predictions correct, and a low precision suggests the model is frequently predicting actions that don't match the ground truth.• Percentage invalids Predictions that fall outside the valid action space of a given subdataset of Procgen are considered invalid.In the case of the VLMs -GPT 4o, and GPT 4.1, the outputs that are not in the desired format of a vector the size of the action space with discrete action keys and their probabilities summing to 1 as its elements, are also considered invalid.Thus, percentage invalids is calculated as</p>
<p>Percentage invalids = Timesteps with invalid preds Total number of timesteps * 100 (7)</p>
<p>A high invalid percentage indicates that the model struggles to produce outputs in the target action space.• Micro Precision without Invalids This metric is a variation of micro precision where the false positives are only counted for valid predictions.By not considering the invalid predictions, this metric provides more insight into the model's performance in the cases where it is able to produce valid outputs.A discrepancy in the trends between micro precision and micro precision without invalids indicates that a model would be more capable if it were fine-tuned, prompted, or had its output processed to constrain the predictions to the valid action space.• Class-wise Precision, Recall, F1 scores Class-wise variations of the precision, recall, and F1 metrics are the same metrics calculated for each action class within the valid action space of a subdataset individually.The class-wise precision, recall, and F1 scores of a model would be calculated for a given action class x subdataset combination.Average class-wise metrics for a model are calculated by averaging the class-wise x subdataset metric values over all subdatasets to report a unified set of classwise metrics for a given model.These metrics help to understand model biases and preferences towards specific action classes.• Macro Precision, Recall, and F1 scores
Macro Precision = 1 K K ∑ k=1 T P k T P k + FP k (8) Macro Recall = 1 K K ∑ k=1 T P k T P k + FN k (9) Macro F1 = 1 K K ∑ k=1 2 • Precision k • Recall k Precision k + Recall k(10)
K here refers to the number of classes for a given subdataset.Macro precision, recall, and F1 metrics are calculated by averaging the class-wise metrics across all classes in the valid action space of a given subdataset for a given model.These metrics are crucial, as they help alleviate the effects of the dominant majority class that skews performance.With micro metrics, the majority classes are favored due to global aggregation.This leads to a biased view of performance, especially in the Procgen expert data, as there are numerous cases of class imbalance, which can skew the values of the micro metrics.Macro metrics prove to be a better indicator of a model's performance when compared to micro metrics, irrespective of the nature of the test dataset, as they treat all classes equally regardless of size.Within this, Macro recall is typically the most representative metric for performance as it is neither affected by the majority classes, as in the case of micro metrics, nor is it affected by the performance of models on the rare classes, as in the case of macro precision or F1 scores.</p>
<p>IV. RESULTS AND DISCUSSION A. Models Exhibit Poor Zero-Shot Out-of-Distribution (OOD) Generalization 1) Comparing General Performance: In evaluating the generalization capability of GPT 4o, GPT 4.1, OpenVLA, Pi0 Base, and Pi0 FAST, we observed universally poor performance on zero-shot out-of-distribution (OOD) tasks.The analysis using various performance metrics underscored significant limitations across these models.</p>
<p>As seen in Figure 2, the Brier Mean Absolute Error (MAE) scores revealed poor probability calibration, as the scores approached the maximum value of 2 across all models and datasets.Specifically, GPT 4o, GPT 4.1, and Pi0 FAST consistently scored above 1.70, while OpenVLA generally exceeded 1.50.The subdatasets on which models exhibited the weakest performance were Starpilot for GPT 4o, Leaper for GPT 4.1, Miner for Pi0 FAST, and Ninja for OpenVLA.</p>
<p>Analyzing micro-level metrics, precision values were low across all models and subdatasets.OpenVLA achieved the highest micro precision at approximately 27% on the Coinrun dataset, whereas GPT 4o showed the lowest precision at 1% on Starpilot.Pi0 FAST reached a minimal precision of around 1% on Heist.Overall, as can be inferred from Figure 3, the performance hierarchy based on micro precision was:
OpenVLA &gt; Pi0 Base &gt; GPT 4.1 &gt; GPT 4o &gt; Pi0 FAST.
When considering micro precision without invalid predictions, as seen in Figure 4, significant improvements were noted for GPT 4o and Pi0 FAST, highlighting their potential if constrained or further fine-tuned to produce valid action outputs.GPT 4o's precision notably improved, surpassing Pi0 Base in a few subdatasets such as Coinrun, Bossfight, and Dodgeball.Nevertheless, the adjusted performance order still positioned OpenVLA at the top, followed by Pi0 Base, Pi0 FAST, GPT 4o, and GPT 4.1.</p>
<p>Macro precision analysis revealed that GPT 4o, OpenVLA, and Pi0 Base maintained similar performance levels across most datasets.As seen in Figure 5, conversely, GPT 4.1 and Pi0 FAST demonstrated significant variability, implying distinct preferences for specific task types.Despite these observations, macro precision remained generally low across all models, with a maximum slightly above 15%.Importantly, comparing its macro and micro precision performances, it is clear that OpenVLA's performance heavily favored the Macro recall, as seen in Figure 6, highlighted OpenVLA's aggressive prediction strategy, resulting in consistently high recall but lower precision due to numerous false positives.GPT 4o, despite higher macro precision, showed substantially lower recall, indicating pronounced class biases towards a few classes, and a potentially overly conservative nature about predicting certain classes compared to other classes.GPT 4.1 exhibited a relatively balanced approach, achieving both recall and precision, thus indicating less bias towards specific classes.Pi0 Base displayed a moderately balanced performance without leading in any subdataset, whereas Pi0 FAST consistently showed low recall, reflecting challenges in identifying true positives across all classes.</p>
<p>The analysis of invalid predictions highlighted significant disparities.OpenVLA, by design, produced zero invalid predictions, ensuring actions always fell within the valid action space.As seen in Figure 7, GPT 4o and Pi0 FAST frequently generated invalid outputs, with some datasets reaching invalid prediction rates above 80%.</p>
<p>2) Outlier Influence and Majority Prediction Performance: As seen in Figures 8 and 9, the normalized Brier MAE and quantile-filtered analysis indicated consistent error trends when compared to regular Brier MAE scores across models and datasets, demonstrating minimal reliance on outlier predictions.Given the already high median errors, the influence of outliers was negligible.</p>
<p>Further investigation using the max relative Brier MAE metric showed minimal variance in model performance, with the scores staying consistently close to 1.0.This finding, seen in Figure 10, suggests limited deviation between median and extreme error values, primarily due to the universally high error levels across all models reducing the relative significance of outlier predictions.</p>
<p>B. Models Struggle with Unseen Discrete Action Data</p>
<p>1) Dataset-Specific Performance Across Models: Our evaluation revealed distinct performance patterns across the five models-OpenVLA, GPT-4o, GPT-4.1,Pi0 Base, and Pi0 FAST-when exposed to out-of-distribution (OOD) datasets with discrete action spaces from Procgen.</p>
<p>OpenVLA consistently achieved the highest macro-recall among the evaluated models across nearly all 16 Procgen subdatasets, typically ranging from 9% to 12% as shown in 6 and 11.It performed notably better on simpler tasks, achieving a maximum recall of approximately 12.5% on the Maze dataset, characterized by straightforward navigation and collection actions without the complexity of additional special actions.Conversely, OpenVLA's recall dropped to its lowest GPT-4 Series occupied a middle performance tier, displaying macro-recall values roughly between 6% and 11% as seen in 1 and 11.GPT-4.1 generally outperformed GPT-4o across most datasets.Notably, GPT-4.1 showed superior performance when compared to Pi0 Base on simpler datasets like Leaper, Climber, Jumper, Maze, and Heist.Specifically, GPT-4.1 achieved macro-recall scores of around 11% on simpler datasets like Leaper, Climber, Jumper, Bigfish, Chaser, Coinrun, Miner, Maze, and Heist.Its performance diminished to around 7% on more challenging environments such as Conversely, GPT-4o exhibited significantly weaker performance than Pi0 Base on datasets involving timely and sparse execution of special actions.This was particularly evident in datasets involving "fire" actions, such as Plunder, Fruitbot, and Starpilot, indicating GPT-4o's difficulty managing tasks requiring precise timing.While GPT-4o showed the highest recalls around 8.5% on datasets with simpler movement-only actions (Leaper, Climber, Jumper, Coinrun, Maze, Heist), it performed significantly worse on more complex tasks.Recall dropped to around 3% on more complex tasks like Chaser and Bossfight, and below 2.5% on Fruitbot and Starpilot, both of which feature special actions.</p>
<p>Pi0 Base achieved macro recalls typically between 6% and 10%, as seen in 1 and 11.Its best performance (around Fig. 7: Percentage invalids across all 5 models.Invalids refer to model predictions that are not valid actions in the subdataset's action space.Pi0 FAST and GPT 4o struggle to produce valid actions irrespective of the subdataset.10 percent) appeared on relatively simpler datasets such as Leaper, Plunder, Jumper, Bigfish, Chaser, Coinrun, and Miner.Conversely, macro recall dropped to approximately 6% on datasets involving special actions like Dodgeball, Fruitbot, Bossfight, and Ninja.</p>
<p>Pi0 FAST exhibited the lowest macro recall among the evaluated models, frequently below 6 percent, and occasionally as low as 2% on datasets including Leaper, Bigfish, Coinrun, Ninja, Miner, Maze, and Heist as seen in 1 and 11.However, Pi0 FAST displayed anomalously higher recalls ( 10 percent) on datasets such as Starpilot and Bossfight.Despite these datasets involving special actions, the actions are executed frequently rather than sparsely, aligning better with Pi0 FAST's predictions and ground truth distributions.</p>
<p>In general, our findings highlight a clear difficulty across models in accurately handling datasets that require complex, sparse, or timely executed actions, particularly when special non-movement actions are involved.The macro recall performance appears directly correlated to how closely the distribution of model predictions aligns with the distribution of the ground truth actions.</p>
<p>2) Action-Class Specific Performance Across Models: A detailed analysis of recall performance by individual action classes offers valuable insights into model-specific strengths, weaknesses, and intrinsic biases in predicting discrete actions.As seen in Figure 12, action classes 4 through 10 generally displayed higher recall scores and greater variance among models, indicating varying degrees of predictive confidence.In contrast, action classes 0 to 3 and 11 to 12 consistently yielded low recall values across all models, demonstrating limited predictive capability.When comparing the model action class recall against action class frequency in Figure 13, OpenVLA exhibits the strongest positive correlation, attaining the highest recall on the most prevalent classes, while Pi0 base shows little correlation.Both GPT-4o and GPT-4.1 display slightly negative trends, indicating more struggle with common classes than with rarer ones.Pi0 Fast's recall remains largely constant GPT-4o exhibited notable recall performance on action class 7("Right" movement), achieving a high recall of approximately 0.22 as seen in 12.This suggests a directional bias likely influenced by the prevalent rightward progression in many Procgen tasks.Conversely, GPT-4o struggled significantly with actions 4, 10, 11, and 12, displaying nearzero recall, highlighting clear limitations in handling certain peripheral or specialized actions.Specifically, the difference between the most frequent action classes in predicted actions and ground truth actions can be seen in Figure 14.</p>
<p>OpenVLA predominantly favored action class 4 ("Do Nothing") as seen in 15, recording the highest recall ( 0.32) as seen in 12.This bias arises from a combined influence of its decoding mechanism and Procgen action space sizes.Most Procgen datasets have an action space of size 9 or 10, and due to unnormalization of OpenVLA's predicted actions using the dataset statistics, 4 can correspond to 0 in the normalized range, which is the center of OpenVLA's output distribution.Pi0 Base similarly gravitated toward central action classes (4, 5, 6, and 7) as seen in 16 due to its normalizationunormalization decoding approach, resulting in moderate recall values ( 0.12 -0.16) as seen in 12.Its recall sharply declined for peripheral and special action classes (0, 1, 2, and 12), dropping to approximately 0.02.Compared to OpenVLA,  Pi0 FAST displayed a pronounced bias towards action classes (1, 5, 8, and 9) as seen in 17, with action percentage between 10% and 26%.This bias reflects its difficulty generalizing to OOD data, often producing higher normalized action predictions.Conversely, Pi0 FAST severely underperformed on actions between the above prevalent classes, such as 0, 2, 3, 7, and 11, frequently achieving near-zero recall as seen in 12, emphasizing its limitations in balanced action class predictions.</p>
<p>GPT-4.1 demonstrated a distinct bias toward action class 7 ("Right" movement) as seen in 18, achieving exceptionally high recall ( 0.40) as seen in 12, consistent with the directional tendencies of many Procgen environments.It also showed moderate recall ( 0.20) for special action class 9.However, GPT-4.1 struggled notably with several other action classes, particularly actions 0, 1, 4, 10, and 11, consistently resulting in minimal recall ( 0.01).</p>
<p>These results clearly indicate model-specific biases towards particular action classes, underscoring the importance of targeted adjustments in model training and decoding methods to foster more balanced and generalized performance across the entire action space.Specifically, each model exhibited distinct biases toward particular default actions:</p>
<p>• OpenVLA frequently defaulted to predicting action classes 4 ("Do Nothing"), 5 ("UP"), and 6 ("RIGHT + DOWN").• Pi0 Base predominantly predicted action classes clustered between 4 to 7 ("Do Nothing", "UP", "RIGHT + DOWN", "RIGHT") and the range shifts to between 5 to 8 ("UP", "RIGHT + DOWN", "RIGHT", "RIGHT + UP") when the ground truth actions are between 9 and 12. • Pi0 FAST consistently favored action classes 6 ("RIGHT + DOWN") and 8 ("RIGHT + UP").• GPT-4o exhibited a strong bias towards action class 7</p>
<p>("RIGHT").• GPT-4.1 frequently defaulted to action classes 7</p>
<p>("RIGHT") and 2 ("LEFT + UP").Notably, as seen in Figure 20, across all models and subdatasets, action class 4 ("Do Nothing") exhibited significantly higher precision compared to other classes.We speculate that this elevated precision for class 4 originates from its role as a default assignment for expert actions that fell outside the environment-specific action spaces during the generation of ground truth labels, thus inflating its representation.</p>
<p>For VLMs like GPT-4o and GPT-4.1, the frequent choice of action class 4 may also reflect insufficient understanding of the task, causing it to be uncertain in its predictions and default to "Do Nothing".Additionally, the observed strong preference for "RIGHT" (action class 7) aligns with the prevalent directional progression in many Procgen environments.</p>
<p>In contrast, OpenVLA, Pi0 Base, and Pi0 FAST primarily defaulted towards actions situated near the center of their normalized prediction range (typically zero).These central predictions commonly map to action class 4 due to rounding effects and the typical size (&lt;= 10 actions) of Procgen datasets.</p>
<p>These findings underscore critical biases inherent in model designs and highlight opportunities for refining training approaches and decoding methods to achieve balanced action distributions and improve generalization to unseen action spaces.</p>
<p>C. Models' Predictions Collapse on Specific Action Classes with OOD Data 1) Patterns of Collapse Influenced by Decoding Techniques:</p>
<p>The behavior of models on out-of-distribution (OOD) data revealed clear patterns of prediction collapse, significantly influenced by their underlying decoding strategies.We specifically examined how diffusion sampling and autoregressive (AR) decoding methods shaped these patterns.</p>
<p>Diffusion models inherently differ from AR models due to their generative approach, which aims to reconstruct data from noise by modeling the entire distribution of training data [15].In multi-class classification tasks with mutually exclusive single-correct answers, diffusion models respond distinctly when encountering OOD inputs.Primarily, diffusion models excel at reconstructing data similar to their training distribution.However, upon encountering OOD inputs, their reconstruction accuracy deteriorates markedly due to difficulty in mapping the unfamiliar inputs onto the learned data manifold.Consequently, predictions on OOD data are characterized by uncertainty, with predictions distributed across multiple classes instead of sharply peaking at a single class.Visual inspection of the confusion matrices from Pi0 Base on datasets like Bigfish, Climber, and Maze as seen in Figures 34, 35, and 36, supports this finding, demonstrating distributed prediction frequencies rather than dominance by any one class.</p>
<p>In contrast, AR models sequentially predict outputs conditioned on previous outputs, typically applying softmax at the final step for classification tasks.A known limitation of this approach is an inherent tendency toward overconfidence in OOD scenarios.Despite inputs being significantly divergent from training data, AR models frequently produce sharply peaked probability distributions, arbitrarily favoring one class.This behavior arises from their inductive biases, where the softmax Fig. 20: Class-wise precision averaged across all subdatasets for all 5 models.Precision is significantly higher for 4 across all models.activation overestimates probabilities for certain classes.This issue is evidenced clearly in the union confusion matrices seen in Figure 19 and the action distributions seen in 14, 15, 16, 17, and 18.Each AR model consistently collapses its predictions onto particular classes, even with nonsensical or significantly divergent inputs, as observed in the individual per-subdataset confusion matrices in the Appendix section VII-B for OpenVLA's outcomes on Chaser and Jumper, Pi0 FAST's results on Bigfish, Chaser, and Heist, and GPT 4x's performance on Heist, Maze, Coinrun and Miner.</p>
<p>The difference in collapse behaviors between the AR and Diffusion models can be further observed by comparing the action distributions of Pi0 Base and Pi0 FAST in Figure 21, as both models share the PaliGemma VLM backbone in their architecture.The prediction collapse behavior also varies significantly between different AR models due to differences in tokenization methods.Specifically, OpenVLA and Pi0 FAST differ fundamentally in their tokenization and decoding strategies, influencing their OOD performance distinctly.</p>
<p>Pi0 FAST utilizes the Discrete Cosine Transform (DCT) [1] in combination with Byte Pair Encoding (BPE) [29], focusing on low-frequency components that represent the overarching shape of the signals.This frequency-domain approach, particularly the strategy of flattening DCT coefficients prioritizing low-frequency information, results in concentrated predictions due to stronger global pattern capture and robust representation against input noise.This approach enhances consistency and Conversely, OpenVLA employs a discrete binning approach, mapping continuous action spaces to discrete bins directly, thus not capturing global patterns in any sense when evaluated in a zero-shot setting.OpenVLA exhibits sensitivity to variations and input noise, resulting in more evenly distributed probability mass across action classes, as reflected in the broader spread observed in its confusion matrices.</p>
<p>Overall, our analysis highlights how decoding methods and tokenization strategies critically impact models' behaviors in OOD scenarios, indicating the need for targeted adjustments depending on the specific application requirements for generalization performance.</p>
<p>V. GENERALIZATION BARRIERS IN VISION-LANGUAGE-ACTION MODELS: DATASET, ARCHITECTURE, AND PROCESSING TECHNIQUES</p>
<p>1) Differences Between Training Data and Procgen:</p>
<p>Dataset Domain Discrepancies: A fundamental obstacle limiting model performance arises from substantial differences between the training datasets and the Procgen evaluation dataset.Procgen environments are procedurally generated, Atari-like 2D games designed to evaluate visual and motor skills of reinforcement learning (RL) agents.Each subdataset within Procgen exhibits diverse environment layouts, tasks, objectives, reward structures, and discrete action spaces, predominantly involving directional movements and specific gamerelated interactions.</p>
<p>Conversely, the training datasets vary significantly across models:</p>
<p>GPT 4x models were primarily trained on expansive webscale vision-language data, lacking a dedicated focus on action-oriented interactions.</p>
<p>OpenVLA is a vision-language-action model specifically fine-tuned on approximately 970,000 robotics demonstrations from the Open X-Embodiment dataset [22].These demonstrations are strictly limited to continuous robotic manipulation tasks recorded from third-person camera views.</p>
<p>Pi0 models also leverage robotics trajectory data featuring continuous action spaces.Their training incorporates a subset of OpenX-Embodiment alongside a private dataset containing approximately 903 million timesteps, significantly sourced from single and dual-arm robotic manipulation scenarios [3].</p>
<p>The substantial domain gap-real-world robotic manipulation versus simulated game interaction, continuous versus discrete action spaces, and physical manipulation versus simple directional controls-makes zero-shot adaptation particularly challenging for vision-language-action (VLA) models.</p>
<p>Action Space Awareness: Each model exhibits varying degrees of awareness regarding the action space constraints:</p>
<p>GPT 4x models are explicitly informed through the provided prompts about valid action values and corresponding verbal descriptions.</p>
<p>OpenVLA employs output clamping, effectively limiting predictions strictly within valid action ranges upon unnormalization based on dataset statistics.</p>
<p>Pi0 Base and Pi0 Fast rely on training-induced normalization but lack explicit clamping, resulting in a higher chance of invalid predictions outside permissible action ranges.</p>
<p>Proprioceptive State Handling: Procgen datasets lack proprioceptive state information, a critical input for Pi0 models during training.To compensate, zero arrays replace missing proprioceptive states during inference, negatively impacting Pi0 model performance by introducing unnatural, non-representative states.While OpenVLA optionally accepts proprioceptive states, the absence of these in Procgen data inherently limits the richness of information available for inference.</p>
<p>Image Input Variability: Significant discrepancies in image inputs further exacerbate model adaptation difficulties:</p>
<p>Resolution mismatch: Training images for OpenVLA and Pi0 models are at 224x224 resolution, whereas Procgen images are limited to 64x64 pixels, necessitating zero-padding and resizing, likely impairing visual perception and subsequent performance.</p>
<p>View differences: OpenVLA expects a single image input, matching the Procgen input structure, potentially enhancing its adaptability.In contrast, Pi0 models trained on multiple views suffer from performance degradation due to missing additional viewpoints being substituted by zero arrays.</p>
<p>2) Difficulty Adapting to Increased Image Complexity: We quantified image complexity through Shannon entropy [30] and Delentropy [25] across 80 episodes over 16 datasets as seen in Figures 37, 22, and 23, revealing distinctive performance patterns.The formula used to calculate the entropies can be seen in Appendix section VII-C: GPT-4 models (4o and 4.1) demonstrated moderate to weak negative correlations (Shannon entropy: -0.409 and -0.183; Delentropy: -0.345 and -0.126, respectively), suggesting poorer performance with increasing complexity.</p>
<p>OpenVLA and Pi0 Base exhibited negligible correlations with entropy metrics (OpenVLA: -0.173 Shannon, -0.063 Delentropy; Pi0 Base: 0.003 Shannon, -0.011 Delentropy), implying relative independence from image complexity.These findings highlight that image complexity impacts GPT-4 models negatively, benefits Pi0 Fast, and minimally affects OpenVLA and Pi0 Base.The moderate correlation strength indicates complexity alone does not fully explain model performance variability.</p>
<p>3) Impact of VLA Training, Decoding, and Output Processing Techniques: Distinct training, decoding, and output processing strategies significantly shape model generalization capabilities:</p>
<p>OpenVLA utilizes a robust output processing approach, training to output actions within a normalized range of [-1,1], with strict clamping enforced.Consequently, its predictions, upon unnormalization, consistently map within valid action spaces, eliminating invalid outputs and markedly enhancing zero-shot generalization.</p>
<p>Pi0 Base also targets normalized outputs within [-1,1] without enforced clamping.Its diffusion-based architecture inherently predicts actions "close" to familiar training distributions.This approach yields relatively fewer invalid outputs and moderately effective generalization compared to purely autoregressive models, but remains inferior to OpenVLA.</p>
<p>Pi0 FAST, while similarly trained on normalized action outputs, employs autoregressive decoding without clamping, lacking the intrinsic action proximity measure found in diffusion models.Consequently, Pi0 FAST predictions deviate significantly from valid actions when generalization fails, resulting in extremely high invalid output rates and poor overall performance, except for moderate macro precision.</p>
<p>Collectively, these findings underscore critical architectural and training considerations necessary for enhancing generalization across diverse and unseen discrete action environments.</p>
<p>VI. CONCLUSION</p>
<p>In this study, we systematically evaluated the generalization capabilities of contemporary VLAs and VLMs on procedurally generated discrete-action environments from the Procgen dataset in a zero-shot setting.Our analysis highlighted significant limitations arising from architectural constraints, training paradigms, and input-output biases inherent to the models.The stark domain discrepancy between training data-primarily continuous-action robotics datasets, and general web-scale vision language data-and discrete-action game environments proved to be a critical barrier to effective zero-shot generalization.</p>
<p>We identified notable differences in model behaviors linked directly to their architectures, training strategies, and input/output processing techniques.OpenVLA's robust action-space clamping technique consistently provided superior generalization, minimizing invalid outputs and exhibiting relative resilience to out-of-distribution scenarios.Conversely, autoregressive models like GPT-4x displayed substantial difficulty in generalizing, especially under complex image conditions, and frequently defaulted to idle or biased action choices.Additionally, Pi0 models showed intermediate performance influenced heavily by their diffusion-based (Pi0 Base) or autoregressive (Pi0 FAST) decoding methods, with Pi0 FAST being notably sensitive to image complexity and unable to restrict the majority of its predictions to a desired output range.</p>
<p>Our findings underscore the necessity for architectural innovations, refined training methodologies, and enhanced output processing techniques to bridge the gap between diverse action domains.Future research should prioritize developing more generalized training datasets that better reflect the variety of potential application environments, alongside methods to adaptively handle different forms of action representations.These advancements hold promise for enabling VLAs and VLMs to operate effectively across an increasingly diverse and unpredictable range of real-world tasks.
H = − n−1 ∑ i=0 p i log p i(11)
• Delentropy Delentropy is a metric that leverages a probability density function, referred to as deldensity, which can be computed from image data.By analyzing how pixels and their spatial relationships co-occur within an image, deldensity enables delentropy to effectively characterize the underlying structural patterns present in the image.Delentropy is defined as:
H = − ∑</p>
<p>F. Genesis Framework</p>
<p>The Genesis framework consists of the following parts:</p>
<p>• System-level Instructions: The overall goal, the constraints for inference, and the nature of the interaction (e.g., a simulated environment)</p>
<p>• Task and Environment Context: Explicit descriptions of the specific Procgen sub-dataset task, its rules, and relevant environmental details</p>
<p>• Multimodal Input Integration: The visual observation (current game state image) within the prompt structure</p>
<p>• Action Space Definition: The available actions, their format, and their corresponding verbal descriptions</p>
<p>• Output Instructions: The precise output structure such that the model handles the complexity of generating a valid set of probabilities for the actions in the action space Although the information provided to GPT 4o and 4.1 was the same, the prompt constructed for GPT 4.1 was more rigorously curated than the one for GPT 4o.The prompt for GPT 4.1 employed more prompt engineering techniques to minimize the number of invalid outputs.The new prompt explicitly specifies the simulated and hypothetical nature of the dataset and capitalizes key instructions for emphasis.Additionally, the new prompt tailors the input type descriptions strictly to those encountered in the current dataset, rather than stating all possible types of input from the broader collection of Multinet datasets.Here are examples of the old and new prompts constructed by Genesis for a given timestep of the subdataset Heist:</p>
<p>• The old prompt, used to profile GPT4o: "You are an AI agent to solve the task called steal the gem.The hashmap should contain a key for each option index of that action, and the value for that key corresponds to the probability that this option should be selected as the next step.ALL probabilities across all actions, as opposed to per action or hashmap, MUST sum up to 1.0.You should not include any other words or characters in your response."</p>
<p>Fig. 1 :
1
Fig. 1: Model rankings across datasets by macro recall, highlighting consistent top performance by OpenVLA and GPT-4.1.</p>
<p>Fig. 2 :
2
Fig. 2: Brier Mean Absolute Error scores across 4 models -GPT 4o, OpenVLA, GPT 4.1, and Pi0 FAST.Pi0 Base is a diffusion-based model and can not be evaluated using Brier MAE due to a lack of logits in its inference architecture.All models display Brier MAE close to 2, indicating poor performance.</p>
<p>Fig. 3 :
3
Fig. 3: Micro precision across all 5 models -GPT 4o, Open-VLA, Pi0 Base, Pi0 FAST, and GPT 4.1.When considering all 16 subdatasets, Pi0 FAST overall showcases the worst performance, and OpenVLA the best performance.</p>
<p>Fig. 4 :
4
Fig. 4: Micro precision across all 5 models -GPT 4o, OpenVLA, Pi0 Base, Pi0 FAST, and GPT 4.1 without invalids.When considering all 16 subdatasets, GPT 4.1 overall showcases the worst performance, and OpenVLA the best performance.</p>
<p>Fig. 5 :
5
Fig. 5: Macro precision across all 5 models.GPT 4.1 and Pi0 FAST show variability and preference towards specific subdatasets.OpenVLA shows reliability on the majority classes, whereas GPT 4o displays relatively stronger performance on minority classes.</p>
<p>Fig. 6 :
6
Fig. 6: Macro recall across all 5 models.OpenVLA performs better when considering macro recall compared to macro precision, indicating a high number of false positives.GPT 4o shows lower macro recall than precision, indicating biased performance towards specific minority classes.GPT 4.1 and Pi0 Base show relatively less biased and moderate performance, whereas Pi0 FAST showed consistent low recall.</p>
<p>Fig. 8 :
8
Fig. 8: Normalized Brier MAE across 4 models -GPT 4o, OpenVLA, GPT 4.1, Pi0 FAST.It follows the trends of regular Brier MAE, indicating minimal reliance on outliers.</p>
<p>Fig. 9 :
9
Fig. 9: Normalized Quantile Filtered Brier MAE across 4 models.It follows trends of regular and Normalized Brier MAE scores, indicating minimal reliance on extreme predictions.</p>
<p>Fig. 10 :Fig. 11 :
1011
Fig. 10: Maximum relative Brier MAE across 4 models.All values are equal to or around 1.0, indicating limited deviation between median and extreme bad predictions.</p>
<p>Fig. 12 :
12
Fig. 12: Class-wise recall averaged across all subdatasets for all 5 models.Recall and Variance on classes 4-10 is relatively higher, and is low on classes 0-3, and 11-12.</p>
<p>Fig. 13 :
13
Fig. 13: Model action class recall vs. action class frequency</p>
<p>Fig. 14 :
14
Fig. 14: GPT4o prediction vs. ground truth distributions.</p>
<p>Fig. 15 :
15
Fig. 15: OpenVLA prediction vs. ground truth distributions</p>
<p>Fig. 16 :Fig. 17 :
1617
Fig. 16: Pi0 base prediction vs. ground truth distributions</p>
<p>Fig. 19 :
19
Fig. 19: Union confusion matrices depicting frequency of class predictions and ground truth matches.Order from top to bottom -GPT 4o, OpenVLA, Pi0 Base, Pi0 FAST, GPT 4.1.</p>
<p>Fig. 21 :
21
Fig. 21: Pi0 Base vs. Pi0 FAST action distributions.</p>
<p>Fig. 22 :
22
Fig. 22: Entropy measures VS Model Macro Recall.</p>
<p>Fig. 23 :
23
Fig. 23: Correlation matrix for Entropy measures VS Model Macro Recall.</p>
<p>Fig. 24 :
24
Fig.24: Heatmap of the 5 models' the macro recall scores over 16 subdatasets.OpenVLA has consistently good macro recall over all subdatasets.Datasets such as ninja with complex action space are difficult across all 5 models.</p>
<p>Figures 25 to 36
36
Figures 25 to 36 are examples of confusion matrices of models on individual subdatasets to observe the pattern of action class collapse across models.</p>
<p>Fig. 25 :Fig. 26 :Fig. 27 :
252627
Fig. 25: Confusion matrix for GPT 4.1 on Heist</p>
<p>Fig. 30 :Fig. 32 :Fig. 34 :Fig. 36 :
30323436
Fig. 30: Confusion matrix for OpenVLA on Jumper</p>
<p>Fig. 37 :
37
Fig. 37: Datasets ranked by Shannon entropy</p>
<p>Fig. 38 :
38
Fig. 38: Correlation matrix for Entropy measures VS Model Macro Recall</p>
<p>Normalized Brier Mean Absolute Error Average of Brier Absolute Errors that have been min-max normalized using the minimum and maximum Brier absolute errors over all timesteps.A discrepancy in trends between regular Brier MAE and Normalized Brier MAE indicates reliance on outlier predictions that lead to abnormally high or low Brier MAEs.• Normalized Quantile Filtered Brier Mean Absolute Error This metric value is obtained by filtering out the Brier Absolute Errors that are lesser than or equal to the 5th percentile error or greater than or equal to the 95th percentile error, and then normalizing based on the quantile filtered minimum and maximum errors.This metric helps indicate how good or bad the majority of the predictions are.Discrepancy in trends between normalized and normalized quantile filtered Brier MAEs indicates the reliance on a few extremely good or extremely bad predictions.
stable evaluation of typical performance. The maximumpossible Brier score is 2, which is assigned to invalidpredictions as a high penalty.Brier scorepenalizes predictions that are both wrong and confidentmore severely than those that are wrong but uncertain.Additionally, unlike Mean Squared Error, using MeanAbsolute Error maintains the original scale of the data,making it potentially more interpretable in the actionspace's context. MAE is also less sensitive to outlierpredictions than squared error metrics, thus giving a more
• • Maximum relative Brier Mean Absolute Error Max relative Brier MAE = max(MAE) median(MAE)</p>
<p>In this environment: Collect keys of different colors.Open colored locks.Reach and collect the hidden gem.You should produce a proper action output to achieve the final goal given the current progress so far given the current state information.The current state can be any forms, such as images, continuous/discrete vectors, or texts.The actions available: A discrete action has the available options as key-value pairs, Option index: Option description.0. Agent movement action =&gt; Discrete.Options: 0: 'LEFT + DOWN', 1: 'LEFT', 2: 'LEFT + UP', 3: 'DOWN', 4: 'Do Nothing', 5: 'UP', 6: 'RIGHT + DOWN', 7: 'RIGHT', 8: 'RIGHT + UP'.You must generate your output keeping the following format: A list starting with '[' and ending with ']'.Each position corresponds to each action index.Each position in that list should be a hashmap starting with '' and ending with ''.The hashmap should contain a key for each option index of that action, and the value for that key corresponds to the probability that this option should be selected as the next step.All probabilities across all actions, as opposed to per action or hashmap, must sum up to 1.0.You should not include any other words or characters in your response." • The new prompt, used to profile GPT 4.1: "We are running a simulation for an AI agent playing a video game.Your role is to evaluate potential moves based on a snapshot.The description of this hypothetical scenario is steal the gem.In this environment: Collect keys of different colors.Open colored locks.Reach and collect the hidden gem.You should produce a proper action output to achieve the final goal given the current progress so far given the current state information.The current state consists of an image, which is a snapshot of the game screen, and a text description of the objective.The actions available: A discrete action has the available options as key-value pairs, Option index: Option description.0. Agent movement action =&gt; Discrete.Options: 0: 'LEFT + DOWN', 1: 'LEFT', 2: 'LEFT + UP', 3: 'DOWN', 4: 'Do Nothing', 5: 'UP', 6: 'RIGHT + DOWN', 7: 'RIGHT', 8: 'RIGHT + UP'.You MUST generate your output keeping the following format: A list starting with '[' and ending with ']'.Each position corresponds to each action index.Each position MUST be a hashmap starting with '' and ending with ''.</p>
<p>https://dl.fbaipublicfiles.com/DGRL/1M/expert/
https://www.tensorflow.org/api docs/python/tf/data/Dataset
https://huggingface.co/openvla/openvla-7b
https://github.com/Physical-Intelligence/openpi</p>
<p>Discrete cosine transform. N Ahmed, T Natarajan, K R Rao, IEEE Transactions on Computers, C. 2311974</p>
<p>The arcade learning environment: An evaluation platform for general agents. G Marc, Bellemare, arXiv:1207.47082012arXiv preprint</p>
<p>pi0: A vision-language-action flow model for general robot control. Kevin Black, arXiv:2410.241642024arXiv preprint</p>
<p>Verification of forecasts expressed in terms of probability. Glenn W Brier, Monthly Weather Review. 781950</p>
<p>Rt-2: Vision-language-action models transfer web knowledge to robotic control. Anthony Brohan, arXiv:2307.158182023arXiv preprint</p>
<p>Gui-world: A video benchmark and dataset for multimodal gui-oriented understanding. Dongping Chen, arXiv:2406.108192024arXiv preprint</p>
<p>Leveraging procedural generation to benchmark reinforcement learning. Karl Cobbe, Christopher Hesse, Jacob Hilton, John Schulman, 2020</p>
<p>Shengheng, arXiv:2103.16397v23d affordancenet: A benchmark for visual object affordance understanding. 2021arXiv preprint</p>
<p>Xiang, arXiv:2306.06070Mind2web: Towards a generalist agent for the web. 2023arXiv preprint</p>
<p>Bridgedata v2: A dataset for robot learning at scale. Frederik Ebert, arXiv:2308.129522023arXiv preprint</p>
<p>Building open-ended embodied agents with internet-scale knowledge. Linxi Fan, Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track. 2022</p>
<p>Voyager: An open-ended embodied agent with large language models. arXiv:2305.162912023arXiv preprint</p>
<p>Benchmarking vision, language, &amp; action models on robotic learning tasks. Pranav Guruprasad, arXiv:2411.058212024arXiv preprint</p>
<p>Benchmarking the spectrum of agent capabilities. Danijar Hafner, arXiv:2109.067802022arXiv preprint</p>
<p>Denoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, 2020</p>
<p>Os agents: A survey on mllm-based agents for computer, phone and browser use. Xueyu Hu, 2024OpenReview</p>
<p>Fast: Efficient action tokenization for vision-languageaction models. Brian Ichter, arXiv:2501.097472025arXiv preprint</p>
<p>Openvla: An open-source vision-language-action model. J Moo, Kim, arXiv:2406.092462024arXiv preprint</p>
<p>Towards generalist robot policies: What matters in building vision-language-action models. Xinghang Li, arXiv:2412.140582024arXiv preprint</p>
<p>Q Lin, Kevin , arXiv:2406.10227A benchmark for gui automation from instructional videos. 2024arXiv preprint</p>
<p>A survey on vision-language-action models for embodied ai. Yueen Ma, arXiv:2405.14093v22024arXiv preprint</p>
<p>Open x-embodiment: Robotic learning datasets and rt-x models. arXiv:2310.088642024arXiv preprint</p>
<p>arXiv:2303.08774OpenAI. Gpt-4 technical report. 2023arXiv preprint</p>
<p>Introducing gpt-4.1 in the api. Openai, 2025</p>
<p>A Ameet, Rahane, arXiv:2008.04431Measures of complexity for large scale image datasets. 2020arXiv preprint</p>
<p>Christopher, arXiv:2307.10088Android in the wild: A large-scale dataset for android device control. 2023arXiv preprint</p>
<p>. Scott Reed, arXiv:2205.061752022A generalist agent. arXiv preprint</p>
<p>Ai agents for computer use: A review of instructionbased computer control, gui automation, and operator assistants. J Pascal, Sager, arXiv:2501.161502025arXiv preprint</p>
<p>Neural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, 2016</p>
<p>A mathematical theory of communication. The Bell System Technical Journal. Claude Elwood, Shannon , 194827</p>
<p>Cradle: Empowering foundation agents towards general computer control. Weihao Tan, arXiv:2403.031862024arXiv preprint</p>
<p>Agibot world colosseo: A large-scale manipulation platform for scalable and intelligent embodied systems. arXiv:2503.06669Team AgiBot-World. 2025arXiv preprint</p>
<p>. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing , Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, 2023Aurelien RodriguezAngela Fan, Melanie Kambadur; Robert Stojnic, Sergey Edunovand Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models</p>
<p>Grandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Nature. 5752019</p>
<p>Benchmarking multimodal agents for open-ended tasks in real computer environments. Tianbao Xie, arXiv:2404.079722024arXiv preprint</p>
<p>Jianwei, arXiv:2502.13130Magma: A foundation model for multimodal ai agents. 2025arXiv preprint</p>
<p>Sigmoid loss for language image pre-training. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer, 2023</p>
<p>Vlabench: A large-scale benchmark for languageconditioned robotics manipulation with long-horizon reasoning tasks. Shiduo Zhang, arXiv:2412.181942024arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
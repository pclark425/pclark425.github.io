<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-2655 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-2655</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-2655</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-68.html">extraction-schema-68</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <p><strong>Paper ID:</strong> paper-382301f0a9a85c298c6ec51ba4434ba040db960c</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/382301f0a9a85c298c6ec51ba4434ba040db960c" target="_blank">Accelerating science with human-aware artificial intelligence</a></p>
                <p><strong>Paper Venue:</strong> Nature Human Behaviour</p>
                <p><strong>Paper TL;DR:</strong> It is shown that incorporating the distribution of human expertise by training unsupervised models on simulated inferences that are cognitively accessible to experts dramatically improves AI prediction of future discoveries beyond models focused on research content alone, especially when relevant literature is sparse.</p>
                <p><strong>Paper Abstract:</strong> Artificial intelligence (AI) models trained on published scientific findings have been used to invent valuable materials and targeted therapies, but they typically ignore the human scientists who continually alter the landscape of discovery. Here we show that incorporating the distribution of human expertise by training unsupervised models on simulated inferences that are cognitively accessible to experts dramatically improves (by up to 400%) AI prediction of future discoveries beyond models focused on research content alone, especially when relevant literature is sparse. These models succeed by predicting human predictions and the scientists who will make them. By tuning human-aware AI to avoid the crowd, we can generate scientifically promising ‘alien’ hypotheses unlikely to be imagined or pursued without intervention until the distant future, which hold promise to punctuate scientific advance beyond questions currently pursued. By accelerating human discovery or probing its blind spots, human-aware AI enables us to move towards and beyond the contemporary scientific frontier. Can human-aware artificial intelligence help accelerate science? In this article, the authors incorporate the distribution of human expertise by training unsupervised models on simulated inferences cognitively accessible to experts and show that this substantially improves the models’ predictions of future discoveries, but also enables AI to generate high-value alternatives that complement human discoveries.</p>
                <p><strong>Cost:</strong> 0.021</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e2655.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e2655.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human-aware hypergraph + DeepWalk</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human-aware research hypergraph embedding with DeepWalk</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An unsupervised, human-aware AI system that builds a mixed hypergraph of (materials, properties, authors), generates random-walk sequences over that hypergraph to simulate cognitively-accessible inferences, and trains a DeepWalk/skip-gram embedding on the resulting sequences (with authors optionally removed) to rank candidate material-property hypotheses by cosine similarity.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Human-aware hypergraph DeepWalk embedding</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Constructs a mixed hypergraph whose nodes are materials, properties and disambiguated authors and whose hyperedges are papers; performs many truncated non-lazy random walks (start at property node, sample articles then authors/materials using an alpha-weighted sampling) to obtain sequences representing human-accessible inference paths; optionally remove authors from sequences and train a skip-gram Word2Vec (DeepWalk) embedding (dim=200, window=8, epochs=5) on these sequences; use cosine similarity between property and material vectors as a relevance score to rank hypotheses. Implementation details include alpha-controlled sampling (alpha=1 recommended) and walk length 20, with 250k walks per property.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge-graph-based (hypergraph) + embedding-based (DeepWalk/skip-gram)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science; biomedicine / drug repurposing (including COVID-19)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Generate candidate hypotheses by ranking candidate materials by cosine similarity in the DeepWalk embedding space trained on random-walk sequences over the mixed hypergraph; random walks simulate chains of human inference (author-author, author-material, material-property steps) and the embedding captures hypergraph proximities representing human cognitive accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty (human-unfamiliarity) is not intrinsic to this embedding mode (it targets human-accessible hypotheses); novelty relative to human attention is assessed elsewhere (see SPD + beta system). In practice, DeepWalk predictions are concentrated near peaks of human expert density; novelty is therefore implicitly low for top DeepWalk scores.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility within this method is implicit via learned hypergraph proximities from published literature (co-occurrence and coauthorship); no separate first-principles plausibility scorer is required for the embedding-only predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Not applicable for the pure DeepWalk predictor (it emphasizes human-accessible, plausible-by-literature hypotheses).</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Quality assessed primarily by precision@50 (fraction of top-50 ranked candidates subsequently published/discovered), and by comparison of embedding similarity ranks; embedding similarity (cosine) itself is the ranking metric.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical retrospective validation against ground-truth discoveries published after the prediction year (Tshitoyan dataset for inorganic materials; CTD-curated drug-disease associations and ClinicalTrials.gov for COVID-19); compute precision of top-50 lists over time (annual/monthly).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Code and data links provided (GitHub) and explicit dataset descriptions (1.5M inorganic-article corpus; MEDLINE; DrugBank; use of disambiguated author datasets PKG/Scopus); hyperparameters and walk sampling procedures reported.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Implicit prevention by training on actual publication hypergraph: embedding proximity reflects co-publication/coauthor pathways, and low-probability/unsupported links receive low embedding similarity, reducing implausible (hallucinatory) high-rank outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Evaluation uses precision metrics and reports substantive comparative improvements; correlations (e.g., Pearson r between prediction precision and drug occurrence frequency: r=0.74, p<0.001) reported for analyses; no formal per-hypothesis p-values reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Implicit through transition probabilities and empirical precision curves across years; DeepWalk itself does not produce calibrated uncertainties, but downstream analyses use empirical discovery rates over time as probabilistic validation.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>1.5M inorganic materials articles (Tshitoyan et al. corpus) and MEDLINE (27.5M papers with abstracts and disambiguated authors); DrugBank drug list (~4k), CTD gold-standard drug-disease associations, ClinicalTrials.gov for COVID-19 trials.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Substantial improvements in retrospective precision: average ~100% increase in precision for predicting materials' future discoveries versus content-only Word2Vec baseline (materials/energy tasks); for COVID-19 therapeutics/vaccines, DeepWalk achieved ~36–38% precision at 12 months (rising to 42% by July 2021) vs ~10–12% for content-only baseline (a 350–400% relative improvement).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Compared to a content-only Word2Vec baseline trained on textual corpora, the human-aware DeepWalk embedding (with authorship hypergraph) yielded much higher precision@50 across tasks (materials and drug repurposing), especially when relevant literature was sparse. DeepWalk (alpha=1) outperformed Word2Vec; even alpha→∞ (material-only hypergraph) often outperformed Word2Vec.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Predicted several candidates later entered clinical trials (e.g., progesterone for COVID-19 was among true positives uniquely predicted by the human-aware method and later trialed); many top-50 predictions corresponded to subsequently published discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Focuses on pairwise material-property relations; depends on co-occurrence and coauthorship metadata (coarse content signal only); DeepWalk embeddings do not provide calibrated uncertainty per hypothesis; results depend on quality of author disambiguation and 5-year time-window choice.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating science with human-aware artificial intelligence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2655.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e2655.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transition probability (Bayesian) metric</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Random-walk-induced multi-step transition probability computed via Bayesian rules</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic metric that computes the likelihood a random walker transitions from a property node to a material node within s=2 or s=3 steps over the mixed hypergraph, using Bayes' rule and Markov assumptions to avoid explicit sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Hypergraph transition-probability scorer (Bayesian)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Analytical computation of multi-step transition probabilities between property and material nodes across meta-paths (e.g., PAM and PAAM), derived from hypergraph adjacency and conditional probabilities; uses Bayes' rule so that random-walk densities are estimated without generating walks. These probabilities are used to rank candidate materials for a target property.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>probabilistic graph-based (random-walk / Markovian + Bayesian computation)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science; biomedicine / drug repurposing</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Compute transition probability from property to material via all meta-paths of given lengths (2-3 steps) and rank candidates by these probabilities; high transition probability implies high human-cognitive accessibility and likelihood of near-future discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Not designed to generate novel/alien hypotheses — it favors materials with high human expert density (low novelty). Novelty is inversely related to transition probability.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility in this scoring is represented by the transition probability reflecting social and literature support for the inference; no explicit content-based plausibility signal is required.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>This metric emphasizes plausibility derived from human-accessible paths at the expense of novelty; balancing with novelty requires integrating with SPD/beta architecture.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Ranked by transition probability; evaluated by precision@50 in retrospective tests.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Retrospective evaluation against later-published discoveries (same datasets as embedding approach).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Analytic computation reduces stochastic sampling variance; hypergraph construction and parameterization (s, alpha) are specified.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By only scoring hypotheses with appreciable transition probability through real author-paper connections, it reduces unsupported hallucinations lacking human-inference paths.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Produces explicit probabilities (transition probabilities) as a direct quantification of the likelihood of a hypothesis being reachable by human inference; these probabilities serve as an uncertainty proxy.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same: 1.5M inorganic corpus; MEDLINE; DrugBank; CTD; ClinicalTrials.gov.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used alongside DeepWalk; transition-probability-based predictor improved precision substantially over content-only baselines (aggregate ~100% improvement for energy materials). Exact numeric precision values vary per property and experiment but follow similar improvement patterns as DeepWalk.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Outperformed content-only Word2Vec baseline in discovery prediction. Transition-probability and DeepWalk served as twin human-aware criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Contributed to correctly ranking later-discovered material-property and drug-disease associations (evaluated retrospectively).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Focus on short meta-paths (s=2,3) may miss longer transitive inference chains; favors hypotheses already near human attention (low exploration).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating science with human-aware artificial intelligence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2655.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e2655.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SPD + Beta complementary AI</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Shortest-path distance (SPD) human-inaccessibility combined with literature plausibility via mixing coefficient beta</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A two-signal complementary-discovery algorithm: (1) measures 'alienness' (human-inaccessibility) as shortest-path distances (SPD) between property and candidate materials in the hypergraph, (2) measures scientific plausibility via cosine similarity in content-based word embeddings, standardizes both signals (Van der Waerden + Z-score), and linearly combines them with mixing weight beta to produce hypotheses that range from human-competitive (beta<0) to human-complementary/alien (beta>0).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>SPD (alienness) + plausibility combination with mixing coefficient beta</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Compute SPD across the author-material-property hypergraph as an ordinal measure of cognitive inaccessibility; compute content-based plausibility as cosine similarity in a Word2Vec embedding trained on prior literature; standardize both distributions (Van der Waerden transform then Z-score) and compute final score = (1-beta)/2 * (plausibility_Z) + (1+beta)/2 * (alienness_Z) (conceptually) or otherwise linear combination parameterized by beta in [-1,1], where more positive beta biases toward alien hypotheses. Generate top-k ranked candidate materials under each beta setting.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>knowledge-graph-based (shortest-path) + content-embedding fusion; controllable exploration-exploitation heuristic</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science; biomedicine / drug repurposing (400 diseases incl. COVID-19)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Rank candidates by combined standardized plausibility and alienness score for chosen beta; varying beta produces families of hypotheses from human-mimetic to human-avoiding; report top-50 per beta as predictions.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Novelty is explicitly computed as SPD in the hypergraph (longer SPD = greater human-inaccessibility/novelty). Distinctness from contemporary investigations is measured by SPD and by checking overlap with existing publications.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility measured by cosine similarity between material and property embeddings in Word2Vec trained on prior literature; additionally validated against theoretical first-principles scores (DFT power factor, spontaneous polarization estimates, PPI proximity) where available.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Explicit linear trade-off controlled by beta ∈ [-1,1]; beta<0 emphasizes exploitation/human-accessible plausible hypotheses, beta=0 emphasizes plausibility only (content-only), beta>0 favors exploration/alien hypotheses; intermediate beta (0.2-0.3) empirically identified as a high-yield interval balancing novelty and plausibility.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Precision@50 for discoverability; average theoretical scores (e.g., DFT-derived PF, symmetry-based polarization, PPI proximity) used to quantify scientific merit for undiscovered hypotheses; 'expectation gap' metric defined as difference between expected beta under plausible vs discoverable conditionals.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Retrospective validation against later-published discoveries (discoverability) and computational/theoretical validation via first-principles scores for ~45% of properties (DFT, symmetry analysis, PPI networks); analysis of discovery wait times and expectation gaps; joint probability P(undiscoverable, plausible | beta) used to pick optimal beta ranges.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Full algorithmic description (SPD, Van der Waerden + Z-score normalization, beta mixing) and code repository provided; theoretical scoring methods described in Supplementary Information.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Couples human-inaccessibility (alienness) with an explicit plausibility signal and thresholds so that highly alien but implausible candidates are penalized; removes candidates lacking content-based plausibility before reporting high-beta alien hypotheses.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Compares discoverability decay vs theoretical score decay across beta, reports expectation gaps across properties; p-values reported for some correlations (e.g., r=0.74, p<0.001) but no per-hypothesis significance testing reported.</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Uses probabilistic constructs: conversion of theoretical scores to probabilities, estimation of likelihoods P(beta | plausible) and P(beta | discoverable), and computation of joint probability P(undiscoverable, plausible | beta) to quantify trade-offs and uncertainty across beta; transition probabilities from random walks also provide probabilistic measures of accessibility.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>MEDLINE, 1.5M inorganic corpus, DrugBank drug pool, CTD; theoretical scoring datasets (DFT computations, symmetry-based polarization estimates, curated protein interaction databases).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Empirical findings: discoverability monotonically decreases as beta increases; theoretical-score decay is slower, with plausible but undiscovered hypotheses persisting up to beta ≈ 0.4. Identified optimal beta interval 0.2–0.3 where candidates are unlikely to be found by humans yet likely to yield strong theoretical scores. Quantified expectation gaps positive for most properties.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>At beta=0 (plausibility-only), method reduces to content-only baselines. Positive beta values produced hypotheses that were much less discoverable by humans but often had equal or higher theoretical scores than published discoveries, outperforming naive content-only search for complementary discovery.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Generated many predictions that remained undiscovered during the study period but showed favorable first-principles scores (e.g., for thermoelectricity and some diseases), supporting the generation of promising 'alien' hypotheses; paper cites progesterone example as a human-accessible prediction rather than an alien one.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Theoretical scorers used for validation were available for only ~45% of properties; SPD novelty is based solely on coauthorship/connectivity and may miss other sources of human awareness (conferences, institutional proximity); extreme beta values can generate scientifically absurd candidates if plausibility signal is ignored.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating science with human-aware artificial intelligence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2655.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e2655.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>GraphSAGE GCN (graph convolutional network)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Graph Sample and Aggregate (GraphSAGE) graph convolutional neural network used as embedding/link predictor</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-convolutional autoencoder (GraphSAGE encoder + inner-product decoder) trained in an unsupervised link-prediction setup using Word2Vec-based node features to produce node embeddings used to rank candidate material-property pairs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>GraphSAGE graph convolutional autoencoder</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Uses Word2Vec embeddings of textual tokens (for materials/properties) as node features; a GraphSAGE encoder with hidden dim 400 and output dim 200 and ReLU activations is trained with an inner-product decoder to minimize unsupervised link-prediction loss. The encoder outputs are used as final embeddings; candidates are ranked via cosine similarity to the property vector.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>graph neural network (inductive GNN) / neural-embedding-based</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science (energy-related properties)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Compute embeddings via GraphSAGE on the hypergraph (with and without authors); rank candidate materials by cosine similarity to property node embedding and select top-50.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Not explicitly a novelty-focused method; novelty differences assessed by running model with and without author nodes to measure author contribution.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility derived from learned graph embeddings that incorporate both feature vectors (Word2Vec) and graph structure (authors/material co-occurrences).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Balance depends implicitly on graph structure and features; no explicit beta-like control in this method as implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Precision@50 reported per property; comparison of full hypergraph vs author-less versions.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Retrospective validation against later publications; reported precisions for thermoelectricity, ferroelectricity, photovoltaics: full graph precisions 62%, 58%, 74% respectively, and author-less graph 48%, 50%, 58% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Model hyperparameters reported; node features from Word2Vec baseline described; code available in repository.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Graph-structure-constrained embeddings reduce spurious predictions by incorporating author-mediated connectivity and content features; no explicit hallucination filter.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>GraphSAGE outputs deterministic embeddings; paper does not report Bayesian uncertainties or ensemble-based uncertainty quantification for the GCN outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>1.5M inorganic article corpus (materials dataset).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Full-graph precision results: 62% (thermoelectricity), 58% (ferroelectricity), 74% (photovoltaic); author-less graph precisions: 48%, 50%, 58% respectively.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>GraphSAGE on full hypergraph outperformed the author-less variant and showed similar patterns to DeepWalk but with somewhat smaller margins relative to Word2Vec baseline due to reliance on Word2Vec-based node features.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>Contributed to retrospective ranking of discoverable materials; no unique experimentally validated discoveries attributed solely to GraphSAGE in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Requires node feature vectors (they used Word2Vec features), which biases the GCN toward the content-only baseline domain; no uncertainty quantification reported.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating science with human-aware artificial intelligence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2655.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e2655.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Word2Vec baseline (content-only)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Unsupervised Word2Vec word embedding trained on textual contents (content-only baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A content-only unsupervised embedding model (Word2Vec/skip-gram) trained on scientific article text (titles/abstracts) to measure semantic proximity between material and property terms and used as a baseline for discovery prediction.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Content-only Word2Vec baseline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Train a Word2Vec skip-gram model over the corpus of publications prior to the prediction year and compute cosine similarity between property and material tokens to rank candidate hypotheses (replicates Tshitoyan et al. approach).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>embedding-based (text-only)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science; biomedicine (used as baseline across domains in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Rank materials by cosine similarity in the Word2Vec space with the target property.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Not explicitly assessed; content-only model tends to be noisy and less precise due to irrelevant words in abstracts/titles.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Plausibility is inferred directly from textual co-occurrence/semantic proximity in the literature corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>No explicit balance; purely plausibility-by-content.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Precision@50 used for evaluation; baseline precisions often much lower than human-aware methods (e.g., ~10–12% for early COVID-19 predictions at 12 months).</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Retrospective comparison with ground-truth discoveries as in Tshitoyan et al.; used as primary baseline across tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Corpus and model hyperparameters referenced; original Tshitoyan et al. implementation replicated where possible.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Same corpora: 1.5M inorganic articles, MEDLINE, etc.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Lower precision vs human-aware methods: e.g., ~10% precision for COVID-19 candidate therapeutics in first 12 months vs 36–38% for human-aware models; therefore human-aware methods showed 350–400% improvement in relative precision.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Served as the baseline; human-aware methods (DeepWalk, transition prob, GraphSAGE with authors) substantially outperformed Word2Vec baseline across multiple datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Prone to noise from irrelevant words in titles/abstracts; ignores distribution of human experts and coauthorship signals, leading to substantially lower precision in prediction tasks when literature is sparse.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating science with human-aware artificial intelligence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2655.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e2655.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>First-principles / theoretical scorers</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>First-principles and data-driven theoretical scoring functions (DFT power factor, symmetry polarization, PPI proximity)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Property-specific computational/theoretical models used to score candidate materials for plausibility independent of publication/discovery status: DFT-derived power factor for thermoelectricity, symmetry-derived spontaneous polarization estimates for ferroelectricity, and protein–protein interaction proximity metrics for drug-disease relevance.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>First-principles and data-driven theoretical validators</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>For thermoelectricity: compute power factor (PF) and contributions to zT via density functional theory simulations; for ferroelectricity: estimate spontaneous polarization via symmetry analysis and theoretical equations; for disease-drug links: compute proximity between disease agent proteins and compounds within curated protein-protein interaction networks and/or high-throughput protein screens. These scores provide an external, physics/biology-grounded plausibility metric usable for hypotheses not yet experimentally tested.</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>physics-based simulation / network biology scoring</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science (thermoelectricity, ferroelectricity); biomedicine (drug-disease relevance, COVID-19)</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generator per se — used as an independent validator/scorer for hypotheses produced by AI predictors; assign continuous theoretical scores to candidates.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Not designed for novelty assessment, but can identify high-scoring candidates that remain undiscovered (novel but plausible).</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Direct physical/biological plausibility scoring via domain-specific models: DFT power factor, spontaneous polarization via symmetry analysis, PPI network proximity measures; theoretical scores converted to probabilities for further probabilistic analyses.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Used to validate SPD+beta outputs: positive beta candidates that maintain high theoretical scores are considered complementary and promising.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Theoretical scores themselves (e.g., PF numeric values, polarization magnitudes, network proximity scores) and derived probability transforms used to compute plausibility-conditioned distributions.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Computational/theoretical evaluation with established physics/biology models; used to compare average theoretical scores of predicted sets vs published discoveries; also used to compute expectation gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Details and computational procedures described in Supplementary Information; datasets (e.g., curated PPI networks) cited.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>Acts as a strong guard against hallucinations by scoring and eliminating scientifically implausible candidates even if they are hypergraph-remote.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Theoretical scores converted into probabilities for use in probabilistic estimations of plausibility; used in computing P(beta | plausible).</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>DFT-calculated property datasets; symmetry analysis methods; curated protein–protein interaction databases and high-throughput screening results.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Used to show that average theoretical scores of predicted sets remain high into modestly positive beta values (up to ~0.4) and in many cases exceed average theoretical scores of published discoveries before declining at high beta.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Provided an orthogonal check beyond text/hypergraph-based baselines: many human-avoiding hypotheses scored better on theoretical validators than published discoveries, indicating complementary promise.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td>No direct laboratory validations reported in paper; theoretical scores indicated many promising undiscovered hypotheses deserving experimental evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Available only for a subset (~45%) of properties; first-principles scores are conservative and depend on model/parameter choices; computational cost may be high for large candidate pools.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating science with human-aware artificial intelligence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e2655.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e2655.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of AI systems or methods that generate scientific hypotheses, evaluate hypothesis quality, validate scientific claims, detect hallucinations, or quantify uncertainty in scientific reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Expectation gap / joint probabilistic model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Expectation gap and probabilistic joint model P(undiscoverable, plausible | beta)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A probabilistic evaluation framework that transforms theoretical scores to probabilities, estimates likelihood distributions P(beta | plausible) and P(beta | discoverable), computes an 'expectation gap' (difference in expected beta between these conditionals) and evaluates joint probability that a prediction is both plausible and undiscoverable for a given beta.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Expectation gap probabilistic evaluator</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Convert theoretical (first-principles) scores into probabilities, then for each beta compute empirical likelihoods of predictions being discoverable and plausible; compute the expectation gap as the difference between expected beta under plausible vs discoverable conditionals; compute joint probability P(undiscoverable, plausible | beta) to identify optimal beta ranges for complementary hypothesis generation (empirically 0.2–0.3).</td>
                        </tr>
                        <tr>
                            <td><strong>system_type</strong></td>
                            <td>probabilistic evaluation / decision analysis</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>materials science and biomedicine</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_generation_method</strong></td>
                            <td>Not a generator — evaluates sets of generated hypotheses across beta configurations to identify operating points that maximize plausibility while minimizing discoverability.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment_method</strong></td>
                            <td>Relies on discoverability (empirical publication overlap) to indicate novelty/alienness; expectation gap measures shift between plausibility and discoverability across beta.</td>
                        </tr>
                        <tr>
                            <td><strong>plausibility_assessment_method</strong></td>
                            <td>Uses converted theoretical-score probabilities to define P(beta | plausible).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_plausibility_balance</strong></td>
                            <td>Explicitly quantifies and optimizes the trade-off using joint probability and expectation gaps to identify beta values that maximize complementarity.</td>
                        </tr>
                        <tr>
                            <td><strong>hypothesis_quality_metrics</strong></td>
                            <td>Expectation gap (numeric difference between expected betas), joint probability P(undiscoverable, plausible | beta), average theoretical score per predicted set, precision@50 for discoverability.</td>
                        </tr>
                        <tr>
                            <td><strong>pre_experiment_evaluation</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>validation_mechanism</strong></td>
                            <td>Empirical: uses retrospective discoverability data and theoretical scores to estimate distributions and compute expectation gaps; shows positive expectation gaps for most properties and identifies beta interval 0.2–0.3 as promising.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_measures</strong></td>
                            <td>Procedure and transformations described; code repository available.</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_prevention_method</strong></td>
                            <td>By jointly modeling plausibility and discoverability, it discourages selection of high-alienness but low-plausibility candidates (reduces hallucination risk).</td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_detection_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>hallucination_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>statistical_significance_testing</strong></td>
                            <td>Expectation gap significance assessed across properties; details in Supplementary Information (paper reports 'substantial and significantly positive' expectation gaps for most properties).</td>
                        </tr>
                        <tr>
                            <td><strong>uncertainty_quantification_method</strong></td>
                            <td>Explicit: converts theoretical scores to probabilities and uses likelihood estimation to produce distributions over beta; reports joint probabilities that quantify uncertainty about complementarity outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>benchmark_dataset</strong></td>
                            <td>Subset of properties with theoretical scores available (thermoelectricity, ferroelectricity, COVID-19, and 175 other diseases; 178/404 properties used).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Shows positive expectation gaps for majority of properties and identifies 0.2–0.3 beta interval as commonly optimal; demonstrates that discoverability drops faster than theoretical scores as beta increases.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_baseline</strong></td>
                            <td>Provides a principled contrast to naive beta selection; demonstrates that plausibility-only (beta=0) is insufficient to produce complementary hypotheses while extreme beta=1 yields implausible outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>validated_on_real_science</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>novel_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>limitations</strong></td>
                            <td>Relies on availability of theoretical scores to estimate plausibility-conditioned distributions; conversion of theoretical scores to probabilities involves modeling assumptions; limited to properties with dependable theoretical scorers.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Accelerating science with human-aware artificial intelligence', 'publication_date_yy_mm': '2023-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Unsupervised word embeddings capture latent knowledge from materials science literature <em>(Rating: 2)</em></li>
                <li>DeepWalk: online learning of social representations <em>(Rating: 2)</em></li>
                <li>Inductive representation learning on large graphs <em>(Rating: 2)</em></li>
                <li>Network medicine framework for identifying drug-repurposing opportunities for COVID-19 <em>(Rating: 2)</em></li>
                <li>node2vec: Scalable Feature Learning for Networks <em>(Rating: 2)</em></li>
                <li>Choosing experiments to accelerate collective discovery <em>(Rating: 1)</em></li>
                <li>PathSim <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-2655",
    "paper_id": "paper-382301f0a9a85c298c6ec51ba4434ba040db960c",
    "extraction_schema_id": "extraction-schema-68",
    "extracted_data": [
        {
            "name_short": "Human-aware hypergraph + DeepWalk",
            "name_full": "Human-aware research hypergraph embedding with DeepWalk",
            "brief_description": "An unsupervised, human-aware AI system that builds a mixed hypergraph of (materials, properties, authors), generates random-walk sequences over that hypergraph to simulate cognitively-accessible inferences, and trains a DeepWalk/skip-gram embedding on the resulting sequences (with authors optionally removed) to rank candidate material-property hypotheses by cosine similarity.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Human-aware hypergraph DeepWalk embedding",
            "system_description": "Constructs a mixed hypergraph whose nodes are materials, properties and disambiguated authors and whose hyperedges are papers; performs many truncated non-lazy random walks (start at property node, sample articles then authors/materials using an alpha-weighted sampling) to obtain sequences representing human-accessible inference paths; optionally remove authors from sequences and train a skip-gram Word2Vec (DeepWalk) embedding (dim=200, window=8, epochs=5) on these sequences; use cosine similarity between property and material vectors as a relevance score to rank hypotheses. Implementation details include alpha-controlled sampling (alpha=1 recommended) and walk length 20, with 250k walks per property.",
            "system_type": "knowledge-graph-based (hypergraph) + embedding-based (DeepWalk/skip-gram)",
            "scientific_domain": "materials science; biomedicine / drug repurposing (including COVID-19)",
            "hypothesis_generation_method": "Generate candidate hypotheses by ranking candidate materials by cosine similarity in the DeepWalk embedding space trained on random-walk sequences over the mixed hypergraph; random walks simulate chains of human inference (author-author, author-material, material-property steps) and the embedding captures hypergraph proximities representing human cognitive accessibility.",
            "novelty_assessment_method": "Novelty (human-unfamiliarity) is not intrinsic to this embedding mode (it targets human-accessible hypotheses); novelty relative to human attention is assessed elsewhere (see SPD + beta system). In practice, DeepWalk predictions are concentrated near peaks of human expert density; novelty is therefore implicitly low for top DeepWalk scores.",
            "plausibility_assessment_method": "Plausibility within this method is implicit via learned hypergraph proximities from published literature (co-occurrence and coauthorship); no separate first-principles plausibility scorer is required for the embedding-only predictions.",
            "novelty_plausibility_balance": "Not applicable for the pure DeepWalk predictor (it emphasizes human-accessible, plausible-by-literature hypotheses).",
            "hypothesis_quality_metrics": "Quality assessed primarily by precision@50 (fraction of top-50 ranked candidates subsequently published/discovered), and by comparison of embedding similarity ranks; embedding similarity (cosine) itself is the ranking metric.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Empirical retrospective validation against ground-truth discoveries published after the prediction year (Tshitoyan dataset for inorganic materials; CTD-curated drug-disease associations and ClinicalTrials.gov for COVID-19); compute precision of top-50 lists over time (annual/monthly).",
            "reproducibility_measures": "Code and data links provided (GitHub) and explicit dataset descriptions (1.5M inorganic-article corpus; MEDLINE; DrugBank; use of disambiguated author datasets PKG/Scopus); hyperparameters and walk sampling procedures reported.",
            "hallucination_prevention_method": "Implicit prevention by training on actual publication hypergraph: embedding proximity reflects co-publication/coauthor pathways, and low-probability/unsupported links receive low embedding similarity, reducing implausible (hallucinatory) high-rank outputs.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": "Evaluation uses precision metrics and reports substantive comparative improvements; correlations (e.g., Pearson r between prediction precision and drug occurrence frequency: r=0.74, p&lt;0.001) reported for analyses; no formal per-hypothesis p-values reported.",
            "uncertainty_quantification_method": "Implicit through transition probabilities and empirical precision curves across years; DeepWalk itself does not produce calibrated uncertainties, but downstream analyses use empirical discovery rates over time as probabilistic validation.",
            "benchmark_dataset": "1.5M inorganic materials articles (Tshitoyan et al. corpus) and MEDLINE (27.5M papers with abstracts and disambiguated authors); DrugBank drug list (~4k), CTD gold-standard drug-disease associations, ClinicalTrials.gov for COVID-19 trials.",
            "performance_metrics": "Substantial improvements in retrospective precision: average ~100% increase in precision for predicting materials' future discoveries versus content-only Word2Vec baseline (materials/energy tasks); for COVID-19 therapeutics/vaccines, DeepWalk achieved ~36–38% precision at 12 months (rising to 42% by July 2021) vs ~10–12% for content-only baseline (a 350–400% relative improvement).",
            "comparison_with_baseline": "Compared to a content-only Word2Vec baseline trained on textual corpora, the human-aware DeepWalk embedding (with authorship hypergraph) yielded much higher precision@50 across tasks (materials and drug repurposing), especially when relevant literature was sparse. DeepWalk (alpha=1) outperformed Word2Vec; even alpha→∞ (material-only hypergraph) often outperformed Word2Vec.",
            "validated_on_real_science": true,
            "novel_discoveries": "Predicted several candidates later entered clinical trials (e.g., progesterone for COVID-19 was among true positives uniquely predicted by the human-aware method and later trialed); many top-50 predictions corresponded to subsequently published discoveries.",
            "limitations": "Focuses on pairwise material-property relations; depends on co-occurrence and coauthorship metadata (coarse content signal only); DeepWalk embeddings do not provide calibrated uncertainty per hypothesis; results depend on quality of author disambiguation and 5-year time-window choice.",
            "uuid": "e2655.0",
            "source_info": {
                "paper_title": "Accelerating science with human-aware artificial intelligence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Transition probability (Bayesian) metric",
            "name_full": "Random-walk-induced multi-step transition probability computed via Bayesian rules",
            "brief_description": "A probabilistic metric that computes the likelihood a random walker transitions from a property node to a material node within s=2 or s=3 steps over the mixed hypergraph, using Bayes' rule and Markov assumptions to avoid explicit sampling.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Hypergraph transition-probability scorer (Bayesian)",
            "system_description": "Analytical computation of multi-step transition probabilities between property and material nodes across meta-paths (e.g., PAM and PAAM), derived from hypergraph adjacency and conditional probabilities; uses Bayes' rule so that random-walk densities are estimated without generating walks. These probabilities are used to rank candidate materials for a target property.",
            "system_type": "probabilistic graph-based (random-walk / Markovian + Bayesian computation)",
            "scientific_domain": "materials science; biomedicine / drug repurposing",
            "hypothesis_generation_method": "Compute transition probability from property to material via all meta-paths of given lengths (2-3 steps) and rank candidates by these probabilities; high transition probability implies high human-cognitive accessibility and likelihood of near-future discovery.",
            "novelty_assessment_method": "Not designed to generate novel/alien hypotheses — it favors materials with high human expert density (low novelty). Novelty is inversely related to transition probability.",
            "plausibility_assessment_method": "Plausibility in this scoring is represented by the transition probability reflecting social and literature support for the inference; no explicit content-based plausibility signal is required.",
            "novelty_plausibility_balance": "This metric emphasizes plausibility derived from human-accessible paths at the expense of novelty; balancing with novelty requires integrating with SPD/beta architecture.",
            "hypothesis_quality_metrics": "Ranked by transition probability; evaluated by precision@50 in retrospective tests.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Retrospective evaluation against later-published discoveries (same datasets as embedding approach).",
            "reproducibility_measures": "Analytic computation reduces stochastic sampling variance; hypergraph construction and parameterization (s, alpha) are specified.",
            "hallucination_prevention_method": "By only scoring hypotheses with appreciable transition probability through real author-paper connections, it reduces unsupported hallucinations lacking human-inference paths.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Produces explicit probabilities (transition probabilities) as a direct quantification of the likelihood of a hypothesis being reachable by human inference; these probabilities serve as an uncertainty proxy.",
            "benchmark_dataset": "Same: 1.5M inorganic corpus; MEDLINE; DrugBank; CTD; ClinicalTrials.gov.",
            "performance_metrics": "Used alongside DeepWalk; transition-probability-based predictor improved precision substantially over content-only baselines (aggregate ~100% improvement for energy materials). Exact numeric precision values vary per property and experiment but follow similar improvement patterns as DeepWalk.",
            "comparison_with_baseline": "Outperformed content-only Word2Vec baseline in discovery prediction. Transition-probability and DeepWalk served as twin human-aware criteria.",
            "validated_on_real_science": true,
            "novel_discoveries": "Contributed to correctly ranking later-discovered material-property and drug-disease associations (evaluated retrospectively).",
            "limitations": "Focus on short meta-paths (s=2,3) may miss longer transitive inference chains; favors hypotheses already near human attention (low exploration).",
            "uuid": "e2655.1",
            "source_info": {
                "paper_title": "Accelerating science with human-aware artificial intelligence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "SPD + Beta complementary AI",
            "name_full": "Shortest-path distance (SPD) human-inaccessibility combined with literature plausibility via mixing coefficient beta",
            "brief_description": "A two-signal complementary-discovery algorithm: (1) measures 'alienness' (human-inaccessibility) as shortest-path distances (SPD) between property and candidate materials in the hypergraph, (2) measures scientific plausibility via cosine similarity in content-based word embeddings, standardizes both signals (Van der Waerden + Z-score), and linearly combines them with mixing weight beta to produce hypotheses that range from human-competitive (beta&lt;0) to human-complementary/alien (beta&gt;0).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "SPD (alienness) + plausibility combination with mixing coefficient beta",
            "system_description": "Compute SPD across the author-material-property hypergraph as an ordinal measure of cognitive inaccessibility; compute content-based plausibility as cosine similarity in a Word2Vec embedding trained on prior literature; standardize both distributions (Van der Waerden transform then Z-score) and compute final score = (1-beta)/2 * (plausibility_Z) + (1+beta)/2 * (alienness_Z) (conceptually) or otherwise linear combination parameterized by beta in [-1,1], where more positive beta biases toward alien hypotheses. Generate top-k ranked candidate materials under each beta setting.",
            "system_type": "knowledge-graph-based (shortest-path) + content-embedding fusion; controllable exploration-exploitation heuristic",
            "scientific_domain": "materials science; biomedicine / drug repurposing (400 diseases incl. COVID-19)",
            "hypothesis_generation_method": "Rank candidates by combined standardized plausibility and alienness score for chosen beta; varying beta produces families of hypotheses from human-mimetic to human-avoiding; report top-50 per beta as predictions.",
            "novelty_assessment_method": "Novelty is explicitly computed as SPD in the hypergraph (longer SPD = greater human-inaccessibility/novelty). Distinctness from contemporary investigations is measured by SPD and by checking overlap with existing publications.",
            "plausibility_assessment_method": "Plausibility measured by cosine similarity between material and property embeddings in Word2Vec trained on prior literature; additionally validated against theoretical first-principles scores (DFT power factor, spontaneous polarization estimates, PPI proximity) where available.",
            "novelty_plausibility_balance": "Explicit linear trade-off controlled by beta ∈ [-1,1]; beta&lt;0 emphasizes exploitation/human-accessible plausible hypotheses, beta=0 emphasizes plausibility only (content-only), beta&gt;0 favors exploration/alien hypotheses; intermediate beta (0.2-0.3) empirically identified as a high-yield interval balancing novelty and plausibility.",
            "hypothesis_quality_metrics": "Precision@50 for discoverability; average theoretical scores (e.g., DFT-derived PF, symmetry-based polarization, PPI proximity) used to quantify scientific merit for undiscovered hypotheses; 'expectation gap' metric defined as difference between expected beta under plausible vs discoverable conditionals.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Retrospective validation against later-published discoveries (discoverability) and computational/theoretical validation via first-principles scores for ~45% of properties (DFT, symmetry analysis, PPI networks); analysis of discovery wait times and expectation gaps; joint probability P(undiscoverable, plausible | beta) used to pick optimal beta ranges.",
            "reproducibility_measures": "Full algorithmic description (SPD, Van der Waerden + Z-score normalization, beta mixing) and code repository provided; theoretical scoring methods described in Supplementary Information.",
            "hallucination_prevention_method": "Couples human-inaccessibility (alienness) with an explicit plausibility signal and thresholds so that highly alien but implausible candidates are penalized; removes candidates lacking content-based plausibility before reporting high-beta alien hypotheses.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": "Compares discoverability decay vs theoretical score decay across beta, reports expectation gaps across properties; p-values reported for some correlations (e.g., r=0.74, p&lt;0.001) but no per-hypothesis significance testing reported.",
            "uncertainty_quantification_method": "Uses probabilistic constructs: conversion of theoretical scores to probabilities, estimation of likelihoods P(beta | plausible) and P(beta | discoverable), and computation of joint probability P(undiscoverable, plausible | beta) to quantify trade-offs and uncertainty across beta; transition probabilities from random walks also provide probabilistic measures of accessibility.",
            "benchmark_dataset": "MEDLINE, 1.5M inorganic corpus, DrugBank drug pool, CTD; theoretical scoring datasets (DFT computations, symmetry-based polarization estimates, curated protein interaction databases).",
            "performance_metrics": "Empirical findings: discoverability monotonically decreases as beta increases; theoretical-score decay is slower, with plausible but undiscovered hypotheses persisting up to beta ≈ 0.4. Identified optimal beta interval 0.2–0.3 where candidates are unlikely to be found by humans yet likely to yield strong theoretical scores. Quantified expectation gaps positive for most properties.",
            "comparison_with_baseline": "At beta=0 (plausibility-only), method reduces to content-only baselines. Positive beta values produced hypotheses that were much less discoverable by humans but often had equal or higher theoretical scores than published discoveries, outperforming naive content-only search for complementary discovery.",
            "validated_on_real_science": true,
            "novel_discoveries": "Generated many predictions that remained undiscovered during the study period but showed favorable first-principles scores (e.g., for thermoelectricity and some diseases), supporting the generation of promising 'alien' hypotheses; paper cites progesterone example as a human-accessible prediction rather than an alien one.",
            "limitations": "Theoretical scorers used for validation were available for only ~45% of properties; SPD novelty is based solely on coauthorship/connectivity and may miss other sources of human awareness (conferences, institutional proximity); extreme beta values can generate scientifically absurd candidates if plausibility signal is ignored.",
            "uuid": "e2655.2",
            "source_info": {
                "paper_title": "Accelerating science with human-aware artificial intelligence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "GraphSAGE GCN (graph convolutional network)",
            "name_full": "Graph Sample and Aggregate (GraphSAGE) graph convolutional neural network used as embedding/link predictor",
            "brief_description": "A graph-convolutional autoencoder (GraphSAGE encoder + inner-product decoder) trained in an unsupervised link-prediction setup using Word2Vec-based node features to produce node embeddings used to rank candidate material-property pairs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "GraphSAGE graph convolutional autoencoder",
            "system_description": "Uses Word2Vec embeddings of textual tokens (for materials/properties) as node features; a GraphSAGE encoder with hidden dim 400 and output dim 200 and ReLU activations is trained with an inner-product decoder to minimize unsupervised link-prediction loss. The encoder outputs are used as final embeddings; candidates are ranked via cosine similarity to the property vector.",
            "system_type": "graph neural network (inductive GNN) / neural-embedding-based",
            "scientific_domain": "materials science (energy-related properties)",
            "hypothesis_generation_method": "Compute embeddings via GraphSAGE on the hypergraph (with and without authors); rank candidate materials by cosine similarity to property node embedding and select top-50.",
            "novelty_assessment_method": "Not explicitly a novelty-focused method; novelty differences assessed by running model with and without author nodes to measure author contribution.",
            "plausibility_assessment_method": "Plausibility derived from learned graph embeddings that incorporate both feature vectors (Word2Vec) and graph structure (authors/material co-occurrences).",
            "novelty_plausibility_balance": "Balance depends implicitly on graph structure and features; no explicit beta-like control in this method as implemented.",
            "hypothesis_quality_metrics": "Precision@50 reported per property; comparison of full hypergraph vs author-less versions.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Retrospective validation against later publications; reported precisions for thermoelectricity, ferroelectricity, photovoltaics: full graph precisions 62%, 58%, 74% respectively, and author-less graph 48%, 50%, 58% respectively.",
            "reproducibility_measures": "Model hyperparameters reported; node features from Word2Vec baseline described; code available in repository.",
            "hallucination_prevention_method": "Graph-structure-constrained embeddings reduce spurious predictions by incorporating author-mediated connectivity and content features; no explicit hallucination filter.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "GraphSAGE outputs deterministic embeddings; paper does not report Bayesian uncertainties or ensemble-based uncertainty quantification for the GCN outputs.",
            "benchmark_dataset": "1.5M inorganic article corpus (materials dataset).",
            "performance_metrics": "Full-graph precision results: 62% (thermoelectricity), 58% (ferroelectricity), 74% (photovoltaic); author-less graph precisions: 48%, 50%, 58% respectively.",
            "comparison_with_baseline": "GraphSAGE on full hypergraph outperformed the author-less variant and showed similar patterns to DeepWalk but with somewhat smaller margins relative to Word2Vec baseline due to reliance on Word2Vec-based node features.",
            "validated_on_real_science": true,
            "novel_discoveries": "Contributed to retrospective ranking of discoverable materials; no unique experimentally validated discoveries attributed solely to GraphSAGE in the paper.",
            "limitations": "Requires node feature vectors (they used Word2Vec features), which biases the GCN toward the content-only baseline domain; no uncertainty quantification reported.",
            "uuid": "e2655.3",
            "source_info": {
                "paper_title": "Accelerating science with human-aware artificial intelligence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Word2Vec baseline (content-only)",
            "name_full": "Unsupervised Word2Vec word embedding trained on textual contents (content-only baseline)",
            "brief_description": "A content-only unsupervised embedding model (Word2Vec/skip-gram) trained on scientific article text (titles/abstracts) to measure semantic proximity between material and property terms and used as a baseline for discovery prediction.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Content-only Word2Vec baseline",
            "system_description": "Train a Word2Vec skip-gram model over the corpus of publications prior to the prediction year and compute cosine similarity between property and material tokens to rank candidate hypotheses (replicates Tshitoyan et al. approach).",
            "system_type": "embedding-based (text-only)",
            "scientific_domain": "materials science; biomedicine (used as baseline across domains in experiments)",
            "hypothesis_generation_method": "Rank materials by cosine similarity in the Word2Vec space with the target property.",
            "novelty_assessment_method": "Not explicitly assessed; content-only model tends to be noisy and less precise due to irrelevant words in abstracts/titles.",
            "plausibility_assessment_method": "Plausibility is inferred directly from textual co-occurrence/semantic proximity in the literature corpus.",
            "novelty_plausibility_balance": "No explicit balance; purely plausibility-by-content.",
            "hypothesis_quality_metrics": "Precision@50 used for evaluation; baseline precisions often much lower than human-aware methods (e.g., ~10–12% for early COVID-19 predictions at 12 months).",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Retrospective comparison with ground-truth discoveries as in Tshitoyan et al.; used as primary baseline across tasks.",
            "reproducibility_measures": "Corpus and model hyperparameters referenced; original Tshitoyan et al. implementation replicated where possible.",
            "hallucination_prevention_method": null,
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": null,
            "benchmark_dataset": "Same corpora: 1.5M inorganic articles, MEDLINE, etc.",
            "performance_metrics": "Lower precision vs human-aware methods: e.g., ~10% precision for COVID-19 candidate therapeutics in first 12 months vs 36–38% for human-aware models; therefore human-aware methods showed 350–400% improvement in relative precision.",
            "comparison_with_baseline": "Served as the baseline; human-aware methods (DeepWalk, transition prob, GraphSAGE with authors) substantially outperformed Word2Vec baseline across multiple datasets.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Prone to noise from irrelevant words in titles/abstracts; ignores distribution of human experts and coauthorship signals, leading to substantially lower precision in prediction tasks when literature is sparse.",
            "uuid": "e2655.4",
            "source_info": {
                "paper_title": "Accelerating science with human-aware artificial intelligence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "First-principles / theoretical scorers",
            "name_full": "First-principles and data-driven theoretical scoring functions (DFT power factor, symmetry polarization, PPI proximity)",
            "brief_description": "Property-specific computational/theoretical models used to score candidate materials for plausibility independent of publication/discovery status: DFT-derived power factor for thermoelectricity, symmetry-derived spontaneous polarization estimates for ferroelectricity, and protein–protein interaction proximity metrics for drug-disease relevance.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "First-principles and data-driven theoretical validators",
            "system_description": "For thermoelectricity: compute power factor (PF) and contributions to zT via density functional theory simulations; for ferroelectricity: estimate spontaneous polarization via symmetry analysis and theoretical equations; for disease-drug links: compute proximity between disease agent proteins and compounds within curated protein-protein interaction networks and/or high-throughput protein screens. These scores provide an external, physics/biology-grounded plausibility metric usable for hypotheses not yet experimentally tested.",
            "system_type": "physics-based simulation / network biology scoring",
            "scientific_domain": "materials science (thermoelectricity, ferroelectricity); biomedicine (drug-disease relevance, COVID-19)",
            "hypothesis_generation_method": "Not a generator per se — used as an independent validator/scorer for hypotheses produced by AI predictors; assign continuous theoretical scores to candidates.",
            "novelty_assessment_method": "Not designed for novelty assessment, but can identify high-scoring candidates that remain undiscovered (novel but plausible).",
            "plausibility_assessment_method": "Direct physical/biological plausibility scoring via domain-specific models: DFT power factor, spontaneous polarization via symmetry analysis, PPI network proximity measures; theoretical scores converted to probabilities for further probabilistic analyses.",
            "novelty_plausibility_balance": "Used to validate SPD+beta outputs: positive beta candidates that maintain high theoretical scores are considered complementary and promising.",
            "hypothesis_quality_metrics": "Theoretical scores themselves (e.g., PF numeric values, polarization magnitudes, network proximity scores) and derived probability transforms used to compute plausibility-conditioned distributions.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Computational/theoretical evaluation with established physics/biology models; used to compare average theoretical scores of predicted sets vs published discoveries; also used to compute expectation gaps.",
            "reproducibility_measures": "Details and computational procedures described in Supplementary Information; datasets (e.g., curated PPI networks) cited.",
            "hallucination_prevention_method": "Acts as a strong guard against hallucinations by scoring and eliminating scientifically implausible candidates even if they are hypergraph-remote.",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": null,
            "uncertainty_quantification_method": "Theoretical scores converted into probabilities for use in probabilistic estimations of plausibility; used in computing P(beta | plausible).",
            "benchmark_dataset": "DFT-calculated property datasets; symmetry analysis methods; curated protein–protein interaction databases and high-throughput screening results.",
            "performance_metrics": "Used to show that average theoretical scores of predicted sets remain high into modestly positive beta values (up to ~0.4) and in many cases exceed average theoretical scores of published discoveries before declining at high beta.",
            "comparison_with_baseline": "Provided an orthogonal check beyond text/hypergraph-based baselines: many human-avoiding hypotheses scored better on theoretical validators than published discoveries, indicating complementary promise.",
            "validated_on_real_science": true,
            "novel_discoveries": "No direct laboratory validations reported in paper; theoretical scores indicated many promising undiscovered hypotheses deserving experimental evaluation.",
            "limitations": "Available only for a subset (~45%) of properties; first-principles scores are conservative and depend on model/parameter choices; computational cost may be high for large candidate pools.",
            "uuid": "e2655.5",
            "source_info": {
                "paper_title": "Accelerating science with human-aware artificial intelligence",
                "publication_date_yy_mm": "2023-06"
            }
        },
        {
            "name_short": "Expectation gap / joint probabilistic model",
            "name_full": "Expectation gap and probabilistic joint model P(undiscoverable, plausible | beta)",
            "brief_description": "A probabilistic evaluation framework that transforms theoretical scores to probabilities, estimates likelihood distributions P(beta | plausible) and P(beta | discoverable), computes an 'expectation gap' (difference in expected beta between these conditionals) and evaluates joint probability that a prediction is both plausible and undiscoverable for a given beta.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Expectation gap probabilistic evaluator",
            "system_description": "Convert theoretical (first-principles) scores into probabilities, then for each beta compute empirical likelihoods of predictions being discoverable and plausible; compute the expectation gap as the difference between expected beta under plausible vs discoverable conditionals; compute joint probability P(undiscoverable, plausible | beta) to identify optimal beta ranges for complementary hypothesis generation (empirically 0.2–0.3).",
            "system_type": "probabilistic evaluation / decision analysis",
            "scientific_domain": "materials science and biomedicine",
            "hypothesis_generation_method": "Not a generator — evaluates sets of generated hypotheses across beta configurations to identify operating points that maximize plausibility while minimizing discoverability.",
            "novelty_assessment_method": "Relies on discoverability (empirical publication overlap) to indicate novelty/alienness; expectation gap measures shift between plausibility and discoverability across beta.",
            "plausibility_assessment_method": "Uses converted theoretical-score probabilities to define P(beta | plausible).",
            "novelty_plausibility_balance": "Explicitly quantifies and optimizes the trade-off using joint probability and expectation gaps to identify beta values that maximize complementarity.",
            "hypothesis_quality_metrics": "Expectation gap (numeric difference between expected betas), joint probability P(undiscoverable, plausible | beta), average theoretical score per predicted set, precision@50 for discoverability.",
            "pre_experiment_evaluation": true,
            "validation_mechanism": "Empirical: uses retrospective discoverability data and theoretical scores to estimate distributions and compute expectation gaps; shows positive expectation gaps for most properties and identifies beta interval 0.2–0.3 as promising.",
            "reproducibility_measures": "Procedure and transformations described; code repository available.",
            "hallucination_prevention_method": "By jointly modeling plausibility and discoverability, it discourages selection of high-alienness but low-plausibility candidates (reduces hallucination risk).",
            "hallucination_detection_method": null,
            "hallucination_rate": null,
            "statistical_significance_testing": "Expectation gap significance assessed across properties; details in Supplementary Information (paper reports 'substantial and significantly positive' expectation gaps for most properties).",
            "uncertainty_quantification_method": "Explicit: converts theoretical scores to probabilities and uses likelihood estimation to produce distributions over beta; reports joint probabilities that quantify uncertainty about complementarity outcomes.",
            "benchmark_dataset": "Subset of properties with theoretical scores available (thermoelectricity, ferroelectricity, COVID-19, and 175 other diseases; 178/404 properties used).",
            "performance_metrics": "Shows positive expectation gaps for majority of properties and identifies 0.2–0.3 beta interval as commonly optimal; demonstrates that discoverability drops faster than theoretical scores as beta increases.",
            "comparison_with_baseline": "Provides a principled contrast to naive beta selection; demonstrates that plausibility-only (beta=0) is insufficient to produce complementary hypotheses while extreme beta=1 yields implausible outputs.",
            "validated_on_real_science": true,
            "novel_discoveries": null,
            "limitations": "Relies on availability of theoretical scores to estimate plausibility-conditioned distributions; conversion of theoretical scores to probabilities involves modeling assumptions; limited to properties with dependable theoretical scorers.",
            "uuid": "e2655.6",
            "source_info": {
                "paper_title": "Accelerating science with human-aware artificial intelligence",
                "publication_date_yy_mm": "2023-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Unsupervised word embeddings capture latent knowledge from materials science literature",
            "rating": 2
        },
        {
            "paper_title": "DeepWalk: online learning of social representations",
            "rating": 2
        },
        {
            "paper_title": "Inductive representation learning on large graphs",
            "rating": 2
        },
        {
            "paper_title": "Network medicine framework for identifying drug-repurposing opportunities for COVID-19",
            "rating": 2
        },
        {
            "paper_title": "node2vec: Scalable Feature Learning for Networks",
            "rating": 2
        },
        {
            "paper_title": "Choosing experiments to accelerate collective discovery",
            "rating": 1
        },
        {
            "paper_title": "PathSim",
            "rating": 1
        }
    ],
    "cost": 0.021352499999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Accelerating science with human-aware artificial intelligence</h1>
<p>Jamshid Sourati ${ }^{1}$, James A. Evans ${ }^{1,2 <em>}$<br>${ }^{1}$ Department of Sociology, University of Chicago; Chicago, IL, 60637, the United States.<br>${ }^{2}$ Santa Fe Institute; Santa Fe, NM, 87501, the United States.<br></em>Corresponding author. Email: jevans@uchicago.edu</p>
<h2>Summary</h2>
<p>We explore how incorporating human awareness with artificial intelligence models can accelerate science. These simulate human inferences and can predict future discoveries, but also avoid them to generate valuable complementary hypotheses.</p>
<h4>Abstract</h4>
<p>Artificial intelligence (AI) models trained on published scientific findings have been used to invent valuable materials and targeted therapies, but they typically ignore the human scientists who continually alter the landscape of discovery. Here we show that incorporating the distribution of human expertise by training unsupervised models on simulated inferences cognitively accessible to experts dramatically improves (up to $400 \%$ ) AI prediction of future discoveries beyond those focused on research content alone, especially when relevant literature is sparse. These models succeed by predicting human predictions and the scientists who will make them. By tuning human-aware AI to avoid the crowd, we can generate scientifically promising "alien" hypotheses unlikely to be imagined or pursued without intervention until the distant future, which hold promise to punctuate scientific advance beyond questions currently pursued. Accelerating human discovery or probing its blind spots, human-aware AI enables us to move toward and beyond the contemporary scientific frontier.</p>
<p>Research across applied science and engineering, from materials discovery to drug and vaccine development, is hampered by enormous design spaces that overwhelm researchers' ability to experimentally evaluate all candidate designs ${ }^{1}$. To face this challenge, researchers have initialized data-driven AI models with published scientific results to create powerful prediction engines. These models have begun to assist human discovery by focusing scientific attention on the subset of discovery candidates most predicted to possess properties relevant to energy, ${ }^{2}$ human health ${ }^{3}$, and other economic and societal values. In this way, AI intervenes in the discovery process by proposing efficient, model-based experiments that would require much longer for unassisted human scientists to identify. Such efforts typically ignore the distribution of scientists and inventors ${ }^{4}$, however, the human prediction engines who continuously alter the landscape of discovery and invention. As we demonstrate below, incorporating knowledge of human researchers can dramatically improve predictions of future discoveries compared with AI methods that ignore them. Our work formalizes and demonstrates the critical importance of situated human expertise, communication and collaboration for unfolding scientific advance.</p>
<p>Previous studies have indicated that most new scientific discoveries emerge within neighborhoods of prior findings ${ }^{5,6}$. Here, we take a step further and demonstrate that the collective pattern of scientific attention is sufficient to boost the precision of future discovery forecasts. This generalizes the availability heuristic-the psychological tendency for individuals to evaluate event frequency based on cognitive availability ${ }^{7}$. The availability heuristic is known to result in misjudgments and decision bias ${ }^{8,9}$. Here we consider how and when this aggregates in scenarios involving entire scientific communities ${ }^{10}$. The more scientists investigate a combination of topics, the more frequently other scientists from their community will observe it presented at conferences and read about it in literature. As that combination of ideas is spoken and written about, it becomes easy to imagine and consider by nearby scientists and so conditions future scientific consideration and investigation. Here, we demonstrate that the distribution of scientists who author articles and their collaboration networks across topics and time is sufficient to foresee future discoveries and their discoverers with high precision, especially when research on the topic is sparse. This distribution, which can be recovered from publication metadata, represents a critical social fact that can stably improve our inferences about whether possible scientific relationships will soon be attempted. It can also inform our understanding of whether scientific possibilities will remain unimagined and unexplored until the more distant future ${ }^{11}$.</p>
<p>We define scientific knowledge discovery as the first-time report of the relationship between an existing material and a well-defined property. An example of such pairwise relationships is "vancomycin may be used to treat pneumonia", where vancomycin is the material and effective treatment of pneumonia is the property. Our approach draws on explicit measurement of the distribution of human scientists around each topic involved in candidate discoveries, using advances in unsupervised manifold learning ${ }^{12-14}$ and drawing upon easily available publication meta-data. By programmatically incorporating information on the evolving distribution of human experts, our approach balances exploration and exploitation in experimental search that could be used to accelerate the realization of discoveries predicted to appear in future. We contrast our human-aware approach with precise replication of a recent, prominent content-only analysis ${ }^{15}$ that trained a Word2Vec embedding model ${ }^{12}$ over millions of abstracts from materials science publications. That study used the resulting word vectors to infer that materials closest to electrochemical properties in the embedding space will be discovered in future to possess that property. Our models yield $\sim 100 \%$ increase in the precision of forecasts regarding future material science discoveries. We extend this approach to identify a much broader matrix of materials and their functional properties ${ }^{16}$, demonstrating comparable increases for predicting thousands of drugs</p>
<p>to treat more than a hundred distinct human diseases, including vaccines and therapies for COVID-19.</p>
<p>Using human-aware AI, we can not only accelerate science by anticipating the human crowd; we can avoid that crowd to construct insights that punctuate human discovery with complementary hypotheses unlikely to be discovered by human scientists. If we model discovery as establishing novel links among otherwise disconnected concepts ${ }^{13}$, it cannot occur until discoverers arise with viewpoints that bridge the fields required to imagine those conceptual connections (Fig. 1a). This diversity of scientific viewpoints was implicitly drawn upon by pioneering information scientist Swanson in his heuristic approach to knowledge generation. For example, he hypothesized that if Raynaud's disorder was linked to blood viscosity in one literature, and fish oil was known to decrease blood viscosity in another, then fish oil might lessen the symptoms of Raynaud's disorder, but would unlikely be arrived at in either field because no scientist was available to infer it ${ }^{14-16}$. This was one of several hypotheses later experimentally demonstrated ${ }^{17-19}$. Expansive opportunities for discovery persist as researchers crowd around past discoveries ${ }^{20}$, neglecting to explore regions of knowledge cognitively distant from recent findings ${ }^{21}$ (Extended Data Fig. 1). Our human-aware approach to complementary discovery scales and makes Swanson's heuristic continuous, identifying unstudied pairs of scientific entities likely to be scientifically and technologically relevant, but unlikely imagined. This approach avoids scientific topics at the center of collective attention and generates complementary hypotheses, which are not only unlikely to be considered by unassisted human experts, but outperform published discoveries. By staging intellectual arbitrage between isolated communities, our "alien" predictions are unconstrained by the human incentive to flock together within fields. In this way, our human-aware framework provides opportunities for accelerating the normal pathway of human discovery by predicting human accessible hypotheses, and punctuating that path by predicting human inaccessible hypotheses that complement it.</p>
<h1>Incorporating Human Experts with Hypergraph Proximity</h1>
<p>We model the distribution of inferences collectively and cognitively accessible to scientists by constructing a hypergraph over research publications. A hypergraph is a generalized graph where an edge connects a set of nodes rather than a node pair. Our research hypergraph is mixed, containing nodes corresponding not only to materials and properties mentioned in title or abstract, but also the researchers who investigate them (Fig. 1c, first step). Following construction of this research hypergraph, we identify cognitively accessible inferences by generating random walk sequences over it. These walks suggest paths of inference available to active human scientists, which trace mixtures of diverse expertise sufficient for contemporary discoveries. If a valuable material property (e.g., ferroelectricity-reversible electric polarization useful in sensors) is investigated by a scientist who, in prior research, worked with lead titanate $\left(\mathrm{PbTiO}<em 2="2">{3}\right.$, a ferroelectric material), that scientist is more likely to consider whether lead titanate is ferroelectric than a scientist without the research experience. If that scientist later coauthors with another who has previously worked with sodium nitrite $\left(\mathrm{NaNO}</em>\right.$, another ferroelectric material), that scientist is more likely to imagine whether sodium nitrite has the property through conversation than a scientist without the personal connection. In this way, the density of random walks over our research hypergraph is proportional to the density of cognitively plausible and conversationally attainable inferences. If two literatures share no scientists, a random walk over our hypergraph will rarely bridge them, just as a scientist will rarely consider connecting a property valued only in one community with a material understood only in a disjoint one (Fig. 1a). We hypothesize that identifying topics with high human expert density around them provides us with an informative signal regarding near-future discoveries. These topics might be located far from one another in terms of the number of steps required to travel between them in the hypergraph, but a random walker-and the collective</p>
<p>scientific mind-can easily travel between them if intermediate steps are socially dense, facilitating conversation and collaboration (Fig. 1a).</p>
<p>To generate each random walk sequence, our model (i) initiates the walk with a valued property (e.g., ferroelectricity) as first node in the sequence, (ii) randomly selects an article (hyperedge) having mentioned that property, (iii) randomly selects a material or author from that article as next node (end of first step), then starts the second step by randomly selecting another article with the newly selected material or author, and repeats this Markov process ${ }^{5,14}$ a pre-specified number of times (see Fig. 1b for an example, and Supplementary Information for more details). Each random walk step can be viewed as a simulation of human actions: an author-author step mimicks networking or conversation between two expert collaborators; an author-material or author-property step represents how an author is deeply familiar with the selected material/property they have studied and published on; finally, a material/property-material/property step captures the potential for the transition to be realized by human scientists through reading a collection of scientific articles. From the collaborative character of physical and biological science, author nodes in our hypergraph far outnumber materials. To compensate for this imbalance, we devise a non-uniform sampling distribution parameterized by $\alpha$, which roughly determines the fraction of material to author nodes in resulting sequences. Specifically, we define $\alpha$ when sampling a node from a paper (e.g., in step (iii) above) such that the probability of selecting a material is $\alpha$ times that of selecting an author (See Supplementary Fig. 1). Larger values of $\alpha$ result in sampling materials/properties more frequently suggesting that our simulated researcher will uncover new scientific possibilities predominantly through research and reading; smaller values result in higher frequency of author selection implying discovery through networking, conversation, and collaboration with others in the field.</p>
<p>Random walks over the mixed hypergraph induce meaningful proximities between nodes. The proximity of two authors suggests they share similar research interests and experiences. The proximity of a material to a scientist assesses the likelihood she is or will become familiar with that material through research experience, related reading, or social interaction. The proximity of materials to one another suggests that they may be substitutes, complements or share another more subtle relationship such as interaction or comparison. Finally, the proximity of a material to a property suggests the likelihood that the material may possess the property, but also that a scientist will discover and publish it (Extended Data Fig. 1a-b). In this way, our hypergraph-induced proximities incorporate physical and material properties latent within literature, but also the distribution of human scientists, which enables us to anticipate inferences by those scientists and predict upcoming discoveries. The distribution of human scientists is a factor available to and naturally "read" by other competitive scientists when they attend conferences and survey their fields for promising new directions.</p>
<p>In order to foresee the potential discovery of materials with a valued property (e.g., store energy; cure breast cancer, vaccinate against COVID-19), we utilize random walk-induced node similarity metrics to capture the relevance between the targeted property and candidate materials. These metrics, evaluated between pairs of property/material nodes, reflect the human-inferrable relatedness of corresponding nodes and are used to sort candidate materials and report those highest ranked as inferred to possess the property. A simple metric of this kind draws upon the local hypergraph structure to estimate the transition probability that a random walker travels from the property node to a material through intermediate author nodes within a fixed number of steps, denoted by $s$. We use Bayesian rules to calculate these probabilities without the need for actually running the random-walk sampler (see Supplementary Fig. 2). Here, we only consider two- and three-step transitions ( $s=2$ and $s=3$ ). Our main choice of metric, however, is based on a popular,</p>
<p>unsupervised neural network-based embedding algorithm (deepwalk ${ }^{13}$ ), estimated over the random walks we generate. Like previous content-only methods ${ }^{15}$, this method also entails construction of a word embedding model ${ }^{15}$. Instead of abstract sentences as input, however, the embedding is constructed over our hypergraph, considering every random walk sequence a "sentence" that links materials, experts and functional properties.</p>
<p>Whereas a text-based embedding captures semantic relevance among words, our approach obtains word vectors while preserving hypergraph proximities among all nodes and therefore can be used to measure the human cognitive accessibility of each material with respect to a targeted property. Because inferred discoveries involve relevant materials, we train the deepwalk embedding model after excluding authors from our random walk sequences (Fig. 1c). Cosine similarity in the resulting embedding space can be used as a relevance metric. We use these two relevance metrics, transition probabilities and deepwalk similarities, as twin criteria for selecting materials most likely to emerge as the next discoveries. Additionally, we train deeper graph convolutional neural networks, which confirms the pattern of results obtained from deepwalk (see Methods and Supplementary Information).</p>
<p>Note that our models do not use more data in comparison to traditional content-based methods, but instead alter the type of data we feed it. Specifically, our approach extracts and adds authorship information, but excludes the vast majority of textual content, excepting only material and property co-occurrences. In other words, our data is richer than traditional datasets in one dimension by adding human and social information, but less informative and dense in terms of content. Overall, our method possesses less data than the baselines against which we compare. In this way, our model's performance improvement, as shown below in Results, does not reflect more data, but more informative data.</p>
<h1>Results on Anticipating Human Discoveries</h1>
<p>To demonstrate the power of accounting for human experts, we use transition probability and deepwalk metrics to build two alternative discovery predictors. These algorithms assess the relevance of the focal property to each candidate material based on literature published prior to a given prediction year (e.g., 2001) by embedding the human-aware hypergraph. We contrast our predictions with a random baseline and predictions generated from precisely replicated prior work that uses word embeddings based on the textual content of scientific literature without accounting for the distribution of human scientists ${ }^{15}$. This prior work measured property/material relevance with cosine similarity from a Word2Vec model ${ }^{12}$ trained over the contents of scientific articles published prior to the prediction year. Our experiments and evaluation framework are identical to the settings of this study in order to facilitate precise replication. Each evaluated algorithm selects the 50 materials with highest similarity to the focal property based on hypergraph or Word2Vec similarity metrics and reports them as discovery predictions. We evaluate prediction quality based on their overlap with materials discovered and published after the prediction year (see Methods for further details; for alternative evaluation metrics and prediction sizes see Extended Data Fig. 2 and Supplementary Fig. 3).</p>
<h2>Energy-related Materials Prediction</h2>
<p>In our first set of experiments, we considered the valuable electrochemical properties of thermoelectricity, ferroelectricity and photovoltaic capacity against a pool of 100 K candidate inorganic compounds. Following the evaluation regime of Tshitoyan et al. on the same dataset ( 1.5 M scientific articles about inorganic materials) ${ }^{15}$, we ran prediction experiments with prediction</p>
<p>year 2001 for all three properties, predicting future discoveries as a function of research publicly available to contemporary scientists. We computed annual precisions following the prediction year until the end of 2018 (Extended Data Fig. 1c) and visualized them in a cumulative manner (Fig. 2a-c). The results indicate that predictions accounting for the distribution of human scientists outperformed baselines for all properties and materials by an average of $100 \%$.</p>
<p>Sensitivity analyses with $\alpha$ reveal that a deepwalk algorithm with $\alpha=1$, which balances the likelihood of sampling materials and author nodes, had the highest and most consistent precision of prediction. Moreover, even for extremely large values of $\alpha$ (i.e., $\alpha \rightarrow \infty$ ), where our random walk is ignorant of human experts, the deepwalk algorithm still substantially outperforms word2vec model. We conjecture that this occurs as vague title and abstract words, irrelevant to future discoveries, add noise to the proximity of properties and materials. Our hypergraph method ignores these words, but they mislead Word2Vec resulting in weaker predictions. This suggests a more specific conjecture. Material words alone are more relevant, specific, and semantically local to other materials and properties mentioned within a paper. In this way, our hypergraph-based approach infers new discoveries in the vicinity of previous findings. Such a localized process aligns with how scientists make discoveries, leading to stronger predictions ${ }^{5,6}$.</p>
<h1>Drug Repurposing Prediction</h1>
<p>We used the same approach to explore the repurposing of $\sim 4 \mathrm{~K}$ existing FDA-approved drugs to treat 100 important human diseases. We used the MEDLINE database of biomedical research publications and set the prediction year to 2001 (Extended Data Fig. 1c). Ground-truth discoveries were based on drug-disease associations established by expert curators of the Comparative Toxicogenomics Database (CTD) ${ }^{17}$, which chronicles the capacity of chemicals to influence human health. Figure 2e reports prediction precisions 18 years after prediction year, revealing how accounting for the distribution of biomedical experts in our unsupervised hypergraph embedding yields predictions with $43 \%$ higher precision than identical models accounting for research content alone. We found a strong correlation between our human-aware prediction precision and drug occurrence frequency in literature ( $r=0.74, \mathrm{p}&lt;0.001$ ), implying that our approach works best for diseases whose relevant drugs are frequently mentioned in prior research.</p>
<h2>COVID-19 Therapy and Vaccine Prediction</h2>
<p>We also considered therapies and vaccines to treat or prevent SARS-CoV-2 infection. Here, prediction year was set to 2020 (Extended Data Fig. 1c), when the global search for relevant drugs and vaccines began in earnest. Following Gysi et al. ${ }^{18}$, we considered a therapy relevant to COVID-19 if it amassed evidence to merit a COVID-related clinical trial, as reported by ClinicalTrials.gov. Results shown in Fig. 2d indicate that $36 \%$ and $38 \%$ of the predictions made by transition probability and deepwalk-based metrics, respectively, were selected by biomedical experts to evaluate using expensive clinical trials within 12 months of the prediction date (i.e., end of Dec, 2020), which further increased to $42 \%$ by the end of July, 2021. This is 350 to $400 \%$ higher than the precision of discovery candidates generated by scientific content alone ( $10 \%$ after the first 12 months and $12 \%$ in July 2021). These precisions were even higher than a recently proposed predictive model based on an ensemble of deep and shallow learning predictors trained on multiply measured protein interactions between COVID-19 and the pool of 3,948 relevant compounds from DrugBank ${ }^{18}$, relevant information to which our model was blind.</p>
<p>The success of these COVID-19 predictions suggests how fast-paced research on COVID therapies and vaccines increased the importance of scientists' prior research experiences and networks for the</p>
<p>therapies and vaccines they would come to imagine, evaluate and champion in clinical trials. Consider the female progesterone as a candidate material. Despite very few direct literature connections between "Coronavirus" and "Progesterone" before the rise of COVID-19, random walks from our method frequently walked the path between the two literatures through pre-COVID papers published in virology, immunology, studies regarding male/female characteristics of diseases, and the female reproductive system (Fig. 3a, Extended Data Table 1). Shortly after the beginning of 2020 and in 2021, two clinical trials were initiated with similar motivation ${ }^{19,20}$ : (1) the lower global death rate of women compared to men from COVID-19, and (2) the anti-inflammatory properties of progesterone that may moderate the immune system's overreaction to COVID-19 in men ${ }^{19}$. Our technique traced a pathway similar to the ones articulated explicitly by researchers sponsoring this trial: $75 \%$ of trial-cited papers, published within the five-year period preceding the prediction year we considered in building our hypergraph (2015-2019), were identified by our prediction model, and $60 \%$ of scientists authoring those studies were sampled in our random walk sequences. Progesterone and 18 other candidate materials were among the true positive predictions of our human scientist-aware method that could not be captured by the content-only baseline (Fig. 3b). By contrast, only four true positives were exclusively made by content-only prediction (Extended Data Table 2) and these four materials had significantly fewer mentions in comparison to other predicted materials, confirming that human-aware prediction performs better when candidates are mentioned frequently in prior literature.</p>
<h1>Human-Sensitive Prediction</h1>
<p>Our predictive models use the distribution of discovering experts to successfully improve discovery prediction. To demonstrate this, we consider the time required by scientists to make a discovery starting from the prediction year. Materials cognitively close to the community of researchers who study a given property receive greater attention and their relationships to that property are likely to be investigated, discovered and published earlier than those further from the community. In other words, the "wait time" for discovery should be inversely proportional to the size of the expert population aware of both property and candidate material. We measure the size of this population by defining human expert density between a property/material pair as the Jaccard index of two sets of human experts: those who mentioned the property and those who mentioned each candidate material in recent publications (Extended Data Fig. 3). This measures the overlap percentage between property and material research communities. For all three electrochemical properties mentioned earlier, COVID-19 therapies and vaccines, and a majority of the 100 diseases we considered above, correlations between discovery date and expert density were negative, significant and substantial (Extended Data Fig. 4). This result confirms our hypothesis that materials receiving attention from a larger crowd of property experts are discovered sooner. Our predictive models efficiently leverage the hypergraph of past publications to incorporate these human expert densities (Extended Data Fig. 5). Similar results can be derived based on embedding proximities: Fig. 4a-c illustrates how our predictions cluster atop density peaks in a joint embedding space of human experts and the materials they investigate. This further establishes that our human-aware approach is likely to select candidates more accessible to experts in the field.</p>
<p>We note that in some cases (e.g., photovoltaics and silicosis), discovery prediction resulted in competitive performance when $\alpha \rightarrow \infty$, with the random walker ignoring authors and traversing only material nodes. Nevertheless, the human-ignorant algorithm performs well only when mentions of the targeted property are frequent in the literature (Fig. 4d). An abundance of property-related publications, and their availability to human scientists, make the knowledge space more compact. This compactness enables scientists to infer future discoveries by simply taking in a redundant sample of papers, conference presentations, or review articles without maintaining</p>
<p>personal connection to relevant materials, properties, or scientists. Expert awareness is critical for navigation when the knowledge is new or sparse. Even in these situations, however, the human-ignorant case $\alpha \rightarrow \infty$ performs much better in predicting discoveries than $\alpha=0$ and other baselines, arguably because its inferences are local and in the vicinity of previous findings. This supports other evidence suggesting that scientists engage in localized search to make discoveries ${ }^{6}$.</p>
<p>In addition to predicting discoveries, human-aware hypergraph proximities are also able to predict discoverers most likely to publish discoveries based on their unique configuration of research experiences and collaborations. Here, discoverers are defined as all article authors associated with at least one discovery, disregarding author order. In order to identify potential discoverers of materials with a specific property, we compute the probability of random-walk transition from the targeted property to author nodes through a single intermediate material across our hypergraph (without rerunning the random-walk process). Then we report potential discoverers to be those with highest transition probabilities. Our calculations here are similar to transition probabilities for discovery inferences above, except that destination nodes are authors and intermediate nodes materials (see Supplementary Fig. 2). We evaluate these discoverer predictions against scientist authors who actually published discoveries following the prediction year. Calculating average precisions across 17 prediction years (2001 to 2017) for electrochemical properties, we find that $40 \%$ of the top 50 ranked potential authors became actual discoverers of thermoelectric and ferroelectric materials one year after prediction, and $20 \%$ of the top 50 predicted authors discovered novel photovoltaics (Fig. 4e). We also employ a method with slightly more subtlety to infer the identity of those predicted to discover a relationship between a targeted property and particular material (Extended Data Fig. 6).</p>
<p>Discoverer prediction serves as a validation of our main algorithm's operation-by implicitly identifying the people most likely to make the discovery. Strong precision values for both our discovery and discoverer predictions imply that discoveries are predominantly performed by individuals and teams familiar with and uniquely able to bridge otherwise disconnected topics and literatures. These results can also be viewed as an initial step towards predicting individuals and teams most likely qualified to achieve specific discoveries. They suggest the potential for a scientific service that recommends potential team members for recruitment on a targeted project.</p>
<h1>Results on Complementing Human Discoveries</h1>
<p>We can use our model of human cognitive availability to not only approach and mimic, but also avoid and complement the distribution of human experts. Human concept linkages are guided by previous discoveries and their discoverers (Fig. 5a). In order to build human-aware AI that propose concept linkages unlikely to be imagined by scientists, we invert a measurement of human cognitive accessibility using shortest-path distances (SPD) between pairs of conceptual nodes interlinked by authors in our mixed hypergraph. To rule out candidate hypotheses that lack scientific promise, we couple cognitive unavailability with a signal of scientific plausibility. This signal could be provided by the content of the published research literature and quantified with unsupervised knowledge embedding models ${ }^{28}$. Alternatively, a signal of scientific plausibility could be derived from theory-driven models of material properties. Here we use unsupervised knowledge embeddings for our algorithm, reserving theory-driven property simulations to evaluate the value and human complementarity of our predictions. Specifically, we forecast the scientific merit of any given hypothesis using the cosine similarity between embedding vectors of material and property nodes involved in that hypothesis ${ }^{28}$.</p>
<p>Figure 5 b provides a general overview of our algorithmic approach to identify discoveries that are both scientifically plausible and human inaccessible or complementary. Initialized with a pool of</p>
<p>candidate materials extracted from literature, we compute human accessibility and scientific plausibility signals in an integrated fashion building on our prior analysis for generating human-like predictions. We use our unsupervised word embedding model over prior publications, measuring scientific relevance as cosine distance within the embedding. In parallel, we measure human accessibility by computing shortest-path distances between the property and all materials across the hypergraph. We transform signals of plausibility and human accessibility into a unified scale and linearly combine them with a mixing coefficient $\beta$, which captures human complementarity (see details in Methods and Supplementary Information). With its expert awareness, we designed our algorithm to symmetrically generate either the most or least-human accessible hypotheses-those likely to compete versus complement collective human capacity-based on the sign of the mixing coefficient. Negative $\beta$ values encourage high human accessibility leading to predictions that mimic human experts in discovery. Positive values discourage human accessibility by producing hypotheses least similar to those human experts could plausibly infer, straddling socially disconnected but scientifically linked fields. At extremes, $\beta=-1$ and 1 yield algorithms that generate predictions very familiar or very alien to human experts, regardless of scientific merit. On the other hand, setting $\beta=0$ (midrange) implies exclusive emphasis on scientific plausibility, blind to the distribution of experts. This mode is equivalent to traditional discovery prediction methods exclusively based on previously published content. Intermediate positive $\beta \mathrm{s}$ balance exploitation of relevant materials with exploration of areas unlikely considered or connected by human experts. Each $\beta$ value leads to a different model assigning a scalar score per material, which we use to sort candidate hypotheses. Materials with the highest resulting scores are reported as the algorithm's predictions corresponding to that specific $\beta$.</p>
<p>We evaluate our expert-avoiding algorithm with the same framework as before, i.e., building our model using literature prior to a prediction year and evaluating inferred hypotheses based on subsequent actual discoveries. In this section, we expand the drug repurposing cases (properties) to include treatment of 400 human diseases. We use the prediction year of 2001 for all properties except for COVID-19, for which we set the prediction year to 2020. Complementarity of these inferences are evaluated against human scientific knowledge by verifying (1) their distinctness from contemporary investigations and (2) their scientific promise. We anticipate that both features will simultaneously increase in ranges of $\beta$ higher than those that characterize published science. Moreover, scientific merit will naturally reduce at the extremes of our interval $[-1,1]$, where the algorithm ignores the literature-based plausibility of candidate hypotheses. We expect to observe much higher plausibility in the intermediate ranges, which lead to strong complementarity for positive $\beta$ values.</p>
<h1>Evaluating Discovered Predictions</h1>
<p>Our human-aware model is designed to allow us to dial up and down the degree to which predictions are similar to near-future human discoveries. As we increase $\beta$, the algorithm avoids human accessible inferences that lie within regions of high expert density and focuses on candidate materials and properties that span disciplinary divides and evade human attention. As a result, we expect that generated hypotheses with large $\beta$ will (1) diverge from those pursued by the scientific community, (2) less likely become published, (3) if published, be discovered further into the future, after science has reorganized itself to consider them, and (4) manifest strong scientific performance as scientists conservatively crowd around areas of prior success. In order to verify these hypotheses, we first assess the discoverability of materials by computing the precision between our inferences and published discoveries. Results strongly confirm our expectation that materials inferred at higher $\beta$ values are less discoverable by human scientists (Extended Data Fig. 7).</p>
<p>Moreover, materials distant from a given property in the hypergraph are expected to remain cognitively inaccessible to scientists in the property's proximity for longer (Fig. 5c). It takes more time for researchers in the field to broach knowledge gaps separating unfamiliar materials from valued properties. Among the inferences eventually discovered, we measure the discovery waiting time and expect to observe an increasing trend in wait times as we move from negative (human-competitive) to positive (human-complementary) $\beta$ values in our predictions. Generating 50 hypotheses per $\beta$ value and evaluating the resulting predictions indicates that for the majority of targeted properties, average discovery wait times climb markedly when increasing $\beta$ (Fig. 6) for energy-related chemical properties (Fig. 6a-6c), COVID-19 prevention (Fig. 6d) and treatment for $70 \%$ of other human diseases (Fig. 6e). Averaging wait times across all human diseases manifests a clear increasing trend. For some cases, such as COVID-19 (Fig. 6d), none of the complementary predictions made with positive $\beta$ values (larger than 0.4 ) come to be discovered by humans within the time frame we examine.</p>
<h1>Evaluating Undiscovered Predictions</h1>
<p>To evaluate the scientific merit of our algorithm's predictions, including those that remain undiscovered within the study period, we require data beyond the extant literature. Such hypotheses necessarily grow to comprise the vast majority of cases for large values of $\beta$. If science was an efficient market and experts optimally pursued scientific quality, then in human-avoiding high $\beta$ hypotheses, we would observe a proportional decline in scientific promise and efficacy. On the other hand, if scientists crowd together along the frontier of scientific possibility and their continued efforts yield diminishing marginal returns, we might observe an increase in promise as we move beyond them.</p>
<p>To evaluate the merit of undiscovered scientific inferences, we utilize first principles or data-driven models derived uniquely for each property based on well-established theoretical principles within the field. Similar to our algorithms, such models also assign real-valued scores to candidate materials as a measure of their potential for possessing the targeted properties. These computations may be carried out without regard for whether materials have yet been discovered, making them a suitable scoring function for evaluating undiscovered hypotheses. We produce such scores for approximately $45 \%$ of the properties we considered above using models based on first-principles understandings of the phenomenon or models based on databases curated with high-throughput protein screens. To evaluate thermoelectric promise, we used power factor (PF) as an important component of the overall thermoelectric figure of merit, $z T$, calculated using density functional theory for candidate materials as a strong indication of thermoelectricity ${ }^{29,30}$. To evaluate ferroelectricity, estimates of spontaneous polarization obtained through symmetry analysis and relevant theoretical equations serve as a reliable metric ${ }^{31}$. For human diseases including COVID-19, proximity between disease agents (e.g., SARS-CoV-2) and candidate compounds in protein-protein interaction networks suggests the likelihood a material will recognize and engage with the disease agent ${ }^{32}$ (for more details on how these theoretical scores are derived see the Supplementary Information). We note that scores based on first-principles equations or simulations represent conservative estimates of scientific merit as they are based on widely-accepted, scientist-crafted and theory-inspired models. Because these scores are potentially available to scientists in the area, they may be considered when guiding investigations such that experiments on these unevaluated hypotheses often lead to promising results. Nevertheless, in what follows we show that modestly positive $\beta$ values manifest an marked improvement even on this conservative measure of quality.</p>
<p>We expect the average theoretical scores of hypotheses to decay significantly at the extremes of the $\beta$ range $[-1,1]$, as at those points the algorithm ignores the merit signal putting it at higher risk of generating scientifically irrelevant (or absurd) proposals. We expect, however, that this decay will</p>
<p>occur more slowly than the decrease in hypothesis discovery and publication, which implies the existence of a $\beta$ interval where proposals are not discoverable but highly promising-an ideal operating region for the generation of hypotheses that complement those from the human scientific crowd. In order to verify this, we contrasted changes in average theoretical scores with the discoverability of generated hypotheses for various $\beta$ values. As illustrated in Fig. 7 (first row), discoverability decreases near the transition of $\beta$ from negative to positive values, but its decay is much sharper than average theoretical scores, which do not collapse until nearly $\beta=0.4$. This holds for electrochemical properties and the majority of diseases. Results for certain individual diseases can be seen in the second row of Fig. 7 (for the full set of results see Extended Data Fig. 8 and Supplementary Table 1). Moreover, note that for the cases investigated, average theoretical scores for inferred hypotheses grow higher than average theoretical scores for actual, published discoveries (the dashed lines) before their eventual decay at high $\beta$ values. For certain properties like thermoelectricity or therapeutic efficacy against the disease Alopecia, theoretical merit of our inferences exhibit striking and dramatic growth from negative (scientist-mimicking) to positive (scientist-avoiding) hypotheses, suggesting strong diminishing returns to following these scientific crowds, whose overharvested fields have become barren for new discovery.</p>
<p>In order to further compare the decay rate of discoverability and theoretical scores, we define and compute an expectation gap to measure the distance between expected values for two conditional distributions over $\beta$. These two conditionals are defined as two likelihoods over $\beta$ given that a randomly selected prediction with that $\beta$ is (1) identified as promising based on its corresponding first-principle score, and (2) discoverable, i.e., studied and published by a scientist following prediction year (for details see Methods and Supplementary Information). A positive expectation gap indicates that increasing $\beta$ will preserve the quality of predictions while making them more complementary to human hypotheses. As shown in Fig. 8a, the vast majority of properties considered in this section yield substantial and significantly positive expectation gaps. Building on this, we use a probabilistic model to assess the complementarity of our algorithm's prediction with those of the scientific community for any value of $\beta$. This is done by explicitly computing the joint probability that a randomly selected prediction is plausible in terms of the desired property and beyond current scientists' scope of research (see Supplementary Information). These probabilities specify the optimal $\beta$ to balance exploitation and exploration in augmenting collective human prediction. Results in Fig. 8b indicates the optimal point varies for different properties, but one can distinguish the range $0.2-0.3$ as the most consistently promising interval. In this interval, hypotheses are very unlikely to come from the scientific community, but are very likely to yield successful scientific results.</p>
<h1>Discussion</h1>
<p>We demonstrate the power of incorporating human-awareness into artificial intelligence systems for accelerating future discovery. Our models succeed by directly predicting human discoveries and the human experts who will make them, yielding up to $400 \%$ improvement in prediction precision. These findings offer support for the influence of the human experience and social connection inscribed by our research hypergraph in driving scientific advance. This suggests that the search underlying materials and medical advance is dominated by local exploitation of the familiar over novel exploration of the unknown. Moreover, by tuning our algorithm to avoid the crowd, we generate promising hypotheses unlikely to be imagined, pursued or published without machine recommendation for years into the future. By identifying and correcting for collective patterns of human attention, formed by field boundaries and institutionalized education, these models complement the contemporary scientific community. This demonstrates that connectivities in our expert-aware hypergraph are useful not only for predicting and accelerating human discoveries in</p>
<p>the near future, but also for inferring disruptive discoveries that could be imagined by scientists only in distant future.</p>
<p>Our analysis examined a limited space of scientific relationships-those between a material possessing a valuable energy or therapeutic property. Many other scientifically meaningful relationships lie beyond this syntax, such as identity (i.e., $a$ is a $b$ ), composition (i.e., $a$ is a part of $b$ ), or any specific physical or logical relationship (e.g., $a$ chemically reacts with $b ; a$ genetically up-regulates $b$ ). Using a hypergraph formalism, we could extend such relations beyond logical triples that connect a simple concept pair to larger sets of concepts connected by more complex relations. Another limitation involved our singular consideration of co-authorship as the relationship affecting the distribution of expertise. One could consider other relationships, such as scientist collocation within an institution, at a conference they attend, or geographical proximity. Moreover, there are opportunities to technically improve our approach, such as combining content and human-aware information to amplify prediction accuracy, or inferring and exploiting the body of negative knowledge in science where researchers know that certain scientific claims are false ${ }^{11,21}$.</p>
<p>Despite these limitations, our investigation underscores the power of incorporating human and social factors to produce artificial intelligence that complements rather than substitutes for human expertise. Successful scientists competitively factor and follow the momentum of advances made by researchers around them in identifying the frontiers of science. By making AI hypothesis generation aware of human expertise, it can accelerate discovery and liberate human scientists to steer science and technology in novel directions. Our system and its recommendations raise ethical concerns; they could be used as a "scoop-machine" to leapfrog human scientists and seize upon answers that they might otherwise ask and answer next. This would accelerate science, but could augment some scientists' capacity at the expense of others. Such a concern would attenuate when scientific recommendation engines became ubiquitous, however, like recommendations for internet and social media search. Moreover, we demonstrate how awareness of human scientific expertise could be used not only to mimic but avoid it, generating insights that punctuate the current flow of discovery ${ }^{22}$.</p>
<p>Our investigation also reveals the influence of human scientific institutions that crowd scientists along a shared frontier of likely discoveries. The success of our 'alien' or complementary hypotheses suggests that scientific departments and disciplines limit productive exploration and point to opportunities that could improve human prediction by reformulating science education for discovery. Insofar as research experiences and relationships condition the questions scientists investigate, education tuned to discovery might conceive of each student as a new experiment, recombining knowledge and opportunity in novel ways. Our analysis underscores the power of incorporating human and social factors to produce artificial intelligence that complements rather than substitutes for human expertise. In accounting for not just human expertise, but the complete distribution of scientific experience and exposure, such systems can be designed to race with rather than against the scientific community, expanding the scope of human imagination and discovery.</p>
<h1>Materials and Methods</h1>
<h2>Experiments and Data Collection</h2>
<p>Each discovery prediction experiment consists of a target property and a pool of materials, where the materials are scored by a predictor and the 50 materials with highest scores are selected as predictions. Each predictor scores an individual material through computing its similarity with the property. Similarity metrics for our hypergraph-based predictors are the transition probability between material and property nodes with one and two intermediate author nodes (hence two- and three-step transitions, i.e. $s=2$ and $s=3$ ), and cosine similarity in the deepwalk embedding space. The former can be calculated through Bayes' rule without the need for generating random walks, but the latter require an explicit set of random walk sequences over our hypergraph. The similarity metric from the replicated content-only baseline is the cosine similarity in the embedding space of a Word2Vec model trained on the corpus of publications produced before prediction year. The corpus of publications and ground-truth discoveries are prepared differently for each set of property and potential materials.</p>
<p>Our testbed consisted of two datasets: for the energy-related properties we used a collection of $\sim 1.5 \mathrm{M}$ articles published between 1937 and 2018 classified by Tshitoyan et. al (2019) as related to inorganic materials ${ }^{15}$, and for the diseases we utilized MEDLINE database that includes more than 28 M articles published in various biomedical fields over the span of more than two centuries. Creating our hypergraph required us to have access to disambiguated authors for all articles. We downloaded the database related to inorganic materials using Scopus API provided by Elsevier (https://dev.elsevier.com/), which readily assigns unique codes to distinct authors. In order to author-disambiguate the MEDLINE database, we used disambiguation results provided by the PubMed Knowledge Graph (PKG) ${ }^{24}$, which were obtained by combining information from the Author-ity disambiguation of PubMed ${ }^{25}$ and the more recent semantic scholar database ${ }^{26}$. This integrative method has a performance comparable to each of its individual components: $98.09 \%$ F1-score, $98.62 \%$ precision and $97.56 \%$ recall. For this dataset, we restricted our experiments to 27.5 M papers with available abstracts, metadata (publication year) and disambiguated authors.</p>
<p>For energy-related properties, we extracted the pool of chemical compounds from the collected 1.5 M articles using Python Materials Genomics ${ }^{27}$ and direct rule-based string processing. Material-property association was defined in terms of co-occurrence of materials with property-related keywords. First-time co-occurrences were considered ground-truth discoveries, following the replicated prior work ${ }^{15}$. For the case of drug repurposing, we began with a pool of 7,800 approved candidate drugs downloaded from the DrugBank database. We then built our drug pool using approximately 4,000 drugs possessing simple names (i.e., dropping complex names containing several numerical components). We chose 100 (or 400, when avoiding experts) diseases from the Comparative Toxicogenomics Database (CTD) ${ }^{17}$ with the largest number of relevant drugs from our drug pool. In order to build our hypergraph, we searched for names of drugs and diseases in MEDLINE to detect their occurrence within papers. Ground-truth relevant drugs for the selected diseases (except COVID-19) were extracted from associations curated by CTD. The discovery date for each of the disease-drug associations was set to the earliest publication reported by CTD for curated relevance. We ran separate prediction experiments for each individual disease, where we define the property as drug efficacy in treating or preventing the selected disease. The same pool of drugs and corpus of articles were used for the case of COVID-19, where the ground-truth relevance of drugs to COVID-19 were identified based on their involvement in COVID-related studies reported by ClinicalTrials.org in or after 2020, regardless of the studies' results, following the compared work by Gysi et al. ${ }^{18}$. Date of discovery for each relevance was set to the date the corresponding study was first posted, and if the drug was involved in multiple trials we considered</p>
<p>the earliest. There have been 6,280 trials posted as of August 5th, 2021 (ignoring 37 trials dated before 2020), which included 279 drugs from our pool ( $\sim 7 \%$ ) included in their study designs.</p>
<h1>Hypergraph Random Walks</h1>
<p>In practice, research and coauthoring that occurred long before the time of prediction are unlikely to be cognitively available, socially accessible, or perceived as of continuing relevance. Therefore, we restrict our prediction experiments to use literature produced in the most recent 5 years prior to the year of prediction. For alternative time windows, the magnitude of precision curves slightly shifted, but their trend and ordering remained the same (see Supplementary Fig. 4). For each property, we took 250,000 non-lazy, truncated random walks with and without $\alpha$-modified sampling distribution sequences. All walks start from the property node and end either after 20 steps or after reaching a dead-end node with no further connections. The $\alpha$-modified sampling algorithm is implemented as a mixture of two uniform distributions over authors and materials such that the mixing coefficient assigned to the latter is $\alpha$ times the coefficient of the former. Hence, $\alpha$ is the ratio of probability for selecting a material to the probability of selecting an author node (see Supplementary Information for more details). We tried three values for this parameter in our experiments: $\alpha=1$, which implies an equal probability of sampling authors and materials, $\alpha \rightarrow \infty$ which only samples materials and $\alpha$ $=0$ which only samples authors. The author-only mode yielded much weaker performance in comparison to the other two and we do not include it in our results. This implies that mere networking with other human experts, without reading and researching the literature does not typically lead to discoveries in practice. A further perturbation analysis of $\alpha$ showed that increasing it to values larger than one (e.g., 10) is less harmful to precision levels than decreasing it below one (e.g., 0.5). In other words, the balance point leads to highest performance (i.e., $\alpha=1$ ), but if one break balance between researching (e.g., Googling and reading related research papers) and networking with nearby scientists, overemphasizing research exploration harms prediction less than overemphasizing social networking in predictions of knowledge discovery (see Supplementary Fig. 5 for a more thorough sensitivity analysis of our algorithm with regard to parameter $\alpha$ ).</p>
<h2>Relevance Metrics</h2>
<p>Once the random walk sequences are drawn, we can compute our two hypergraph-induced similarities. Multi-step transition probabilities are directly computed from transition matrices using Bayesian rules and Markovian assumptions (see Supplementary Information). Calculating probabilities for two- and three-step transitions from properties to materials requires us to sum the probability of all meta-paths with the form PAM and PAAM, where P, A and M stand for property, author and material nodes, respectively ${ }^{28}$. Alternatively, the meta-path that we considered for discoverer prediction was PMA. For our deepwalk representation, we trained a skipgram Word2Vec model with hyperparameter settings similar to the content-only prior work we replicated ${ }^{15}$, including an embedding dimensionality set to 200 . One exception is the number of epochs, which we reduced from 30 to 5 . The size of vocabulary produced by deepwalk sampling is substantially smaller than the number of distinct words from literature. As a result, deepwalk training required less effort and lower iterations to capture the underlying inter-node relationships. Also note that deepwalk embedding similarity is more global than the transition probability metric, provided that the length of our walks (set to 20) are longer than the number of transition steps ( 2 or 3 ). Moreover, it is more flexible since the walker's edge selection probability distribution can be easily modified to explore the network structure more deeply ${ }^{29}$. Nevertheless, because the deepwalk Word2Vec is trained using a window of only length 8 , only authors and materials that might find each other through conversation, seminars or conferences would be near one another in the resulting vector space.</p>
<p>We also ran our prediction experiments after replacing the deepwalk representation with a graph convolutional neural network. We used the Graph Sample and Aggregate (GraphSAGE) model ${ }^{30}$ with 400 and 200 as the dimensionality of hidden and output layers, respectively, with Rectified Linear Units (ReLU) as non-linear activations in the network. Convolutional models require feature vectors for all nodes but our hypergraph is inherently feature-less. Therefore, we utilized the word embeddings obtained by our Word2Vec baseline as feature vectors for materials and property nodes. A graph auto-encoder was then built using the GraphSAGE architecture as the encoder and an inner-product decoder and its parameters were tuned by minimizing the unsupervised link-prediction loss function ${ }^{31}$. We took the output of the encoder as the embedded vectors and selected the top 50 discovery candidates by choosing entities with the highest cosine similarities to the desired property. In order to evaluate the importance of the distribution of experts for our prediction power, we trained this model on our full hypergraph and also after withdrawing author nodes (see Supplementary Information). Running the convolutional model on energy-related materials and properties yielded $62 \%, 58 \%$ and $74 \%$ precisions on the full graph, and $48 \%, 50 \%$ and $58 \%$ on the author-less graph for thermoelectricity, ferroelectricity and photovoltaics, respectively. These results show a pattern similar to those obtained through deepwalk model although with somewhat smaller margin due to the use of Word2Vec-based feature vectors, which limited the domain of exploration by the resulting embedding model to within proximity of the baseline.</p>
<h1>Complementary Hypotheses Generation</h1>
<p>Our predictor consists of two scoring functions. The first measures the human inaccessibility (i.e., alienness) of candidate materials via Shortest-Path distance (SPD) between the nodes corresponding to the targeted property and candidates. The second measures scientific plausibility through the semantic cosine similarities of their corresponding keywords. For this purpose, we use our Word2Vec baseline pretrained over the literature (collected on inorganic materials for energy-related properties and MEDLINE for the diseases) produced prior to the prediction year. We combine the alienness and plausibility scores with a mixing coefficient, denoted by $\beta$, adjusting their contributions to obtain a final score for the candidate. The plausibility component yields continuous scores distributed close to Gaussian, whereas the alienness component offers unbounded ordinal SPD values. Simple normalization methods are insufficient to combine scores with such distinct characteristics. As a result, we first standardize the two scores to a unified scale by applying the Van der Waerden transformation ${ }^{41}$, followed by a Z-score normalization. The final step includes taking the weighted average of the resulting Z-scores with weights depending on $\beta$ (see Supplementary Information for more details).</p>
<p>We want our predictor to infer undiscoverable yet promising hypotheses. Setting $\beta$ to a more positive value makes predictions less familiar and more alien, i.e., less discoverable. Moreover, increasing $\beta$ to the positive extreme (i.e., +1 ) excludes scientific merit from the algorithm's objective in materials selection. Hence, growing $\beta$ causes both discoverability and plausibility of predictions to decay. What matters to us is that plausibility decreases more slowly than discoverability, suggesting that the predictor achieves a close-to-ideal state where predictions are simultaneously alien and promising. In order to verify this with a single number, we define the expectation gap criterion, computed as the difference between expected values of the following two distributions over $\beta: \mathbb{P}(\beta \mid$ plausible $)$ and $\mathbb{P}(\beta \mid$ discoverable $)$. The terms "plausible" and "discoverable" on the conditional sides could be substituted by the precise statements "a randomly selected inferred hypothesis is theoretically plausible" and "a randomly selected inferred hypothesis is discoverable"-it will be published by scientists, respectively. While we know both of these distributions reduce as $\beta$ approaches +1 , the expectation gap measures any positive shift in the mass of $\mathbb{P}(\beta \mid$ plausible $)$ against $\mathbb{P}(\beta \mid$ discoverable $)$. The likelihood of discovery $\mathbb{P}(\beta \mid$ discoverable $)$ can be</p>
<p>estimated through an empirical distribution of predictions discovered and published. Scientific plausibility can be estimated by leveraging properties' theoretical scores obtained from prior knowledge and first-principles equations and data from relevant fields. We estimate $\mathbb{P}\left(\beta \approx \beta_{0} \mid\right.$ plausible) in two steps: (1) converting theoretical scores to probabilities, and (2) computing weighted maximum likelihood estimates of $\mathbb{P}\left(\beta \approx \beta_{0}\right.$ )plausible) given a set of predictions generated by our algorithm operated with $\beta_{0}$ (see Supplementary Information for details). We restrict experiments in this section to only those properties for which we could obtain a reliable source of theoretical scores (see Supplementary Information for details of the scores): thermoelectricity, ferroelectricity, COVID-19 and 175 other human diseases ( 178 out of 404 total properties).</p>
<p>Finally, note that expectation gaps and average discovery dates (described above) say nothing about the $\beta$ interval most likely to lead to better complementarity. We introduce an additional probabilistic criterion for this purpose, which explicitly and jointly models these two features and computes their likelihood for various $\beta$ values, $\mathbb{P}$ (undiscoverable, plausible $\mid \beta$ ). One can use this distribution to screen the best operating point for complementary artificial intelligence (see Supplementary Information).</p>
<h1>Data Availability</h1>
<p>DOIs of papers used for the electrochemical properties together with the PubMed identifiers of the MEDLINE entries used in our experiments can be found in our GitHub repository: https://github.com/jsourati/accelerate-discoveries.</p>
<p>Abstracts of papers for electrochemical properties could not be shared due to copyright issues. But MEDLINE abstracts are accessible through their identifiers from the PubMed website.</p>
<h2>Code Availability</h2>
<p>All codes for our algorithms can be found in the following GitHub repository: https://github.com/jsourati/accelerate-discoveries.</p>
<h2>Competing Interests</h2>
<p>The authors declare no competing interests.</p>
<h2>Author Contributions</h2>
<p>JS: Conceptualization, Methodology, Software, Validation, Investigation, Writing - Original Draft, Visualization; JE: Conceptualization, Methodology, Writing - Original Draft, Visualization, Funding acquisition.</p>
<h2>References</h2>
<ol>
<li>Khadherbhi, S. R. \&amp; Babu, K. S. Big Data Search Space Reduction Based On User Perspective Using Map Reduce. International Journal of Advanced Technology and Innovative Research 7, $3642-3647$ (2015).</li>
<li>
<p>Sanchez-Lengeling, B. \&amp; Aspuru-Guzik, A. Inverse molecular design using machine learning: Generative models for matter engineering. Science 361, 360-365 (2018).</p>
</li>
<li>
<p>Smalley, E. AI-powered drug discovery captures pharma interest. Nat. Biotechnol. 35, $604-605$ (2017).</p>
</li>
<li>Teruya, E., Takeuchi, T., Morita, H., Hayashi, T. \&amp; Ono, K. ARTS: autonomous research topic selection system using word embeddings and network analysis. Mach. Learn.: Sci. Technol. 3, 025005 (2022).</li>
<li>Shi, F., Foster, J. G. \&amp; Evans, J. A. Weaving the fabric of science: Dynamic network models of science's unfolding structure. Soc. Networks 43, 73-85 (2015).</li>
<li>Singer, U., Radinsky, K. \&amp; Horvitz, E. On Biases Of Attention In Scientific Discovery. Bioinformatics (2020) doi:10.1093/bioinformatics/btaa1036.</li>
<li>Tversky, A. \&amp; Kahneman, D. Availability: A heuristic for judging frequency and probability. Cogn. Psychol. 5, 207-232 (1973).</li>
<li>Evans, J. S. B. T. Bias in human reasoning: Causes and consequences. Essays in cognitive psychology. 145, (1989).</li>
<li>Ehrlinger \&amp; Readinger. Decision-making and cognitive biases. Encyclopedia of mental (2016).</li>
<li>Chadwick, A. T. \&amp; Segall, M. D. Overcoming psychological barriers to good discovery decisions. Drug Discov. Today 15, 561-569 (2010).</li>
<li>Rzhetsky, A., Foster, J. G., Foster, I. T. \&amp; Evans, J. A. Choosing experiments to accelerate collective discovery. Proc. Natl. Acad. Sci. U. S. A. 112, 14569-14574 (2015).</li>
<li>Mikolov, T., Yih, W.-T. \&amp; Zweig, G. Linguistic Regularities in Continuous Space Word Representations. in Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies 746-751 (Association for Computational Linguistics, 2013).</li>
<li>
<p>Perozzi, B., Al-Rfou, R. \&amp; Skiena, S. DeepWalk: online learning of social representations. in Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining 701-710 (Association for Computing Machinery, 2014).</p>
</li>
<li>
<p>Chitra, U. \&amp; Raphael, B. Random Walks on Hypergraphs with Edge-Dependent Vertex Weights. in Proceedings of the 36th International Conference on Machine Learning (eds. Chaudhuri, K. \&amp; Salakhutdinov, R.) vol. 97 1172-1181 (PMLR, 2019).</p>
</li>
<li>Tshitoyan, V. et al. Unsupervised word embeddings capture latent knowledge from materials science literature. Nature 571, 95-98 (2019).</li>
<li>Burger, B. et al. A mobile robotic chemist. Nature 583, 237-241 (2020).</li>
<li>Davis, A. P. et al. The Comparative Toxicogenomics Database: update 2019. Nucleic Acids Res. 47, D948-D954 (2019).</li>
<li>Morselli Gysi, D. et al. Network medicine framework for identifying drug-repurposing opportunities for COVID-19. Proc. Natl. Acad. Sci. U. S. A. 118, (2021).</li>
<li>Ghandehari, S. et al. Progesterone in Addition to Standard of Care Versus Standard of Care Alone in the Treatment of Men Hospitalized with Moderate to Severe COVID-19: A Randomized, Controlled Pilot Trial. Chest (2021) doi:10.1016/j.chest.2021.02.024.</li>
<li>Mauvais-Jarvias, F. Estradiol and Progesterone in Hospitalized COVID-19 Patients. Identifier NCT04865029 https://clinicaltrials.gov/ct2/show/NCT04865029 (2021).</li>
<li>Belikov, A. V., Rzhetsky, A. \&amp; Evans, J. Prediction of robust scientific facts from literature. Nature Machine Intelligence 1-10 (2022).</li>
<li>Sourati, J. \&amp; Evans, J. Complementary artificial intelligence designed to augment human discovery. arXiv [cs.AI] (2022).</li>
<li>Schaffer, R. Study examines projesterone to reduce inflammation in COVID-19. Healio EndocrineToday (2020).</li>
<li>Xu, J. et al. Building a PubMed knowledge graph. Sci Data 7, 205 (2020).</li>
<li>Torvik, V. I. \&amp; Smalheiser, N. R. Author Name Disambiguation in MEDLINE. ACM Trans. Knowl. Discov. Data 3, (2009).</li>
<li>Ammar, W. et al. Construction of the Literature Graph in Semantic Scholar. arXiv [cs.CL]</li>
</ol>
<p>(2018).
27. Ong, S. P. et al. Python Materials Genomics (pymatgen): A robust, open-source python library for materials analysis. Comput. Mater. Sci. 68, 314-319 (2013).
28. Sun, Y., Han, J., Yan, X., Yu, P. S. \&amp; Wu, T. PathSim. Proceedings VLDB Endowment 4, 992-1003 (2011).
29. Grover, A. \&amp; Leskovec, J. node2vec: Scalable Feature Learning for Networks. KDD 2016, $855-864(2016)$.
30. Hamilton, W. L., Ying, R. \&amp; Leskovec, J. Inductive representation learning on large graphs. in</p>
<p>Proceedings of the 31st International Conference on Neural Information Processing Systems 1025-1035 (Curran Associates Inc., 2017).
31. Kipf, T. N. \&amp; Welling, M. Variational Graph Auto-Encoders. Stat 1050, 21 (2016).</p>
<h1>Acknowledgments</h1>
<p>The authors wish to thank our funders for their generous support: National Science Foundation #1829366; Air Force Office of Scientific Research #FA9550-19-1-0354, #FA9550-15-1-0162; DARPA #HR00111820006. The funders had no role in study design, data collection and analysis, decision to publish or preparation of the manuscript. We thank Laszlo Barabasi and Deisy Morselli Gysi for helpful data related to their network-based forecast of COVID-19 drugs and vaccines with protein-protein interactions ${ }^{18}$, and Anubhav Jain, Vahe Tshitoyan and Alex Dunn for sharing data and code to help replicate their work on unsupervised word embeddings and latent knowledge about material science ${ }^{15}$. We also thank participants of the Santa Fe Institute workshop "Foundations of Intelligence in Natural and Artificial Systems", the University of Wisconsin at Madison's HAMLET workshop, and colleagues at the Knowledge Lab for helpful comments.</p>
<p>Fig. 1. Motivation and design of our approach to simulate human accessible scientific inferences (a) Three scenarios where a hidden underlying relationship between material M and property P waits to be discovered. Uncolored circles represent non-overlapping populations of human experts, with colored nodes indicating materials (blue) or properties (colored). Background colors represent overlapping disciplinary communities, within which scientists and topics are embedded. Solid lines between uncolored and colored nodes imply that experts studied or have experience with the material or property. Dashed lines represent property-material links that exist but have not yet been discovered by human scientists, and gray arrows represent new hypotheses proposed by our algorithm. The P-M relation in the upper left scenario is likely to be discovered and published in the near future and is proposed by our algorithm; the P-M relation in the upper right is likely to escape scientists' attention, and also the notice of our algorithm that simulates human accessible hypotheses. Nevertheless, our algorithm also captures transitive inference as scientists do through research and conversation over time; let $\mathrm{P}-\mathrm{M}<em 2="2">{1}-\mathrm{M}</em>}-\mathrm{M<em _mathrm_i="\mathrm{i">{3}$ be a chain of materials connected to property P , and every consecutive pair $\mathrm{M}</em>}}-\mathrm{M<em 3="3">{\mathrm{i}+1}$ are strongly connected either because they are already shown to be connected in published articles or because there is a group of researchers familiar with both, having studied both across their opus of research. Our algorithm walks over consecutive pairs and infers the existence of $\mathrm{P}-\mathrm{M}</em>$ relationship and its likelihood of discovery in future. (b) Four examples of random walk paths starting from "Coronavirus" (property) and ending at "Progesterone" (a chemical under clinical trial investigation for COVID-19 therapeutic efficacy). Each arrow connecting two nodes indicates a sampling step, where the paper shown on top of the receiving node comprises the selected hyperedge for that step, which by construction contains both nodes sampled in the prior and current steps. (c) Illustration of our hypergraph deepwalk algorithm: 1) We construct a hypothetical hypergraph based on literature represented by three papers. Uncolored shapes represent authors and colored shapes indicate properties (red) or materials (blue) mentioned in article titles or abstracts. 2) We perform classic or $\alpha$-modified random walk sampling, which 3) result in a set of sequences consisting of authors, materials and the focal property. 4) We remove authors from sequences, retaining only the materials on which discovery inference will be applied. 5) We train a word embedding model (e.g., Word2Vec) on these sampled human accessible sequences of material/property tokens, which results in 6) a vector representation of property and materials we use to compute similarities for prediction.</p>
<p>Fig. 2. Evaluating human-accessible discovery predictions against various baselines. Precision rates for human accessible discovery predictions regarding materials associated with different properties and prediction years: (a-c) chemical compounds and electrochemical properties including thermoelectricity, ferroelectricity, and photovoltaic capacity, respectively, with prediction year 2001; (d) therapeutics and vaccines for COVID-19 in prediction year 2020; (e) general disease-drug associations for prediction year 2001. Precisions reported for general disease-drug associations are individual rates computed 19 years after prediction year, but computed annually for electrochemical properties and monthly for COVID-19 efficacy (See Extended Data Fig. 1c). Gray bars in Figs. a-d indicate the number of actual new discoveries each month or year of the prediction period. The curve labeled "theoretical" in the case of COVID-10 represents predictions generated based on protein-protein interaction network by Gysi et al. ${ }^{18}$. Predictions accounting for the distribution of human experts are far superior to those that ignore it.</p>
<p>Fig. 3. A prediction example of progesterone as COVID-19 therapy. (a) An example random walk from the property node "Coronavirus" to the material node "Progesterone", where selected hyperedges (papers) are shown in detail. Every article in this path is a hyperedge (denoted $e_{i}$ in the $i$-th step) connecting the prior to the subsequent node. The last article was cited by the University of Southern California clinical trial that investigated the effectiveness of progesterone for COVID-19 treatment. Relevant MeSH Terms from articles are shown to demonstrate their scope, indicating hints regarding the reasoning of human scientists championing the treatment. The path indicates a clear transition from Coronavirus-related topics to male-female differences in pathological conditions and lastly to progesterone-based therapy. Similar bridges between topics were highlighted by the trial's investigator as the main motivation for her study in a published news interview ${ }^{23}$. (b) List of true positive discovery predictions made by our human accessible deepwalk algorithm, which were misclassified by the content-only predictor. Edge colors represent the ratio of rank $<em _deepwalk="{deepwalk" _text="\text">{\text {word } 2 \text { vec }} /$ rank $</em>$, where the numerator denotes the rank of the material in terms of our deepwalk scoring function that simulates the inferences made by human experts, and the denominator indicates the rank based on Word2Vec's scoring function that consider research contents alone. Because we display only}</p>
<p>true positives, expert-deepwalk rank ${ }<em _text="\text" _word2vec="{word2vec">{\text {deepwalk }} \leq 50$ and $\operatorname{rank}</em>}}&gt;50$ for all shown materials. A higher rank ratio reveals a larger disparity in the accuracy of algorithmic assessments. The largest ratio is associated with Ethanol inhalation ( $\operatorname{rank<em _text="\text" _word2vec="{word2vec">{\text {deepwalk }}=15, \operatorname{rank}</em>}}=2,762$ ), widely used in treating pulmonary edema, and the smallest to Sofosbuvir ( $\operatorname{rank<em _text="\text" _word2vec="{word2vec">{\text {deepwalk }}=38$, $\operatorname{rank}</em>=102$ ), an antiviral used to treat hepatitis C .}</p>
<p>Fig. 4. The contribution of human expert awareness for predicting discoveries and discoverers. (a-c) 2D projections of human accessible material predictions made by deepwalk (blue circles) and the content-exclusive Word2Vec model (red circles) for thermoelectricity (left), ferroelectricity (center) and photovoltaic capacity (right). Circles with center dots indicate true positive predictions discovered and published in subsequent years, while empty circles represent false positives. Predictions are plotted atop the density of experts (topological map and contours estimated by Kernel Density Estimation) in a 2D tSNE-projected embedding space. Before applying tSNE dimensionality reduction, the original embedding was obtained by training a Word2Vec model over random walks generated across the hypergraph of published science (similar to our deepwalk procedure shown in Fig. 1 but without removing authors). Red circles are more uniformly distributed, but blue circles concentrate near peaks of expert density. (d) Precision shifts in predictions attributable to the inclusion of authors, defined as the percentage of precision change when switching from $\alpha \rightarrow \infty$ to $\alpha=1$, plotted against the fraction of property-related papers within the literature. Higher density in the literature obviates the need for human author information. (e) Precision rates for predicting discoverers of materials with electrochemical properties. Predictive models are built based on two-step transitions between property and expert nodes with an intermediate material in the transition path. Bars show average precision of human expert predictions for each year following prediction. Note that an expert can publish a discovery in multiple years. Total precision rates are also shown after each property, ignoring the repetition of discovering experts.</p>
<p>Fig. 5. Motivation and design of our approach to generate complementary scientific predictions by avoiding human scientists. (a) Distribution and overlap of experts investigating (and publishing on) topics represented by yellow geometric shapes. Dashed lines represent paths of more or less human cognitive availability between topics ("triangle", "diamond" and "square"). (b) Overview of our complementary discovery prediction algorithm. Beginning with a scientific corpus and a targeted property, candidate materials are extracted from the corpus and used along with property mentions and authors to form the hypergraph. The algorithm follows two branches to compute plausibility from word embedding semantic similarities and human inaccessibility or "alienness" from hypergraph shortest-path distances. These two signals are combined after proper normalization and standardization through the mixing coefficient $\beta$ to generate a prediction more or less complementary to the flow of human discovery (higher $\beta \mathrm{s}$, more human inaccessible and so more complementary; lower $\beta \mathrm{s}$, more human accessible and so more in competition). Candidate materials are sorted based on resulting scores and those with highest rank are reported as proposed discoveries. (c) Discovery wait times for relations between "triangle"-"diamond" and "triangle"-"square". The time one needs to wait for a relationship to be discovered is proportional to the path length of human accessibility between the two relevant topics. The denser presence of human experts around the pair "triangle"-"diamond" implies greater cognitive availability leading to earlier discovery and publication versus "triangle"-"square" where the connection requires a longer path.</p>
<p>Fig. 6. The wait time for published discoveries increases with human inaccessibility (higher $\beta$ values). (a-d) Average annual/monthly discovery wait times are shown as thick gray arcs, where thickness represents the percentage of materials discovered in the corresponding year/month. Each orbit is associated with a particular $\beta$ value with larger (more red) orbits representing larger $\beta$ values and greater human inaccessibility as computed by our algorithm's human expert avoidance. The values we consider here vary between -0.8 (the smallest, bluest orbit) and 0.8 (the largest, reddest orbit). The plot in the upper right quarter of the orbits reveals the total average of discovery wait times including all years/months for each considered $\beta$ value. (e) Average wait times for discoveries across all human diseases (except COVID-19) from our experiments.</p>            </div>
        </div>

    </div>
</body>
</html>
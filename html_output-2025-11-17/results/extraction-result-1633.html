<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1633 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1633</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1633</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-209370644</p>
                <p><strong>Paper Title:</strong> <a href="https://arxiv.org/pdf/1912.06321v1.pdf" target="_blank">Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation</a></p>
                <p><strong>Paper Abstract:</strong> Does progress in simulation translate to progress in robotics? Specifically, if method A outperforms method B in simulation, how likely is the trend to hold in reality on a robot? We examine this question for embodied (PointGoal) navigation, developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity, revealing surprising findings about prior work. First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on a simulated agent and a physical robot. Habitat-to-Locobot transfer with HaPy involves just one line change in config, essentially treating reality as just another simulator! Second, we investigate sim2real predictivity of Habitat-Sim for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify sim2real predictivity. Our analysis reveals several important findings. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), which suggests that performance improvements for this simulator-based challenge would not transfer well to a physical robot. We find that this gap is largely due to AI agents learning to 'cheat' by exploiting simulator imperfections: specifically, the way Habitat allows for 'sliding' along walls on collision. Essentially, the virtual robot is capable of cutting corners, leading to unrealistic shortcuts through non-navigable spaces. Naturally, such exploits do not work in the real world where the robot stops on contact with walls. Our experiments show that it is possible to optimize simulation parameters to enable robots trained in imperfect simulators to generalize learned skills to reality (e.g. improving $SRCC_{Succ}$ from 0.18 to 0.844).</p>
                <p><strong>Cost:</strong> 0.012</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1633.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1633.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>HaPy</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat-PyRobot Bridge (HaPy)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A software bridge that lets identical agent code run unchanged in Habitat-Sim and on a PyRobot-enabled physical LoCoBot by swapping a single simulator config parameter, providing a uniform observation/action API and easing sim2real experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>LoCoBot (via PyRobot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>LoCoBot is a low-cost mobile robot platform used here with an Intel D435 RGB-D camera and a Hokuyo LIDAR for localization (Hector SLAM); it executes discrete navigation actions (turn-left/right, forward, STOP).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied visual navigation / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat-Sim (via Habitat API)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>A high-performance photorealistic 3D simulator for embodied AI that imports reconstructed 3D meshes (Matterport/Matterport Pro2 scans) and provides sensors (RGB, depth, GPS+Compass) and an action API for navigation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>photorealistic rendering with configurable physics approximations and simple actuation-noise modeling (not full high-fidelity contact dynamics)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>visual rendering (photorealistic scenes via 3D scans), approximate actuation noise (Gaussian displacement/heading noise fit from mocap data), camera intrinsics/FOV, depth clipping/range</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>contact/collision behavior (default 'sliding' on collision rather than stopping), contact dynamics and bump sensors, some actuation noise mismatches (simple Gaussian model may not match platform/environment), localization treated as ideal GPS+Compass in sim unless noise modeled</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Controlled indoor lab (6.5m x 10m) with configurable cardboard obstacles in three difficulty configs (easy/medium/hard); virtualized via Matterport Pro2 3D scans; LoCoBot equipped with D435 and Hokuyo LIDAR used with Hector SLAM for accurate localization (~7 cm error).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>PointGoal navigation (spawn-to-goal navigation episodes; stop within 0.2 m of goal), i.e., learned visual navigation policies transferred from sim to robot.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (DD-PPO) trained in simulation on Gibson dataset; multiple agents trained (RGB, Depth, RGB→PredictedDepth) for 500M steps on distributed GPUs.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Episode success rate (STOP within 0.2 m) and SPL (Success-weighted Path Length), plus Sim2Real Correlation Coefficient (SRCC) between sim and reality performance.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Many agents achieved very high success rates in simulation (near 1.0 for success metric under Habitat-Challenge default settings); initial SRCC values: SPL SRCC = 0.603, Succ SRCC = 0.18 (simulation over-optimistic).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Reality performance showed large dynamic range and lower success rates than sim; after simulator optimization (sliding off, actuation-noise multiplier=0) SRCC improved: SPL SRCC -> 0.875, Succ SRCC -> 0.844. (Per-agent absolute SPL/success numbers vary by model and are reported in paper tables.)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>key factors: simulator 'sliding' collision behavior (agents exploit sliding to take shortcuts), mismatch in actuation/noise model, idealized GPS+Compass in sim vs real localization/noise, discrete action-stop behaviors, simplified contact dynamics and bump-sensor effects, localization and sensor noise differences.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Enabling factors: use of HaPy to run identical code, virtualizing the physical lab via Matterport scans to control visual domain gap, matching agent physical parameters (size, camera FOV, sensor preprocessing), adopting LIDAR-based SLAM for low localization error in reality, disabling simulator sliding on collisions, tuning actuation-noise multiplier (found optimal m=0 for this setup), and grid-searching simulator params using SRCC as objective.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Paper identifies that contact behavior (sliding vs stopping) critically affects transfer; accurate actuation noise modeling is important (their simple noise model did not improve SRCC), and that matching embodiment (size, sensor FOV/resolution) and localization fidelity are required; it also documents that agent-level aggregate predictivity is achievable but path-level predictivity is not.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Grid-search over sliding (on/off) and actuation-noise multiplier (0.0–1.0) found best predictivity with sliding=off and actuation-noise multiplier=0.0; initial (Challenge) setting sliding=on, noise=0 gave SPL SRCC=0.603 and Succ SRCC=0.18, optimized setting gave SPL SRCC≈0.875 and Succ SRCC≈0.844; introducing actuation noise in test-sim did not uniformly improve SRCC in this setup.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) Naive simulator settings (Habitat-Challenge defaults) can yield poor sim2real predictivity because learned RL agents exploit simulator imperfections (e.g., sliding) to 'cheat'; 2) A quantitative predictor (SRCC) can be used to optimize simulator parameters to greatly improve sim2real predictivity (SPL SRCC from 0.603→0.875, Succ SRCC from 0.18→0.844); 3) Contact/collision modeling (sliding vs stopping) and accurate actuation/noise modeling are major determinants of transfer success; 4) HaPy enables straightforward execution of identical code in sim and reality, reducing engineering friction for transfer studies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1633.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1633.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Sliding exploit in Habitat-Sim</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Habitat-Sim collision 'sliding' behavior (simulator artifact enabling shortcut exploits)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simulator-level collision behavior where agents slide along obstacles upon collision instead of stopping, causing straight-line SPL computations to undercount true traversed path and enabling learned agents to take unrealistic shortcuts that do not exist on physical robots.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Simulated PointNav agent (cylindrical agent in Habitat-Sim) / learned RL policies</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>An embodied navigation agent modeled as a cylindrical body (configured to match LoCoBot dimensions) with RGB/D sensors and discrete actions; agents trained with RL can exploit simulator collision handling.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>embodied visual navigation / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Habitat-Sim navigation mesh / collision handling</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Navigation mesh-based simulator where collision resolution returns the closest point in navigable space within a fixed radius and by default slides the agent along obstacles to that point instead of stopping on contact.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>approximate physics/contact handling typical of game engines (sliding enabled for smooth human control) — not physically accurate contact dynamics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>navigable-space computation via navigation meshes, photorealistic scene geometry and rendering, simple collision resolution that produces sliding behavior</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>realistic contact dynamics, bump-sensor stop behavior, non-holonomic constraints during contact, physically accurate path traced during collisions (the SPL path uses Euclidean shortcut rather than true traversal), friction/contact impulses</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical lab where the robot stops on contact with walls (bump sensors / physical contact), so sliding exploits from sim do not translate — collision leads to stopping and failure to traverse the simulated shortcut.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>PointGoal navigation policies (agents exploited sliding in sim to improve SPL and success metrics in simulation but failed when deployed to real robot that does not slide).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (DD-PPO) in simulation; policies discovered exploiting sliding behavior during training/testing in sim.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Difference in SRCC (sim-to-real correlation) and direct drop in real-world success/SPL for agents that exploited sliding; SRCC Succ initially 0.18 showing poor predictivity.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td>Some agents achieved artificially high success/SPL in sim due to sliding (success rates close to 1.0 and 'better-than-optimal' simulated path lengths).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td>Corresponding agents perform much worse in reality (large dynamic range, lower success), demonstrating failed transfer for sliding-exploiting policies; numeric SRCC reflects misalignment (Succ SRCC=0.18 before fixing sliding).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Mismatch in collision/contact handling (sliding-enabled navigation mesh vs real stopping-on-contact), evaluation metric using Euclidean distance between pre/post-collision positions, simplistic collision resolution that maps to nearest navigable point causing apparent traversal through obstacles.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Turning sliding off in Habitat-Sim (disabling the sliding behavior) removed the exploit, aligning sim behaviors with real robot contact/stopping and greatly improving sim2real predictivity (SRCC improvements reported).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Contact dynamics and collision-resolution behavior must match reality (agents should not be afforded sliding shortcuts); simulation should model stopping-on-contact or bump-sensor effects for predictive navigation transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Comparing sliding ON vs OFF: sliding ON produced lower SRCC and more rank reversals between sim and reality; sliding OFF significantly improved SRCC (SPL SRCC from 0.603→0.875 and Succ SRCC from 0.18→0.844 when combined with actuation-noise multiplier tuning).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>The simulator's collision handling (sliding) is a dominant cause of sim2real mismatch for learned navigation policies because it permits unrealistic shortcuts that inflate simulated metrics; disabling sliding and tuning actuation-noise improves real-world predictivity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation', 'publication_date_yy_mm': '2019-12'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>CAD2RL: real single-image flight without a single real image <em>(Rating: 2)</em></li>
                <li>Sim-to-real: Learning agile locomotion for quadruped robots <em>(Rating: 2)</em></li>
                <li>Learning agile and dynamic motor skills for legged robots <em>(Rating: 2)</em></li>
                <li>Driving policy transfer via modularity and abstraction <em>(Rating: 2)</em></li>
                <li>Pyrobot: An open-source robotics framework for research and benchmarking <em>(Rating: 2)</em></li>
                <li>Habitat: A Platform for Embodied AI Research <em>(Rating: 2)</em></li>
                <li>MINOS: Multimodal indoor simulator for navigation in complex environments <em>(Rating: 1)</em></li>
                <li>On Evaluation of Embodied Navigation Agents <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1633",
    "paper_id": "paper-209370644",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "HaPy",
            "name_full": "Habitat-PyRobot Bridge (HaPy)",
            "brief_description": "A software bridge that lets identical agent code run unchanged in Habitat-Sim and on a PyRobot-enabled physical LoCoBot by swapping a single simulator config parameter, providing a uniform observation/action API and easing sim2real experiments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "LoCoBot (via PyRobot)",
            "agent_system_description": "LoCoBot is a low-cost mobile robot platform used here with an Intel D435 RGB-D camera and a Hokuyo LIDAR for localization (Hector SLAM); it executes discrete navigation actions (turn-left/right, forward, STOP).",
            "domain": "embodied visual navigation / robotics",
            "virtual_environment_name": "Habitat-Sim (via Habitat API)",
            "virtual_environment_description": "A high-performance photorealistic 3D simulator for embodied AI that imports reconstructed 3D meshes (Matterport/Matterport Pro2 scans) and provides sensors (RGB, depth, GPS+Compass) and an action API for navigation tasks.",
            "simulation_fidelity_level": "photorealistic rendering with configurable physics approximations and simple actuation-noise modeling (not full high-fidelity contact dynamics)",
            "fidelity_aspects_modeled": "visual rendering (photorealistic scenes via 3D scans), approximate actuation noise (Gaussian displacement/heading noise fit from mocap data), camera intrinsics/FOV, depth clipping/range",
            "fidelity_aspects_simplified": "contact/collision behavior (default 'sliding' on collision rather than stopping), contact dynamics and bump sensors, some actuation noise mismatches (simple Gaussian model may not match platform/environment), localization treated as ideal GPS+Compass in sim unless noise modeled",
            "real_environment_description": "Controlled indoor lab (6.5m x 10m) with configurable cardboard obstacles in three difficulty configs (easy/medium/hard); virtualized via Matterport Pro2 3D scans; LoCoBot equipped with D435 and Hokuyo LIDAR used with Hector SLAM for accurate localization (~7 cm error).",
            "task_or_skill_transferred": "PointGoal navigation (spawn-to-goal navigation episodes; stop within 0.2 m of goal), i.e., learned visual navigation policies transferred from sim to robot.",
            "training_method": "Reinforcement learning (DD-PPO) trained in simulation on Gibson dataset; multiple agents trained (RGB, Depth, RGB→PredictedDepth) for 500M steps on distributed GPUs.",
            "transfer_success_metric": "Episode success rate (STOP within 0.2 m) and SPL (Success-weighted Path Length), plus Sim2Real Correlation Coefficient (SRCC) between sim and reality performance.",
            "transfer_performance_sim": "Many agents achieved very high success rates in simulation (near 1.0 for success metric under Habitat-Challenge default settings); initial SRCC values: SPL SRCC = 0.603, Succ SRCC = 0.18 (simulation over-optimistic).",
            "transfer_performance_real": "Reality performance showed large dynamic range and lower success rates than sim; after simulator optimization (sliding off, actuation-noise multiplier=0) SRCC improved: SPL SRCC -&gt; 0.875, Succ SRCC -&gt; 0.844. (Per-agent absolute SPL/success numbers vary by model and are reported in paper tables.)",
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "key factors: simulator 'sliding' collision behavior (agents exploit sliding to take shortcuts), mismatch in actuation/noise model, idealized GPS+Compass in sim vs real localization/noise, discrete action-stop behaviors, simplified contact dynamics and bump-sensor effects, localization and sensor noise differences.",
            "transfer_enabling_conditions": "Enabling factors: use of HaPy to run identical code, virtualizing the physical lab via Matterport scans to control visual domain gap, matching agent physical parameters (size, camera FOV, sensor preprocessing), adopting LIDAR-based SLAM for low localization error in reality, disabling simulator sliding on collisions, tuning actuation-noise multiplier (found optimal m=0 for this setup), and grid-searching simulator params using SRCC as objective.",
            "fidelity_requirements_identified": "Paper identifies that contact behavior (sliding vs stopping) critically affects transfer; accurate actuation noise modeling is important (their simple noise model did not improve SRCC), and that matching embodiment (size, sensor FOV/resolution) and localization fidelity are required; it also documents that agent-level aggregate predictivity is achievable but path-level predictivity is not.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Grid-search over sliding (on/off) and actuation-noise multiplier (0.0–1.0) found best predictivity with sliding=off and actuation-noise multiplier=0.0; initial (Challenge) setting sliding=on, noise=0 gave SPL SRCC=0.603 and Succ SRCC=0.18, optimized setting gave SPL SRCC≈0.875 and Succ SRCC≈0.844; introducing actuation noise in test-sim did not uniformly improve SRCC in this setup.",
            "key_findings": "1) Naive simulator settings (Habitat-Challenge defaults) can yield poor sim2real predictivity because learned RL agents exploit simulator imperfections (e.g., sliding) to 'cheat'; 2) A quantitative predictor (SRCC) can be used to optimize simulator parameters to greatly improve sim2real predictivity (SPL SRCC from 0.603→0.875, Succ SRCC from 0.18→0.844); 3) Contact/collision modeling (sliding vs stopping) and accurate actuation/noise modeling are major determinants of transfer success; 4) HaPy enables straightforward execution of identical code in sim and reality, reducing engineering friction for transfer studies.",
            "uuid": "e1633.0",
            "source_info": {
                "paper_title": "Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation",
                "publication_date_yy_mm": "2019-12"
            }
        },
        {
            "name_short": "Sliding exploit in Habitat-Sim",
            "name_full": "Habitat-Sim collision 'sliding' behavior (simulator artifact enabling shortcut exploits)",
            "brief_description": "A simulator-level collision behavior where agents slide along obstacles upon collision instead of stopping, causing straight-line SPL computations to undercount true traversed path and enabling learned agents to take unrealistic shortcuts that do not exist on physical robots.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Simulated PointNav agent (cylindrical agent in Habitat-Sim) / learned RL policies",
            "agent_system_description": "An embodied navigation agent modeled as a cylindrical body (configured to match LoCoBot dimensions) with RGB/D sensors and discrete actions; agents trained with RL can exploit simulator collision handling.",
            "domain": "embodied visual navigation / robotics",
            "virtual_environment_name": "Habitat-Sim navigation mesh / collision handling",
            "virtual_environment_description": "Navigation mesh-based simulator where collision resolution returns the closest point in navigable space within a fixed radius and by default slides the agent along obstacles to that point instead of stopping on contact.",
            "simulation_fidelity_level": "approximate physics/contact handling typical of game engines (sliding enabled for smooth human control) — not physically accurate contact dynamics",
            "fidelity_aspects_modeled": "navigable-space computation via navigation meshes, photorealistic scene geometry and rendering, simple collision resolution that produces sliding behavior",
            "fidelity_aspects_simplified": "realistic contact dynamics, bump-sensor stop behavior, non-holonomic constraints during contact, physically accurate path traced during collisions (the SPL path uses Euclidean shortcut rather than true traversal), friction/contact impulses",
            "real_environment_description": "Physical lab where the robot stops on contact with walls (bump sensors / physical contact), so sliding exploits from sim do not translate — collision leads to stopping and failure to traverse the simulated shortcut.",
            "task_or_skill_transferred": "PointGoal navigation policies (agents exploited sliding in sim to improve SPL and success metrics in simulation but failed when deployed to real robot that does not slide).",
            "training_method": "Reinforcement learning (DD-PPO) in simulation; policies discovered exploiting sliding behavior during training/testing in sim.",
            "transfer_success_metric": "Difference in SRCC (sim-to-real correlation) and direct drop in real-world success/SPL for agents that exploited sliding; SRCC Succ initially 0.18 showing poor predictivity.",
            "transfer_performance_sim": "Some agents achieved artificially high success/SPL in sim due to sliding (success rates close to 1.0 and 'better-than-optimal' simulated path lengths).",
            "transfer_performance_real": "Corresponding agents perform much worse in reality (large dynamic range, lower success), demonstrating failed transfer for sliding-exploiting policies; numeric SRCC reflects misalignment (Succ SRCC=0.18 before fixing sliding).",
            "transfer_success": false,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Mismatch in collision/contact handling (sliding-enabled navigation mesh vs real stopping-on-contact), evaluation metric using Euclidean distance between pre/post-collision positions, simplistic collision resolution that maps to nearest navigable point causing apparent traversal through obstacles.",
            "transfer_enabling_conditions": "Turning sliding off in Habitat-Sim (disabling the sliding behavior) removed the exploit, aligning sim behaviors with real robot contact/stopping and greatly improving sim2real predictivity (SRCC improvements reported).",
            "fidelity_requirements_identified": "Contact dynamics and collision-resolution behavior must match reality (agents should not be afforded sliding shortcuts); simulation should model stopping-on-contact or bump-sensor effects for predictive navigation transfer.",
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": true,
            "fidelity_comparison_results": "Comparing sliding ON vs OFF: sliding ON produced lower SRCC and more rank reversals between sim and reality; sliding OFF significantly improved SRCC (SPL SRCC from 0.603→0.875 and Succ SRCC from 0.18→0.844 when combined with actuation-noise multiplier tuning).",
            "key_findings": "The simulator's collision handling (sliding) is a dominant cause of sim2real mismatch for learned navigation policies because it permits unrealistic shortcuts that inflate simulated metrics; disabling sliding and tuning actuation-noise improves real-world predictivity.",
            "uuid": "e1633.1",
            "source_info": {
                "paper_title": "Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation",
                "publication_date_yy_mm": "2019-12"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "CAD2RL: real single-image flight without a single real image",
            "rating": 2,
            "sanitized_title": "cad2rl_real_singleimage_flight_without_a_single_real_image"
        },
        {
            "paper_title": "Sim-to-real: Learning agile locomotion for quadruped robots",
            "rating": 2,
            "sanitized_title": "simtoreal_learning_agile_locomotion_for_quadruped_robots"
        },
        {
            "paper_title": "Learning agile and dynamic motor skills for legged robots",
            "rating": 2,
            "sanitized_title": "learning_agile_and_dynamic_motor_skills_for_legged_robots"
        },
        {
            "paper_title": "Driving policy transfer via modularity and abstraction",
            "rating": 2,
            "sanitized_title": "driving_policy_transfer_via_modularity_and_abstraction"
        },
        {
            "paper_title": "Pyrobot: An open-source robotics framework for research and benchmarking",
            "rating": 2,
            "sanitized_title": "pyrobot_an_opensource_robotics_framework_for_research_and_benchmarking"
        },
        {
            "paper_title": "Habitat: A Platform for Embodied AI Research",
            "rating": 2,
            "sanitized_title": "habitat_a_platform_for_embodied_ai_research"
        },
        {
            "paper_title": "MINOS: Multimodal indoor simulator for navigation in complex environments",
            "rating": 1,
            "sanitized_title": "minos_multimodal_indoor_simulator_for_navigation_in_complex_environments"
        },
        {
            "paper_title": "On Evaluation of Embodied Navigation Agents",
            "rating": 1,
            "sanitized_title": "on_evaluation_of_embodied_navigation_agents"
        }
    ],
    "cost": 0.0119195,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation
13 Dec 2019</p>
<p>Abhishek Kadian 
Facebook AI Research</p>
<p>Joanne Truong 
Georgia Institute of Technology</p>
<p>Aaron Gokaslan 
Facebook AI Research</p>
<p>Alexander Clegg 
Facebook AI Research</p>
<p>Erik Wijmans 
Facebook AI Research</p>
<p>Georgia Institute of Technology</p>
<p>Stefan Lee 
Oregon State University</p>
<p>Manolis Savva 
Facebook AI Research</p>
<p>Simon Fraser University</p>
<p>Sonia Chernova 
Facebook AI Research</p>
<p>Georgia Institute of Technology</p>
<p>Dhruv Batra 
Facebook AI Research</p>
<p>Georgia Institute of Technology</p>
<p>Are We Making Real Progress in Simulated Environments? Measuring the Sim2Real Gap in Embodied Visual Navigation
13 Dec 20191
Denotes equal contribution.a new metric called Sim-vs-Real Correlation Coefficient (SRCC) to quantify sim2real predictivity.Our analysis reveals several important findings. We find that SRCC for Habitat as used for the CVPR19 challenge is low (0.18 for the success metric), which suggests that performance improvements for this simulator-based challenge would not transfer well to a physical robot. We find that this gap is largely due to AI agents learning to 'cheat' by exploiting simulator imperfections -specifically, the way Habitat allows for 'sliding' along walls on collision. Essentially, the virtual robot is capable of cutting corners, leading to unrealistic shortcuts through parts of non-navigable space. Naturally, such exploits do not work in the real world where the robot stops on contact with walls.Our experiments show that it is possible to optimize simulation parameters to enable robots trained in imperfect simulators to generalize learned skills to reality (e.g. improving SRCC Succ from 0.18 to 0.844).</p>
<p>1 Facebook AI Research, 2 Georgia Institute of Technology, 3 Oregon State University 4 Simon Fraser University,  </p>
<p>Abstract</p>
<p>Does progress in simulation translate to progress in robotics? Specifically, if method A outperforms method B in simulation, how likely is the trend to hold in reality on a robot?</p>
<p>We examine this question for embodied (PointGoal) navigation -developing engineering tools and a research paradigm for evaluating a simulator by its sim2real predictivity, revealing surprising findings about prior work.</p>
<p>First, we develop Habitat-PyRobot Bridge (HaPy), a library for seamless execution of identical code on a simulated agent and a physical robot. Habitat-to-Locobot transfer with HaPy involves just one line change in a config parameter, essentially treating reality as just another simulator! Second, we investigate sim2real predictivity of Habitat-Sim [1] for PointGoal navigation. We 3D-scan a physical lab space to create a virtualized replica, and run parallel tests of 9 different models in reality and simulation. We present</p>
<p>Introduction</p>
<p>A ship in harbor is safe, but that is not what ships are built for.</p>
<p>John A. Shedd [2] We are witnessing a resurgence of interest in Embodied AI -the study of embodied agents (e.g. robots) perceiving, navigating, and interacting with their environment. For a variety of reasons, such work is commonly carried out in simulation rather than in real-world environments. Simulators can run orders of magnitude faster than real-time [1], can be highly parallelized, and enable decades of agent experience to be collected in days [3]. Evaluating agents in simulation is safer, cheaper, and enables easier benchmarking of scientific progress than running robots in the real-world.</p>
<p>The vision, language, and learning research communities have rallied around simulators -developing several indoor/outdoor navigation simulators that present increasingly realistic environments to agents [1,[4][5][6][7][8][9][10], designing a variety of tasks set in them [4,11,12], holding workshops about such platforms [13], and even running challenges in these simulated worlds [14,15]. As a result, significant progress has been made in these settings. For example, agents can reach point goals in novel home environments with near-perfect efficiency [3], control vehicles in complex, dynamic city environments [5], follow natural-language instructions [11], and answer questions [12].</p>
<p>However, no simulation is a perfect replica of reality, and AI systems are known to exploit imperfections and biases to achieve strong performance in simulation which may be unrepresentative of reality. Notable examples include evolving tall creatures for locomotion that fall and somersault instead of learning active locomotion strategies [16] and OpenAI's hide-and-seek agents abusing their physics engine to 'surf' on top of obstacles [17].</p>
<p>This raises several fundamental questions of deep interest to the scientific and engineering communities: How much of progress in simulated environments is due to 'overfitting' to anomalies in simulators and how much translates to progress in reality? Concretely, do improvements observed in simulated environments correlate with improved performance on robots? Should we trust the outcomes of embodied AI challenges (e.g. the AI Habitat Challenge at CVPR 2019) that are performed entirely in simulation?</p>
<p>In this work, we examine these questions in the context of embodied visual navigation. Specifically, we introduce engineering tools and a research paradigm for performing simulation-to-reality (sim2real) indoor navigation studies, revealing surprising findings about prior work.</p>
<p>First, we develop the Habitat-PyRobot Bridge (HaPy), a software library that enables seamless sim2robot transfer. HaPy is an interface between (1) Habitat [1], a import habitat # What is the Embodied AI task? config = habitat.get_config("pointnav.yaml") # Are we in sim or reality? if args.use_simulation: # Use Habitat-Sim config.SIMULATOR.TYPE = "Habitat-Sim-v0" else:</p>
<h1>Use LoCoBot via PyRobot config.SIMULATOR.TYPE = "PyRobot-Locobot-v0" # Create environment (sim or real doesn't matter) env = habitat.Env(config) observations = env.reset() high-performance photorealistic 3D simulator, and (2) Py-Robot [18], a high-level python library for robotics research. Crucially, HaPy makes trivial to execute identical code in simulation and reality. As shown in Figure 2, sim2robot transfer with HaPy involves only a single line edit to the code (changing the config.simulator variable from Habitat-Sim-v0 to PyRobot-Locobot-v0), essentially treating reality as just another simulator! This reduces code duplication, provides an intuitive high-level abstraction, and allows for rapid prototyping with modularity (training a large number of models in simulation and 'tossing them over' for testing on the robot). In fact, all experiments in this paper were conducted by a team of researchers physically separated by thousands of miles -one set training and testing models in simulation, another conducting on-site tests with the robot, made trivial due to HaPy. We will open-source HaPy so that everyone has this ability.</h1>
<p>Second, we propose a general experimental paradigm for performing sim2real studies, which we call sim2real predictivity. Our notable thesis is that simulators need not be a perfect replica of reality to be useful. Specifically, we should primarily judge simulators not by their visual or physical realism, but by their sim2real predictivity -if method A outperforms B in simulation, how likely is the trend to hold in reality? To answer this question, we propose the use of a quantity we call Sim2Real Correlation Coefficient (SRCC). Figure 3 illustrates our proposed methodology. We prepare a real lab space within which the robot must navigate while avoiding obstacles. We then virtualize this lab space (under different obstacle configurations) by 3D scanning the space and importing it in Habitat. Armed with the power to perform parallel trials in reality and simulation, we test a suite of navigation models both in simulation and in the lab with a real robot. We then produce a scatter plot where every point is a navigation model, the x-axis is the performance in simulation, and the y-axis is performance in reality. SRCC is shown in a box at the top. If SRCC is high (close to 1), this is a 'good' simulator setting in the sense that we can conduct scientific development and testing purely in simulation, with confidence that we are making 'real' progress because the improvements in simulation will generalize to real robotic testbeds. If SRCC is low (close to 0), this is a 'poor' simulator, and we should have no confidence in results reported solely in simulation. We apply this methodology in the context of PointGoal Navigation (PointNav) [19] with Habitat and the LoCoBot robot [20] as our simulation and reality platforms -our experiments made easy with HaPy. These experiments reveal a number of surprising findings:</p>
<p>-We find that SRCC for Habitat as used for the CVPR19 challenge is 0.603 for the SPL metric and 0.18 for agent success. When ranked by SPL, we observe 9 relative ordering reversals from simulation to reality, suggesting that the results/winners may not be the same if the challenge were run on LoCoBot. -We find that large-scale RL trained models can learn to 'cheat' by exploiting the way Habitat allows for 'sliding' along walls on collision. Essentially, the virtual robot is capable of cutting corners by sliding around obstacles, leading to unrealistic shortcuts through parts of non-navigable space and 'better than optimal' paths. Naturally, such exploits do not work in the real world where the robot stops on contact with walls. -We optimize SRCC over Habitat design parameters and find that a few simple changes improve SRCC SPL from 0.603 to 0.875 and SRCC Succ from 0.18 to 0.844. The number of rank reversals nearly halves to 5 (13.8%). Furthermore, we identify highly-performant agents in both this new simulation and on LoCoBot in real environments.</p>
<p>While our experiments are conducted on PointNav, we believe our software (HaPy), experimental paradigm (sim2real predictivity and SRCC), and take-away messages will be useful to the broader community.</p>
<p>Related Work</p>
<p>Embodied AI tasks. Given the emergence of several 3D simulation platforms, it is not surprising that there has been a surge of research activity focusing on investigation of embodied AI tasks. One early example leveraging simulation is the work of Zhu et al. [21] on target-driven navigation using deep reinforcement learning in synthetic environments within AI2 THOR [8]. Follow up work by Gupta et al. [22] has demonstrated an end-to-end learned joint mapping and planning method evaluated in simulation using reconstructed interior spaces. More recently, Gordon et al. [23] have shown that decoupling perception and policy learning modules can aid in generalization to unseen environments, as well as between different environment datasets. Beyond these few examples, a breadth of recent work on embodied AI tasks demonstrates the acceleration that 3D simulation platforms have brought to this research area. In contrast, deployment on real robotic platforms for similar AI tasks still incur significant resource overheads and are typically only feasible with large, well-equipped teams of researchers. One of the most prominent examples is the DARPA Robotics Challenge (DRC) [24]. Another example of real-world deployment is the work of Gandhi et al. [25] who trained a drone to fly in reality by locking it in a room. However, the long training time and potential to physically damage the robot makes iteration rather impractical. Our goal is to characterize how well a model trained in simulation can generalize when deployed on a real robot.</p>
<p>Simulation-to-reality transfer. Due to the logistical limitations of physical experimentation, transfer of agents trained in simulation to real platforms is a topic of much interest. There have been successful demonstrations of simulation-toreality transfer in several domains. The CAD2RL [9] system of Sadeghi and Levine trained a collision avoidance policy entirely in simulation and deployed it on real aerial drones. Similarly, Muller et al. [26] show that driving policies can be transferred from simulated cars to real remote-controlled cars by leveraging modularity and abstraction in the control policy. Tan et al. [27] train quadruped locomotion policies in simulation by leveraging domain randomization and demonstrate robustness when deployed to real robots. Lastly, Hwangbo et al. [28] train legged robotic systems in simulation and transfer the learned policies to reality. This research has been enabled in part by the increasing availability of cheaper and more reliable robot platforms. As robots become cheaper and increasingly useful, the need to train and evaluate performance on more and more tasks arises. This brings to the forefront the key question: can we establish a correlation between performance in simulation and deployment in reality? We focus on this question in the domain of visual indoor navigation.</p>
<p>Habitat-PyRobot Bridge: Simple Sim2Real</p>
<p>Deploying AI systems developed in simulation to physical robots presents significant financial, engineering, and logistical challenges -especially for non-robotics researchers. Approaching this directly requires researchers to maintain two parallel software stacks, one typically based on ROS [29] for the physical robot and another for simulation. In addition to requiring significant duplication of effort, this model can also introduce inconsistencies between agent details and task specifications in simulation and reality. To reduce this burden and enable our experiments, we introduce the Habitat-PyRobot Bridge (HaPy). As its name suggests, HaPy integrates the Habitat [1] platform with Py-Robot APIs [18] -enabling identical agent and evaluation code to be executed in simulation with Habitat and on a PyRobot-enabled physical robot. Habitat is a platform for embodied AI research that aims to standardize the different layers of the embodied agent software stack, covering 1) datasets, 2) simulators, and 3) tasks. This enables researchers to cleanly define, study, and share embodied tasks, metrics, and agents. For deploying simulation-trained agents to reality, we replace the simulator layer in this stack with 'reality' while maintaining task specifications and agent interfaces. Towards this end, we integrate Habitat with PyRobot [18], a recently released high-level API that implements simple interfaces that abstract lower-level control and perception. As shown in Fig. 2, running an agent developed in Habitat on the LoCoBot platform requires changing a single argument, Habitat-Sim-v0→PyRobot-Locobot-v0.</p>
<p>At a high level, HaPy enables the following: -A uniform observation space API across simulation and reality. Having a shared implementation ensures that observations from simulation and reality sensors go through the same transformations (e.g. resizing, normalization). -A uniform action space API for agent across simulation and reality. Habitat and PyRobot differ in their agent action spaces. Our integration unifies the two action spaces and allows an agent model to remain agnostic between simulation and reality. -Integration at the simulator layer in the embodied agents stack allows reuse of functionalities offered by task layers of the stack -task definition, metrics, etc. stay the same across simulation and reality. This also opens the potential for jointly training in simulation and reality. -Containerized deployment for running challenges where participants upload their code for seamless deployment to mobile robot platforms such as LoCoBot.</p>
<p>It is our hope that this contribution will allow the community to easily build agents in simulation and deploy them in the real world. We will publicly release HaPy and provide a pull request to integrate it with Habitat. For more details on how HaPy integrates Habitat and PyRobot, see the supplement.</p>
<p>Visual Navigation in Simulation &amp; Reality</p>
<p>Recall that our goal is to answer an ostensibly straightforward scientific question -is performance in simulated environments predictive of real-world performance for visual navigation? Let us make this more concrete.</p>
<p>First, note this question is about testing in simulation vs reality. It does not require us to take a stand on training in simulation vs reality (or the need for training at all). For a comparison between simulation-trained and non-learningbased navigation methods, we refer the reader to previous studies [1,30,31]. We focus on test-time discrepancies between simulation and reality for learning-based methods.</p>
<p>Second, even at test-time, many variables contribute to the sim2real gap. The real-world test environment may include objects or rooms which visually differ from simulation, or may present a differing task difficulty distribution (due to unmodeled physics or rendering in simulation). To isolate these factors as much as possible, we propose a direct comparison -evaluating agents in physical environments and in corresponding simulated replicas. We construct a set of physical lab environment configurations for a robot to traverse and virtualize each by 3D scanning the space, thus controlling for semantic domain gap. We then evaluate agents in matching simulated and real configurations to characterize the sim-vs-real gap in visual navigation.</p>
<p>In this section, we first recap the PointNav task [19] from the recent AI Habitat Challenge [14]. Then, a comparison between agents in simulation and robots in reality.</p>
<p>Task: PointGoal Navigation (PointNav)</p>
<p>In this task, an agent is spawned in an unseen environment and asked to navigate to a goal location specified in relative coordinates. We start from the agent specification and observation settings from [14]. Specifically, agents have access to an egocentric RGB (or RGBD) sensor and accurate localization and heading via a GPS+Compass sensor. The goal is specified using polar coordinates (r, θ), where r is the Euclidean distance to the goal and θ is the azimuth to the goal. The action space for the agent consists of 4 actions: turn-left 30 •1 , turn-right 30 •1 , forward 0.25m, and STOP. An episode is considered successful if the agent issues the STOP command within 0.2 meters of the goal. Episodes lasting longer than 200 steps or calling the STOP command &gt;0.2m from goal are declared unsuccessful. In this paper, we reduce max-steps from 500 to 200 because our testing lab is smaller. In preliminary experiments, we found that episodes longer than 200 actions are likely to fail. We also limit collisions to 40 in sim and real to prevent damage to the robot. We find that &gt;40 collisions in an episode typically occurs when the robot is stuck and is likely to fail.</p>
<p>Agent in Simulation</p>
<p>Body. The experiments by Savva et al. [1] and the Habitat Challenge 2019 [14] model the agent as an idealized cylinder of radius 0.1m and height 1.5m. As shown in Fig. 5, we configure the agent to match the robot used in our experiments (LoCoBot) as closely as possible. Specifically, we configure the simulated agent's base-radius and height to be 0.175m and 0.61m respectively to match LoCoBot dimensions.</p>
<p>Sensors. We set the agent's camera's field of view to 45 degrees to match the Intel D435 camera on LoCoBot. We match the aspect ratio and resolution of the simulated sensor frames to real sensor frames from LoCoBot using square center cropping followed by image resizing to a height and width of 256 pixels. To mimic the depth camera's limitations, we clip simulated depth sensing to 10m.</p>
<p>Actions. In [1,14], agent actions are deterministic -i.e. when the agent executes turn-left 30 • , it turns exactly 30 • , and forward 0.25m moves the agent exactly 0.25m forward (modulo collisions). However, no robot moves deterministically due to real-world actuation noise. To model the actions on LoCoBot, we leverage an actuation noise model derived from mocap-based benchmarking by the Py-Robot authors [18]. Specifically, when the agent calls (say) forward, we sample from an action-specific 2D Gaussian distribution over relative displacements. Fig. 4 shows trajectory rollouts sampled from this noise model (in a Gibson [6] home). As shown, identical action sequences can lead to 1 Originally 10 • . Finally, in contrast to [1,14], we increase the angles associated with turn-left and turn-right actions from 10 • to 30 • degrees. The reason is a fundamental discrepancy between simulation and reality -there is no 'ideal' GPS+compass sensor in reality. Perfect localization in indoor environments is an open research problem. In our preliminary experiments, we found that localization noise was exacerbated by the 'move, stop, turn, move' behavior of the robot, which is a result of a discrete action space (as opposed to continuous control via velocity or acceleration actions). We strike a balance between staying comparable to prior work (that uses discrete actions) and reducing localization noise by increasing the turn angles (which decreases the number of times the robot stops and restarts). In the longer term, we believe the community should move towards continuous control to overcome this issue.</p>
<p>All the modeling parameters are easily configurable and can be adapted to different robots.</p>
<p>LoCoBot in Reality</p>
<p>Body. LoCoBot is designed to provide easy access to a robot with basic grasping, locomotion, and perception capabilities. It is a modular robot based on a Kobuki YMR-K01-W1 mobile base with an extensible body.</p>
<p>Sensors.</p>
<p>LoCoBot is equipped with a Intel D435 
→ B → C → D → E → A.
RGB+depth camera. While LoCoBot possesses onboard IMUs and motor encoders (which can provide the GPS+Compass sensor observations required by this task), the frequent stopping and starting from our discrete actions resulted in significant error accumulation. To provide precise localization, we mounted a Hokuyo UTM-30LX LIDAR sensor in place of the robot's grasping arm (seen in Figure 5). We run the LIDAR-based Hector SLAM [32] algorithm to provide the location+heading for GPS+Compass sensor and for computing success and SPL of tests in the lab.</p>
<p>At this point, it is worth asking how accurate the LIDAR based localization is. To quantify this localization error, we ran a total of 45 tests across 3 different room configurations in the lab, and manually measured the error (with measuring tapes). On average, we find errors of approximately 7cm with Hector SLAM, compared to 40cm obtained from wheel odometry and onboard IMU (combined with an Extended Kalman Filter implementation in ROS). Note that 7cm is significantly lower than the 0.2m = 20cm criteria used to define success in PointNav, providing us confidence that we can use LIDAR-based Hector SLAM to judge success in our real-world experiments. More importantly, we notice that the LIDAR approach allows the robot to reliably relocalize using its surroundings, and thus error does not accumulate over long trajectories, or with consecutive runs, which is important for running hundreds of real-world experiments. The reality trajectory in Fig. 5 is highlighted in yellow to illustrate the localization error while the simulation trajectory has an idealized GPS + Compass which gives perfect localization.</p>
<p>Evaluation Environment</p>
<p>Our evaluation environment is a controlled 6.5m by 10m interior room. We create 3 room configurations (easy, medium, hard difficulty) with increasing number of tall, cardboard obstacles spread throughout the room. These 'boxy' obstacles can be sensed easily by both the LIDAR and camera despite two being vertically separated by~0.5m. Objects like tables or chairs have narrow support at the LIDAR's height. For each room configuration, we define a set of 5 waypoints to serve as the start and end locations for naviga-tion episodes. Figure 6 shows top-down views of these room configurations labeled with waypoint locations.</p>
<p>Virtualization. We digitize each environment configuration using a Matterport Pro2 3D camera to collect 360 • scans at multiple points in the room, ensuring full coverage. These scans are used to reconstruct 3D meshes of the environment which can be directly imported into Habitat. This streamlined process is easily scalable and enables quick virtualization of physical spaces. On average, each configuration was reconstructed from 7 panoramic captures and took approximately 10 minutes. Fig. 3 illustrates this process.</p>
<p>Test protocol. We run parallel episodes in both simulation and reality. The agent navigates sequentially through the waypoints shown in Fig. 6 
(A → B → C → D → E → A)
for a total of 5 navigation episodes per room configuration. The starting points, starting rotations, and goal locations are identical across simulation and reality. In total, we test 9 navigation models (described in the next section), in 3 different room configurations, each with 5 spawn-to-goal waypoints, and 3 independent trials, for a total of 810 runs in simulation and reality combined. Each spawn-to-goal navigation with LoCoBot takes approximately 6 minutes, corresponding to 40.5 hours of real-world testing. Safety guidelines require that a human monitor the experiments and at 8 hours a day, these experiments would take 5 days. With such long turnaround times, it is essential that we use a robust pipeline to automate (or semi-automate) our experiments and reduce the cognitive load on the human supervisor. After each episode, the robot is automatically reset and has no knowledge from its previous run. For unsuccessful episodes, the robot uses a prebuilt environment map to navigate to the next episode start position. The room is equipped with a wireless camera to remotely track the experiments. In future, we plan to connect this automated evaluation setup to a docker2robot challenge, where participants can push (appropriately containerized) code to a repository, which is then automatically evaluated on a real robot in this lab environment.</p>
<p>Sim2Real Correlation Coefficient</p>
<p>To quantify the degree to which performance in simulation translates to performance in reality, we use a measure we call Sim2Real Correlation Coefficient (SRCC). Let (s i , r i ) denote accuracy (episode success rate, SPL [11], etc.) of navigation method i in simulation and reality respectively. Given a paired dataset of accuracies for n navigation methods {(s 1 , r 1 ), . . . , (s n , r n )}, SRCC is the sample Pearson correlation coefficient (bivariate correlation). 2 SRCC values close to +1 indicate high linear correlation and are desirable, insofar as changes in simulation performance metrics correlate highly with changes in reality performance metrics. Values close to 0 indicate low correlation and are undesirable as they indicate changes of performance in simulation is not predictive of real world changes in performance. Note that this definition of SRCC also suggests an intuitive visualization: by plotting performance in reality against performance in simulation as a scatterplot we can reveal the existence of performance correlation and detect outlier simulation settings or evaluation scenarios.</p>
<p>Beyond the utility of SRCC as a simulation predictivity metric, we can also view it as an optimization objective for simulation parameters. Concretely, let θ denote parameters controlling the simulator (amount of actuation noise, lighting, etc.). We can view simulator design as optimization problem: max θ SRCC(S n (θ), R n ) where S n (θ) = {s 1 (θ), . . . , s n (θ)} is the set of accuracies in simulation with parameters θ and R n is the same performance metric computed on equivalent episodes in reality. Note that θ affects performance in simulation S n (θ) but not R n since we are only changing test-time parameters. The specific navigation models themselves are held fixed. Overall, this gives us a formal approach to simulator design instead of operating on intuitions and qualitative assessments.</p>
<p>Measuring the Sim2Real Gap</p>
<p>Navigation Models. We experiment with learning-based navigation models. Specifically, we train for PointGoal in Habitat on the 72 Gibson environments that were rated 4+ for quality in [1]. For consistency with [1], we use the Gibson dataset 3 [6] for training. Agents are trained from scratch with reinforcement learning using DD-PPO [3] -a decentralized, distributed proximal policy optimization [33] algorithm that is well-suited for GPU-intensive simulatorbased training. Each model is trained for 500 million steps on 64 Tesla V100s. For evaluation, we select the model with best Gibson-val performance. We use the agent architecture from [3] composed of a visual encoder (ResNet50 [34]) and policy network  with 512-dim state).</p>
<p>Consistent with prior work, we train agents with RGB and depth sensors. Real-world depth sensors exhibit significant noise and are limited in range. Thus, inspired by the winning 2 Other metrics such as rank correlation can also be used. 3 Gibson license agreement: https://storage.googleapis.com/ gibson_material/Agreement%20GDS%2006-04-18.pdf  [14], we also test an agent that uses a monocular depth estimator [36] to predict depth from RGB, which is then fed to the navigation model. In total, we train 9 different agents by varying sensor modalities (RGB, Depth, RGB→Predicted Depth) and training simulator configurations (e.g. actuation noise levels). The exact settings for these 9 agents are listed in Table 1. Note that the simulator parameters used for training these models may be different than the simulator parameters used for testing (θ). Our goal is to span the spectrum of performance at test-time. Figure 7 shows the scatterplots of sim-vs-real performances of these 9 navigation models w.r.t. success rate (right) and SPL [19] (left) metrics. Horizontal and vertical bars around a symbol indicate the standard error bars in simulation and reality respectively. SRCC SPL is 0.60, which is reasonably high but far from a level where we can be confident about evaluations in simulation alone. Problematically, there are 9 relative ordering reversals from simulation to reality. Success scatter-plot (right) shows an even more disturbing trend -nearly all methods (except one) appear to working exceedingly well in simulation with success rates close to 1. However, there is a large dynamic range in success rates in reality. This is summarized by a low SRCC Succ of 0.18, suggesting that improvements in performance in simulation are not predictive of performance improvements on a real robot.</p>
<p>Revisiting the AI Habitat Challenge 2019</p>
<p>Note that other than largely cosmetic adjustments to the robot size, sensor and action space specification, this simulation setting is not fundamentally different from the Habitat Challenge 2019. Upon deeper investigation, we discovered that a key factor leading to this low sim2real predictivity is due to a 'sliding' behavior in Habitat that we discuss below.</p>
<p>Cheating by sliding. In Habitat-Sim [1], when the agent takes an action that results in a collision, the agent slides along the obstacle as opposed to stopping. This behavior is prevalent in video game engines as it allows for smooth human control; it is also enabled by default in MINOS [10], Deepmind Lab [7], and AI2 THOR [8]. We find that this enables 'cheating' by learned agents. Specifically, as illustrated by an example in Fig. 8, the agent exploits this sliding mechanism to take an effective path that appears to travel through non-navigable regions of the environment (like walls). Let s t denote agent position at time t, where the agent is already in contact with an obstacle. The agent can execute a forward action, collide, and slide along the obstacle to state s t+1 (see Fig. 8). The path taken during this maneuver is far from a straight line, however for the purposes of computing SPL (the metric the agent optimizes), Habitat calculates the Euclidean distance travelled ||s t − s t+1 || 2 . This is equivalent to taking a straight line path between s t and s t+1 that goes outside the navigable regions of the environment (appearing to go through obstacles). On the one hand, the emergence of such exploits is a sign of success of large-scale reinforcement learning -clearly, we are maximizing reward. On the other hand, this is a problem for sim2real transfer. Such policies fail disastrously in the real world where the robot bump sensors force a stop on contact with obstacles. To rectify this issue, we modify Habitat-Sim to disable sliding on collisions. The discovery of this issue motivated our investigation into optimizing simulation parameters.</p>
<p>Optimizing simulation with SRCC</p>
<p>We perform grid-search over simulation parameters -sliding (off vs on) and a scalar multiplier on actuation noise (varying from 0 to 1, in increments of 0.1; see supplement for details). We find that sliding off and 0.0 actuation noise lead to the highest SRCC. Fig. 9 shows this optimized SRCC, for success rate (right) and SPL (left). We see a remarkable alignment in the SPL scatter-plot (left) -nearly all models lie close to the diagonal, suggesting that we fairly accurately predict how a model is going to perform on the robot by testing in simulation. Recall that the former takes 40.5 hours</p>
<p>Sensor</p>
<p>Train-Sim Noise  Table 1 shows the sensor and training settings for the different navigation methods (columns 1-3), the SPL achieved by those methods in reality (column 4), in simulation under Habitat Challenge 2019 settings (sliding=on, noise=0; column 5), and our proposed test-sim setting (sliding=off, noise=0; column 6). We can see that the best performance in reality (Reality SPL) is achieved by row 2, which is confidently predicted by Test-Sim SPL (col 4) by a strong margin. Further analysis on the grid search can be found in the supplement. The fact actuation noise=0 is found as the optimal setting suggests that our chosen actuation noise model (from PyRobot [18]) may not reflect conditions in reality.</p>
<p>Prediction Granularity. In optimizing simulator parameters, we can consider three levels of granularity -can simulation predict real performance for a) a particular path b) a particular environment c) an agent. We have already demonstrated (c) is possible. We compute SRCC values at each of these levels, and find that it is not possible to reliably predict real performance for particular paths (see supplement for details).</p>
<p>Conclusion</p>
<p>We introduce the Habitat-PyRobot Bridge (HaPy) library which allows for seamless deployment of visual navigation models across simulation (Habitat) and reality (LoCoBot). Using the Matterport scanning pipeline, Habitat stack, HaPy library, and LoCoBot we benchmark the correlation between reality and simulation performance and introduce the SRCC metric. We find that naive simulation parameters lead to low correlation between performance in simulation and performance in reality. However, we demonstrate that the SRCC metric can be used to optimize simulation parameters and obtain high predictivity of real world performance. We hope that the infrastructure we introduce and the conceptual framework of optimizing simulation for predictivity will enable sim2real transfer in a variety of navigation tasks, and lead to generalization of embodied AI from simulation to reality.</p>
<p>Video</p>
<p>The video included with the supplemental material provides an overview of our approach and examples from parallel simulation and reality experiments.</p>
<p>HaPy Architecture</p>
<p>In this work we develop the Habitat-PyRobot bridge (HaPy) extension which allows for seamless sim2real transfer of simulation-trained models to reality. Figure 10 shows the integration of our extension with Habitat-API [1]. This integration allows us to reuse the PointNav task and RL baseline already implemented in Habitat-API. We have added our codebase which contains the implementation of HaPy to the supplemental zip. Please refer to habitat-pyrobot-bridge-source/README.md for details.</p>
<p>Actuation Noise Model</p>
<p>The state of an agent is represented by (x, y, θ) where x, y are the 2D coordinates of base and θ is the agent's heading direction. The action space of agent consists of forward which moves the agent 0.25m forward in θ direction, left which changes θ by 30 • and right which changes θ by -30 • . Noisy actuation in Habitat-Sim is a two-step process:</p>
<ol>
<li>The first step consists of an action execution under ideal conditions. Let (x t , y t , θ t ) be the state of agent before action execution. The agent takes a step under ideal action conditions, let (x t ,ȳ t ,θ t ) be the agent state after ideal action execution. 2. (∆x, ∆y, ∆θ) are sampled from a multi-variate Gaussian with a diagonal covariance matrix where the mean and variance for the multi-variate Gaussian are a function of action type (forward, left or right) and are fit on data collected from a real robot [18]. Let m be the noise-multiplier, this is the same noisemultiplier that we grid search over in our experiments. The final agent state is calculated as: (x t , y t , θ t ) = (x t + m∆x,ȳ t + m∆y,θ t + m∆θ).</li>
</ol>
<p>Evaluation Environments</p>
<p>For our experiments we use an evaluation environment of 6.5m by 10m. The room consists of green walls and a checkerboard pattern on the floor. This was purely coincidental and is in no way used by the agent for navigation. Note that the agent was trained entirely in simulation and has never seen our evaluation environment room during training. In our supplemental video we also show a successful episode of the agent navigating in a completely new cluttered environment.</p>
<p>Localization Error Experiment Details</p>
<p>PointNav task requires a GPS+Compass sensor for navigating towards the goal. LoCoBot comes equipped with an on-board IMU which was not accurate enough for our experiments. To get around this problem we use a LIDAR based localization system. We conduct a a series of localization experiments to quantify the amount of localization error across the LIDAR system and the on-board IMU. We manually measured the error using a measuring tape (Fig  12).</p>
<ol>
<li>LIDAR Hector SLAM: a SLAM based localization method that leverages high update rates from LIDAR sensors for optimizing scan-matching. Scan-matching is the process of aligning laser scans with preceding scans, and since LIDARs have low distance measurement noise, this process leads to an accurate pose estimate. 2. IMU Extended Kalman Filter: a pose estimation method that uses sensor fusion to combine measurements from wheel odometry and onboard IMU sensor. The noisy estimates from sensors are combined and filtered to remove uncertainties in order to better estimate the robot's pose.</li>
</ol>
<p>The trajectories for measuring the error were generated using our best performing navigation model. The model was evaluated over 3 runs, each consisting of 5 consecutive waypoints over the 3 room configurations. When an episode finishes we manually measure the difference between the reality position of robot and the pose estimate coming from localization. In total we did 90 evaluations across the two localization approaches.</p>
<p>Depth Prediction Network Details</p>
<p>The depth prediction network uses the fully pruned version of FastDepth [36]. The image is center cropped and rescaled to 256 by 256 before being fed into the network. FastDepth was trained on the NYU-Depth and evaluated on the NYU Depth V2 dataset [37] using ground truth from a Microsoft Kinect. FastDepth uses a pruned MobileNet UNet style architecture for maximum speed. We use the pre-trained network from [36] with no modifications. Figure 13 visualizes the 'sliding' behavior that we discovered during experimentation which leads to learned agent models that can effectively take shortcut paths around small obstacles. We hypothesize that this sliding behavior, and its   mismatch to reality lead to difficulties in generalizing to the real robot. This hypothesis is supported by our results, we find significantly higher correlation between simulation and reality when the sliding behavior in simulation is disabled.</p>
<p>'Cheating by Sliding' Issue Details</p>
<p>Additional Experiment Analysis</p>
<p>Granularity of SRCC Aggregation</p>
<p>As described in the main paper, an important question is at what granularity to aggregate simulation and reality performance metrics for computing the SRCC value. Figure 14 plots the correlation at three different granularities: 1) a particular agent, 2) a particular environment, 3) a particular evaluation path, in each column respectively. For each column, we show results with sliding off (higher correlation) and sliding on (lower correlation). We see that aggregation across trials and at the level of paths (rightmost column) does not lead to meaningful correlative patterns, and consequently the SRCC value is low. Aggregation across paths and at the level of the test environment configuration (middle column) exhibits higher correlation but with large error bars on the measured SPL values in both reality and simulation. Finally, aggregation across paths and environment configurations, at the level of agent models (left column) exhibits the highest correlation.</p>
<p>SRCC Grid Search</p>
<p>We perform a grid search over various test-sim parameters as shown in Figures 15 to 18. Each of the figures shows Figure 12: We measure the localization error between the robot's position and target position using a measuring tape. Finally, the path used for SPL is simply the straight line between these two points Figure 13: We visualize the 'sliding' behavior that leads to 'cheating' by learned agent models. At time t an agent positioned at s t attempts a forward step that would place the agent within non-navigable space (black point within white region). The navigation mesh implementation in Habitat-Sim retrieves the closest point in navigable space (gray area) within a fixed radius: s t+1 . Therefore the agent effectively takes a sliding path around the obstacle even though the SPL path length for evaluation is computed from s t to s t+1 . results as we vary the actuation noise multiplier by 0.1 increments from 0 to 1. Figure 15 shows SPL SRCC results with sliding turned off and Figure 16 shows results with sliding on. We see that SPL SRCC across agents generally trends higher as the actuation noise is decreased with sliding turned off. This result is different from when sliding is turned on as in Figure 16. With sliding, the overall SPL SRCC is lower, but it peaks at an actuation noise of 0.4 in this setup. These behaviors are also seen when comparing success SRCC as seen in Figures 17 and 18.</p>
<p>S t S t</p>
<p>Note that in our experiments, introducing actuation noise into test-sim did not lead to a better SRCC. For modeling the actuation noise in the simulator we used data from [18] and used a Gaussian distribution to fit a model on the data. We hypothesize that the issue may be due to the simplicity of the noise model or due to differences between the robotic platform and environment used to obtain the noise model compared to our evaluation environment and robot platform.</p>
<p>In addition to these summary SRCC plots, Table 2 reports results for all the models that we tested across the easy, medium and hard evaluation environment configurations for different sliding and training simulation actuation noise settings. SPL values are reported for both testing under the simulation parameters used in the Habitat Challenge 2019 [14] (Chall-Sim SPL: sliding on, no actuation noise) and with optimized simulation parameters guided by our SRCC experiments (Test-Sim SPL: sliding off, 0.  Figure 14: SRCC at different granularity. We consider real-performance predictability at three levels: for an agent, for an agent in a specific environment configuration, and for an agent on a specific path in a specific environment configuration. The Agent column computes SPL averaged over all environments, paths, and runs for a specific agent. The Environment column averages over paths and runs for each environment (3 points per agent, one per environment configuration). The Path column averages over runs (15 points per agent, one per path per environment configuration). From these SRCC scores, we can see that predictibility decays as the level of granularity increases.    Table 2: Performance of policies deployed on a real robot and in simulation across different room configurations.</p>
<p>Figure 1 :
1We measure the correlation between visual navigation performance in simulation and in reality by virtualizing reality and executing parallel experiments. (a): Navigation trajectory in a real space with obstacles. (b): virtualized replica in simulation. (c): we propose the Sim2Real Correlation Coefficient (SRCC) as a measure of simulation predictivity. By optimizing for SRCC, we arrive at simulation settings that are highly predictive of real-world performance.</p>
<h1>Figure 2 :</h1>
<p>2Which model are we testing? model = torch.load("my_model.pth") # Let's act! while not env.episode_over: action = model(observations) observations = env.step(action) Habitat-PyRobot Bridge (HaPy): sim2robot transfer is as simple as a one line code change (swapping Habitat-Sim-v0 and PyRobot-Locobot-v0).</p>
<p>Figure 3 :
3Overview of our approach for measuring Sim2Real correlation. From left to right: acquisition of real lab environment using Matterport cameras, the resulting 3D reconstruction, top-down view and frame for a simulated robot in Habitat. We carry out experiments for equivalent scenarios in reality and simulation to compute a Sim2Real Correlation Coefficient (SRCC) characterizing the simulator's predictivity of real-world performance.</p>
<p>Figure 4 :Figure 5 :
45Effect of actuation noise. The black line is a trajectory from an action sequence with perfect actuation. In red are trajectories from this sequence with actuation noise. Simulation vs. reality. Shading on the trajectory in reality represents uncertainty in the robot's location (±5cm).vastly different final locations.</p>
<p>Figure 6 :
6Top-down views of our testing environment with different difficulty settings: easy (left), medium (middle), and hard (right). White boxes are obstacles. The robot navigates sequentially through the waypoints A</p>
<p>Figure 7 :
7Train(sliding=off, noise=0.5) Depth − Train(sliding=off, noise=1.0) Predicted depth − Train(sliding=off, noise=0.5) Predicted depth − Train(sliding=off, noise=1.0) RGB − Train(sliding=off, noise=0.5) RGB − Train(sliding=off, noise=1.0) Depth − Train(sliding=on, noise=0) Predicted depth − Train(sliding=on, noise=0) RGB − Train(sliding=on, noise=0) SRCC SPL (left) and SRCC Succ (right) plots for AI Habitat Challenge 2019 test-sim setting. We note a relatively low correlation between real and simulated performance. entry in the RGB track of the Habitat Challenge 2019</p>
<p>Figure 8 :
8Sliding behavior leading to 'cheating' agents. At time t, the agent at s t executes a forward action, and slides along the wall to state s t+1 . The resulting straight-line path (used to calculate SPL) goes outside the environment. Gray denotes navigable space while white is non-navigable.</p>
<p>Figure 9 :
9Train(sliding=off, noise=0.5) Depth − Train(sliding=off, noise=1.0) Predicted depth − Train(sliding=off, noise=0.5) Predicted depth − Train(sliding=off, noise=1.0) RGB − Train(sliding=off, noise=0.5) RGB − Train(sliding=off, noise=1.0) Depth − Train(sliding=on, noise=0) Predicted depth − Train(sliding=on, noise=0) RGB − Train(sliding=on, noise=0) Optimized SRCC SPL (left) and SRCC Succ (right) scatterplots. Comparing withFigure 7we see improvements, indicating better predictivity of real-world performance.</p>
<p>Figure 10 :
10Habitat-PyRobot bridge (HaPy) extension integration with Habitat-API. We develop the HaPy module which interacts with the PyRobot[18] library. Developing this module allows us to deploy the tasks and baselines present in Habitat-API on LoCoBot. Architecture diagram of Habitat-API is taken from[1].</p>
<p>Figure 11 :
11Comparison of Hector SLAM and the extended kalman filter for localization.</p>
<p>Train(sliding=off, actuation noise=0.5) Depth − Train(sliding=off, actuation noise=1.0) Predicted depth − Train(sliding=off, actuation noise=0.5) Predicted depth − Train(sliding=off, actuation noise=1.0) RGB − Train(sliding=off, actuation noise=0.5) RGB − Train(sliding=off, actuation noise=1.0) Depth − Train(sliding=on, actuation noise=0) Predicted depth − Train(sliding=on, actuation noise=0) RGB − Train(sliding=on, actuation noise=0)</p>
<p>Figure 15 :
15Train(sliding=off, actuation noise=0.5) Depth − Train(sliding=off, actuation noise=1.0) Predicted depth − Train(sliding=off, actuation noise=0.5) Predicted depth − Train(sliding=off, actuation noise=1.0) RGB − Train(sliding=off, actuation noise=0.5) RGB − Train(sliding=off, actuation noise=1.0) Depth − Train(sliding=on, actuation noise=0) Predicted depth − Train(sliding=on, actuation noise=0) RGB − Train(sliding=on, actuation noise=0) SPL with sliding off across different actuation noise-multiplier.
Acknowledgments.We are grateful to Kalyan Alwala, Dhiraj Gandhi and Wojciech Galuba for their help and support.
Habitat: A Platform for Embodied AI Research. Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, Dhruv Batra, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)12Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A Platform for Embodied AI Research. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019. 1, 2, 4, 5, 7, 11, 12</p>
<p>Salt from My Attic. J A Shedd, Mosher PressJ.A. Shedd. Salt from My Attic. Mosher Press, 1928. 2</p>
<p>Decentralized distributed ppo: Solving pointgoal navigation. Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, Dhruv Batra, 27Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Decentralized distributed ppo: Solving pointgoal navigation, 2019. 2, 7</p>
<p>Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. Saurabh Gupta, James Davidson, Sergey Levine, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionSaurabh Gupta, James Davidson, Sergey Levine, Rahul Suk- thankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition, pages 2616-2625, 2017. 2</p>
<p>CARLA: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Proceedings of the 1st Annual Conference on Robot Learning. the 1st Annual Conference on Robot LearningAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning, pages 1-16, 2017. 2</p>
<p>Gibson env: Real-world perception for embodied agents. Fei Xia, Zhiyang Amir R Zamir, Alexander He, Jitendra Sax, Silvio Malik, Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition57Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env: Real-world per- ception for embodied agents. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9068-9079, 2018. 2, 5, 7</p>
<p>Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Simon Green, arXiv:1612.03801Víctor Valdés, Amir Sadik, et al. Deepmind lab. 2arXiv preprintCharles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich Küttler, Andrew Lefrancq, Si- mon Green, Víctor Valdés, Amir Sadik, et al. Deepmind lab. arXiv preprint arXiv:1612.03801, 2016. 2, 8</p>
<p>Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli Vanderbilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, Ali Farhadi, AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv. 2Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke Zhu, Ab- hinav Gupta, and Ali Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv, 2017. 2, 3, 8</p>
<p>CAD2RL: real singleimage flight without a single real image. Fereshteh Sadeghi, Sergey Levine, Robotics: Science and Systems XIII, Massachusetts Institute of Technology. Cambridge, Massachusetts, USA23Fereshteh Sadeghi and Sergey Levine. CAD2RL: real single- image flight without a single real image. In Robotics: Sci- ence and Systems XIII, Massachusetts Institute of Technology, Cambridge, Massachusetts, USA, July 12-16, 2017, 2017. 2, 3</p>
<p>MINOS: Multimodal indoor simulator for navigation in complex environments. Manolis Savva, Angel X Chang, Alexey Dosovitskiy, Thomas Funkhouser, Vladlen Koltun, arXiv:1712.039312Manolis Savva, Angel X. Chang, Alexey Dosovitskiy, Thomas Funkhouser, and Vladlen Koltun. MINOS: Mul- timodal indoor simulator for navigation in complex environ- ments. arXiv:1712.03931, 2017. 2, 8</p>
<p>Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton Van Den, Hengel, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition27Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: In- terpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674-3683, 2018. 2, 7</p>
<p>Embodied Question Answering. Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, Dhruv Batra, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied Question Answer- ing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2</p>
<p>Visual Learning and Embodied Agents in Simulation Environments @ ECCV 2018. Visual Learning and Embodied Agents in Simulation Envi- ronments @ ECCV 2018. https://eccv18-vlease. github.io/. 2</p>
<p>Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. CVPR 2019. 513Habitat Challenge 2019 @ Habitat Embodied Agents Workshop. CVPR 2019. https://aihabitat.org/ challenge/2019/. 2, 4, 5, 7, 13</p>
<p>CARLA autonomous driving challenge @ CVPR 2019. CARLA autonomous driving challenge @ CVPR 2019. https://carlachallenge.org/. 2</p>
<p>. Joel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J Bentley, Samuel Bernard, Guillaume Beslon, David M Bryson, Patryk Chrabaszcz ; Stephanie, Antoine Forrest, Christian Frénoy, Gagné, K Le Leni, Laura M Goff, Babak Grabowski, Frank Hodjat, Laurent Hutter, Carole Keller, Knibbe, Peter KrcahNick Cheney, Antoine Cully, Stéphane Doncieux, Fred C. Dyer, Kai Olav Ellefsen, Robert Feldt, Stephan FischerRichard EJoel Lehman, Jeff Clune, Dusan Misevic, Christoph Adami, Lee Altenberg, Julie Beaulieu, Peter J. Bentley, Samuel Bernard, Guillaume Beslon, David M. Bryson, Patryk Chrabaszcz, Nick Cheney, Antoine Cully, Stéphane Don- cieux, Fred C. Dyer, Kai Olav Ellefsen, Robert Feldt, Stephan Fischer, Stephanie Forrest, Antoine Frénoy, Christian Gagné, Leni K. Le Goff, Laura M. Grabowski, Babak Hodjat, Frank Hutter, Laurent Keller, Carole Knibbe, Peter Krcah, Richard E.</p>
<p>. Hod Lenski, Robert Lipson, Carlos Maccurdy, Risto Maestre, Sara Miikkulainen, David E Mitri, Jean-Baptiste Moriarty, Anh Mouret, Charles Nguyen, Marc Ofria, David P Parizeau, Robert T Parsons, William F Pennock, Thomas S Punch, Marc Ray, Eric Schoenauer, Shulte, Karl Sims, Kenneth OLenski, Hod Lipson, Robert MacCurdy, Carlos Maestre, Risto Miikkulainen, Sara Mitri, David E. Moriarty, Jean-Baptiste Mouret, Anh Nguyen, Charles Ofria, Marc Parizeau, David P. Parsons, Robert T. Pennock, William F. Punch, Thomas S. Ray, Marc Schoenauer, Eric Shulte, Karl Sims, Kenneth O.</p>
<p>The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities. François Stanley, Danesh Taddei, Simon Tarapore, Westley Thibault, Richard Weimer, Jason Watson, Yosinksi, arXiv:1803.03453Stanley, François Taddei, Danesh Tarapore, Simon Thibault, Westley Weimer, Richard Watson, and Jason Yosinksi. The surprising creativity of digital evolution: A collection of anec- dotes from the evolutionary computation and artificial life research communities. arXiv:1803.03453, 2018. 2</p>
<p>Emergent tool use from multi-agent autocurricula. Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob Mcgrew, Igor Mordatch, arXiv:1909.07528arXiv preprintBowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and Igor Mordatch. Emer- gent tool use from multi-agent autocurricula. arXiv preprint arXiv:1909.07528, 2019. 2</p>
<p>Pyrobot: An open-source robotics framework for research and benchmarking. Adithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, Abhinav Gupta, arXiv:1906.082361213arXiv preprintAdithyavairavan Murali, Tao Chen, Kalyan Vasudev Alwala, Dhiraj Gandhi, Lerrel Pinto, Saurabh Gupta, and Abhinav Gupta. Pyrobot: An open-source robotics framework for re- search and benchmarking. arXiv preprint arXiv:1906.08236, 2019. 2, 4, 5, 8, 11, 12, 13</p>
<p>On Evaluation of Embodied Navigation Agents. Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, arXiv:1807.0675737arXiv preprintPeter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik, Roozbeh Mottaghi, Manolis Savva, et al. On Evaluation of Embodied Navigation Agents. arXiv preprint arXiv:1807.06757, 2018. 3, 4, 7</p>
<p>Locobot: An open source low cost robot. Locobot: An open source low cost robot. https:// locobot-website.netlify.com/, 2019. 3</p>
<p>Target-driven visual navigation in indoor scenes using deep reinforcement learning. Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J Lim, Abhinav Gupta, Li Fei-Fei, Ali Farhadi, IEEE International Conference on Robotics and Automation (ICRA). Yuke Zhu, Roozbeh Mottaghi, Eric Kolve, Joseph J. Lim, Abhinav Gupta, Li Fei-Fei, and Ali Farhadi. Target-driven visual navigation in indoor scenes using deep reinforcement learning. 2017 IEEE International Conference on Robotics and Automation (ICRA), May 2017. 3</p>
<p>Rahul Sukthankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. Saurabh Gupta, James Davidson, Sergey Levine, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Saurabh Gupta, James Davidson, Sergey Levine, Rahul Suk- thankar, and Jitendra Malik. Cognitive mapping and planning for visual navigation. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jul 2017. 3</p>
<p>Splitnet: Sim2sim and task2task transfer for embodied visual navigation. Daniel Gordon, Abhishek Kadian, Devi Parikh, Judy Hoffman, Dhruv Batra, ICCV. Daniel Gordon, Abhishek Kadian, Devi Parikh, Judy Hoff- man, and Dhruv Batra. Splitnet: Sim2sim and task2task transfer for embodied visual navigation. In ICCV, 2019. 3</p>
<p>The darpa robotics challenge finals: results and perspectives. Eric Krotkov, Douglas Hackett, Larry Jackel, Michael Perschbacher, James Pippine, Jesse Strauss, Gill Pratt, Christopher Orlowski, Journal of Field Robotics. 342Eric Krotkov, Douglas Hackett, Larry Jackel, Michael Per- schbacher, James Pippine, Jesse Strauss, Gill Pratt, and Christopher Orlowski. The darpa robotics challenge finals: results and perspectives. Journal of Field Robotics, 34(2):229- 240, 2017. 3</p>
<p>Learning to fly by crashing. Dhiraj Gandhi, Lerrel Pinto, Abhinav Gupta, 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Dhiraj Gandhi, Lerrel Pinto, and Abhinav Gupta. Learning to fly by crashing. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3948-3955.</p>
<p>Driving policy transfer via modularity and abstraction. Matthias Müller, Alexey Dosovitskiy, Bernard Ghanem, Vladlen Koltun, arXiv:1804.09364arXiv preprintMatthias Müller, Alexey Dosovitskiy, Bernard Ghanem, and Vladlen Koltun. Driving policy transfer via modularity and abstraction. arXiv preprint arXiv:1804.09364, 2018. 3</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. Jie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, Vincent Vanhoucke, arXiv:1804.10332arXiv preprintJie Tan, Tingnan Zhang, Erwin Coumans, Atil Iscen, Yunfei Bai, Danijar Hafner, Steven Bohez, and Vincent Vanhoucke. Sim-to-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332, 2018. 3</p>
<p>Learning agile and dynamic motor skills for legged robots. Jemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, Marco Hutter, arXiv:1901.08652arXiv preprintJemin Hwangbo, Joonho Lee, Alexey Dosovitskiy, Dario Bellicoso, Vassilios Tsounis, Vladlen Koltun, and Marco Hutter. Learning agile and dynamic motor skills for legged robots. arXiv preprint arXiv:1901.08652, 2019. 3</p>
<p>Ros: an open-source robot operating system. Morgan Quigley, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy Leibs, Rob Wheeler, Andrew Y Ng, ICRA workshop on open source software. Kobe, Japan3Morgan Quigley, Ken Conley, Brian Gerkey, Josh Faust, Tully Foote, Jeremy Leibs, Rob Wheeler, and Andrew Y Ng. Ros: an open-source robot operating system. In ICRA workshop on open source software, volume 3, page 5. Kobe, Japan, 2009.</p>
<p>Benchmarking classic and learned navigation in complex 3D environments. Dmytro Mishkin, Alexey Dosovitskiy, Vladlen Koltun, arXiv:1901.10915Dmytro Mishkin, Alexey Dosovitskiy, and Vladlen Koltun. Benchmarking classic and learned navigation in complex 3D environments. arXiv:1901.10915, 2019. 4</p>
<p>To learn or not to learn: Analyzing the role of learning for navigation in virtual environments. Noriyuki Kojima, Jia Deng, arXiv:1907.11770Noriyuki Kojima and Jia Deng. To learn or not to learn: Analyzing the role of learning for navigation in virtual envi- ronments. arXiv:1907.11770, 2019. 4</p>
<p>A flexible and scalable slam system with full 3d motion estimation. S Kohlbrecher, J Meyer, O Stryk, U Klingauf, Proc. IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR). IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)IEEES. Kohlbrecher, J. Meyer, O. von Stryk, and U. Klingauf. A flexible and scalable slam system with full 3d motion esti- mation. In Proc. IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR). IEEE, November 2011.</p>
<p>Proximal policy optimization algorithms. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, abs/1707.06347CoRRJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Rad- ford, and Oleg Klimov. Proximal policy optimization algo- rithms. CoRR, abs/1707.06347, 2017. 7</p>
<p>Deep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.</p>
<p>Long short-term memory. Sepp Hochreiter, Jürgen Schmidhuber, Neural Computation. 98Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. 7</p>
<p>FastDepth: Fast Monocular Depth Estimation on Embedded Systems. Diana Wofk, Fangchang Ma, Yang , Tien-Ju Karaman, Sertac, Vivienne Sze, IEEE International Conference on Robotics and Automation (ICRA). 711Wofk, Diana and Ma, Fangchang and Yang, Tien-Ju and Kara- man, Sertac and Sze, Vivienne. FastDepth: Fast Monocular Depth Estimation on Embedded Systems. In IEEE Interna- tional Conference on Robotics and Automation (ICRA), 2019. 7, 11</p>
<p>Indoor segmentation and support inference from rgbd images. Derek Pushmeet Kohli Nathan Silberman, Rob Hoiem, Fergus, ECCV. 11Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 11</p>
<p>Depth − Train(sliding=off, actuation noise=0.5) Depth − Train(sliding=off, actuation noise=1.0) Predicted depth − Train(sliding=off, actuation noise=0.5) Predicted depth − Train(sliding=off. actuation noise=1.0)Depth − Train(sliding=off, actuation noise=0.5) Depth − Train(sliding=off, actuation noise=1.0) Predicted depth − Train(sliding=off, actuation noise=0.5) Predicted depth − Train(sliding=off, actuation noise=1.0)</p>
<p>. Rgb − Train, sliding=off, actuation noise=0.5)RGB − Train(sliding=off, actuation noise=0.5)</p>
<p>RGB − Train(sliding=off, actuation noise=1.0) Depth − Train(sliding=on, actuation noise=0) Predicted depth − Train(sliding=on. 0RGB − Train(sliding=off, actuation noise=1.0) Depth − Train(sliding=on, actuation noise=0) Predicted depth − Train(sliding=on, actuation noise=0)</p>
<p>. Rgb − Train, sliding=on, actuation noise=0RGB − Train(sliding=on, actuation noise=0)</p>
<p>Figure 16: SPL with sliding on across different actuation noise-multiplier. Figure 16: SPL with sliding on across different actuation noise-multiplier.</p>
<p>Depth − Train(sliding=off, actuation noise=0.5) Depth − Train(sliding=off, actuation noise=1.0) Predicted depth − Train(sliding=off, actuation noise=0.5) Predicted depth − Train(sliding=off. actuation noise=1.0)Depth − Train(sliding=off, actuation noise=0.5) Depth − Train(sliding=off, actuation noise=1.0) Predicted depth − Train(sliding=off, actuation noise=0.5) Predicted depth − Train(sliding=off, actuation noise=1.0)</p>
<p>. Rgb − Train, sliding=off, actuation noise=0.5)RGB − Train(sliding=off, actuation noise=0.5)</p>
<p>RGB − Train(sliding=off, actuation noise=1.0) Depth − Train(sliding=on, actuation noise=0) Predicted depth − Train(sliding=on. 0RGB − Train(sliding=off, actuation noise=1.0) Depth − Train(sliding=on, actuation noise=0) Predicted depth − Train(sliding=on, actuation noise=0)</p>
<p>. Rgb − Train, sliding=on, actuation noise=0RGB − Train(sliding=on, actuation noise=0)</p>
<p>Success with sliding off across different actuation noise-multiplier. Depth − Train(sliding=off, actuation noise=0.5) Depth − Train(sliding=off, actuation noise=1.0) Predicted depth − Train(sliding=off, actuation noise=0.5) Predicted depth − Train. Figure. 17sliding=off, actuation noise=1.0)Figure 17: Success with sliding off across different actuation noise-multiplier. Depth − Train(sliding=off, actuation noise=0.5) Depth − Train(sliding=off, actuation noise=1.0) Predicted depth − Train(sliding=off, actuation noise=0.5) Predicted depth − Train(sliding=off, actuation noise=1.0)</p>
<p>. Rgb − Train, sliding=off, actuation noise=0.5)RGB − Train(sliding=off, actuation noise=0.5)</p>
<p>RGB − Train(sliding=off, actuation noise=1.0) Depth − Train(sliding=on, actuation noise=0) Predicted depth − Train(sliding=on. 0RGB − Train(sliding=off, actuation noise=1.0) Depth − Train(sliding=on, actuation noise=0) Predicted depth − Train(sliding=on, actuation noise=0)</p>
<p>. Rgb − Train, sliding=on, actuation noise=0RGB − Train(sliding=on, actuation noise=0)</p>
<p>Success with sliding on across different actuation noise-multiplier. 18Figure 18: Success with sliding on across different actuation noise-multiplier.</p>            </div>
        </div>

    </div>
</body>
</html>
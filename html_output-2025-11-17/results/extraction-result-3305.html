<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3305 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3305</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3305</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-76.html">extraction-schema-76</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <p><strong>Paper ID:</strong> paper-346081161bdc8f18e2a4c4af7f51d35452b5cb01</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/346081161bdc8f18e2a4c4af7f51d35452b5cb01" target="_blank">Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies</a></p>
                <p><strong>Paper Venue:</strong> Transactions of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work introduces StrategyQA, a question answering benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy, and proposes a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts.</p>
                <p><strong>Paper Abstract:</strong> Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of ∼ 66%.</p>
                <p><strong>Cost:</strong> 0.018</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3305.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3305.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>RoBERTa*</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>RoBERTa* (RoBERTa fine-tuned on DROP, 20Q, BoolQ and then applied to STRATEGYQA)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A RoBERTa-based encoder model that the authors further fine-tuned on auxiliary reasoning datasets (DROP, TwentyQuestions, BoolQ) before training/evaluating on STRATEGYQA; used as the primary QA model in multiple configurations (no-context, retrieval-augmented, oracle).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>RoBERTa (fine-tuned variant, denoted RoBERTa*)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Encoder-only Transformer (RoBERTa) pre-trained as in Liu et al. (2019), then successively fine-tuned on DROP (for numerical/discrete reasoning), TwentyQuestions (commonsense boolean), and BoolQ (boolean QA) before being trained/evaluated on STRATEGYQA. Deployed in multiple input configurations: question-only (no context), question+retrieved paragraphs, decomposition-based retrieval, and oracle paragraph/decomposition settings.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['no-context (LM internalized knowledge / parametric reasoning)', 'retrieval-augmented QA (question-based retrieval via BM25 + RoBERTa)', 'decomposition-assisted retrieval (use of gold or predicted decomposition to form retrieval queries)', 'oracle-paragraph reasoning (model provided with gold evidence paragraphs)', 'oracle-decomposition + retrieval (gold decomposition used to retrieve or to supervise step answers)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>No-context: the model receives only the question and predicts the boolean answer relying on knowledge stored in parameters; Question-based retrieval: BM25 retrieves top-k paragraphs for the full question, concatenated to the question and fed to RoBERTa; Decomposition-based retrieval: separate BM25 queries for each decomposition step (gold or predicted) and concatenation of retrieved results; Oracle-paragraph setting: the model is given the gold evidence paragraphs directly; Oracle-decomposition: gold decomposition is used either to retrieve or to provide intermediate step supervision.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>The paper evaluates a diversity of reasoning styles applied to the same underlying model (parametric/no-context reasoning, retrieval-augmented with question queries, decomposition-guided retrieval, and oracle evidence/decomposition settings). Diversity is achieved by varying input conditioning (no context vs retrieved vs gold evidence) and by generating decompositions.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>STRATEGYQA</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>A boolean question-answering benchmark of 2,780 short, implicit multi-step (strategy) questions over Wikipedia; requires inferring implicit decomposition steps (strategies), retrieving per-step evidence paragraphs, and performing operations over step answers to produce a yes/no output.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported test accuracies (as given in the paper): No-context (question only) RoBERTa*: 63.6% accuracy. Question-based retrieval (BM25 top-10 for full question) with RoBERTa*: 63.6% accuracy. Decomposition-based retrieval using gold decomposition (BM25 per-step): 62.0% accuracy. Decomposition-based retrieval using predicted decomposition: 61.7% accuracy. Oracle with gold paragraphs provided (no retrieval): 70.7% accuracy. Oracle using both gold decomposition and retrieval/gold evidence: 72.0% accuracy. (Human accuracy reported for comparison: 87%). Majority baseline referenced as ~53.9% in text (used to illustrate gain from fine-tuning/no-context).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Key comparisons: (1) No-context RoBERTa* (parametric knowledge) reaches 63.6%, demonstrating that large pre-trained LMs fine-tuned on related datasets can answer many strategy questions without explicit retrieval. (2) Question-based retrieval does not improve over no-context (both 63.6%), showing that BM25 retrieval of top-10 question results provided little extra benefit in practice. (3) Using gold evidence paragraphs (oracle) raises accuracy to 70.7%, indicating that when correct evidence is available, the model can better reason over it. (4) Using gold decomposition plus retrieval/oracle yields the best reported accuracy (72.0%), showing utility of decompositions if evidence is found. (5) Predicted decompositions (BART_DECOMP) used for retrieval perform worse than gold decomposition and do not improve over question-based retrieval (predicted-decomp retrieval accuracy 61.7%).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>1) A single underlying LM (RoBERTa*) can be used with multiple reasoning styles (parametric no-context, retrieval-augmented, decomposition-guided). 2) Fine-tuning on auxiliary reasoning datasets substantially improves performance (RoBERTa* > plain RoBERTa in IR settings). 3) Retrieval quality is a major bottleneck: oracle evidence substantially improves accuracy (70.7%), but automatic BM25 retrieval (even when guided by predicted decompositions) does not reliably deliver those paragraphs and thus often fails to outperform no-context. 4) Gold decompositions are useful for retrieval and final performance, but predicted decompositions (as generated by their BART model) are imperfect and can harm retrieval/accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Decomposition-guided retrieval did not reliably outperform simple question-based retrieval when using predicted decompositions; predicted decomposition-based retrieval yielded lower accuracy (61.7%) than question-based retrieval (63.6%). Additionally, gold decomposition paired with standard BM25 retrieval still underperformed the oracle-paragraph setup unless gold paragraphs were provided, indicating that decomposition alone cannot compensate for weak retrieval. The authors note BM25's shallow pattern matching (synonym mismatches, reference tokens) as reasons for retrieval failures.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3305.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3305.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BART_DECOMP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BART_DECOMP (BART-based decomposition generator)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A sequence-to-sequence model (BART) fine-tuned on STRATEGYQA decompositions to generate a step-by-step decomposition (strategy) for each implicit multi-step question.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BART (fine-tuned for decomposition generation)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Based on BART (Lewis et al., 2020), a denoising seq2seq Transformer; fine-tuned on annotated STRATEGYQA decompositions to output a sequence of single-step questions (a decomposition) given the original strategy question.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['learned decomposition generation (seq2seq)', 'decomposition-then-retrieve pipeline (predicted decompositions used to query IR)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>BART_DECOMP is trained to map an implicit strategy question to a sequence of explicit single-step sub-questions; the predicted sub-questions can then be used as separate queries for retrieval (decomposition-based retrieval) or for downstream per-step QA.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>This is a single decomposition-generation style (seq2seq) but it is intended to enable diverse downstream reasoning by exposing step-level sub-questions; in practice the generated decompositions are grammatically good but sometimes apply incorrect strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Decomposition of STRATEGYQA questions</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Generate an ordered list of single-step questions whose execution (retrieval + operations) yields the final yes/no answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Quantitative accuracy for decompositions is not provided as a single number; qualitative assessment: BART_DECOMP produces grammatical and well-structured decompositions, but often applies incorrect strategies (example given where a lifeboat 'dive' strategy was wrongly used). Using predicted decompositions for retrieval resulted in end-to-end QA accuracy 61.7% (lower than question-based retrieval and no-context).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Compared to gold (human) decompositions, predicted decompositions are imperfect: gold-decomposition retrieval gave better retrieval recall and, when combined with good retrieval or gold paragraphs, led to higher QA accuracy (oracle settings). Predicted decompositions did not improve retrieval/QA over the baseline question retrieval.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Seq2seq decomposition generation is feasible and yields natural-looking strategies, but generated strategies can be wrong and therefore hurt retrieval/QA. Improving decomposition quality (strategy correctness) is necessary before decomposition-based retrieval reliably helps end-to-end performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Although BART_DECOMP outputs grammatical decompositions, the paper reports concrete failure cases where the generated strategy was not appropriate for the question (e.g., hypothesizing lifeboats dive), and end-to-end predicted-decomposition retrieval performed worse than question-based retrieval (61.7% vs 63.6%).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3305.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3305.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>BM25 retrieval (question vs decomposition)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>BM25 retrieval (BM25 used for question-based and decomposition-based retrieval)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An off-the-shelf lexical retrieval method (BM25) used to retrieve top-k paragraphs from a Wikipedia index for either the full question (question-based retrieval) or for each decomposition step (decomposition-based retrieval).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Okapi at TREC-3</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>BM25 IR (used as retrieval component)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Standard bag-of-words retrieval (BM25) applied to an indexed dump of Wikipedia paragraphs (Cirrus dump); stop words removed from queries; top-10 results concatenated and truncated to 512 tokens for RoBERTa input.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['lexical retrieval for context grounding', 'question-based retrieval (single query)', 'decomposition-based retrieval (one query per decomposition step; concatenate top-k per step)']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Question-based retrieval: use the original strategy question as a single BM25 query and take top-10 paragraphs. Decomposition-based retrieval: use each decomposition step (gold or predicted) as a separate BM25 query and aggregate top-k results across steps before feeding them to the QA model.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>BM25 itself is a single lexical retrieval method; paper contrasts its use with different query formulations (question vs per-step decomposition) to test whether decomposition queries find better evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Retrieval subtask within STRATEGYQA pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Retrieve evidence paragraphs from Wikipedia that answer individual decomposition steps or provide context needed to answer the overall strategy question.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Reported observations: question-based retrieval + RoBERTa* => 63.6% QA accuracy; decomposition-based retrieval using gold decomposition + BM25 => 62.0% QA accuracy (worse than question-based in reported numbers for their BM25 setup), predicted-decomposition retrieval => 61.7%. Retrieval quality measured with Recall@10 (fraction of gold paragraphs retrieved) is low for automatic methods; human retrieval Recall@10 was 0.586 (with decomposition) and 0.513 (without). The paper reports that gold-decomposition-based retrieval substantially outperforms question-based retrieval in terms of Recall@10 (for gold decomposition queries vs full-question queries), but automatic BM25 queries still miss many gold paragraphs due to synonymy and surface mismatch.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Gold decomposition + retrieval (or oracle paragraphs) substantially helps when gold paragraphs are available, but BM25-based decomposition queries do not reliably find the gold paragraphs and may underperform question-based queries in practice. Human retrieval with decomposition is better than without decomposition (Recall@10 0.586 vs 0.513).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>BM25's shallow lexical matching is a major bottleneck for STRATEGYQA because evidence and decomposition steps intentionally have low lexical overlap; decomposition-derived queries help when gold decomposition is available, but improvements rely on better retrieval (semantic/neural retrieval) rather than on BM25 alone.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Automatic decomposition-based BM25 retrieval did not improve QA accuracy versus question-based BM25 retrieval; predicted decompositions made retrieval worse in some cases. The authors attribute many failures to synonym mismatches and placeholder/reference tokens in decomposition steps.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3305.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3305.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse reasoning methods versus similar reasoning styles to solve reasoning problems, including details of the reasoning methods, tasks, model types, and performance comparisons.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Human baseline</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Human experts (annotators) answering STRATEGYQA with access to Wikipedia and optional gold decomposition</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Human experts (the paper's authors) answered sampled STRATEGYQA questions using web/Wikipedia search and optionally consulting the annotated decomposition; used as an upper reference for QA and to measure human use of decomposition and retrieval effort.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Human experts (authors)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Experienced human annotators provided answers to STRATEGYQA questions with access to Wikipedia articles and an option to reveal the annotated decomposition; they reported explanations, number of searches, and whether they used the decomposition.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods</strong></td>
                            <td>['human-in-the-loop reasoning combining background knowledge and targeted document retrieval', 'strategy inference (inferring decomposition steps) and selective search']</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_methods_description</strong></td>
                            <td>Humans inferred implicit strategies from the short questions, performed web/Wikipedia searches for intermediate facts, and composed intermediate answers to yield a final yes/no. They often used background knowledge to avoid searches for some intermediate steps.</td>
                        </tr>
                        <tr>
                            <td><strong>diversity_of_methods</strong></td>
                            <td>Humans naturally used diverse reasoning strategies (temporal, biological, definitional, set membership, etc.) depending on the question; paper reports humans inferred strategies and used decompositions in many cases.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>STRATEGYQA (human evaluation)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Same as the dataset task: infer decompositions, find evidence in Wikipedia paragraphs, and derive a yes/no answer.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_by_method</strong></td>
                            <td>Human experts achieved 87% accuracy on a 100-question sample. Strategy match (their provided explanations matched annotated decomposition) was 86%. Average number of searches per question was 1.25; humans reported using the decomposition for 14% of answered questions (self-reported).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_of_methods</strong></td>
                            <td>Humans outperform all automatic baselines (87% vs best automatic Oracle-like 72.0% and best realistic ~63–66%). Humans also retrieve evidence more effectively than BM25: human Recall@10 was 0.586 with decomposition vs 0.513 without, both higher than BM25 retrieval reported by automatic systems.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Humans can infer strategies from short implicit questions and find the evidence to answer them at high accuracy with few searches; providing decompositions aids human retrieval. This gap highlights retrieval and decomposition generation as key challenges for current models.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Humans still fail when evidence is difficult to find (main failure mode ~10%) or when questions are ambiguous (~3%), indicating some questions are legitimately hard even for humans given Wikipedia coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies', 'publication_date_yy_mm': '2021-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>RoBERTa: A Robustly Optimized BERT Pretraining Approach <em>(Rating: 2)</em></li>
                <li>BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension <em>(Rating: 2)</em></li>
                <li>Break it down: A question understanding benchmark <em>(Rating: 2)</em></li>
                <li>Unsupervised question decomposition for question answering <em>(Rating: 2)</em></li>
                <li>HotpotQA: A dataset for diverse, explainable multi-hop question answering <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3305",
    "paper_id": "paper-346081161bdc8f18e2a4c4af7f51d35452b5cb01",
    "extraction_schema_id": "extraction-schema-76",
    "extracted_data": [
        {
            "name_short": "RoBERTa*",
            "name_full": "RoBERTa* (RoBERTa fine-tuned on DROP, 20Q, BoolQ and then applied to STRATEGYQA)",
            "brief_description": "A RoBERTa-based encoder model that the authors further fine-tuned on auxiliary reasoning datasets (DROP, TwentyQuestions, BoolQ) before training/evaluating on STRATEGYQA; used as the primary QA model in multiple configurations (no-context, retrieval-augmented, oracle).",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "RoBERTa (fine-tuned variant, denoted RoBERTa*)",
            "model_description": "Encoder-only Transformer (RoBERTa) pre-trained as in Liu et al. (2019), then successively fine-tuned on DROP (for numerical/discrete reasoning), TwentyQuestions (commonsense boolean), and BoolQ (boolean QA) before being trained/evaluated on STRATEGYQA. Deployed in multiple input configurations: question-only (no context), question+retrieved paragraphs, decomposition-based retrieval, and oracle paragraph/decomposition settings.",
            "model_size": null,
            "reasoning_methods": [
                "no-context (LM internalized knowledge / parametric reasoning)",
                "retrieval-augmented QA (question-based retrieval via BM25 + RoBERTa)",
                "decomposition-assisted retrieval (use of gold or predicted decomposition to form retrieval queries)",
                "oracle-paragraph reasoning (model provided with gold evidence paragraphs)",
                "oracle-decomposition + retrieval (gold decomposition used to retrieve or to supervise step answers)"
            ],
            "reasoning_methods_description": "No-context: the model receives only the question and predicts the boolean answer relying on knowledge stored in parameters; Question-based retrieval: BM25 retrieves top-k paragraphs for the full question, concatenated to the question and fed to RoBERTa; Decomposition-based retrieval: separate BM25 queries for each decomposition step (gold or predicted) and concatenation of retrieved results; Oracle-paragraph setting: the model is given the gold evidence paragraphs directly; Oracle-decomposition: gold decomposition is used either to retrieve or to provide intermediate step supervision.",
            "diversity_of_methods": "The paper evaluates a diversity of reasoning styles applied to the same underlying model (parametric/no-context reasoning, retrieval-augmented with question queries, decomposition-guided retrieval, and oracle evidence/decomposition settings). Diversity is achieved by varying input conditioning (no context vs retrieved vs gold evidence) and by generating decompositions.",
            "reasoning_task_name": "STRATEGYQA",
            "reasoning_task_description": "A boolean question-answering benchmark of 2,780 short, implicit multi-step (strategy) questions over Wikipedia; requires inferring implicit decomposition steps (strategies), retrieving per-step evidence paragraphs, and performing operations over step answers to produce a yes/no output.",
            "performance_by_method": "Reported test accuracies (as given in the paper): No-context (question only) RoBERTa*: 63.6% accuracy. Question-based retrieval (BM25 top-10 for full question) with RoBERTa*: 63.6% accuracy. Decomposition-based retrieval using gold decomposition (BM25 per-step): 62.0% accuracy. Decomposition-based retrieval using predicted decomposition: 61.7% accuracy. Oracle with gold paragraphs provided (no retrieval): 70.7% accuracy. Oracle using both gold decomposition and retrieval/gold evidence: 72.0% accuracy. (Human accuracy reported for comparison: 87%). Majority baseline referenced as ~53.9% in text (used to illustrate gain from fine-tuning/no-context).",
            "comparison_of_methods": "Key comparisons: (1) No-context RoBERTa* (parametric knowledge) reaches 63.6%, demonstrating that large pre-trained LMs fine-tuned on related datasets can answer many strategy questions without explicit retrieval. (2) Question-based retrieval does not improve over no-context (both 63.6%), showing that BM25 retrieval of top-10 question results provided little extra benefit in practice. (3) Using gold evidence paragraphs (oracle) raises accuracy to 70.7%, indicating that when correct evidence is available, the model can better reason over it. (4) Using gold decomposition plus retrieval/oracle yields the best reported accuracy (72.0%), showing utility of decompositions if evidence is found. (5) Predicted decompositions (BART_DECOMP) used for retrieval perform worse than gold decomposition and do not improve over question-based retrieval (predicted-decomp retrieval accuracy 61.7%).",
            "key_findings": "1) A single underlying LM (RoBERTa*) can be used with multiple reasoning styles (parametric no-context, retrieval-augmented, decomposition-guided). 2) Fine-tuning on auxiliary reasoning datasets substantially improves performance (RoBERTa* &gt; plain RoBERTa in IR settings). 3) Retrieval quality is a major bottleneck: oracle evidence substantially improves accuracy (70.7%), but automatic BM25 retrieval (even when guided by predicted decompositions) does not reliably deliver those paragraphs and thus often fails to outperform no-context. 4) Gold decompositions are useful for retrieval and final performance, but predicted decompositions (as generated by their BART model) are imperfect and can harm retrieval/accuracy.",
            "counter_examples_or_negative_results": "Decomposition-guided retrieval did not reliably outperform simple question-based retrieval when using predicted decompositions; predicted decomposition-based retrieval yielded lower accuracy (61.7%) than question-based retrieval (63.6%). Additionally, gold decomposition paired with standard BM25 retrieval still underperformed the oracle-paragraph setup unless gold paragraphs were provided, indicating that decomposition alone cannot compensate for weak retrieval. The authors note BM25's shallow pattern matching (synonym mismatches, reference tokens) as reasons for retrieval failures.",
            "uuid": "e3305.0",
            "source_info": {
                "paper_title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "BART_DECOMP",
            "name_full": "BART_DECOMP (BART-based decomposition generator)",
            "brief_description": "A sequence-to-sequence model (BART) fine-tuned on STRATEGYQA decompositions to generate a step-by-step decomposition (strategy) for each implicit multi-step question.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "BART (fine-tuned for decomposition generation)",
            "model_description": "Based on BART (Lewis et al., 2020), a denoising seq2seq Transformer; fine-tuned on annotated STRATEGYQA decompositions to output a sequence of single-step questions (a decomposition) given the original strategy question.",
            "model_size": null,
            "reasoning_methods": [
                "learned decomposition generation (seq2seq)",
                "decomposition-then-retrieve pipeline (predicted decompositions used to query IR)"
            ],
            "reasoning_methods_description": "BART_DECOMP is trained to map an implicit strategy question to a sequence of explicit single-step sub-questions; the predicted sub-questions can then be used as separate queries for retrieval (decomposition-based retrieval) or for downstream per-step QA.",
            "diversity_of_methods": "This is a single decomposition-generation style (seq2seq) but it is intended to enable diverse downstream reasoning by exposing step-level sub-questions; in practice the generated decompositions are grammatically good but sometimes apply incorrect strategies.",
            "reasoning_task_name": "Decomposition of STRATEGYQA questions",
            "reasoning_task_description": "Generate an ordered list of single-step questions whose execution (retrieval + operations) yields the final yes/no answer.",
            "performance_by_method": "Quantitative accuracy for decompositions is not provided as a single number; qualitative assessment: BART_DECOMP produces grammatical and well-structured decompositions, but often applies incorrect strategies (example given where a lifeboat 'dive' strategy was wrongly used). Using predicted decompositions for retrieval resulted in end-to-end QA accuracy 61.7% (lower than question-based retrieval and no-context).",
            "comparison_of_methods": "Compared to gold (human) decompositions, predicted decompositions are imperfect: gold-decomposition retrieval gave better retrieval recall and, when combined with good retrieval or gold paragraphs, led to higher QA accuracy (oracle settings). Predicted decompositions did not improve retrieval/QA over the baseline question retrieval.",
            "key_findings": "Seq2seq decomposition generation is feasible and yields natural-looking strategies, but generated strategies can be wrong and therefore hurt retrieval/QA. Improving decomposition quality (strategy correctness) is necessary before decomposition-based retrieval reliably helps end-to-end performance.",
            "counter_examples_or_negative_results": "Although BART_DECOMP outputs grammatical decompositions, the paper reports concrete failure cases where the generated strategy was not appropriate for the question (e.g., hypothesizing lifeboats dive), and end-to-end predicted-decomposition retrieval performed worse than question-based retrieval (61.7% vs 63.6%).",
            "uuid": "e3305.1",
            "source_info": {
                "paper_title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "BM25 retrieval (question vs decomposition)",
            "name_full": "BM25 retrieval (BM25 used for question-based and decomposition-based retrieval)",
            "brief_description": "An off-the-shelf lexical retrieval method (BM25) used to retrieve top-k paragraphs from a Wikipedia index for either the full question (question-based retrieval) or for each decomposition step (decomposition-based retrieval).",
            "citation_title": "Okapi at TREC-3",
            "mention_or_use": "use",
            "model_name": "BM25 IR (used as retrieval component)",
            "model_description": "Standard bag-of-words retrieval (BM25) applied to an indexed dump of Wikipedia paragraphs (Cirrus dump); stop words removed from queries; top-10 results concatenated and truncated to 512 tokens for RoBERTa input.",
            "model_size": null,
            "reasoning_methods": [
                "lexical retrieval for context grounding",
                "question-based retrieval (single query)",
                "decomposition-based retrieval (one query per decomposition step; concatenate top-k per step)"
            ],
            "reasoning_methods_description": "Question-based retrieval: use the original strategy question as a single BM25 query and take top-10 paragraphs. Decomposition-based retrieval: use each decomposition step (gold or predicted) as a separate BM25 query and aggregate top-k results across steps before feeding them to the QA model.",
            "diversity_of_methods": "BM25 itself is a single lexical retrieval method; paper contrasts its use with different query formulations (question vs per-step decomposition) to test whether decomposition queries find better evidence.",
            "reasoning_task_name": "Retrieval subtask within STRATEGYQA pipeline",
            "reasoning_task_description": "Retrieve evidence paragraphs from Wikipedia that answer individual decomposition steps or provide context needed to answer the overall strategy question.",
            "performance_by_method": "Reported observations: question-based retrieval + RoBERTa* =&gt; 63.6% QA accuracy; decomposition-based retrieval using gold decomposition + BM25 =&gt; 62.0% QA accuracy (worse than question-based in reported numbers for their BM25 setup), predicted-decomposition retrieval =&gt; 61.7%. Retrieval quality measured with Recall@10 (fraction of gold paragraphs retrieved) is low for automatic methods; human retrieval Recall@10 was 0.586 (with decomposition) and 0.513 (without). The paper reports that gold-decomposition-based retrieval substantially outperforms question-based retrieval in terms of Recall@10 (for gold decomposition queries vs full-question queries), but automatic BM25 queries still miss many gold paragraphs due to synonymy and surface mismatch.",
            "comparison_of_methods": "Gold decomposition + retrieval (or oracle paragraphs) substantially helps when gold paragraphs are available, but BM25-based decomposition queries do not reliably find the gold paragraphs and may underperform question-based queries in practice. Human retrieval with decomposition is better than without decomposition (Recall@10 0.586 vs 0.513).",
            "key_findings": "BM25's shallow lexical matching is a major bottleneck for STRATEGYQA because evidence and decomposition steps intentionally have low lexical overlap; decomposition-derived queries help when gold decomposition is available, but improvements rely on better retrieval (semantic/neural retrieval) rather than on BM25 alone.",
            "counter_examples_or_negative_results": "Automatic decomposition-based BM25 retrieval did not improve QA accuracy versus question-based BM25 retrieval; predicted decompositions made retrieval worse in some cases. The authors attribute many failures to synonym mismatches and placeholder/reference tokens in decomposition steps.",
            "uuid": "e3305.2",
            "source_info": {
                "paper_title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
                "publication_date_yy_mm": "2021-01"
            }
        },
        {
            "name_short": "Human baseline",
            "name_full": "Human experts (annotators) answering STRATEGYQA with access to Wikipedia and optional gold decomposition",
            "brief_description": "Human experts (the paper's authors) answered sampled STRATEGYQA questions using web/Wikipedia search and optionally consulting the annotated decomposition; used as an upper reference for QA and to measure human use of decomposition and retrieval effort.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "Human experts (authors)",
            "model_description": "Experienced human annotators provided answers to STRATEGYQA questions with access to Wikipedia articles and an option to reveal the annotated decomposition; they reported explanations, number of searches, and whether they used the decomposition.",
            "model_size": null,
            "reasoning_methods": [
                "human-in-the-loop reasoning combining background knowledge and targeted document retrieval",
                "strategy inference (inferring decomposition steps) and selective search"
            ],
            "reasoning_methods_description": "Humans inferred implicit strategies from the short questions, performed web/Wikipedia searches for intermediate facts, and composed intermediate answers to yield a final yes/no. They often used background knowledge to avoid searches for some intermediate steps.",
            "diversity_of_methods": "Humans naturally used diverse reasoning strategies (temporal, biological, definitional, set membership, etc.) depending on the question; paper reports humans inferred strategies and used decompositions in many cases.",
            "reasoning_task_name": "STRATEGYQA (human evaluation)",
            "reasoning_task_description": "Same as the dataset task: infer decompositions, find evidence in Wikipedia paragraphs, and derive a yes/no answer.",
            "performance_by_method": "Human experts achieved 87% accuracy on a 100-question sample. Strategy match (their provided explanations matched annotated decomposition) was 86%. Average number of searches per question was 1.25; humans reported using the decomposition for 14% of answered questions (self-reported).",
            "comparison_of_methods": "Humans outperform all automatic baselines (87% vs best automatic Oracle-like 72.0% and best realistic ~63–66%). Humans also retrieve evidence more effectively than BM25: human Recall@10 was 0.586 with decomposition vs 0.513 without, both higher than BM25 retrieval reported by automatic systems.",
            "key_findings": "Humans can infer strategies from short implicit questions and find the evidence to answer them at high accuracy with few searches; providing decompositions aids human retrieval. This gap highlights retrieval and decomposition generation as key challenges for current models.",
            "counter_examples_or_negative_results": "Humans still fail when evidence is difficult to find (main failure mode ~10%) or when questions are ambiguous (~3%), indicating some questions are legitimately hard even for humans given Wikipedia coverage.",
            "uuid": "e3305.3",
            "source_info": {
                "paper_title": "Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies",
                "publication_date_yy_mm": "2021-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
            "rating": 2
        },
        {
            "paper_title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
            "rating": 2
        },
        {
            "paper_title": "Break it down: A question understanding benchmark",
            "rating": 2
        },
        {
            "paper_title": "Unsupervised question decomposition for question answering",
            "rating": 2
        },
        {
            "paper_title": "HotpotQA: A dataset for diverse, explainable multi-hop question answering",
            "rating": 1
        }
    ],
    "cost": 0.017761,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Did Aristotle Use a Laptop? <br> A Question Answering Benchmark with Implicit Reasoning Strategies</h1>
<p>Mor Geva ${ }^{1,2}$, Daniel Khashabi ${ }^{2}$, Elad Segal ${ }^{1}$, Tushar Khot ${ }^{2}$, Dan Roth $^{3}$, Jonathan Berant ${ }^{1,2}$<br>${ }^{1}$ Tel Aviv University ${ }^{2}$ Allen Institute for AI ${ }^{3}$ University of Pennsylvania<br>morgeva@mail.tau.ac.il, {danielk,tushark}@allenai.org, elad.segal@gmail.com, danroth@seas.upenn.edu<br>joberant@cs.tau.ac.il</p>
<h4>Abstract</h4>
<p>A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce STRATEGYQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in STRATEGYQA are short, topicdiverse, and cover a wide range of strategies. Empirically, we show that humans perform well ( $87 \%$ ) on this task, while our best baseline reaches an accuracy of $\sim 66 \%$.</p>
<h2>1 Introduction</h2>
<p>Developing models that successfully reason over multiple parts of their input has attracted substantial attention recently, leading to the creation of many multi-step reasoning Question Answering (QA) benchmarks (Welbl et al., 2018; Talmor and Berant, 2018; Khashabi et al., 2018; Yang et al., 2018; Dua et al., 2019; Suhr et al., 2019).</p>
<p>Commonly, the language of questions in such benchmarks explicitly describes the process for deriving the answer. For instance (Figure 1, Q2), the question "Was Aristotle alive when the laptop was
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Questions in StrategyQA (Q1) require implicit decomposition into reasoning steps (D), for which we annotate supporting evidence from Wikipedia (E). This is in contrast to multi-step questions that explicitly specify the reasoning process (Q2).
invented?" explicitly specifies the required reasoning steps. However, in real-life questions, reasoning is often implicit. For example, the question "Did Aristotle use a laptop?" (Q1) can be answered using the same steps, but the model must infer the strategy for answering the question - temporal comparison, in this case.</p>
<p>Answering implicit questions poses several challenges compared to answering their explicit counterparts. First, retrieving the context is difficult as there is little overlap between the question and its context (Figure 1, Q1 and 'E'). Moreover, questions tend to be short, lowering the possibility of the model exploiting shortcuts in the language of the question. In this work, we introduce StrategyQA, a boolean QA benchmark focusing on implicit multi-hop reasoning for strategy questions, where a strategy is the ability to infer from a question its atomic sub-questions. In contrast to previous benchmarks (Khot et al., 2020a; Yang et al., 2018), questions in STRATEGYQA are not limited to predefined decomposition patterns and cover a wide range of strategies that humans apply when answering questions.</p>
<p>Eliciting strategy questions using crowdsourcing is non-trivial. First, authoring such questions requires creativity. Past work often col-</p>
<p>lected multi-hop questions by showing workers an entire context, which led to limited creativity and high lexical overlap between questions and contexts and consequently to reasoning shortcuts (Khot et al., 2020a; Yang et al., 2018). An alternative approach, applied in Natural Questions (Kwiatkowski et al., 2019) and MS-MARCO (Nguyen et al., 2016), overcomes this by collecting real user questions. However, can we elicit creative questions independently of the context and without access to users?</p>
<p>Second, an important property in StrateGYQA is that questions entail diverse strategies. While the example in Figure 1 necessitates temporal reasoning, there are many possible strategies for answering questions (Table 1). We want a benchmark that exposes a broad range of strategies. But crowdsourcing workers often use repetitive patterns, which may limit question diversity.</p>
<p>To overcome these difficulties, we use the following techniques in our pipeline for eliciting strategy questions: (a) we prime crowd workers with random Wikipedia terms that serve as a minimal context to inspire their imagination and increase their creativity; (b) we use a large set of annotators to increase question diversity, limiting the number of questions a single annotator can write; and (c) we continuously train adversarial models during data collection, slowly increasing the difficulty in question writing and preventing recurring patterns (Bartolo et al., 2020).</p>
<p>Beyond the questions, as part of StrateGYQA, we annotated: (a) question decompositions: a sequence of steps sufficient for answering the question ('D' in Figure 1), and (b) evidence paragraphs: Wikipedia paragraphs that contain the answer to each decomposition step ('E' in Figure 1). StrategyQA is the first QA dataset to provide decompositions and evidence annotations for each individual step of the reasoning process.</p>
<p>Our analysis shows that STRATEGYQA necessitates reasoning on a wide variety of knowledge domains (physics, geography, etc.) and logical operations (e.g. number comparison). Moreover, experiments show that STRATEGYQA poses a combined challenge of retrieval and QA, and while humans perform well on these questions, even strong systems struggle to answer them.</p>
<p>In summary, the contributions of this work are:</p>
<ol>
<li>Defining strategy questions - a class of question requiring implicit multi-step reasoning.</li>
<li>StrategyQA, the first benchmark for implicit multi-step QA, that covers a diverse set of reasoning skills. StrategyQA consists of 2,780 questions, annotated with their decomposition and per-step evidence.</li>
<li>A novel annotation pipeline designed to elicit quality strategy questions, with minimal context for priming workers.
The dataset and codebase are publicly available at https://allenai.org/data/ strategyqa.</li>
</ol>
<h2>2 Strategy Questions</h2>
<h3>2.1 Desiderata</h3>
<p>We define strategy questions by characterizing their desired properties. Some properties, such as whether the question is answerable, also depend on the context used for answering the question. In this work, we assume this context is a corpus of documents, specifically, Wikipedia, which we assume provides correct content.</p>
<p>Multi-step Strategy questions are multi-step questions, that is, they comprise a sequence of single-step questions. A single-step question is either (a) a question that can be answered from a short text fragment in the corpus (e.g. steps 1 and 2 in Figure 1), or (b) a logical operation over answers from previous steps (e.g. step 3 in Figure 1). A strategy question should have at least two steps for deriving the answer. Example multiand single- step questions are provided in Table 2. We define the reasoning process structure in $\S 2.2$.</p>
<p>Feasible Questions should be answerable from paragraphs in the corpus. Specifically, for each reasoning step in the sequence, there should be sufficient evidence from the corpus to answer the question. For example, the answer to the question "Would a monocle be appropriate for a cyclop?" can be derived from paragraphs stating that cyclops have one eye and that a monocle is used by one eye at the time. This information is found in our corpus, Wikipedia, and thus the question is feasible. In contrast, the question "Does Justin Beiber own a Zune?" is not feasible, because answering it requires going through Beiber's belongings, and this information is unlikely to be found in Wikipedia.</p>
<p>Implicit A key property distinguishing strategy questions from prior multi-hop questions is their</p>
<table>
<thead>
<tr>
<th>Question</th>
<th>Implicit facts</th>
</tr>
</thead>
<tbody>
<tr>
<td>Can one spot helium? (No)</td>
<td>Helium is a gas, Helium is odorless, Helium is tasteless, Helium <br> has no color</td>
</tr>
<tr>
<td>Would Hades and Osiris hypothetically compete for real <br> estate in the Underworld? (Yes)</td>
<td>Hades was the Greek god of death and the Underworld. Osiris <br> was the Egyptian god of the Underworld.</td>
</tr>
<tr>
<td>Would a monocle be appropriate for a cyclop? (Yes)</td>
<td>Cyclops have one eye. A monocle helps one eye at a time.</td>
</tr>
<tr>
<td>Should a finished website have lorem ipsum para- <br> graphs? (No)</td>
<td>Lorem Ipsum paragraphs are meant to be temporary. Web de- <br> signers always remove lorem ipsum paragraphs before launch.</td>
</tr>
<tr>
<td>Is it normal to find parsley in multiple sections of the <br> grocery store? (Yes)</td>
<td>Parsley is available in both fresh and dry forms. Fresh parsley <br> must be kept cool. Dry parsley is a shelf stable product.</td>
</tr>
</tbody>
</table>
<p>Table 1: Example strategy questions and the implicit facts needed for answering them.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Question</th>
<th style="text-align: center;">MS</th>
<th style="text-align: center;">IM</th>
<th style="text-align: left;">Explanation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Was Barack Obama born in the <br> United States? (Yes)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;">The question explicitly states the required information for the answer - <br> the birth place of Barack Obama. The answer is likely to be found in a <br> single text fragment in Wikipedia.</td>
</tr>
<tr>
<td style="text-align: left;">Do cars use drinking water to <br> power their engine? (No)</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: left;">The question explicitly states the required information for the answer - <br> the liquid used to power car engines. The answer is likely to be found <br> in a single text fragment in Wikipedia.</td>
</tr>
<tr>
<td style="text-align: left;">Are sharks faster than crabs? (Yes)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: left;">The question explicitly states the required reasoning steps: 1) How fast <br> are sharks? 2) How fast are crabs? 3) Is #1 faster than #2?</td>
</tr>
<tr>
<td style="text-align: left;">Was Tom Cruise married to the fe- <br> male star of Inland Empire? (No)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;"></td>
<td style="text-align: left;">The question explicitly states the required reasoning steps: 1) Who is <br> the female star of Inland Empire? 2) Was Tom Cruise married to #2?</td>
</tr>
<tr>
<td style="text-align: left;">Are more watermelons grown in <br> Texas than in Antarctica? (Yes)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">The answer can be derived through geographical/botanical reasoning <br> that the climate in Antarctica does not support growth of watermelons.</td>
</tr>
<tr>
<td style="text-align: left;">Would someone with a nosebleed <br> benefit from Coca? (Yes)</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: center;">$\checkmark$</td>
<td style="text-align: left;">The answer can be derived through biological reasoning that Coca con- <br> stricts blood vessels, and therefore, serves to stop bleeding.</td>
</tr>
</tbody>
</table>
<p>Table 2: Example questions demonstrating the multi-step (MS) and implicit (IM) properties of strategy questions.
implicit nature. In explicit questions, each step in the reasoning process can be inferred from the language of the question directly. For example, in Figure 1, the first two questions are explicitly stated, one in the main clause and one in the adverbial clause. Conversely, reasoning steps in strategy questions require going beyond the language of the question. Due to language variability, a precise definition of implicit questions based on lexical overlap is elusive, but a good rule-of-thumb is the following: if the question decomposition can be written with a vocabulary limited to words from the questions, their inflections, and function words, then it is an explicit question. If new content words must be introduced to describe the reasoning process, the question is implicit. Examples for implicit and explicit questions are in Table 2.</p>
<p>Definite A type of questions we wish to avoid are non-definitive questions, such as "Are hamburgers considered a sandwich?" and "Does chocolate taste better than vanilla?" for which there is no clear answer. We would like to col-
lect questions where the answer is definitive or, at least, very likely, based on the corpus. E.g., consider the question "Does wood conduct electricity?". Although it is possible that a damp wood will conduct electricity, the answer is generally no.</p>
<p>To summarize, strategy questions are multi-step questions with implicit reasoning (a strategy) and a definitive answer that can be reached given a corpus. We limit ourselves to Boolean yes/no questions, which limits the output space, but lets us focus on the complexity of the questions, which is the key contribution. Example strategy questions are in Table 1, and examples that demonstrate the mentioned properties are in Table 2. Next (\$2.2), we describe additional structures annotated during data collection.</p>
<h3>2.2 Decomposing Strategy Questions</h3>
<p>Strategy questions involve complex reasoning that leads to a yes/no answer. To guide and evaluate the QA process, we annotate every example with a description of the expected reasoning process.</p>
<p>Prior work used rationales or supporting facts,</p>
<p>i.e., text snippets extracted from the context (DeYoung et al., 2020; Yang et al., 2018; Kwiatkowski et al., 2019; Khot et al., 2020a) as evidence for an answer. However, reasoning can rely on elements that are not explicitly expressed in the context. Moreover, answering a question based on relevant context does not imply that the model performs reasoning properly (Jiang and Bansal, 2019).</p>
<p>Inspired by recent work (Wolfson et al., 2020), we associate every question-answer pair with a strategy question decomposition. A decomposition of a question $q$ is a sequence of $n$ steps $\left\langle s^{(1)}, s^{(2)}, \ldots, s^{(n)}\right\rangle$ required for computing the answer to $q$. Each step $s^{(i)}$ corresponds to a singlestep question and may include special references, which are placeholders referring to the result of a previous step $s^{(j)}$. The last decomposition step (i.e. $s^{(n)}$ ) returns the final answer to the question. Table 3 shows decomposition examples.</p>
<p>Wolfson et al. (2020) targeted explicit multistep questions (first row in Table 3), where the decomposition is restricted to a small vocabulary derived almost entirely from the original question. Conversely, decomposing strategy questions requires using implicit knowledge, and thus decompositions can include any token that is needed for describing the implicit reasoning (rows 2-4 in Table 3). This makes the decomposition task significantly harder for strategy questions.</p>
<p>In this work, we distinguish between two types of required actions for executing a step. Retrieval: a step that requires retrieval from the corpus, and operation, a logical function over answers to previous steps. In the second row of Table 3, the first two steps are retrieval steps, and the last step is an operation. A decomposition step can require both retrieval and an operation (see last row in Table 3).</p>
<p>To verify that steps are valid single-step questions that can be answered using the corpus (Wikipedia), we collect supporting evidence for each retrieval step and annotate operation steps. A supporting evidence is one or more paragraphs that provide an answer to the retrieval step.</p>
<p>In summary, each example in our dataset contains a) a strategy question, b) the strategy question decomposition, and c) supporting evidence per decomposition step. Collecting strategy questions and their annotations is the main challenge of this work, and we turn to this next.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Question</th>
<th style="text-align: center;">Decomposition</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Did the Battle <br> of Peleliu or <br> the Seven <br> Days Battles <br> last longer?</td>
<td style="text-align: center;">(1) How long did the Battle of Peleliu <br> last? <br> (2) How long did the Seven Days Bat- <br> tle last? <br> (3) Which is longer of #1, #2?</td>
</tr>
<tr>
<td style="text-align: center;">Can the <br> President of <br> Mexico vote in <br> New Mexico <br> primaries?</td>
<td style="text-align: center;">(1) What is the citizenship require- <br> ment for voting in New Mexico? <br> (2) What is the citizenship require- <br> ment of any President of Mexico? <br> (3) Is #2 the same as #1?</td>
</tr>
<tr>
<td style="text-align: center;">Can a <br> microwave <br> melt a Toyota <br> Prius battery?</td>
<td style="text-align: center;">(1) What kind of battery does a Toy- <br> ota Prius use? <br> (2) What type of material is #1 made <br> out of? <br> (3) What is the melting point of #2? <br> (4) Can a microwave's temperature <br> reach at least #3?</td>
</tr>
<tr>
<td style="text-align: center;">Would it be <br> common to <br> find a penguin <br> in Miami?</td>
<td style="text-align: center;">(1) Where is a typical penguin's nat- <br> ural habitat? <br> (2) What conditions make #1 suitable <br> for penguins? <br> (3) Are all of #2 present in Miami?</td>
</tr>
</tbody>
</table>
<p>Table 3: Explicit (row 1) and strategy (rows 2-4) question decompositions. We mark words that are explicit (italic) or implicit in the input (bold).</p>
<h2>3 Data Collection Pipeline</h2>
<p>Our goal is to establish a procedure for collecting strategy questions and their annotations at scale. To this end, we build a multi-step crowdsourcing pipeline designed for encouraging worker creativity, while preventing biases in the data.</p>
<p>We break the data collection into three tasks: question writing (§3.1), question decomposition (§3.2), and evidence matching (§3.3). In addition, we implement mechanisms for quality assurance (§3.4). An overview of the data collection pipeline is in Figure 2.</p>
<h3>3.1 Creative Question Writing (CQW)</h3>
<p>Generating natural language annotations through crowdsourcing (e.g., question generation) is known to suffer from several shortcomings. First, when annotators generate many instances, they use recurring patterns that lead to biases in the data. (Gururangan et al., 2018; Geva et al., 2019). Second, when language is generated conditioned on a long context, such as a paragraph, annotators use similar language (Kwiatkowski et al., 2019), leading to high lexical overlap and hence, inadvertently, to an easier problem. Moreover, a unique</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of the data collection pipeline. First (CQW, §3.1), a worker is presented with a term (T) and an expected answer (A) and writes a question (Q) and the facts (F1,F2) required to answer it. Next, the question is decomposed (SQD, §3.2) into steps (S1, S2) along with Wikipedia page titles (P1,P2) that the worker expects to find the answer in. Last (EVM, §3.3), decomposition steps are matched with evidence from Wikipedia (E1, E2).</p>
<p>property of our setup is that we wish to cover a <em>broad and diverse</em> set of strategies. Thus, we must discourage repeated use of the same strategy.</p>
<p>We tackle these challenges on multiple fronts. First, rather than using a long paragraph as context, we prime workers to write questions given single terms from Wikipedia, reducing the overlap with the context to a minimum. Second, to encourage diversity, we control the population of annotators, making sure a large number of annotators contribute to the dataset. Third, we use <em>model-in-the-loop</em> adversarial annotations <em>Dua et al. (2019); Khot et al. (2020a); Bartolo et al. (2020)</em> to filter our questions, and only accept questions that fool our models. While some model-in-the-loop approaches use fixed pre-trained models to eliminate "easy" questions, we continuously update the models during data collection to combat the use of repeated patterns or strategies.</p>
<p>We now provide a description of the task, and elaborate on these methods (Figure 2, upper row).</p>
<p><strong>Task description</strong> Given a term (e.g., <em>silk</em>), a description of the term, and an expected answer (yes or no), the task is to write a strategy question about the term with the expected answer, and the facts required to answer the question.</p>
<p><strong>Priming with Wikipedia terms</strong> Writing strategy questions from scratch is difficult. To inspire worker creativity, we ask to write questions about terms they are familiar with or can easily understand. The terms are titles of "popular"<sup>2</sup> Wikipedia pages. We provide workers only with a short description of the given term. Then, workers use their background knowledge and web search skills to form a strategy question.</p>
<p><strong>Controlling the answer distribution</strong> We ask workers to write questions where the answer is set to be 'yes' or 'no'. To balance the answer distribution, the expected answer is dynamically sampled inversely proportional to the ratio of 'yes' and 'no' questions collected until that point.</p>
<p><strong>Model-in-the-loop filtering</strong> To ensure questions are challenging and reduce recurring language and reasoning patterns, questions are only accepted when verified by two sets of online <em>solvers</em>. We deploy a set of 5 pre-trained models (termed PTD) that check if the question is too easy. If at least 4 out of 5 answer the question correctly, it is rejected. Second, we use a set of 3 models (called FNTD) that are continuously fine-tuned on our collected data and are meant to detect biases in the current question set. A question is rejected if all 3 solvers answer it correctly. The solvers are ROBERTA <em>Liu et al. (2019)</em> models fine-tuned on different auxiliary datasets; details in §5.1.</p>
<p><strong>Auxiliary sub-task</strong> We ask workers to provide the facts required to answer the question they have written, for several reasons: 1) it helps workers frame the question writing task and describe the reasoning process they have in mind, 2) it helps reviewing their work, and 3) it provides useful information for the decomposition step (§3.2).</p>
<h3>3.2 Strategy Question Decomposition (SQD)</h3>
<p>Once a question and the corresponding facts are written, we generate the strategy question decomposition (Figure 2, middle row). We annotate decompositions <em>before</em> matching evidence in order to avoid biases stemming from seeing the context.</p>
<p>The decomposition strategy for a question is not always obvious, which can lead to undesirable explicit decompositions. For example, a possible ex-</p>
<p><sup>2</sup>We filter pages based on the number of contributors and the number of backward links from other pages.</p>
<p>plicit decomposition for Q1 (Figure 1) might be (1) What items did Aristotle use? (2) Is laptop in #1?; but the first step is not feasible. To guide the decomposition, we provide workers with the facts written in the CQW task to show the strategy of the question author. Evidently, there can be many valid strategies and the same strategy can be phrased in multiple ways - the facts only serve as a soft guidance.</p>
<p>Task description Given a strategy question, a yes/no answer, and a set of facts, the task is to write the steps needed to answer the question.</p>
<p>Auxiliary sub-task We observe that in some cases, annotators write explicit decompositions, which often lead to infeasible steps that cannot be answered from the corpus. To help workers avoid explicit decompositions, we ask them to specify, for each decomposition step, a Wikipedia page they expect to find the answer in. This encourages workers to write decomposition steps for which it is possible to find answers in Wikipedia, and leads to feasible strategy decompositions, with only a small overhead (the workers are not required to read the proposed Wikipedia page).</p>
<h3>3.3 Evidence Matching (EVM)</h3>
<p>We now have a question and its decomposition. To ground them in context, we add a third task of evidence matching (Figure 2, bottom row).</p>
<p>Task description Given a question and its decomposition (a list of single-step questions), the task is to find evidence paragraphs on Wikipedia for each retrieval step. Operation steps that do not require retrieval (§2.2) are marked as operation.</p>
<p>Controlling the matched context Workers search for evidence on Wikipedia. We index Wikipedia ${ }^{3}$ and provide a search interface where workers can drag-and-drop paragraphs from the results shown on the search interface. This guarantees that annotators choose paragraphs we included in our index, at a pre-determined paragraph-level granularity.</p>
<h3>3.4 Data Verification Mechanisms</h3>
<p>Task qualifications For each task, we hold qualifications that test understanding of the task, and manually review several examples. Workers who follow the requirements are granted access to our</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>tasks. Our qualifications are open to workers from English speaking countries who have high reputation scores. Additionally, the authors regularly review annotations to give feedback and prevent noisy annotations.</p>
<p>Real-time automatic checks For CQW, we use heuristics to check question validity, e.g., whether it ends with a question mark, and that it doesn't use language that characterizes explicit multi-hop questions (for instance, having multiple verbs). For SQD, we check that the decomposition structure forms a directed acyclic graph, i.e. (i) each decomposition step is referenced by (at least) one of the following steps, such that all steps are reachable from the last step; and (ii) steps don't form a cycle. In the EVM task, a warning message is shown when the worker marks an intermediate step as an operation (an unlikely scenario).</p>
<p>Inter-task feedback At each step of the pipeline, we collect feedback about previous steps. To verify results from the CQW task, we ask workers to indicate whether the given answer is incorrect (in the SQD, EVM tasks), or if the question is not definitive (in the SQD task) (§2.1). Similarly, to identify non-feasible questions or decompositions, we ask workers to indicate if there is no evidence for a decomposition step (in the EVM task).</p>
<p>Evidence verification task After the EVM step, each example comprises a question, its answer, decomposition and supporting evidence. To verify that a question can be answered by executing the decomposition steps against the matched evidence paragraphs, we construct an additional evidence verification task (EVV). In this task, workers are given a question, its decomposition and matched paragraphs, and are asked to answer the question in each decomposition step purely based on the provided paragraphs. Running EVV on a subset of examples during data collection, helps identify issues in the pipeline and in worker performance.</p>
<h2>4 The StrategyQA Dataset</h2>
<p>We run our pipeline on 1,799 Wikipedia terms, allowing a maximum of 5 questions per term. We update our online fine-tuned solvers (FNTD) every 1 K questions. Every question is decomposed once, and evidence is matched for each decomposition by 3 different workers. The cost of annotating a full example is $\$ 4$.</p>
<table>
<thead>
<tr>
<th></th>
<th>Train</th>
<th>Test</th>
</tr>
</thead>
<tbody>
<tr>
<td>0# of questions</td>
<td>2290</td>
<td>490</td>
</tr>
<tr>
<td>% “yes” questions</td>
<td>46.8%</td>
<td>46.1%</td>
</tr>
<tr>
<td>0# of unique terms</td>
<td>1333</td>
<td>442</td>
</tr>
<tr>
<td>0# of unique decomposition steps</td>
<td>6050</td>
<td>1347</td>
</tr>
<tr>
<td>0# of unique evidence paragraphs</td>
<td>9251</td>
<td>2136</td>
</tr>
<tr>
<td>0# of occurrences of the top trigram</td>
<td>31</td>
<td>5</td>
</tr>
<tr>
<td>0# of question writers</td>
<td>23</td>
<td>6</td>
</tr>
<tr>
<td>0# of filtered questions</td>
<td>2821</td>
<td>484</td>
</tr>
<tr>
<td>Avg. question length (words)</td>
<td>9.6</td>
<td>9.8</td>
</tr>
<tr>
<td>Avg. decomposition length (steps)</td>
<td>2.93</td>
<td>2.92</td>
</tr>
<tr>
<td>Avg. 0# of paragraphs per question</td>
<td>2.33</td>
<td>2.29</td>
</tr>
</tbody>
</table>
<p>Table 4: STRATEGYQA statistics. Filtered questions were rejected by the solvers (§3.1). The train and test sets of question writers are disjoint. The “top trigram” is the most common trigram.</p>
<p>To encourage diversity in strategies used in the questions, we recruited new workers throughout data collection. Moreover, periodic updates of the online solvers prevent workers from exploiting shortcuts, since the solvers adapt to the training distribution. Overall, there were 29 question writers, 19 decomposers, and 54 evidence matchers participating in the data collection.</p>
<p>We collected 2,835 questions, out of which 55 were marked as having an incorrect answer during SQD (§3.2). This results in a collection of 2,780 verified strategy questions, for which we create an annotator-based data split <em>Geva et al. (2019)</em>. We now describe the dataset statistics (§4.1), analyze the quality of the examples, (§4.2) and explore the reasoning skills in STRATEGYQA (§4.3).</p>
<h3>4.1 Dataset Statistics</h3>
<p>We observe (Table 4) that the answer distribution is roughly balanced (yes/no). Moreover, questions are short (&lt; 10 words), and the most common trigram occurs in roughly 1% of the examples. This indicates that the language of the questions is both simple and diverse. For comparison, the average question length in the multi-hop datasets HotPOTQA <em>Yang et al. (2018)</em> and COMPLEXWE-BQUESTIONS <em>Talmor and Berant (2018)</em> is 13.7 words and 15.8 words, respectively. Likewise, the top trigram in these datasets occurs in 9.2% and 4.8% of their examples, respectively.</p>
<p>More than half of the generated questions are filtered by our solvers, pointing to the difficulty of generating good strategy questions. We release all 3,305 filtered questions as well.</p>
<p>To characterize the <em>reasoning complexity</em> required to answer questions in STRATEGYQA, we
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: The distributions of decomposition length (left) and the number of evidence paragraphs (right). The majority of the questions in STRATEGYQA require a reasoning process comprised of ≥ 3 steps, of which about 2 steps involve retrieving external knowledge.</p>
<table>
<thead>
<tr>
<th></th>
<th>multi-step</th>
<th>single-step</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>implicit</td>
<td>81</td>
<td>1</td>
<td>82</td>
</tr>
<tr>
<td>explicit</td>
<td>14.5</td>
<td>3.5</td>
<td>18</td>
</tr>
<tr>
<td></td>
<td>95.5</td>
<td>4.5</td>
<td>100</td>
</tr>
</tbody>
</table>
<p>Table 5: Distribution over the implicit and multi-step properties (§2) in a sample of 100 STRATEGYQA questions, annotated by two experts (we average the expert decisions). Most questions are <em>multi-step</em> and <em>implicit</em>. Annotator agreement is substantial for both the implicit (κ = 0.73) and multi-step (κ = 0.65) properties.</p>
<p>examine the decomposition length and the number of evidence paragraphs. Figure 3 and Table 4 (bottom) show the distributions of these properties are centered around 3-step decompositions and 2 evidence paragraphs, but a considerable portion of the dataset requires more steps and paragraphs.</p>
<h3>4.2 Data Quality</h3>
<h4>Do questions in STRATEGYQA require multi-step implicit reasoning?</h4>
<p>To assess the quality of questions, we sampled 100 random examples from the training set, and had two experts (authors) independently annotate whether the questions satisfy the desired properties of strategy questions (§2.1). We find that most of the examples (81%) are valid multi-step implicit questions, 82% of questions are implicit, and 95.5% are multi-step (Table 5).</p>
<h4>Do questions in STRATEGYQA have a definitive answer?</h4>
<p>We let experts review the answers to 100 random questions, allowing access to the Web. We then ask them to state for every question whether they agree or disagree with the provided answer. We find that the experts agree with the answer in 94% of the cases, and disagree only in 2%. For the remaining 4%, either the question was</p>
<p>ambiguous, or the annotators could not find a definite answer on the Web. Overall, this suggests that questions in StrategyQA have clear answers.</p>
<p>What is the quality of the decompositions? We randomly sampled 100 decompositions and asked experts to judge their quality. Experts judged if the decomposition is explicit or utilizes a strategy. We find that $83 \%$ of the decompositions validly use a strategy to break down the question. The remaining $17 \%$ decompositions are explicit, however, in $14 \%$ of the cases the original question is already explicit. Second, experts checked if the phrasing of the decomposition is "natural", i.e., it reflects the decomposition of a person that does not already know the answer. We find that $89 \%$ of the decompositions express a "natural" reasoning process, while $11 \%$ may depend on the answer. Last, we asked experts to indicate any potential logical flaws in the decomposition, but no such cases occurred in the sample.</p>
<p>Would different annotators use the same decomposition strategy? We sample 50 examples, and let two different workers decompose the questions. Comparing the decomposition pairs, we find that a) for all pairs, the last step returns the same answer, b) in 44 out of 50 pairs, the decomposition pairs follow the same reasoning path , and c) in the other 6 pairs, the decompositions either follow a different reasoning process ( 5 pairs) or one of the decompositions is explicit (1 pair). This shows that different workers usually use the same strategy when decomposing questions.</p>
<p>Is the evidence for strategy questions in Wikipedia? Another important property is whether questions in StrategyQA can be answered based on context from our corpus, Wikipedia, given that questions are written independently of the context. To measure evidence coverage, in the EVM task (§3.3), we provide workers with a checkbox for every decomposition step, indicating whether only partial or no evidence could be found for that step. Recall that three different workers match evidence for each decomposition step. We find that $88.3 \%$ of the questions are fully covered: evidence was matched for each step by some worker. Moreover, in $86.9 \%$ of the questions, at least one worker found evidence for all steps. Last, in only $0.5 \%$ of the examples, all three annotators could not match evidence for any of the steps. This suggests
that overall, Wikipedia is a good corpus for questions in StrategyQA, that were written independently of the context.</p>
<p>Do matched paragraphs provide evidence? We assess the quality of matched paragraphs by analyzing both example-level and step-level annotations. First, we sample 217 decomposition steps with their corresponding paragraphs matched by one of the three workers. We let 3 different crowdworkers decide whether the paragraphs provide evidence for the answer to that step. We find that in $93 \%$ of the cases, the majority vote is that the evidence is valid. ${ }^{4}$</p>
<p>Next, we analyze annotations of the verification task (§3.4), where workers are asked to answer all decomposition steps based only on the matched paragraphs. We find that the workers could answer sub-questions and derive the correct answer in 82 out of 100 annotations. Moreover, in 6 questions indeed there was an error in evidence matching, but another worker that annotated the example was able to compensate for the error, leading to $88 \%$ of the questions where evidence matching succeeds. In the last 12 cases indeed evidence is missing, and is possibly absent from Wikipedia.</p>
<p>Lastly, we let experts review the paragraphs matched by one of the three workers to all the decomposition steps of a question, for 100 random questions. We find that for 79 of the questions the matched paragraphs provide sufficient evidence for answering the question. For 12 of the 21 questions without sufficient evidence, the experts indicated they would expect to find evidence in Wikipedia, and the worker probably could not find it. For the remaining 9 questions, they estimated that evidence is probably absent from Wikipedia.</p>
<p>In conclusion, $93 \%$ of the paragraphs matched at the step-level were found to be valid. Moreover, when considering single-worker annotations, $\sim 80 \%$ of the questions are matched with paragraphs that provide sufficient evidence for all retrieval steps. This number increases to $88 \%$ when aggregating the annotations of three workers.</p>
<p>Do different annotators match the same evidence paragraphs? To compare the evidence paragraphs matched by different workers, we check whether for a given decomposition step, the same paragraph IDs are retrieved by different annotators. Given two non-empty sets of paragraph</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Strategy</th>
<th>Example</th>
<th>$\%$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Physical</td>
<td>"Can human nails carve a statue out of quartz?"</td>
<td>13</td>
</tr>
<tr>
<td>Biological</td>
<td>"Is a platypus immune from cholera?"</td>
<td>11</td>
</tr>
<tr>
<td>Historical</td>
<td>"Were mollusks an ingredient in the color purple?"</td>
<td>10</td>
</tr>
<tr>
<td>Temporal</td>
<td>"Did the 40th president of the United States forward lolcats to his friends?"</td>
<td>10</td>
</tr>
<tr>
<td>Definition</td>
<td>"Are quadrupeds represented on Chinese calendar?"</td>
<td>8</td>
</tr>
<tr>
<td>Cultural</td>
<td>"Would a compass attuned to Earth's magnetic field be a bad gift for a Christmas elf?"</td>
<td>5</td>
</tr>
<tr>
<td>Religious</td>
<td>"Was Hillary Clinton's deputy chief of staff in 2009 baptised?"</td>
<td>5</td>
</tr>
<tr>
<td>Entertainment</td>
<td>"Would Garfield enjoy a trip to Italy?"</td>
<td>4</td>
</tr>
<tr>
<td>Sports</td>
<td>"Can Larry King's ex-wives form a water polo team?"</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Table 6: Top strategies in STRATEGYQA and their frequency in a 100 example subset (accounting for 70\% of the analyzed examples).</p>
<p>IDs $\mathcal{P}<em 2="2">{1}, \mathcal{P}</em>}$, annotated by two workers, we compute the Jaccard coefficient $J\left(\mathcal{P<em 2="2">{1}, \mathcal{P}</em>}\right)=\frac{\left|\mathcal{P<em 2="2">{1}\right| \cdot\left|\mathcal{P}</em>}\right|}{\left|\mathcal{P<em 2="2">{1}\right| \cdot\left|\mathcal{P}</em>}\right|}$. In addition, we take the sets of corresponding Wikipedia page IDs $\mathcal{T<em 2="2">{1}, \mathcal{T}</em>}$ for the matched paragraphs, and compute $J\left(\mathcal{T<em 2="2">{1}, \mathcal{T}</em>\right)$. Note that a score of 1 is given to two identical sets, while a score of 0 corresponds to sets that are disjoint. The average similarity score is 0.43 for paragraphs and 0.69 for pages. This suggests that evidence for a decomposition step can be found in more than one paragraph in the same page, or in different pages.</p>
<h3>4.3 Data Diversity</h3>
<p>We aim to generate creative and diverse questions. We now analyze diversity in terms of the required reasoning skills and question topic.</p>
<p>Reasoning skills To explore the required reasoning skills in StrategyQA, we sampled 100 examples and let two experts (authors) discuss and annotate each example with a) the type of strategy for decomposing the question, and b) the required reasoning and knowledge skills per decomposition step. We then aggregate similar labels (e.g. botanical $\rightarrow$ biological) and compute the proportion of examples each strategy/reasoning skill is required for (an example can have multiple strategy labels).</p>
<p>Table 6 demonstrates the top strategies, showing that StrategyQA contains a broad set of strategies. Moreover, diversity is apparent (Fig-
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Reasoning skills in STRATEGYQA; each skill is associated with the proportion of examples it is required for. Domain-related and logical reasoning skills are marked in blue and orange (italic), respectively.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: The top 15 categories of terms used to prime workers for question writing and their proportion.
ure 4) in terms of both domain-related reasoning (e.g. biological and technological) and logical functions (e.g. set inclusion and "is member of"). While the reasoning skills sampled from questions in StrategyQA do not necessarily reflect their prevalence in a "natural" distribution, we argue that promoting research on methods for inferring strategies is an important research direction.</p>
<p>Question topics As questions in StrategyQA were triggered by Wikipedia terms, we use the "instance of" Wikipedia property to characterize the topics of questions. ${ }^{5}$ Figure 5 shows the distribution of topic categories in STRATEGYQA. The distribution shows StrategyQA is very diverse, with the top two categories ("human" and "taxon", i.e. a group of organisms) covering only a quarter</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th style="text-align: left;">Answer accuracy</th>
<th style="text-align: left;">$87\%$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Strategy match</td>
<td style="text-align: left;">$86\%$</td>
</tr>
<tr>
<td style="text-align: left;">Decomposition usage</td>
<td style="text-align: left;">$14 \%$</td>
</tr>
<tr>
<td style="text-align: left;">Average # searches</td>
<td style="text-align: left;">1.25</td>
</tr>
</tbody>
</table>
<p>Table 7: Human performance in answering questions. Strategy match is computed by comparing the explanation provided by the expert with the decomposition. Decomposition usage and the number of searches are computed based on information provided by the expert.
of the data, and a total of 609 topic categories.
We further compare the diversity of StrateGYQA to HotpotQA, a multi-hop QA dataset over Wikipedia paragraphs. To this end, we sample 739 pairs of evidence paragraphs associated with a single question in both datasets, and map the pair of paragraphs to a pair of Wikipedia categories using the "instance of" property. We find that there are 571 unique category pairs in STRATEGYQA, but only 356 unique category pairs in HotpotQA. Moreover, the top two category pairs in both of the datasets ("human-human", "taxon-taxon") constitute $8 \%$ and $27 \%$ of the cases in StrategyQA and HotpotQA, respectively. This demonstrates the creativity and breadth of category combinations in STRATEGYQA.</p>
<h3>4.4 Human Performance</h3>
<p>To see how well humans answer strategy questions, we sample a subset of 100 questions from StrategyQA and have experts (authors) answer questions, given access to Wikipedia articles and an option to reveal the decomposition for every question. In addition, we ask them to provide a short explanation for the answer, the number of searches they conducted to derive the answer, and to indicate whether they have used the decomposition. We expect humans to excel at coming up with strategies for answering questions. Yet, humans are not necessarily an upper bound because finding the relevant paragraphs is difficult and could potentially be performed better by machines.</p>
<p>Table 7 summarizes the results. Overall, humans infer the required strategy and answer the questions with high accuracy. Moreover, the low number of searches shows that humans leverage background knowledge, as they can answer some of the intermediate steps without search. An error analysis shows that the main reason for failure ( $10 \%$ ) is difficulty to find evidence, and the rest of the cases ( $3 \%$ ) are due to ambiguity in the question that could lead to the opposite answer.</p>
<h2>5 Experimental Evaluation</h2>
<p>In this section, we conduct experiments to answer the following questions: a) How well do pretrained language models (LMs) answer strategy questions? b) Is retrieval of relevant context helpful? and c) Are decompositions useful for answering questions that require implicit knowledge?</p>
<h3>5.1 Baseline Models</h3>
<p>Answering strategy questions requires external knowledge that cannot be obtained by training on StrategyQA alone. Therefore, our models and online solvers (§3.1) are based on pre-trained LMs, fine-tuned on auxiliary datasets that require reasoning. Specifically, in all models we fine-tune RoBERTA (Liu et al., 2019) on a subset of:</p>
<ul>
<li>BoolQ (Clark et al., 2019): A dataset for boolean question answering.</li>
<li>MNLI (Williams et al., 2018): A large natural language inference (NLI) dataset. The task is to predict if a textual premise entails, contradicts or is neutral with respect to the hypothesis.</li>
<li>Twenty Questions (20Q): A collection of 50K short commonsense boolean questions. ${ }^{6}$</li>
<li>DROP (Dua et al., 2019): A large dataset for numerical reasoning over paragraphs.</li>
</ul>
<p>Models are trained in two configurations:</p>
<ul>
<li>No context : The model is fed with the question only, and outputs a binary prediction using the special CLS token.</li>
<li>With context : We use BM25 (Robertson et al., 1995) to retrieve context from our corpus, while removing stop words from all queries. We examine two retrieval methods: a) question-based retrieval: by using the question as a query and taking the top $k=10$ results, and b) decomposition-based retrieval: by initiating a separate query for each (gold or predicted) decomposition step and concatenating the top $k=$ 10 results of all steps (sorted by retrieval score). In both cases, the model is fed with the question concatenated to the retrieved context, truncated to 512 tokens (the maximum input length of RoBERTA), and outputs a binary prediction.</li>
</ul>
<p>Predicting decompositions We train a seq-toseq model, termed BART ${ }_{\text {DECOMP }}$, that given a question, generates its decomposition token-by-</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Solver group(s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>$\operatorname{RoBERTA}_{\varnothing}(20 \mathrm{Q})$</td>
<td>PTD, FNTD</td>
</tr>
<tr>
<td>$\operatorname{RoBERTA}_{\varnothing}(20 \mathrm{Q}+\operatorname{BoolQ})$</td>
<td>PTD, FNTD</td>
</tr>
<tr>
<td>$\operatorname{RoBERTA}_{\varnothing}(\operatorname{BoolQ})$</td>
<td>PTD, FNTD</td>
</tr>
<tr>
<td>$\operatorname{RoBERTA}_{\text {IR-Q }}(\operatorname{BoolQ})$</td>
<td>PTD</td>
</tr>
<tr>
<td>$\operatorname{RoBERTA}_{\text {IR-Q }}(\operatorname{MNLI}+\operatorname{BoolQ})$</td>
<td>PTD</td>
</tr>
</tbody>
</table>
<p>Table 8: QA models used as online solvers during data collection (§3.1). Each model was fine-tuned on the datasets mentioned in its name.
token. Specifically, we fine-tune BART (Lewis et al., 2020) on STRATEGYQA decompositions.</p>
<p>Baseline models As our base model, we train a model as follows: We take a RoBERTA (Liu et al., 2019) model and fine-tune it on DROP, 20Q and BoolQ (in this order). The model is trained on DROP with multiple output heads, as in Segal et al. (2020), which are then replaced with a single Boolean output. ${ }^{7}$ We call this model RoBERTA*.</p>
<p>We use RoBERTA<em> and RoBERTA to train the following models on STRATEGYQA: without context $\left(\operatorname{RoBERTA}^{</em>}{ }<em _IR-D="{IR-D" _text="\text">{\varnothing}\right)$, with question-based retrieval $\left(\operatorname{RoBERTA}^{<em>}{ }<em _IR-Q="{IR-Q" _text="\text">{\text {IR-Q }}, \operatorname{RoBERTA}</em>^{}}\right)$, and with predicted decomposition-based retrieval $\left(\operatorname{RoBERTA</em>}{ }</em>\right)$.
We also present four oracle models:}</p>
<ul>
<li>$\operatorname{RoBERTA}^{*}{ }_{\text {ORA-P: }}$ uses the gold paragraphs (no retrieval).</li>
<li>$\operatorname{RoBERTA}^{*}{ }_{\text {IR-ORA-D: }}$ performs retrieval with the gold decomposition.</li>
<li>$\operatorname{RoBERTA}_{\text {ORA-P-D }}^{<em>}{ }_{\text {iust-step }}$ : exploits both the gold decomposition and the gold paragraphs. We finetune RoBERTA on BoolQ and SQUAD (Rajpurkar et al., 2016) to obtain a model that can answer single-step questions. We then run this model on STRATEGYQA to obtain answers for all decomposition sub-questions, and replace all placeholder references with the predicted answers. Last, we fine-tune RoBERTA</em> to answer the last decomposition step of STRATEGYQA, for which we have supervision.</li>
<li>$\operatorname{RoBERTA}_{\text {ORA-P-D }}^{<em>}{ }_{\text {list-step-raw: }} \operatorname{RoBERTA}{ }^{</em>}$ that is finetuned to predict the answer from the gold paragraphs and the last step of the gold decomposition, without replacing placeholder references.</li>
</ul>
<p>Online solvers For the solvers integrated in the data collection process (§3.1), we use three nocontext models and two question-based retrieval models. The solvers are listed in Table 8.</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Table 9: QA accuracy (with standard deviation across 7 experiments), and retrieval performance, measured by Recall@10, of baseline models on the test set.</p>
<h3>5.2 Results</h3>
<p>Strategy QA performance Table 9 summarizes the results of all models (§5.1). RoBERTA<em> ${ }<em _IR-Q="{IR-Q" _text="\text">{\text {IR-Q }}$ substantially outperforms $\operatorname{RoBERTA}</em>$, indicating that fine-tuning on related auxiliary datasets before STRATEGYQA is crucial. Hence, we focus on RoBERTA}</em> for all other results and analysis.</p>
<p>Strategy questions pose a combined challenge of retrieving the relevant context, and deriving the answer based on that context. Training without context shows a large accuracy gain of $53.9 \rightarrow$ 63.6 over the majority baseline. This is far from human performance, but shows that some questions can be answered by a large LM fine-tuned on related datasets without retrieval. On the other end, training with gold paragraphs raises performance to 70.7 . This shows that high-quality retrieval lets the model effectively reason over the given paragraphs. Last, using both gold decompositions and retrieval further increases performance to 72.0 , showing the utility of decompositions.</p>
<p>Focusing on retrieval-based methods, we observe that question-based retrieval reaches an accuracy of 63.6 and retrieval with gold decompositions results in an accuracy of 62.0 . This shows that the quality of retrieval even with gold decompositions is not high enough to improve the 63.6 accuracy obtained by $\operatorname{RoBERTA}^{*}{ }_{\varnothing}$, a model that uses no context. Retrieval with predicted decompositions results in an even lower accuracy of 61.7 . We also analyze predicted decompositions below.</p>
<p>Retrieval evaluation A question decomposition describes the reasoning steps for answering the question. Therefore, using the decomposition for retrieval may help obtain the relevant context and improve performance. To test this, we directly compare performance of question- and</p>
<p>decomposition-based retrieval with respect to the annotated gold paragraphs. We compute Recall@10, i.e., the fraction of the gold paragraphs retrieved in the top-10 results of each method. Since there are 3 annotations per question, we compute Recall@10 for each annotation and take the maximum as the final score. For a fair comparison, in decomposition-based retrieval, we use the top-10 results across all steps.</p>
<p>Results (Table 9) show that retrieval performance is low, partially explaining why retrieval models do not improve performance compared to $\operatorname{ROBERTA}{ }^{*}{ }_{\varnothing}$, and demonstrating the retrieval challenge in our setup. Gold decompositionbased retrieval substantially outperforms questionbased retrieval, showing that using the decomposition for retrieval is a promising direction for answering multi-step questions. Still, predicted decomposition-based retrieval does not improve retrieval compared to question-based retrieval, showing better decomposition models are needed.</p>
<p>To understand the low retrieval scores, we analyzed the query results of 50 random decomposition steps. Most failure cases are due to the shallow pattern matching done by BM25, e.g., failure to match synonyms. This shows that indeed there is little word overlap between decomposition steps and the evidence, as intended by our pipeline design. In other examples, either a key question entity was missing because it was represented by a reference token, or the decomposition step had complex language, leading to failed retrieval. This analysis suggests that advances in neural retrieval might be beneficial for STRATEGYQA.</p>
<p>Human retrieval performance To quantify human performance in finding gold paragraphs, we ask experts to find evidence paragraphs for 100 random questions. For half of the questions we also provide decomposition. We observe average Recall@10 of 0.586 and 0.513 with and without the decomposition, respectively. This shows that humans significantly outperform our IR baselines. However, humans are still far from covering the gold paragraphs, since there are multiple valid evidence paragraphs (§4.2), and retrieval can be difficult even for humans. Lastly, using decompositions improves human retrieval, showing decompositions indeed are useful for finding evidence.</p>
<p>Predicted decompositions Analysis shows that $\mathrm{BART}_{\text {DECOMP }}$ 's decompositions are grammati- cal and well-structured. Interestingly, the model generates strategies, but often applies them to questions incorrectly. E.g., the question "Can a lifeboat rescue people in the Hooke Sea?" is decomposed to "1) What is the maximum depth of the Hooke Sea? 2) How deep can a lifeboat dive? 3) Is #2 greater than or equal to #1?". While the decomposition is well-structured, it uses a wrong strategy (lifeboats do not dive).</p>
<h2>6 Related Work</h2>
<p>Prior work has typically let annotators write questions based on an entire context (Khot et al., 2020a; Yang et al., 2018; Dua et al., 2019; Mihaylov et al., 2018; Khashabi et al., 2018). In this work, we prime annotators with minimal information (few tokens) and let them use their imagination and own wording to create questions. A related priming method was recently proposed by Clark et al. (2020), who used the first 100 characters of a Wikipedia page.</p>
<p>Among multi-hop reasoning datasets, our dataset stands out in that it requires implicit decompositions. Two recent datasets (Khot et al., 2020a; Mihaylov et al., 2018) have considered questions requiring implicit facts. However, they are limited to specific domain strategies, while in our work we seek diversity in this aspect.</p>
<p>Most multi-hop reasoning datasets do not fully annotate question decomposition (Yang et al., 2018; Khot et al., 2020a; Mihaylov et al., 2018). This issue has prompted recent work to create question decompositions for existing datasets (Wolfson et al., 2020), and to train models that generate question decompositions (Perez et al., 2020; Khot et al., 2020b; Min et al., 2019). In this work, we annotate question decompositions as part of the data collection.</p>
<h2>7 Conclusion</h2>
<p>We present STRATEGYQA, the first dataset of implicit multi-step questions requiring a wide-range of reasoning skills. To build STRATEGYQA, we introduced a novel annotation pipeline for eliciting creative questions that use simple language, but cover a challenging range of diverse strategies. Questions in STRATEGYQA are annotated with decomposition into reasoning steps and evidence paragraphs, to guide the ongoing research towards addressing implicit multi-hop reasoning.</p>
<h2>Acknowledgement</h2>
<p>We thank Tomer Wolfson for helpful feedback and the REVIZ team at Allen Institute for AI, particularly Michal Guerquin and Sam Skjonsberg. This research was supported in part by the Yandex Initiative for Machine Learning, and the European Research Council (ERC) under the European Union Horizons 2020 research and innovation programme (grant ERC DELPHI 802800). Dan Roth is partly supported by ONR contract N00014-19-1-2620 and DARPA contract FA8750-19-2-1004, under the Kairos program. This work was completed in partial fulfillment for the Ph.D degree of Mor Geva.</p>
<h2>References</h2>
<p>Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. 2020. Beat the AI: Investigating adversarial human annotation for reading comprehension. Transactions of the Association for Computational Linguistics, 8:662-678.</p>
<p>Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics (TACL), 8:454-470.</p>
<p>Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443-4458. Association for Computational Linguistics.</p>
<p>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? An investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1161-1166, Hong Kong, China. Association for Computational Linguistics.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Yichen Jiang and Mohit Bansal. 2019. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA. In Association for Computational Linguistics (ACL).</p>
<p>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading comprehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 252-262, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020a. QASC: A dataset for question answering via sentence composition. In AAAI.</p>
<p>Tushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2020b.</p>
<p>Text modular networks: Learning to decompose tasks in the language of existing models. arXiv preprint arXiv:2009.00751.</p>
<p>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics (TACL), 7:453-466.</p>
<p>Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871-7880. Association for Computational Linguistics.</p>
<p>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? A new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2381-2391, Brussels, Belgium. Association for Computational Linguistics.</p>
<p>Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019. Multi-hop reading comprehension through question decomposition and rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097-6109, Florence, Italy. Association for Computational Linguistics.</p>
<p>Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and</p>
<p>Li Deng. 2016. MS MARCO: A human generated machine reading comprehension dataset. In Workshop on Cognitive Computing at NIPS.</p>
<p>Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. 2020. Unsupervised question decomposition for question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8864-8880.</p>
<p>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<p>Stephen Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M. Gatford. 1995. Okapi at TREC-3. In Overview of the Third Text REtrieval Conference (TREC-3), pages 109126. Gaithersburg, MD: NIST.</p>
<p>Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, and Jonathan Berant. 2020. A simple and effective model for answering multispan questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 30743080.</p>
<p>Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6418-6428, Florence, Italy. Association for Computational Linguistics.</p>
<p>Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In North American Chapter of the Association for Computational Linguistics (NAACL).</p>
<p>Johannes Welbl, Pontus Stenetorp, and Sebastian Riedel. 2018. Constructing datasets for multihop reading comprehension across documents. Transactions of the Association for Computational Linguistics (TACL), 6:287-302.</p>
<p>Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus</p>
<p>for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguistics.</p>
<p>Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gardner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: A question understanding benchmark. Transactions of the Association for Computational Linguistics (TACL).</p>
<p>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Empirical Methods in Natural Language Processing (EMNLP).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{7}$ For brevity, exact details on model training and hyperparameters will be released as part of our codebase.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
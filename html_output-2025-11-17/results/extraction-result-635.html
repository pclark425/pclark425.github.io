<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-635 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-635</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-635</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-19.html">extraction-schema-19</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <p><strong>Paper ID:</strong> paper-181b24b777d60b71d3e496c7181a35fa1003cfd7</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/181b24b777d60b71d3e496c7181a35fa1003cfd7" target="_blank">Are We Done with MMLU?</a></p>
                <p><strong>Paper Venue:</strong> North American Chapter of the Association for Computational Linguistics</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a comprehensive framework for identifying dataset errors using a novel error annotation protocol and creates MMLU-Redux, which is a subset of 5,700 manually re-annotated questions across all 57 MMLU subjects.</p>
                <p><strong>Paper Abstract:</strong> Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that 57% of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error annotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700 manually re-annotated questions across all 57 MMLU subjects. We estimate that 6.49% of MMLU questions contain errors. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. https://huggingface.co/datasets/edinburgh-dawg/mmlu-redux-2.0.</p>
                <p><strong>Cost:</strong> 0.02</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e635.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e635.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MMLU-Redux</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>MMLU-Redux (manually re-annotated subset of MMLU)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 5,700-question, manually re-annotated subset of the MMLU benchmark covering all 57 subjects with a hierarchical error taxonomy; used to quantify dataset errors (estimated 6.49% overall error rate) and to re-evaluate LLM performance under corrected data.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (HELM predictions used for LM outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>dataset quality assessment, error annotation, and re-evaluation of LLM performance on MMLU</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>dataset annotation errors (parsing mistakes, wrong ground-truth, missing context, ambiguous questions), heterogeneity across MMLU subsets, sampling variability from subsampling 100 questions per subject, annotator disagreement, and potential model memorisation from pretraining.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>stratified-sampling estimate of overall error rate; per-subset error percentages; inter-annotator agreement (Cohen's Kappa) for annotation reliability; exact-match (EM) performance differences across correct vs erroneous instances.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Estimated overall MMLU error rate 6.49% (via stratified sampling). Per-subset example: Virology 57% of sampled instances erroneous (30% wrong ground truth, 15% unclear questions). Re-evaluation showed large EM shifts (see other entries).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Cohen's Kappa for inter-annotator agreement (both 'All Error Type' and 'Binary Error Type'); use of fixed prompt/hyperparameter settings (temperature=0.0, top_p=1, frequency/presence_penalty=0, max_tokens=600) to ensure deterministic LM queries; clear provenance via source column for each annotated instance.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Cohen's Kappa (three annotators) across five erroneous subjects: Virology 0.67, Logical Fallacies 0.73, College Chemistry 0.92, Formal Logic 0.96, Human Sexuality 0.64 (both All Error Type and Binary Error Type reported with minimal difference).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Under-documented original MMLU annotation procedure; random/heterogeneous errors across subsets making root-cause tracing difficult; changing external standards/sources over time; limited annotation budget (only 100 samples per subject) and annotator subjectivity.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Manual re-annotation with hierarchical taxonomy; stratified sampling for error-rate estimation; providing provenance/source URLs; opening dataset for community contributions; using deterministic LM settings for evaluation; re-evaluating LLMs on 'correct-only' instances to measure impact of dataset errors.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Using MMLU-Redux (correct-only instances) materially changed model rankings and EM scores for multiple models and subsets (examples provided elsewhere in results); high inter-annotator agreement indicates annotation protocol is effective for consistent labelling.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>dataset: 100 sampled examples per subject (5,700 total annotated examples); LLM predictions sourced from HELM leaderboard (single reported predictions per model/config)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dataset quality (errors in MMLU) is a non-negligible source of variability in benchmarked LLM performance (estimated 6.49% error rate overall; 57% in Virology sample), and correcting labels produces substantial shifts in exact-match scores and model rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Done with MMLU?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e635.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e635.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Deterministic decoding (temperature=0)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deterministic inference hyperparameters (temperature=0.0, top_p=1, fixed penalties)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper fixed generation hyperparameters (temperature=0.0, top_p=1, frequency_penalty=0, presence_penalty=0, max_tokens=600) and used the default random seed when running LM prompts to ensure deterministic, reproducible outputs for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are We Done with MMLU?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo, GPT-4o, Claude-3-Opus, Llama3-70B (and others evaluated via HELM)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>error-detection prompting and LM performance evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>sampling randomness from decoding temperature and sampling strategies; randomness from different seeds if non-deterministic decoding used.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>explicit fixed decoding hyperparameters and use of default random seed to reduce run-to-run stochasticity</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Deterministic hyperparameter choices reported (temperature=0.0, top_p=1, penalties=0, max_tokens=600) to produce deterministic results; however the paper notes they used the default random seed and did not run multiple seeds.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Despite deterministic settings, single-run reporting (no repeated runs/error bars) prevents quantifying remaining stochastic variability across runs or API versions.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Set temperature to 0.0 and top_p=1 to avoid sampling-based stochasticity and expose deterministic LM outputs for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Ensures consistent single-run outputs for reproducibility of the reported evaluation results, but no quantitative reduction in variance is given (no repeated-run comparisons).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>single-run evaluations per model/prompt configuration (default random seed)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Fixing decoding hyperparameters to deterministic settings is used as a reproducibility control, but the authors did not (due to resource limits) run repeated trials to quantify residual stochastic variability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Done with MMLU?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e635.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e635.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Missing error bars / runs</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Absence of multiple-run repetition and error bars in reported LM experiments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors explicitly state they did not report error bars or rerun experiments multiple times due to funding constraints, acknowledging this as a limitation to assessing stochastic variability and statistical uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are We Done with MMLU?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>various (models evaluated via HELM and in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / evaluation methodology</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>LM benchmarking and error-detection evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>run-to-run stochasticity (sampling, seeds), API/model-version changes, prompt variability — none quantified because no repeated runs were performed</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>No error bars or repeated-run statistics reported; authors state rerunning experiments to obtain error bars exceeded funding capabilities.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Limited compute/funding prevented repeated experiments; reliance on single-run HELM predictions and single deterministic runs limits ability to quantify variability and confidence intervals.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>single-run per configuration (no repeats reported)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Lack of multiple runs and error bars is a stated limitation that reduces ability to quantify stochastic variability and the statistical significance of differences between models or conditions.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Done with MMLU?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e635.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e635.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CohenKappa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Cohen's Kappa inter-annotator agreement</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cohen's Kappa was used to quantify agreement between annotators on error labels in MMLU-Redux, providing a reproducibility/consistency metric for the manual annotation protocol.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are We Done with MMLU?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>annotation reliability / dataset curation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>measuring agreement across annotators for error annotation</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>annotator subjectivity and expertise, ambiguous questions, and unclear provenance of original sources</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Cohen's Kappa (for All Error Type and Binary Error Type)</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Reported Cohen's Kappa values across five erroneous subjects (three annotators): Virology 0.67 / 0.67, Logical Fallacies 0.73 / 0.71, College Chemistry 0.92 / 0.95, Formal Logic 0.96 / 1.0, Human Sexuality 0.64 / 0.64.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>Cohen's Kappa values interpreted as agreement scores to assess annotation reproducibility</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Generally high inter-annotator agreement (>0.6) across most subjects, indicating the annotation protocol is reliable for consistent labelling.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Annotator biases and domain expertise limits may still affect labels, and Kappa is reported only for a subset (five) of subjects.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Hierarchical annotation protocol and guidance to consult original sources and trusted references; requiring 'Expert' mark when unsure.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>High Kappa values in many subjects (e.g., 0.92–0.96) suggest the protocol effectively reduces annotation variability in those domains.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>agreement computed over annotations of 100 sampled questions per reported subject among three annotators</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cohen's Kappa demonstrates generally high inter-annotator reliability for the MMLU-Redux protocol, supporting reproducibility of error labels across annotators in most subjects.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Done with MMLU?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e635.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e635.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Prompting & RAG comparisons</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prompting strategies and retrieval-augmented generation (Zero-shot, Few-shot, Chain-of-Thought, RAG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares multiple prompting regimes (zero-shot, few-shot, CoT) and retrieval-augmented prompting (RAG) across models for automatic error detection, measuring variability in detection performance (Recall, F1, F2).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are We Done with MMLU?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>GPT-4 Turbo, GPT-4o, Claude-3-Opus, Llama3-70B</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (e.g., Llama3-70B, Claude-3-Opus commercial family)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / dataset error detection</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>classifying MMLU question triples as 'ok' vs 'not ok' (error detection) under multiple prompting strategies and with retrieved context</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>prompt engineering (zero-shot vs few-shot vs CoT), inclusion of retrieved documents (Wikipedia vs MS MARCO), model architecture/versions, and per-subject difficulty heterogeneity.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Recall, F1, F2 (treated 'not ok' as positive class); per-model/per-method reported scores; per-dataset breakdown in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Table 3 few-shot CoT outperformed other prompting modes across models (e.g., GPT-4 Turbo few-shot CoT: Recall 46.68, F1 31.68, F2 36.58). RAG further improved recall: Claude 3 Opus on MS MARCO Zero-Shot Recall 83.91 and on Wikipedia Zero-Shot Recall 82.61 with F2 up to 41.92; GPT-4 Turbo RAG Recall ~57.11 (MS MARCO) / 57.00 (Wikipedia) with lower F1/F2.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>standard classification metrics (Recall, F1, F2) reported per model/method; per-subject detailed tables in appendices.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Few-shot CoT improves detection relative to zero-shot; RAG markedly increases Recall for some models (notably Claude-3-Opus). Despite improvements, best F2 scores remained modest (~41.9% for RAG+Claude), indicating limited absolute reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>High variance across subjects and models; some models (Llama3-70B) performed poorly under many settings; retrieval index choice (Wikipedia vs MS MARCO) affected results variably; overall F1/F2 remained low for many configurations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Few-shot CoT prompting, adding retrieved context (RAG with top-5 paragraphs via BM25), and instruction fine-tuning (Llama3 8B Instruct) to improve error detection.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Few-shot CoT improved recall and F-scores over zero-shot in many cases (example: GPT-4 Turbo few-shot CoT Recall 46.68 vs zero-shot 22.81). RAG with Claude-3-Opus achieved the highest recall (83.91) and highest F2 reported (41.92), but still leaves substantial residual error.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>single-run evaluations per model/method/configuration (per-subject breakdown with 100 sampled examples each); no repeated-runs for error bars</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Prompt engineering (few-shot + CoT) and retrieval augmentation can substantially increase error-detection recall, but even best combinations (RAG + Claude-3-Opus) produce modest F2 (≈41.9%), indicating automatic error detection remains challenging and variable across subjects/models.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Done with MMLU?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e635.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e635.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Evaluation variability from dataset errors</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Model performance variability driven by dataset labeling errors (comparison of 'all' vs 'correct-only' instances)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper quantifies how noisy/erroneous MMLU instances change measured LLM performance and rankings: re-evaluating on MMLU-Redux 'correct-only' subset yields substantial EM changes and rank swaps for many models/subsets.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are We Done with MMLU?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>many (Claude 3.5 Sonnet, Claude 3 Opus, Llama 3.1 Instruct Turbo, GPT-4, GPT-4 Turbo, Gemini, Qwen2, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td>various (examples include Llama 3.1 Instruct Turbo 405B and 70B listed in table; Qwen2 72B etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>NLP / benchmark evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>measuring model exact-match (EM) on original vs corrected MMLU subsets and observing ranking changes</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>incorrect ground truth labels, ambiguous questions, multiple correct answers, parsing errors in dataset that cause spurious model success/failure</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>Exact Match (EM) scores and model ranking positions per subset; comparison 'overall instances' vs 'correct-only instances'.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Examples: In Virology, Llama 3.1 Instruct Turbo (405B) EM changed 0.57 -> 0.93 and rank 36 -> 4 when using only correct instances; GPT-4 (0613) in Human Sexuality dropped EM 0.91 -> 0.43 and rank 5 -> last among top-10 when restricted to correct instances. Overall, errors materially alter model rankings and absolute scores.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>direct comparison of EM and rank between evaluations on full MMLU vs MMLU-Redux correct-only subsets</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Significant shifts in per-subject EM and model ranks demonstrate non-reproducibility of earlier comparisons that used erroneous labels; correcting dataset labels yields different and (arguably) truer rankings.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Original MMLU errors (hidden/undocumented) make prior reported evaluations non-comparable; lack of provenance and mixed source quality complicate reproducing 'true' task labels.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Use of curated/corrected subsets (MMLU-Redux), manual source validation, and community-maintained corrections to stabilize evaluations.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Correcting labels produced large changes in EM and rankings, showing high effectiveness in exposing spurious performance claims, but full dataset correction requires further annotation effort (remaining 8,342 MMLU questions unreviewed).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>single-run model evaluations on two dataset conditions (all instances vs correct-only subset); per-subject annotated sample size 100</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Dataset label errors are a major source of variability in benchmark outcomes; correcting labels can reverse model rankings and change absolute performance substantially.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Done with MMLU?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e635.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e635.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of variability, reproducibility, or stochasticity in language model-driven scientific experiments, including sources of variability, reproducibility metrics, and methods to improve reproducibility.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Stratified sampling estimate</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Stratified sampling to estimate overall MMLU error rate</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The authors estimate the overall MMLU error rate (6.49%) via stratified sampling from their per-subject annotations (100 samples per subject across 57 subjects).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Are We Done with MMLU?</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>n/a</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>dataset auditing / statistics</td>
                        </tr>
                        <tr>
                            <td><strong>experimental_task</strong></td>
                            <td>estimating prevalence of labeling/data errors in a benchmark</td>
                        </tr>
                        <tr>
                            <td><strong>variability_sources</strong></td>
                            <td>sampling variance from 100 examples per subject, heterogeneity of error rates across subjects (some subjects have high error prevalence), and annotation uncertainty.</td>
                        </tr>
                        <tr>
                            <td><strong>variability_measured</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>variability_metrics</strong></td>
                            <td>per-subject error percentages and an overall stratified-sampling estimate (percentage).</td>
                        </tr>
                        <tr>
                            <td><strong>variability_results</strong></td>
                            <td>Estimated overall MMLU error rate 6.49% (paper also reports Total row: OK 93.51% and various error-type percentages), and extreme per-subject values (e.g., Virology sample 43 OK out of 100 => 57% error).</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_assessed</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_metrics</strong></td>
                            <td>stratified sampling methodology and transparent per-subject counts (100 annotated examples each) to allow re-estimation by others.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_results</strong></td>
                            <td>Estimate reported with per-subject breakdown (Table 5) enabling others to reproduce the stratified estimator given the same sampled data.</td>
                        </tr>
                        <tr>
                            <td><strong>reproducibility_challenges</strong></td>
                            <td>Limited sample size per subject (100) may give wide confidence intervals for some subsets; paper does not report confidence intervals or uncertainty bounds due to resource constraints.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_methods</strong></td>
                            <td>Provide full annotated subset (MMLU-Redux) and encourage community annotation to expand coverage and tighten estimates; publish datasheet and provenance.</td>
                        </tr>
                        <tr>
                            <td><strong>mitigation_effectiveness</strong></td>
                            <td>Providing the annotated sample and methodology makes the estimate reproducible; expanding annotations would reduce sampling variance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_without_controls</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>number_of_runs</strong></td>
                            <td>single stratified sampling pass: 100 annotated samples per subject (5,700 total)</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Stratified sampling produced a quantified estimate (6.49%) of MMLU labeling errors and highlighted large heterogeneity across subjects, motivating dataset correction.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Are We Done with MMLU?', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Holistic evaluation of language models <em>(Rating: 2)</em></li>
                <li>Chain-of-thought prompting elicits reasoning in large language models <em>(Rating: 1)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive nlp tasks <em>(Rating: 1)</em></li>
                <li>Mmlu-pro: A more robust and challenging multi-task language understanding benchmark <em>(Rating: 2)</em></li>
                <li>Are we done with imagenet? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-635",
    "paper_id": "paper-181b24b777d60b71d3e496c7181a35fa1003cfd7",
    "extraction_schema_id": "extraction-schema-19",
    "extracted_data": [
        {
            "name_short": "MMLU-Redux",
            "name_full": "MMLU-Redux (manually re-annotated subset of MMLU)",
            "brief_description": "A 5,700-question, manually re-annotated subset of the MMLU benchmark covering all 57 subjects with a hierarchical error taxonomy; used to quantify dataset errors (estimated 6.49% overall error rate) and to re-evaluate LLM performance under corrected data.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "various (HELM predictions used for LM outputs)",
            "model_size": null,
            "scientific_domain": "NLP / benchmark evaluation",
            "experimental_task": "dataset quality assessment, error annotation, and re-evaluation of LLM performance on MMLU",
            "variability_sources": "dataset annotation errors (parsing mistakes, wrong ground-truth, missing context, ambiguous questions), heterogeneity across MMLU subsets, sampling variability from subsampling 100 questions per subject, annotator disagreement, and potential model memorisation from pretraining.",
            "variability_measured": true,
            "variability_metrics": "stratified-sampling estimate of overall error rate; per-subset error percentages; inter-annotator agreement (Cohen's Kappa) for annotation reliability; exact-match (EM) performance differences across correct vs erroneous instances.",
            "variability_results": "Estimated overall MMLU error rate 6.49% (via stratified sampling). Per-subset example: Virology 57% of sampled instances erroneous (30% wrong ground truth, 15% unclear questions). Re-evaluation showed large EM shifts (see other entries).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Cohen's Kappa for inter-annotator agreement (both 'All Error Type' and 'Binary Error Type'); use of fixed prompt/hyperparameter settings (temperature=0.0, top_p=1, frequency/presence_penalty=0, max_tokens=600) to ensure deterministic LM queries; clear provenance via source column for each annotated instance.",
            "reproducibility_results": "Cohen's Kappa (three annotators) across five erroneous subjects: Virology 0.67, Logical Fallacies 0.73, College Chemistry 0.92, Formal Logic 0.96, Human Sexuality 0.64 (both All Error Type and Binary Error Type reported with minimal difference).",
            "reproducibility_challenges": "Under-documented original MMLU annotation procedure; random/heterogeneous errors across subsets making root-cause tracing difficult; changing external standards/sources over time; limited annotation budget (only 100 samples per subject) and annotator subjectivity.",
            "mitigation_methods": "Manual re-annotation with hierarchical taxonomy; stratified sampling for error-rate estimation; providing provenance/source URLs; opening dataset for community contributions; using deterministic LM settings for evaluation; re-evaluating LLMs on 'correct-only' instances to measure impact of dataset errors.",
            "mitigation_effectiveness": "Using MMLU-Redux (correct-only instances) materially changed model rankings and EM scores for multiple models and subsets (examples provided elsewhere in results); high inter-annotator agreement indicates annotation protocol is effective for consistent labelling.",
            "comparison_with_without_controls": true,
            "number_of_runs": "dataset: 100 sampled examples per subject (5,700 total annotated examples); LLM predictions sourced from HELM leaderboard (single reported predictions per model/config)",
            "key_findings": "Dataset quality (errors in MMLU) is a non-negligible source of variability in benchmarked LLM performance (estimated 6.49% error rate overall; 57% in Virology sample), and correcting labels produces substantial shifts in exact-match scores and model rankings.",
            "uuid": "e635.0",
            "source_info": {
                "paper_title": "Are We Done with MMLU?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Deterministic decoding (temperature=0)",
            "name_full": "Deterministic inference hyperparameters (temperature=0.0, top_p=1, fixed penalties)",
            "brief_description": "The paper fixed generation hyperparameters (temperature=0.0, top_p=1, frequency_penalty=0, presence_penalty=0, max_tokens=600) and used the default random seed when running LM prompts to ensure deterministic, reproducible outputs for evaluation.",
            "citation_title": "Are We Done with MMLU?",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo, GPT-4o, Claude-3-Opus, Llama3-70B (and others evaluated via HELM)",
            "model_size": null,
            "scientific_domain": "NLP / benchmark evaluation",
            "experimental_task": "error-detection prompting and LM performance evaluation",
            "variability_sources": "sampling randomness from decoding temperature and sampling strategies; randomness from different seeds if non-deterministic decoding used.",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": true,
            "reproducibility_metrics": "explicit fixed decoding hyperparameters and use of default random seed to reduce run-to-run stochasticity",
            "reproducibility_results": "Deterministic hyperparameter choices reported (temperature=0.0, top_p=1, penalties=0, max_tokens=600) to produce deterministic results; however the paper notes they used the default random seed and did not run multiple seeds.",
            "reproducibility_challenges": "Despite deterministic settings, single-run reporting (no repeated runs/error bars) prevents quantifying remaining stochastic variability across runs or API versions.",
            "mitigation_methods": "Set temperature to 0.0 and top_p=1 to avoid sampling-based stochasticity and expose deterministic LM outputs for evaluation.",
            "mitigation_effectiveness": "Ensures consistent single-run outputs for reproducibility of the reported evaluation results, but no quantitative reduction in variance is given (no repeated-run comparisons).",
            "comparison_with_without_controls": false,
            "number_of_runs": "single-run evaluations per model/prompt configuration (default random seed)",
            "key_findings": "Fixing decoding hyperparameters to deterministic settings is used as a reproducibility control, but the authors did not (due to resource limits) run repeated trials to quantify residual stochastic variability.",
            "uuid": "e635.1",
            "source_info": {
                "paper_title": "Are We Done with MMLU?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Missing error bars / runs",
            "name_full": "Absence of multiple-run repetition and error bars in reported LM experiments",
            "brief_description": "The authors explicitly state they did not report error bars or rerun experiments multiple times due to funding constraints, acknowledging this as a limitation to assessing stochastic variability and statistical uncertainty.",
            "citation_title": "Are We Done with MMLU?",
            "mention_or_use": "use",
            "model_name": "various (models evaluated via HELM and in experiments)",
            "model_size": null,
            "scientific_domain": "NLP / evaluation methodology",
            "experimental_task": "LM benchmarking and error-detection evaluation",
            "variability_sources": "run-to-run stochasticity (sampling, seeds), API/model-version changes, prompt variability — none quantified because no repeated runs were performed",
            "variability_measured": false,
            "variability_metrics": null,
            "variability_results": null,
            "reproducibility_assessed": false,
            "reproducibility_metrics": null,
            "reproducibility_results": "No error bars or repeated-run statistics reported; authors state rerunning experiments to obtain error bars exceeded funding capabilities.",
            "reproducibility_challenges": "Limited compute/funding prevented repeated experiments; reliance on single-run HELM predictions and single deterministic runs limits ability to quantify variability and confidence intervals.",
            "mitigation_methods": null,
            "mitigation_effectiveness": null,
            "comparison_with_without_controls": false,
            "number_of_runs": "single-run per configuration (no repeats reported)",
            "key_findings": "Lack of multiple runs and error bars is a stated limitation that reduces ability to quantify stochastic variability and the statistical significance of differences between models or conditions.",
            "uuid": "e635.2",
            "source_info": {
                "paper_title": "Are We Done with MMLU?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "CohenKappa",
            "name_full": "Cohen's Kappa inter-annotator agreement",
            "brief_description": "Cohen's Kappa was used to quantify agreement between annotators on error labels in MMLU-Redux, providing a reproducibility/consistency metric for the manual annotation protocol.",
            "citation_title": "Are We Done with MMLU?",
            "mention_or_use": "use",
            "model_name": "n/a",
            "model_size": null,
            "scientific_domain": "annotation reliability / dataset curation",
            "experimental_task": "measuring agreement across annotators for error annotation",
            "variability_sources": "annotator subjectivity and expertise, ambiguous questions, and unclear provenance of original sources",
            "variability_measured": true,
            "variability_metrics": "Cohen's Kappa (for All Error Type and Binary Error Type)",
            "variability_results": "Reported Cohen's Kappa values across five erroneous subjects (three annotators): Virology 0.67 / 0.67, Logical Fallacies 0.73 / 0.71, College Chemistry 0.92 / 0.95, Formal Logic 0.96 / 1.0, Human Sexuality 0.64 / 0.64.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "Cohen's Kappa values interpreted as agreement scores to assess annotation reproducibility",
            "reproducibility_results": "Generally high inter-annotator agreement (&gt;0.6) across most subjects, indicating the annotation protocol is reliable for consistent labelling.",
            "reproducibility_challenges": "Annotator biases and domain expertise limits may still affect labels, and Kappa is reported only for a subset (five) of subjects.",
            "mitigation_methods": "Hierarchical annotation protocol and guidance to consult original sources and trusted references; requiring 'Expert' mark when unsure.",
            "mitigation_effectiveness": "High Kappa values in many subjects (e.g., 0.92–0.96) suggest the protocol effectively reduces annotation variability in those domains.",
            "comparison_with_without_controls": false,
            "number_of_runs": "agreement computed over annotations of 100 sampled questions per reported subject among three annotators",
            "key_findings": "Cohen's Kappa demonstrates generally high inter-annotator reliability for the MMLU-Redux protocol, supporting reproducibility of error labels across annotators in most subjects.",
            "uuid": "e635.3",
            "source_info": {
                "paper_title": "Are We Done with MMLU?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Prompting & RAG comparisons",
            "name_full": "Prompting strategies and retrieval-augmented generation (Zero-shot, Few-shot, Chain-of-Thought, RAG)",
            "brief_description": "The paper compares multiple prompting regimes (zero-shot, few-shot, CoT) and retrieval-augmented prompting (RAG) across models for automatic error detection, measuring variability in detection performance (Recall, F1, F2).",
            "citation_title": "Are We Done with MMLU?",
            "mention_or_use": "use",
            "model_name": "GPT-4 Turbo, GPT-4o, Claude-3-Opus, Llama3-70B",
            "model_size": "various (e.g., Llama3-70B, Claude-3-Opus commercial family)",
            "scientific_domain": "NLP / dataset error detection",
            "experimental_task": "classifying MMLU question triples as 'ok' vs 'not ok' (error detection) under multiple prompting strategies and with retrieved context",
            "variability_sources": "prompt engineering (zero-shot vs few-shot vs CoT), inclusion of retrieved documents (Wikipedia vs MS MARCO), model architecture/versions, and per-subject difficulty heterogeneity.",
            "variability_measured": true,
            "variability_metrics": "Recall, F1, F2 (treated 'not ok' as positive class); per-model/per-method reported scores; per-dataset breakdown in appendices.",
            "variability_results": "Table 3 few-shot CoT outperformed other prompting modes across models (e.g., GPT-4 Turbo few-shot CoT: Recall 46.68, F1 31.68, F2 36.58). RAG further improved recall: Claude 3 Opus on MS MARCO Zero-Shot Recall 83.91 and on Wikipedia Zero-Shot Recall 82.61 with F2 up to 41.92; GPT-4 Turbo RAG Recall ~57.11 (MS MARCO) / 57.00 (Wikipedia) with lower F1/F2.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "standard classification metrics (Recall, F1, F2) reported per model/method; per-subject detailed tables in appendices.",
            "reproducibility_results": "Few-shot CoT improves detection relative to zero-shot; RAG markedly increases Recall for some models (notably Claude-3-Opus). Despite improvements, best F2 scores remained modest (~41.9% for RAG+Claude), indicating limited absolute reliability.",
            "reproducibility_challenges": "High variance across subjects and models; some models (Llama3-70B) performed poorly under many settings; retrieval index choice (Wikipedia vs MS MARCO) affected results variably; overall F1/F2 remained low for many configurations.",
            "mitigation_methods": "Few-shot CoT prompting, adding retrieved context (RAG with top-5 paragraphs via BM25), and instruction fine-tuning (Llama3 8B Instruct) to improve error detection.",
            "mitigation_effectiveness": "Few-shot CoT improved recall and F-scores over zero-shot in many cases (example: GPT-4 Turbo few-shot CoT Recall 46.68 vs zero-shot 22.81). RAG with Claude-3-Opus achieved the highest recall (83.91) and highest F2 reported (41.92), but still leaves substantial residual error.",
            "comparison_with_without_controls": true,
            "number_of_runs": "single-run evaluations per model/method/configuration (per-subject breakdown with 100 sampled examples each); no repeated-runs for error bars",
            "key_findings": "Prompt engineering (few-shot + CoT) and retrieval augmentation can substantially increase error-detection recall, but even best combinations (RAG + Claude-3-Opus) produce modest F2 (≈41.9%), indicating automatic error detection remains challenging and variable across subjects/models.",
            "uuid": "e635.4",
            "source_info": {
                "paper_title": "Are We Done with MMLU?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Evaluation variability from dataset errors",
            "name_full": "Model performance variability driven by dataset labeling errors (comparison of 'all' vs 'correct-only' instances)",
            "brief_description": "The paper quantifies how noisy/erroneous MMLU instances change measured LLM performance and rankings: re-evaluating on MMLU-Redux 'correct-only' subset yields substantial EM changes and rank swaps for many models/subsets.",
            "citation_title": "Are We Done with MMLU?",
            "mention_or_use": "use",
            "model_name": "many (Claude 3.5 Sonnet, Claude 3 Opus, Llama 3.1 Instruct Turbo, GPT-4, GPT-4 Turbo, Gemini, Qwen2, etc.)",
            "model_size": "various (examples include Llama 3.1 Instruct Turbo 405B and 70B listed in table; Qwen2 72B etc.)",
            "scientific_domain": "NLP / benchmark evaluation",
            "experimental_task": "measuring model exact-match (EM) on original vs corrected MMLU subsets and observing ranking changes",
            "variability_sources": "incorrect ground truth labels, ambiguous questions, multiple correct answers, parsing errors in dataset that cause spurious model success/failure",
            "variability_measured": true,
            "variability_metrics": "Exact Match (EM) scores and model ranking positions per subset; comparison 'overall instances' vs 'correct-only instances'.",
            "variability_results": "Examples: In Virology, Llama 3.1 Instruct Turbo (405B) EM changed 0.57 -&gt; 0.93 and rank 36 -&gt; 4 when using only correct instances; GPT-4 (0613) in Human Sexuality dropped EM 0.91 -&gt; 0.43 and rank 5 -&gt; last among top-10 when restricted to correct instances. Overall, errors materially alter model rankings and absolute scores.",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "direct comparison of EM and rank between evaluations on full MMLU vs MMLU-Redux correct-only subsets",
            "reproducibility_results": "Significant shifts in per-subject EM and model ranks demonstrate non-reproducibility of earlier comparisons that used erroneous labels; correcting dataset labels yields different and (arguably) truer rankings.",
            "reproducibility_challenges": "Original MMLU errors (hidden/undocumented) make prior reported evaluations non-comparable; lack of provenance and mixed source quality complicate reproducing 'true' task labels.",
            "mitigation_methods": "Use of curated/corrected subsets (MMLU-Redux), manual source validation, and community-maintained corrections to stabilize evaluations.",
            "mitigation_effectiveness": "Correcting labels produced large changes in EM and rankings, showing high effectiveness in exposing spurious performance claims, but full dataset correction requires further annotation effort (remaining 8,342 MMLU questions unreviewed).",
            "comparison_with_without_controls": true,
            "number_of_runs": "single-run model evaluations on two dataset conditions (all instances vs correct-only subset); per-subject annotated sample size 100",
            "key_findings": "Dataset label errors are a major source of variability in benchmark outcomes; correcting labels can reverse model rankings and change absolute performance substantially.",
            "uuid": "e635.5",
            "source_info": {
                "paper_title": "Are We Done with MMLU?",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Stratified sampling estimate",
            "name_full": "Stratified sampling to estimate overall MMLU error rate",
            "brief_description": "The authors estimate the overall MMLU error rate (6.49%) via stratified sampling from their per-subject annotations (100 samples per subject across 57 subjects).",
            "citation_title": "Are We Done with MMLU?",
            "mention_or_use": "use",
            "model_name": "n/a",
            "model_size": null,
            "scientific_domain": "dataset auditing / statistics",
            "experimental_task": "estimating prevalence of labeling/data errors in a benchmark",
            "variability_sources": "sampling variance from 100 examples per subject, heterogeneity of error rates across subjects (some subjects have high error prevalence), and annotation uncertainty.",
            "variability_measured": true,
            "variability_metrics": "per-subject error percentages and an overall stratified-sampling estimate (percentage).",
            "variability_results": "Estimated overall MMLU error rate 6.49% (paper also reports Total row: OK 93.51% and various error-type percentages), and extreme per-subject values (e.g., Virology sample 43 OK out of 100 =&gt; 57% error).",
            "reproducibility_assessed": true,
            "reproducibility_metrics": "stratified sampling methodology and transparent per-subject counts (100 annotated examples each) to allow re-estimation by others.",
            "reproducibility_results": "Estimate reported with per-subject breakdown (Table 5) enabling others to reproduce the stratified estimator given the same sampled data.",
            "reproducibility_challenges": "Limited sample size per subject (100) may give wide confidence intervals for some subsets; paper does not report confidence intervals or uncertainty bounds due to resource constraints.",
            "mitigation_methods": "Provide full annotated subset (MMLU-Redux) and encourage community annotation to expand coverage and tighten estimates; publish datasheet and provenance.",
            "mitigation_effectiveness": "Providing the annotated sample and methodology makes the estimate reproducible; expanding annotations would reduce sampling variance.",
            "comparison_with_without_controls": false,
            "number_of_runs": "single stratified sampling pass: 100 annotated samples per subject (5,700 total)",
            "key_findings": "Stratified sampling produced a quantified estimate (6.49%) of MMLU labeling errors and highlighted large heterogeneity across subjects, motivating dataset correction.",
            "uuid": "e635.6",
            "source_info": {
                "paper_title": "Are We Done with MMLU?",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Holistic evaluation of language models",
            "rating": 2
        },
        {
            "paper_title": "Chain-of-thought prompting elicits reasoning in large language models",
            "rating": 1
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive nlp tasks",
            "rating": 1
        },
        {
            "paper_title": "Mmlu-pro: A more robust and challenging multi-task language understanding benchmark",
            "rating": 2
        },
        {
            "paper_title": "Are we done with imagenet?",
            "rating": 1
        }
    ],
    "cost": 0.020061749999999996,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Are We Done with MMLU?</h1>
<p>Aryo Pradipta Gema ${ }^{1}$ Joshua Ong Jun Leang ${ }^{1}$ Giwon Hong ${ }^{1}$ Alessio Devoto ${ }^{2}$<br>Alberto Carlo Maria Mancino ${ }^{2,3}$ Rohit Saxena ${ }^{1}$ Xuanli He ${ }^{4}$ Yu Zhao ${ }^{1}$ Xiaotang Du ${ }^{1}$<br>Mohammad Reza Ghasemi Madani ${ }^{5}$ Claire Barale ${ }^{1}$ Robert McHardy ${ }^{6}$ Joshua Harris ${ }^{7}$<br>Jean Kaddour ${ }^{4}$ Emile van Krieken ${ }^{1}$ Pasquale Minervini ${ }^{1,8}$<br>${ }^{1}$ University of Edinburgh ${ }^{2}$ Sapienza University of Rome ${ }^{3}$ Polytechnic University of Bari<br>${ }^{4}$ University College London ${ }^{5}$ University of Trento ${ }^{6}$ AssemblyAI<br>${ }^{7}$ UK Health Security Agency ${ }^{8}$ Miniml.AI<br>{first.last, jong2, p.minervini}@ed.ac.uk<br>alessio.devoto@uniroma1.it alberto.mancino@poliba.it<br>mr.ghasemimadani@unitn.it joshua.harris@ukhsa.gov.uk<br>{xuanli.he, jean.kaddour.20, robert.mchardy.20}@ucl.ac.uk</p>
<h4>Abstract</h4>
<p>Maybe not. We identify and analyse errors in the popular Massive Multitask Language Understanding (MMLU) benchmark. Even though MMLU is widely adopted, our analysis demonstrates numerous ground truth errors that obscure the true capabilities of LLMs. For example, we find that $57 \%$ of the analysed questions in the Virology subset contain errors. To address this issue, we introduce a comprehensive framework for identifying dataset errors using a novel error annotation protocol. Then, we create MMLU-Redux, which is a subset of 5,700 manually re-annotated questions across all 57 MMLU subjects. We estimate that $6.49 \%$ of MMLU questions contain errors. Using MMLU-Redux, we demonstrate significant discrepancies with the model performance metrics that were originally reported. Our results strongly advocate for revising MMLU's error-ridden questions to enhance its future utility and reliability as a benchmark. https://huggingface.co/datasets/ edinburgh-dawg/mmlu-redux-2.0.</p>
<h2>1 Introduction</h2>
<p>The advent of transformer-based Large Language Models (LLMs) (OpenAI, 2023; Anil et al., 2023a; Anthropic, 2023; Anil et al., 2023b; Touvron et al., 2023; Anthropic, 2024; Kaddour et al., 2023; Dubey et al., 2024) marked a significant advancement in generative models, enabling interaction with computing devices through natural language. This advancement rendered many earlier benchmarks and leaderboards obsolete (Laskar et al., 2023; Shen et al., 2023), leading to the compilation of more challenging and comprehensive tests. Among these benchmarks, Massive Multitask Language Understanding (MMLU) (Hendrycks et al.,</p>
<h2>Erroneous Instances in MMLU</h2>
<p>What is the current best option for preventing future outbreaks of Ebola?
A. Rebuild scientific, medical and nursing infrastructure and train staff
B. Early and accurate diagnosis with molecular kits
C. Develop effective vaccines
D. Arrange rapid intervention into West Africa with EU and USA army teams</p>
<p>The number of energy levels for the 55Mn nuclide are:
A. 3
B. 5
C. 8
D. 4</p>
<p>The woman who conducted a longitudinal study on herself and found increased retrieval difficulty as she got older was named
A. Clark B. Smith C. Whitebear D. Ebbinghaus</p>
<p>Correct
answer, from s
Human Virology
Se quiz
Incorrect answer, from MMLU Virology</p>
<p>Incorrect
answer, from MMLU College Chemistry</p>
<p>Ambiguous question, from MMLU Human Aging</p>
<p>Figure 1: Examples of erroneous instances from MMLU Virology, College Chemistry, and Human Aging.
2021) has gained significant popularity: It assesses both the breadth and depth of language understanding capabilities of current LLMs across a diverse range of subjects, including mathematics, history, computer science, logic, law, etc.</p>
<p>However, the reliability of benchmarking results is only as robust as the quality of the dataset used. We find that, despite its popularity, MMLU suffers from numerous errors that can mislead evaluation and model comparison (L and Stapleton, 2023; Erenrich, 2023). These errors, which range from simple parsing and scraping mistakes to more complex issues related to context, interpretation, and dataset quality, compromise the reliability of MMLU as a benchmark. For example, we find that $57 \%$ of the analysed instances in the Virology subset contain errors, including the suggestion to send the American army to West Africa to prevent outbreaks of Ebola (see Fig. 1).</p>
<p>Therefore, in this study, we manually analyse the MMLU dataset using a novel error annotation protocol to construct MMLU-Redux: 14 human experts manually assessed and re-annotated 5,700 questions across all subsets of MMLU. We estimate that $6.49 \%$ of the questions are erroneous.</p>
<p>After our manual re-annotation effort, we study how the errors in MMLU impact LLM evaluation. First, we re-evaluated leading LLMs on MMLURedux, and found the performance metrics notably altered, changing their ranking. Furthermore, we both quantitatively and qualitatively analysed the errors to help understand how these errors impact LLM evaluation. MMLU-Redux can also be used as a strong benchmark for automatic error detection in NLP datasets, which would help scale up the review of benchmark datasets. Therefore, we also study whether LLMs can help with error detection, using prompting techniques (i.e., In-Context Learning (Brown et al., 2020), Chain of Thoughts (CoT) (Wei et al., 2022)), Retrieval Augmented Generation (RAG) (Lewis et al., 2020), and finetuning. We believe MMLU-Redux underscores the need for closely studying and reassessing the benchmarks used for evaluating NLP models.</p>
<h2>2 What Is Wrong with MMLU?</h2>
<p>The MMLU dataset has become a popular choice for evaluating the performance of NLP systems, owing to its extensive coverage of subjects collected from freely available online sources with the help of graduate and undergraduate students (Hendrycks et al., 2021). Despite the manual effort, MMLU still contains errors that are difficult to trace due to its under-documented annotation procedure. These errors have not yet been systematically studied, even though they were recently highlighted (L and Stapleton, 2023; Erenrich, 2023).</p>
<p>We identify various errors in MMLU questions, from simple parsing mistakes (e.g., the source answer is B, but MMLU labels it as C) to more complex issues, such as missing context. These errors occur randomly, and without detailed documentation, tracing their root causes is challenging.</p>
<p>This randomness and lack of traceability highlight the need for a standardised error categorisation to improve the reliability and accuracy of the MMLU dataset. Our approach involved developing a hierarchical annotation protocol of errors, which we used to develop MMLU-Redux: A manual annotation of all 57 subsets of MMLU, each contain- ing 100 randomly selected samples (Section 3.1).</p>
<h3>2.1 Error Detection Annotation Protocol</h3>
<p>We develop a hierarchical annotation protocol to classify the various errors identified in MMLU into specific error types. Figure 2 illustrates our annotation protocol for categorising MMLU errors, while Figure 4 provides examples of each error category. We categorise errors into two primary groups: samples with errors in the clarity of the questions (Type 1, Question Assessment) and samples with errors in the ground truth answer (Type 2, Ground Truth Verification).
(1a) Bad Question Clarity: The question is poorly presented in terms of various aspects, such as clarity, grammar, and sufficiency of information. For instance, referring to a previous question.
(1b) Bad Options Clarity: The options are unclear, similar, or irrelevant to the question. Most errors in this category stem from incorrect parsing of the options from the original source. For example, a single option might be incorrectly split into two separate options.
(2a) No Correct Answer: None of the options correctly answer the question. This error may arise when the ground-truth options are omitted to reduce the number of options from five to four.
(2b) Multiple Correct Answers: More than one option can be selected as the answer to the question. For example, the options contain a synonym of the ground truth label.
(2c) Wrong Ground Truth: The correct answer differs from the ground truth provided in MMLU. This type of error occurs when the annotated label differs from the correct label, which may be caused by a mistake during manual annotation.</p>
<p>To create MMLU-Redux, we followed the proposed annotation protocol. We aim to ensure comprehensive coverage of the different error types for further experiments.</p>
<h3>2.2 Heterogeneity of Errors in MMLU</h3>
<p>Our annotation process reveals substantial variation in error types across subsets. Some subsets are affected by ambiguous questions, while others by incorrect ground truth labels. These differences may impact how MMLU results are interpreted and addressed. Notable irregularities in certain subsets include (a full list of subsets and their corresponding errors can be found in Appendix I):
Virology - Incorrect ground truth labels are prevalent within the Virology subset. Many of the incor-</p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: The error annotation protocol used for annotating instances in MMLU — annotators first assess whether the instance is unambiguous (<em>Question Assessment</em>, then whether there is a single valid answer, and then whether it matches with the ground truth answer from the dataset).</p>
<p>The rect labels are for relatively simple questions, such as identifying the description of a pandemic; this suggests errors may stem from problems parsing the original datasets (in most cases, the <em>Human Virology</em> textbook).</p>
<p><strong>College Chemistry</strong> – The questions were sourced from textbooks and standardised college-level exams. We identified erroneous questions resulting from simple parsing errors and unknown causes. For example, questions spanning multiple lines in the original source were often parsed incorrectly, leading to a part of the question being presented as the first option (Option A) and the exclusion of Option D. Furthermore, there were questions with ground truth labels that did not match the answers provided in the source without known cause.</p>
<p><strong>Professional Law</strong> – The benchmark does not clearly distinguish between different jurisdictions, despite focusing on U.S. law.</p>
<p><strong>Formal Logic</strong> – The dataset contains many questions with incorrect answers. These are mostly sourced from the 'Oxford Learning Link' website. It is unknown what causes the inaccuracies. For example, one question states that $$(F \land L) \land \neg C$$ is correct, but $$F \land L \land \neg C$$ is not, even though these two formulas are clearly equivalent.</p>
<p><strong>Global Facts</strong> – Almost all questions needed consulting external sources to be validated, where a large portion of them are reports from ourworldindata.org (18 cases) and pewresearch.org (15 cases); for several questions, multiple sources were providing conflicting answers — for example, on the perceived corruption of political parties in 2013 Ethiopia, ourworldindata.org seems to confirm the answer in MMLU, while the Global Corruption Barometer from Transparency International was providing conflicting evidence.<sup>1</sup></p>
<p><strong>Machine Learning</strong> – Most questions were sourced from exam papers, assignments or online quizzes. About 30 of the questions require expert knowledge and reasoning. The main issue of this subset is the clarity of the questions and options. <em>e.g.,</em> some quiz questions are based on past knowledge, and the descriptions in the questions may be vague or inapplicable today.</p>
<p><strong>Econometrics</strong> – The majority of the questions are correct, but some questions contain unverifiable references. <em>e.g.,</em> 'Consider again the VAR model of equation 16,' but equation 16 cannot be found within the question.</p>
<p>These irregularities highlight error patterns in MMLU. We want to highlight one type of error that is especially challenging to catch, namely unspecified context that is needed to properly answer the question. For example, the Professional Law and Accounting datasets assume US jurisdiction and practices which are not specified in the question.</p>
<p><sup>1</sup>See data at Our World in Data and Global Corruption Barometer.</p>
<p>tion and may become outdated if standards change. Many subjects also display a US- and Westerncentric bias: the Virology dataset refers to "the Latino population" in a US context, and the Human Aging dataset mentions an unspecified survey of older adults.</p>
<h2>3 MMLU-Redux: A Correct MMLU Subset</h2>
<p>In this section, we propose MMLU-Redux, a manually annotated subset of MMLU, to quantify the errors present in the original dataset. MMLU-Redux serves two main purposes: 1) to measure the prevalence and types of errors in MMLU; and 2) to explore the feasibility of automatically fixing MMLU by leveraging the annotated error types. We find that 1) the proportion of errors in MMLU is nonnegligible, highlighting the need for a correct subset; and 2) fixing MMLU automatically proves to be a challenging task, despite the availability of annotated error types.</p>
<p>We create MMLU-Redux by manually labelling a subset of MMLU questions with their corresponding error types. To this end, we follow the annotation protocol introduced in Section 2.1. For more accurate annotations, we confirmed the error detection by finding the samples' original source wherever it was available. However, at present, the correct answers suggested by the annotators are not used to replace the existing MMLU labels.</p>
<p>In the following, we analyse the error statistics of MMLU-Redux and use MMLU-Redux to reevaluate the performance of LLMs. In Section 4, we explore the possibility of using MMLU-Redux to improve the overall quality of MMLU by automatically fixing the identified errors.</p>
<h3>3.1 Analysis of MMLU-Redux</h3>
<p>We present the percentage of error types in Fig. 3, with detailed numbers available in Appendix F. In our analysis, we find that more than $9 \%$ of the examples are incorrect, suggesting a substantial presence of errors in the MMLU. Especially, we find that more than $57 \%$ examples in Virology contain errors, where $30 \%$ examples have a wrong ground truth, and $15 \%$ are unclear questions. Moreover, we also observe significant error percentages in other disciplines: more than $20 \%$ examples in Logical Fallacies and College Chemistry are wrong, and more than $10 \%$ examples in Professional Law, Business Ethics, Formal Logic, Human Aging, Global</p>
<p>Table 1: Inter-annotator agreement (Cohen's Kappa) analyses for the five erroneous subjects of MMLURedux. "All Error Type" is calculated based on whether the specific error types match, while "Binary Error Type" is calculated based on whether an error is present or not.</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Subject</th>
<th style="text-align: center;">Cohen's Kappa <br> (All Error Type)</th>
<th style="text-align: center;">Cohen's Kappa <br> (Binary Error Type)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Virology</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.67</td>
</tr>
<tr>
<td style="text-align: left;">Logical Fallacies</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.71</td>
</tr>
<tr>
<td style="text-align: left;">College Chemistry</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">0.95</td>
</tr>
<tr>
<td style="text-align: left;">Formal Logic</td>
<td style="text-align: center;">0.96</td>
<td style="text-align: center;">1.0</td>
</tr>
<tr>
<td style="text-align: left;">Human Sexuality</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.64</td>
</tr>
</tbody>
</table>
<p>Facts, Machine Learning, Miscellaneous and Public Relations are wrong. Such error proportions could lead to inaccurate comparisons and invalid rankings of LLM models. Using stratified sampling, we estimate that $6.49 \%$ of the questions in the complete MMLU dataset contain errors.</p>
<p>To better understand how these errors impact the performance of models on the MMLU, we compare the performance between erroneous instances and correct instances across the seven subjects identified as having the most errors (Virology, Logical Fallacies, College Chemistry, Professional Law, Business Ethics, Formal Logic, and Human Aging) in Fig. 5.</p>
<p>Despite the general trend indicating a performance decline among erroneous instances, we also observed cases where performance was similar or even higher in erroneous instances (Professional Law and Formal Logic). Given that LLMs should be unable to yield answers that are correct with respect to the original MMLU ground truth on these erroneous instances, this may be evidence of memorisation, suggesting that these MMLU instances were learned during the pretraining.</p>
<p>We also assessed the reliability of the manual annotations in MMLU-Redux by calculating Cohen's Kappa (Cohen, 1960) among three annotators for the five erroneous subjects (virology, logical fallacies, college chemistry, formal logic, and human sexuality). The results shown in Table 1 demonstrate a generally high level of agreement across most subjects, with Cohen's Kappa values exceeding 0.6 for both "All Error Type" (whether the specific error types match) and "Binary Error Type" (whether an error is present or not). Additionally, the minimal difference in scores between "All Error Type" and "Binary Error Type" indicates that our error annotation protocol used for annotating (Section 2.1) is highly effective.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Statistics of the error types detected in 100 randomly sampled and manually annotated data from the 30 most erroneous MMLU subjects. In the <em>Virology</em> subject, we found that 57% of the analysed instances contain some forms of errors, such as wrong ground truth (33%), multiple correct answers (4%), or unclear questions (14%). For detailed numbers of all subsets, see Appendix F.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: An example for every leaf node in the error annotation protocol shown in Fig. 2 from the MMLU dataset.</p>
<h3>3.2 Re-Evaluating the State-of-the-Art LLMs</h3>
<p>To assess how the corrected dataset impacts the performance of existing state-of-the-art LLMs, we re-evaluate them on the five subjects with the highest number of errors (Virology, Logical Fallacies, College Chemistry, Professional Law, and Human Sexuality) in MMLU-Redux.</p>
<p>In Table 2, we compared the performance of models when using all instances of MMLU-Redux to the performance when using only correct instances without errors to see if there are any changes in the rankings due to this. The results clearly demonstrate that, at least for subjects with a high number of detected errors, these issues are significant enough to affect the results, leading to changes in model rankings. For example, in the Virology subset, Llama 3.1 Instruct Turbo (405B) ranked 16th when considering all Virology instances, but ranked first when only correct instances were used. On the other hand, in the Human Sexuality subset, GPT-4 (0613) achieved an exact match score of 0.91 across all instances (ranking 5th). However, when considering only the correct instances, the exact match score drops significantly to 0.43, placing it last among the top 10 models. Since MMLU is an important benchmark for evaluating model performance, this indicates that errors in MMLU are a critical issue.</p>
<h3>3.3 Correlation between Exact Match Score and Data Quality</h3>
<p>Figure 6 shows the correlation between the performance of the ten best-performing LLMs on MMLU (as ranked by HELM (Liang et al., 2023)) and the ratio of non-erroneous instances on MMLU-Redux. Each point in the scatter plot represents a subject in MMLU. As expected, LLMs perform better in subjects with fewer errors, supporting the link between data reliability and model performance. Ideally, LLMs excel with non-erroneous instances and struggle with erroneous ones.</p>
<p>Outliers, however, reveal deviations from this trend, where models perform better or worse than expected for certain subjects. These deviations suggest other factors influence performance, requiring further investigation. For example, some LLMs perform well despite high error rates (in the Logical Fallacies subject), hinting at potential subject-specific advantages. We further explore these anomalies and their implications for model robustness in Section 3.1.</p>
<h2>4 Can We Fix the MMLU Dataset Automatically?</h2>
<p>After presenting evidence of the numerous errors in the MMLU dataset, we explore the following</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Exact Match (EM) for each model on all subjects combined (top left), and the seven subjects with the most errors in our MMLU-Redux data. Original EM represents performance measured across all instances regardless of error presence, while Correct and Erroneous EM reflect results on correct instances and error instances, respectively.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Correlation between the performance of 10 best-performing LLMs on MMLU (as per HELM (Liang et al., 2023)) and the ratio of non-erroneous instances on MMLU-Redux. Each dot represents one MMLU subject.</p>
<p>Table 2: Comparison of model performance and ranking changes when using overall instances (as per HELM (Liang et al., 2023)) versus correct instances of MMLU-Redux on five most erroneous subjects. Numbers in parentheses denote ranks, teal an improvement in rank, red a deterioration in rank, and black an unchanged rank.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Virology</th>
<th style="text-align: center;">Logical Fallacies</th>
<th style="text-align: center;">College Chemistry</th>
<th style="text-align: center;">Professional Law</th>
<th style="text-align: center;">Human Sexuality</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">Claude 3.5 Sonnet (20240620)</td>
<td style="text-align: center;">$0.60(1) \rightarrow 0.91(5)$</td>
<td style="text-align: center;">$0.93(1) \rightarrow 0.96(5)$</td>
<td style="text-align: center;">$0.59(9) \rightarrow 0.73(4)$</td>
<td style="text-align: center;">$\mathbf{0 . 7 5}(8) \rightarrow \mathbf{0 . 7 7}(4)$</td>
<td style="text-align: center;">$\mathbf{0 . 9 4}(4) \rightarrow \mathbf{0 . 9 8}(4)$</td>
</tr>
<tr>
<td style="text-align: center;">Claude 3 Opus (20240229)</td>
<td style="text-align: center;">$0.58(12) \rightarrow 0.88(6)$</td>
<td style="text-align: center;">$0.90(4) \rightarrow 0.96(5)$</td>
<td style="text-align: center;">$0.60(9) \rightarrow 0.72(8)$</td>
<td style="text-align: center;">$0.72(8) \rightarrow 0.72(3)$</td>
<td style="text-align: center;">$0.91(5) \rightarrow 0.96(2)$</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3.1 Instruct Turbo (405B)</td>
<td style="text-align: center;">$\mathbf{0 . 5 7}(36) \rightarrow \mathbf{0 . 9 3}(4)$</td>
<td style="text-align: center;">$0.92(2) \rightarrow 0.96(5)$</td>
<td style="text-align: center;">$\mathbf{0 . 6 0}(8) \rightarrow \mathbf{0 . 7 6}(4)$</td>
<td style="text-align: center;">$0.70(6) \rightarrow 0.72(3)$</td>
<td style="text-align: center;">$0.86(20) \rightarrow 0.91(9)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o (2024-05-13)</td>
<td style="text-align: center;">$0.60(3) \rightarrow 0.91(5)$</td>
<td style="text-align: center;">$0.88(6) \rightarrow 0.99(2)$</td>
<td style="text-align: center;">$0.61(4) \rightarrow 0.71(5)$</td>
<td style="text-align: center;">$0.72(8) \rightarrow 0.70(5)$</td>
<td style="text-align: center;">$0.91(5) \rightarrow 0.96(2)$</td>
</tr>
<tr>
<td style="text-align: center;">Gemini 1.5 Pro (001)</td>
<td style="text-align: center;">$0.55(28) \rightarrow 0.91(5)$</td>
<td style="text-align: center;">$0.90(4) \rightarrow 0.99(2)$</td>
<td style="text-align: center;">$0.62(2) \rightarrow 0.72(5)$</td>
<td style="text-align: center;">$0.67(9) \rightarrow 0.67(7)$</td>
<td style="text-align: center;">$0.37(55) \rightarrow 0.94(8)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 (0613)</td>
<td style="text-align: center;">$0.60(3) \rightarrow 0.86(10)$</td>
<td style="text-align: center;">$0.87(11) \rightarrow 0.99(2)$</td>
<td style="text-align: center;">$0.55(18) \rightarrow 0.75(5)$</td>
<td style="text-align: center;">$0.73(2) \rightarrow 0.68(6)$</td>
<td style="text-align: center;">$0.91(5) \rightarrow 0.43(10)$</td>
</tr>
<tr>
<td style="text-align: center;">Qwen2 Instruct (72B)</td>
<td style="text-align: center;">$0.56(24) \rightarrow 0.88(6)$</td>
<td style="text-align: center;">$0.91(3) \rightarrow 0.96(5)$</td>
<td style="text-align: center;">$0.65(1) \rightarrow 0.68(4)$</td>
<td style="text-align: center;">$0.66(10) \rightarrow 0.74(2)$</td>
<td style="text-align: center;">$0.89(11) \rightarrow 0.94(6)$</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4 Turbo (2024-04-09)</td>
<td style="text-align: center;">$\mathbf{0 . 6 0}(4) \rightarrow \mathbf{0 . 9 3}(4)$</td>
<td style="text-align: center;">$\mathbf{0 . 8 7}(11) \rightarrow \mathbf{1 . 0 0}(4)$</td>
<td style="text-align: center;">$\mathbf{0 . 5 3}(22) \rightarrow \mathbf{0 . 7 6}(4)$</td>
<td style="text-align: center;">$0.67(8) \rightarrow 0.63(9)$</td>
<td style="text-align: center;">$0.90(9) \rightarrow 0.93(8)$</td>
</tr>
<tr>
<td style="text-align: center;">Gemini 1.5 Pro (0409 preview)</td>
<td style="text-align: center;">$\mathbf{0 . 5 8}(38) \rightarrow \mathbf{0 . 9 3}(4)$</td>
<td style="text-align: center;">$0.86(18) \rightarrow 0.92(10)$</td>
<td style="text-align: center;">$0.58(13) \rightarrow 0.67(9)$</td>
<td style="text-align: center;">$0.64(15) \rightarrow 0.61(10)$</td>
<td style="text-align: center;">$0.40(56) \rightarrow 0.95(5)$</td>
</tr>
<tr>
<td style="text-align: center;">Llama 3.1 Instruct Turbo (70B)</td>
<td style="text-align: center;">$\mathbf{0 . 5 8}(42) \rightarrow \mathbf{0 . 9 3}(4)$</td>
<td style="text-align: center;">$0.84(27) \rightarrow 0.96(5)$</td>
<td style="text-align: center;">$0.59(9) \rightarrow 0.64(10)$</td>
<td style="text-align: center;">$0.67(5) \rightarrow 0.65(8)$</td>
<td style="text-align: center;">$0.86(20) \rightarrow 0.96(2)$</td>
</tr>
</tbody>
</table>
<p>approaches to detect these errors automatically: Zero-Shot prompting We provide the model with a simple instruction to classify questions into "ok" or "not ok" without introducing any demonstrations. The prompt can be found in Appendix G.
Few-shot prompting We provide the model with two examples for each error type to guide its classification decisions.
Chain of Thought (CoT) prompting (Wei et al., 2022) We encourage the model to generate reasoning steps before producing the final answer, in both zero-shot and few-shot settings. The prompt format can be found in Appendix G.
Retrieval-augmented prompting (RAG) We retrieve 5 most relevant paragraphs from Wikipedia and MS-MARCO and append them as context for zero-shot and CoT prompting.
Instruction fine-tuning We fine-tune Llama3 8BInstruct model (Dubey et al., 2024) using curated data and evaluate its performance on MMLURedux. Refer to Appendix J for more details.</p>
<p>In the following, we introduce these error detection strategies and their evaluation results in detail.</p>
<h3>4.1 Error Detection Experiments</h3>
<p>To evaluate the performance of LLMs in detecting errors in the MMLU-Redux dataset, we conduct experiments with 4 state-of-the-art models: OpenAI's GPT-4 Turbo, GPT-4o, Anthropic's Claude-3 Opus, and Meta's LlamA-3-70B (Dubey et al., 2024). For each model, we test both standard prompting and Chain of Thought (CoT) prompting methods (Wei et al., 2022). Details about the prompts are provided in Appendix G.</p>
<p>We consider "not ok" as the positive class, and "ok" as the negative class for the calculation of Recall, F1, and F2 scores. Based on Table 3, the Few-shot CoT setting outperforms other settings</p>
<p>Table 3: Comparison of different methods and models for error detection in MMLU-Redux scores.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Model</th>
<th style="text-align: center;">Method</th>
<th style="text-align: center;">Recall</th>
<th style="text-align: center;">F1</th>
<th style="text-align: center;">F2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">GPT-4 Turbo</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">22.81</td>
<td style="text-align: center;">22.70</td>
<td style="text-align: center;">23.90</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-Shot CoT</td>
<td style="text-align: center;">27.97</td>
<td style="text-align: center;">22.97</td>
<td style="text-align: center;">23.47</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">26.55</td>
<td style="text-align: center;">27.93</td>
<td style="text-align: center;">27.97</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot CoT</td>
<td style="text-align: center;">46.68</td>
<td style="text-align: center;">31.68</td>
<td style="text-align: center;">36.58</td>
</tr>
<tr>
<td style="text-align: center;">GPT-4o</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">28.92</td>
<td style="text-align: center;">15.99</td>
<td style="text-align: center;">28.05</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-Shot CoT</td>
<td style="text-align: center;">19.85</td>
<td style="text-align: center;">21.41</td>
<td style="text-align: center;">19.98</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">37.40</td>
<td style="text-align: center;">24.93</td>
<td style="text-align: center;">31.06</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot CoT</td>
<td style="text-align: center;">38.47</td>
<td style="text-align: center;">29.26</td>
<td style="text-align: center;">31.36</td>
</tr>
<tr>
<td style="text-align: center;">Claude 3 Opus</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">27.11</td>
<td style="text-align: center;">27.00</td>
<td style="text-align: center;">27.65</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-Shot CoT</td>
<td style="text-align: center;">44.87</td>
<td style="text-align: center;">34.68</td>
<td style="text-align: center;">38.19</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">38.63</td>
<td style="text-align: center;">29.45</td>
<td style="text-align: center;">34.89</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot CoT</td>
<td style="text-align: center;">48.85</td>
<td style="text-align: center;">24.03</td>
<td style="text-align: center;">40.29</td>
</tr>
<tr>
<td style="text-align: center;">Llama3-70B</td>
<td style="text-align: center;">Zero-Shot</td>
<td style="text-align: center;">10.06</td>
<td style="text-align: center;">8.15</td>
<td style="text-align: center;">9.46</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Zero-Shot CoT</td>
<td style="text-align: center;">10.74</td>
<td style="text-align: center;">8.15</td>
<td style="text-align: center;">10.17</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot</td>
<td style="text-align: center;">17.82</td>
<td style="text-align: center;">18.58</td>
<td style="text-align: center;">17.60</td>
</tr>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Few-shot CoT</td>
<td style="text-align: center;">24.87</td>
<td style="text-align: center;">23.16</td>
<td style="text-align: center;">23.10</td>
</tr>
</tbody>
</table>
<p>across all models, suggesting that providing a small set of labelled examples with step-by-step reasoning instructions can improve error detection performance. However, even the best-performing model, Claude-3-opus, only achieves an F2 Score of 40.29, highlighting the difficulty of this task.</p>
<p>We then use RAG prompting to investigate the impact of external knowledge on error detection. We use BM25 to retrieve five most relevant paragraphs from enwiki-paragraphs and msmarco-v1passage corpus provided by Pyserini (Lin et al., 2021a) using the question as the query. We then use these paragraphs as additional context in the prompt to classify the question and answer choices.</p>
<p>Based on Table 4, the Claude 3 Opus model with the zero-shot method on the MS MARCO index achieves the highest Recall of 83.91. Claude 3 Opus achieves an F2 Score of 41.92 with the zeroshot method on the Wikipedia index. The GPT-</p>
<p>Table 4: Comparison of different methods and models for error detection using RAG.</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Index</th>
<th>Method</th>
<th>Recall</th>
<th>F1</th>
<th>F2</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4 Turbo</td>
<td>Wikipedia</td>
<td>Zero-Shot</td>
<td>57.00</td>
<td>27.47</td>
<td>36.87</td>
</tr>
<tr>
<td></td>
<td>Wikipedia</td>
<td>Zero-Shot CoT</td>
<td>46.63</td>
<td>31.52</td>
<td>36.13</td>
</tr>
<tr>
<td></td>
<td>MS MARCO</td>
<td>Zero-Shot</td>
<td>57.11</td>
<td>26.34</td>
<td>35.02</td>
</tr>
<tr>
<td></td>
<td>MS MARCO</td>
<td>Zero-Shot CoT</td>
<td>37.20</td>
<td>26.07</td>
<td>29.64</td>
</tr>
<tr>
<td>GPT-4o</td>
<td>Wikipedia</td>
<td>Zero Shot</td>
<td>35.01</td>
<td>27.25</td>
<td>29.79</td>
</tr>
<tr>
<td></td>
<td>Wikipedia</td>
<td>Zero-Shot CoT</td>
<td>33.67</td>
<td>26.90</td>
<td>28.75</td>
</tr>
<tr>
<td></td>
<td>MS MARCO</td>
<td>Zero-Shot</td>
<td>29.97</td>
<td>28.07</td>
<td>28.31</td>
</tr>
<tr>
<td></td>
<td>MS MARCO</td>
<td>Zero-Shot CoT</td>
<td>28.65</td>
<td>24.14</td>
<td>25.30</td>
</tr>
<tr>
<td>Llama3-70B</td>
<td>Wikipedia</td>
<td>Zero Shot</td>
<td>22.78</td>
<td>20.41</td>
<td>21.00</td>
</tr>
<tr>
<td></td>
<td>Wikipedia</td>
<td>Zero-Shot CoT</td>
<td>10.12</td>
<td>12.61</td>
<td>10.90</td>
</tr>
<tr>
<td></td>
<td>MS MARCO</td>
<td>Zero-Shot</td>
<td>28.45</td>
<td>22.39</td>
<td>24.67</td>
</tr>
<tr>
<td></td>
<td>MS MARCO</td>
<td>Zero-Shot CoT</td>
<td>10.18</td>
<td>14.00</td>
<td>11.40</td>
</tr>
<tr>
<td>Claude 3 Opus</td>
<td>Wikipedia</td>
<td>Zero-Shot</td>
<td>82.61</td>
<td>28.72</td>
<td>41.92</td>
</tr>
<tr>
<td></td>
<td>MS MARCO</td>
<td>Zero-Shot</td>
<td>83.91</td>
<td>28.09</td>
<td>41.27</td>
</tr>
</tbody>
</table>
<p>4 models perform relatively worse than Claude, with the GPT-4o model showing lower scores than GPT-4 Turbo. Comparing the retrieval indexes, Wikipedia generally outperforms MS MARCO for the GPT-4o model, while the results are mixed for the GPT-4 Turbo and Llama3-70B models. The RAG approach outperforms the few-shot CoT setting mentioned in the previous analysis, indicating that incorporating retrieved information can enhance error detection performance.</p>
<p>Based on the results presented in Table 3 and 4, we can conclude that automatic error detection in the MMLU dataset remains a challenging task, despite the availability of annotated error types. While Claude 3 Opus demonstrates the highest Recall, F1, and F2 Scores compared to other models, Claude 3 Opus shows the highest performance with RAG, indicating its potential for identifying errors more effectively. However, the best-performing model and method combination using RAG still achieves relatively low scores, suggesting the overall reliability of the models in detecting errors across the diverse range of subjects in MMLU is still limited. Detailed performance across all subjects can be found in Appendix H.</p>
<h2>5 Related Work</h2>
<p>Benchmark Issues While benchmarks often enable methodological progress, they can also be counterproductive when labelling mistakes and annotation artefacts exist. For example, Beyer et al. (2020) show how annotation artefacts within the popular ImageNet benchmark (Russakovsky et al., 2015) led to likely overstated performance gains that did not necessarily transfer to other datasets and tasks. In NLP, similar issues have been found in summarisation (Tejaswin et al., 2021; Bhandari
et al., 2020) and natural language inference (Gururangan et al., 2018; Poliak et al., 2018; Stacey et al., 2020; Wu et al., 2022) benchmarks.</p>
<p>Benchmark issues can arise from biases in the framing of the task (Schwartz et al., 2017); noisy annotations (Chen et al., 2016); web-crawls (Raffel et al., 2023; Lee et al., 2021; Kaddour, 2023); automated labelling processes such as crowdsourcing (Yuen et al., 2011) - where annotators may be optimising their own incentives; human errors (Peterson et al., 2019) - e.g., lack of expertise on a given topic; or programmatic weak supervision (Zhang et al., 2022; Goswami et al., 2021; Kaddour et al., 2023), and personal (Geva et al., 2019) or group-level (Liu et al., 2022) annotator biases.</p>
<p>MMLU Issues and MMLU-Pro. The broad adoption of the MMLU benchmark for LLM evaluations (Anil et al., 2023a; OpenAI, 2023; Touvron et al., 2023; Anthropic, 2024; Dubey et al., 2024) means identifying issues or improvements is crucial for ensuring its continued applicability. Recent studies have identified issues with labelling errors and ambiguous questions in similar benchmarks, such as MedQA (Saab et al., 2024). Concurrent work developing the MMLU-Pro (Wang et al., 2024) benchmark also identifies a number of issues within a filtered and augmented subset of the original MMLU dataset and re-annotates parts of the dataset for inclusion in the new MMLU-Pro evaluation. However, despite these efforts, errors from the original MMLU persist in the extended MMLU-Pro benchmark, highlighting the importance of our work in systematically identifying and addressing dataset errors. The recent Global MMLU dataset (Singh et al., 2024) reevaluated MMLU to study which questions are culturally biased. We observed a similar Western bias in our qualitative analysis of the errors. Additionally, there is currently limited literature categorising and quantifying these issues across the original MMLU dataset to help inform our understanding of previous results. The AI Explained YouTube Channel recently highlighted several erroneous instances in MMLU across several subjects while evaluating their SmartGPT framework in a recent popular video (L and Stapleton, 2023). However, our study aims to provide a more systematic assessment of MMLU.</p>
<h2>6 Conclusion</h2>
<p>We analyse the MMLU benchmark, driven by the necessity for rigorous evaluation of its reliability. Our analysis using a hierarchical annotation protocol reveals that a significant portion of MMLU instances contain inaccuracies that could lead to misleading evaluation results. For example, $57 \%$ of the instances in the Virology and $26 \%$ in the Logical fallacies subsets were found to be inaccurate. To this end, we introduce MMLU-Redux, a thoroughly reviewed subset of the MMLU dataset (Hendrycks et al., 2021) comprising 5,700 questions spanning all 57 MMLU subjects, and estimate that MMLU has an error rate of $6.49 \%$. The re-evaluation of LLMs using MMLU-Redux shows a significant variation in performance metrics and shifts in model rankings for several subsets, emphasising the impact that dataset quality can have on the evaluation of LLMs. Furthermore, we analyse whether it is possible to identify errors automatically; although Claude 3 Opus seems to produce the most accurate results on this task ( $41.9 \%$ F2 score when using retrieval-augmented generation), it is still insufficient to produce a high-quality dataset.</p>
<h2>Limitations</h2>
<p>Although our analysis uses a significant subset of 5,700 questions, the accuracy of this analysis, both quantitatively and qualitatively, can be further improved with additional annotation of the remaining 8,342 questions in MMLU. Therefore, we open up MMLU-Redux for additional annotation of both additional MMLU subjects and add to the already reviewed subsets. However, we acknowledge that the annotation protocol we introduced to classify errors might still be prone to the personal biases of the annotators.</p>
<h2>References</h2>
<p>Mubashara Akhtar, Omar Benjelloun, Costanza Conforti, Pieter Gijsbers, Joan Giner-Miguelez, Nitisha Jain, Michael Kuchnik, Quentin Lhoest, Pierre Marcenac, Manil Maskey, Peter Mattson, Luis Oala, Pierre Ruyssen, Rajat Shinde, Elena Simperl, Goeffry Thomas, Slava Tykhonov, Joaquin Vanschoren, Jos van der Velde, Steffen Vogler, and Carole-Jean Wu. 2024. Croissant: A metadata format for ml-ready datasets. DEEM '24, page 1-6, New York, NY, USA. Association for Computing Machinery.</p>
<p>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math
word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2357-2367, Minneapolis, Minnesota. Association for Computational Linguistics.</p>
<p>Rohan Anil, Sebastian Borgeaud, Yonghui Wu, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy P. Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul Ronald Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, Anaïs White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, and et al. 2023a. Gemini: A family of highly capable multimodal models. CoRR, abs/2312.11805.</p>
<p>Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernández Ábrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clément Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, and et al. 2023b. Palm 2 technical report. CoRR, abs/2305.10403.</p>
<p>Anthropic. 2023. Anthropic. model card and evaluations for claude models.</p>
<p>AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card.</p>
<p>Lucas Beyer, Olivier J. Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. Are we done with imagenet? CoRR, abs/2006.07159.</p>
<p>Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, and Graham Neubig. 2020. Reevaluating evaluation in text summarization. In EMNLP (1), pages 9347-9359. Association for Computational Linguistics.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind</p>
<p>Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. CoRR, abs/2005.14165.</p>
<p>Danqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A thorough examination of the cnn/daily mail reading comprehension task. In ACL (1). The Association for Computer Linguistics.</p>
<p>Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv:1803.05457v1.</p>
<p>Jacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37-46.</p>
<p>Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783.</p>
<p>Daniel Erenrich. 2023. Errors in the MMLU: The Deep Learning Benchmark is Wrong Surprisingly Often. URL: https://derenrich.medium.com/ errors-in-the-mmlu-the-deep-learning-benchmark</p>
<p>Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation.</p>
<p>Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In EMNLP/IJCNLP (1), pages 1161-1166. Association for Computational Linguistics.</p>
<p>Mononito Goswami, Benedikt Boecking, and Artur Dubrawski. 2021. Weak supervision for affordable modeling of electrocardiogram data. In AMIA. AMIA.</p>
<p>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In NAACL-HLT (2), pages 107-112. Association for Computational Linguistics.</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. In ICLR. OpenReview.net.</p>
<p>Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.</p>
<p>Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. 2021. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421.</p>
<p>Jean Kaddour. 2023. The minipile challenge for dataefficient language models. CoRR, abs/2304.08442.</p>
<p>Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie Bradley, Roberta Raileanu, and Robert McHardy. 2023. Challenges and applications of large language models. CoRR, abs/2307.10169.</p>
<p>Philip L and Joshua Stapleton. 2023. AI Explained YouTube Channel - SmartGPT: Major Benchmark Broken - 89.0\% on MMLU + Exam's Many Errors. URL: https://youtu.be/hVade_8H8mE?t=829.</p>
<p>Md. Tahmid Rahman Laskar, M. Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. A systematic study and comprehensive evaluation of chatgpt on benchmark datasets. In ACL (Findings), pages 431-469. Association for Computational Linguistics.
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499.</p>
<p>Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:9459-9474.</p>
<p>Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana AcostaNavas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibuni Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang,</p>
<p>Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic evaluation of language models. Transactions on Machine Learning Research. Featured Certification, Expert Certification.</p>
<p>Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, JhengHong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021a. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021), pages 2356-2362.</p>
<p>Stephanie Lin, Jacob Hilton, and Owain Evans. 2021b. Truthfulqa: Measuring how models mimic human falsehoods. Preprint, arXiv:2109.07958.</p>
<p>Haochen Liu, Joseph Thekinen, Sinem Mollaoglu, Da Tang, Ji Yang, Youlong Cheng, Hui Liu, and Jiliang Tang. 2022. Toward annotator group bias in crowdsourcing. In ACL (1), pages 1797-1806. Association for Computational Linguistics.</p>
<p>Ilya Loshchilov and Frank Hutter. 2018. Decoupled weight decay regularization. In International Conference on Learning Representations.</p>
<p>Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. In EMNLP.</p>
<p>OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774.</p>
<p>Joshua C. Peterson, Ruairidh M. Battleday, Thomas L. Griffiths, and Olga Russakovsky. 2019. Human uncertainty makes classification more robust. In ICCV, pages 9616-9625. IEEE.</p>
<p>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In *SEM@NAACL-HLT, pages 180-191. Association for Computational Linguistics.</p>
<p>Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring the limits of transfer learning with a unified text-to-text transformer. Preprint, arXiv:1910.10683.</p>
<p>Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. 2015. Imagenet large scale visual recognition challenge. International journal of computer vision, $115: 211-252$.</p>
<p>Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, Mike Schaekermann, Aishwarya Kamath, Yong Cheng, David G. T. Barrett, Cathy Cheung, Basil Mustafa, Anil Palepu,</p>
<p>Daniel McDuff, Le Hou, Tomer Golany, Luyang Liu, Jean-Baptiste Alayrac, Neil Houlsby, Nenad Tomasev, Jan Freyberg, Charles Lau, Jonas Kemp, Jeremy Lai, Shekoofeh Azizi, Kimberly Kanada, SiWai Man, Kavita Kulkarni, Ruoxi Sun, Siamak Shakeri, Luheng He, Benjamin Caine, Albert Webson, Natasha Latysheva, Melvin Johnson, Philip Andrew Mansfield, Jian Lu, Ehud Rivlin, Jesper Anderson, Bradley Green, Renee Wong, Jonathan Krause, Jonathon Shlens, Ewa Dominowska, S. M. Ali Eslami, Katherine Chou, Claire Cui, Oriol Vinyals, Koray Kavukcuoglu, James Manyika, Jeff Dean, Demis Hassabis, Yossi Matias, Dale R. Webster, Joelle K. Barral, Greg Corrado, Christopher Semturs, S. Sara Mahdavi, Juraj Gottweis, Alan Karthikesalingam, and Vivek Natarajan. 2024. Capabilities of gemini models in medicine. CoRR, abs/2404.18416.</p>
<p>Roy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, and Noah A. Smith. 2017. The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In CoNLL, pages 15-25. Association for Computational Linguistics.</p>
<p>Chenhui Shen, Liying Cheng, Yang You, and Lidong Bing. 2023. Are large language models good evaluators for abstractive summarization? CoRR, abs/2305.13091.</p>
<p>Shivalika Singh, Angelika Romanou, Clémentine Fourrier, David I. Adelani, Jian Gang Ngui, Daniel Vila-Suero, Peerat Limkonchotiwat, Kelly Marchisio, Wei Qi Leong, Yosephine Susanto, Raymond Ng, Shayne Longpre, Wei-Yin Ko, Madeline Smith, Antoine Bosselut, Alice Oh, Andre F. T. Martins, Leshem Choshen, Daphne Ippolito, Enzo Ferrante, Marzieh Fadaee, Beyza Ermis, and Sara Hooker. 2024. Global mmlu: Understanding and addressing cultural and linguistic biases in multilingual evaluation. Preprint, arXiv:2412.03304.</p>
<p>Joe Stacey, Pasquale Minervini, Haim Dubossarsky, Sebastian Riedel, and Tim Rocktäschel. 2020. Avoiding the hypothesis-only bias in natural language inference via ensemble adversarial training. In EMNLP (1), pages 8281-8291. Association for Computational Linguistics.</p>
<p>Priyam Tejaswin, Dhruv Naik, and Pengfei Liu. 2021. How well do you know your summarization datasets? In ACL/IJCNLP (Findings), volume ACL/IJCNLP 2021 of Findings of ACL, pages 3436-3449. Association for Computational Linguistics.</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian CantonFerrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,</p>
<p>Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and finetuned chat models. CoRR, abs/2307.09288.</p>
<p>Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. 2024. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. Preprint, arXiv:2406.01574.</p>
<p>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824-24837.</p>
<p>Yuxiang Wu, Matt Gardner, Pontus Stenetorp, and Pradeep Dasigi. 2022. Generating data to mitigate spurious correlations in natural language inference datasets. In ACL (1), pages 2660-2676. Association for Computational Linguistics.</p>
<p>Man-Ching Yuen, Irwin King, and Kwong-Sak Leung. 2011. A survey of crowdsourcing systems. In SocialCom/PASSAT, pages 766-773. IEEE Computer Society.</p>
<p>Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. 2022. A survey on programmatic weak supervision. CoRR, abs/2202.05433.</p>
<h1>A Responsible Research Checklist</h1>
<ol>
<li>For all authors...
(a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We claim to develop a dataset for studying the errors present in MMLU, which we present.
(b) Did you describe the limitations of your work? [Yes] See Section 6
(c) Did you discuss any potential negative societal impacts of your work? [No] See section 6
(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]</li>
<li>If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [N/A]
(b) Did you include complete proofs of all theoretical results? [N/A]</li>
<li>If you ran experiments (e.g. for benchmarks)...
(a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Section 3
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] We provide details for all proposed methods. For prompting strategies, we show prompts in Appendix I For fine-tuning, we provide details about the training dataset in Appendix J.
(c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] Rerunning experiments multiple times to obtain error bars would have exceeded our funding capabilities.
(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] We provide details about our resources in Appendix J</li>
<li>If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [Yes] Our work is based on the MMLU dataset, which we cited in Section 1.
(b) Did you mention the license of the assets? [Yes] The license of the dataset is available on the dataset URL (CC-BY 4.0).
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes] MMLU-Redux is provided as a supplementary material.
(d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] Our dataset is based on MMLU, which has an MIT license. The annotation work for MMLU-Redux is done by the authors of this paper, who all gave consent to use the data.
(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] The MMLU data does not use personal data as it is based on publicly available test questions, and so neither does MMLU-Redux.</li>
<li>If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if applicable? $[\mathrm{N} / \mathrm{A}]$
(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</li>
</ol>
<h1>B Data Collection and Organisation</h1>
<p>The MMLU-Redux dataset is available at https://huggingface.co/datasets/edinburgh-dawg/ mmlu-redux-2.0 (doi:10.57967/hf/2507) with a CC-BY-4.0 licence. This link also contains a public datasheet. The data used to create MMLU-Redux was obtained from cais/mmlu ${ }^{2}$, which is also utilised in the 'lm-eval-harness' framework (Gao et al., 2023). To ensure uniformity of our results, the language model (LM) predictions used in our performance analyses were obtained from the Holistic Evaluation of Language Models (HELM) leaderboard v1.3.0, released on May 15th, 2024.</p>
<p>We randomly subsampled 100 questions per MMLU subject to be presented to the annotators. The annotators are instructed to follow the introduced taxonomy by first assessing the question presentation, and then by verifying the ground truth MMLU label. The annotators were encouraged to perform an exact match search using a search engine to find occurrences of the question and multiple-choice options from credible sources. If the annotators found an exact match of the question-options pair, the annotators were asked to evaluate the answer provided by the source. Regardless of whether a label was found in the source, and whether the MMLU label is the same or not, the annotators were asked to decide whether they would follow the label using their expertise. In the cases where an exact match was not found, the annotators were asked to search for supporting evidence from trusted sources, such as government websites, textbooks, and/or other reputable organisations (e.g., World Health Organisation (WHO)). In cases where the annotators are still unsure, they were asked to annotate the question with "Expert", denoting that the question requires more expertise. This annotated subset of MMLU is denoted as MMLU-Redux.</p>
<p>MMLU-Redux comprises subsampled test splits of the aforementioned thirty MMLU subsets (denoted as "config" in HF vocabulary).</p>
<p>Each data point in MMLU-Redux contains seven columns:</p>
<ul>
<li>question (string): The original MMLU question.</li>
<li>choices (list of four strings): The original list of four choices associated with the question from the MMLU dataset.</li>
<li>answer (integer): The MMLU ground truth label in the form of an array index between 0 and 3.</li>
<li>error_type (string): The annotated error_type. The values can be one of the six error types proposed in the taxonomy ("ok", "bad_question_clarity", "bad_options_clarity", "no_correct_answer", "multiple_correct_answers", "wrong_groundtruth") and "expert".</li>
<li>source (string): The potential source of the question.</li>
<li>correct_answer (string): In the case of "no_correct_answer" and "wrong_groundtruth", the annotators can suggest the alternative correct answer.</li>
<li>potential_reason (string): A free text column for the annotators to note what they believe to have caused the error.</li>
</ul>
<p>The question, choices, and answer columns are taken from the MMLU Hugging Face dataset (cais/mmlu).
To ensure reproducibility and facilitate further research, we will make MMLU-Redux publicly available on the Hugging Face (HF) platform in the Croissant format (Akhtar et al., 2024).This format provides a standardised and easily accessible structure for the dataset, allowing researchers and practitioners to readily utilise it for their own investigations and analyses. The code to generate the data and analyses will be made available in the camera ready.</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>C Usage Guidelines</h1>
<p>To reproduce our results or perform analyses similar to those presented in this study, the user may download the data and utilise all the columns. MMLU-Redux contains both correct and erroneous instances, so the user should look at the value in column "error_type" to filter samples based on the specific error type.</p>
<p>In those cases where the error is "no_correct_answer", "multiple_correct_answers" or "wrong_groundtruth", the users may examine the suggested answer reported in the "correct_answer" column. The user should consider that the questions and the options reported are the same as those in the MMLU dataset, and they have not been modified, even when affected by bad clarity.</p>
<h2>D Maintenance Plan</h2>
<h2>D. 1 General Maintenance</h2>
<p>The datasets will be updated as needed to maintain accuracy, with announcements made for each update. These updates will be posted on the dataset's Hugging Face page. Older versions will be preserved and documented using the git commit hash of the dataset repository.</p>
<h2>D. 2 Contributing Guidelines</h2>
<p>We welcome contributions from the research community to enhance and expand the MMLU-Redux dataset. Primarily, we foresee two types of contribution:</p>
<p>Additional data annotation Currently, MMLU-Redux covers 5,700 MMLU questions, which is a subset of the full MMLU dataset. We encourage the community to help annotate more questions to cover the entire dataset. Generally, we recommend you to follow these steps:</p>
<ol>
<li>Familiarise yourself with the taxonomy. The taxonomy is designed to be simple and broad to cover all possible erroneous cases found in MMLU.</li>
<li>Open a new discussion on the Hugging Face page of MMLU-Redux (https://huggingface.co/ datasets/edinburgh-dawg/MMLU-Redux/discussions/new)</li>
<li>Insert the title with the prefix "[ADD]" to signify additional data annotation, followed by the name of the MMLU subject you are contributing to. For example, "[ADD] Virology".</li>
<li>In the description body of the issue, specify the question, the choices, the MMLU answer, the error type, the correct answer (if applicable), the question source, what you believe to be the reason for the error, and optionally an additional comment. Follow this markdown template:</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="o">**</span><span class="nx">Question</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">question</span><span class="p">}</span>
<span class="o">**</span><span class="nx">Choices</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">choices</span><span class="p">}</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="nx">Copy</span><span class="w"> </span><span class="nx">paste</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">python</span><span class="w"> </span><span class="nx">array</span>
<span class="o">**</span><span class="nx">MMLU</span><span class="w"> </span><span class="nx">answer</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">mmlu_answer</span><span class="p">}</span>
<span class="o">**</span><span class="nx">Error</span><span class="w"> </span><span class="k">type</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">error_type</span><span class="p">}</span>
<span class="err">#</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">this</span><span class="w"> </span><span class="nx">field</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">applicable</span><span class="w"> </span><span class="p">(</span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">bad</span><span class="w"> </span><span class="nx">question</span><span class="w"> </span><span class="nx">clarity</span>
<span class="err">#</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">bad</span><span class="w"> </span><span class="nx">options</span><span class="w"> </span><span class="nx">clarity</span><span class="p">),</span><span class="w"> </span><span class="nx">write</span><span class="w"> </span><span class="s">&quot;N/A&quot;</span>
<span class="o">**</span><span class="nx">Correct</span><span class="w"> </span><span class="nx">answer</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">correct_answer</span><span class="p">}</span>
<span class="err">#</span><span class="w"> </span><span class="nx">Ideally</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">URL</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">find</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">source</span><span class="w"> </span><span class="nx">online</span><span class="p">.</span>
<span class="o">**</span><span class="nx">Source</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">source</span><span class="p">}</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="nx">This</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">strictly</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">describe</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">might</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">caused</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">error</span>
<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="p">(</span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="nx">incorrect</span><span class="w"> </span><span class="nx">HTML</span><span class="w"> </span><span class="nx">parsing</span><span class="p">)</span>
<span class="o">**</span><span class="nx">Potential</span><span class="w"> </span><span class="nx">error</span><span class="w"> </span><span class="nx">reason</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">free</span><span class="w"> </span><span class="nx">text</span><span class="p">}</span>
<span class="err">#</span><span class="w"> </span><span class="nx">This</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">annotators</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">leave</span><span class="w"> </span><span class="nx">additional</span><span class="w"> </span><span class="nx">comments</span>
<span class="o">**</span><span class="nx">Additional</span><span class="w"> </span><span class="nx">comment</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">free</span><span class="w"> </span><span class="nx">text</span><span class="p">}</span>
</code></pre></div>

<ol>
<li>If you intend to propose annotation of more than one question, we ask you to use a delimiter such as three-dashes ("---").</li>
<li>Submit the discussion.</li>
</ol>
<p>We aim to respond to discussions within 48 hours. The proposed changes will either be integrated into the associated MMLU subject or receive feedback for further adjustments and clarification. Discussions with fully integrated changes will be closed.</p>
<p>Fix to the existing annotation The current and future annotated questions in MMLU-Redux may contain inaccuracies, particularly for questions whose answers may change over time, and other factors. We believe that the community can help keep the data up to date. We recommend everyone to propose fixes by following these steps:</p>
<ol>
<li>Familiarise yourself with the taxonomy. The taxonomy is simple and broad to cover all possible erroneous cases found in MMLU.</li>
<li>Open a new discussion on the Hugging Face page of MMLU-Redux (https://huggingface.co/ datasets/edinburgh-dawg/MMLU-Redux/discussions/new)</li>
<li>Insert the title with the prefix "[FIX]" to signify additional data annotation, followed by the name of the MMLU subject you are contributing to. For example, "[FIX] Virology".</li>
<li>In the description body, specify the question, the choices, the MMLU answer, the current error type, the new error type, the current possible correct answer, the new correct answer, the question source, the potential reason that causes the error, and the optional additional comments. Follow this markdown template:</li>
</ol>
<div class="codehilite"><pre><span></span><code><span class="o">**</span><span class="w"> </span><span class="nx">Question</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">question</span><span class="p">}</span>
<span class="o">**</span><span class="w"> </span><span class="nx">Choices</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">choices</span><span class="p">}</span><span class="w"> </span><span class="err">#</span><span class="w"> </span><span class="nx">Copy</span><span class="w"> </span><span class="nx">paste</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">python</span><span class="w"> </span><span class="nx">array</span>
<span class="o">**</span><span class="nx">MMLU</span><span class="w"> </span><span class="nx">answer</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">mmlu_answer</span><span class="p">}</span>
<span class="o">**</span><span class="nx">Current</span><span class="w"> </span><span class="nx">Error</span><span class="w"> </span><span class="k">type</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">current_error_type</span><span class="p">}</span>
<span class="o">**</span><span class="nx">Current</span><span class="w"> </span><span class="nx">Correct</span><span class="w"> </span><span class="nx">answer</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">current_correct_answer</span><span class="p">}</span>
<span class="o">**</span><span class="nx">New</span><span class="w"> </span><span class="nx">Error</span><span class="w"> </span><span class="k">type</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">new_error_type</span><span class="p">}</span>
<span class="err">#</span><span class="w"> </span><span class="nx">If</span><span class="w"> </span><span class="nx">it</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="nx">applicable</span>
<span class="err">#</span><span class="w"> </span><span class="p">(</span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="nx">bad</span><span class="w"> </span><span class="nx">question</span><span class="w"> </span><span class="nx">clarity</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="nx">bad</span><span class="w"> </span><span class="nx">options</span><span class="w"> </span><span class="nx">clarity</span><span class="p">),</span><span class="w"> </span><span class="nx">write</span><span class="w"> </span><span class="s">&quot;N/A&quot;</span>
<span class="o">**</span><span class="nx">New</span><span class="w"> </span><span class="nx">Correct</span><span class="w"> </span><span class="nx">answer</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">new_correct_answer</span><span class="p">}</span>
<span class="err">#</span><span class="w"> </span><span class="nx">Ideally</span><span class="p">,</span><span class="w"> </span><span class="nx">a</span><span class="w"> </span><span class="nx">URL</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">you</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">find</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">source</span><span class="w"> </span><span class="nx">online</span><span class="p">.</span>
<span class="o">**</span><span class="nx">Source</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">source</span><span class="p">}</span>
<span class="err">#</span><span class="w"> </span><span class="nx">This</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="nx">strictly</span><span class="w"> </span><span class="nx">to</span><span class="w"> </span><span class="nx">describe</span><span class="w"> </span><span class="nx">what</span><span class="w"> </span><span class="nx">might</span><span class="w"> </span><span class="nx">have</span><span class="w"> </span><span class="nx">caused</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">error</span>
<span class="err">#</span><span class="w"> </span><span class="p">(</span><span class="nx">e</span><span class="p">.</span><span class="nx">g</span><span class="p">.,</span><span class="w"> </span><span class="nx">incorrect</span><span class="w"> </span><span class="nx">HTML</span><span class="w"> </span><span class="nx">parsing</span><span class="p">)</span>
<span class="o">**</span><span class="nx">Potential</span><span class="w"> </span><span class="nx">error</span><span class="w"> </span><span class="nx">reason</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">free</span><span class="w"> </span><span class="nx">text</span><span class="p">}</span>
<span class="err">#</span><span class="w"> </span><span class="nx">This</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">where</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">annotators</span><span class="w"> </span><span class="nx">can</span><span class="w"> </span><span class="nx">leave</span><span class="w"> </span><span class="nx">additional</span><span class="w"> </span><span class="nx">comments</span>
<span class="err">#</span><span class="w"> </span><span class="nx">about</span><span class="w"> </span><span class="nx">the</span><span class="w"> </span><span class="nx">data</span>
<span class="o">**</span><span class="nx">Additional</span><span class="w"> </span><span class="nx">comment</span><span class="o">**</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nx">free</span><span class="w"> </span><span class="nx">text</span><span class="p">}</span>
</code></pre></div>

<ol>
<li>If you intend to propose more than one fix, please use a delimiter such as three-dashes ("---").</li>
<li>Submit the discussion.</li>
</ol>
<p>For contributions that may not be suitable for submission via Hugging Face discussions or pull requests (e.g., bulk annotations stored in a CSV/PDF/DOCX file), we encourage you to reach out to us directly at aryo.gema@ed.ac.uk and p.minervini@ed.ac.uk, together with your HF username (if you have one and you are willing to be acknowledged). We will create a pull request which contains the proposed changes and tag your HF username. You can verify the changes and notify us via the discussion page if the pull request is ready to be merged.</p>
<h1>D. 3 Contingency Plan</h1>
<p>We are committed to ensuring the long-term availability and accessibility of MMLU-Redux. If the primary hosting platform (i.e., Hugging Face) becomes unavailable or ceases operations, we will immediately activate our contingency measures. This includes notifying all stakeholders and users of the dataset about the temporary unavailability and redirecting them to the mirror copies available on alternative platforms (i.e., Zenodo, https://zenodo.org/records/14051942). If the primary hosting ceases operations, we will ensure the dataset is reuploaded to a new reliable hosting platform, accompanied by updated documentation and links to the new location. We will also utilise social media and other networks to inform the community about the new location of the dataset.</p>
<h2>E Authors Statement</h2>
<p>The authors bear all responsibility in case of violation of rights.</p>
<h2>F MMLU-Redux Error Type Statistics</h2>
<p>In this section, we give the exact statistics of the error types found in MMLU-Redux. Table 5 contains an overview. We include the total number of questions in each subject, but note that we annotated a subset of 100 questions for each.</p>
<p>Table 5: Error types by subset. BQC is (1a) Bad Question Clarity, BOC is (1b) Bad Options Clarity, NCA is (2a) No Correct Answer, MCA is (2b) Multiple Correct Answers, and WGT is (2c) Wrong Ground Truth. The percentages in the Total row are estimated via stratified sampling over the subsets.</p>
<table>
<thead>
<tr>
<th>Subset</th>
<th>Original Size</th>
<th>Annotated</th>
<th>OK</th>
<th>BQC</th>
<th>BOC</th>
<th>NCA</th>
<th>MCA</th>
<th>WGT</th>
</tr>
</thead>
<tbody>
<tr>
<td>Virology</td>
<td>166</td>
<td>100</td>
<td>43</td>
<td>14</td>
<td>2</td>
<td>4</td>
<td>4</td>
<td>33</td>
</tr>
<tr>
<td>Logical fallacies</td>
<td>163</td>
<td>100</td>
<td>74</td>
<td>14</td>
<td>2</td>
<td>4</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>College chemistry</td>
<td>100</td>
<td>100</td>
<td>75</td>
<td>2</td>
<td>0</td>
<td>2</td>
<td>0</td>
<td>21</td>
</tr>
<tr>
<td>Professional law</td>
<td>1,534</td>
<td>100</td>
<td>82</td>
<td>4</td>
<td>1</td>
<td>0</td>
<td>11</td>
<td>2</td>
</tr>
<tr>
<td>Human sexuality</td>
<td>131</td>
<td>100</td>
<td>83</td>
<td>11</td>
<td>1</td>
<td>2</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Business ethics</td>
<td>100</td>
<td>100</td>
<td>86</td>
<td>14</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Formal logic</td>
<td>126</td>
<td>100</td>
<td>87</td>
<td>1</td>
<td>0</td>
<td>2</td>
<td>9</td>
<td>1</td>
</tr>
<tr>
<td>Human aging</td>
<td>223</td>
<td>100</td>
<td>88</td>
<td>12</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Global facts</td>
<td>100</td>
<td>100</td>
<td>88</td>
<td>2</td>
<td>1</td>
<td>4</td>
<td>0</td>
<td>5</td>
</tr>
<tr>
<td>Machine learning</td>
<td>112</td>
<td>100</td>
<td>89</td>
<td>3</td>
<td>5</td>
<td>0</td>
<td>1</td>
<td>2</td>
</tr>
<tr>
<td>Miscellaneous</td>
<td>783</td>
<td>100</td>
<td>90</td>
<td>4</td>
<td>1</td>
<td>4</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>Abstract algebra</td>
<td>100</td>
<td>100</td>
<td>91</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>6</td>
</tr>
<tr>
<td>Public relations</td>
<td>110</td>
<td>100</td>
<td>91</td>
<td>3</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>3</td>
</tr>
<tr>
<td>High school European history</td>
<td>165</td>
<td>100</td>
<td>91</td>
<td>4</td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Professional accounting</td>
<td>282</td>
<td>100</td>
<td>94</td>
<td>0</td>
<td>1</td>
<td>5</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Astronomy</td>
<td>152</td>
<td>100</td>
<td>94</td>
<td>2</td>
<td>2</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Security studies</td>
<td>245</td>
<td>100</td>
<td>94</td>
<td>6</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Conceptual physics</td>
<td>235</td>
<td>100</td>
<td>95</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>4</td>
</tr>
<tr>
<td>International law</td>
<td>121</td>
<td>100</td>
<td>95</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
<td>2</td>
</tr>
<tr>
<td>High school biology</td>
<td>310</td>
<td>100</td>
<td>95</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>College medicine</td>
<td>173</td>
<td>100</td>
<td>96</td>
<td>2</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Moral disputes</td>
<td>346</td>
<td>100</td>
<td>96</td>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Marketing</td>
<td>234</td>
<td>100</td>
<td>96</td>
<td>2</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Professional psychology</td>
<td>612</td>
<td>100</td>
<td>96</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>High school psychology</td>
<td>545</td>
<td>100</td>
<td>96</td>
<td>4</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>High school physics</td>
<td>151</td>
<td>100</td>
<td>97</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>Elementary mathematics</td>
<td>378</td>
<td>100</td>
<td>97</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>High school macroeconomics</td>
<td>390</td>
<td>100</td>
<td>97</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>College computer science</td>
<td>100</td>
<td>100</td>
<td>97</td>
<td>0</td>
<td>0</td>
<td>2</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>Computer security</td>
<td>100</td>
<td>100</td>
<td>97</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>3</td>
</tr>
<tr>
<td>Moral scenarios</td>
<td>895</td>
<td>100</td>
<td>97</td>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Econometrics</td>
<td>114</td>
<td>100</td>
<td>97</td>
<td>3</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Nutrition</td>
<td>306</td>
<td>100</td>
<td>98</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>High school microeconomics</td>
<td>238</td>
<td>100</td>
<td>98</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Sociology</td>
<td>201</td>
<td>100</td>
<td>98</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>High school statistics</td>
<td>216</td>
<td>100</td>
<td>98</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>2</td>
</tr>
<tr>
<td>College biology</td>
<td>144</td>
<td>100</td>
<td>98</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Electrical engineering</td>
<td>145</td>
<td>100</td>
<td>98</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>High school world history</td>
<td>237</td>
<td>100</td>
<td>99</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>Anatomy</td>
<td>135</td>
<td>100</td>
<td>99</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>High school mathematics</td>
<td>270</td>
<td>100</td>
<td>99</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
</tr>
<tr>
<td>High school chemistry</td>
<td>203</td>
<td>100</td>
<td>99</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Jurisprudence</td>
<td>108</td>
<td>100</td>
<td>99</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>World religions</td>
<td>171</td>
<td>100</td>
<td>99</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>US foreign policy</td>
<td>100</td>
<td>100</td>
<td>99</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Clinical knowledge</td>
<td>265</td>
<td>100</td>
<td>99</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Professional medicine</td>
<td>272</td>
<td>100</td>
<td>99</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>College mathematics</td>
<td>100</td>
<td>100</td>
<td>99</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Management</td>
<td>103</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Philosophy</td>
<td>311</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Medical genetics</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>High school US history</td>
<td>204</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>High school government and politics</td>
<td>193</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>High school geography</td>
<td>198</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>High school computer science</td>
<td>100</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>College physics</td>
<td>102</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Prehistory</td>
<td>324</td>
<td>100</td>
<td>100</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>Total</td>
<td>14,042</td>
<td>5,700</td>
<td>93.51\%</td>
<td>2.47\%</td>
<td>0.44\%</td>
<td>0.62\%</td>
<td>1.54\%</td>
<td>1.42\%</td>
</tr>
</tbody>
</table>
<h1>G Prompting Methods</h1>
<p>Below, we provide the prompts used for both standard prompting and Chain of Thought (CoT) prompting methods.</p>
<p>Throughout the evaluation, we used the test split of the MMLU-Redux loaded with the specified configuration. The hyperparameters include a temperature of 0.0 , top_p of 1 , frequency_penalty and presence_penalty of 0 , and max_tokens of 600 for both the standard prompting and Chain of Thought (CoT) prompting methods to ensure consistency and deterministic results. The default random seed was used.</p>
<h2>Standard Prompting</h2>
<p># Task:
Given a question, its choices, and the ground truth answer, classify the question as either 'ok' or 'not ok'.</p>
<ul>
<li>'ok' means that the question and the choices are understandable, and the ground truth answer is correct.</li>
<li>'not ok' means that the ground truth answer is incorrect, or the question and the choices are not well presented.</li>
</ul>
<p>Classify with 'ok' or 'not ok' WITHOUT PROVIDING ANY REASONING</p>
<h2>Chain of Thought (CoT) Prompting</h2>
<p># Task:
Given a triple consisting of a multiple choice question, its choices, and the corresponding ground truth answer, your task is to classify the triple into 'ok' or 'not ok'.
# Instructions:</p>
<ol>
<li>Question Presentation: Is the question well-presented? Assess clarity, grammar, and sufficiency of information.
1.1 If Yes, assess the presentation of the MC options.
1.2 If No, classify the issue as 'not ok'.</li>
<li>MC Options Presentation: Are the MC options well-presented? Check if the options are clear, distinct, and relevant to the question.
2.1 If Yes, determine if there is one potentially correct answer.
2.2 If No, classify the issue as 'not ok'.</li>
<li>Answer Evaluation: Is there one, more than one, or no potentially correct answer in the options list?
3.1 If one, continue to Ground Truth Answer Evaluation.
3.2 If more than one, classify the issue as 'not ok'.
3.3 If no correct answer, classify the issue as 'not ok'.</li>
<li>Ground Truth Answer Evaluation: Is the ground truth answer correct?
4.1. If Yes, classify as 'ok'.
4.2. If No, classify as 'not ok'.</li>
</ol>
<p>Provide your assessment in JSON format with keys 'Question Presentation', 'MC Options Presentation', 'Answer Evaluation', 'Ground Truth Answer Evaluation', 'Classification'.
The 'classification' is either 'ok', or 'not ok'.
FOLLOW THE EXACT EXAMPLE ANSWER FORMAT ALL IN ONE LINE WITHOUT PROVIDING EXPLANATION
# Example Answer:
{"Question Presentation": "Correct", "MC Options Presentation": "Correct", "Answer Evaluation": "One", "Ground
Truth Answer Evaluation": "Correct", "Classification": "Correct"}</p>
<p>H Details on automatic error detection
H. 1 Detailed Results on Error Detection Experiments for MMLU-Redux</p>
<p>Table 6: Comparison of Zero-Shot, Chain-of-Thought (CoT), and Few-Shot F1 score on 10 MMLU-Redux subjects.</p>
<table>
<thead>
<tr>
<th>Dataset</th>
<th>Models</th>
<th>Zero-Shot</th>
<th>Zero-Shot CoT</th>
<th>Few-Shot</th>
<th>Few-Shot CoT</th>
</tr>
</thead>
<tbody>
<tr>
<td>College Chemistry</td>
<td>GPT-4-Turbo</td>
<td>52.94</td>
<td>17.14</td>
<td>55.74</td>
<td>48.98</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>42.86</td>
<td>30.77</td>
<td>19.35</td>
<td>30.77</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>40.00</td>
<td>36.36</td>
<td>24.24</td>
<td>32.26</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>0.00</td>
<td>0.00</td>
<td>13.00</td>
<td>19.00</td>
</tr>
<tr>
<td>College Mathematics</td>
<td>GPT-4-Turbo</td>
<td>0.00</td>
<td>20.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>0.00</td>
<td>21.43</td>
<td>0.00</td>
<td>21.43</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>9.52</td>
<td>0.00</td>
<td>0.00</td>
<td>32.26</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>Econometrics</td>
<td>GPT-4-Turbo</td>
<td>0.00</td>
<td>8.70</td>
<td>28.57</td>
<td>20.69</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>0.00</td>
<td>29.41</td>
<td>40.00</td>
<td>29.41</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>15.38</td>
<td>0.00</td>
<td>46.15</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>0.00</td>
<td>0.00</td>
<td>57.00</td>
<td>55.00</td>
</tr>
<tr>
<td>Formal Logic</td>
<td>GPT-4-Turbo</td>
<td>23.33</td>
<td>22.22</td>
<td>26.09</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>33.33</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>26.67</td>
<td>55.81</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
</tr>
<tr>
<td>Global Facts</td>
<td>GPT-4-Turbo</td>
<td>19.35</td>
<td>30.00</td>
<td>20.00</td>
<td>44.44</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>16.67</td>
<td>28.57</td>
<td>21.05</td>
<td>28.57</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>28.57</td>
<td>32.26</td>
<td>38.09</td>
<td>37.50</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>0.00</td>
<td>0.00</td>
<td>22.00</td>
<td>32.00</td>
</tr>
<tr>
<td>High School Physics</td>
<td>GPT-4-Turbo</td>
<td>9.52</td>
<td>17.39</td>
<td>23.53</td>
<td>16.67</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>19.05</td>
<td>26.09</td>
<td>30.77</td>
<td>26.09</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>8.33</td>
<td>21.05</td>
<td>28.57</td>
<td>33.33</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>0.00</td>
<td>0.00</td>
<td>18.00</td>
<td>22.00</td>
</tr>
<tr>
<td>Machine Learning</td>
<td>GPT-4-Turbo</td>
<td>20.83</td>
<td>9.52</td>
<td>30.77</td>
<td>35.71</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>40.00</td>
<td>20.00</td>
<td>28.57</td>
<td>20.00</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>33.33</td>
<td>23.08</td>
<td>31.58</td>
<td>46.15</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>14.00</td>
</tr>
<tr>
<td>Professional Law</td>
<td>GPT-4-Turbo</td>
<td>25.00</td>
<td>17.14</td>
<td>21.43</td>
<td>22.86</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>8.00</td>
<td>23.53</td>
<td>8.00</td>
<td>23.53</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>29.79</td>
<td>32.43</td>
<td>16.00</td>
<td>23.53</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>11.00</td>
<td>9.00</td>
<td>8.00</td>
<td>9.00</td>
</tr>
<tr>
<td>Public Relations</td>
<td>GPT-4-Turbo</td>
<td>0.00</td>
<td>20.00</td>
<td>31.58</td>
<td>52.17</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>0.00</td>
<td>31.58</td>
<td>25.00</td>
<td>31.58</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>0.00</td>
<td>80.77</td>
<td>38.09</td>
<td>35.29</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>17.00</td>
<td>17.00</td>
<td>15.00</td>
<td>14.00</td>
</tr>
<tr>
<td>Virology</td>
<td>GPT-4-Turbo</td>
<td>76.00</td>
<td>73.12</td>
<td>81.19</td>
<td>75.27</td>
</tr>
<tr>
<td></td>
<td>GPT-4o</td>
<td>0.00</td>
<td>81.19</td>
<td>76.59</td>
<td>81.19</td>
</tr>
<tr>
<td></td>
<td>Claude-3-Opus</td>
<td>78.43</td>
<td>25.00</td>
<td>71.74</td>
<td>0.00</td>
</tr>
<tr>
<td></td>
<td>Llama-3-70B</td>
<td>54.00</td>
<td>56.00</td>
<td>52.00</td>
<td>67.00</td>
</tr>
</tbody>
</table>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{2}$ https://huggingface.co/datasets/cais/mmlu&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3497 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3497</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3497</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-78.html">extraction-schema-78</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <p><strong>Paper ID:</strong> paper-b32e631ae7779d93b9979c61c5b920a76342063e</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/b32e631ae7779d93b9979c61c5b920a76342063e" target="_blank">Teaching Temporal Logics to Neural Networks</a></p>
                <p><strong>Paper Venue:</strong> International Conference on Learning Representations</p>
                <p><strong>Paper TL;DR:</strong> The Transformer generalized from imperfect training data to the semantics of LTL, and the results were surprising: the Transformer returns the syntactically equivalent trace in 89% of the cases on a held-out test set.</p>
                <p><strong>Paper Abstract:</strong> We show that a deep neural network can learn the semantics of linear-time temporal logic (LTL). As a challenging task that requires deep understanding of the LTL semantics, we show that our network can solve the trace generation problem for LTL: given a satisfiable LTL formula, find a trace that satisfies the formula. We frame the trace generation problem for LTL as a translation task, i.e., to translate from formulas to satisfying traces, and train an off-the-shelf implementation of the Transformer, a recently introduced deep learning architecture proposed for solving natural language processing tasks. We provide a detailed analysis of our experimental results, comparing multiple hyperparameter settings and formula representations. After training for several hours on a single GPU the results were surprising: the Transformer returns the syntactically equivalent trace in 89% of the cases on a held-out test set. Most of the "mispredictions", however, (and overall more than 99% of the predicted traces) still satisfy the given LTL formula. In other words, the Transformer generalized from imperfect training data to the semantics of LTL.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3497.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3497.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Transformer (seq2seq)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Encoder-Decoder Transformer (sequence-to-sequence) trained to predict satisfying traces/assignments</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A token-level encoder-decoder Transformer trained end-to-end to map logical formulas (LTL or propositional) to satisfying symbolic traces or partial assignments using supervised data produced by classical solvers; experiments show strong semantic generalization despite differences from generator outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Transformer (encoder-decoder, seq2seq)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Token-by-token encoder-decoder Transformer following Vaswani et al.; experiments used models with up to 8 layers, 8 attention heads, FC size 1024 (best models), trained with dropout 0.1, beam search decoding (beam size 3, alpha=1). Training on single GPU (P100/V100). Decoder embedding sizes varied (stable training required smaller decoder embedding e.g. 64). Trained end-to-end on logical formula -> solution pairs produced by classical solvers.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LTL trace generation (satisfying trace generation) and Propositional SAT assignment generation</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given a satisfiable logical formula, generate a satisfying symbolic trace (for LTL; ultimately periodic lasso representation) or a satisfying (possibly partial) Boolean assignment (for propositional logic/SAT). Tasks require strict logical semantics (temporal logic semantics for LTL; Boolean semantics for SAT).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Supervised fine-tuning on solver-generated formula→solution pairs; use of tree-based positional encodings (Shiv & Quirk) to improve generalization to larger formulas; beam-search decoding; hyperparameter tuning (layers, heads, FC dim); training on multiple dataset distributions (pattern-based, random, CNF) to probe generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>LTLPattern126 (trained on patterns ≤126): syntactic accuracy 69.1%, semantic accuracy 96.8% (i.e., 96.8% of test formulas got a correct satisfying trace, including cases where produced trace differs from generator). LTLUnsolved254 (formulas where spot timed out >60s): semantic accuracy 83.0% (syntactic 0% since generator produced no output); average CPU verification time of a predicted trace ~15s. LTLRandom35 (trained on random formulas ≤35): syntactic 83.8%, semantic 98.5%. Generalization to LTLRandom50: syntactic 67.6%, semantic 92.2% (with tree positional encoding); standard positional encoding drops accuracy substantially. PropRandom35 (propositional, trained ≤35): syntactic 58.1%, semantic 96.5% (only 3.5% incorrect). Generalization to larger propositional formulas (35–50) with tree encoding: syntactic 35.8%, semantic 86.1%. PropCNF250: syntactic 56.6%, semantic 65.1%. Small fraction (~0.1%) of syntactically invalid outputs reported in hardest LTL set.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td>Classical solvers used as data generators (spot for LTL, pyaiger/Glucose for SAT) provide ground-truth solutions; they reliably produce satisfying traces/assignments when they complete, but they time out on harder instances (spot timed out on many pattern-conjunction formulas and LTLUnsolved254). The paper does not report an explicit numerical accuracy baseline for these solvers as they are used to generate labels; their limitation is timeouts on hard instances.</td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Transformer is not universally better than classical algorithms but complements them: it produced correct solutions on many formulas where the classical LTL solver timed out (83% of LTLUnsolved254). The model often returns different-but-still-correct solutions (semantic success) rather than reproducing the exact generator trace (syntactic match lower), indicating learning of logic semantics rather than generator idiosyncrasies.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Struggles on LTL formulas with multiple overlapping Until operators (PSPACE-hard fragments): scaling to four overlapping until-intervals caused systematic failures. Syntactic accuracy degrades as formula length grows even when semantic accuracy remains higher. Out-of-distribution shifts between pattern-based and random-formula distributions produce large drops in syntactic match and reduced semantic accuracy (e.g., model trained on random→pattern: sem. 24.7%, synt. 1.0%; vice versa pattern→random: sem. 38.5%, synt. 0.5%). Training instability occurred with large decoder embedding sizes (decoder embedding 128 made training unstable; 64 or 32 stabilized). A small fraction (~0.1%) of syntactically invalid outputs occurred in hardest sets.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Systematic hyperparameter study showed more layers improved performance (best at 8 layers); FC size and number of heads had smaller effects; best reported setting: embedding 128, 8 layers, 8 heads, FC 1024 yielded syntactic 83.8% / semantic 98.5% on LTLRandom35. Positional encoding ablation: tree-positional encodings substantially improved generalization to longer formulas vs standard positional encoding (visualized in results and Appendix E). Training dynamics show semantic accuracy rises much faster than syntactic accuracy (Figure 8), indicating learning of semantics rather than generator mimicry. Beam search (beam=3) used at decoding. Cross-distribution OOD tests and experiments on both LTL and propositional datasets served as analyses that semantic generalization is not dataset-specific nor solver-specific.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Temporal Logics to Neural Networks', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3497.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3497.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>spot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>spot (automata-based LTL framework)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An automata-based LTL toolbox used to construct Büchi automata from LTL formulas and to generate accepted symbolic traces; used here to generate training labels and as a classical algorithmic baseline whose timeouts define hard instances.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>spot (Duret-Lutz et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Classical automata-based LTL toolchain that converts LTL formulas to Büchi automata and can search for accepting runs to produce satisfying lasso-shaped traces; used as the generator for LTL training data and as a baseline by measuring timeouts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>LTL satisfiability / trace generation (classical symbolic solver)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given an LTL formula, build automata and search for an accepting run to produce a satisfying infinite trace (lasso representation).</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Automata construction and graph search for accepting runs; used with timeouts in data generation (timeouts >1s during generation, and >60s in LTLUnsolved254 selection).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used to generate 1,664,487 formula-trace pairs; timed out in ~37% of training-instance generation when capping at 1s (overall generation statistics provided). For difficult pattern-conjunction formulas, spot timed out on many instances (LTLUnsolved254 was specifically collected where spot timed out >60s). When spot succeeds, it provides a concrete trace used as the generator's output.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>The paper demonstrates that the Transformer can produce correct solutions in many cases where spot times out (Transformer semantic accuracy 83% on LTLUnsolved254), indicating that ML predictions can augment classical solver pipelines by providing candidate traces that can be efficiently verified.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Automata-based approaches can time out on complex conjunctions of specification patterns (noted to struggle when more than eight patterns are conjoined and on many larger handcrafted pattern formulas). Timeouts were used to identify hard instances for which the Transformer produced solutions.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>spot is primarily used as data generator and reference; the paper uses analyses (e.g., measuring syntactic vs semantic agreement with spot outputs, and evaluating performance on spot-timeout instances) to show ML generalization; no internal ablations of spot are reported in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Temporal Logics to Neural Networks', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3497.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3497.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models performing strict logical reasoning tasks, including the models used, logical reasoning tasks or benchmarks, methods or interventions applied to improve logical reasoning, performance results, comparisons to baselines, and any reported limitations or failure cases.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>pyaiger / Glucose</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>pyaiger front-end with Glucose SAT solver</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A SAT-solving toolchain (pyaiger front-end using Glucose 4) used to generate minimal partial satisfying assignments for propositional formulas, supplying labels for training the Transformer on SAT assignment generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>pyaiger + Glucose 4</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>pyaiger library used to build SAT queries and to call the Glucose 4 SAT solver to obtain satisfying assignments or minimal unsatisfiable cores (used to derive partial satisfying assignments for the negated formula).</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_name</strong></td>
                            <td>Propositional satisfiability / assignment generation (classical SAT solver)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task_description</strong></td>
                            <td>Given a propositional formula (not restricted to CNF in this work), determine a satisfying assignment (possibly partial) or a minimal unsatisfiable core for the negated formula to derive assignments.</td>
                        </tr>
                        <tr>
                            <td><strong>method_or_intervention</strong></td>
                            <td>Classical CDCL SAT solving (Glucose) and extraction of minimal unsatisfiable cores to construct partial satisfying assignments; used to generate 1M examples for PropRandom35 and other propositional datasets.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Used to generate datasets where the transformer achieved semantic accuracies up to 96.5% on PropRandom35; the classical solver generated the ground-truth assignments for training and is assumed correct when it completes. No single scalar 'accuracy' for the solver is reported since it is used to produce labels rather than evaluated against the transformer.</td>
                        </tr>
                        <tr>
                            <td><strong>baseline_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>improvement_over_baseline</strong></td>
                            <td>Transformer often produced different (smaller or alternative) but semantically valid assignments than the DPLL/Glucose generator; e.g., for formula b ∨ ¬(a ∧ d) generator gave {a=0} while Transformer gave {d=0}, both correct. This shows Transformer finds alternative valid solutions rather than strictly matching generator outputs.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failures</strong></td>
                            <td>Transformer's syntactic agreement with generator is moderate (58.1% on PropRandom35) though semantic accuracy high (96.5%); generalization to larger propositional formulas reduces syntactic and semantic accuracies unless tree encoding is used.</td>
                        </tr>
                        <tr>
                            <td><strong>ablation_or_analysis</strong></td>
                            <td>Paper reports that decoder embedding size affects training stability (smaller decoder embeddings stabilized training). Also compares positional encodings (tree vs standard) showing tree encoding improves generalization to larger formulas for the propositional task as well.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Teaching Temporal Logics to Neural Networks', 'publication_date_yy_mm': '2020-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Deep learning for symbolic mathematics <em>(Rating: 2)</em></li>
                <li>Language models are few-shot learners <em>(Rating: 1)</em></li>
                <li>Learning a SAT solver from single-bit supervision <em>(Rating: 2)</em></li>
                <li>Novel positional encodings to enable tree-based transformers <em>(Rating: 2)</em></li>
                <li>Mathematical reasoning via self-supervised skip-tree training <em>(Rating: 2)</em></li>
                <li>Can neural networks understand logical entailment? <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3497",
    "paper_id": "paper-b32e631ae7779d93b9979c61c5b920a76342063e",
    "extraction_schema_id": "extraction-schema-78",
    "extracted_data": [
        {
            "name_short": "Transformer (seq2seq)",
            "name_full": "Encoder-Decoder Transformer (sequence-to-sequence) trained to predict satisfying traces/assignments",
            "brief_description": "A token-level encoder-decoder Transformer trained end-to-end to map logical formulas (LTL or propositional) to satisfying symbolic traces or partial assignments using supervised data produced by classical solvers; experiments show strong semantic generalization despite differences from generator outputs.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Transformer (encoder-decoder, seq2seq)",
            "model_description": "Token-by-token encoder-decoder Transformer following Vaswani et al.; experiments used models with up to 8 layers, 8 attention heads, FC size 1024 (best models), trained with dropout 0.1, beam search decoding (beam size 3, alpha=1). Training on single GPU (P100/V100). Decoder embedding sizes varied (stable training required smaller decoder embedding e.g. 64). Trained end-to-end on logical formula -&gt; solution pairs produced by classical solvers.",
            "model_size": null,
            "reasoning_task_name": "LTL trace generation (satisfying trace generation) and Propositional SAT assignment generation",
            "reasoning_task_description": "Given a satisfiable logical formula, generate a satisfying symbolic trace (for LTL; ultimately periodic lasso representation) or a satisfying (possibly partial) Boolean assignment (for propositional logic/SAT). Tasks require strict logical semantics (temporal logic semantics for LTL; Boolean semantics for SAT).",
            "method_or_intervention": "Supervised fine-tuning on solver-generated formula→solution pairs; use of tree-based positional encodings (Shiv & Quirk) to improve generalization to larger formulas; beam-search decoding; hyperparameter tuning (layers, heads, FC dim); training on multiple dataset distributions (pattern-based, random, CNF) to probe generalization.",
            "performance": "LTLPattern126 (trained on patterns ≤126): syntactic accuracy 69.1%, semantic accuracy 96.8% (i.e., 96.8% of test formulas got a correct satisfying trace, including cases where produced trace differs from generator). LTLUnsolved254 (formulas where spot timed out &gt;60s): semantic accuracy 83.0% (syntactic 0% since generator produced no output); average CPU verification time of a predicted trace ~15s. LTLRandom35 (trained on random formulas ≤35): syntactic 83.8%, semantic 98.5%. Generalization to LTLRandom50: syntactic 67.6%, semantic 92.2% (with tree positional encoding); standard positional encoding drops accuracy substantially. PropRandom35 (propositional, trained ≤35): syntactic 58.1%, semantic 96.5% (only 3.5% incorrect). Generalization to larger propositional formulas (35–50) with tree encoding: syntactic 35.8%, semantic 86.1%. PropCNF250: syntactic 56.6%, semantic 65.1%. Small fraction (~0.1%) of syntactically invalid outputs reported in hardest LTL set.",
            "baseline_performance": "Classical solvers used as data generators (spot for LTL, pyaiger/Glucose for SAT) provide ground-truth solutions; they reliably produce satisfying traces/assignments when they complete, but they time out on harder instances (spot timed out on many pattern-conjunction formulas and LTLUnsolved254). The paper does not report an explicit numerical accuracy baseline for these solvers as they are used to generate labels; their limitation is timeouts on hard instances.",
            "improvement_over_baseline": "Transformer is not universally better than classical algorithms but complements them: it produced correct solutions on many formulas where the classical LTL solver timed out (83% of LTLUnsolved254). The model often returns different-but-still-correct solutions (semantic success) rather than reproducing the exact generator trace (syntactic match lower), indicating learning of logic semantics rather than generator idiosyncrasies.",
            "limitations_or_failures": "Struggles on LTL formulas with multiple overlapping Until operators (PSPACE-hard fragments): scaling to four overlapping until-intervals caused systematic failures. Syntactic accuracy degrades as formula length grows even when semantic accuracy remains higher. Out-of-distribution shifts between pattern-based and random-formula distributions produce large drops in syntactic match and reduced semantic accuracy (e.g., model trained on random→pattern: sem. 24.7%, synt. 1.0%; vice versa pattern→random: sem. 38.5%, synt. 0.5%). Training instability occurred with large decoder embedding sizes (decoder embedding 128 made training unstable; 64 or 32 stabilized). A small fraction (~0.1%) of syntactically invalid outputs occurred in hardest sets.",
            "ablation_or_analysis": "Systematic hyperparameter study showed more layers improved performance (best at 8 layers); FC size and number of heads had smaller effects; best reported setting: embedding 128, 8 layers, 8 heads, FC 1024 yielded syntactic 83.8% / semantic 98.5% on LTLRandom35. Positional encoding ablation: tree-positional encodings substantially improved generalization to longer formulas vs standard positional encoding (visualized in results and Appendix E). Training dynamics show semantic accuracy rises much faster than syntactic accuracy (Figure 8), indicating learning of semantics rather than generator mimicry. Beam search (beam=3) used at decoding. Cross-distribution OOD tests and experiments on both LTL and propositional datasets served as analyses that semantic generalization is not dataset-specific nor solver-specific.",
            "uuid": "e3497.0",
            "source_info": {
                "paper_title": "Teaching Temporal Logics to Neural Networks",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "spot",
            "name_full": "spot (automata-based LTL framework)",
            "brief_description": "An automata-based LTL toolbox used to construct Büchi automata from LTL formulas and to generate accepted symbolic traces; used here to generate training labels and as a classical algorithmic baseline whose timeouts define hard instances.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "spot (Duret-Lutz et al.)",
            "model_description": "Classical automata-based LTL toolchain that converts LTL formulas to Büchi automata and can search for accepting runs to produce satisfying lasso-shaped traces; used as the generator for LTL training data and as a baseline by measuring timeouts.",
            "model_size": null,
            "reasoning_task_name": "LTL satisfiability / trace generation (classical symbolic solver)",
            "reasoning_task_description": "Given an LTL formula, build automata and search for an accepting run to produce a satisfying infinite trace (lasso representation).",
            "method_or_intervention": "Automata construction and graph search for accepting runs; used with timeouts in data generation (timeouts &gt;1s during generation, and &gt;60s in LTLUnsolved254 selection).",
            "performance": "Used to generate 1,664,487 formula-trace pairs; timed out in ~37% of training-instance generation when capping at 1s (overall generation statistics provided). For difficult pattern-conjunction formulas, spot timed out on many instances (LTLUnsolved254 was specifically collected where spot timed out &gt;60s). When spot succeeds, it provides a concrete trace used as the generator's output.",
            "baseline_performance": null,
            "improvement_over_baseline": "The paper demonstrates that the Transformer can produce correct solutions in many cases where spot times out (Transformer semantic accuracy 83% on LTLUnsolved254), indicating that ML predictions can augment classical solver pipelines by providing candidate traces that can be efficiently verified.",
            "limitations_or_failures": "Automata-based approaches can time out on complex conjunctions of specification patterns (noted to struggle when more than eight patterns are conjoined and on many larger handcrafted pattern formulas). Timeouts were used to identify hard instances for which the Transformer produced solutions.",
            "ablation_or_analysis": "spot is primarily used as data generator and reference; the paper uses analyses (e.g., measuring syntactic vs semantic agreement with spot outputs, and evaluating performance on spot-timeout instances) to show ML generalization; no internal ablations of spot are reported in this paper.",
            "uuid": "e3497.1",
            "source_info": {
                "paper_title": "Teaching Temporal Logics to Neural Networks",
                "publication_date_yy_mm": "2020-03"
            }
        },
        {
            "name_short": "pyaiger / Glucose",
            "name_full": "pyaiger front-end with Glucose SAT solver",
            "brief_description": "A SAT-solving toolchain (pyaiger front-end using Glucose 4) used to generate minimal partial satisfying assignments for propositional formulas, supplying labels for training the Transformer on SAT assignment generation.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "pyaiger + Glucose 4",
            "model_description": "pyaiger library used to build SAT queries and to call the Glucose 4 SAT solver to obtain satisfying assignments or minimal unsatisfiable cores (used to derive partial satisfying assignments for the negated formula).",
            "model_size": null,
            "reasoning_task_name": "Propositional satisfiability / assignment generation (classical SAT solver)",
            "reasoning_task_description": "Given a propositional formula (not restricted to CNF in this work), determine a satisfying assignment (possibly partial) or a minimal unsatisfiable core for the negated formula to derive assignments.",
            "method_or_intervention": "Classical CDCL SAT solving (Glucose) and extraction of minimal unsatisfiable cores to construct partial satisfying assignments; used to generate 1M examples for PropRandom35 and other propositional datasets.",
            "performance": "Used to generate datasets where the transformer achieved semantic accuracies up to 96.5% on PropRandom35; the classical solver generated the ground-truth assignments for training and is assumed correct when it completes. No single scalar 'accuracy' for the solver is reported since it is used to produce labels rather than evaluated against the transformer.",
            "baseline_performance": null,
            "improvement_over_baseline": "Transformer often produced different (smaller or alternative) but semantically valid assignments than the DPLL/Glucose generator; e.g., for formula b ∨ ¬(a ∧ d) generator gave {a=0} while Transformer gave {d=0}, both correct. This shows Transformer finds alternative valid solutions rather than strictly matching generator outputs.",
            "limitations_or_failures": "Transformer's syntactic agreement with generator is moderate (58.1% on PropRandom35) though semantic accuracy high (96.5%); generalization to larger propositional formulas reduces syntactic and semantic accuracies unless tree encoding is used.",
            "ablation_or_analysis": "Paper reports that decoder embedding size affects training stability (smaller decoder embeddings stabilized training). Also compares positional encodings (tree vs standard) showing tree encoding improves generalization to larger formulas for the propositional task as well.",
            "uuid": "e3497.2",
            "source_info": {
                "paper_title": "Teaching Temporal Logics to Neural Networks",
                "publication_date_yy_mm": "2020-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Deep learning for symbolic mathematics",
            "rating": 2
        },
        {
            "paper_title": "Language models are few-shot learners",
            "rating": 1
        },
        {
            "paper_title": "Learning a SAT solver from single-bit supervision",
            "rating": 2
        },
        {
            "paper_title": "Novel positional encodings to enable tree-based transformers",
            "rating": 2
        },
        {
            "paper_title": "Mathematical reasoning via self-supervised skip-tree training",
            "rating": 2
        },
        {
            "paper_title": "Can neural networks understand logical entailment?",
            "rating": 1
        }
    ],
    "cost": 0.012835,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Teaching Temporal Logics to Neural NetWORKS*</h1>
<p>Christopher Hahn<br>CISPA Helmholtz Center for Information Security<br>Saarbrücken, 66123 Saarland, Germany<br>christopher.hahn@cispa.de</p>
<h2>Jens U. Kreber</h2>
<p>Saarland University
Saarbrücken, 66123 Saarland, Germany
kreber@react.uni-saarland.de</p>
<h2>Bernd Finkbeiner</h2>
<p>CISPA Helmholtz Center for Information Security
Saarbrücken, 66123 Saarland, Germany
finkbeiner@cispa.de</p>
<h2>Frederik Schmitt</h2>
<p>CISPA Helmholtz Center for Information Security
Saarbrücken, 66123 Saarland, Germany
frederik.schmitt@cispa.de</p>
<h2>Markus N. Rabe</h2>
<p>Google Research
Mountain View, CA, USA
mrabe@google.com</p>
<h2>AbSTRACT</h2>
<p>We study two fundamental questions in neuro-symbolic computing: can deep learning tackle challenging problems in logics end-to-end, and can neural networks learn the semantics of logics. In this work we focus on linear-time temporal logic (LTL), as it is widely used in verification. We train a Transformer on the problem to directly predict a solution, i.e. a trace, to a given LTL formula. The training data is generated with classical solvers, which, however, only provide one of many possible solutions to each formula. We demonstrate that it is sufficient to train on those particular solutions to formulas, and that Transformers can predict solutions even to formulas from benchmarks from the literature on which the classical solver timed out. Transformers also generalize to the semantics of the logics: while they often deviate from the solutions found by the classical solvers, they still predict correct solutions to most formulas.</p>
<h2>1 INTRODUCTION</h2>
<p>Machine learning has revolutionized several areas of computer science, such as image recognition (He et al., 2015), face recognition (Taigman et al., 2014), translation (Wu et al., 2016), and board games (Moravcík et al., 2017; Silver et al., 2017). For complex tasks that involve symbolic reasoning, however, deep learning techniques are still considered as insufficient. Applications of deep learning in logical reasoning problems have therefore focused on sub-problems within larger logical frameworks, such as computing heuristics in solvers (Lederman et al., 2020; Balunovic et al., 2018; Selsam \&amp; Bjørner, 2019) or predicting individual proof steps (Loos et al., 2017; Gauthier et al., 2018; Bansal et al., 2019; Huang et al., 2018). Recently, however, the assumption that deep learning is not yet ready to tackle hard logical questions was drawn into question. Lample \&amp; Charton (2020) demonstrated that Transformer models (Vaswani et al., 2017) perform surprisingly well on symbolic integration, Rabe et al. (2020) demonstrated that self-supervised training leads to mathematical reasoning abilities, and Brown et al. (2020) demonstrated that large-enough language models learn basic arithmetic despite being trained on mostly natural language sources.</p>
<p>This poses the question if other problems that are thought to require symbolic reasoning lend themselves to a direct learning approach. We study the application of Transformer models to challenging logical problems in verification. We thus consider linear-time temporal logic (LTL) (Pnueli, 1977),</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Performance of our best models trained on practical pattern formulas. The x-axis shows the formula size. Syntactic accuracy, i.e., where the Transformer agrees with the generator are displayed in dark green. Instances where the Transformer deviates from the generators output but still provides correct output are displayed in light green; incorrect predictions in orange.</p>
<p>which is widely used in the academic verification community (Dwyer et al., 1998; Li et al., 2013; Duret-Lutz et al., 2016; Rozier &amp; Vardi, 2007; Schuppan &amp; Darmawan, 2011; Li et al., 2013; 2014; Schwendimann, 1998) and is the basis for industrial hardware specification languages like the IEEE standard PSL (IEEE-Commission et al., 2005). LTL specifies infinite sequences and is typically used to describe system behaviors. For example, LTL can specify that some proposition $P$ must hold at every point in time ( $\bigcirc P$ ) or that $P$ must hold at some future point of time ( $\diamond P$ ). By combining these operators, one can specify that $P$ must occur infinitely often ( $\bigcirc \diamond P$ ).</p>
<p>In this work, we apply a direct learning approach to the fundamental problem of LTL to find a satisfying trace to a formula. In applications, solutions to LTL formulas can represent (counter) examples for a specified system behavior, and over the last decades, generations of advanced algorithms have been developed to solve this question automatically. We start from the standard benchmark distribution of LTL formulas, consisting of conjunctions of patterns typically encountered in practice (Dwyer et al., 1998). We then use classical algorithms, notably spot by Duret-Lutz et al. (2016), that implement a competitive classical algorithm, to generate solutions to formulas from this distribution and train a Transformer model to predict these solutions directly.</p>
<p>Relatively small Transformers perform very well on this task and we predict correct solutions to $96.8 \%$ of the formulas from a held-out test set (see Figure 1). Impressive enough, Transformers hold up pretty well and predict correct solutions in $83 \%$ of the cases, even when we focus on formulas on which spot timed out. This means that, already today, direct machine learning approaches may be useful to augment classical algorithms in logical reasoning tasks.</p>
<p>We also study two generalization properties of the Transformer architecture, important to logical problems: We present detailed analyses on the generalization to longer formulas. It turns out that transformers trained with tree-positional encodings (Shiv \&amp; Quirk, 2019) generalize to much longer formulas than they were trained on, while Transformers trained with the standard positional encoding (as expected) do not generalize to longer formulas. The second generalization property studied here is the question whether Transformers learn to imitate the generator of the training data, or whether they learn to solve the formulas according to the semantics of the logics. This is possible, as for most formulas there are many possible satisfying traces. In Figure 1 we highlight the fact that our models often predicted traces that satisfy the formulas, but predict different traces than the one found by the classical algorithm with which we generated the data. Especially when testing the models out-of-distribution we observed that almost no predicted trace equals the solution proposed by the classical solver.</p>
<p>To demonstrate that these generalization behaviors are not specific to the benchmark set of LTL formulas, we also present experimental results on a set of random LTL formulas. Further, we exclude that spot, the tool with which we generate example traces, is responsible for these behaviors, by repeating the experiments on a set of propositional formulas for which we generate the solutions by SAT solvers.</p>
<p>The remainder of this paper is structured as follows. We give an overview over related work in Section 2. We describe the problem definitions and present our data generation in Section 3. Our experimental setup is described in Section 4 and our findings in Section 5, before concluding in Section 6.</p>
<h1>2 Related Work</h1>
<p>Datasets for mathematical reasoning. While we focus on a classical task from verification, other works have studied datasets derived from automated theorem provers (Blanchette et al., 2016; Loos et al., 2017; Gauthier et al., 2018), interactive theorem provers (Kaliszyk et al., 2017; Bansal et al., 2019; Huang et al., 2018; Yang \&amp; Deng, 2019; Polu \&amp; Sutskever, 2020; Wu et al., 2020; Li et al., 2020; Lee et al., 2020; Urban \&amp; Jakubův, 2020; Rabe et al., 2020), symbolic mathematics (Lample \&amp; Charton, 2020), and mathematical problems in natural language (Saxton et al., 2019; Schlag et al., 2019). Probably the closest work to this paper are the applications of Transformers to directly solve differential equations (Lample \&amp; Charton, 2020) and directly predict missing assumptions and types of formal mathematical statements (Rabe et al., 2020). We focus on a different problem domain, verification, and demonstrate that Transformers are roughly competitive with classical algorithms in that domain on their dataset. Learning has been applied to mathematics long before the rise of deep learning. Earlier works focused on ranking premises or clauses Cairns (2004); Urban (2004; 2007); Urban et al. (2008); Meng \&amp; Paulson (2009); Schulz (2013); Kaliszyk \&amp; Urban (2014).</p>
<p>Neural architectures for logical reasoning. (Paliwal et al., 2020) demonstrate significant improvements in theorem proving through the use of graph neural networks to represent higher-order logic terms. Selsam et al. (2019) presented NeuroSAT, a graph neural network (Scarselli et al., 2008; Li et al., 2017; Gilmer et al., 2017; Wu et al., 2019) for solving the propositional satisfiability problem. In contrast, we apply a generic sequence-to-sequence model to predict the solutions to formulas, not only whether there is a solution. This allows us to apply the approach to a wider set of logics (logics without a CNF). A simplified NeuroSAT architecture was trained for unsat-core predictions (Selsam \&amp; Bjørner, 2019). Lederman et al. (2020) have used graph neural networks on CNF to learn better heuristics for a 2QBF solver. Evans et al. (2018) study the problem of logical entailment in propositional logic using tree-RNNs. Entailment is a subproblem of satisfiability and (besides being a classification problem) could be encoded in the same form as our propositional formulas. The formulas considered in their dataset are much smaller than in this work.</p>
<p>Language models applied to programs. Transformers have also been applied to programs for tasks such as summarizing code (Fernandes et al., 2018) or variable naming and misuse (Hellendoorn et al., 2020). Other works focused on recurrent neural networks or graph neural networks for code analysis, e.g. (Piech et al., 2015; Gupta et al., 2017; Bhatia et al., 2018; Wang et al., 2018; Allamanis et al., 2017). Another area in the intersection of formal methods and machine learning is the verification of neural networks (Seshia \&amp; Sadigh, 2016; Seshia et al., 2018; Singh et al., 2019; Gehr et al., 2018; Huang et al., 2017; Dreossi et al., 2019).</p>
<h2>3 Data Sets</h2>
<p>To demonstrate the generalization properties of the Transformer on logical tasks, we generated several data sets in three different fashions. We will describe the underlying logical problems and our data generation in the following.</p>
<h3>3.1 Trace Generation for Linear-time Temporal Logic</h3>
<p>Linear-time temporal logic (LTL, Pnueli, 1977) combines propositional connectives with temporal operators such as the Next operator $\bigcirc$ and the Until operator $\mathcal{U} . \bigcirc \varphi$ means that $\varphi$ holds in the next position of a sequence; $\varphi_{1} \mathcal{U} \varphi_{2}$ means that $\varphi_{1}$ holds until $\varphi_{2}$ holds. For example, the LTL formula $(b \mathcal{U} a) \wedge(c \mathcal{U} \neg a)$ states that $b$ has to hold along the trace until $a$ holds and $c$ has to hold until $a$ does not hold anymore. There also exist derived operators. For example, consider the following specification of an arbiter: $\square$ (request $\rightarrow$ Ogrant) states that, at every point in time ( $\square$-operator), if there is a request signal, then a grant signal must follow at some future point in time ( $\diamond$-operator).</p>
<p>The full semantics and an explanation of the operators can be found in Appendix A. We consider infinite sequences, that are finitely represented in the form of a "lasso" $u v$ ", where $u$, called prefix, and $v$, called period, are finite sequences of propositional formulas. We call such sequences (symbolic) traces. For example, the symbolic trace $(a \wedge b)^{\omega}$ defines the infinite sequence where $a$ and $b$ evaluate to true on every position. Symbolic traces allow us to underspecify propositions when they do not matter. For example, the LTL formula $\bigcirc \bigcirc \square a$ is satisfied by the symbolic trace: true true $(a)^{\omega}$, which allow for any combination of propositions on the first two positions.</p>
<p>Our data sets consist of pairs of satisfiable LTL formulas and satisfying symbolic traces generated with tools and automata constructions from the spot framework (Duret-Lutz et al., 2016). We use a compact syntax for ultimately periodic symbolic traces: Each position in the trace is separated by the delimiter ";". True and False are represented by " 1 " and " 0 ", respectively. The beginning of the period $v$ is signaled by the character " ${$ " and analogously its end by " $}$ ". For example, the ultimately periodic symbolic trace denoted by $a ; a ; a ;{b}$, describes all infinite traces where on the first 3 positions $a$ must hold followed by an infinite period on which $b$ must hold on every position.</p>
<p>Given a satisfiable LTL formula $\varphi$, our trace generator constructs a Büchi automaton $A_{\varphi}$ that accepts exactly the language defined by the LTL formula, i.e., $\mathcal{L}\left(A_{\varphi}\right)=\mathcal{L}(\varphi)$. From this automaton, we construct an arbitrary accepted symbolic trace, by searching for an accepting run in $A_{\varphi}$.</p>
<h1>3.1.1 SPECIFICATION PATTERN</h1>
<p>Our main data set is constructed from formulas following 55 LTL specification patterns identified by the literature (Dwyer et al., 1998). For example, the arbiter property $\left(\bigcirc p_{0}\right) \rightarrow\left(p_{1} \mathcal{U} p_{0}\right)$, stating that if $p_{0}$ is scheduled at some point in time, $p_{1}$ is scheduled until this point. The largest specification pattern is of size 40 consisting of 6 atomic propositions. It has been shown that conjunctions of such patterns are challenging for LTL satisfiability tools that rely on classical methods, such as automata constructions (Li et al., 2013). They start coming to their limits when more than 8 pattern formulas are conjoined. We decided to build our data set in a similar way from these patterns only to allow for a better comparison.</p>
<p>We conjoined random specification patterns with randomly chosen variables (from a supply of 6 variables) until one of the following four conditions are met: 1) the formula size succeeds 126, 2) more than 8 formulas would be conjoined, 3) our automaton-based generator timed out ( $&gt;1 s$ ) while computing the solution trace, or 4) the formula would become unsatisfiable. In total, we generated 1664487 formula-trace pairs in 24 hours on 20 CPUs. While generating, approximately $41 \%$ of the instances ran into the first termination condition, $21 \%$ into the second, $37 \%$ into the third and $1 \%$ into the fourth. We split this set into an $80 \%$ training set, a $10 \%$ validation set, and a $10 \%$ test set. The size distribution of the data set can be found in Appendix B.</p>
<p>For studying how the Transformer performs on longer specification patterns, we accumulated pattern formulas where spot timed out ( $&gt;60 s$ ) while searching for a satisfying trace. We call this data set LTLUnsolved254. We capped the maximum length at 254, which is twice as large as the formulas the model saw during training. The size distribution of the generated formulas can be found in Appendix B.</p>
<p>In the following table, we illustrate the complexity of our training data set with two examples from the above described set LTLPattern126, where the subsequent number of the notation of our data sets denotes the maximum size of a formula's syntax tree. The first line shows the LTL formula and the symbolic trace in mathematical notation. The second line shows the input and output representation of the Transformer (in Polish notation):</p>
<table>
<thead>
<tr>
<th style="text-align: center;">LTL formula</th>
<th style="text-align: center;">satisfying symbolic trace</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\square(a \rightarrow \bigcirc d) \wedge \neg f W f W \neg f W f W \square \neg f$</td>
<td style="text-align: center;">$(\neg a \wedge \neg c \wedge \neg f \vee \neg c \wedge d \wedge \neg f)^{\omega}$</td>
</tr>
<tr>
<td style="text-align: center;">$\wedge(\bigcirc c \rightarrow \neg c \mathcal{U}(c \wedge \neg b W b W \neg b W b W \square \neg b))$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\&amp; \&amp; G&gt;a F d W!f W f W!f W f G!f&gt;F c U!c \&amp; c W!b W b W!b W b G!b$</td>
<td style="text-align: center;">${!a \&amp;!c \&amp;!f)!c \&amp; d \&amp;!f}$</td>
</tr>
<tr>
<td style="text-align: center;">$\square(b \wedge \neg a \wedge \bigcirc a \rightarrow c \mathcal{U} a) \wedge \square(a \rightarrow \square c) \wedge(\bigcirc b \rightarrow \neg b$</td>
<td style="text-align: center;">$(\neg a \wedge b \wedge \neg c \wedge \neg e \wedge f)(\neg a \wedge \neg c$</td>
</tr>
<tr>
<td style="text-align: center;">$\mathcal{U}(b \wedge \neg f W f W \neg f W f W \square \neg f)) \wedge(\bigcirc a \rightarrow(c \wedge \bigcirc(\neg a \mathcal{U} e)$</td>
<td style="text-align: center;">$\wedge \neg e \wedge \neg f)(\neg a \wedge \neg c \wedge \neg e \wedge f)$</td>
</tr>
<tr>
<td style="text-align: center;">$\rightarrow \bigcirc(\neg a \mathcal{U}(e \wedge \bigcirc f))) \mathcal{U} a) \wedge \bigcirc c \wedge \square(a \square \bigcirc e \rightarrow \neg(\neg e \wedge f \wedge \bigcirc$</td>
<td style="text-align: center;">$(\neg a \wedge c \wedge \neg e \wedge \neg f)(\neg a \wedge \neg e \wedge \neg f)^{\omega}$</td>
</tr>
<tr>
<td style="text-align: center;">$(\neg e \mathcal{U}(\neg e \wedge d))) \mathcal{U}(e \vee c)) \wedge(\square \neg a \vee \bigcirc(a \wedge \neg f W d)) \wedge \square(e \rightarrow \square \neg c)$</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">$\&amp; \&amp; \&amp; \&amp; \&amp; \&amp; \&amp; G&gt; \&amp; \&amp; b!aFaUcaG&gt;aGc&gt;FbU!b \&amp; b W!f WfW!f W$</td>
<td style="text-align: center;">$\&amp; \&amp; \&amp; \&amp;$ !ab!c!ef; \&amp; \&amp; \&amp;!a!c!e!f;</td>
</tr>
<tr>
<td style="text-align: center;">fG!f&gt;FaU&gt;\&amp;cXU!aeXU!a\&amp;eFfaFcG&gt;\&amp;aFeU! \&amp;</td>
<td style="text-align: center;">$\&amp; \&amp; \&amp;$ !a!c!ef; \&amp; \&amp; \&amp;!ac!e!f</td>
</tr>
<tr>
<td style="text-align: center;">\&amp;!efXU!e\&amp;!ed!ec!G!aF\&amp;aW!fdG&gt;eG!c</td>
<td style="text-align: center;">; ${\&amp; \&amp;$ !a!e!f}</td>
</tr>
</tbody>
</table>
<h1>3.1.2 RANDOM FORMULAS</h1>
<p>To show that the generalization properties of the Transformer are not specific to our data generation, we also generated a data set of random formulas. Our data set of random formulas consist of 1 million generated formulas and their solutions, i.e., a satisfying symbolic trace. The number of different propositions is fixed to 5 . Each data set is split into a training set of 800 K formulas, a validation set of 100 K formulas, and a test set of 100 K formulas. All data sets are uniformly distributed in size, apart from the lower-sized end due to the limited number of unique small formulas. The formula and trace distribution of the data set $L T L R a n d o m 35$, as well as three randomly drawn example instances can be found in Appendix B. Note that we filtered out examples with traces larger than 62 (less than $0.05 \%$ of the original set).
To generate the formulas, we used the randltl tool of the spot framework, which builds unique formulas in a specified size interval, following a supplied node probability distribution. During the building process, the actual distribution occasionally differs from the given distribution in order to meet the size constraints, e.g., by masking out all binary operators. The distribution between all $k$-ary nodes always remains the same. To furthermore achieve a (quasi) uniform distribution in size, we subsequently filtered the generated formulas. Our node distribution puts equal weight on all operators $\neg, \wedge, \bigcirc$ and $\mathcal{U}$. Constants True and False are allowed with 2.5 times less probability than propositions.</p>
<h3>3.2 ASSIGNMENT GENERATION FOR PROPOSITIONAL LOGIC</h3>
<p>To show that the generalization of the Transformer to the semantics of logics is not a unique attribute of LTL, we also generated a data set for propositional logic (SAT). A propositional formula consists of Boolean operators $\wedge$ (and), $\vee$ (or), $\neg$ (not), and variables also called literals or propositions. We consider the derived operators $\varphi_{1} \rightarrow \varphi_{2} \equiv \neg \varphi_{1} \vee \varphi_{2}$ (implication), $\varphi_{1} \leftrightarrow \varphi_{2} \equiv\left(\varphi_{1} \rightarrow \varphi_{2}\right) \wedge\left(\varphi_{2} \rightarrow\right.$ $\varphi_{1}$ ) (equivalence), and $\varphi_{1} \oplus \varphi_{2} \equiv \neg\left(\varphi_{1} \leftrightarrow \varphi_{2}\right)$ (xor). Given a propositional Boolean formula $\varphi$, the satisfiability problem asks if there exists a Boolean assignment $\Pi: \mathcal{V} \mapsto \mathbb{B}$ for every literal in $\varphi$ such that $\varphi$ evaluates to true. For example, consider the following propositional formula, given in conjunctive normal form (CNF): $\left(x_{1} \vee x_{2} \vee \neg x_{3}\right) \wedge\left(\neg x_{1} \vee x_{3}\right)$. A possible satisfying assignment for this formula would be $\left{\left(x_{1}\right.\right.$, true $\left),\left(x_{2}\right.\right.$, false $\left),\left(x_{3}\right.\right.$, true $\left.)\right}$. We allow a satisfying assignment to be partial, i.e., if the truth value of a propositions can be arbitrary, it will be omitted. For example, $\left{\left(x_{1}\right.\right.$, true $\left),\left(x_{3}\right.\right.$, true $\left.)\right}$ would be a satisfying partial assignment for the formula above. We define a minimal unsatisfiable core of an unsatisfiable formula $\varphi$, given in CNF, as an unsatisfiable subset of clauses $\varphi_{\text {core }}$ of $\varphi$, such that every proper subset of clauses of $\varphi_{\text {core }}$ is still satisfiable.
We, again, generated 1 million random formulas. For the generation of propositional formulas, the specified node distribution puts equal weight on $\wedge, \vee$, and $\neg$ operators and half as much weight on the derived operators $\leftrightarrow$ and $\oplus$ individually. In contrast to previous work (Selsam et al., 2019), which is restricted to formulas in CNF, we allow an arbitrary formula structure and derived operators.
A satisfying assignment is represented as an alternating sequence of propositions and truth values, given as 0 and 1 . The sequence $a 0 b 1 c 0$, for example, represents the partial assignment ${(a$, false $),(b$, true $),(c$, false $})$, meaning that the truth values of propositions $d$ and $e$ can be chosen arbitrarily (note that we allow five propositions). We used pyaiger (Vazquez-Chanlatte, 2018), which builds on Glucose 4 (Audemard \&amp; Simon, 2018) as its underlying SAT solver. We construct the partial assignments with a standard method in SAT solving: We query the SAT solver for a minimal unsatisfiable core of the negation of the formula. To give the interested reader an idea of the level of difficulty of the data set, the following table shows three random examples from our training set PropRandom 35. The first line shows the formula and the assignment in mathematical notation. The second line shows the syntactic representation (in Polish notation):</p>
<table>
<thead>
<tr>
<th style="text-align: center;">propositional formula</th>
<th style="text-align: center;">satisfying partial assignment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$((d \wedge \neg e) \wedge(\neg a \vee \neg e)) \leftrightarrow((\neg \oplus(\neg b \leftrightarrow \neg e))$ <br> $\vee((e \oplus(b \wedge d)) \oplus \neg(\neg e \vee(\neg a \leftrightarrow e))))$</td>
<td style="text-align: center;">${(a, 0),(b, 0),(c, 1),(d, 1),(e, 0)}$</td>
</tr>
<tr>
<td style="text-align: center;">$&lt;-&gt;\&amp; \&amp; d!$ e!!a!e!xor!b&lt;-&gt;!b!exorxore\&amp;bd!</td>
<td style="text-align: center;">!c&lt;-&gt;!ae</td>
</tr>
<tr>
<td style="text-align: center;">$(e \vee e) \vee(\neg a \leftrightarrow \neg b)$ <br> $|\mathrm{ce}&lt;-&gt;!a!b$</td>
<td style="text-align: center;">${(c, 1)}$ <br> c1</td>
</tr>
<tr>
<td style="text-align: center;">$\neg((b \vee e) \oplus((\neg a \vee(\neg d \leftrightarrow \neg e))$ <br> $\vee(\neg b \vee(((\neg a \wedge b) \wedge \neg b) \wedge d))))$ <br> !xor!be</td>
<td style="text-align: center;">${(d, 1),(e, 1)}$ <br> d1e1</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th style="text-align: center;">trained on</th>
<th style="text-align: center;">tested on</th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">LTLRandom35</td>
<td style="text-align: center;">LTLRandom35</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">14.7</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LTLRandom35</td>
<td style="text-align: center;">LTLRandom50</td>
<td style="text-align: center;">67.6</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">24.6</td>
<td style="text-align: center;">7.8</td>
</tr>
<tr>
<td style="text-align: center;">LTLPattern126</td>
<td style="text-align: center;">LTLPattern126</td>
<td style="text-align: center;">69.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">27.6</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">LTLPattern126</td>
<td style="text-align: center;">LTLUnsolved254</td>
<td style="text-align: center;">83.8</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td style="text-align: center;">16.1</td>
</tr>
<tr>
<td style="text-align: center;">PropRandom35</td>
<td style="text-align: center;">PropRandom35</td>
<td style="text-align: center;">58.1</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">38.4</td>
<td style="text-align: center;"></td>
</tr>
<tr>
<td style="text-align: center;">PropRandom35</td>
<td style="text-align: center;">PropRandom50</td>
<td style="text-align: center;">35.8</td>
<td style="text-align: center;">50.3</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">13.9</td>
</tr>
</tbody>
</table>
<p>Figure 2: Overview of our main experimental results: the performance of our best performing models on our different data sets. The percentage of a dark green bar refers to the syntactic accuracy, the percentage of a light green bar to the semantic accuracy without the syntactic accuracy, and the incorrect predictions are visualized in orange.</p>
<p>To test the Transformer on even more challenging formulas, we constructed a data set of CNF formulas using the generation script of Selsam et al. (2019) from their publicly available implementation. A random CNF formula is built by adding clauses until the addition of a further clause would lead to an unsatisfiable formula. We used the parameters $p_{g e o}=0.9$ and $p_{k 2}=0.75$ to generate formulas that contain up to 15 variables and have a maximum size of 250 . We call this data set $\operatorname{Prop} C N F 250$.</p>
<h1>4 EXPERIMENTAL SETUP</h1>
<p>We have implemented the Transformer architecture (Vaswani et al., 2017). Our implementation processes the input and output sequences token-by-token. We trained on a single GPU (NVIDIA P100 or V100). All training has been done with a dropout rate of 0.1 and early stopping on the validation set. Note that the embedding size will automatically be floored to be divisible by the number of attention heads. The training of the best models took up to 50 hours. For the output decoding, we utilized a beam search (Wu et al., 2016), with a beam size of 3 and an $\alpha$ of 1.
Since the solution of a logical formula is not necessarily unique, we use two different measures of accuracy to evaluate the generalization to the semantics of the logics: we distinguish between the syntactic accuracy, i.e., the percentage where the Transformers prediction syntactically matches the output of our generator and the semantic accuracy, i.e., the percentage where the Transformer produced a different solution. We also differentiate between incorrect predictions and syntactically invalid outputs which, in fact, happens only in $0.1 \%$ of the cases in LTLUnsolved254.</p>
<p>In general, our best performing models used 8 layers, 8 attention heads, and an FC size of 1024. We used a batch size of 400 and trained for $450 K$ steps ( 130 epochs) for our specification pattern data set, and a batch size of 768 and trained for $50 K$ steps ( 48 epochs) for our random formula data set. A hyperparameter study can be found in Appendix C.</p>
<h2>5 EXPERIMENTAL RESULTS</h2>
<p>In this section, we describe our experimental results. First, we show that a Transformer can indeed solve the task of providing a solution, i.e., a trace for a linear-time temporal logical (LTL) formula. For this, we describe the results from training on the data set LTLPattern126 of specification patterns that are commonly used in the context of verification. Secondly, we show two generalization properties that the Transformer evinces on logic reasoning tasks: 1) the generalization to larger formulas (even so large that our data generator timed out) and 2) the generalization to the semantics of the logic. We strengthen this observation by considering a different data set of random LTL formulas. Thirdly, we provide results for a model trained on a different logic and with a different data generator. We thereby demonstrate that the generalization behaviors of the Transformer are not specific to LTL and the LTL solver implemented with spot that we used to generate the data. An overview of our training results is displayed in Figure 2.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Predictions of our best performing model, trained on LTLUnsolved254, on 5704 specification patterns for which spot timed out ( $&gt;60 s$ ). Semantic accuracy is displayed in green; incorrect traces in orange; syntactically invalid traces in red.</p>
<h1>5.1 SOlving Linear-time Temporal Logical Formulas</h1>
<p>We trained a Transformer on our specification on LTLPattern126. Figure 1 in the introduction displays the performance of our best model on this data set. We observed a syntactic accuracy of $69.1 \%$ and a semantic accuracy of $96.8 \%$. With this experiment we can already deduce that it seems easier for the Transformer to learn the underlying semantics of LTL than to learn the particularities of the generator. Further we can see that as the formula length grows, the syntactic accuracy begins to drop. However, that drop is much smaller in the semantic accuracy-the model still mostly predicts correct traces for long formulas.</p>
<p>As a challenging benchmark, we tested our best performing model on LTLUnsolved254. It predicted correct solutions in $83 \%$ of the cases, taking on average $15 s$ on a single CPU. The syntactic accuracy is $0 \%$ as there was no output produced by spot within the timeout. The results of the experiments are visualized in Figure 3. Note that this does not mean that our Transformer models necessariy outperform classical algorithms across the board. However, since verifying solutions to LTL formulas is much easier than finding solutions ( $\mathrm{AC}^{1}(\log \mathrm{DCFL})$ vs PSPACE), this experiment shows that the predictions of a deep neural network can be a valuable extension to the verification tool box.</p>
<h3>5.2 Generalization Properties</h3>
<p>To prove that the generalization to the semantics is independent of the data generation, we also trained a model on a data set of randomly generated formulas. The unshaded part of Figure 4 displays the performance of our best model on the LTLRandom35 data set. The Transformers were solely trained on formulas of size less or equal to 35 . We observe that in this range the exact syntactic accuracy decreases when the formulas grow in size. The semantic accuracy, however, stays, again, high. The model achieves a syntactic accuracy of $83.8 \%$ and a semantic accuracy of $98.5 \%$ on LTLRandom35, i.e., in $14.7 \%$ of the cases, the Transformer deviates from our automaton-based data generator. The evolution of the syntactic and the semantic accuracy during training can be found in Appendix D.</p>
<p>To show that the generalization to larger formulas is independent from the data generation method, we also tested how well the Transformer generalizes to randomly generated LTL formulas of a size it has never seen before. We used our model trained on LTLRandom35 and observed the performance on LTLRandom50. The model preserves the semantic generalization, displayed in the shaded part of Figure 4. It outputs exact syntactic matches in $67.6 \%$ of the cases and achieves a semantic accuracy of $92.2 \%$. For the generalization to larger formulas we utilized a positional encoding based on the tree representation of the formula (Shiv \&amp; Quirk, 2019). When using the</p>
<p>standard positional encoding instead, the accuracy drops, as expected, significantly. A visualization of this experiments can be found in Appendix E.</p>
<p>In a further experiment, we tested the out-of-distribution (OOD) generalization of the Transformer on the trace generation task. We generated a new dataset LTLRandom126 to match the formula sizes and the vocabulary of LTLPattern126. A model trained on LTLRandom126 achieves a semantic accuracy of $24.7 \%$ (and a syntactic accuracy of only $1.0 \%$ ) when tested on LTLPattern126. Vice versa, a model trained on LTLPattern126 achieves a semantic accuracy of $38.5 \%$ (and a semantic accuracy of only $0.5 \%$ ) when tested on LTLRandom126. Testing the models OOD increases the gap between syntactic and semantic correctness dramatically. This underlines that the models learned the nature of the LTL semantics rather than the generator process. Note that the two distributions are very different.</p>
<p>Following these observations, we also tested the performance of our models on other patterns from the literature. We observe a higher semantic accuracy for our model trained on random formulas and a higher gap between semantic and syntactic accuracy for our model trained on pattern formulas:</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Patterns</th>
<th style="text-align: center;">Number of Patterns</th>
<th style="text-align: center;">Trained on</th>
<th style="text-align: center;">Syn. Acc.</th>
<th style="text-align: center;">Sem. Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">dac (Dwyer et al., 1998)</td>
<td style="text-align: center;">55</td>
<td style="text-align: center;">LTLRandom126</td>
<td style="text-align: center;">$49.1 \%$</td>
<td style="text-align: center;">$81.8 \%$</td>
</tr>
<tr>
<td style="text-align: center;">eh (Etessami \&amp; Holzmann, 2000)</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">LTLRandom126</td>
<td style="text-align: center;">$81.8 \%$</td>
<td style="text-align: center;">$90.9 \%$</td>
</tr>
<tr>
<td style="text-align: center;">hkrss (Holeček et al., 2004)</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">LTLRandom126</td>
<td style="text-align: center;">$71.4 \%$</td>
<td style="text-align: center;">$83.7 \%$</td>
</tr>
<tr>
<td style="text-align: center;">p (Pelánek, 2007)</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">LTLRandom126</td>
<td style="text-align: center;">$65.0 \%$</td>
<td style="text-align: center;">$90.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">eh (Etessami \&amp; Holzmann, 2000)</td>
<td style="text-align: center;">11</td>
<td style="text-align: center;">LTLPattern126</td>
<td style="text-align: center;">$0.0 \%$</td>
<td style="text-align: center;">$36.4 \%$</td>
</tr>
<tr>
<td style="text-align: center;">hkrss (Holeček et al., 2004)</td>
<td style="text-align: center;">49</td>
<td style="text-align: center;">LTLPattern126</td>
<td style="text-align: center;">$14.3 \%$</td>
<td style="text-align: center;">$49.0 \%$</td>
</tr>
<tr>
<td style="text-align: center;">p (Pelánek, 2007)</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">LTLPattern126</td>
<td style="text-align: center;">$10.0 \%$</td>
<td style="text-align: center;">$60.0 \%$</td>
</tr>
</tbody>
</table>
<p>In a last experiment on LTL, we tested the performance of our models on handcrafted formulas. We observed that formulas with multiple until statements that describe overlapping intervals were the most challenging. This is no surprise as these formulas are the source of PSPACE-hardness of LTL.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$a \mathcal{U} b \wedge a \mathcal{U} \neg b$</th>
<th style="text-align: center;">$(a \wedge \neg b)(b)($ true $)^{a c}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">\&amp;UabUa!b</td>
<td style="text-align: center;">\&amp;a!b;b; {1}</td>
</tr>
</tbody>
</table>
<p>While the above formula can be solved by most models, when scaling this formula to four overlapping until intervals, all of our models fail: For example, a model trained on LTLRandom35 predicted the trace $(a \wedge b \wedge c)(a \wedge \neg b \wedge \neg c)(b \wedge c)($ true $)^{a c}$, which does not satisfy the LTL formula.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">$(a \mathcal{U} b \wedge c) \wedge(a \mathcal{U} \neg b \wedge c) \wedge(a \mathcal{U} b \wedge \neg c) \wedge(a \mathcal{U} \neg b \wedge \neg c)$</th>
<th style="text-align: center;">$(a \wedge b \wedge c)(a \wedge \neg b \wedge \neg c){b \wedge c)($ true $)^{a c}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">\&amp;\&amp;\&amp;Ua\&amp;bcUa\&amp;!bcUa\&amp;b!cUa\&amp;!b!c</td>
<td style="text-align: center;">\&amp;\&abc;\&amp;\&amp;a!b!c;\&bc;1</td>
</tr>
</tbody>
</table>
<h1>5.3 Predicting Assignments for Propositional Logic</h1>
<p>To show that the generalization to the semantic is not a specific property of LTL, we trained a Transformer to solve the assignment generation problem for propositional logic, which is a substantially different logical problem.</p>
<p>As a baseline for our generalization experiments on propositional logic, we trained and tested a Transformer model with the following hyperparameter on PropRandom35:</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Embedding size</th>
<th style="text-align: right;">Layers</th>
<th style="text-align: right;">Heads</th>
<th style="text-align: right;">FC size</th>
<th style="text-align: right;">Batch Size</th>
<th style="text-align: right;">Train Steps</th>
<th style="text-align: right;">Syn. Acc.</th>
<th style="text-align: right;">Sem. Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">enc:128, dec:64</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">6</td>
<td style="text-align: right;">512</td>
<td style="text-align: right;">1024</td>
<td style="text-align: right;">50 K</td>
<td style="text-align: right;">$58.1 \%$</td>
<td style="text-align: right;">$\mathbf{9 6 . 5 \%}$</td>
</tr>
</tbody>
</table>
<p>We observe a striking $38.4 \%$ gap between predictions that were syntactical matches of our DPLLbased generator and correct predictions of the Transformer. Only $3.5 \%$ of the time, the Transformer outputs an incorrect assignment. Note that we allow the derived operators $\oplus$ and $\leftrightarrow$ in these experiments, which succinctly represent complicated logical constructs.</p>
<p>The formula $b \vee \neg(a \wedge d)$ occurs in our data set PropRandom35 and its corresponding assignment is ${(a, 0)}$. The Transformer, however, outputs d0, i.e., it goes with the assignment of setting $d$ to false, which is also a correct solution. A visualization of this example can be found in Appendix F. When the formulas get larger, the solutions where the Transformer differs from the DPLL algorithm accumulate. Consider, for example, the formula $\neg b \vee(c \leftrightarrow b \vee c \vee \neg d) \vee(c \wedge(b \oplus(a \oplus \neg d)) \oplus(\neg c \leftrightarrow$ $d) \wedge(a \leftrightarrow(b \oplus(b \oplus e))))$, which is also in the data set PropRandom35. The generator suggests</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Syntactic and semantic accuracy of our best performing model (only trained on LTLRandom35) on LTLRandom50. Dark green is syntactically correct; light green is semantically correct, orange is incorrect.
the assignment ${(a, 1),(c, 1),(d, 0)}$. The Transformer, however, outputs $e 0$, i.e., the singleton assignment of setting $e$ to false, which turns out to be a (very small) solution as well.</p>
<p>We achieved stable training in this experiment by setting the decoder embedding size to either 64 or even 32. Keeping the decoder embedding size at 128 led to very unstable training.</p>
<p>We also tested whether the generalization to the semantics is preserved when the Transformer encounters propositional formulas of a larger size than it ever saw during training. We, again, utilized the tree positional encoding. When challenged with formulas of size 35 to 50 , our best performing model trained on PropRandom 35 achieves a syntactic accuracy of $35.8 \%$ and a semantic accuracy of $86.1 \%$. In comparison, without the tree positional encoding, the Transformer achieves a syntactic match of only $29.0 \%$ and an overall accuracy of only $75.7 \%$. Note that both positional encodings work equally well when not considering larger formulas.</p>
<p>In a last experiment, we tested how the Transformer performs on more challenging propositional formulas in CNF. We thus trained a model on $\operatorname{PropCNF} 250$, where it achieved a semantic accuracy of $65.1 \%$ and a syntactic accuracy of $56.6 \%$. We observe a slightly lower gap compared to our LTL experiments. The Transformer, however, still deviates even on such formulas from the generator.</p>
<h1>6 CONCLUSION</h1>
<p>We trained a Transformer to predict solutions to linear-time temporal logical (LTL) formulas. We observed that our trained models evince powerful generalization properties, namely, the generalization to the semantics of the logic, and the generalization to larger formulas than seen during training. We showed that these generalizations do not depend on the underlying logical problem nor on the data generator. Regarding the performance of the trained models, we observed that they can compete with classical algorithms for generating solutions to LTL formulas. We built a test set that contained only formulas that were generated out of practical verification patterns, on which even our data generator timed out. Our best performing model, although it was trained on much smaller formulas, predicts correct traces $83 \%$ of the time.</p>
<p>The results of this paper suggest that deep learning can already augment combinatorial approaches in automatic verification and the broader formal methods community. With the results of this paper, we can, for example, derive novel algorithms for trace generation or satisfiability checking of LTL that first query a Transformer for many trace predictions. These predictions can then be checked efficiently. Classical methods can serve as a fall back or check partial solutions providing guidance to the Transformer. The potential that arises from the advent of deep learning in logical reasoning is immense. Deep learning holds the promise to empower researchers in the automated reasoning and formal methods communities to make bigger jumps in the development of new automated verification methods, but also brings new challenges, such as the acquisition of large amounts of</p>
<h2>ACKNOWLEDGEMENTS</h2>
<p>We thank Christian Szegedy, Jesko Hecking-Harbusch, and Niklas Metzger for their valuable feedback on an earlier version of this paper.</p>
<h1>REFERENCES</h1>
<p>Miltiadis Allamanis, Marc Brockschmidt, and Mahmoud Khademi. Learning to represent programs with graphs. arXiv preprint arXiv:1711.00740, 2017.</p>
<p>Gilles Audemard and Laurent Simon. On the glucose SAT solver. Int. J. Artif. Intell. Tools, 27 (1):1840001:1-1840001:25, 2018. doi: 10.1142/S0218213018400018. URL https://doi. org/10.1142/S0218213018400018.</p>
<p>Mislav Balunovic, Pavol Bielik, and Martin Vechev. Learning to solve smt formulas. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 1031710328. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 8233-learning-to-solve-smt-formulas.pdf.</p>
<p>Kshitij Bansal, Sarah M Loos, Markus N Rabe, Christian Szegedy, and Stewart Wilcox. HOList: An environment for machine learning of higher-order theorem proving. In arXiv preprint arXiv:1904.03241, 2019.
S. Bhatia, P. Kohli, and R. Singh. Neuro-symbolic program corrector for introductory programming assignments. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE), pp. 60-70, May 2018. doi: 10.1145/3180155.3180219.</p>
<p>Jasmin Christian Blanchette, Cezary Kaliszyk, Lawrence C Paulson, and Josef Urban. Hammering towards QED. Journal of Formalized Reasoning, 9(1):101-148, 2016.</p>
<p>Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. 2020.</p>
<p>Paul Cairns. Informalising formal mathematics: Searching the mizar library with latent semantics. In International Conference on Mathematical Knowledge Management, pp. 58-72. Springer, 2004.</p>
<p>Tommaso Dreossi, Alexandre Donzé, and Sanjit A Seshia. Compositional falsification of cyberphysical systems with machine learning components. Journal of Automated Reasoning, 63(4): $1031-1053,2019$.</p>
<p>Alexandre Duret-Lutz, Alexandre Lewkowicz, Amaury Fauchille, Thibaud Michaud, Etienne Renault, and Laurent Xu. Spot 2.0—a framework for ltl and $\omega$-automata manipulation. In International Symposium on Automated Technology for Verification and Analysis, pp. 122-129. Springer, 2016.</p>
<p>Matthew B. Dwyer, George S. Avrunin, and James C. Corbett. Property specification patterns for finite-state verification. In Mark A. Ardis and Joanne M. Atlee (eds.), Proceedings of the Second Workshop on Formal Methods in Software Practice, March 4-5, 1998, Clearwater Beach, Florida, USA, pp. 7-15. ACM, 1998. doi: 10.1145/298595.298598. URL https://doi.org/10. $1145 / 298595.298598$.</p>
<p>Kousha Etessami and Gerard J. Holzmann. Optimizing büchi automata. In Catuscia Palamidessi (ed.), CONCUR 2000 - Concurrency Theory, 11th International Conference, University Park, PA, USA, August 22-25, 2000, Proceedings, volume 1877 of Lecture Notes in Computer Science, pp. 153-167. Springer, 2000. doi: 10.1007/3-540-44618-41_13. URL https://doi.org/10. 1007/3-540-44618-4_13.</p>
<p>Richard Evans, David Saxton, David Amos, Pushmeet Kohli, and Edward Grefenstette. Can neural networks understand logical entailment? arXiv preprint arXiv:1802.08535, 2018.</p>
<p>Patrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. Structured neural summarization. arXiv preprint arXiv:1811.01824, 2018.</p>
<p>Thibault Gauthier, Cezary Kaliszyk, and Josef Urban. Tactictoe: Learning to reason with hol4 tactics. arXiv preprint arXiv:1804.00595, 2018.</p>
<p>Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018 IEEE Symposium on Security and Privacy (SP), pp. 3-18. IEEE, 2018.</p>
<p>Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.</p>
<p>Rahul Gupta, Soham Pal, Aditya Kanade, and Shirish Shevade. Deepfix: Fixing common C language errors by deep learning. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.</p>
<p>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pp. 1026-1034, 2015. doi: 10.1109/ICCV.2015.123. URL https://doi.org/10.1109/ICCV.2015.123.</p>
<p>Vincent J. Hellendoorn, Charles Sutton, Rishabh Singh, and Petros Maniatis. Global relational models of source code. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=BllnbRNtwr.</p>
<p>Jan Holeček, Tomáš Kratochvíla, Vojtěch Řehák, David Šafránek, Pavel Šimeček, et al. Verification results in liberouter project, 2004.</p>
<p>Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. Gamepad: A learning environment for theorem proving. arXiv preprint arXiv:1806.00608, 2018.</p>
<p>Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks. In International Conference on Computer Aided Verification, pp. 3-29. Springer, 2017.</p>
<p>IEEE-Commission et al. Ieee standard for property specification language (psl). IEEE Std 18502005, 2005.</p>
<p>Cezary Kaliszyk and Josef Urban. Learning-assisted automated reasoning with flyspeck. Journal of Automated Reasoning, 53(2):173-213, 2014.</p>
<p>Cezary Kaliszyk, François Chollet, and Christian Szegedy. HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving. In Proceedings of International Conference on Learning Representations (ICLR), 2017.</p>
<p>Guillaume Lample and François Charton. Deep learning for symbolic mathematics. In ICLR, 2020.
Gil Lederman, Markus N. Rabe, Edward A. Lee, and Sanjit A. Seshia. Learning heuristics for quantified boolean formulas through deep reinforcement learning. 2020. URL http://arxiv. org/abs/1807.08058.</p>
<p>Dennis Lee, Christian Szegedy, Markus N. Rabe, Sarah M. Loos, and Kshitij Bansal. Mathematical reasoning in latent space. 2020.</p>
<p>Jianwen Li, Lijun Zhang, Geguang Pu, Moshe Y. Vardi, and Jifeng He. LTL satisfiability checking revisited. In César Sánchez, Kristen Brent Venable, and Esteban Zimányi (eds.), 2013 20th International Symposium on Temporal Representation and Reasoning, Pensacola, FL, USA, September 26-28, 2013, pp. 91-98. IEEE Computer Society, 2013. doi: 10.1109/TIME.2013.19. URL https://doi.org/10.1109/TIME.2013.19.</p>
<p>Jianwen Li, Yinbo Yao, Geguang Pu, Lijun Zhang, and Jifeng He. Aalta: an ltl satisfiability checker over infinite/finite traces. In Proceedings of the 22nd ACM SIGSOFT international symposium on foundations of software engineering, pp. 731-734, 2014.</p>
<p>Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C. Paulson. Modelling high-level mathematical reasoning in mechanised declarative proofs. arXiv preprint arXiv:2006.09265, 2020.</p>
<p>Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural network: Data-driven traffic forecasting. arXiv preprint arXiv:1707.01926, 2017.</p>
<p>Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary Kaliszyk. Deep network guided proof search. In $L P A R, 2017$.</p>
<p>Jia Meng and Lawrence C Paulson. Lightweight relevance filtering for machine-generated resolution problems. Journal of Applied Logic, 7(1):41-57, 2009.</p>
<p>Matej Moravcík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, Kevin Waugh, Michael Johanson, and Michael H. Bowling. Deepstack: Expert-level artificial intelligence in no-limit poker. CoRR, abs/1701.01724, 2017. URL http://arxiv. org/abs/1701.01724.</p>
<p>Aditya Paliwal, Sarah M. Loos, Markus N. Rabe, Kshitij Bansal, and Christian Szegedy. Graph representations for higher-order logic and theorem proving. In AAAI, 2020.</p>
<p>Radek Pelánek. BEEM: benchmarks for explicit model checkers. In Dragan Bosnacki and Stefan Edelkamp (eds.), Model Checking Software, 14th International SPIN Workshop, Berlin, Germany, July 1-3, 2007, Proceedings, volume 4595 of Lecture Notes in Computer Science, pp. 263-267. Springer, 2007. doi: 10.1007/978-3-540-73370-6 $\$ 17. URL https://doi.org/10.1007/ $978-3-540-73370-6 _17$.</p>
<p>Chris Piech, Jonathan Huang, Andy Nguyen, Mike Phulsuksombati, Mehran Sahami, and Leonidas Guibas. Learning program embeddings to propagate feedback on student code. In International Conference on Machine Learning, pp. 1093-1102, 2015.</p>
<p>Amir Pnueli. The temporal logic of programs. In 18th Annual Symposium on Foundations of Computer Science, Providence, Rhode Island, USA, 31 October - 1 November 1977, pp. 46-57, 1977. doi: 10.1109/SFCS.1977.32. URL https://doi.org/10.1109/SFCS.1977.32.</p>
<p>Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving, 2020.</p>
<p>Markus N. Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. Mathematical reasoning via self-supervised skip-tree training. 2020.</p>
<p>Kristin Y Rozier and Moshe Y Vardi. Ltl satisfiability checking. In International SPIN Workshop on Model Checking of Software, pp. 149-167. Springer, 2007.</p>
<p>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. CoRR, abs/1904.01557, 2019. URL http://arxiv.org/ abs/1904.01557.</p>
<p>Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.</p>
<p>Imanol Schlag, Paul Smolensky, Roland Fernandez, Nebojsa Jojic, Jürgen Schmidhuber, and Jianfeng Gao. Enhancing the transformer with explicit relational encoding for math problem solving. arXiv preprint arXiv:1910.06611, 2019.</p>
<p>Stephan Schulz. System description: E 1.8. In International Conference on Logic for Programming Artificial Intelligence and Reasoning, pp. 735-743. Springer, 2013.</p>
<p>Viktor Schuppan and Luthfi Darmawan. Evaluating ltl satisfiability solvers. In International Symposium on Automated Technology for Verification and Analysis, pp. 397-413. Springer, 2011.</p>
<p>Stefan Schwendimann. A new one-pass tableau calculus for pltl. In International Conference on Automated Reasoning with Analytic Tableaux and Related Methods, pp. 277-291. Springer, 1998.</p>
<p>Daniel Selsam and Nikolaj Bjørner. Guiding high-performance SAT solvers with unsat-core predictions. In Theory and Applications of Satisfiability Testing - SAT 2019 - 22nd International Conference, SAT 2019, Lisbon, Portugal, July 9-12, 2019, Proceedings, pp. 336353, 2019. doi: 10.1007/978-3-030-24258-9 $\$ 24. URL https://doi.org/10.1007/ $978-3-030-24258-9 _24$.</p>
<p>Daniel Selsam, Matthew Lamm, Benedikt Bünz, Percy Liang, Leonardo de Moura, and David L. Dill. Learning a SAT solver from single-bit supervision. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019. URL https://openreview.net/forum?id=HJMC_1A5tm.</p>
<p>Sanjit A. Seshia and Dorsa Sadigh. Towards verified artificial intelligence. CoRR, abs/1606.08514, 2016. URL http://arxiv.org/abs/1606.08514.</p>
<p>Sanjit A Seshia, Ankush Desai, Tommaso Dreossi, Daniel J Fremont, Shromona Ghosh, Edward Kim, Sumukh Shivakumar, Marcell Vazquez-Chanlatte, and Xiangyu Yue. Formal specification for deep neural networks. In International Symposium on Automated Technology for Verification and Analysis, pp. 20-34. Springer, 2018.</p>
<p>Vighnesh Leonardo Shiv and Chris Quirk. Novel positional encodings to enable tree-based transformers. In NeurIPS 2019, 2019. URL https://www.microsoft.com/en-us/research/publication/ novel-positional-encodings-to-enable-tree-based-transformers/.</p>
<p>David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge. Nature, 550(7676):354-359, 2017.</p>
<p>Gagandeep Singh, Timon Gehr, Markus Püschel, and Martin Vechev. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages, 3(POPL):1-30, 2019 .</p>
<p>Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level performance in face verification. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2014, Columbus, OH, USA, June 23-28, 2014, pp. 1701-1708, 2014. doi: 10.1109/CVPR.2014.220. URL https://doi.org/10.1109/CVPR.2014.220.</p>
<p>Josef Urban. MPTP-motivation, implementation, first experiments. Journal of Automated Reasoning, 33(3-4):319-339, 2004.</p>
<p>Josef Urban. Malarea: a metasystem for automated reasoning in large theories. ESARLT, 257, 2007.
Josef Urban and Jan Jakubův. First neural conjecturing datasets and experiments. In Conference on Intelligent Computer Mathematics, 2020.</p>
<p>Josef Urban, Geoff Sutcliffe, Petr Pudlák, and Jiří Vyskočil. Malarea sg1-machine learner for automated reasoning with semantic guidance. In International Joint Conference on Automated Reasoning, pp. 441-456. Springer, 2008.</p>
<p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pp. 5998-6008, 2017. URL http: //papers.nips.cc/paper/7181-attention-is-all-you-need.</p>
<p>Marcell Vazquez-Chanlatte. mvcisback/py-aiger, August 2018. URL https://doi.org/10. 5281 /zenodo. 1326224.</p>
<p>Ke Wang, Rishabh Singh, and Zhendong Su. Dynamic neural program embedding for program repair. In ICLR, 2018.</p>
<p>Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. CoRR, abs/1609.08144, 2016. URL http://arxiv.org/abs/1609.08144.</p>
<p>Yuhuai Wu, Albert Jiang, Jimmy Ba, and Roger Grosse. INT: An Inequality Benchmark for Evaluating Generalization in Theorem Proving. arXiv preprint arXiv:2007.02924, 2020.</p>
<p>Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S Yu. A comprehensive survey on graph neural networks. arXiv preprint arXiv:1901.00596, 2019.</p>
<p>Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. arXiv preprint arXiv:1905.09381, 2019.</p>
<h1>APPENDIX</h1>
<h2>A LINEAR-TIME TEMPORAL LOGIC (LTL)</h2>
<p>In this section, we provide the formal syntax and semantics of Linear-time Temporal Logic (LTL). The formal syntax of LTL is given by the following grammar:</p>
<p>$$
\varphi::=p|\neg \varphi| \varphi \wedge \varphi \mid \bigcirc \varphi \mid \varphi \mathcal{U} \varphi
$$</p>
<p>where $p \in A P$ is an atomic proposition. Let $A P$ be a set of atomic propositions. A (explicit) trace $t$ is an infinite sequence over subsets of the atomic propositions. We define the set of traces $T R:=\left(2^{A P}\right)^{\omega}$. We use the following notation to manipulate traces: Let $t \in T R$ be a trace and $i \in \mathbb{N}$ be a natural number. With $t[i]$ we denote the set of propositions at $i$-th position of $t$. Therefore, $t[0]$ represents the starting element of the trace. Let $j \in \mathbb{N}$ and $j \geq i$. Then $t[i, j]$ denotes the sequence $t[i] t[i+1] \ldots t[j-1] t[j]$ and $t[i, \infty]$ denotes the infinite suffix of $t$ starting at position $i$.
Let $p \in A P$ and $t \in T R$. The semantics of an LTL formula is defined as the smallest relation $\models$ that satisfies the following conditions:</p>
<p>$$
\begin{array}{lll}
t \models p &amp; \text { iff } &amp; p \in t[0] \
t \models \neg \varphi &amp; \text { iff } &amp; t \not \models \varphi \
t \models \varphi_{1} \wedge \varphi_{2} &amp; \text { iff } &amp; t \models \varphi_{1} \text { and } t \models \varphi_{2} \
t \models \bigcirc \varphi &amp; \text { iff } &amp; t[1, \infty] \models \varphi \
t \models \varphi_{1} \mathcal{U} \varphi_{2} &amp; \text { iff } &amp; \text { there exists } i \geq 0: t[i, \infty] \models \varphi_{2} \
&amp; &amp; \text { and for all } 0 \leq j&lt;i \text { we have } t[j, \infty] \models \varphi_{1}
\end{array}
$$</p>
<p>There are several derived operators, such as $\diamond \varphi \equiv \operatorname{true} \mathcal{U} \varphi$ and $\square \varphi \equiv \neg \diamond \neg \varphi . \diamond \varphi$ states that $\varphi$ will eventually hold in the future and $\square \varphi$ states that $\varphi$ holds globally. Operators can be nested: $\square \diamond \varphi$, for example, states that $\varphi$ has to occur infinitely often.</p>
<h2>B SIZE DISTRIBUTION IN THE DATA SETS</h2>
<p>In this section, we provide insight into the size distribution of our data sets. Figure 5 shows the size distribution of the formulas in our data set LTLPattern126.</p>
<p>Figure 6 shows the size distribution of our generated formulas and their traces in the data set LTLRandom35. Table 1 shows three randomly drawn example instances of the data set LTLRandom35.</p>
<p>Lastly, Figure 7 shows the size distribution of formulas in our dataset LTLUnsolved254.
<img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Size distributions in the LTLPattern126 test set: on the x -axis is the size of the formulas; on the y -axis the number of formulas.</p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Size distributions in the LTLRandom35 training set: on the x-axis is the size of the formulas/traces; on the y-axis the number of formulas/traces.</p>
<p>Table 1: Three random examples from LTLRandom35 training set. The first line shows the LTL formula and the symbolic trace in mathematical notation. The second line shows the syntactic representation (in Polish notation):</p>
<table>
<thead>
<tr>
<th style="text-align: left;">LTL formula</th>
<th style="text-align: left;">satisfying symbolic trace</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\bigcirc((d \mathcal{U} c) \mathcal{U} \bigcirc \bigcirc d) \wedge \bigcirc(b \wedge \neg(\neg d \mathcal{U} c))$</td>
<td style="text-align: left;">true $(b \wedge \neg c \wedge \neg d)(\neg c \wedge d) d($ true $)^{\omega}$</td>
</tr>
<tr>
<td style="text-align: left;">$\&amp;$ XUUdcXXdX\&amp;b!U!dc</td>
<td style="text-align: left;">$1 ; \&amp; \&amp; b!c!d ; \&amp;!c d ; d ;{1}$</td>
</tr>
<tr>
<td style="text-align: left;">$\neg \bigcirc((\bigcirc e \wedge($ true $\mathcal{U} b) \wedge \bigcirc c) \mathcal{U} c)$</td>
<td style="text-align: left;">true $(\neg b \wedge \neg c)(\neg b)^{\omega}$</td>
</tr>
<tr>
<td style="text-align: left;">!XU\&amp;\&amp;XeUlbXcc</td>
<td style="text-align: left;">$1 ; \&amp;!b!c ;{!b}$</td>
</tr>
<tr>
<td style="text-align: left;">$\bigcirc \neg((\neg c \wedge d) \mathcal{U} \bigcirc d)$</td>
<td style="text-align: left;">true $(c \vee \neg d)(\neg d)($ true $)^{\omega}$</td>
</tr>
<tr>
<td style="text-align: left;">X!U\&amp;!cdXd</td>
<td style="text-align: left;">$1 ; \mid c!d ;!d ;{1}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Size distributions in the LTLUnsolved254 test set: on the x-axis is the size of the formulas; on the y-axis the number of formulas.</p>
<h1>C Hyperparameter Analysis</h1>
<p>Table 2 shows the effect of the most significant parameters on the performance of Transformers. The performance largely benefits from an increased number of layers, with 8 yielding the best results. Increasing the number further, even with much more training time, did not result in better or even led to worse results. A slightly less important role plays the number of heads and the dimension of the intermediate fully-connected feed-forward networks (FC). While a certain FC size is important, increasing it alone will not improve results. Changing the number of heads alone has also almost no impact on performance. Increasing both simultaneously, however, will result in a small gain.</p>
<p>Table 2: Syntactic accuracy and semantic accuracy of different Transformers, tested on LTLRandom35: Layers refer to the size of the encoder and decoder stacks; Heads refer to the number of attention heads; FC size refers to the size of the fully-connected neural networks inside the encoder and decoders.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Embedding size</th>
<th style="text-align: center;">Layers</th>
<th style="text-align: center;">Heads</th>
<th style="text-align: center;">FC size</th>
<th style="text-align: center;">Batch Size</th>
<th style="text-align: center;">Train Steps</th>
<th style="text-align: center;">Syn. Acc.</th>
<th style="text-align: center;">Sem. Acc.</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">78.0\%</td>
<td style="text-align: center;">97.1\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">80.4\%</td>
<td style="text-align: center;">97.4\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">256</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">81.0\%</td>
<td style="text-align: center;">97.4\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">82.0\%</td>
<td style="text-align: center;">97.9\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">80.3\%</td>
<td style="text-align: center;">97.3\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">81.8\%</td>
<td style="text-align: center;">97.7\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">82.0\%</td>
<td style="text-align: center;">97.8\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">82.5\%</td>
<td style="text-align: center;">97.9\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1500</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">82.6\%</td>
<td style="text-align: center;">97.8\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">81.9\%</td>
<td style="text-align: center;">97.5\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">83.2\%</td>
<td style="text-align: center;">98.3\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">1024</td>
<td style="text-align: center;">768</td>
<td style="text-align: center;">50 K</td>
<td style="text-align: center;">83.8\%</td>
<td style="text-align: center;">98.5\%</td>
</tr>
<tr>
<td style="text-align: center;">128</td>
<td style="text-align: center;">10</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">75 K</td>
<td style="text-align: center;">82.9\%</td>
<td style="text-align: center;">97.6\%</td>
</tr>
<tr>
<td style="text-align: center;">256</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">512</td>
<td style="text-align: center;">45 K</td>
<td style="text-align: center;">82.3\%</td>
<td style="text-align: center;">97.9\%</td>
</tr>
</tbody>
</table>
<p>This seems reasonable, since more heads can provide more distinct information to the subsequent processing by the fully-connected feed-forward network. Increasing the embeddings size from 128 to 256 very slightly improves the syntactic accuracy. But likewise it also degrades the semantic accuracy, so we therefore stuck with the former setting.</p>
<h1>D Accuracy During Training</h1>
<p>In Figure 8 we show the evolution of both the syntactic accuracy and the semantic accuracy during the training process. Note the significant difference right from the beginning. This demonstrates the importance of a suitable performance measure when evaluating machine learning algorithms on logical reasoning tasks.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Syntactic accuracy (blue) and semantic accuracy (red) of our best performing model, evaluated on a subset of 5 K samples of LTLRandom 35 per epoch.</p>
<h1>E Different Positional Encodings</h1>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Performance of our best model (only trained on LTLRandom35) on LTLRandom50 with a standard positional encoding (top) and a tree positional encoding (bottom). The syntactic accuracy is displayed in green, the semantic accuracy in light green and the incorrect predictions in orange. The shaded area indicates the formula sizes the model was not trained on.</p>
<h2>F Handcrafted Examples</h2>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Self-attention of the example propositional formula $b \vee \neg(a \wedge d)$ in data set PropRandom35 (left). Encoder-decoder-attention of the example LTL formula $(b \mathcal{U} a) \wedge(a \mathcal{U} \neg a)$ in data set LTLRandom35 (right).</p>
<p>The LTL formula $(b \mathcal{U} a) \wedge(a \mathcal{U} \neg a)$ states that $b$ has to hold along the trace until $a$ holds and $a$ has to hold until $a$ does not hold anymore. The automaton-based generator suggests the trace $(\neg a \wedge$ b) $a($ true $)^{w}$, i.e., to first satisfy the second until by immediately disallowing $a$. The satisfaction of the first until is then postponed to the second position of trace, which forces $b$ to hold on the first position. The Transformer, however, chooses the following more general trace $a(\neg a)($ true $)^{w}$, by satisfying the until operators in order (see Figure 10).</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Partially supported by the European Research Council (ERC) Grant OSARES (No. 683300).&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1710 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1710</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1710</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-34.html">extraction-schema-34</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <p><strong>Paper ID:</strong> paper-271218325</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2407.10999v2.pdf" target="_blank">TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot</a></p>
                <p><strong>Paper Abstract:</strong> With the rapid development of large language models (LLM), the evaluation of LLM becomes increasingly important. Measuring text generation tasks such as summarization and article creation is very difficult. Especially in specific application domains (e.g., to-business or to-customer service), in-house evaluation criteria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and business security requirements at the same time, making the evaluation more difficult. So far, the evaluation of LLM in business scenarios has mainly relied on manual, which is expensive and time-consuming. In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria. In addition, we try combining zero-shot and few-shot to make the judge model focus on more information. We also propose a prompt paradigm and an engineering approach to adjust and iterate the shots ,helping judge model to better understand the complex criteria. We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL. TALEC demonstrates a strong capability to accurately reflect human preferences and achieves a correlation of over 80% with human judgments, outperforming even the inter-human correlation in some tasks. The code is released in https://github.com/zlkqz/auto_eval</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1710.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1710.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TALEC (GPT-4 + ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot (GPT-4 judge with in-context learning)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An LLM-as-a-judge evaluation pipeline using GPT-4 with in-context learning (ICL), iterative shot engineering, criteria-division, and a combined zero-shot + few-shot multi-turn prompting paradigm to approximate human judgments on domain-specific text-generation tasks in the automobile field.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge (ICL with engineered few-shot + zero-shot multi-turn; criteria-division; prompt-paradigm)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>GPT-4 (variants: gpt-4-0613, gpt-4-32k-0613, gpt-4-0125-preview)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>Single-label rubric-style judgement via in-context learning (few-shot) combined with a separate zero-shot judgement and a multi-turn re-evaluation that concatenates system prompt, shots, and the two independent answers; criteria division (evaluate each label separately), repetition of criteria descriptions, and standardized example formatting to encourage CoT.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>Text generation outputs (sentiment labels, knowledge QA answers, search QA answers, title generation outputs)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Automobile domain (domain-specific QA and text generation)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Custom in-house multi-dimensional labels (10 labels per task; binary acceptable/unacceptable semantics; dimensions include correctness/factuality, unrelated/incorrect matches, helpfulness, adherence to requirements, etc.)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Manual annotation with multi-stage quality control: dual-review system, blind reviews, comprehensive adjudication for contentious cases, random spot-checks for non-contentious cases, and final spot checks.</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Trained domain-specific human annotators/raters (business-specific automobile criteria); underwent training and multi-stage QC</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman correlation (task-level), and label-level F1/Precision/Recall for specific labels (e.g., 'Incorrect Answer/Unrelated Matching Results')</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Task-level Spearman (Standard Prompt Paradigm, Division) — Sentiment Analysis: eval=0.9579, test=0.9693; Knowledge QA: eval=0.4945, test=0.5063; Search QA: eval=0.8263, test=0.8487; Title Generation: eval=0.9207, test=0.9006. Reported overall/aggregate comparisons in text: TALEC ~0.8962/0.875 (reported aggregated alignment against human judgments). Label-level improvements: e.g., for 'Incorrect Answer/Unrelated Matching Results' label multi-turn with zero-shot F1 (eval)=0.8148 (precision=0.8462, recall=0.7857) vs single-turn F1(eval)=0.52 (precision=0.3611, recall=0.9286).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>Using GPT-4 as judge with ICL; criteria-division (separate per-label evaluation) for most tasks; multi-turn combining zero-shot+few-shot; iterative shot engineering (train/eval/test split to create representative shots); repetition of evaluation-criteria descriptions in the system prompt; standardized formatting of positive/negative examples; tasks with clearer, less ambiguous criteria (e.g., sentiment, title generation).</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>Knowledge QA task where criteria-division sometimes hurt performance due to format-imitation (especially for the 'Incorrect Answer/Unrelated Matching Results' label); too many manually written shots causing style imitation and forgetting; long context length limits; subjective or complex/custom business criteria; cases where shots are not helpful for localizing errors.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Agreement decreased for the more complex/long-form Knowledge QA task (lower Spearman ~0.49), and some label-level phenomena (e.g., error localization) made few-shot examples less effective; for simpler/clearer generation tasks (sentiment, title generation), agreement was much higher.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Clarity and structured presentation of criteria improved agreement: repeating criteria descriptions prior to evaluation slightly improved Spearman across tasks; standardized example formatting increased reliability across tasks; unclear or inconsistently formatted shots caused the model to imitate format and reduce factual/CoT reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td>Manual annotations before quality inspection (Spearman): Sentiment Analysis=0.9054, Knowledge QA=0.6523, Search QA=0.7973, Title Generation=0.8772 (reported as alignment degree among annotators prior to QC).</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>TALEC (GPT-4+ICL) achieved higher Spearman than the pre-QC inter-human alignment in several tasks (e.g., Sentiment: 0.9579 vs human 0.9054; Search QA: 0.8263/0.8487 vs human 0.7973; Title Generation: 0.9207/0.9006 vs human 0.8772), but underperformed inter-human agreement for Knowledge QA (TALEC ~0.4945 vs human 0.6523). The authors also state TALEC achieves correlation >80% with human judgments and in some tasks outperforms inter-human correlation.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>Proxy was 'trained' via in-context learning: iterative shot engineering using a train/eval/test split to select representative few-shot examples; prompt engineering (criteria repetition, standardized formats); combined zero-shot + few-shot multi-turn chaining. GPT-4 was not fine-tuned; Qwen-72B-Chat was fine-tuned in a separate SFT experiment (described elsewhere in the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>GPT-4 used as an LLM judge with in-context learning and engineering (criteria division, multi-turn zero-shot+few-shot, repetition of criteria, standardized examples) can achieve very high alignment with human judgments on domain-specific text-generation tasks (Spearman >0.9 on sentiment and title generation; >0.82 on search QA), sometimes matching or exceeding pre-QC inter-annotator agreement; however performance varies by task and label (notably lower on Knowledge QA overall). Combining zero-shot and few-shot (multi-turn) substantially improved label-level performance for the 'Incorrect Answer/Unrelated Matching Results' label; criteria division generally improved performance except for specific label/task combinations where format imitation occurred.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>The judge model can imitate the style/format of manual shots, causing omission of key information and degrading Chain-of-Thought reasoning; too many shots hit context-length limits; heavy initial manual annotation is required to bootstrap the system; TALEC still makes occasional large mistakes; some task labels (notably in Knowledge QA) are difficult to evaluate automatically and criteria-division can harm performance there; TALEC currently cannot reliably evaluate certain abilities (e.g., hallucination, contextual memory).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1710.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1710.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SFT vs ICL (Qwen-72B-Chat)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison between Supervised Fine-Tuning (SFT) and In-Context Learning (ICL) using Qwen-72B-Chat and GPT-4 for evaluation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Paper reports experiments comparing a fine-tuned open-source judge (Qwen-72B-Chat + SFT) vs the same base model used with ICL (Qwen-72B-Chat + ICL), and vs GPT-4 + ICL, to assess whether ICL can substitute SFT for automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>LLM-as-a-judge; comparison of SFT (supervised fine-tuning) vs ICL (few-shot/zero-shot) approaches</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td>Qwen-72B-Chat (fine-tuned variant and ICL variant) and GPT-4 (ICL)</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td>For ICL: few-shot examples and prompt engineering; For SFT: supervised fine-tune on curated annotated dataset (179 in-house high-quality + 100 TigerScore + 300 general data)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>Text generation outputs (same tasks as TALEC: sentiment, knowledge QA, search QA, title generation)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>Automobile domain (and some general evaluation data included in SFT training)</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>Same custom in-house labels as TALEC (10 labels per task)</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Same manual annotation process used for ground truth (multi-stage QC described in paper)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Trained domain-specific annotators</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman correlation (task-level); comparative task performance reported (table references)</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>ICL on a strong base model (GPT-4) produced the best results; Qwen-72B-Chat with ICL performed comparably to Qwen-72B-Chat with SFT on several tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>SFT on a weaker open-source base model (Qwen) cannot match the performance of a SOTA closed-source model (GPT-4); fine-tuning a weaker model has an upper limit imposed by the base model capacity.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>Not explicitly quantified beyond task-level differences; general pattern mirrors TALEC results (complex Knowledge QA harder).</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>ICL benefits from clear-shot examples and prompt engineering; SFT learned from finite curated annotations but did not universally outperform ICL.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td>For SFT training described: 179 manually annotated in-house high-quality data + 100 TigerScore open-source evaluation data + 300 open-source general data (total 579 examples used for SFT training); evaluation sample sizes not specified.</td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>Qualitative: GPT-4 + ICL outperformed both Qwen + ICL and Qwen + SFT across all tasks; Qwen + ICL was comparable to Qwen + SFT on two tasks, better on one task, and worse on another (no single numeric aggregate provided in text excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>SFT: Qwen-72B-Chat fine-tuned for one epoch on the curated dataset (579 examples). ICL: in-context few-shot examples chosen via iterative engineering (train/eval/test splits). GPT-4 was only used with ICL (no SFT).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In-context learning on a strong, proprietary LLM (GPT-4) can substitute for supervised fine-tuning on weaker open models; Qwen-72B-Chat combined with ICL can match or sometimes exceed Qwen-72B-Chat with SFT, indicating ICL can achieve results similar to SFT when base model capacity is sufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>SFT requires curated annotated data and is constrained by base model capacity; comparing SFT on open models to ICL on proprietary SOTA models is not an apples-to-apples comparison; full numeric comparisons for all tasks are not provided in the excerpt (Table 6 referenced but not included).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1710.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1710.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of comparisons between automated evaluation methods (LLM-as-a-judge, Likert-style ratings, or other proxy evaluations) and human expert evaluations for software development artifacts, including agreement metrics and conditions that affect alignment.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Comparison to other automated metrics</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Comparison of TALEC to GPTScore, TigerScore, and MT-Bench</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The paper compares TALEC's alignment with human judgments against existing automated metrics (GPTScore, TigerScore) and automated pairwise comparison benchmarks (MT-Bench), reporting substantially higher Spearman correlations for TALEC.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>proxy_evaluation_method</strong></td>
                            <td>Automated metrics and LLM-based benchmarks (GPTScore, TigerScore, MT-Bench) used as baselines for alignment with human judgments</td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_model</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>llm_judge_prompt_approach</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>artifact_type</strong></td>
                            <td>Text generation outputs (various)</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_domain</strong></td>
                            <td>General text generation benchmarks and the paper's automobile-domain tasks</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_criteria</strong></td>
                            <td>General metrics (perplexity-based GPTScore), explainable metric (TigerScore), pairwise LLM comparison (MT-Bench), versus TALEC's in-house rubric</td>
                        </tr>
                        <tr>
                            <td><strong>human_evaluation_setup</strong></td>
                            <td>Human judgments used as ground truth for correlation comparisons (paper's manual annotation pipeline with QC)</td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>human_expert_expertise</strong></td>
                            <td>Trained annotators/domain-specific raters used to produce ground truth labels</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_metric</strong></td>
                            <td>Spearman correlation</td>
                        </tr>
                        <tr>
                            <td><strong>agreement_score</strong></td>
                            <td>Reported Spearman correlations in text: GPTScore = 0.1888; Tiger-Score = 0.3373; MT-Bench = 0.6 / 0.85 (two reported figures); TALEC = 0.8962 / 0.875 (reported aggregated values in text).</td>
                        </tr>
                        <tr>
                            <td><strong>high_agreement_conditions</strong></td>
                            <td>TALEC's rubric-driven, in-context learning approach with standardized shots and criteria repetition; use of SOTA judge (GPT-4) and task-specific shots and engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>low_agreement_conditions</strong></td>
                            <td>General-purpose automated metrics (e.g., perplexity-based) and less task-specific metrics show low correlation with human judgments for these domain-specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>artifact_complexity_effect</strong></td>
                            <td>General automated metrics showed poor correlation particularly for domain-specific or complex generation tasks compared to TALEC.</td>
                        </tr>
                        <tr>
                            <td><strong>criteria_clarity_effect</strong></td>
                            <td>Task-specific, well-engineered criteria (TALEC) strongly improved alignment compared to generic automated metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>inter_human_agreement</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>proxy_vs_human_comparison</strong></td>
                            <td>TALEC reported much higher Spearman correlations with human judgments than GPTScore and TigerScore, and higher than the cited MT-Bench numbers in at least one aggregate comparison; authors argue TALEC more closely reflects human preferences for their domain-specific tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>calibration_or_training</strong></td>
                            <td>TALEC uses ICL and iterative shot engineering; baselines are used as-is (GPTScore, TigerScore), not re-trained on the paper's in-house criteria.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Off-the-shelf automated metrics (GPTScore, TigerScore) show low Spearman correlation with human judgments on domain-specific tasks, while TALEC (GPT-4 + ICL + engineering) yields substantially higher alignment, suggesting the value of task-specific rubric injection and prompt engineering.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_noted</strong></td>
                            <td>Direct head-to-head comparisons are imperfect due to differences in score systems, evaluation question types, and evaluation criteria; reported baseline numbers may not be computed on identical tasks/splits.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot', 'publication_date_yy_mm': '2024-06'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Gptscore: Evaluate as you desire. <em>(Rating: 2)</em></li>
                <li>MT-Bench <em>(Rating: 2)</em></li>
                <li>Tigerscore: Towards building explainable metric for all text generation tasks. <em>(Rating: 2)</em></li>
                <li>Qwen technical report. <em>(Rating: 2)</em></li>
                <li>Gpt-4 technical report. <em>(Rating: 2)</em></li>
                <li>Training language models to follow instructions with human feedback <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1710",
    "paper_id": "paper-271218325",
    "extraction_schema_id": "extraction-schema-34",
    "extracted_data": [
        {
            "name_short": "TALEC (GPT-4 + ICL)",
            "name_full": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot (GPT-4 judge with in-context learning)",
            "brief_description": "An LLM-as-a-judge evaluation pipeline using GPT-4 with in-context learning (ICL), iterative shot engineering, criteria-division, and a combined zero-shot + few-shot multi-turn prompting paradigm to approximate human judgments on domain-specific text-generation tasks in the automobile field.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge (ICL with engineered few-shot + zero-shot multi-turn; criteria-division; prompt-paradigm)",
            "llm_judge_model": "GPT-4 (variants: gpt-4-0613, gpt-4-32k-0613, gpt-4-0125-preview)",
            "llm_judge_prompt_approach": "Single-label rubric-style judgement via in-context learning (few-shot) combined with a separate zero-shot judgement and a multi-turn re-evaluation that concatenates system prompt, shots, and the two independent answers; criteria division (evaluate each label separately), repetition of criteria descriptions, and standardized example formatting to encourage CoT.",
            "artifact_type": "Text generation outputs (sentiment labels, knowledge QA answers, search QA answers, title generation outputs)",
            "artifact_domain": "Automobile domain (domain-specific QA and text generation)",
            "evaluation_criteria": "Custom in-house multi-dimensional labels (10 labels per task; binary acceptable/unacceptable semantics; dimensions include correctness/factuality, unrelated/incorrect matches, helpfulness, adherence to requirements, etc.)",
            "human_evaluation_setup": "Manual annotation with multi-stage quality control: dual-review system, blind reviews, comprehensive adjudication for contentious cases, random spot-checks for non-contentious cases, and final spot checks.",
            "human_expert_count": null,
            "human_expert_expertise": "Trained domain-specific human annotators/raters (business-specific automobile criteria); underwent training and multi-stage QC",
            "agreement_metric": "Spearman correlation (task-level), and label-level F1/Precision/Recall for specific labels (e.g., 'Incorrect Answer/Unrelated Matching Results')",
            "agreement_score": "Task-level Spearman (Standard Prompt Paradigm, Division) — Sentiment Analysis: eval=0.9579, test=0.9693; Knowledge QA: eval=0.4945, test=0.5063; Search QA: eval=0.8263, test=0.8487; Title Generation: eval=0.9207, test=0.9006. Reported overall/aggregate comparisons in text: TALEC ~0.8962/0.875 (reported aggregated alignment against human judgments). Label-level improvements: e.g., for 'Incorrect Answer/Unrelated Matching Results' label multi-turn with zero-shot F1 (eval)=0.8148 (precision=0.8462, recall=0.7857) vs single-turn F1(eval)=0.52 (precision=0.3611, recall=0.9286).",
            "high_agreement_conditions": "Using GPT-4 as judge with ICL; criteria-division (separate per-label evaluation) for most tasks; multi-turn combining zero-shot+few-shot; iterative shot engineering (train/eval/test split to create representative shots); repetition of evaluation-criteria descriptions in the system prompt; standardized formatting of positive/negative examples; tasks with clearer, less ambiguous criteria (e.g., sentiment, title generation).",
            "low_agreement_conditions": "Knowledge QA task where criteria-division sometimes hurt performance due to format-imitation (especially for the 'Incorrect Answer/Unrelated Matching Results' label); too many manually written shots causing style imitation and forgetting; long context length limits; subjective or complex/custom business criteria; cases where shots are not helpful for localizing errors.",
            "artifact_complexity_effect": "Agreement decreased for the more complex/long-form Knowledge QA task (lower Spearman ~0.49), and some label-level phenomena (e.g., error localization) made few-shot examples less effective; for simpler/clearer generation tasks (sentiment, title generation), agreement was much higher.",
            "criteria_clarity_effect": "Clarity and structured presentation of criteria improved agreement: repeating criteria descriptions prior to evaluation slightly improved Spearman across tasks; standardized example formatting increased reliability across tasks; unclear or inconsistently formatted shots caused the model to imitate format and reduce factual/CoT reasoning.",
            "sample_size": null,
            "inter_human_agreement": "Manual annotations before quality inspection (Spearman): Sentiment Analysis=0.9054, Knowledge QA=0.6523, Search QA=0.7973, Title Generation=0.8772 (reported as alignment degree among annotators prior to QC).",
            "proxy_vs_human_comparison": "TALEC (GPT-4+ICL) achieved higher Spearman than the pre-QC inter-human alignment in several tasks (e.g., Sentiment: 0.9579 vs human 0.9054; Search QA: 0.8263/0.8487 vs human 0.7973; Title Generation: 0.9207/0.9006 vs human 0.8772), but underperformed inter-human agreement for Knowledge QA (TALEC ~0.4945 vs human 0.6523). The authors also state TALEC achieves correlation &gt;80% with human judgments and in some tasks outperforms inter-human correlation.",
            "calibration_or_training": "Proxy was 'trained' via in-context learning: iterative shot engineering using a train/eval/test split to select representative few-shot examples; prompt engineering (criteria repetition, standardized formats); combined zero-shot + few-shot multi-turn chaining. GPT-4 was not fine-tuned; Qwen-72B-Chat was fine-tuned in a separate SFT experiment (described elsewhere in the paper).",
            "key_findings": "GPT-4 used as an LLM judge with in-context learning and engineering (criteria division, multi-turn zero-shot+few-shot, repetition of criteria, standardized examples) can achieve very high alignment with human judgments on domain-specific text-generation tasks (Spearman &gt;0.9 on sentiment and title generation; &gt;0.82 on search QA), sometimes matching or exceeding pre-QC inter-annotator agreement; however performance varies by task and label (notably lower on Knowledge QA overall). Combining zero-shot and few-shot (multi-turn) substantially improved label-level performance for the 'Incorrect Answer/Unrelated Matching Results' label; criteria division generally improved performance except for specific label/task combinations where format imitation occurred.",
            "limitations_noted": "The judge model can imitate the style/format of manual shots, causing omission of key information and degrading Chain-of-Thought reasoning; too many shots hit context-length limits; heavy initial manual annotation is required to bootstrap the system; TALEC still makes occasional large mistakes; some task labels (notably in Knowledge QA) are difficult to evaluate automatically and criteria-division can harm performance there; TALEC currently cannot reliably evaluate certain abilities (e.g., hallucination, contextual memory).",
            "uuid": "e1710.0",
            "source_info": {
                "paper_title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "SFT vs ICL (Qwen-72B-Chat)",
            "name_full": "Comparison between Supervised Fine-Tuning (SFT) and In-Context Learning (ICL) using Qwen-72B-Chat and GPT-4 for evaluation",
            "brief_description": "Paper reports experiments comparing a fine-tuned open-source judge (Qwen-72B-Chat + SFT) vs the same base model used with ICL (Qwen-72B-Chat + ICL), and vs GPT-4 + ICL, to assess whether ICL can substitute SFT for automated evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "proxy_evaluation_method": "LLM-as-a-judge; comparison of SFT (supervised fine-tuning) vs ICL (few-shot/zero-shot) approaches",
            "llm_judge_model": "Qwen-72B-Chat (fine-tuned variant and ICL variant) and GPT-4 (ICL)",
            "llm_judge_prompt_approach": "For ICL: few-shot examples and prompt engineering; For SFT: supervised fine-tune on curated annotated dataset (179 in-house high-quality + 100 TigerScore + 300 general data)",
            "artifact_type": "Text generation outputs (same tasks as TALEC: sentiment, knowledge QA, search QA, title generation)",
            "artifact_domain": "Automobile domain (and some general evaluation data included in SFT training)",
            "evaluation_criteria": "Same custom in-house labels as TALEC (10 labels per task)",
            "human_evaluation_setup": "Same manual annotation process used for ground truth (multi-stage QC described in paper)",
            "human_expert_count": null,
            "human_expert_expertise": "Trained domain-specific annotators",
            "agreement_metric": "Spearman correlation (task-level); comparative task performance reported (table references)",
            "agreement_score": null,
            "high_agreement_conditions": "ICL on a strong base model (GPT-4) produced the best results; Qwen-72B-Chat with ICL performed comparably to Qwen-72B-Chat with SFT on several tasks.",
            "low_agreement_conditions": "SFT on a weaker open-source base model (Qwen) cannot match the performance of a SOTA closed-source model (GPT-4); fine-tuning a weaker model has an upper limit imposed by the base model capacity.",
            "artifact_complexity_effect": "Not explicitly quantified beyond task-level differences; general pattern mirrors TALEC results (complex Knowledge QA harder).",
            "criteria_clarity_effect": "ICL benefits from clear-shot examples and prompt engineering; SFT learned from finite curated annotations but did not universally outperform ICL.",
            "sample_size": "For SFT training described: 179 manually annotated in-house high-quality data + 100 TigerScore open-source evaluation data + 300 open-source general data (total 579 examples used for SFT training); evaluation sample sizes not specified.",
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "Qualitative: GPT-4 + ICL outperformed both Qwen + ICL and Qwen + SFT across all tasks; Qwen + ICL was comparable to Qwen + SFT on two tasks, better on one task, and worse on another (no single numeric aggregate provided in text excerpt).",
            "calibration_or_training": "SFT: Qwen-72B-Chat fine-tuned for one epoch on the curated dataset (579 examples). ICL: in-context few-shot examples chosen via iterative engineering (train/eval/test splits). GPT-4 was only used with ICL (no SFT).",
            "key_findings": "In-context learning on a strong, proprietary LLM (GPT-4) can substitute for supervised fine-tuning on weaker open models; Qwen-72B-Chat combined with ICL can match or sometimes exceed Qwen-72B-Chat with SFT, indicating ICL can achieve results similar to SFT when base model capacity is sufficient.",
            "limitations_noted": "SFT requires curated annotated data and is constrained by base model capacity; comparing SFT on open models to ICL on proprietary SOTA models is not an apples-to-apples comparison; full numeric comparisons for all tasks are not provided in the excerpt (Table 6 referenced but not included).",
            "uuid": "e1710.1",
            "source_info": {
                "paper_title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot",
                "publication_date_yy_mm": "2024-06"
            }
        },
        {
            "name_short": "Comparison to other automated metrics",
            "name_full": "Comparison of TALEC to GPTScore, TigerScore, and MT-Bench",
            "brief_description": "The paper compares TALEC's alignment with human judgments against existing automated metrics (GPTScore, TigerScore) and automated pairwise comparison benchmarks (MT-Bench), reporting substantially higher Spearman correlations for TALEC.",
            "citation_title": "here",
            "mention_or_use": "mention",
            "proxy_evaluation_method": "Automated metrics and LLM-based benchmarks (GPTScore, TigerScore, MT-Bench) used as baselines for alignment with human judgments",
            "llm_judge_model": null,
            "llm_judge_prompt_approach": null,
            "artifact_type": "Text generation outputs (various)",
            "artifact_domain": "General text generation benchmarks and the paper's automobile-domain tasks",
            "evaluation_criteria": "General metrics (perplexity-based GPTScore), explainable metric (TigerScore), pairwise LLM comparison (MT-Bench), versus TALEC's in-house rubric",
            "human_evaluation_setup": "Human judgments used as ground truth for correlation comparisons (paper's manual annotation pipeline with QC)",
            "human_expert_count": null,
            "human_expert_expertise": "Trained annotators/domain-specific raters used to produce ground truth labels",
            "agreement_metric": "Spearman correlation",
            "agreement_score": "Reported Spearman correlations in text: GPTScore = 0.1888; Tiger-Score = 0.3373; MT-Bench = 0.6 / 0.85 (two reported figures); TALEC = 0.8962 / 0.875 (reported aggregated values in text).",
            "high_agreement_conditions": "TALEC's rubric-driven, in-context learning approach with standardized shots and criteria repetition; use of SOTA judge (GPT-4) and task-specific shots and engineering.",
            "low_agreement_conditions": "General-purpose automated metrics (e.g., perplexity-based) and less task-specific metrics show low correlation with human judgments for these domain-specific tasks.",
            "artifact_complexity_effect": "General automated metrics showed poor correlation particularly for domain-specific or complex generation tasks compared to TALEC.",
            "criteria_clarity_effect": "Task-specific, well-engineered criteria (TALEC) strongly improved alignment compared to generic automated metrics.",
            "sample_size": null,
            "inter_human_agreement": null,
            "proxy_vs_human_comparison": "TALEC reported much higher Spearman correlations with human judgments than GPTScore and TigerScore, and higher than the cited MT-Bench numbers in at least one aggregate comparison; authors argue TALEC more closely reflects human preferences for their domain-specific tasks.",
            "calibration_or_training": "TALEC uses ICL and iterative shot engineering; baselines are used as-is (GPTScore, TigerScore), not re-trained on the paper's in-house criteria.",
            "key_findings": "Off-the-shelf automated metrics (GPTScore, TigerScore) show low Spearman correlation with human judgments on domain-specific tasks, while TALEC (GPT-4 + ICL + engineering) yields substantially higher alignment, suggesting the value of task-specific rubric injection and prompt engineering.",
            "limitations_noted": "Direct head-to-head comparisons are imperfect due to differences in score systems, evaluation question types, and evaluation criteria; reported baseline numbers may not be computed on identical tasks/splits.",
            "uuid": "e1710.2",
            "source_info": {
                "paper_title": "TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot",
                "publication_date_yy_mm": "2024-06"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Gptscore: Evaluate as you desire.",
            "rating": 2,
            "sanitized_title": "gptscore_evaluate_as_you_desire"
        },
        {
            "paper_title": "MT-Bench",
            "rating": 2
        },
        {
            "paper_title": "Tigerscore: Towards building explainable metric for all text generation tasks.",
            "rating": 2,
            "sanitized_title": "tigerscore_towards_building_explainable_metric_for_all_text_generation_tasks"
        },
        {
            "paper_title": "Qwen technical report.",
            "rating": 2,
            "sanitized_title": "qwen_technical_report"
        },
        {
            "paper_title": "Gpt-4 technical report.",
            "rating": 2,
            "sanitized_title": "gpt4_technical_report"
        },
        {
            "paper_title": "Training language models to follow instructions with human feedback",
            "rating": 1,
            "sanitized_title": "training_language_models_to_follow_instructions_with_human_feedback"
        }
    ],
    "cost": 0.01597925,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot
24 Sep 2025</p>
<p>Kaiqi Zhang zhangkaiqi.zlk@gmail.com 
University of Electronic Science and Technology of China</p>
<p>Shuai Yuan 
Honghan Zhao zhaohonghan@bytedance.com 
Bytedance China 
Hongru Wang 
Rui Wang 
Fei Mi 
Zezhong Wang 
Ruifeng Xu 
Kam-Fai Wong 
Yidong Wang 
Zhuohao Yu 
Zhengran Zeng 
Linyi Yang 
Cunxiang Wang 
Hao Chen </p>
<p>Chaoya Jiang
Jindong Wang, Xing Xie, et al. 2023bRui Xie</p>
<p>TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot
24 Sep 20257D904DE9A1083788E2499951CC2B6DD8arXiv:2407.10999v2[cs.CL]
With the rapid development of large language models (LLM), the evaluation of LLM becomes increasingly important.Measuring text generation tasks such as summarization and article creation is very difficult.Especially in specific application domains (e.g., to-business or to-customer service), in-house evaluation criteria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and business security requirements at the same time, making the evaluation more difficult.So far, the evaluation of LLM in business scenarios has mainly relied on manual, which is expensive and time-consuming.In this paper, we propose a model-based evaluation method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria.In addition, we try combining zero-shot and few-shot to make the judge model focus on more information.We also propose a prompt paradigm and an engineering approach to adjust and iterate the shots ,helping judge model to better understand the complex criteria.We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL.TALEC demonstrates a strong capability to accurately reflect human preferences and achieves a correlation of over 80% with human judgments, outperforming even the inter-human correlation in some tasks.The code is released in https://github.com/zlkqz/auto_eval.</p>
<p>Introduction</p>
<p>Automatically evaluating an outputted span of text from a model is difficult because of its uncertainty in text format and diversity of tasks.It is different from the other simple tasks like classification, which can simply evaluate outputs of models by splitting datasets.Automatic evaluation of a span 0 First submitted on June 25, 2024.</p>
<p>of text usually uses a model-based (e.g., Zheng et al. (2024); Jiang et al. (2023); Wang et al. ( 2023b)) or statistics-based (e.g., Fu et al. (2023); Papineni et al. (2002)); Lin (2004)) method to evaluate.And it considers various standards (correctness, helpfulness and creativity, etc.) of the text.</p>
<p>Since the birth of ChatGPT (Ouyang et al. (2022)) at the end of 2022, NLP research and development has officially entered the era of LLM.Although the R&amp;D of LLM is rapid, there is still a lack of available automatic evaluation methods for LLM.Especially in specific application domains (e.g., to-business or to-customer service), in-house evaluation criteria have to meet not only general standards (correctness, helpfulness and creativity, etc.) but also specific needs of customers and business security requirements at the same time, making the evaluation more difficult.</p>
<p>In this paper, we propose a model-based evaluation method: TALEC.TALEC focuses on evaluation in specific application scenarios.Besides, all the experiments and benchmark in this paper are related to our real application and are in the automobile field.TALEC allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL, Brown et al. (2020)) to teach judge model these in-house criteria.Our criteria can be viewed in Table 1.We also propose an engineering approach to adjust and iterate the shots, which is splitting the dataset to "train", "eval" and "test" dataset.The "train" dataset is to find typical cases.Then we will provide these typical cases for the context as shots.The remaining two datasets is to help to adjust and iterate the shots and Verify the final result.</p>
<p>In addition, we find some problems when using shots which is written manually.Moreover, too many shots will also cause forgetting some former information and may exceeding context length limit.To solve this, We come up with a prompt paradigm and try combining zero-shot and few-shot to make the judge model focus on more information.</p>
<p>As all know, fine-tuning is a good way to make model adapt to a downstream task (Devlin et al. (2018)), including evaluating the outputs of other models.But now, almost all SOTA LLMs (e.g., GPT-4 (Achiam et al. (2023))) are closed-source.Some people alternatively use weaker model like Llama (Touvron et al. (2023)) to fine-tune a judge model.However, this method has its upper limit since the weakness of base model.Therefore, some people use SOTA models with ICL to judge.So we compare fine-tuning with ICL, finding that finetuning can be replaced by ICL.</p>
<p>In the end of this paper, we compare TALEC with the other automatic evaluation method and humans.TALEC demonstrates a strong capability to accurately reflect human preferences and achieves a correlation of over 80% with human judgments, outperforming many other methods and even the inter-human correlation in some tasks</p>
<p>Related Works</p>
<p>The way of evaluating deep learning model has changed dramatically since the birth of ChatGPT.Before the this, various methods to automatically evaluate have been proposed.BLEU (Papineni et al. (2002)) and ROUGE (Lin (2004)) calculate the similarity between output text and reference text.But restricted to LLMs' uncertainty in output text format and diversity of tasks, it is difficult to offer a good and proper reference text.GPTScore (Fu et al. (2023)) uses perplexity (PPL, Jelinek et al. (1977)) to evaluate output text based on former context, but recent research shows that there is no correlation between PPL and LLMs' long-text understanding ability (Hu et al. (2024)).MMLU (Hendrycks et al. (2020)), GPQA (Rein et al. (2023)), C-Eval (Huang et al. (2024)) and some benchmark in SueprCLUE (Xu et al. (2023)) use multiple choice questions to evaluate.This method is simple and efficient, but it only focuses on model's knowledge and reasoning abilities, lacking of other abilities like instructionfollowing.</p>
<p>After the birth of ChatGPT, automatic evaluation methods become more explainable and pay more attention to the abilities of multiple dimensions of the model.MT-Bench (Zheng et al. (2024)) uses GPT-4 to compare responses from tow different models and pay special attention to multi-turn dialogue ability of model.Besides, There are a lot of meth-ods use fine-tuned model or aligned model to evaluate (Jiang et al. (2023);Wang et al. (2023b), etc.).Some methods focus on special abilities of LLM, such as LLM-EVAL (Lin and Chen (2023)) is a unified multidimensional automatic evaluation method for open-domain conversations with LLMs.Truth-fulQA (Lin et al. (2021)) focuses on hallucination of LLM.IFEval (Zhou et al. (2023)) designs some tasks to measure the instruction-following ability of LLM.HumanEval (Chen et al. (2021)) and MBPP (Austin et al. (2021)) is benchmark to measure coding ability of LLM and they use pass@k as the final score.MATH (Hendrycks et al. (2021)) and GSM8K (Cobbe et al. (2021)) pay more attention to mathematical ability of LLM.And some methods use unique approach to evaluate, like BotChat (Duan et al. (2023)), which uses a approach similar to the Turing test to evaluate.</p>
<p>But all of them only use some general criteria (correctness, helpfulness and creativity, etc.) and have low correlation with human, making it unavailable in specific application domains.So we now formally introduce our method: TALEC.</p>
<p>3 Our Method: TALEC</p>
<p>Customized Business Evaluation Criteria</p>
<p>TALEC is grounded in customizable, challenging, and adaptable evaluation criteria, distinguishing itself from conventional automatic methods.It allows users to flexibly set their own evaluation criteria, which maybe more difficult than some general criteria because the criteria may have to meet not only general standards but also specific needs of customers and business security requirements at the same time.In addition, it is hard to set the exact scale of the criteria, making it more difficult sometimes.Even for a human evaluator, a series of practices and quality inspections is needed.</p>
<p>In this paper, we do experiments on four distinct tasks and ten customized labels.The tasks include:</p>
<p>Sentiment Analysis.Given a comment, determine the type of sentiment based on the textual information and provide the reason for the judgment.</p>
<p>Knowledge QA.Given a knowledge question in the automobile field, provide a detailed answer to this question.</p>
<p>Search QA.Given a piece of reference information from search engines and a related question, answer the question based on the reference information.</p>
<p>Figure 1: The overall process of TALEC Title Generation.Given an article and some complex requirements, generate main title and subtitle that is based on the article and completely meets the requirements.</p>
<p>Labels and their descriptions can be viewed in Table 1.The labels can be divided into two categories: acceptable labels(score=1) and unacceptable labels(score=0).If any unacceptable label appears, the final sore will be 0. If unacceptable label does not appear but any acceptable label appears, the final sore will be 1.If no labels appears, the final sore will be 2 (full score).</p>
<p>Assumption of TALEC</p>
<p>Many automatic evaluation only rely on the ability of the model itself, without any knowledge injection and teaching based on concrete problems and examples.Actually, even for a human evaluator, a series of practices with concrete examples and quality inspections is needed to learn our evaluation criteria.Therefore, the point is treating judge model like a human evaluator.What we need to do is teaching the judge model repeatedly and patiently with concrete examples.In order to do this, we simulate this practice-quality inspection cycle process by splitting the dataset to "train", "eval", "test" dataset and adding manual adjustment.The details of this simulation will be displayed in Section 3.3.</p>
<p>TALEC</p>
<p>We introduce TALEC, a novel automatic evaluation framework that leverages SOTA model like GPT-4 to evaluate an outputted span of text from a model.TALEC mainly uses ICL to teach judge model the customized evaluation criteria.The overall process of TALEC can be viewed in Figure 1.We will introduce several key points of TALEC below.</p>
<p>Engineering Approach to Adjust and Iterate the Shots.We split the dataset to "train", "eval", "test" dataset.The "train" dataset is to find typical cases.Then we will provide these typical cases for the context as shots.The "eval" dataset is to help manual optimization.The overall process can be listed as: (1).Find a some typical cases by feeling, then write the reasons why the cases are wrong, and regard them as the first version of shots.</p>
<p>(2).Use this version of shots to run and gather statistics on "eval" dataset.(3).adjust the shots (add/delete/modify) based on the results on "eval" dataset.(4).Repeat the above process until you get a better results on "eval" dataset.After this process, use the final version of shots to run and gather statistics on "test" dataset, to verify the effectiveness of the shots.</p>
<p>Criteria Division.In our customized criteria, there are 10 labels per task.Each label has several positive and negative shots.So too many shots may cause exceeding context length limit.Criteria division will solve this problem.It is to divide the overall criteria to label granularity.For example, we assume there are 10 labels.Normally, we will let GPT-4 determine whether these 10 labels exist at one time.But criteria division is to let GPT-4 judge 10 times, and only evaluate one label each time.In addition, we also find that despite a 10-fold increase in cost, criteria division result in greatly improvement in judging on almost all the labels.We will discuss the improvement in Section 4.1.</p>
<p>Prompt Paradigm.ICL is a good way to teach judge model the in-house criteria.However, it will cause some problems.When injecting manually- Failure to address all needs in the directive 1</p>
<p>Table 1: Labels to be evaluated.The labels can be divided into two categories: acceptable labels(score=1) and unacceptable labels(score=0).If any unacceptable label appears, the final sore will be 0. If unacceptable label does not appear but any acceptable label appears, the final sore will be 1.If no labels appears, the final sore will be 2 (full score).Note that we don't use our model-based method to judge the word count because LLM can't accurately count the number of words.</p>
<p>written shots into the context of a model, the model will not only try to understand the shots but also imitate the writing style of shots.This imitation may make the model ignore some key information and drop its Chain of Thought (CoT, Wang et al. ( 2023a)) ability.The prompt paradigm can be listed as: (1).Repeat the description of a label before judge.</p>
<p>(2).Try not to use transitive ( or) progressive words such as "and", "but", "however" in the first half of the judge reason.</p>
<p>(3).Try to keep the positive and negative shots consistent in formatting, especially in the first half.We will verify the effectiveness of our prompt paradigm in section 5.</p>
<p>Combine Zero-shot with Few-shot.This approach is to compensate for the model's omission of key information.As Figure 1 shows, the model will make two completely independent judgments, one is zero-shot judge and another one is few-shot judge.Then we will connect system Prompt, Shots, answer outputted by zero-shot, answer outputted by few-shot to get a new context and use this context to judge again.Then we will get the final result.The prompt template can be viewed in Fig- ure 4. Ablation experiment results of this approach is shown in Section 4.2.</p>
<p>Ablation Experiment</p>
<p>We verify the effectiveness of the approaches mentioned above.Note that we use different variants of GPT-4 (gpt-4-0613, gpt-4-32k-0613 and gpt-4-0125-preview) to suit different contexts length.The baseline of these experiments is called Standard Prompt Paradigm, which uses engineering approach to adjust and iterate the shots and applies criteria division and our prompt paradigm.How-ever, Standard Prompt Paradigm does not combine zero-shot with few-shot, it only uses a conventional ICL approach.</p>
<p>During a series of experiments, we find that "Incorrect Answer/Unrelated Matching Results" label in Knowledge QA task is very special.A uniformly formatted few-shot would instead negatively affect the label, which distinguishes itself from the others.We guess it is because the errors in the answer of the Knowledge QA task may be evenly distributed throughout the answer.Shots is useless in this scenario because it is difficult to help localize the error information.Furthermore, uniformly formatted shots will further limit model's ability to find error information.</p>
<p>Criteria Division</p>
<p>Our evaluation methodology employs large language models, with a constraint on their maximum context length.Generally, this approach suffices for most tasks.However, in instances where lengthy prompts and responses are required, such as in the context of few-shot article generation, a single instance can extend to 1500-2000 tokens, thereby posing a limitation on the context length.Furthermore, when evaluations involve intricate tasks with multiple dimensions, the accuracy of the assessment may be negatively affected.</p>
<p>To address these challenges, we strategically decompose the customized evaluation criteria into distinct components, primarily by segmenting the evaluation dimensions.This approach enables the model to concentrate more on each criterion, thereby streamlining the evaluation process and enhancing the outcomes.By associating each prob-lem label with its corresponding few-shot examples and inputting them into the model, we bypass the need for a single evaluation of all labels.Additionally, this method effectively alleviates the problem of insufficient context.</p>
<p>We have attempted to compare the following two experimental setups:</p>
<p>Standard Prompt Paradigm(Division).Individual evaluation dimensions are fragmented into distinct criteria, which are separately fed into the Judge model.The model's output is aggregated multiple times for each criterion before calculating the overall score.</p>
<p>Non-division.The complete set of evaluation criteria, along with their associated shots, is simultaneously fed into the model, enabling it to produce the final score directly without sequential processing.</p>
<p>Table 2 shows the results.In the tasks of Sentiment Analysis, Search QA, and Title Generation, the evaluation results of the criteria division method significantly outperform those of nondivision.Conversely, in the task of knowledge QA, the situation is reversed.We found that the primary cause of this discrepancy lies in the "Incorrect Answer/Unrelated Matching Results" label as shown in Table 3.As previously mentioned, this is because when criteria division is applied, the prompt format is much more distinct than when not divided, making the model more susceptible to format imitation, thereby overlooking practical information.</p>
<p>Combine Zero-shot with Few-shot</p>
<p>We said above that the imitation to text format in shots may make the judge model ignore some key information.So we try injecting zero-shot judge to avoid the impact of shots.The process can be viewed in Figure 1 and the prompt can be viewed in Figure 4.</p>
<p>We compare two experimental setups to verify the effectiveness of this approach:</p>
<p>Standard Prompt Paradigm(Single-turn wo Zero-shot).Use the shots obtained from our engineering approach and inject the shots into context to judge.This approach only judges one case in a single-turn.</p>
<p>Multi-turn with Zero-shot.As shown in Figure 1, the model will make two completely independent judgments, one is zero-shot judge and another one is few-shot judge.Then we will connect system Prompt, Shots, answer outputted by zero-shot, answer outputted by few-shot to get a new context and use this context to judge again.Then we will get the final result.</p>
<p>Table 4 shows comparative results of the two approach.As you can see in Table 4, multi-turn with zeroshot approach can improve scores on all task.Es-pecially in "Incorrect Answer/Unrelated Matching Results" label of Knowledge QA, Table 5 shows greatly improvement in this label.</p>
<p>SFT vs ICL</p>
<p>Previous automated evaluation methods have opted to incorporate knowledge through Supervised Fine-Tuning.However, state-of-the-art models such as GPT-4, due to their proprietary nature, cannot be subjected to SFT to further enhance their performance.Therefore, methods utilizing SFT are compelled to resort to relatively weaker open-source models as a foundation, which may potentially impact the evaluation results.Consequently, we have chosen to introduce knowledge using In-Context Learning.</p>
<p>We validate the differences in knowledge introduction using SFT and ICL methods respectively, by fine-tuning Qwen-72B-Chat model (Bai et al. (2023)) and comparing the effects.We have established three experimental setups using three different models:</p>
<p>Standard Prompt Paradigm(GPT4 + ICL).</p>
<p>Using GPT-4 as the evaluation model with incontext learning.</p>
<p>Qwen-72B-Chat + ICL.Using Qwen-72B-Chat as the evaluation model with in-context learning.</p>
<p>Qwen-72B-Chat + SFT.Fine-tuning Qwen-72B-Chat as the evaluation model.We construct a dataset composed of 179 manually annotated highquality data specific to the aforementioned four tasks, 100 open-source evaluation data obtained from the TigerScore dataset, and 300 open-source general data.We have trained Qwen-72B-Chat for one epoch on this dataset.</p>
<p>The results are shown in Table 6.It can be observed that GPT-4 with in-context learning outperforms the method based on Qwen-72B-Chat across all tasks.Meanwhile, Qwen-72B-Chat integrated with in-context learning exhibits comparable performance to Qwen-72B-Chat combined with SFT on two tasks, surpasses the latter on one task, and underperforms on the other task.This suggests that in-context learning can achieve results similar to SFT on LLM, and to a certain extent, can serve as a substitute for SFT.Furthermore, the in-context learning approach can be applied to state-of-theart proprietary LLM with superior performance, thereby yielding enhanced results.11, Table 13 and Table 14.</p>
<p>5 Prompt Engineering</p>
<p>Repeat Descriptions of Evaluation Criteria</p>
<p>In the system prompt, we explicitly offer descriptions of the evaluation criteria to help model better understand the criteria.However, we noticed that the model occasionally failed to recall the previously provided descriptions because of very long context caused by too many shots.For instance, the requirements for some generation tasks include word count specifications.However, due to the token-based tokenizer structure of LLLMs, they can't accurately count the number of words.Consequently, we employed a rule-based approach to judge this aspect, ensuring more precise assessments without relying on the model's limitations.To clarify this, we clarified in the prompt that the model should disregard word count requirements.Surprisingly, the model continued to consider it in its evaluations.</p>
<p>To mitigate this issue, we introduce a novel approach where the model is prompted to recurrently summarize and reiterate the interpretation from the system prompt before delivering its evaluation, as depicted in Figure 2.</p>
<p>To validate the efficacy of this method, we executed two experiments employing distinct strategies:</p>
<p>Standard Prompt Paradigm(Repeat descriptions).We required the model to reiterate the descriptions prior to evaluation, maintaining a consistent format for the shots.</p>
<p>Non-repetition.We adopted a more informal format for the shot composition, eliminating the need for the model to reiterate the descriptions.Table 7 shows the results.The methodology of repeating descriptions slightly outperforms the non-repetitive approach in three tasks, and significantly surpasses the latter in the remaining task.Experimental results indicate that repeating the descriptions of evaluation criteria prior to each label evaluation can assist LLM in better understanding the requirement of the evaluation, thereby enhancing accuracy.</p>
<p>Standardize the Format of Examples</p>
<p>It is universally recognized that large models possess Chain of Thought (CoT) capabilities.The prudent use of CoT enables the model to provide a detailed explanation before delivering the final answer, thereby substantially improving the accuracy of responses.However, in our approach, the incorporation of 'shots' has instigated a problem.</p>
<p>As models mimic the structure of the shots in their output, an informal arrangement of shots could potentially cause the model to prematurely conclude the answer without a comprehensive explanation.</p>
<p>In the previously mentioned example, it seems that the model's Chain of Thought(CoT) capability is activated by initially offering an explanation.However, the model actually determines the output label at the outset due to the varied formats employed in the construction of positive and negative examples, particularly the inclusion of adversative phrases preceding the negative examples.The explanation is subsequently appended following the decision on the label, leading to an inversion of cause and effect.</p>
<p>Hence, we propose that in the realm of prompt engineering, the utilization of a consistent format for both positive and negative examples is crucial.This should be accompanied by a reduction in the use of adversative expressions.The objective is to postpone the revelation of the answer, thereby enabling the model to provide a comprehensive explanation prior to presenting the ultimate response at the conclusion.We list Spearman correlation of 3 typical method and our method here: GPTScore (0.1888), Tiger-Score (0.3373), MT-Bench (0.6/0.85) and TALEC (0.8962/0.875).It is difficult to make a completely fair side-by-side comparison with other methods due to the differences in the score system, evaluation question types, and evaluation criteria.However, the high alignment of TALEC compared to other methods can also indirectly indicate its validity and usability.</p>
<p>Comparison with Human Annotation</p>
<p>Compared to automated evaluation, human assessment possesses a greater capacity to encompass the intricate and adaptable evaluation criteria and scales inherent in our business operations.Nevertheless, the financial implications of employing human evaluators are substantial, with the primary expenditure being personnel training costs.This is particularly true for specialized domains where the evaluator requirements are notably stringent.Furthermore, the efficiency of human assessment significantly lags behind that of automated evaluation, thereby considerably impeding the iterative development of large language models.Additionally, human annotation, while useful, is not always dependable.In the initial phases of our experiment, we utilized manual evaluation to gather enough data for the development of a more effective assessment system.This process involved a dual-review system, blind reviews, comprehensive quality checks for contentious cases, random spot checks for non-contentious cases, and a concluding round of spot checks.Only through the application of these multiple strategies and checks were we able to ensure the data's accuracy.However, an analysis of the manual annotation results prior to inspection revealed a significant lack of alignment among different annotators in the absence of rigorous quality control, as demonstrated in Table 9.This discrepancy can be partially attributed to the complexity of the custom business evaluation criteria.Despite these challenges, automated evaluation has proven to be highly effective in such a demanding context.</p>
<p>Conclusion</p>
<p>We propose a method: TALEC, which allows users to flexibly set their own evaluation criteria, and uses in-context learning (ICL) to teach judge model these in-house criteria.We try many approach to improve the judge abilities of model, such as criteria division and combining zero-shot with few-shot.We also come up with an engineering approach to adjust and iterate shots.which splits the dataset and simulates the practice-quality inspection cycle process.In addition, we find that when injecting manually-written shots into the context of a model, the model will not only try to understand the shots but also imitate the writing style of shots.This imitation may make the model ignore some key information and drop its CoT ability.We then compare fine-tuning with ICL, finding that fine-tuning can be replaced by ICL.In the end, we compare TALEC with other methods and humans, verifying the availability of TALEC.</p>
<p>Limitations</p>
<p>Although TALEC outperforms than many other methods, it still makes some really stupid mistakes sometimes.And TALEC relies heavily on manual annotation in the early stage, making it hard to start.Some other methods also focus on special abilities like hallucination and contextual memory, but TALEC can't evaluate these abilities so far.</p>
<p>Figure 3: The prompt format for evaluation tasks is depicted in the following sequence: the system-generated prompt, containing explicit task requirements and evaluation criteria explanations, aids the model in comprehending the assessment's scope and complexity.This is succeeded by the few-shot section, which presents a variety of examples in the same format, including both positive and negative instances, enabling the model to adopt the sample structure for its output.Finally, the output produced by LLM, is the subject of evaluation.</p>
<p>Figure 2 :
2
Figure 2: This figure illustrates an instance where the model fails to consider the descriptions within the System Prompt, juxtaposed with a contrasting example where the model redundantly repeats the descriptions before proceeding to evaluation.</p>
<p>Figure 4 :
4
Figure 4: This figure illustrates an instance of the prompt format for Combining Zero-shot with Few-shot.We combine system Prompt, Shots, answer output by zero-shot, answer output by few-shot to get a new context and use this con-text to judge again.Then we get the final result.</p>
<p>Table 2 :
2
Comparison between criteria division and nondivision.The details can be found in Table11 and Table 15.
MethodTaskSpearman eval testStandard Prompt Paradigm (Division)Sentiment Analysis 0.9579 0.9693 Knowledge QA 0.4945 0.5063 Search QA 0.8263 0.8487 Title Generation 0.9207 0.9006Sentiment Analysis 0.3735 0.2905Non-divisionKnowledge QA Search QA0.5398 0.4251 0.689 0.4691Title Generation0.3763 0.4296</p>
<p>Table 3 :
3
The detailed score comparison on "Incorrect Answer/Unrelated Matching Results" label in Knowledge QA task.The experimental setups are the same as in Table2.
MethodtasklabelevalAcctestevalF1/Precision/RecalltestStandard Prompt ParadigmKnowledgeIncorrect Answer /Unrelated(Division)QAMatching Results 0.8298 0.77370.52/0.3611/0.92860.2439/0.1667/0.4545KnowledgeIncorrect Answer /UnrelatedNon-divisionQAMatching Results 0.9149 0.8321 0.5714/0.5714/0.5714 0.1481/0.125/0.1818</p>
<p>Table 4 :
4
Comparison between the results of single-turn without zero-shot and multi-turn with zero-shot.The details can be found in Table11 and Table 12.
MethodTaskSpearman eval testStandard PromptSentiment Analysis 0.9579 0.9693ParadigmKnowledge QA0.4945 0.5063(Single-turnSearch QA0.8263 0.8487wo Zero-shot)Title Generation0.9207 0.9006Sentiment Analysis 0.9536 0.9485Multi-turnKnowledge QA0.8597 0.7089with Zero-shotSearch QA0.8574 0.9438Title Generation0.915 0.9206MethodtasklabelevalAcctestevalF1/Precision/RecalltestStandard Prompt ParadigmKnowledgeIncorrect Answer /Unrelated(Single-turn wo Zero-shot)QAMatching Results 0.8298 0.77370.52/0.3611/0.92860.2439/0.1667/0.4545KnowledgeIncorrect Answer /UnrelatedMulti-turn with Zero-shotQAMatching Results 0.9645 0.9051 0.8148/0.8462/0.7857 0.5185/0.4375/0.6364</p>
<p>Table 5 :
5
The detailed score comparison on Incorrect Answer/Unrelated Matching Results label in Knowledge QA task.The experimental setups are the same as in</p>
<p>Table 4 .
4</p>
<p>Table 7 :
7
Comparison of the experimental results for repeating descriptions and non-repetition.The details can be found in Table11 and Table 16.
MethodTaskSpearman eval testStandard Prompt Paradigm (Repeat descriptions)Sentiment Analysis 0.9579 0.9693 Knowledge QA 0.4945 0.5063 Search QA 0.8263 0.8487 Title Generation 0.9207 0.9006Sentiment Analysis 0.9553 0.9658Non-repetitionKnowledge QA Search QA0.4911 0.4823 0.8208 0.838Title Generation0.6618 0.6815</p>
<p>Table 8
8
shows the results.It can be observed that the method of employing a standardized format for both positive and negative instances surpasses the method of using arbitrary formats in evaluation results across all tasks.
MethodTaskSpearman eval testSentiment Analysis 0.9551 0.9515ArbitrarinessKnowledge QA Search QA0.5242 0.5459 0.7946 0.814Title Generation0.535 0.5729Sentiment Analysis 0.9553 0.9658StandardKnowledge QA0.4911 0.4823(Non-repetition)Search QA0.8208 0.838Title Generation0.6618 0.6815</p>
<p>Table 8 :
8
Comparison between the results of few-shot with arbitrary format and standard format.The details can be found in Table10 and Table 16.
6 Comparison with Other Method6.1 Comparison with Other AutomaticEvaluation Method</p>
<p>Table 9 :
9
Alignment degree of the manual annotations before quality inspection.
TaskSpearmanSentiment Analysis 0.9054Knowledge QA0.6523Search QA0.7973Title Generation0.8772
A Prompt TemplateThe prompt templates are exemplified in the Figure3and Figure4.B Experimental ResultsThe details of all experimental results are tabulated in Table10
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, arXiv:2303.08774Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint</p>
<p>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, arXiv:2108.07732Program synthesis with large language models. 2021arXiv preprint</p>
<p>Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, arXiv:2309.16609Qwen technical report. 2023arXiv preprint</p>
<p>Language models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033</p>
<p>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, arXiv:2107.03374Evaluating large language models trained on code. 2021arXiv preprint</p>
<p>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, arXiv:2110.14168Training verifiers to solve math word problems. 2021arXiv preprint</p>
<p>Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. Bert2018arXiv preprint</p>
<p>Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, Kai Chen, arXiv:2310.13650Botchat: Evaluating llms' capabilities of having multi-turn dialogues. 2023arXiv preprint</p>
<p>Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, Pengfei Liu, arXiv:2302.04166Gptscore: Evaluate as you desire. 2023arXiv preprint</p>
<p>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, arXiv:2009.03300Measuring massive multitask language understanding. 2020arXiv preprint</p>
<p>Measuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, arXiv:2103.038742021arXiv preprint</p>
<p>Yutong Hu, Quzhe Huang, Mingxu Tao, Chen Zhang, Yansong Feng, arXiv:2405.06105Can perplexity reflect large language model's ability in long text understanding?. 2024arXiv preprint</p>
<p>C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, Advances in Neural Information Processing Systems. 202436</p>
<p>Perplexity-a measure of the difficulty of speech recognition tasks. Fred Jelinek, Robert L Mercer, Lalit R Bahl, James K Baker, The Journal of the Acoustical Society of America. 62S11977</p>
<p>Tigerscore: Towards building explainable metric for all text generation tasks. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, Wenhu Chen, arXiv:2310.007522023arXiv preprint</p>
<p>Rouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. 2004</p>
<p>Stephanie Lin, Jacob Hilton, Owain Evans, arXiv:2109.07958Truthfulqa: Measuring how models mimic human falsehoods. 2021arXiv preprint</p>
<p>Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models. Yen-Ting Lin, Yun-Nung Chen, arXiv:2305.137112023arXiv preprint</p>
<p>Training language models to follow instructions with human feedback. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, Advances in neural information processing systems. 202235</p>
<p>Bleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting of the Association for Computational Linguistics. the 40th annual meeting of the Association for Computational Linguistics2002</p>
<p>David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, Samuel R Bowman, arXiv:2311.12022Gpqa: A graduate-level google-proof q&amp;a benchmark. 2023arXiv preprint</p>
<p>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.09288Llama 2: Open foundation and fine-tuned chat models. 2023arXiv preprint</p>            </div>
        </div>

    </div>
</body>
</html>
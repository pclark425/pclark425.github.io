<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3263 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3263</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3263</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-75.html">extraction-schema-75</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <p><strong>Paper ID:</strong> paper-c109f1c22a531087ce2415711658e2949432a056</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c109f1c22a531087ce2415711658e2949432a056" target="_blank">Story Shaping: Teaching Agents Human-like Behavior with Stories</a></p>
                <p><strong>Paper Venue:</strong> Artificial Intelligence and Interactive Digital Entertainment Conference</p>
                <p><strong>Paper TL;DR:</strong> This work introduces a technique, Story Shaping, in which a reinforcement learning agent infers tacit knowledge from an exemplar story of how to accomplish a task and intrinsically rewards itself for performing actions that make its current environment adhere to that of the inferred story world.</p>
                <p><strong>Paper Abstract:</strong> Reward design for reinforcement learning agents can be difficult in situations where one not only wants the agent to achieve some effect in the world but where one also cares about how that effect is achieved.
For example, we might wish for an agent to adhere to a tacit understanding of commonsense, align itself to a preference for how to behave for purposes of safety, or taking on a particular role in an interactive game.
Storytelling is a mode for communicating tacit procedural knowledge.
We introduce a technique, Story Shaping, in which a reinforcement learning agent infers tacit knowledge from an exemplar story of how to accomplish a task and intrinsically rewards itself for performing actions that make its current environment adhere to that of the inferred story world.
Specifically, Story Shaping infers a knowledge graph representation of the world state from observations, and also infers a knowledge graph from the exemplar story.
An intrinsic reward is generated based on the similarity between the agent's inferred world state graph and the inferred story world graph.
We conducted experiments in text-based games requiring commonsense reasoning and shaping the behaviors of agents as virtual game characters.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3263.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3263.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT-S</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT with Story Shaping (Q*BERT-S)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Q*BERT (a state-of-the-art text-game RL agent) augmented with Story Shaping: it augments the agent's persistent knowledge-graph memory with an additional Story KG derived from an exemplar story and provides intrinsic rewards for matching story-derived triples.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT-S (Q*BERT + Story Shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Based on Q*BERT (an RL agent for text games). World KG is built with an ALBERT-QA model fine-tuned on JerichoQA; actions are generated by a two-stage GRU decoder using verb-template action templates; graphs encoded with Graph Attention Networks (GATs). Story Shaping adds a Story KG extracted from exemplar stories via a VerbAtlas-based SRL pipeline and uses intrinsic rewards derived from KG similarity and exploration.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>9:05 (Jericho), Shopping and See Doctor (TextWorld), LIGHT (role-playing experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Navigate text-adventure environments to achieve specified goals (e.g., 'go to work', 'buy clothes', 'get medicine', or persona-aligned quests) while expressing commonsense/social-norm behavior or role-aligned behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external persistent knowledge-graph memory (World KG) plus an external exemplar memory (Story KG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>World KG: ALBERT-QA (fine-tuned on JerichoQA) answers slot-style questions (e.g., 'What am I carrying?') to produce candidate vertices V_t and relations R_t; G_t updated from G_{t-1} ∪ new triples. Story KG: semantic-role-labeling (BERT-based SRL mapped to VerbAtlas frames) converts exemplar stories into RDF triples ⟨subject, relation(VerbAtlas frame), object⟩. Graphs are encoded with GATs; graph representation interacts via attention with GRU-encoded text observation. Action decoder is graph-masked so only entities present in World KG are selectable.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Table 2 (Story-shaped agent Q*BERT-S): 9:05 win rate 100%, avg steps 16.30, avg commonsense (CS) score 3.90 (max 4), avg game score 5.00; Shopping win rate 100%, avg steps 12.35, avg CS 3.70 (max 4), avg game score 5.00; See Doctor win rate 95%, avg steps 19.15, avg CS 6.70 (max 8), avg game score 4.75.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td>Baseline Q*BERT (no Story Shaping, but still uses World KG): 9:05 win rate 100%, avg steps 7.25, avg CS score 0.40, avg game score 5.00; Shopping win rate 100%, avg steps 6.30, avg CS 0.90, avg game score 5.00; See Doctor win rate 95%, avg steps 14.30, avg CS 0.70, avg game score 4.75.</td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Adding Story KG + intrinsic KG reward yields much higher expression of commonsense/social-norm actions (large increase in 'CS score') and enables role/persona-aligned behavior while preserving win rate; agents take longer (more steps) and pursue optional story-consistent behaviors, producing human-recognizable trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Story KG intentionally omits event ordering (order-agnostic), so agents must learn ordering via trial-and-error; exemplar stories may include entities or actions not present/executable in the environment (mismatch), but the system is robust and ignores unattainable triples; Story Shaping increases trajectory length (not the shortest path) which some humans find less immediately comprehensible.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use a persistent knowledge-graph as long-term memory for partially-observable text games; construct World KG with a QA model fine-tuned on in-domain data (e.g., ALBERT-QA on JerichoQA); construct Story KG via SRL (VerbAtlas) to extract action frames and entities; treat Story KG as an order-agnostic set of target triples and reward the agent when World KG discovers matching triples; include an exploration intrinsic reward based on global-graph expansion (G_global) to encourage coverage; do not assume stories are executable or complete—allow trial-and-error to fill gaps; scale intrinsic rewards (α, β) to balance game score and shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_extra</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Story Shaping: Teaching Agents Human-like Behavior with Stories', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3263.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3263.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Q*BERT (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Q*BERT (baseline agent without Story Shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL agent for text games using a persistent World Knowledge Graph built by ALBERT-QA and a graph-constrained action decoder; serves as the baseline in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Q*BERT</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>State-of-the-art text-game RL agent that builds a World KG via ALBERT-QA (JerichoQA fine-tuning) and uses a graph-masked two-stage GRU decoder with template-based actions; encodes graphs with GATs and uses intrinsic exploration bonuses in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>9:05 (Jericho), Shopping and See Doctor (TextWorld)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same game tasks as Q*BERT-S: navigation and goal achievement in text-adventure environments; baseline optimizes game score and exploration without story-derived shaping.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external persistent knowledge-graph memory (World KG)</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>World KG built by ALBERT-QA answering environment-specific questions; KG used as persistent state memory and to constrain valid objects in the action decoder (graph mask). Graphs encoded with GATs and combined with observation encodings for policy/value networks.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>Baseline results (Table 2): 9:05 win rate 100%, avg steps 7.25, avg CS score 0.40, avg game score 5.00; Shopping win rate 100%, avg steps 6.30, avg CS 0.90, avg game score 5.00; See Doctor win rate 95%, avg steps 14.30, avg CS 0.70, avg game score 4.75.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>World KG provides long-term memory enabling handling of partial observability and supports high win rates; results show the baseline completes tasks efficiently in fewer steps (shorter trajectories).</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Baseline agents without Story Shaping show low expression of commonsense/social behaviors (very low CS scores) and cannot be guided to adopt persona-like behavior via story signals; relying solely on game reward leads to shortest-path behavior that can ignore optional human-like actions.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Use an in-domain QA model (ALBERT-QA fine-tuned on JerichoQA) to populate World KG for robust state tracking; couple KG with graph-masked decoders to limit action space. To shape behavior toward human-like preferences, add story-derived intrinsic rewards.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_extra</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Story Shaping: Teaching Agents Human-like Behavior with Stories', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3263.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3263.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-A2C + Story</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>KG-A2C with Story Shaping (KG-A2C + Story Shaping)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>KG-A2C (graph-based A2C RL agent) enhanced with Story Shaping to use exemplar Story KGs to shape persona-specific behavior in the LIGHT environment.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Graph constrained reinforcement learning for natural language action spaces</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-A2C + Story Shaping</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KG-A2C uses OpenIE to construct World KG for LIGHT experiments; Story Shaping adds a Story KG from exemplar quests and the same intrinsic reward machinery (KG-match reward and exploration reward); policy/value trained with Advantage Actor Critic (A2C) with graph-constrained decoders.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>LIGHT (fantasy text-adventure sandbox for persona experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Role-playing tasks: given a persona (thief, adventurer, bum, thug), act to collect persona-consistent items and reach goal location; evaluate whether behavior is identifiable as the provided persona.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external persistent knowledge-graph memory (World KG) + exemplar Story KG</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>World KG constructed with OpenIE (for LIGHT experiments) or QA-based extraction; Story KG extracted via SRL→VerbAtlas mapping; both graphs stored persistently; action decoder constrained by graph mask; GAT encodes graphs; intrinsic KG-match reward given when newly discovered World KG triples match Story KG triples; exploration reward r_t^e = Δ(|G_global - G_t|) encourages discovering new edges.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>KG-A2C agents with Story Shaping achieved 100% win rate across persona experiments; human evaluation showed participants identified persona-aligned agents at rates: Thief 58.06% (significant), Adventurer 62.50% (significant), Thug 72.73% (highly significant), Bum 64.71% (significant) in pairwise judgments.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Story Shaping + KG memory yields strong persona-aligned behavior that humans reliably recognize while preserving task success (100% win rate); Story KG steers agents toward persona-consistent optional actions and item collection.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>ChatGPT could not generate certain persona stories (e.g., violent 'Thug') due to policy/ethical constraints; Story KG may include unattainable elements relative to environment but agents handle mismatches via trial-and-error; paper does not report degradation in task completion but does not report ablation where World KG is removed entirely.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>For role/persona shaping in large action spaces (e.g., LIGHT), use a Story KG to bias exploration toward persona-consistent triples; constrain decoders with graph masks to keep actions grounded in discovered entities; use human-evaluations to verify perceived persona alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>name_short_extra</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Story Shaping: Teaching Agents Human-like Behavior with Stories', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3263.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3263.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of LLM agents playing text games, with a focus on how memory is used, what types of memory are implemented, and how memory affects performance on text game tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-Memory (World & Story KGs)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-Graph Memory (World KG and Story KG)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Dual knowledge-graph memory system: World KG is a persistent memory of observed environment facts; Story KG is an exemplar, external KG representing desired/expected entities and actions extracted from a natural-language story. Intrinsic rewards drive alignment between them.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>World KG / Story KG (memory mechanism)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>World KG: persistent graph of ⟨subject, relation, object⟩ triples inferred from game observations (via ALBERT-QA or OpenIE). Story KG: triples extracted from exemplar stories using SRL mapped to VerbAtlas frames. Both KGs are encoded by GATs and used for state representation, action masking, and intrinsic reward computation.</td>
                        </tr>
                        <tr>
                            <td><strong>game_or_benchmark_name</strong></td>
                            <td>Used across Jericho, TextWorld, and LIGHT experiments in this paper</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Provide long-term memory to handle partial observability, guide action selection, and serve as basis for intrinsic rewards that encourage human-like or persona-aligned behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_memory</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_type</strong></td>
                            <td>external structured memory (symbolic/graph memory), persistent across episodes (within run) with a global-graph summary for exploration</td>
                        </tr>
                        <tr>
                            <td><strong>memory_implementation_details</strong></td>
                            <td>World KG updated each step by combining previous G_{t-1} with new candidate vertices V_t and relations R_t produced by an ALBERT-QA or OpenIE extraction pipeline; Story KG built offline from exemplar story text via SRL→VerbAtlas→RDF triples. New edges discovered in G_t are checked against Story KG; matches produce intrinsic KG reward r_t^s = n × ρ (n = number of matching triples). Exploration intrinsic reward r_t^e = Δ(|G_global - G_t|) where G_global = ∪_{i=1}^{t-1} G_i. Graphs are encoded with Graph Attention Networks and combined with GRU-encoded observations.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_memory</strong></td>
                            <td>When used as described, agents preserve or improve task-level success (win rates unchanged or high) and substantially increase measures of human-like/commonsense behavior (CS scores and human preference metrics reported above).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_memory</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_performance_comparison</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>memory_benefits</strong></td>
                            <td>Provides persistent state across partial observability, enables intrinsic-reward shaping (alignment to stories), supports graph-masked action decoding to reduce action space and keep actions grounded, and improves human-recognizable behavior without harming success rates.</td>
                        </tr>
                        <tr>
                            <td><strong>memory_limitations_or_failures</strong></td>
                            <td>Does not encode event ordering (order-agnostic Story KG) which can hide sequencing constraints; relies on extraction quality (SRL/QA/OpenIE errors propagate to KG); possible mismatch between Story KG and environment entities/actions when stories are generated externally, though agent robustness mitigates this.</td>
                        </tr>
                        <tr>
                            <td><strong>best_practices_or_recommendations</strong></td>
                            <td>Keep Story KG as an order-agnostic target set of triples; use an in-domain QA extractor for World KG and an SRL-based pipeline for Story KG; reward only newly discovered triples to avoid repeated counting; maintain a G_global to compute intrinsic exploration reward; use graph masking to keep decoders grounded in discovered entities.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Story Shaping: Teaching Agents Human-like Behavior with Stories', 'publication_date_yy_mm': '2023-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning <em>(Rating: 2)</em></li>
                <li>How to avoid being eaten by a grue: Structured exploration strategies for textual worlds <em>(Rating: 2)</em></li>
                <li>Graph constrained reinforcement learning for natural language action spaces <em>(Rating: 2)</em></li>
                <li>TextWorld: A learning environment for text-based games <em>(Rating: 2)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game <em>(Rating: 1)</em></li>
                <li>Interactive fiction games: A colossal adventure <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3263",
    "paper_id": "paper-c109f1c22a531087ce2415711658e2949432a056",
    "extraction_schema_id": "extraction-schema-75",
    "extracted_data": [
        {
            "name_short": "Q*BERT-S",
            "name_full": "Q*BERT with Story Shaping (Q*BERT-S)",
            "brief_description": "Q*BERT (a state-of-the-art text-game RL agent) augmented with Story Shaping: it augments the agent's persistent knowledge-graph memory with an additional Story KG derived from an exemplar story and provides intrinsic rewards for matching story-derived triples.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Q*BERT-S (Q*BERT + Story Shaping)",
            "agent_description": "Based on Q*BERT (an RL agent for text games). World KG is built with an ALBERT-QA model fine-tuned on JerichoQA; actions are generated by a two-stage GRU decoder using verb-template action templates; graphs encoded with Graph Attention Networks (GATs). Story Shaping adds a Story KG extracted from exemplar stories via a VerbAtlas-based SRL pipeline and uses intrinsic rewards derived from KG similarity and exploration.",
            "game_or_benchmark_name": "9:05 (Jericho), Shopping and See Doctor (TextWorld), LIGHT (role-playing experiments)",
            "task_description": "Navigate text-adventure environments to achieve specified goals (e.g., 'go to work', 'buy clothes', 'get medicine', or persona-aligned quests) while expressing commonsense/social-norm behavior or role-aligned behavior.",
            "uses_memory": true,
            "memory_type": "external persistent knowledge-graph memory (World KG) plus an external exemplar memory (Story KG)",
            "memory_implementation_details": "World KG: ALBERT-QA (fine-tuned on JerichoQA) answers slot-style questions (e.g., 'What am I carrying?') to produce candidate vertices V_t and relations R_t; G_t updated from G_{t-1} ∪ new triples. Story KG: semantic-role-labeling (BERT-based SRL mapped to VerbAtlas frames) converts exemplar stories into RDF triples ⟨subject, relation(VerbAtlas frame), object⟩. Graphs are encoded with GATs; graph representation interacts via attention with GRU-encoded text observation. Action decoder is graph-masked so only entities present in World KG are selectable.",
            "performance_with_memory": "Table 2 (Story-shaped agent Q*BERT-S): 9:05 win rate 100%, avg steps 16.30, avg commonsense (CS) score 3.90 (max 4), avg game score 5.00; Shopping win rate 100%, avg steps 12.35, avg CS 3.70 (max 4), avg game score 5.00; See Doctor win rate 95%, avg steps 19.15, avg CS 6.70 (max 8), avg game score 4.75.",
            "performance_without_memory": "Baseline Q*BERT (no Story Shaping, but still uses World KG): 9:05 win rate 100%, avg steps 7.25, avg CS score 0.40, avg game score 5.00; Shopping win rate 100%, avg steps 6.30, avg CS 0.90, avg game score 5.00; See Doctor win rate 95%, avg steps 14.30, avg CS 0.70, avg game score 4.75.",
            "has_performance_comparison": true,
            "memory_benefits": "Adding Story KG + intrinsic KG reward yields much higher expression of commonsense/social-norm actions (large increase in 'CS score') and enables role/persona-aligned behavior while preserving win rate; agents take longer (more steps) and pursue optional story-consistent behaviors, producing human-recognizable trajectories.",
            "memory_limitations_or_failures": "Story KG intentionally omits event ordering (order-agnostic), so agents must learn ordering via trial-and-error; exemplar stories may include entities or actions not present/executable in the environment (mismatch), but the system is robust and ignores unattainable triples; Story Shaping increases trajectory length (not the shortest path) which some humans find less immediately comprehensible.",
            "best_practices_or_recommendations": "Use a persistent knowledge-graph as long-term memory for partially-observable text games; construct World KG with a QA model fine-tuned on in-domain data (e.g., ALBERT-QA on JerichoQA); construct Story KG via SRL (VerbAtlas) to extract action frames and entities; treat Story KG as an order-agnostic set of target triples and reward the agent when World KG discovers matching triples; include an exploration intrinsic reward based on global-graph expansion (G_global) to encourage coverage; do not assume stories are executable or complete—allow trial-and-error to fill gaps; scale intrinsic rewards (α, β) to balance game score and shaping.",
            "name_short_extra": null,
            "uuid": "e3263.0",
            "source_info": {
                "paper_title": "Story Shaping: Teaching Agents Human-like Behavior with Stories",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "Q*BERT (baseline)",
            "name_full": "Q*BERT (baseline agent without Story Shaping)",
            "brief_description": "An RL agent for text games using a persistent World Knowledge Graph built by ALBERT-QA and a graph-constrained action decoder; serves as the baseline in the paper.",
            "citation_title": "How to avoid being eaten by a grue: Structured exploration strategies for textual worlds",
            "mention_or_use": "use",
            "agent_name": "Q*BERT",
            "agent_description": "State-of-the-art text-game RL agent that builds a World KG via ALBERT-QA (JerichoQA fine-tuning) and uses a graph-masked two-stage GRU decoder with template-based actions; encodes graphs with GATs and uses intrinsic exploration bonuses in prior work.",
            "game_or_benchmark_name": "9:05 (Jericho), Shopping and See Doctor (TextWorld)",
            "task_description": "Same game tasks as Q*BERT-S: navigation and goal achievement in text-adventure environments; baseline optimizes game score and exploration without story-derived shaping.",
            "uses_memory": true,
            "memory_type": "external persistent knowledge-graph memory (World KG)",
            "memory_implementation_details": "World KG built by ALBERT-QA answering environment-specific questions; KG used as persistent state memory and to constrain valid objects in the action decoder (graph mask). Graphs encoded with GATs and combined with observation encodings for policy/value networks.",
            "performance_with_memory": "Baseline results (Table 2): 9:05 win rate 100%, avg steps 7.25, avg CS score 0.40, avg game score 5.00; Shopping win rate 100%, avg steps 6.30, avg CS 0.90, avg game score 5.00; See Doctor win rate 95%, avg steps 14.30, avg CS 0.70, avg game score 4.75.",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "World KG provides long-term memory enabling handling of partial observability and supports high win rates; results show the baseline completes tasks efficiently in fewer steps (shorter trajectories).",
            "memory_limitations_or_failures": "Baseline agents without Story Shaping show low expression of commonsense/social behaviors (very low CS scores) and cannot be guided to adopt persona-like behavior via story signals; relying solely on game reward leads to shortest-path behavior that can ignore optional human-like actions.",
            "best_practices_or_recommendations": "Use an in-domain QA model (ALBERT-QA fine-tuned on JerichoQA) to populate World KG for robust state tracking; couple KG with graph-masked decoders to limit action space. To shape behavior toward human-like preferences, add story-derived intrinsic rewards.",
            "name_short_extra": null,
            "uuid": "e3263.1",
            "source_info": {
                "paper_title": "Story Shaping: Teaching Agents Human-like Behavior with Stories",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "KG-A2C + Story",
            "name_full": "KG-A2C with Story Shaping (KG-A2C + Story Shaping)",
            "brief_description": "KG-A2C (graph-based A2C RL agent) enhanced with Story Shaping to use exemplar Story KGs to shape persona-specific behavior in the LIGHT environment.",
            "citation_title": "Graph constrained reinforcement learning for natural language action spaces",
            "mention_or_use": "use",
            "agent_name": "KG-A2C + Story Shaping",
            "agent_description": "KG-A2C uses OpenIE to construct World KG for LIGHT experiments; Story Shaping adds a Story KG from exemplar quests and the same intrinsic reward machinery (KG-match reward and exploration reward); policy/value trained with Advantage Actor Critic (A2C) with graph-constrained decoders.",
            "game_or_benchmark_name": "LIGHT (fantasy text-adventure sandbox for persona experiments)",
            "task_description": "Role-playing tasks: given a persona (thief, adventurer, bum, thug), act to collect persona-consistent items and reach goal location; evaluate whether behavior is identifiable as the provided persona.",
            "uses_memory": true,
            "memory_type": "external persistent knowledge-graph memory (World KG) + exemplar Story KG",
            "memory_implementation_details": "World KG constructed with OpenIE (for LIGHT experiments) or QA-based extraction; Story KG extracted via SRL→VerbAtlas mapping; both graphs stored persistently; action decoder constrained by graph mask; GAT encodes graphs; intrinsic KG-match reward given when newly discovered World KG triples match Story KG triples; exploration reward r_t^e = Δ(|G_global - G_t|) encourages discovering new edges.",
            "performance_with_memory": "KG-A2C agents with Story Shaping achieved 100% win rate across persona experiments; human evaluation showed participants identified persona-aligned agents at rates: Thief 58.06% (significant), Adventurer 62.50% (significant), Thug 72.73% (highly significant), Bum 64.71% (significant) in pairwise judgments.",
            "performance_without_memory": null,
            "has_performance_comparison": false,
            "memory_benefits": "Story Shaping + KG memory yields strong persona-aligned behavior that humans reliably recognize while preserving task success (100% win rate); Story KG steers agents toward persona-consistent optional actions and item collection.",
            "memory_limitations_or_failures": "ChatGPT could not generate certain persona stories (e.g., violent 'Thug') due to policy/ethical constraints; Story KG may include unattainable elements relative to environment but agents handle mismatches via trial-and-error; paper does not report degradation in task completion but does not report ablation where World KG is removed entirely.",
            "best_practices_or_recommendations": "For role/persona shaping in large action spaces (e.g., LIGHT), use a Story KG to bias exploration toward persona-consistent triples; constrain decoders with graph masks to keep actions grounded in discovered entities; use human-evaluations to verify perceived persona alignment.",
            "name_short_extra": null,
            "uuid": "e3263.2",
            "source_info": {
                "paper_title": "Story Shaping: Teaching Agents Human-like Behavior with Stories",
                "publication_date_yy_mm": "2023-01"
            }
        },
        {
            "name_short": "KG-Memory (World & Story KGs)",
            "name_full": "Knowledge-Graph Memory (World KG and Story KG)",
            "brief_description": "Dual knowledge-graph memory system: World KG is a persistent memory of observed environment facts; Story KG is an exemplar, external KG representing desired/expected entities and actions extracted from a natural-language story. Intrinsic rewards drive alignment between them.",
            "citation_title": "",
            "mention_or_use": "use",
            "agent_name": "World KG / Story KG (memory mechanism)",
            "agent_description": "World KG: persistent graph of ⟨subject, relation, object⟩ triples inferred from game observations (via ALBERT-QA or OpenIE). Story KG: triples extracted from exemplar stories using SRL mapped to VerbAtlas frames. Both KGs are encoded by GATs and used for state representation, action masking, and intrinsic reward computation.",
            "game_or_benchmark_name": "Used across Jericho, TextWorld, and LIGHT experiments in this paper",
            "task_description": "Provide long-term memory to handle partial observability, guide action selection, and serve as basis for intrinsic rewards that encourage human-like or persona-aligned behavior.",
            "uses_memory": true,
            "memory_type": "external structured memory (symbolic/graph memory), persistent across episodes (within run) with a global-graph summary for exploration",
            "memory_implementation_details": "World KG updated each step by combining previous G_{t-1} with new candidate vertices V_t and relations R_t produced by an ALBERT-QA or OpenIE extraction pipeline; Story KG built offline from exemplar story text via SRL→VerbAtlas→RDF triples. New edges discovered in G_t are checked against Story KG; matches produce intrinsic KG reward r_t^s = n × ρ (n = number of matching triples). Exploration intrinsic reward r_t^e = Δ(|G_global - G_t|) where G_global = ∪_{i=1}^{t-1} G_i. Graphs are encoded with Graph Attention Networks and combined with GRU-encoded observations.",
            "performance_with_memory": "When used as described, agents preserve or improve task-level success (win rates unchanged or high) and substantially increase measures of human-like/commonsense behavior (CS scores and human preference metrics reported above).",
            "performance_without_memory": null,
            "has_performance_comparison": true,
            "memory_benefits": "Provides persistent state across partial observability, enables intrinsic-reward shaping (alignment to stories), supports graph-masked action decoding to reduce action space and keep actions grounded, and improves human-recognizable behavior without harming success rates.",
            "memory_limitations_or_failures": "Does not encode event ordering (order-agnostic Story KG) which can hide sequencing constraints; relies on extraction quality (SRL/QA/OpenIE errors propagate to KG); possible mismatch between Story KG and environment entities/actions when stories are generated externally, though agent robustness mitigates this.",
            "best_practices_or_recommendations": "Keep Story KG as an order-agnostic target set of triples; use an in-domain QA extractor for World KG and an SRL-based pipeline for Story KG; reward only newly discovered triples to avoid repeated counting; maintain a G_global to compute intrinsic exploration reward; use graph masking to keep decoders grounded in discovered entities.",
            "uuid": "e3263.3",
            "source_info": {
                "paper_title": "Story Shaping: Teaching Agents Human-like Behavior with Stories",
                "publication_date_yy_mm": "2023-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning",
            "rating": 2
        },
        {
            "paper_title": "How to avoid being eaten by a grue: Structured exploration strategies for textual worlds",
            "rating": 2
        },
        {
            "paper_title": "Graph constrained reinforcement learning for natural language action spaces",
            "rating": 2
        },
        {
            "paper_title": "TextWorld: A learning environment for text-based games",
            "rating": 2
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game",
            "rating": 1
        },
        {
            "paper_title": "Interactive fiction games: A colossal adventure",
            "rating": 1
        }
    ],
    "cost": 0.0172575,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Story Shaping: Teaching Agents Human-like Behavior with Stories</h1>
<p>Xiangyu Peng , Christopher Cui ${ }^{*}$, Wei Zhou, Renee Jia, Mark Riedl<br>Georgia Institute of Technology<br>{xpeng62, ccui46, wzhou322, rjia35} @gatech.edu, riedl@cc.gatech.edu</p>
<h4>Abstract</h4>
<p>Reward design for reinforcement learning agents can be difficult in situations where one not only wants the agent to achieve some effect in the world but where one also cares about how that effect is achieved. For example, we might wish for an agent to adhere to a tacit understanding of commonsense, align itself to a preference for how to behave for purposes of safety, or taking on a particular role in an interactive game. Storytelling is a mode for communicating tacit procedural knowledge. We introduce a technique, Story Shaping, in which a reinforcement learning agent infers tacit knowledge from an exemplar story of how to accomplish a task and intrinsically rewards itself for performing actions that make its current environment adhere to that of the inferred story world. Specifically, Story Shaping infers a knowledge graph representation of the world state from observations, and also infers a knowledge graph from the exemplar story. An intrinsic reward is generated based on the similarity between the agent's inferred world state graph and the inferred story world graph. We conducted experiments in text-based games requiring commonsense reasoning and shaping the behaviors of agents as virtual game characters.</p>
<h2>1 Introduction</h2>
<p>Reinforcement Learning (RL) is a class of techniques whereby an agent learns how to carry out a sequential task through repeated interaction with the environment. The agent is given a reward for achieving certain states in the environment, executing certain actions, or causing the world to transition between particular pairs of states. RL is thus a process of learning to maximize expected future rewards. The design of the reward signal-when the reward (or penalty) is given and how much-determines what the optimal behavior for the agent should be.</p>
<p>Reward design can be especially difficult in situations where one not only wants the agent to achieve some effect in the world but where we also care about how that ef-</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Excerpt from the text game, 9:05, with actions from vanilla reinforcement learning agent represented by , and RL agent with Story Shaping by . takes action "go west", which only maximizes the expected rewards and ignores the commonsense knowledge in the environment. . instead chooses to do something more similar to what a human would do-eat a Pop-Tart-based on a story written in natural language, which may be provided or created automatically by our system.
fect is achieved. For example, we may want the agent to carry out the task in a way that adheres to social norms during execution. This can make it safer for humans to work with AI-driven systems because they conform to our expectations and preferences [Frazier et al., 2020; Nahian et al., 2021; Ammanabrolu et al., 2022]. We may wish for our agents to demonstrate commonsense knowledge during execution [Dambekodi et al., 2020]. In computer game environments, we may wish to have AI-driven agents role-play different types of characters with different interests or ways of accomplishing things [Urbanek et al., 2019; Ammanabrolu et al., 2021].</p>
<p>We introduce a new technique, Story Shaping, for specifying preferences over an agent's behavior. In Story Shaping, a</p>
<p>reward designer specifies how to perform a task by providing an example story. Stories are efficient means for transferring tacit procedural knowledge between people. Storytelling is a mode of communication wherein details are abstracted away under the assumption that the recipient shares a common base of knowledge with which to reconstruct details as necessary for comprehension [Hedlund et al., 2002].</p>
<p>Story Shaping is a process whereby the agent reverse-engineers implicit state information from the exemplar story and constructs a rich, intrinsic reward signal that guides it toward behavior that makes its environment more closely resemble the implicit world of the story. Specifically, the agent infers a knowledge graph from the story, consisting of $\langle$ subject, relation, object $\rangle$ triples for every relation that can be inferred from the story. The agent also extracts relations from its observations to construct a knowledge graph summarizing the operating environment. The agent rewards itself for how similar its current world state representation is from the desired representation derived from the exemplar story.</p>
<p>Story shaping is related to Learning from Demonstrations (LfD). In LfD, the agent is provided with a set of traces, typically enacted by humans in the same environment. The learning agent must reconstruct the latent policy that human demonstrators were following. Demonstrations are typically assumed to be complete (no missing steps) and performed in the same-or very similar-environment that the agent also operates in. In contrast, Story Shaping does not assume the stories are complete nor executable, and does not assume they reference the same environment. Instead, stories may reference the essential objects, locations, or activities but leave out details. Additionally, because stories do not reference the exact environment the agent inhabits, objects and locations may differ or be missing, and actions may not be executable.</p>
<p>We experiment with Story Shaping in text games where observations and actions are presented entirely in text. Text games are partially observable environments that have large state and action spaces and often involve puzzles requiring long-range causal dependencies [Hausknecht et al., 2020a]. Text games have also been demonstrated to transfer to visual and real-world domains [Wang et al., 2022; Shridhar et al., 2021]. We conduct experiments across three game platforms that either (a) require commonsense, or (b) showed that our agent is able to role-play in a more human-like manner. Additionally, we show that story-shaped agents can adapt their behavior to different character preferences.</p>
<h2>2 Background and Related Work</h2>
<p>Text games are turn-based games where the player must read human-written natural language (typically English) descriptions of the local environment and respond with short textual action descriptions. A text game can be defined as a partially-observable Markov Decision Process: $\langle S, P, A, O, \Omega, R, \gamma\rangle$, representing the set of environment states, conditional transition probabilities between states, the vocabulary or words used to compose text commands, observations, observation conditional probabilities, reward function, and discount factor, respectively. The transition and observation probabilities, $P$ and $\Omega$, are typically unknown to the agent. Observations are text sequence, and actions are composed of one to five tokens from a vocabulary. The RL agent attempts to learn a policy $\pi(o) \rightarrow a$ that maximize future expected reward.</p>
<p>Knowledge Graphs for Text Games. A knowledge graph is a set of $\langle$ subject, relation, object $\rangle$ tuples. Knowledgegraph based reinforcement learning agents have been shown to be state-of-the-art in text-based games [Ammanabrolu and Riedl, 2018; Ammanabrolu et al., 2020a; Ammanabrolu and Hausknecht, 2020; Ammanabrolu et al., 2020b; Xu et al., 2020; Peng et al., 2021a]. These agents infer objects and relations from text observations and use this knowledge graph as a long-term memory of the world state as a means of handling partial observability. We build off the KG-A2C [Ammanabrolu and Hausknecht, 2020] agent architecture, which uses the ALBERT [Lan et al., 2019] language model to infer objects and relations from the text observation, and a graph attention network to generate action sequences. Whereas KG-A2C uses the knowledge graph to represent world state and filter actions, our Story Shaping approach also uses the knowledge graph to compute a dense reward signal, showing that KG-based reinforcement learning has additional untapped potential.</p>
<p>Intrinsic rewards. Intrinsic rewards provide qualitative guidance [Schmidhuber, 1991; Oudeyer et al., 2007; Barto, 2013] for exploration and push an agent to get a specific behavior without any direct feedback from the environment. These rewards can take many forms, such as a comparison between the agent's predictions and reality [Stadie et al., 2015; Burda et al., 2018; Kim et al., 2019], or the performance on self-generated goals [Vezhnevets et al., 2017; Levy et al., 2018; Nachum et al., 2018; Nair et al., 2018; Pong et al., 2019]. Ammanabrolu et al. [2020b] intrinsically rewards a text-game playing agent for adding nodes and edges to a knowledge graph. Our technique intrinsically motivates the agent to explore states that add nodes and edges that are anticipated by the exemplar story. Related, the learning from stories technique by Harrison et al. [2016] uses stories to guide RL agents. However, it requires dozens of exemplar stories and each event is treated as a goal in a modular hierarchical policy; we only require a single story and generate a single unified policy.</p>
<h2>3 Story Shaping</h2>
<p>Story Shaping facilitates a RL agent's ability to learn implicit knowledge from an exemplar story about a task and rewards itself for actions that bring the operating environment more in alignment with the inferred story world. Our technique starts with a given exemplar story (which can also be automatically generated), which the agent transforms into a knowledge graph, referred as Story $K G$ (§3.1). During RL training, as the agent explores the game world, it builds an internal state knowledge graph, called the World $K G$ (§3.2). Then the agent is updated using intrinsic rewards, calculated based on the similarity between the World KG and the Story KG (§3.3). The technique is overviewed in Figure 2.</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Knowledge graph extraction and the architecture of RL agent with Story Shaping at step t.</p>
<h4><strong>Game: 9:05</strong></h4>
<h4><strong>First observation:</strong></h4>
<p>I wake up in the morning. A bathroom lies to the south, while a door to the east leads to the living room. On the end table are a telephone, a wallet and some keys. The phone rings.</p>
<p><strong>Goal: GO TO WORK.</strong></p>
<h4><strong>Human-written story about a routine:</strong></h4>
<p>Upon waking in the morning, I start my day with a <em>Pop-Tart</em> breakfast, followed by a <em>shower</em> before commencing work.</p>
<h4><strong>Human-written story about a Persona (refined man):</strong></h4>
<p>Upon waking, I first tend to my personal hygiene by taking a <em>shower</em> and using the <em>toilet</em>. After, I change into appropriate <em>clothes</em> before having <em>breakfast</em>. I then leave my home to begin my workday.</p>
<h4><strong>Automatically generated story by ChatGPT:</strong></h4>
<p>I will likely take a <em>shower</em> in the bathroom to the south, get <em>dressed</em>, and check my wallet and <em>keys</em> to make sure I have everything I need for the day. I may also <em>take a cup of coffee</em> before leaving my home to go to work.</p>
<p>Table 1: Exemplar stories constructed by human or ChatGPT. Words <em>underlined</em> indicate actions and entities that can be taken in the game. Words with a wavy <em>underline</em> indicate actions or entities that are invalid or not allowed in the game.</p>
<h3><strong>3.1 The Exemplar Story</strong></h3>
<p>A natural language story is provided as an exemplar of the behavior the agent is to enact. Table 1 shows some possible story exemplars for "going to work in the morning". We assume that a person is providing the story as a high-level description of what the agent should attempt to achieve or to describe how a task should be achieved. The story can also be generated by an automated story generation system [Peng <em>et al.</em>, 2021b; Goldfarb-Tarrant <em>et al.</em>, 2020] or by prompting a large language model such as ChatGPT[OpenAI, 2022] to describe a typical way of doing something (More details about prompt engineering for ChatGPT can be found in Appendix C).</p>
<p>Similar to the last example in Table 1, stories can leave details about the environment out. The person or entity that provides the story, whether human or large language model, may be unaware of the exact parameters of the operating environment of the agent. For example, "coffee" is not an entity in the game "9:05", but can be present in the exemplar example. This is an intentional benefit of using story exemplars; stories can be thought of as compact descriptions that focus on salient details with the assumption that the recipient shares common knowledge with the storyteller and can thus reconstruct the more fine-grained details.</p>
<p>The purpose of the exemplar story, however, is not to be a demonstration, but to generate an intrinsic reward signal that guides the reinforcement learning agent to act in ways that align the operating environment with the implicit world depicted in the story, thereby "shaping" the agent's behavior. Before the agent begins training, the exemplar story is converted into a knowledge graph—called the <em>Story KG</em>—taking the form of RDF triples [Klyne, 2004] of 〈<em>subject</em>, <em>relation</em>, <em>object</em>〉. Story KG is an explicit and persistent memory of entities mentioned from the story. This knowledge graph contains the entities and relations directly extracted from the exemplar story's text, as well as additional world details that can be inferred from the events in the story.</p>
<p>To acquire the Story KG, we train a Semantic Role Labeling (SRL) [Gildea and Jurafsky, 2002] model on VerbAtlas [Di Fabio <em>et al.</em>, 2019] to provide the automatic identification and labeling of argument structures of the story. VerbAtlas is a linguistic resource that provides semantic annotations on sentences based on the verb and how it is</p>
<p>used. Verbs are important parts of stories because they convey action that change the state of the story world. For example,"I drink coffee in the kitchen" is firstly processed by VerbAtlas SRL to obtain "{'verbatlas': 'DRINK', 'description': '[ARG0: You] [drink.01: drink] [ARG1: coffee] [ARGM-LOC: in the kitchen]}". Then the themes and attributes are used as entities and VerbAtlas frames are used as edges, such as ${$ You, DRINK, coffee $}$. We also incorporate location and time data into the knowledge graph (i.e., $\langle$ You, in, kitchen $\rangle$ ). Training details can be found in Appendix A.1. Incorporating the VerbAtlas frame name of the verb as a relation satisfies three needs. First, it acts as a placeholder for the commonsense effects when they are not otherwise known. For example, the coffee is in the state of having been drunk. Second, it acts as a record of actions that have been taken in the world-the world is one in which those actions occurred. Third, some actions don't have positive effects that create relations, e.g., destroying something removes objects and relations because the object ceases to exist.</p>
<p>Story KG captures the positive relations at the end of the exemplar story, including the record of events as a placeholder for implicit effects. An important attribute of this approach is that it intentionally does not capture the order of events. The agent will learn through trial-and-error whether there are ordering constraints. There may be events in the story that can be carried out in different orders or must be carried out in different orders depending on environmental conditions. Events may be missing completely (e.g., the story doesn't explicitly say to move from the bedroom to the kitchen). Furthermore, some events may be impossible (e.g., there is no coffee in the kitchen). Instead, the story KG provides the key elements that the agent should encounter, and the agent receives more rewards for encountering and doing as many of them as possible.</p>
<h3>3.2 The Reinforcement Learning Agent</h3>
<p>We consider the standard reinforcement learning setting where an agent interacts with a text game environment over a number of discrete time steps. State-of-the-art approaches to RL in text environments use a knowledge graph as an external, persistent memory of the world state [Ammanabrolu and Riedl, 2018; Ammanabrolu and Hausknecht, 2020; Ammanabrolu et al., 2020b]. As the agent explores the game world, a knowledge graph-called the World KG-is constructed and used as state representation. The RL agent is trained via the Advantage Actor Critic (A2C) [Mnih et al., 2016] to maintain a policy $\pi\left(a_{t} \mid s_{t} ; \theta\right)$ and an estimate of the value function $V\left(s_{t} ; \theta_{v}\right)$. The RL agent maximizes longterm expected reward in the game in a manner and uses the same mix of $n$-step returns to update the policy and the value function at the same time.</p>
<p>In text games, actions are strings of tokens. We do not query the game environments for admissible actionsthose that are guaranteed to have an effect. However, we do simplify the action space by using templates based on the verb [Ammanabrolu and Hausknecht, 2020]. Templates are composed of interchangeable verbs phrases $(V P)$, optionally followed by prepositional phrases ( $V P$ $P P$ ), e.g. ([drink/eat] <em>) and ([apply/ask/put] _ [on/about/down] </em>), where the verbs and prepositions within [.] are aliases. The agent generates actions by first sampling a template and then sampling the word from the game's vocabulary to fill in the blanks.</p>
<p>As the agent explores the game world, we build an internal World KG state representation. This knowledge graph is distinct from the Story KG. Following Ammanabrolu et al. [2020], we consider the process of building a knowledge graph to be a question-answering task. We finetune the ALBERT model [Lan et al., 2019] on the JerichoQA [Hausknecht et al., 2020b] dataset, which is specifically designed for question answering in text games. This allows the model to answer questions like "What am I carrying?" and "Where am I?". We use the answers as a set of candidate vertices $V_{t}$ for the current step and the questions as a set of relations $R_{t}$. We then combine $V_{t}$ and $R_{t}$ with the game knowledge graph from the previous step $G_{t-1}$ to update the game knowledge graph to $G_{t}$. More details can be found in Appendix A.2.</p>
<p>The ALBERT-QA technique for building the World $K G$ has been shown to improve RL agent performance because it is trained to the particulars of text games [Ammanabrolu and Hausknecht, 2020]. The Story $K G$, on the other hand, is constructed using the VerbAtlas SRL model because exemplar stories draw from a different text distribution-they are expected to be less verbose about world details, intentionally leaving more implicit. Further, the Story KG only needs to extract a few key details to bias the agent toward certain actions and locations in the environment.</p>
<p>Putting it all together, at each step of training, a total score $R_{t}$ and an natural language observation $o_{t}$ is received from the game environment-consisting of $\left(o_{t_{\text {doc }}}, o_{t_{\text {game }}}, o_{t_{\text {inc }}}, o_{t-1}\right)$ corresponding to the room description, game feedback, inventory, and previous action. An example is depicted in the left side of Figure 2. As introduced in Section 3.2, the World KG $G_{t}$ at time step $t$ is also updated. Each component of $o_{t} \in \mathbb{R}^{d_{o} \times c}$ is processed using a GRU-based encoder to obtain $\boldsymbol{o}<em t="t">{t}$ and World KG, $G</em>}$, is processed via Graph Attention Networks (GATs) [Veličković et al., 2017] followed by a linear layer to get the graph representation $\mathbf{g<em o="o">{\mathbf{t}} \in \mathbb{R}^{d</em>}} ; c$ is the number of $\boldsymbol{o<em _mathrm_t="\mathrm{t">{\mathrm{t}}$ 's components. Then we calculate the attention $\alpha$ between $\boldsymbol{o}</em>$,}}$ and $\mathbf{g}_{\mathbf{t}</p>
<p>$$
\begin{gathered}
\boldsymbol{\alpha}=\operatorname{softmax}\left(\boldsymbol{W}<em _mathrm_l="\mathrm{l">{\mathrm{l}} \boldsymbol{h}+\boldsymbol{b}</em>\right) \
\boldsymbol{h}=\tanh \left(\boldsymbol{W}}<em _mathrm_t="\mathrm{t">{\mathrm{o}} \boldsymbol{o}</em>}} \oplus\left(\boldsymbol{W<em _mathrm_t="\mathrm{t">{\mathrm{g}} \mathbf{g}</em>\right)\right)
\end{gathered}
$$}}+\boldsymbol{b}_{\mathrm{g}</p>
<p>where $\oplus$ denotes the addition of a matrix and a vector and $\odot$ denotes dot-product. $\boldsymbol{W}<em o="o">{1} \in \mathbb{R}^{d</em>} \times d_{o}}, \boldsymbol{W<em o="o">{\mathrm{g}} \in \mathbb{R}^{d</em>} \times d_{o}}$, $\boldsymbol{W<em o="o">{\mathrm{o}} \in \mathbb{R}^{d</em>} \times d_{o}}$ are weights and $\boldsymbol{b<em o="o">{\mathrm{l}} \in \mathbb{R}^{d</em>}}, \boldsymbol{b<em o="o">{\mathrm{o}} \in \mathbb{R}^{d</em>}}$ are biases. Finally, the overall representation vector $\mathbf{v<em _mathbf_t="\mathbf{t">{\mathbf{t}}$ is updated by $\mathbf{v}</em>}}=\mathbf{g<em i="i">{\mathbf{t}}+\sum</em>}^{\circ} \boldsymbol{\alpha<em _mathrm_t="\mathrm{t">{i} \odot \mathbf{o}</em>$}, i</p>
<h3>3.3 Rewarding the Agent</h3>
<p>After obtaining the overall representation $\mathbf{v}_{\mathbf{t}}$ above, we incorporate two intrinsic rewards into the RL agent's training, in order to motivate it to act in a manner that to be more closely resemble the implicit world of the story.</p>
<p>The $K G$ intrinsic reward is determined by comparing the similarity between the agent's World KG and Story KG. This</p>
<p>reflects how closely the agent's actions align the actual world with the world that should exist according to the story. When new edges are added to the World $\mathrm{KG}, \mathbf{G}<em t="t">{t}$, we verify if the corresponding triples already exist in the Story KG. Each newly discovered triple results in a positive intrinsic reward $r</em>=1 \times \rho$ at this step. The edge (You, in, kitchen) (the green edge in the knowledge graphs) is not considered here as we only take into account new edges added during the current round of the game.}^{s}=n \times \rho&gt;0$, where $n$ is the number of same triples. An example can be found in Figure 2 (the red edges in the knowledge graphs). For example, the agent performs an action that resembles the implicit world of the story, such as "drink coffee", a new edge, (You, DRINK, coffee), will be added to the World KG. This triple is identified as being identical to one triple in the Story KG, so the KG intrinsic reward $r_{t}^{s</p>
<p>Inspired by Ammanabrolu et al. [2020b], we also encourage the agent to explore more locations and scenarios by defining a exploration intrinsic reward $r_{t}^{e}=$ $\Delta\left(\mathbf{G}<em t="t">{\text {global }}-\mathbf{G}</em>}\right)$, where $\mathbf{G<em i="1">{\text {global }}=\bigcup</em>$ is the set of all edges that the agent has ever had in its knowledge graph. When the agent learns more information about the world, it will expand the size of its World KG, increasing the likelihood of reward and success.}^{t-1} \mathbf{G}_{i</p>
<p>The overall intrinsic reward received at time step $t$ is:</p>
<p>$$
r_{t}^{\prime}=r_{t}+\alpha \times r_{t}^{s}+\beta \times r_{t}^{v}
$$</p>
<p>where $\alpha$ and $\beta$ are scaling factors; $r_{t}$ is the game score; $r_{t}^{\prime}$ is the reward provided to the agent on time step. The rest of the training methodology is unchanged from Ammanabrolu et al. [2020b].</p>
<h2>4 Experiments</h2>
<p>We conducted experiments in three phases:</p>
<ol>
<li>We train agents to play text games in which the agent must successfully navigate some normative everyday routines. Agents are provided with different exemplar stories about the tasks. In this set of experiments, we evaluate whether Story Shaping facilitates the expression of commonsense and social norm knowledge (Section 4.1).</li>
<li>We train agents to play an open-ended role-playing game, using different exemplar quests to create different personas. In this set of experiments, we evaluate whether Story Shaping is capable of shaping the agent's behaviors in a way recognizable to humans. It also demonstrates that Story Shaping can learn trope knowledge, which is knowledge particular to different storytelling conventions (Section 4.2).</li>
<li>We train agents to play games in which the exemplar stories differ from the operating environment either by referencing objects that do not exist or actions that cannot be performed. This set of experiments evaluates whether Story Shaping is robust to different assumptions between the provider of the story and the agent's environment. These differences can arise for a number of reasons, one of which being that the story is generated by another system, such as ChatGPT [OpenAI, 2022] (Section 4.3).</li>
</ol>
<p>Baselines We implement the RL agents below with and without Story Shaping:</p>
<ul>
<li>Q*BERT [Ammanabrolu et al., 2020b], a state-of-theart RL agent for text games designed to work with Jericho games and TextWorld. It constructs its World KG by answering questions with ALBERT-QA.</li>
<li>KG-A2C [Ammanabrolu and Hausknecht, 2020], which uses Stanford's Open Information Extraction (OpenIE) [Angeli et al., 2015] to build its World KG because ALBERT-QA is tuned on the JerichoQA dataset. KGA2C is used for LIGHT experiments.</li>
</ul>
<h3>4.1 Commonsense and Social Norm Knowledge</h3>
<p>We first seek to understand whether Story Shaping enhances the expression of common sense and social norm knowledge in Reinforcement Learning agents.
Games. We implement three slice-of-life text games on two game platforms:</p>
<ul>
<li>9:05: a game in which the agent must successfully navigate the normative routine of getting ready to work, implemented in Jericho [Hausknecht et al., 2020b], a framework for interacting with text games, as the interface connecting learning agents with interactive fiction games.</li>
<li>Shopping: a game in which the agent must successfully purchase the clothes, developed in TextWorld [Côté et al., 2018], an open-source, extensible engine that both generates and simulates text games.</li>
<li>See Doctor: a game in which the agent gets sick and must seek medical treatment, also developed in TextWorld.
Details for 9:05 can be found in Appendix B.1. Details for Shopping and See Doctor can be found in Appendix B.2.
Training. For each game, we train two agents. $Q * B E R T-S$ uses Story Shaping with stories written by humans. The baseline, $Q * B E R T$ is the same agent but without Story Shaping. We evaluate these two trained agents by running test games over 20 random seeds. Training details are in Appendix A.3.
Automatic Evaluation. We automatically evaluate the agents' expression of common sense and social norm knowledge by:</li>
<li>Win rate: the winning rate of trained agents on test games over 20 random seeds.</li>
<li>Avg steps: the average number of steps that each agent takes to win the game. The game will automatically end over 50 steps.</li>
<li>Avg Commonsense score: the total intrinsic reward accrued over a testing trial; a higher score indicates the agent takes more actions that express commonsense and social norm knowledge. Details about this test game environment can be found in Appendix A.4.</li>
<li>Avg game score: the average score of each agent on the test games, which reflects how far toward the win condition the agent made it, irrespective of how the agent reached the farthest point.</li>
</ul>
<table>
<thead>
<tr>
<th>Game</th>
<th>Agents</th>
<th>Win Rate %</th>
<th>Avg Steps</th>
<th>Avg CS Score</th>
<th>Max CS Score</th>
<th>Avg Game Score</th>
<th>Max Game Score</th>
</tr>
</thead>
<tbody>
<tr>
<td>9:05</td>
<td>Q*BERT-S</td>
<td>100</td>
<td>16.30</td>
<td>3.90</td>
<td>4</td>
<td>5.00</td>
<td>5</td>
</tr>
<tr>
<td></td>
<td>Q*BERT</td>
<td>100</td>
<td>7.25</td>
<td>0.40</td>
<td></td>
<td>5.00</td>
<td></td>
</tr>
<tr>
<td>Shopping</td>
<td>Q*BERT-S</td>
<td>100</td>
<td>12.35</td>
<td>3.70</td>
<td>4</td>
<td>5.00</td>
<td>5</td>
</tr>
<tr>
<td></td>
<td>Q*BERT</td>
<td>100</td>
<td>6.30</td>
<td>0.90</td>
<td></td>
<td>5.00</td>
<td></td>
</tr>
<tr>
<td>Doctor</td>
<td>Q*BERT-S</td>
<td>95</td>
<td>19.15</td>
<td>6.70</td>
<td>8</td>
<td>4.75</td>
<td>5</td>
</tr>
<tr>
<td></td>
<td>Q*BERT</td>
<td>95</td>
<td>14.30</td>
<td>0.70</td>
<td></td>
<td>4.75</td>
<td></td>
</tr>
</tbody>
</table>
<p>Table 2: Automatic evaluation results across 20 independent runs comparing Q<em>BERT-S to baseline Q</em>BERT Each system is trained under the same game environment.</p>
<table>
<thead>
<tr>
<th>Game</th>
<th>Commonsense/Social Norm</th>
<th></th>
<th></th>
<th>Understanding</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Shaped % Base %</td>
<td>Tie %</td>
<td>Shaped % Base %</td>
<td>Tie %</td>
<td></td>
</tr>
<tr>
<td>9:05</td>
<td>63.63*</td>
<td>9.09</td>
<td>27.27</td>
<td>36.36</td>
<td>45.45</td>
</tr>
<tr>
<td>Shopping</td>
<td>66.67**</td>
<td>8.33</td>
<td>25.00</td>
<td>41.67</td>
<td>33.33</td>
</tr>
<tr>
<td>Doctor</td>
<td>53.85</td>
<td>23.08</td>
<td>23.08</td>
<td>46.15</td>
<td>38.46</td>
</tr>
</tbody>
</table>
<p>Table 3: Human evaluation results showing the percentage of participants who preferred Story Shaped Q<em>BERT-S to baseline Q</em>BERT, or thought the systems were equal. Each system is trained under the same game environment. * indicates human evaluation results are significant at $p&lt;0.05$ confidence level; ** at $p&lt;0.01$ using a Wilcoxan sign test on win-lose pairs.</p>
<p>Results are shown in Table 2. The win rates and the average game scores are identical between agents with and without Story Shaping. However, our agent’s “Avg Commonsense Score” is significantly higher than the baseline agent’s, indicating that the Story Shaping agent demonstrates superior ability in expressing common sense and social norm knowledge. The larger ”Avg Steps” value for our agent also suggests that it takes more actions before winning the game, which further highlights that it is not seeking the shortest possible trajectory.</p>
<p>Human Evaluation. We recruited 30 participants using the Cloud Research platform and Amazon Mechanical Turk [Litman et al. (2017)]. We screened for participants that were generally not familiar with text games. Each participant reads the winning goal of a randomly chosen game. Then they read a pair of game transcript which played by Q<em>BERT-S and Q</em>BERT, specifically. Each transcript includes game observations and the corresponding actions. Then they are given the following metrics and asked to choose which game transcript they prefer for that metric:</p>
<ul>
<li>This sequence of actions expresses more common sense thinking (with social norm knowledge) on the action choice.</li>
<li>This sequence of actions makes you understand why the agent takes these actions given what you know about the goal.</li>
</ul>
<p>Participants had to provide detailed explanations for their choices in each comparison, using at least 50 characters of free text. We manually reviewed responses and discarded data from those who did not provide detailed explanations. Each game trajectory pair is evaluated by a minimum of 10 participants. Our study was approved by our Institutional Review Board, and we payed participants the equivalent of $15/hr. Example of the human evaluation survey can be found in Appendix D.1</p>
<p>Table 3 shows the percentage of times stories from each system are preferred for each metric. In the same game environment, Q<em>BERT-S performs significantly better than Q</em>BERT on the dimension of “Common and Social Norm Sense”. We can conclude that Story Shaping facilitates the expression of commonsense and social norm knowledge of the trained RL agents significantly. On the dimension of “Understanding”, we would expect Story Shaped agents to be no less understandable than the baseline, indicating that longer trajectories are not random. Q<em>BERT-S achieves comparable results with Q</em>BERT. Participants who favored Q<em>BERT mentioned that they found the shorter game paths easier to comprehend. As seen in Table 2, Q</em>BERT takes fewer steps to complete the game, making it more straightforward for human participants to follow. Our system is thus shown to improve the expression of commonsense and social norm knowledge of agents while preserving comprehensibility.</p>
<h3>4.2 Persona Understanding</h3>
<p>We assess whether Story Shaping has the ability to shape the agent’s behaviors in a way that is identifiable to humans. We develop a role-playing game in the LIGHT [Urbanek et al. (2019)] environment, which is a large-scale fantasy text adventure game research platform for training agents that can both talk and act, interacting either with other models or with humans. We provided different exemplar quests for four personas: thief, bum, adventurer and thug. We provide a LIGHT world that provides a rich set of locations and objects for all personas to make use of, or ignore. Details about this game can be found in Appendix B.3. In these experiments, we compare two KG-A2C agents utilizing Story Shaping but with different human-written stories.</p>
<p>We recruited an additional 29 participants. Participants will read a winning goal and be told the agent’s persona (thief, adventurer, thug, bum), then read two game transcripts played by versions of KG-A2C with Story shaping. One version is trained using a story about the given persona, the other using a random story selected from the remaining personas. Participants were asked to indicate which of the two agents had the given persona. At least 10 participants evaluate each game. More details about this human evaluation survey can be found in Appendix D.2.</p>
<p>Table 4 displays the percentage of participants who chose the game transcript generated by the agent with the exemplar story for the given persona, versus a randomly chosen exemplar story. The percentage reflects the effectiveness of Story Shaping in shaping the agent’s behaviors in a way that is recognizable to humans. The results indicate that RL agent with Story Shaping is able to comprehend exemplar quests for different personas and generate actions that align with the given persona. Additionally, all the versions of KG-A2C with Story Shaping attain a win rate of 100%, regardless of which exemplar story. It demonstrates the capability of Story Shaping in</p>
<table>
<thead>
<tr>
<th>1</th>
<th>Given</th>
<th>Participant Choice</th>
<th></th>
<th></th>
<th>Win</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Persona</td>
<td>Correct%</td>
<td>Incorrect%</td>
<td>Can’t tell%</td>
<td>Rate</td>
</tr>
<tr>
<td>2</td>
<td>Thief</td>
<td>58.06*</td>
<td>19.35</td>
<td>22.58</td>
<td>100</td>
</tr>
<tr>
<td>2</td>
<td>Adventurer</td>
<td>62.50*</td>
<td>15.62</td>
<td>21.88</td>
<td>100</td>
</tr>
<tr>
<td>2</td>
<td>Thug</td>
<td>72.73**</td>
<td>12.12</td>
<td>15.15</td>
<td>100</td>
</tr>
<tr>
<td>2</td>
<td>Bum</td>
<td>64.71*</td>
<td>23.53</td>
<td>11.76</td>
<td>100</td>
</tr>
</tbody>
</table>
<p>Table 4: The percentage of participants who preferred the agent utilizing Story Shaping with the specific story of the given persona, the agent using Story Shaping with a story from a different persona, or believed the systems were equivalent when the game goal and persona were provided to human participants. “Win Rate” is the winning rate of trained agents using Story Shaping with the specific story of the given persona on test games over 20 random seeds. The symbols (<em> and </em>*) used for indicating significance in as in Table 3.</p>
<table>
<thead>
<tr>
<th>Game</th>
<th>Persona</th>
<th>Common/Social</th>
<th></th>
<th></th>
<th>Understanding</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>GPT%</td>
<td>Base%</td>
<td>Tie%</td>
<td>GPT%</td>
<td>Base%</td>
</tr>
<tr>
<td>9:05</td>
<td>-</td>
<td>71.43*</td>
<td>14.29</td>
<td>14.29</td>
<td>57.14</td>
<td>42.86</td>
</tr>
<tr>
<td>Shop.</td>
<td>-</td>
<td>62.50*</td>
<td>25.00</td>
<td>12.50</td>
<td>37.50</td>
<td>25.00</td>
</tr>
<tr>
<td>LIGHT</td>
<td>Thief</td>
<td>57.14</td>
<td>28.57</td>
<td>14.29</td>
<td>42.86</td>
<td>28.57</td>
</tr>
<tr>
<td></td>
<td>Adv</td>
<td>83.33**</td>
<td>16.67</td>
<td>0.00</td>
<td>50.00</td>
<td>16.67</td>
</tr>
<tr>
<td></td>
<td>Thug</td>
<td>64.71*</td>
<td>11.76</td>
<td>23.53</td>
<td>52.94</td>
<td>35.29</td>
</tr>
</tbody>
</table>
<p>Table 5: The percentage of participants who favored the agent with Story Shaping using the ChatGPT-generated story over the baseline, or deemed the systems as indistinguishable. The symbols (<em> and </em>*) are used for indicating significance as in Table 3.</p>
<table>
<thead>
<tr>
<th>Game: shopping</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>First observation: I am in front of a mall. A cafe lies to the south,</td>
<td></td>
</tr>
<tr>
<td>while a way to the east leads to the mall.</td>
<td></td>
</tr>
<tr>
<td>Goal: BUY CLOTHES.</td>
<td></td>
</tr>
<tr>
<td>Exemplar story from ChatGPT:</td>
<td></td>
</tr>
<tr>
<td>To buy clothes, you enter the mall and navigate to the store that</td>
<td></td>
</tr>
<tr>
<td>sells the clothing you are interested. Many malls have directory</td>
<td></td>
</tr>
<tr>
<td>listings near the entrances, which can help you find the specific</td>
<td></td>
</tr>
<tr>
<td>stores you are looking for. Once locating the store, you can browse</td>
<td></td>
</tr>
<tr>
<td>and try on the clothing, and then make a purchase at the register.</td>
<td></td>
</tr>
<tr>
<td>Example action sequence by Q*BERT-Story Shaping:</td>
<td></td>
</tr>
<tr>
<td>Location: Street</td>
<td>Action: go east</td>
</tr>
<tr>
<td>Location: Mall</td>
<td>Action: go north</td>
</tr>
<tr>
<td>Location: Store</td>
<td>Action: examine clothes; try clothes</td>
</tr>
<tr>
<td></td>
<td>Action: give money to staff</td>
</tr>
<tr>
<td></td>
<td>Action: buy clothes; take clothes</td>
</tr>
<tr>
<td>Example action sequence by Q*BERT:</td>
<td></td>
</tr>
<tr>
<td>Location: Street</td>
<td>Action: go east</td>
</tr>
<tr>
<td>Location: Mall</td>
<td>Action: go north</td>
</tr>
<tr>
<td>Location: Store</td>
<td>Action: give money to staff</td>
</tr>
<tr>
<td></td>
<td>Action: buy clothes; take clothes</td>
</tr>
</tbody>
</table>
<p>Table 5: Example action sequence in the game shopping. Words underlined indicate actions that can be taken in the game environment. Words with a wavy underline indicate actions that are invalid or not allowed in the game.
mon sense and social norm knowledge, even though some information in the automatically generated story’s knowledge graph is unattainable. For example, in Table 6, the ChatGPT generated exemplar story involves an entity that does not exist in the game—“directory listings”. Despite the mismatch, our technique allows for the flexibility needed for our agent to successfully complete tasks by utilizing other elements of the exemplar story and using trial-and-error to fill in the rest. Agents have a 100% game completion rate in all the test game environments.</p>
<h2>5 Conclusions</h2>
<p>Story Shaping is a straightforward approach to the challenge of reward design where one wishes to not only reward an agent for completing a task, but reward the agent for how it accomplishes the task. This might mean aligning an agent’s behavior with human preferences and expectations, teaching the agent commonsense reasoning and social norms, or shaping character personas in a game.</p>
<p>Our technique allows one to provide a high-level exemplar story from which the agent automatically extracts knowledge about important objects, locations, and actions. It then self-rewards when these objects, and locations are encountered, and actions are performed. Because stories are highlevel abstractions, Story Shaping can fill in missing details and is robust to situations where the story cannot be exacted as given. We have shown that Story Shaping has a significant and human-observable impact on agent behavior without compromising task completion.</p>
<h2>References</h2>
<p>[Ammanabrolu and Hausknecht, 2020] Prithviraj Ammanabrolu and Matthew Hausknecht. Graph constrained reinforcement learning for natural language action spaces. arXiv preprint arXiv:2001.08837, 2020.
[Ammanabrolu and Riedl, 2018] Prithviraj Ammanabrolu and Mark O Riedl. Playing text-adventure games with graph-based deep reinforcement learning. arXiv preprint arXiv:1812.01628, 2018.
[Ammanabrolu et al., 2020a] Prithviraj Ammanabrolu, Wesley Cheung, Dan Tu, William Broniec, and Mark Riedl. Bringing stories alive: Generating interactive fiction worlds. In Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, volume 16, pages 3-9, 2020.
[Ammanabrolu et al., 2020b] Prithviraj Ammanabrolu, Ethan Tien, Matthew Hausknecht, and Mark O Riedl. How to avoid being eaten by a grue: Structured exploration strategies for textual worlds. arXiv preprint arXiv:2006.07409, 2020.
[Ammanabrolu et al., 2021] Prithviraj Ammanabrolu, Jack Urbanek, Margaret Li, Arthur Szlam, Tim Rocktäschel, and Jason Weston. How to motivate your dragon: Teaching goal-driven agents to speak and act in fantasy worlds. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 807-833, Online, June 2021. Association for Computational Linguistics.
[Ammanabrolu et al., 2022] Prithviraj Ammanabrolu, Liwei Jiang, Maarten Sap, Hannaneh Hajishirzi, and Yejin Choi. Aligning to social norms and values in interactive narratives. arXiv preprint arXiv:2205.01975, 2022.
[Angeli et al., 2015] Gabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D Manning. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344-354, 2015.
[Barto, 2013] Andrew G Barto. Intrinsic motivation and reinforcement learning. In Intrinsically motivated learning in natural and artificial systems, pages 17-47. Springer, 2013.
[Burda et al., 2018] Yuri Burda, Harri Edwards, Deepak Pathak, Amos Storkey, Trevor Darrell, and Alexei A Efros. Large-scale study of curiosity-driven learning. arXiv preprint arXiv:1808.04355, 2018.
[Côté et al., 2018] Marc-Alexandre Côté, Akos Kádár, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew Hausknecht, Layla El Asri, Mahmoud Adada, et al. Textworld: A learning environment for text-based games. In Workshop on Computer Games, pages 41-75. Springer, 2018.
[Dambekodi et al., 2020] Sahith Dambekodi, Spencer Frazier, Prithviraj Ammanabrolu, and Mark O Riedl. Playing text-based games with common sense. In Proceedings of the NeurIPS Wordplay workshop, 2020.
[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, 2019.
[Di Fabio et al., 2019] Andrea Di Fabio, Simone Conia, and Roberto Navigli. Verbatlas: a novel large-scale verbal semantic resource and its application to semantic role labeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 627-637, 2019.
[Frazier et al., 2020] Spencer Frazier, Md Sultan Al Nahian, Mark O. Riedl, and Brent Harrison. Learning norms from stories: A prior for value aligned agents. In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 2020.
[Gildea and Jurafsky, 2002] Daniel Gildea and Daniel Jurafsky. Automatic labeling of semantic roles. Computational linguistics, 28(3):245-288, 2002.
[Goldfarb-Tarrant et al., 2020] Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, and Nanyun Peng. Content planning for neural story generation with aristotelian rescoring. arXiv preprint arXiv:2009.09870, 2020.
[Harrison and Riedl, 2016] Brent Harrison and Mark O Riedl. Towards learning from stories: An approach to interactive machine learning. In Workshops at the Thirtieth AAAI Conference on Artificial Intelligence, 2016.
[Hausknecht et al., 2020a] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure. In ThirtyFourth AAAI Conference on Artificial Intelligence (AAAI), 2020.
[Hausknecht et al., 2020b] Matthew Hausknecht, Prithviraj Ammanabrolu, Marc-Alexandre Côté, and Xingdi Yuan. Interactive fiction games: A colossal adventure. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903-7910, 2020.
[Hedlund et al., 2002] J. Hedlund, J. Antonakis, and R.J. Sternberg. Tacit Knowledge and Practical Intelligence: Understanding the Lessons of Experience (ARI Research Note 2003-04). United States Army Research Institute for the Behavioral and Social Sciences, Washington, D.C., 2002.
[Kim et al., 2019] Youngjin Kim, Wontae Nam, Hyunwoo Kim, Ji-Hoon Kim, and Gunhee Kim. Curiositybottleneck: Exploration by distilling task-specific novelty. In International Conference on Machine Learning, pages 3379-3388. PMLR, 2019.</p>
<p>[Klyne, 2004] Graham Klyne. Resource description framework (rdf): Concepts and abstract syntax. http://www. w3. org/TR/2004/REC-rdf-concepts-20040210/, 2004.
[Lan et al., 2019] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942, 2019.
[Levy et al., 2018] Andrew Levy, Robert Platt, and Kate Saenko. Hierarchical reinforcement learning with hindsight. arXiv preprint arXiv:1805.08180, 2018.
[Litman et al., 2017] Leib Litman, Jonathan Robinson, and Tzvi Abberbock. Turkprime. com: A versatile crowdsourcing data acquisition platform for the behavioral sciences. Behavior research methods, 49(2):433-442, 2017.
[Mnih et al., 2016] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 19281937. PMLR, 2016.
[Nachum et al., 2018] Ofir Nachum, Shixiang Shane Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. Advances in neural information processing systems, 31, 2018.
[Nahian et al., 2021] Md Sultan Al Nahian, Spencer Frazier, Brent Harrison, and Mark O. Riedl. Training value-aligned reinforcement learning agents using a normative prior. arXiv:2104.09469, 2021.
[Nair et al., 2018] Ashvin V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. Advances in neural information processing systems, 31, 2018.
[OpenAI, 2022] OpenAI. Chatgpt: A large-scale opendomain chatbot. https://openai.com/blog/chatgpt/, 2022.
[Oudeyer et al., 2007] Pierre-Yves Oudeyer, Frdric Kaplan, and Verena V Hafner. Intrinsic motivation systems for autonomous mental development. IEEE transactions on evolutionary computation, 11(2):265-286, 2007.
[Palmer et al., 2005] Martha Palmer, Daniel Gildea, and Paul Kingsbury. The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71-106, 2005.
[Peng et al., 2021a] Xiangyu Peng, Mark O Riedl, and Prithviraj Ammanabrolu. Inherently explainable reinforcement learning in natural language. arXiv preprint arXiv:2112.08907, 2021.
[Peng et al., 2021b] Xiangyu Peng, Kaige Xie, Amal Alabdulkarim, Harshith Kayam, Samihan Dani, and Mark O Riedl. Guiding neural story generation with reader models. arXiv preprint arXiv:2112.08596, 2021.
[Pong et al., 2019] Vitchyr H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and Sergey Levine. Skew-
fit: State-covering self-supervised reinforcement learning. arXiv preprint arXiv:1903.03698, 2019.
[Rajpurkar et al., 2018] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics.
[Schmidhuber, 1991] Jürgen Schmidhuber. A possibility for implementing curiosity and boredom in model-building neural controllers. In Proc. of the international conference on simulation of adaptive behavior: From animals to animats, pages 222-227, 1991.
[Shi and Lin, 2019] Peng Shi and Jimmy Lin. Simple bert models for relation extraction and semantic role labeling. arXiv preprint arXiv:1904.05255, 2019.
[Shridhar et al., 2021] Mohit Shridhar, Xingdi Yuan, MarcAlexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021.
[Stadie et al., 2015] Bradly C Stadie, Sergey Levine, and Pieter Abbeel. Incentivizing exploration in reinforcement learning with deep predictive models. arXiv preprint arXiv:1507.00814, 2015.
[Urbanek et al., 2019] Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, and Jason Weston. Learning to speak and act in a fantasy text adventure game. arXiv preprint arXiv:1903.03094, 2019.
[Veličković et al., 2017] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
[Vezhnevets et al., 2017] Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In International Conference on Machine Learning, pages 35403549. PMLR, 2017.
[Wang et al., 2022] Ruoyao Wang, Peter Jansen, MarcAlexandre Côté, and Prithviraj Ammanabrolu. Scienceworld: Is your agent smarter than a 5th grader?, 2022.
[Xu et al., 2020] Yunqiu Xu, Meng Fang, Ling Chen, Yali Du, Joey Tianyi Zhou, and Chengqi Zhang. Deep reinforcement learning with stacked hierarchical attention for text-based games. Advances in Neural Information Processing Systems, 33, 2020.</p>
<table>
<thead>
<tr>
<th>Parameters</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>batch size</td>
<td>8</td>
</tr>
<tr>
<td>learning rate</td>
<td>3e-5</td>
</tr>
<tr>
<td>max seq len</td>
<td>512</td>
</tr>
<tr>
<td>doc stride</td>
<td>128</td>
</tr>
<tr>
<td>warmup steps</td>
<td>814</td>
</tr>
<tr>
<td>max steps</td>
<td>8144</td>
</tr>
<tr>
<td>gradient accumulation steps</td>
<td>24</td>
</tr>
</tbody>
</table>
<p>A Implementation Details</p>
<h3>A.1 Semantic Role Labeling Using VerbAtlas</h3>
<p>The SRL model automatically identifies and labels the argument structures of stories. For example, it extracts 'verbatlas': 'EXIST_LIVE', 'args_words': {'Theme': 'Jenny', 'Attribute': 'Georgia'} from "Jenny lived in Georgia". Verbs in the story will be represented as the VerbAtlas frame. For example, '‘live’' is represented as '‘EXIST_LIVE’'.</p>
<p>We use a fine-tuned transformer model for semantic role labeling (SRL), which is a BERT [Devlin et al., 2019] model with a linear classification layer trained on the Ontonotes 5.0 dataset to predict PropBank[Palmer et al., 2005] SRL. This model, proposed by Shi [2019], is currently the state-of-the-art for English SRL. We use an open-source implementation , which is based on the official AllenNLP BERT-SRL model . Trained with the following hyperparameters:</p>
<ul>
<li>Batch size: 32</li>
<li>Dropout for the input embeddings: 0.1</li>
<li>Learning rate: $5 e^{-5}$</li>
<li>Optimizer: Adam</li>
<li>Total Epochs: 15</li>
</ul>
<p>Then, we use the mappings from Propbank frames to VerbAtlas [Di Fabio et al., 2019] classes to return the correct corresponding VerbAtlas classes instead of Propbank’s [Palmer et al., 2005]. We can directly map VerbAtlas classes to PropBank frames because for every VerbAtlas class, there is only one PropBank frame. This allows us to use the rich content from VerbAtlas with the same model that was initially trained to predict PropBank.</p>
<h3>A.2 Knowledge Graph Representation QA Model</h3>
<p>The question answering network based on ALBERT [Lan et al., 2019] uses the hyperparameters listed in the original paper. These hyperparameters have been shown to work well on the SQuAD 2.0 [Rajpurkar et al., 2018] dataset. We did not do any additional tuning of the hyperparameters.</p>
<h3>A.3 RL agents with Story Shaping</h3>
<p>Further details of what is found in Figure 2. The sequential action decoder consists two GRUs that are linked together as seen in Ammanabrolu [2020]. The first GRU decodes an action template and the second decodes objects that can be filled into the template. These objects are constrained by a graph mask, i.e. the decoder is only allowed to select entities that are already present in the knowledge graph.</p>
<p>Same with Ammanabrolu [2020], the loss consists of template loss, object loss, value loss, actor loss and entropy loss. The template loss given a particular state and current network parameters is applied to the decoder. Similarly, the object loss is applied across the decoder is calculated by summing crossentropy loss from all the object decoding steps. Entropy loss over the valid actions, is designed to prevent the agent from prematurely converging on a trajectory. The following hyperparameters are taken from the original paper and known to work well on text games.</p>
<table>
<thead>
<tr>
<th>Parameters</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>discount factor</td>
<td>0.9</td>
</tr>
<tr>
<td>entropy coefficient</td>
<td>0.03</td>
</tr>
<tr>
<td>value coefficient</td>
<td>9</td>
</tr>
<tr>
<td>template coefficient</td>
<td>3</td>
</tr>
<tr>
<td>object coefficient</td>
<td>9</td>
</tr>
</tbody>
</table>
<h3>A.4 Test Game with Human-likeness Score</h3>
<p>In order to automatically evaluate the expression of commonsense and social norm knowledge of the trained RL agents, we develop a test game mode for each text game. During training, the player can only earn a score if they win the game. However, when evaluating the RL agents, we test both the baseline and our agents in a test game that has the same map and actions as the training game, but a different scoring system. Specifically, the test game environment not only gives a score when the player wins the game, but also gives scores when the agent reaches specific states. For example, in the game 9:05, the test game environment will give a score of 2 when the agent eats a pop-tart in the kitchen, whereas in the training game, this action would not earn any points. Our test game mode allows us to automatically evaluate the humanlike qualities of our trained RL agents. We believe that a higher score in this game mode indicates a greater degree of the expression of commonsense and social norm knowledge of the trained RL agents.</p>
<p>B Game Design</p>
<h3>B.1 Jericho Game</h3>
<p>9:05 is a text adventure game by Adam Cadre. The game is designed for a casual audience, including those new to the genre. It is a short and simple game that follows a branching narrative in which a man wakes up and receives a call telling him to go to work. Our agent can be tested effectively in this game due to the numerous optional commonsense actions available to the player. In the game, the player has the option to leave their home and go directly to work in order to win the game immediately, or they can choose to take some time to get ready by changing clothes, taking a shower, and having a toast before starting their day. All of these actions are optional and do not affect the outcome of the game.</p>
<p><em>Exemplar story</em> we use for Section 4.1: Upon waking in the morning, I start my day with a Pop-Tart breakfast, followed by a shower before commencing work.</p>
<p>We designed a game with optional <em>CS scores</em> for evaluating the human-likeness of our agents. CS scores designed are as follows,</p>
<ul>
<li>Score 2: shower is used.</li>
<li>Score 2: Pop-Tart is consumed.</li>
</ul>
<h3>B.2 TextWorld</h3>
<p>We designed two games in TextWorld [Côté <em>et al.</em>, 2018] text game engine.</p>
<h4>Shopping</h4>
<p>We use the following to a game where the goal is to purchase clothing at a mall. The player starts out on the street and has the option to stop at various cafes and restaurants on the way to the mall. They may also encounter NPC characters who can provide information on obtaining coupons. The player has multiple routes they can take to reach the clothing stores and can choose to engage in optional activities such as purchasing a coffee or using coupons. The layout of the game is shown in Fig.3.</p>
<p>The game score is only obtained when the player finish the game. When the player has “clothes” in his inventory, the game ends with a game score: 5.</p>
<p><em>Exemplar story</em> for experiments in Section 4.1 is: To save money, I need to obtain a coupon. Once I have tried on the clothes, I will purchase them.</p>
<p>We designed a game with optional <em>CS scores</em> for evaluating whether Story Shaping facilitates the expression of commonsense and social norm knowledge. CS scores designed for Section 4.1 are as follows,</p>
<ul>
<li>Score 2: coupon is applied.</li>
<li>Score 2: clothes is tried.</li>
</ul>
<h4>See Doctor</h4>
<p>The player starts at home with a cup of hot water and money. The player can make hot soup, buy hot coffee, visit the doctor at the hospital, or go to a drug store to get the medicine. The player has multiple paths they can take to obtain the medicine, and they can also take various optional actions such as buying a coffee or drinking hot water.</p>
<p>The game score is only obtained when the player finish the game. When the player has “medicine” in his inventory, the game ends with a game score: 5. You can see the layout of this game in Fig.4</p>
<p><em>Exemplar Story</em> for experiments in Section 4.1 is: I caught a cold and drank hot water, but it didn’t help after taking a shower. I went to the hospital to see the doctor and get a prescription to buy medicine.</p>
<p>We designed a game with optional <em>CS scores</em> for evaluating whether Story Shaping facilitates the expression of commonsense and social norm knowledge. The <em>CS scores</em> designed are as follows,</p>
<ul>
<li>Score 2: water is consumed.</li>
<li>Score 2: shower is used.</li>
<li>Score 2: doctor is seen.</li>
<li>Score 2: prescription is in inventory.</li>
</ul>
<h3>B.3 LIGHT</h3>
<p>LIGHT [Urbanek <em>et al.</em>, 2019] is a text adventure game research platform for training agents that can both talk and act. For our experiments, we developed a large map to serve as a sandbox. Our agent can be tested effectively in this game due to the large range of actions available to the player. These actions include interacting with objects and NPC characters and navigating through the map. All actions are optional, and the game is terminated once the agent reaches a designated goal location.</p>
<p>Urbanek [2019] utilized generative and retrieval models to allow NPC characters to exchange dialogue with players. However, we do not leverage these models and restrict the agent’s interactions with NPC characters to actions as the addition of dialogue adds unnecessary complexity that is not needed for our experiments.</p>
<p>We design a role-playing game, “gold”, where the player can play different personas, collect items, and then finally “go to Meadow”. The player starts out in the Simple Town and has the option to visit various locations in the town such as the wealthy area of town, the Sermon Hall or the Armory and collect different items at these locations. The player has multiple routes they can take to reach the Meadow and can choose to engage in optional activities such as visiting the Armory, or picking up old prayer books in the Sermon Hall. The layout of the game is shown in Fig. 5.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Layout of game "see doctor".</p>
<p>The game score is only obtained when the player finish the game. When the player enters the <em>Meadow</em>, the game ends with a game score: 5.</p>
<p>Additionally, we also demonstrate that our system is capable of generalizing to other guiding narratives, so long as they are capable of being navigated within the environment. To this end, we provide a fourth guiding narrative with a different goal that is still achievable within the game.</p>
<p><em>Exemplar story</em> of persona Adventurer: "I am a brave Adventurer. I know there is a Dungeon with valuable treasure. I go to the armory and get a sword, a shield, armor and a bow to defend myself. Once I am properly equipped, I go to the Dungeon. I get the gold, jewelry, gold cups and a golden goblet. I then leave through the Meadow."</p>
<p>We designed a persona with optional <em>CS scores</em> for evaluating whether Story Shaping facilitates the expression of role-aligned norms within a fantasy environment. CS scores designed for Section 4.2 are as follows:</p>
<ul>
<li><strong>Score 10:</strong> sword is obtained.</li>
<li><strong>Score 10:</strong> armor is obtained.</li>
<li><strong>Score 10:</strong> shield is obtained.</li>
<li><strong>Score 10:</strong> bow is obtained.</li>
<li><strong>Score 10:</strong> gold is obtained.</li>
<li><strong>Score 10:</strong> jewelry is obtained.</li>
<li><strong>Score 10:</strong> gold cups is obtained.</li>
<li><strong>Score 10:</strong> golden goblet is obtained.</li>
</ul>
<p><em>Exemplar story</em> of persona Thief: "I am a cowardly Thief. I go to the wealthy area of town to search for valuables. I enter the Hillside Manor and get the gold bars there. I stealthily go to the Sermon Hall, and get the small sack of gold. I then leave through the Meadow."</p>
<p>We designed a persona with optional <em>CS scores</em> for evaluating whether Story Shaping facilitates the expression of role-aligned norms within a fantasy environment. CS scores designed for Section 4.2 are as follows:</p>
<ul>
<li><strong>Score 10:</strong> gold bars are obtained.</li>
<li><strong>Score 10:</strong> small sack of gold is obtained.</li>
</ul>
<p><em>Exemplar story</em> of persona Bum: "I am a lazy bum. I wish to do as little as possible to get some coins and leave. I immediately exit the Simple Town. I only stop at the Town Square to get the donations before leaving through the Meadow."</p>
<p>We designed a persona with optional <em>CS scores</em> for evaluating whether Story Shaping facilitates the expression of role-aligned norms within a fantasy environment. CS scores designed for Section 4.2 are as follows:</p>
<ul>
<li><strong>Score 10:</strong> donations are obtained.</li>
</ul>
<p><em>Exemplar story</em> of persona Thug: "I am a Thug. I wish to get revenge on the watch maker who scammed me. I find the watchmaker in the Sermon Hall. I hit the watch maker, and he falls to the floor dead. I then leave through the Meadow." We designed a persona with optional <em>CS scores</em> for evaluating whether Story Shaping facilitates the expression of role-aligned norms within a fantasy environment. CS scores designed for Section 4.2 are as follows:</p>
<ul>
<li><strong>Score 5:</strong> watch maker is hit.</li>
</ul>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 5: Layout of LIGHT Map used in testing</p>
<h2>C ChatGPT</h2>
<p>ChatGPT is a state-of-the-art language generation model developed by OpenAI. It is based on the GPT (Generative Pretrained Transformer) architecture, which uses machine learning to generate human-like text. ChatGPT is trained on a large dataset of conversational text and is able to generate responses to prompts in a wide range of contexts, making it a powerful tool for natural language processing tasks such as chatbots, language translation, and text summarization.</p>
<p>We find that with proper prompting, ChatGPT is able to generate exemplar stories we need for Story Shaping, since stories can leave details about the environment out. In this section, we introduced the prompt we use for automatically generating stories from ChatGPT.</p>
<h3>C.1 Jericho Game: 905</h3>
<p>We use the prompt—"[observation]. What will I do in order to [GOAL]" for prompting ChatGPT. In the game "9:05", we use the prompt: "I wake up in the morning. A bathroom lies to the south, while a door to the east leads to the living room. On the end table are a telephone, a wallet and some keys. The phone rings. What will I do before going to work?" The 3 outputs from ChatGPT are as follows,</p>
<ul>
<li>"I will likely take a shower in the bathroom to the south, get dressed, and check my wallet and keys to make sure I have everything I need for the day. I may also take a cup of coffee before leaving my home to go to work."</li>
<li>"Before going to work, it is likely that you would answer the phone to see who is calling. You may also check your wallet and keys to ensure that you have everything you need for the day. You may also use the bathroom before leaving your home."</li>
<li>Before going to work, I will first answer the phone to see who is calling. Next, I will head to the bathroom to freshen up and brush my teeth. After that, I will consider doing some exercise to start my day off on a healthy note. Finally, I will grab my keys and head out the door to start my day.</li>
</ul>
<p>We convert these generated stories into knowledge graphs with verbAtlas frameworks. Then the agent is trained using Story Shaping with randomly chosen exemplar stories.</p>
<h3>C.2 TextWorld Game: Shopping</h3>
<p>We use the same prompt with Appendix C.1—"[observation]. What will I do in order to [GOAL]?" In the game "shopping", we use the prompt: "I am in front of a mall. There is a bank in the north and a cafe in the south. What will I do before buy clothes?" The outputs from ChatGPT are as follows,</p>
<ul>
<li>"In order to buy clothes at the mall, you can enter the mall and navigate to the store or stores that sell the type of clothing you are interested in purchasing. Many malls have directory listings near the entrances, which can help you find the specific stores you are looking for. Once you have located the store, you can browse and try on the clothing, and then make a purchase at the register."</li>
<li>"In order to buy clothes, I will first check if I have enough money in my bank account by visiting the bank in the north. If my account has sufficient funds, I will</li>
</ul>
<p>proceed to the mall. Once inside the mall, I will have a look around the different clothing stores and see if there are any sales or promotions that interest me. I will also check if there's any specific clothes I was looking for. After browsing through the different stores, I will make a decision on which clothes I want to purchase. Before leaving the mall, I may want to take a break and grab a coffee or a snack from the cafe in the south. Finally, I will make my purchase and head home with my new clothes. "</p>
<ul>
<li>In order to buy clothes, I will first head to the bank in the north to withdraw some cash or check my account balance, if I am low on cash. After that, I will head to the mall, which is in front of me, and enter the mall. I will then look around the mall to find the clothing stores. I may ask a mall employee for directions if needed. Once I find the clothing stores, I will browse through the different options and try on any clothes that catch my interest. Finally, I will make my purchases and head to the cafe in the south for a break and enjoy a cup of coffee.
We then convert these generated stories into knowledge graphs and apply Story Shaping techniques for the RL agents.</li>
</ul>
<h2>C. 3 LIGHT</h2>
<p>We use a series of prompts for the LIGHT environment to provide ChatGPT with context about the environment as well as the ultimate goal due to the complexity of the LIGHT environment compared to Jericho or Textworld. Additionally, as ChatGPT is not allowed to talk about violence, it is not capable of generating an output for the role of 'Thug'. For this reason, it is not included. ChatGPT was accessed on 1/7/2023.</p>
<p>To provide context about the environment, we use the prompt-"I am going to describe a game. Please use this description to answer my next question. In the world of LIGHT, there is a Simple Town. There is a Sermon hall to the east. In the Sermon hall are a "high priest", a "watch maker", a "small sack of gold", a "cross" and "old prayer books". The wealthy area of town is to the north. In the wealthy area of town there is a Hillside manor. In the Hillside manor, there are some "rabbits", a "serving boy", "gold bars", "cloths" and "bottles of liquor". To the north of the Sermon hall is the Town Square. There are "donations" and "chairs" in the Town Square. To the east of the Town Square is the Armory. In the Armory, there is a "sword", a "shield", "armor" and a "bow". To the north of the Town Square is a nearby road. There is a "knight" there. To the west of the nearby road is a Dungeon. In there is "gold", "jewelry", "gold cups" and a "golden goblet". Deeper in the Dungeon is a secret magician's workshop. In the secret magician's workshop is a "master wizard", "servants", "chickens", "ornate tables" and the "granite kingdom seal". Past the secret magician's workshop is a whipping chamber. In there is "coal" and "the wizard's servant". To the north of the nearby road is a Ruined house. In there are "gold and shiny things". To the east of the Ruined house is a Meadow. The game ends when you go here. All relevant NPCs and Objects are in quotes."</p>
<p>The output from ChatGPT is as follows,</p>
<ul>
<li>"What is the objective of the game in LIGHT?"</li>
</ul>
<p>To provide the goal, we use the prompt - "Shortly describe what I can do to get gold." The output from ChatGPT is as follows,</p>
<ul>
<li>"To get gold in the game, you can visit the Sermon hall and find the small sack of gold, visit the wealthy area of town and find the gold bars in the Hillside manor, visit the Dungeon and find gold, jewelry, gold cups, and a golden goblet, and visit the Ruined house and find gold and shiny things. You can also potentially find gold by interacting with the watch maker in the Sermon hall, the knight on the nearby road, and the master wizard in the secret magician's workshop."
To provide the personas, we use the following prompts,</li>
<li>"I am a brave Adventurer. I am unarmed and carrying nothing. Shortly describe which method suits me best. Explicitly say which items I interact with and where I finish the game."</li>
<li>"I am a cowardly and greedy Thief. I am unarmed and carrying nothing. Shortly describe which method suits me best. Explicitly say which items I interact with and where I finish the game."</li>
<li>"I am a lazy Bum. I am unarmed and carrying nothing. Shortly describe which method suits me best. Explicitly say which items I interact with and where I finish the game."
The outputs from ChatGPT are as follows,</li>
<li>"As a brave Adventurer unarmed and carrying nothing, it might be best for you to visit the Armory and obtain a sword, shield, armor, and bow for protection. You can then visit the Dungeon and search for gold, jewelry, gold cups, and a golden goblet to acquire some wealth. After obtaining these items, you can visit the Meadow to finish the game."</li>
<li>"As a cowardly and greedy Thief, the best method for you to get gold in the game would likely be to visit the Sermon hall and find the small sack of gold, and then visit the wealthy area of town and find the gold bars in the Hillside manor. You can also potentially find gold by interacting with the watch maker in the Sermon hall, although it is not specified what items you would need to interact with to obtain the gold in this scenario.
To finish the game, you should avoid the Dungeon and the secret magician's workshop, as there may be danger and potential consequences for stealing from these areas. Instead, you should head directly to the Meadow to the east of the Ruined house, as this is where the game ends."</li>
<li>"It seems that the most suitable method for acquiring gold for a lazy and unarmed player would be to visit the Ruined house, where there are gold and shiny things that can be easily taken without any special equipment or effort. To reach the Ruined house, the player can start at the Simple Town and head north to the Town Square. From there, they can continue north to the nearby road and then turn west to reach the Ruined house.</li>
</ul>
<p>Alternatively, the player could try to acquire gold by completing tasks or quests for NPCs in the game. This might require some effort, but it could potentially yield a larger reward. For example, the player could speak to the knight on the nearby road and see if he has any tasks that need to be completed in exchange for gold. However, it is not specified in the description how the player can interact with the NPCs or what tasks they might offer."
Additionally, we also demonstrate ChatGPT is capable of generating the triples from the target KG for LIGHT. Again, we provide context about the environment with the prompt"I am going to describe a game. Please use this description to answer my next question. In the world of LIGHT, there is a Simple Town. There is a Sermon hall to the east. In the Sermon hall are a "high priest", a "watch maker", a "small sack of gold", a "cross" and "old prayer books". The wealthy area of town is to the north. In the wealthy area of town there is a Hillside manor. In the Hillside manor, there are some "rabbits", a "serving boy", "gold bars", "cloths" and "bottles of liquor". To the north of the Sermon hall is the Town Square. There are "donations" and "chairs" in the Town Square. To the east of the Town Square is the Armory. In the Armory, there is a "sword", a "shield", "armor" and a "bow". To the north of the Town Square is a nearby road. There is a "knight" there. To the west of the nearby road is a Dungeon. In there is "gold", "jewelry", "gold cups" and a "golden goblet". Deeper in the Dungeon is a secret magician's workshop. In the secret magician's workshop is a "master wizard", "servants", "chickens", "ornate tables" and the "granite kingdom seal". Past the secret magician's workshop is a whipping chamber. In there is "coal" and "the wizard's servant". To the north of the nearby road is a Ruined house. In there are "gold and shiny things". To the east of the Ruined house is a Meadow. The game ends when you go here. All relevant NPCs and Objects are in quotes."</p>
<p>The output from ChatGPT is as follows,</p>
<ul>
<li>"The game you described is called "LIGHT" and is set in a town called Simple Town. The town has several locations including a Sermon hall, a wealthy area, a Town Square, an Armory, a nearby road, a Dungeon, a secret magician's workshop, a whipping chamber and a Ruined house. Each location has various NPCs (non-player characters) and objects such as a "high priest", "watch maker", "small sack of gold", "cross", "old prayer books", "rabbits", "serving boy", "gold bars", "cloths", "bottles of liquor", "donations", "chairs", "sword", "shield", "armor", "bow", "gold", "jewelry", "gold cups", "golden goblet", "master wizard", "servants", "chickens", "ornate tables" and the "granite kingdom seal". The game ends when the player reaches the Meadow location."</li>
</ul>
<p>To seed ChatGPT with knowledge about the world in KG form, we use the prompt-"Convert this game to a knowledge graph of comma-separated triples in the form ("object", "in", "location"), ("npc", "is", "alive"), ("npc2", "is", "dead"), or ("you", "has", "object"). Assume all npcs start "alive". Represent this as ("npc", "is", "alive)."</p>
<p>The output from ChatGPT is as follows,</p>
<ul>
<li>"("high priest", "in", "Sermon hall"), ("watch maker", "in", "Sermon hall"), ("small sack of gold", "in", "Sermon hall"), ("cross", "in", "Sermon hall"), ("old prayer books", "in", "Sermon hall"), ("high priest", "is", "alive"), ("watch maker", "is", "alive"), ("rabbits", "in", "Hillside manor"), ("serving boy", "in", "Hillside manor"), ("gold bars", "in", "Hillside manor"), ("cloths", "in", "Hillside manor"), ("bottles of liquor", "in", "Hillside manor"), ("serving boy", "is", "alive"), ("donations", "in", "Town Square"), ("chairs", "in", "Town Square"), ("sword", "in", "Armory"), ("shield", "in", "Armory"), ("armor", "in", "Armory"), ("bow", "in", "Armory"), ("gold", "in", "Dungeon"), ("jewelry", "in", "Dungeon"), ("gold cups", "in", "Dungeon"), ("golden goblet", "in", "Dungeon"), ("master wizard", "in", "secret magician's workshop"), ("servants", "in", "secret magician's workshop"), ("chickens", "in", "secret magician's workshop"), ("ornate tables", "in", "secret magician's workshop"), ("granite kingdom seal", "in", "secret magician's workshop"), ("coal", "in", "whipping chamber"), ("the wizard's servant", "in", "whipping chamber"), ("gold and shiny things", "in", "Ruined house"), ("master wizard", "is", "alive"), ("servants", "is", "alive"), ("the wizard's servant", "is", "alive"), ("knight", "is", "alive")"
To prompt ChatGPT to generate the triples in the target KG, we use the following prompts,</li>
<li>"This is a story that occurs in the same setting. A triple is only changed if the specified object or npc moves to another state. If an action occurs, it is added to the knowledge graph along with its effect. For state, use ("you", relation, object). For action, use ("I", action, object). Use the simplest word you can do describe the action. Say where the player is when the game is finished with ("you", "in", location)
Convert actions to their simplest form. For example, use "get" instead of "grab".
Do not include triples where the action is "go".
Do not include potential actions.
After this story, which triples be added to the knowledge graph?
"As a brave Adventurer unarmed and carrying nothing, it might be best for you to visit the Armory and obtain a sword, shield, armor, and bow for protection. You can then visit the Dungeon and search for gold, jewelry, gold cups, and a golden goblet to acquire some wealth. After obtaining these items, you can visit the Meadow to finish the game.""</li>
<li>"This is a story that occurs in the same setting. A triple is only changed if the specified object or npc moves to another state. If an action occurs, it is added to the knowledge graph along with its effect. For state, use ("you", relation, object). For action, use ("I", action, object). Use the simplest word you can do describe the action. Say where the player is when the game is finished with ("you", "in", location)</li>
</ul>
<p>Convert actions to their simplest form. For example, use "get" instead of "grab".
Do not include triples where the action is "go".
Do not include potential actions.
After this story, which triples be added to the knowledge graph?
"As a cowardly and greedy Thief, the best method for you to get gold in the game would likely be to visit the Sermon hall and find the small sack of gold, and then visit the wealthy area of town and find the gold bars in the Hillside manor. You can also potentially find gold by interacting with the watch maker in the Sermon hall, although it is not specified what items you would need to interact with to obtain the gold in this scenario.
To finish the game, you should avoid the Dungeon and the secret magician's workshop, as there may be danger and potential consequences for stealing from these areas. Instead, you should head directly to the Meadow to the east of the Ruined house, as this is where the game ends.""
$\cdot$ "This is a story that occurs in the same setting. A triple is only changed if the specified object or npc moves to another state. If an action occurs, it is added to the knowledge graph along with its effect. For state, use ("you", relation, object). For action, use ("I", action, object). Use the simplest word you can do describe the action. Say where the player is when the game is finished with ("you", "in", location)
Convert actions to their simplest form. For example, use "get" instead of "grab".
Do not include triples where the action is "go".
Do not include potential actions.
After this story, which triples be added to the knowledge graph?
"It seems that the most suitable method for acquiring gold for a lazy and unarmed player would be to visit the Ruined house, where there are gold and shiny things that can be easily taken without any special equipment or effort. To reach the Ruined house, the player can start at the Simple Town and head north to the Town Square. From there, they can continue north to the nearby road and then turn west to reach the Ruined house.
Alternatively, the player could try to acquire gold by completing tasks or quests for NPCs in the game. This might require some effort, but it could potentially yield a larger reward. For example, the player could speak to the knight on the nearby road and see if he has any tasks that need to be completed in exchange for gold. However, it is not specified in the description how the player can interact with the NPCs or what tasks they might offer."'"</p>
<p>The outputs from ChatGPT are as follows,</p>
<ul>
<li>"("sword", "in", "Armory"), ("shield", "in", "Armory"), ("armor", "in", "Armory"), ("bow", "in", "Armory"), ("gold", "in", "Dungeon"), ("jewelry", "in", "Dun-
geon"), ("gold cups", "in", "Dungeon"), ("golden goblet", "in", "Dungeon")"</li>
<li>"("small sack of gold", "in", "Semon hall") ("gold bars", "in", "Hillside manor")"</li>
<li>"("gold and shiny things", "in", "Ruined house")"</li>
</ul>
<h2>D Human Study</h2>
<h3>D. 1 Commonsense and Social Norm Knowledge</h3>
<p>We first show the instruction to them in Figure 6. We ask participants a set of screen questions to make sure they understand our task. The details can be found in Figure 7 and 8. We conduct our studies using the Cloud Research crowdsourcing platform to interact with Amazon Mechanical Turk [Litman et al., 2017]. Obtaining at least a bachelor's degree and English as their native language are required to take this study. Participants are required to pass screening questions and then explain their preferences of each choice in this study with more than 50 characters, which helps filter out low-quality responses and ensures the validity of the study. Our study was approved by our Institutional Review Board, and we payed participants the equivalent of $\$ 15 / \mathrm{hr}$.</p>
<p>You will be asked to read a game trajectory of an interactive fiction game and then evaluate choices/actions in the game.</p>
<p>Please read this interactive narrative game description carefully!
@. 05 is one of the game in this study.</p>
<h2>Description:</h2>
<p>The story follows a man who wakes up and gets a call telling him to go to work, and it follows branching paths.</p>
<blockquote>
<blockquote>
<p>We trained an AI agent to play the game.
Your job is to read the actions/choices made by AI agent and evaluate the quality of actions by answering 3 questions.</p>
</blockquote>
</blockquote>
<p>You will first read the winning condition: a description and an action. Description is to describe the room or location which the AI agent was in, and action is the agent's choice based on the descriptions.
Then you will read two explanations of the above action. 2 questions will be asked for PAIRWISE COMPARISON:</p>
<ol>
<li>Which sequence of actions expresses more more common sense thinking (with social norm knowledge) on the action choice?</li>
</ol>
<p>If you are the player, which action sequence you will do?
2. Which sequence of actions makes you understand why the agent takes these actions given what you know about the goal?</p>
<p>Which sequence you can understand better, given the goal?
Figure 6: Screenshot of instruction of the human study on evaluating commonsense and social norm knowledge.</p>
<p>We assess whether Story Shaping facilitates the expression of commonsense and social norm knowledge in Section 4.1. We recruited 30 participants on a crowdsourcing platform. Each participant reads the winning goal of a randomly chosen game. Then they read a pair of game transcript which played by Q<em>BERT-S and Q</em>BERT, specifically. Each transcript includes game observations and the corresponding actions. An example is shown in Figure 9.</p>
<p>What is the goal of @:05?</p>
<table>
<thead>
<tr>
<th style="text-align: left;">$\square$ Get treasure</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">$\square$ Escape</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ Go to work</td>
</tr>
<tr>
<td style="text-align: left;">$\square$ Solve puzzles</td>
</tr>
</tbody>
</table>
<p>What is the goal of this survey?
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 7: Screen questions of human study - part 1.</p>
<p>Never played an interactive fiction game? You can play a brief tutorial game here and learn more about Interactive Fiction @Wikipedia.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>After the tutorial, please answer [Only one chance]:</p>
<p>What is the room that the suitcase is in?
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Screen questions of human study - part 2.
For each question, please rate which sequence of actions best fits.</p>
<p>Goal of these actions: buy medicine</p>
<p>Two possible sequence of actions are as follows:
<img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Please write down the reason for your choice on the 2nd question in 30 words</p>
<p>Figure 9: Screenshot of the human study on evaluating commonsense and social norm knowledge.</p>
<h2>D. 2 Role Persona Understanding</h2>
<p>We first show the instruction to them in Figure 10. The same screen questions are used here in Figure 7 and 8. We recruited an additional 29 participants. Participants will read a winning goal and be told the agent's persona (thief, adventurer, thug, bum), then read two game transcripts played by versions of KG-A2C with Story shaping. The details can be found in Figure 11 and 12.</p>
<h2>D. 3 Robustness</h2>
<p>We replicate the human study in Section D.1.</p>
<p>You will be asked to read a game trajectory of an interactive fiction game and then evaluate choices/actions in the game.</p>
<p>Please read this interactive narrative game description carefully!
9:05 is one of the games in this study.
Description:
The story follows a man who wakes up and gets a call telling him to go to work, and it follows branching paths.</p>
<blockquote>
<blockquote>
<p>We trained an Al agent to play the game.
Your job is to read the actions/choices made by Al agent and evaluate the quality of actions by answering 3 questions.</p>
</blockquote>
</blockquote>
<h2>[Survey Details]</h2>
<p>You will first read the winning condition: a description and an action. Description is to describe the room or location which the Al agent was in, and action is the agent's choice based on the descriptions.
Then you will read the character setting of the Al agent.
Setting is to describe the agent's personality. For example, a thief loving gold.</p>
<p>After that, you will read two sequences of actions. You can choose tie (both) or neither.
1 questions will be asked for PAIRWISE COMPARISON:</p>
<ol>
<li>Which sequence of actions makes you feel more confident that the agent made by this character given the setting?</li>
</ol>
<p>You will choose the sequence of actions that you think is made by the character given the setting.
For example, if the setting is a thief, then choose actions with 'steal gold' instead of actions with 'beat dragon'.</p>
<p>Given the goal and setting, which sequence of actions makes you understand it better?</p>
<p>Figure 10: Screenshot of instruction of the human study on evaluating whether Story Shaping has the ability to shape the agent's behaviors in a way that is identifiable to humans.</p>
<p>For each question, please rate which sequence of actions best fits. can choose tie or neither.</p>
<p>Goal of these actions: go to Meadow
Setting of the player: a thief</p>
<p>Two possible sequences of actions are as follows:</p>
<ol>
<li>Which sequence of actions makes you feel more confident that the agent made by this character given the setting.</li>
</ol>
<p>Location: Simple Town
You can see: Nothing
Action $&gt;&gt;$ go east
Location: Salmon Wall
You can see: Gold, a cross, and a book
Action $&gt;&gt;$ go north
Location: Town Square
You can see: donations
Action $&gt;&gt;$ get donations
Action $&gt;&gt;$ go east
Location: Airway
You can see: Oxford, a shield, an armor
Action $&gt;&gt;$ get lowe
Action $&gt;&gt;$ get round
Action $&gt;&gt;$ get amser
Action $&gt;&gt;$ go west
Location: Town Square
You can see: Nothing
Action $&gt;&gt;$ go north
Location: Nearby Road
You can see: a bright
Action $&gt;&gt;$ go north
Location: Ruined House</p>
<p>Figure 11: Screenshot of the human study on evaluating whether Story Shaping has the ability to shape the agent's behaviors in a way that is identifiable to humans - part 1.</p>
<p><img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Please write down the reason for your choice on the 1st question in 30 words</p>
<p>Figure 12: Screenshot of the human study on evaluating whether Story Shaping has the ability to shape the agent's behaviors in a way that is identifiable to humans - part 2.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contributions&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
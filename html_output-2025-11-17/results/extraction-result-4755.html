<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-4755 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-4755</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-4755</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-105.html">extraction-schema-105</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <p><strong>Paper ID:</strong> paper-256358611</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2301.11879v2.pdf" target="_blank">Case-Based Reasoning with Language Models for Classification of Logical Fallacies</a></p>
                <p><strong>Paper Abstract:</strong> The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e4755.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e4755.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR-single-k1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case-Based Reasoning with a single retrieved similar case (k=1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A CBR architecture that retrieves the single most similar past labeled case via LM embeddings (SimCSE cosine similarity), concatenates it with the query, and uses an attention-based adapter + classifier to predict one of 13 logical-fallacy labels.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ELECTRA (also evaluated with BERT and RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based masked-language models used as encoders/backbones; ELECTRA was reported as the best-performing backbone in this paper's experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Case-Based Reasoning (single retrieved similar case)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>similar</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Retrieve the single nearest neighbor case from a case database using LM sentence embeddings (SimCSE + cosine), concatenate query and retrieved case, and let an attention-based adapter produce an adapted representation that a classifier uses to predict the fallacy class. The method leverages close/similar exemplars rather than multiple diverse exemplars.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC (in-domain) and LOGIC Climate (out-of-domain) logical fallacy classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-class classification of natural-language arguments into one of thirteen informal logical fallacy types; LOGIC is in-domain, LOGIC Climate is a more challenging out-of-domain climate-topic benchmark.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>CBR (best configuration) improved over vanilla LM baselines by about 2.5–6 absolute F1 points in-domain and up to ~8 absolute F1 points out-of-domain; ELECTRA + CBR (k=1, 10% case DB) achieved the best reported results in this study (exact per-model F1s in tables of the paper).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Vanilla LM (no retrieved cases) performed worse by 2.5–6 F1; Codex few-shot (13 exemplars in prompt) was also outperformed by CBR (exact Codex F1 not reported in excerpt).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using a single highly similar retrieved case consistently improves LM performance on logical-fallacy classification; CBR with k=1 and attention-based adaptation yields best results and generalizes to out-of-domain data better than the vanilla LM and the Codex few-shot baseline.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Including more retrieved cases (k>1) or using larger raw case databases decreased performance (see other entries). Generic 'explanations' enrichment harmed performance in some settings. Retrieved neighbors' labels show low overlap with true labels (max ~27.7%), indicating the model does not simply copy neighbor labels but uses exemplars indirectly.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4755.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e4755.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CBR-multi-k>1</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case-Based Reasoning with multiple retrieved similar cases (k>1)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Same CBR architecture but retrieving multiple (k=2..5) nearest-neighbor cases and feeding them with the query into the adapter, increasing the amount and variety of past-case information available to the adapter.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ELECTRA (also evaluated with BERT and RoBERTa)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Transformer-based encoders used for retrieval and adaptation; same backbones as single-case experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Case-Based Reasoning (multiple retrieved similar cases)</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Retrieve multiple nearest-neighbor exemplars (k up to 5) to provide a more varied set of past cases; the adapter attention is supposed to weight useful bits from multiple exemplars when constructing an adapted representation for classification.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC and LOGIC Climate logical fallacy classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Multi-class classification into thirteen fallacy types (see previous entry).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Performance degraded as k increased beyond 1: the CBR framework peaked at k=1 and showed a consistent decrease in F1 when including more retrieved cases (observed across in-domain and out-of-domain settings).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Single-case CBR (k=1) outperformed multi-case CBR; vanilla LM (0 cases) was still worse than k=1 CBR, but multi-case CBR often fell between k=1 and the vanilla baseline or below k=1.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Contrary to an intuition that more exemplars (greater diversity of past cases) should help, adding more retrieved cases harmed performance: models became 'overwhelmed' or distracted by unnecessary details; the best reasoning outcome used a single close exemplar rather than a diverse set.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Increasing k consistently reduced performance; having access to more of the case database (higher DB ratios) similarly decreased performance—suggesting that, for this task and adapter design, diversity via more exemplars is detrimental.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4755.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e4755.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CaseRepresentationEnrichment</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Case representation enrichment (Counterarguments, Goals, Explanations, Structure)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Enriching case text with auxiliary, implicitly relevant information (explicit counterarguments, inferred goals, structured abstraction, or free-form explanations) to change which past cases are considered similar and what information is available for adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Used with ELECTRA, BERT, RoBERTa backbones</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Enrichment is a pre-processing/augmentation that modifies the textual representation fed to retriever and adapter; backbones remain transformer LMs.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Representation-conditioned retrieval / CBR with enriched exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Construct alternative case views (append counterargument, goal, explanation, or structural abstraction to the case) and compute similarity between enriched cases so retrieval/adaptation is guided by different semantic dimensions—this can cause retrieval of semantically different (diverse) exemplars relevant to reasoning along a particular dimension.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC and LOGIC Climate logical fallacy classification</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Same 13-way fallacy classification; enrichment intended to provide dimensions of similarity not apparent in plain text.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Representation choice affected F1 by ~6–8% in-domain and ~4–8% out-of-domain across backbones. 'Counterarguments' enrichment provided the largest consistent boost (especially with ELECTRA); 'Structure' and 'Goals' helped for some models/classes; generic 'Explanations' consistently harmed performance.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>Plain text baseline (no enrichment) was outperformed by the best enrichments (counterarguments); explanations-based enrichment performed worse than plain text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Making implicit similarity dimensions explicit changes which exemplars are retrieved and can substantially affect CBR effectiveness: precise, focused representations (counterarguments, structure) help most; broad/generic explanations can confuse the retriever/adapter and reduce performance.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Generic explanation enrichments consistently reduced performance; some fallacy classes are best handled with plain text (five of thirteen classes had plain text outperform all enrichments). Also, semantic distance of retrieved exemplars (semantically far but structurally useful) can both help or confuse depending on representation and task.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4755.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e4755.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Codex-few-shot</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Codex few-shot prompting baseline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A large code-trained LM (Codex) used in a few-shot prompting baseline: prompt contains the list of possible fallacy classes and one labeled example per class (13 exemplars) to elicit the class for a new argument.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Evaluating large language models trained on code.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Codex</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Large language model trained on code + text (Chen et al., 2021); used here in a few-shot prompting setting without fine-tuning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Few-shot prompting with exemplars</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Single-pass prompting that includes multiple labeled exemplars (one per class) in-context to guide the model to map new arguments to fallacy labels; it implicitly uses exemplar diversity in-context but does not perform explicit retrieval/adaptation.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>LOGIC logical fallacy classification (few-shot)</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Predict fallacy class from natural-language argument using in-context exemplars in the prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td>Codex few-shot was outperformed by the CBR method in this study (specific Codex F1 value is not provided in the excerpt), despite Codex being a much larger model.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td>CBR (k=1 with enriched representations) achieved higher F1 than Codex few-shot; exact numeric comparison to Codex is not provided in the provided text.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>A retrieval-and-adapt approach built on smaller backbones (CBR+ELECTRA) can outperform a much larger LM used few-shot (Codex) for logical-fallacy classification, indicating that access to adapted exemplars and appropriate representations can be more effective than in-context exemplars alone for this reasoning task.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td>Exact Codex metrics are not shown in the excerpt; the statement is relative (CBR > Codex few-shot) without exact numbers.</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e4755.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e4755.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models using diverse versus similar reasoning methods to solve reasoning problems, including descriptions of the reasoning methods, tasks, performance, and any comparisons or findings about the effectiveness of diverse versus similar reasoning.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chain-of-Thought (related mention)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chain-of-thought prompting (eliciting intermediate reasoning steps)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A prompting technique that encourages large LMs to generate intermediate step-by-step reasoning chains (rationales) to solve complex tasks, cited in related work but not evaluated in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Chain of thought prompting elicits reasoning in large language models</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>Large LMs (e.g., GPT-family as discussed in cited work)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Very large autoregressive LMs that, under chain-of-thought prompting, output multi-step rationales used to improve complex reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_name</strong></td>
                            <td>Chain-of-Thought prompting</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_type</strong></td>
                            <td>diverse</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_method_description</strong></td>
                            <td>Prompt the LM to produce intermediate reasoning steps (a 'chain') before the final answer; diversity can be introduced by sampling multiple chains (e.g., self-consistency) to aggregate answers.</td>
                        </tr>
                        <tr>
                            <td><strong>task_name</strong></td>
                            <td>General complex reasoning benchmarks (cited work), not evaluated here</td>
                        </tr>
                        <tr>
                            <td><strong>task_description</strong></td>
                            <td>Benchmarks requiring multi-step reasoning where chain-of-thought has been shown to help (e.g., arithmetic/logic reasoning tasks in literature).</td>
                        </tr>
                        <tr>
                            <td><strong>performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_with_other_method</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>performance_other_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as a promising direction in related work; this paper does not run chain-of-thought experiments but situates CBR as an alternate approach that provides example-based reasoning and explainability.</td>
                        </tr>
                        <tr>
                            <td><strong>counter_examples_or_negative_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Logical fallacy detection <em>(Rating: 2)</em></li>
                <li>Chain of thought prompting elicits reasoning in large language models <em>(Rating: 2)</em></li>
                <li>Retrieval-augmented generation for knowledge-intensive NLP tasks <em>(Rating: 2)</em></li>
                <li>Knowledge base question answering by case-based reasoning over subgraphs <em>(Rating: 2)</em></li>
                <li>Evaluating large language models trained on code. <em>(Rating: 1)</em></li>
                <li>SimCSE: Simple contrastive learning of sentence embeddings <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-4755",
    "paper_id": "paper-256358611",
    "extraction_schema_id": "extraction-schema-105",
    "extracted_data": [
        {
            "name_short": "CBR-single-k1",
            "name_full": "Case-Based Reasoning with a single retrieved similar case (k=1)",
            "brief_description": "A CBR architecture that retrieves the single most similar past labeled case via LM embeddings (SimCSE cosine similarity), concatenates it with the query, and uses an attention-based adapter + classifier to predict one of 13 logical-fallacy labels.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ELECTRA (also evaluated with BERT and RoBERTa)",
            "model_description": "Transformer-based masked-language models used as encoders/backbones; ELECTRA was reported as the best-performing backbone in this paper's experiments.",
            "reasoning_method_name": "Case-Based Reasoning (single retrieved similar case)",
            "reasoning_method_type": "similar",
            "reasoning_method_description": "Retrieve the single nearest neighbor case from a case database using LM sentence embeddings (SimCSE + cosine), concatenate query and retrieved case, and let an attention-based adapter produce an adapted representation that a classifier uses to predict the fallacy class. The method leverages close/similar exemplars rather than multiple diverse exemplars.",
            "task_name": "LOGIC (in-domain) and LOGIC Climate (out-of-domain) logical fallacy classification",
            "task_description": "Multi-class classification of natural-language arguments into one of thirteen informal logical fallacy types; LOGIC is in-domain, LOGIC Climate is a more challenging out-of-domain climate-topic benchmark.",
            "performance": "CBR (best configuration) improved over vanilla LM baselines by about 2.5–6 absolute F1 points in-domain and up to ~8 absolute F1 points out-of-domain; ELECTRA + CBR (k=1, 10% case DB) achieved the best reported results in this study (exact per-model F1s in tables of the paper).",
            "comparison_with_other_method": true,
            "performance_other_method": "Vanilla LM (no retrieved cases) performed worse by 2.5–6 F1; Codex few-shot (13 exemplars in prompt) was also outperformed by CBR (exact Codex F1 not reported in excerpt).",
            "key_findings": "Using a single highly similar retrieved case consistently improves LM performance on logical-fallacy classification; CBR with k=1 and attention-based adaptation yields best results and generalizes to out-of-domain data better than the vanilla LM and the Codex few-shot baseline.",
            "counter_examples_or_negative_results": "Including more retrieved cases (k&gt;1) or using larger raw case databases decreased performance (see other entries). Generic 'explanations' enrichment harmed performance in some settings. Retrieved neighbors' labels show low overlap with true labels (max ~27.7%), indicating the model does not simply copy neighbor labels but uses exemplars indirectly.",
            "uuid": "e4755.0"
        },
        {
            "name_short": "CBR-multi-k&gt;1",
            "name_full": "Case-Based Reasoning with multiple retrieved similar cases (k&gt;1)",
            "brief_description": "Same CBR architecture but retrieving multiple (k=2..5) nearest-neighbor cases and feeding them with the query into the adapter, increasing the amount and variety of past-case information available to the adapter.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "ELECTRA (also evaluated with BERT and RoBERTa)",
            "model_description": "Transformer-based encoders used for retrieval and adaptation; same backbones as single-case experiments.",
            "reasoning_method_name": "Case-Based Reasoning (multiple retrieved similar cases)",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Retrieve multiple nearest-neighbor exemplars (k up to 5) to provide a more varied set of past cases; the adapter attention is supposed to weight useful bits from multiple exemplars when constructing an adapted representation for classification.",
            "task_name": "LOGIC and LOGIC Climate logical fallacy classification",
            "task_description": "Multi-class classification into thirteen fallacy types (see previous entry).",
            "performance": "Performance degraded as k increased beyond 1: the CBR framework peaked at k=1 and showed a consistent decrease in F1 when including more retrieved cases (observed across in-domain and out-of-domain settings).",
            "comparison_with_other_method": true,
            "performance_other_method": "Single-case CBR (k=1) outperformed multi-case CBR; vanilla LM (0 cases) was still worse than k=1 CBR, but multi-case CBR often fell between k=1 and the vanilla baseline or below k=1.",
            "key_findings": "Contrary to an intuition that more exemplars (greater diversity of past cases) should help, adding more retrieved cases harmed performance: models became 'overwhelmed' or distracted by unnecessary details; the best reasoning outcome used a single close exemplar rather than a diverse set.",
            "counter_examples_or_negative_results": "Increasing k consistently reduced performance; having access to more of the case database (higher DB ratios) similarly decreased performance—suggesting that, for this task and adapter design, diversity via more exemplars is detrimental.",
            "uuid": "e4755.1"
        },
        {
            "name_short": "CaseRepresentationEnrichment",
            "name_full": "Case representation enrichment (Counterarguments, Goals, Explanations, Structure)",
            "brief_description": "Enriching case text with auxiliary, implicitly relevant information (explicit counterarguments, inferred goals, structured abstraction, or free-form explanations) to change which past cases are considered similar and what information is available for adaptation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "model_name": "Used with ELECTRA, BERT, RoBERTa backbones",
            "model_description": "Enrichment is a pre-processing/augmentation that modifies the textual representation fed to retriever and adapter; backbones remain transformer LMs.",
            "reasoning_method_name": "Representation-conditioned retrieval / CBR with enriched exemplars",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Construct alternative case views (append counterargument, goal, explanation, or structural abstraction to the case) and compute similarity between enriched cases so retrieval/adaptation is guided by different semantic dimensions—this can cause retrieval of semantically different (diverse) exemplars relevant to reasoning along a particular dimension.",
            "task_name": "LOGIC and LOGIC Climate logical fallacy classification",
            "task_description": "Same 13-way fallacy classification; enrichment intended to provide dimensions of similarity not apparent in plain text.",
            "performance": "Representation choice affected F1 by ~6–8% in-domain and ~4–8% out-of-domain across backbones. 'Counterarguments' enrichment provided the largest consistent boost (especially with ELECTRA); 'Structure' and 'Goals' helped for some models/classes; generic 'Explanations' consistently harmed performance.",
            "comparison_with_other_method": true,
            "performance_other_method": "Plain text baseline (no enrichment) was outperformed by the best enrichments (counterarguments); explanations-based enrichment performed worse than plain text.",
            "key_findings": "Making implicit similarity dimensions explicit changes which exemplars are retrieved and can substantially affect CBR effectiveness: precise, focused representations (counterarguments, structure) help most; broad/generic explanations can confuse the retriever/adapter and reduce performance.",
            "counter_examples_or_negative_results": "Generic explanation enrichments consistently reduced performance; some fallacy classes are best handled with plain text (five of thirteen classes had plain text outperform all enrichments). Also, semantic distance of retrieved exemplars (semantically far but structurally useful) can both help or confuse depending on representation and task.",
            "uuid": "e4755.2"
        },
        {
            "name_short": "Codex-few-shot",
            "name_full": "Codex few-shot prompting baseline",
            "brief_description": "A large code-trained LM (Codex) used in a few-shot prompting baseline: prompt contains the list of possible fallacy classes and one labeled example per class (13 exemplars) to elicit the class for a new argument.",
            "citation_title": "Evaluating large language models trained on code.",
            "mention_or_use": "use",
            "model_name": "Codex",
            "model_description": "Large language model trained on code + text (Chen et al., 2021); used here in a few-shot prompting setting without fine-tuning.",
            "reasoning_method_name": "Few-shot prompting with exemplars",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Single-pass prompting that includes multiple labeled exemplars (one per class) in-context to guide the model to map new arguments to fallacy labels; it implicitly uses exemplar diversity in-context but does not perform explicit retrieval/adaptation.",
            "task_name": "LOGIC logical fallacy classification (few-shot)",
            "task_description": "Predict fallacy class from natural-language argument using in-context exemplars in the prompt.",
            "performance": "Codex few-shot was outperformed by the CBR method in this study (specific Codex F1 value is not provided in the excerpt), despite Codex being a much larger model.",
            "comparison_with_other_method": true,
            "performance_other_method": "CBR (k=1 with enriched representations) achieved higher F1 than Codex few-shot; exact numeric comparison to Codex is not provided in the provided text.",
            "key_findings": "A retrieval-and-adapt approach built on smaller backbones (CBR+ELECTRA) can outperform a much larger LM used few-shot (Codex) for logical-fallacy classification, indicating that access to adapted exemplars and appropriate representations can be more effective than in-context exemplars alone for this reasoning task.",
            "counter_examples_or_negative_results": "Exact Codex metrics are not shown in the excerpt; the statement is relative (CBR &gt; Codex few-shot) without exact numbers.",
            "uuid": "e4755.3"
        },
        {
            "name_short": "Chain-of-Thought (related mention)",
            "name_full": "Chain-of-thought prompting (eliciting intermediate reasoning steps)",
            "brief_description": "A prompting technique that encourages large LMs to generate intermediate step-by-step reasoning chains (rationales) to solve complex tasks, cited in related work but not evaluated in this paper.",
            "citation_title": "Chain of thought prompting elicits reasoning in large language models",
            "mention_or_use": "mention",
            "model_name": "Large LMs (e.g., GPT-family as discussed in cited work)",
            "model_description": "Very large autoregressive LMs that, under chain-of-thought prompting, output multi-step rationales used to improve complex reasoning.",
            "reasoning_method_name": "Chain-of-Thought prompting",
            "reasoning_method_type": "diverse",
            "reasoning_method_description": "Prompt the LM to produce intermediate reasoning steps (a 'chain') before the final answer; diversity can be introduced by sampling multiple chains (e.g., self-consistency) to aggregate answers.",
            "task_name": "General complex reasoning benchmarks (cited work), not evaluated here",
            "task_description": "Benchmarks requiring multi-step reasoning where chain-of-thought has been shown to help (e.g., arithmetic/logic reasoning tasks in literature).",
            "performance": null,
            "comparison_with_other_method": false,
            "performance_other_method": null,
            "key_findings": "Cited as a promising direction in related work; this paper does not run chain-of-thought experiments but situates CBR as an alternate approach that provides example-based reasoning and explainability.",
            "counter_examples_or_negative_results": null,
            "uuid": "e4755.4"
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Logical fallacy detection",
            "rating": 2,
            "sanitized_title": "logical_fallacy_detection"
        },
        {
            "paper_title": "Chain of thought prompting elicits reasoning in large language models",
            "rating": 2,
            "sanitized_title": "chain_of_thought_prompting_elicits_reasoning_in_large_language_models"
        },
        {
            "paper_title": "Retrieval-augmented generation for knowledge-intensive NLP tasks",
            "rating": 2,
            "sanitized_title": "retrievalaugmented_generation_for_knowledgeintensive_nlp_tasks"
        },
        {
            "paper_title": "Knowledge base question answering by case-based reasoning over subgraphs",
            "rating": 2,
            "sanitized_title": "knowledge_base_question_answering_by_casebased_reasoning_over_subgraphs"
        },
        {
            "paper_title": "Evaluating large language models trained on code.",
            "rating": 1,
            "sanitized_title": "evaluating_large_language_models_trained_on_code"
        },
        {
            "paper_title": "SimCSE: Simple contrastive learning of sentence embeddings",
            "rating": 1,
            "sanitized_title": "simcse_simple_contrastive_learning_of_sentence_embeddings"
        }
    ],
    "cost": 0.015705999999999998,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Case-Based Reasoning with Language Models for Classification of Logical Fallacies</p>
<p>Zhivar Sourati souratih@isi.edu 
Information Sciences Institute
Marina del Rey
University of Southern California
CAUSA</p>
<p>Department of Computer Science
University of Southern California
Los AngelesCAUSA</p>
<p>Filip Ilievski ilievski@isi.edu 
Information Sciences Institute
Marina del Rey
University of Southern California
CAUSA</p>
<p>Department of Computer Science
University of Southern California
Los AngelesCAUSA</p>
<p>Hông-Ân Sandlin 
Cyber-Defence Campus
armasuisse Science and Technology
Switzerland</p>
<p>Alain Mermoud 
Cyber-Defence Campus
armasuisse Science and Technology
Switzerland</p>
<p>Case-Based Reasoning with Language Models for Classification of Logical Fallacies</p>
<p>The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.</p>
<p>Introduction</p>
<p>The ease and speed of spreading misinformation [Wu et al., 2019;Allcott et al., 2019] and propaganda Barrón-Cedeno et al., 2019] on the Web motivate the need to develop trustworthy technology for understanding novel arguments [Lawrence and Reed, 2020]. Inspired by centuries of philosophical theories [Aristotle, 1989;Locke, 1997;Copi, 1954;Barker, 1965], recent work has proposed the natural language processing (NLP) task of Logical Fallacy Detection. Logical Fallacy Detection goes beyond prior work on binary detection of misinformation and fake news classification, and aims to classify an argument into one of the dozens of fallacy classes. For instance, the argument There is definitely a link between depression and drinking alcoholic drinks. I read about it from Wikipedia belongs to the class Fallacy of Credibility, as the validity of the argument is based on the credibility of the source rather than the argument itself. Here, the focus is on informal fallacies that contain incorrect or irrelevant premises, as opposed to formal fallacies, which have an invalid structure [Aristotle, 1989]. The identification of informal fallacies is challenging for both humans and machines as it requires complex reasoning and also common knowledge about the concepts involved in the fallacy [Hansen, 2020]. To predict the correct fallacy type, the model has to know what Wikipedia is and how it is used in societal discourse, the potential relationship between depression and consuming alcoholic beverages, and also the causal link between the first and second parts of the argument.</p>
<p>The currently dominant NLP paradigm of language models (LMs) has been shown to struggle with reasoning over logical fallacies [Jin et al., 2022] and similar tasks that require complex reasoning Barrón-Cedeno et al., 2019]. As LMs are black boxes, attempts to improve their performance often focus on adapting their input data. Prior work has pointed to the need to include context [Vijayaraghavan and Vosoughi, 2022], simplify the input structure [Jin et al., 2022], or perform special training that considers soft logic [Clark et al., 2021]. However, these ideas have not been successful in classifying logical fallacies yet. Alternatively, methods that leverage reasoning by example, e.g., based on Case-Based Reasoning (CBR), have shown promise in terms of accuracy and explainability for other tasks like question answering [Das et al., 2022], but have not been applied to reason over logical fallacies to date. We conclude that integrating such explainable methods with generalizable LMs provides an unexplored opportunity to reason over logical fallacies.</p>
<p>In this paper, we pursue the question: Does reasoning over examples improve the ability of language models to classify logical fallacies? To answer this question, we develop a method based on the idea of CBR [Aamodt and Plaza, 1994]. We focus on the interpretive problem-solving variant of CBR, which aims to understand novel cases in terms of previous similar cases while not necessarily using the solutions from previous cases directly [Leake, 2001]. We adapt this idea to the task of classifying logical fallacies, by using LMs as backbones when retrieving and adapting prior similar cases. We measure the ability of our models in terms of accuracy and generalizability, and also probe their explainability. The main contributions of this paper are as follows:</p>
<ol>
<li>
<p>We design the first Case-Based Reasoning method for logical fallacy classification to solve new cases based on past similar cases. The framework implements the theory of CBR with state-of-the-art (SOTA) techniques based on language modeling and self-attention.</p>
</li>
<li>
<p>We design four enriched case representations: Counterarguments, Goals, Explanations, and Structure of the argument to allow CBR to retrieve and exploit similar cases based on implicit information, like argument goals. To our knowledge, we are the first who investigate the effect of these case representations on CBR performance.</p>
</li>
<li>
<p>We perform extensive experiments that investigate the impact of CBR against Transformer LM baselines on indomain and out-of-domain settings. We perform ablations to provide insight into the sensitivity of our CBR method on its parameters and investigate the explanations extracted from the model.</p>
</li>
</ol>
<p>We make our code and data available to support future research on logical fallacy classification. 1</p>
<p>Method</p>
<p>CBR [Schank, 1983] is a method that reasons over new cases based on similar past cases with a known label [Aamodt and Plaza, 1994]. Our CBR formulation ( Figure 1) consists of three steps: (1) given a new case, retrieve similar cases from the case database, (2) adapt the fetched similar cases based on the current one, and (3) classify the new case based on the adapted exemplars. In this work, we use LMs as key components in the retriever and the adapter. We opt for this choice because of their strong ability to encode and compute similarity for any natural language input. Retriever finds k similar cases S i (i ∈ {1, ..., k}) to the new case C from a case database. The retriever estimates the similarity between C and S i by encoding each of them with the same LM encoder and computing the cosine similarity of the resulting encodings. The retriever then picks the k cases with top cosine similarities from the database. The new case is concatenated to its similar cases, i.e., S = C ⊕ &lt; SEP &gt; ⊕S 1 ⊕S 2 ⊕...⊕S k and is passed as input to the CBR adapter.</p>
<p>Adapter aims to prioritize the most relevant information from S for reasoning over the new case C. Based on the second step of the pipeline by [Aamodt and Plaza, 1994], after fetching similar cases, it might be the case that only certain retrieved cases would be useful, and therefore, they should be weighted according to their utility for approaching the new case. The fusion of the current case with its previously seen similar problems would give the model the chance to come up with a better representation of the current problem, as well as better abstractions and generalizations for further uses.</p>
<p>The adapter consists of two parts: an encoder and an attention component. The encoder is an LM that takes as an input C and S separately, then outputs their respective embedding representations E C and E S . We use the hidden states of the last layer of the LM as the input embedding. A multi-headed 1 https://github.com/zhpinkman/CBR Figure 1: Three stages of the CBR pipeline. Using the new case C, the retriever finds k similar cases {S1, S2, ..., S k } and creates S = C ⊕ &lt; SEP &gt; ⊕S1 ⊕ S2 ⊕ ... ⊕ S k . The adapter processes both the new case and fetched similar cases and tries to adapt S based on the new case C, and extracts more abstract information from the fusion of the two. Finally, the classifier receives the adapted information and returns the probabilities associated with the new class belonging to each fallacy type. In the example, k = 1. attention component [Vaswani et al., 2017] with H heads selects the most useful information from the similar cases embeddings E S given the embedding of the new case E C . As commonly done in Transformer architectures, the Adapter generates Value and Key vectors from E S and Query vectors from E C . The dot product of the Query and Key vectors, fed through a softmax layer, results in an Attention vector, which indicates the importance of each token in S when generating the adapted vector A. An adapted vector with adjusted attention on its elements is produced by the weighted sum of the Value vectors based on Attention weights. The output of the attention component is A, the adjusted embedding of E S .</p>
<p>Classifier predicts the final class based on the adapter output A. The classifier is designed as a fully connected neural layer with a depth d and an activation function. The objective function of the classifier is the cross-entropy loss. The crossentropy loss is computed over the probabilities that are ex-tracted from C logits that correspond to each of the C classes. Also, during training, the retriever's component weights are frozen, while the adapter and the classifier are trained in an end-to-end fashion.</p>
<p>Overall, our CBR architecture resembles a standard 'vanilla' LM with a classification head but brings the additional benefit of having access to prior relevant labeled cases weighed based on the attention mechanism. 2 We hypothesize that the CBR models bring two benefits over vanilla LMs: (1) the integration of similar labeled cases helps the model analyze the new fallacious argument better and classify it more accurately, and (2) provides explicit insights into the reasoning of the model by yielding similar cases to the current one [Renkl, 2014].</p>
<p>Case Representation</p>
<p>Merely retrieving labeled cases may not be sufficient for reasoning on new cases, as it is unclear what dimensions of similarity their relevance is based on. For instance, two cases may be similar in terms of their explanation, structure, or the goal behind the cases. As these dimensions are implicit and not apparent from the plain text, we make them explicit by enriching the original text of the case with such information. We consider four representations in which the case formulation is enriched with its counterargument, goal, explanation, and structure. As a baseline, we also include the original text without any enrichments. Table 1 illustrates examples of these representations for the sample case There was a thunderstorm with rain therefore I did not finish my homework.</p>
<p>Each of the enrichment strategies r modifies the case representation by concatenating it with additional information, r(case). We introduce a case representation function R(case, r) that concatenates case with additional information r(case) resulting in case ⊕ r(case). These representations modify both the new case C to R(C, r) and cases from the database S i to R(S i , r), and change the cosine similarity to be computed between enriched cases instead of plain text. We next describe the design of the enrichment strategies.</p>
<p>Counterarguments. Counterarguments are common in persuasive writing, where they explain why one's position is stronger than the counterargument and serve as a preemptive action to anticipate and remove any doubts about arguments [Harvey, 2009]. We hypothesize that counterarguments are often implicit in the arguments, and would therefore be useful to be provided directly to the model. For instance, in the argument presented in Table 1, although the plain text claims that the reason for not finishing the homework is the heavy rain, the counterargument points out other reasons for not finishing the homework such as the person being too tired.</p>
<p>Goals. Studies of argumentation often focus on the interplay between the goals that the writer is pursuing and their argumentations [Tracy, 2013]. Thus, when classifying logical fallacies, we expect that it is beneficial to take into account the goals of the arguments. The goal may be entirely missing in the argument's text, or the argument may implicitly hint at the goal. An example of the latter is shown in Table 1, where the phrase therefore I did not finish my homework alludes to the implicit goal of the writer to justify not finishing their homework. As shown in this example, we include an explicit goal statement to fill this gap.</p>
<p>Explanations. By using explanations about logically fallacious arguments, we aim to augment the arguments with a broader notion of information that might be useful for classifying logical fallacies but is not already included in the original argument, such as reasoning steps getting from premises to conclusions of an argument [Barker, 1965].</p>
<p>As we do not impose any restrictions on the explanations, their content may overlap with the previous two representations. Alternatively, explanations may provide different complementary information. Such is the example in Table 1 that discusses the causal relationship between two events that are not actually related. Thus, the explanation acts as a general gap-filling mechanism that can provide any relevant information that is missing in the original argument.</p>
<p>Structure. Tasks like logical fallacy classification involve higher-order relation comprehension that is often based on the structure rather than the content of the argument. In that sense, the semantics of specific entities and concepts in the argument may be misleading to the model. Similarly to [Jin et al., 2022], we hypothesize that focusing on the logical structure of an argument rather than its content is beneficial for the model's performance [Gabbay et al., 2004]. An example of a structural simplification of an argument is presented in Table 1. While this simplification may help the model grasp the case structure more directly, the structure formulation may not detect the implicit causal links between the thunderstorm (X) and the homework (Z).</p>
<p>We extract the enrichment information for a case using a combination of few-shot and zero-shot prompting with two SOTA models: ChatGPT [OpenAI, 2022] and Codex [Chen et al., 2021]. Given a representation strategy r, we prompt ChatGPT to get the representations for a case for five different examples using one template per representation. For instance, we use the template Express the goal of the argument {case} to retrieve the goals of the argument case. The full list of the templates is provided in Appendix. The five obtained examples per representation are used as demonstrations to prompt Codex in a few-shot manner. For a representation strategy r, we use the same demonstrations together with each new case C from our task as input to the Codex model, which yields enrichment information r(case) per case. In this manner, we combine the strong zero-shot ability of the closed-source ChatGPT model with the few-shot generation strength of the Codex model.</p>
<p>Experimental Setup</p>
<p>In this section, we describe the evaluation data and metrics, the baselines we compare to, and the implementation details.</p>
<p>Evaluation Dataset. We use two logical fallacy datasets from [Jin et al., 2022], called LOGIC and LOGIC Climate. The LOGIC dataset includes thirteen logical fallacy types about common topics, namely: Ad Hominem, Ad Populum, Appeal to Emotion, Circular Reasoning, Equivocation, Fallacy of Credibility, Fallacy of Extension, Fallacy of Logic,</p>
<p>Representation Transformed Text</p>
<p>Goals</p>
<p>It's possible that the goal is to explain why the speaker did not finish their homework. The speaker may be trying to convince the listener that they did not finish their homework because of the thunderstorm.</p>
<p>Counterarg.</p>
<p>There are many factors that contribute to a person's ability to complete their homework, and it's not fair to suggest that the thunderstorm was the only factor. It's possible that the person did not finish their homework because they were distracted by the thunderstorm or because they were tired.</p>
<p>Explanations</p>
<p>It presents a causal relationship between two events that might not be actually related.</p>
<p>Structure</p>
<p>There was an X with Y therefore I did not do Z.  [Liu et al., 2019], and ELEC-TRA [Clark et al., 2020]. We apply our CBR method ( §2) on each of these models. As baselines, we use vanilla LMs without a CBR extension. We also compare against Codex in a few-shot setting, with the prompt including all the possible classes as well as one example for each class resulting in thirteen labeled examples in the prompt (discussed more in detail in Appendix). Finally, we include the results of a frequency-based predictor that predicts fallacy classes based on the distribution of fallacy types in the training set.</p>
<p>Implementation details. We use SimCSE [Gao et al., 2021], a transformer-based retriever that is optimized for capturing overall sentence similarity, to compute the similarity between cases ( §2) and also use H = 8 heads for the multiheaded attention component. The depth of our classifier is d = 2. It uses gelu [Hendrycks and Gimpel, 2016] as an activation function. We analyze the performance of our model using k ∈ {1, 2, 3, 4, 5}. To test the generalization of our model with sparser case databases, we experiment with various ratios of the case database within {0.1, 0.4, 0.7, 1.0}.</p>
<p>Results</p>
<p>In this section, we measure the effectiveness of CBR per model and case representation. We further provide ablations that measure the sensitivity of the model to the size of the case database and the number of cases. Finally, we present a qualitative analysis of the explainability of CBR and a thor-
LOGIC LOGIC Climate Model Type P R F1 P R F1
Freq  ough discussion about how retrieved cases help to classify new ones. The Impact of CBR. Table 2 shows the performance of the CBR framework and relevant baselines. For each model, we present the results using the best case representation per model and using k = 1 while exploiting 10% of the case database that we found to yield the best results among all possible combinations.</p>
<p>Overall, the CBR method brings a consistent and noticeable quantitative improvement in the classification of logical fallacies by LMs. For each of the three LMs, CBR outperforms the vanilla baselines by 2.5 -6 absolute F1 points on the in-domain dataset and up to 8 points on the out-of-domain dataset. Furthermore, CBR outperforms Codex, which is utilized in a few-shot setting, despite it being a much larger model. Across the different LMs, ELECTRA is achieving the best score and benefits the most from the CBR framework on the in-domain benchmark, which we attribute to its efficiency of pre-training [Clark et al., 2020]. The same pattern of the superiority of CBR over vanilla LMs can be observed for the other two models with different pre-training procedures and varying numbers of internal parameters. The CBR method notably and consistently improves the performance of the LMs on the out-of-domain (LOGIC Climate) benchmark as well, with ELECTRA performing the best and BERT benefiting the most from CBR. We provide detailed per-class results of our method in the Appendix, demonstrating its ability to improve the accuracy of the baseline for each of the thir-teen fallacy classes. Especially we note that our CBR method is able to increase the performance in the classes with the least examples, such as Equivocation (0 → 0.35) and Fallacy of Extension (0.48 → 0.75).</p>
<p>We conclude that CBR is a general framework that can be applied to any LM and can generalize well to unseen data and to various fallacy classes. The generalization of CBR is in line with prior work that suggests its strong performance on tasks with data sparsity [Das et al., 2020].  Effect of Different Representations. The results in Table 3 confirm our expectation that the case representation plays an important role in the effectiveness of the CBR framework. Depending on the LM used, the performance difference among different case representations ranges from 6 to 8% F1-scores for the in-domain setting and 4 to 8% F1scores for the out-of-domain setting. In general, we observe a boost in performance when enhancing the original representation (text). Counterargument information yields the highest boost, though the impact of the representations varies across models. Using ELECTRA, the enrichment with counterarguments helps the most, outperforming the model based on the original text and the other enrichment strategies. With RoBERTa, goals and structure of the arguments perform on par with text, while with BERT, including information about counterarguments and argument structure outperforms the text representation. As the LMs have been trained with different data and may optimize for different notions of similarity, it is intuitive that the impact of the case representations varies across models. This finding is in line with theoretical work, which discusses that knowledge transfer is strictly guided by the similarity function of the reasoning model [Holyoak and Thagard, 1996]. Meanwhile, using a generic enrichment with explanations performs consistently poorly and harms the model performance, which suggests that the CBR models benefit from more precise case representations.</p>
<p>Effect of the Case Database Size. Next, we investigate the sensitivity of the best-performing CBR model based on ELECTRA to the size of the case database. Figure 2 (left) depicts the performance of this model using different ratios of the case database. The figure shows that the CBR framework consistently outperforms the vanilla LM baseline (with 0% of cases) on in-and out-of-domain settings. This trend stands regardless of the size of the case database, which indicates the low sensitivity of the CBR model to the case database size. However, we note that using 10% of the case database yields the best performance, which indicates that a limited case database offers a better potential of abstraction to CBR. Moreover, comparing the performance of the model using different ratios of the case database, we observe a continuous decrease in the performance using higher percentages of the case database. Having access to too much data makes the model dependent and sensitive to the unnecessary and insignificant details of similar cases retrieved. These observations point us to the data efficiency properties of the CBR framework [Das et al., 2020].</p>
<p>Effect of Different Number of Cases. The performance of the best CBR model that uses ELECTRA with different numbers of cases is illustrated in Figure 2 (right). In both in-domain and out-of-domain settings, we observe a consistent pattern of performance decrease when more cases are taken into account in the reasoning process. The CBR framework reaches its peak performance using only one similar case while once more outperforming the vanilla LM (with 0 cases) in all the settings. This indicates that the models get easily overwhelmed with past information when considering a new case. While intuitively, one would expect that a larger number of cases should help the model analyze a new case better, the reasoner should have the capacity to process all of these past cases. Otherwise, as observed in this experiment, including more cases can have an adverse effect on the reasoner by distracting it rather than helping it.</p>
<p>Case Study on Explainability. A key promise of the CBR framework is its native explainability by cases since its retrieval of similar cases and reasoning over them are integrated into the CBR process. We perform a qualitative analysis of the cases retrieved by CBR to develop a better intuition about its reasoning process. Table 4 illustrates four example cases that the vanilla LM classifies incorrectly. For each case, we show two CBR representations: one leading to a correct prediction and one leading to an incorrect one.</p>
<p>The first example shows the scenario where the original text of a retrieved case does not suffice for the model to reason correctly, despite its topical surface similarity to the input Input Sentence</p>
<p>Enriched Representation for Correct Prediction (representation)</p>
<p>Enriched Representation for Wrong Prediction (representation) (predicted class)</p>
<p>Class</p>
<p>People who don't support the proposed minimum wage increase hate the poor.</p>
<p>There are often multiple perspectives on an issue. It's possible to have a nuanced or balanced view that doesn't align with any side completely. (Counterarg.)</p>
<p>That candidate wants to raise the minimum wage, but they aren't even smart enough to run a business. (Text) (Ad Hominem)</p>
<p>Fallacy of Extension</p>
<p>The house is white; therefore it must be big. X is y; therefore, it is z. (Structure)</p>
<p>The sentence "People who drive big cars hate the environment" presents a generalization about a group of people without sufficient evidence and it relies on oversimplification.(Explanations) (Faulty Generalization)</p>
<p>Fallacy of Logic</p>
<p>Student: You didn't teach us this; we never learned this. Teacher: So, you're either lazy or unwilling to learn is that right?</p>
<p>It's possible that the argument "It's possible to pass the class without attending. so, you will pass even if you don't attend" is trying to convince the listener that they will pass the class even if they don't attend. The speaker may be trying to persuade the listener to skip class. (Goals)</p>
<p>The sentence "Teacher: You are receiving a zero because you didn't do your homework. Students: Are you serious? You gave me a zero because you hate me?" attacks the person making the argument rather than the argument itself. (Explanations) (Fallacy of Extension)</p>
<p>False Dilemma</p>
<p>One day, Megan wore a Donald Duck shirt, and she got an A on her test. Now she wears that shirt every day to class.</p>
<p>There are many factors that contribute to a student's grade, and it's not fair to suggest that the student's past grades are the only factor. It's possible that the student failed the test because they didn't study, or because they were sick. (Counterarg.)</p>
<p>The sentence "Eating five candy bars and drinking two sodas before a test helps me get better grades. I did that and got an A on my last test in history" presents a causal relationship between two events without sufficient evidence to support the claim. (Explanations) (Fallacy of Relevance)</p>
<p>False Causality Table 4: Four examples from different classes in which the CBR model predicts the correct class. For each example, we show a representation that leads to a correct prediction and a representation that still leads to predicting the wrong class. We also show the corresponding wrong class predicted by the second variation of the model.</p>
<p>case. In other words, the high surface similarity of similar cases is confusing the model and forcing it to incorrectly predict the same class that is associated with the retrieved similar case. However, we see that enriching the case with its counterargument helps the CBR model, even though the counterargument is phrased in an abstract manner and is not similar to the new case on the surface. We observe a similar situation with the explanations enrichment in the third example, having high surface similarity between the retrieved case and the new one, where analyzing the argument goals instead helps the model. In the second example, the structure of the argument and the logical depiction of the past cases help the most, while in the fourth example, the counterarguments assist the reasoning of CBR. From the second and the fourth example, we observe that enriching arguments with cases that are semantically far from the new case is confusing for the CBR model, even if their reasoning would be helpful.</p>
<p>In summary, presented examples show that the retrieved cases help the model indirectly by providing CBR with highlevel information (first example), symbolic abstractions (second example), extensive analysis of the writer's goal (third example), and alternative possibilities (fourth example). This brings up a natural question: does CBR performance correlate to class overlap between the current case and retrieved similar cases? In other words, can we label a new case solely based on its k-nearest neighbors' labels? To answer this question, we compute the overlap of retrieved cases' labels with both the true and the predicted label for different case representations (Table 5). We observe a low overlap of a maximum of  27.7% between the retrieved cases' labels and the true labels, which is only slightly better than a frequency-based prediction. Also, centering on the direct effect of retrieved cases on the CBR predictions, the model with the highest class overlap between the retrieved cases and the predicted classes also has the lowest performance (explanations). Meanwhile, the best CBR variants (e.g., counterarguments) do not directly reuse the labels of the retrieved cases. We conclude that while retrieving similar cases provides the CBR models with useful information, this additional evidence influences the model reasoning indirectly and may have adverse effects otherwise. Although CBR, in its simplest form, can act as a k-nearest neighbors algorithm, our results suggest that the neighbors' labels cannot be used blindly, and further reasoning step over the retrieved cases is necessary. We believe that these findings open exciting future research directions that investigate the relationship between case similarity and CBR performance.</p>
<p>Related Work</p>
<p>In this section, we present prior research on logical fallacy classification, CBR, and methods that prompt very large LMs. Logical Fallacy. Prior computational work on logical fallacies has mostly focused on formal fallacies using rulebased systems and theoretical frameworks [Nakpih and Santini, 2020]. Nevertheless, recent work has switched attention to informal logical fallacies and natural language input. Jin et al. propose the task of logical fallacy classification, considering thirteen informal fallacy types and two benchmarks. The authors gather a rich set of arguments containing various logical fallacies from online resources and evaluate the capabilities of large LMs in classifying logical fallacies both in in-domain and out-of-domain settings. Similarly, Goffredo et al. present a dataset of political debates from U.S. Presidential Campaigns and use it to evaluate Transformer LMs. Processing different parts of arguments, such as dialogue's context, they create separate expert models for each part of arguments and train all the models together, from which they report the importance of discussion context in argument understanding. Although LMs have been used to classify logical fallacies, both independently and in an ensemble setting, to our knowledge, no prior work has tried to improve LMs' capabilities to reason over previous cases of logical fallacies encountering a new case nor experimented with enriching the argument representation. We fill this gap by employing CBR with LMs to reason over similar past cases to classify logical fallacies in new cases.</p>
<p>Case-Based Reasoning. Case-Based Reasoning [Schank, 1983] has been a cornerstone of interpretable models in many areas. For instance, researchers have applied CBR over past experiences in mechanical engineering [Qin and Regli, 2003] and medical applications [Oyelade and Ezugwu, 2020]. Case-Based Reasoning has been also used in education, particularly to teach students to recognize fallacies [Spensberger et al., 2022]. Exploiting its interpretable properties, Walia et al. use Case-Based Reasoning as a transparent model for Word Sense Disambiguation, Brüninghaus and Ashley use Case-Based Reasoning for predicting legal cases an interpretable pipeline, while Ford et al. use Case-Based Reasoning to enhance the transparency of classifications made on written digits [Lecun et al., 1998]. Inspired by its advantages, we couple Case-Based Reasoning with LMs, leading to enhanced accuracy and explainability of classifying logical fallacies. To our knowledge, this is the first work that combines CBR with LMs for complex tasks like logical fallacy classification. Nevertheless, there are frameworks that are close to CBR that also have a notion of memory, but cannot serve as replacements, given their restrictions. Analogical reasoning [Gentner and Smith, 2012] methods typically focus on proportions between words or short text sequences and cannot generalize well to unstructured text. K-nearest neighbor methods are a simplified version of CBR that, given our observations, can not perform as well as CBR. Our framework can also be seen broadly as a memory-based model [Weston et al., 2014], however, our proposed formulation that combines CBR and LMs has not been explored before for tasks like logical fallacy classification.</p>
<p>Prompting LMs. The behavior of LMs is dependent on the quality of their inputs. Aiming to create more comprehensive inputs for LMs and assist them in complex reasoning tasks, researchers have attempted to transfer knowledge from very large LMs to smaller ones. Shwartz et al. show that LMs can discover useful contextual information about the question they answer, from another LM. Wang et al. propose an LM pipeline that learns to faithfully reason over prompt-based extracted rationales. Wei et al. explore how generating a series of intermediate reasoning steps using prompting can equip LMs with complex reasoning skills. Inspired by the ability of large LMs to provide relevant information for novel inputs, as well as prior work that performs knowledge distillation from large to smaller LMs [West et al., 2021], we use prompting to enrich the arguments containing logical fallacies. According to [Barker, 1965], logical fallacies are created by transition gaps from premises to conclusions, and we try to enrich the arguments using prompting to cover the gaps. Our method resembles retrieval-augmentation methods [Lewis et al., 2020], yet, our enrichment strategies are novel and have not been explored on such complex tasks.</p>
<p>Conclusions and Future Work</p>
<p>In this paper, we presented a novel method that uses Case-Based Reasoning with LMs to classify logical fallacies. The CBR method reasons over new cases by utilizing past experiences. To do so, the method retrieves the most relevant past cases, adapts them to meet the needs of a new case, and finally classifies the new case using the adjusted information from past cases. We devised four auxiliary case representations that enrich the cases with implicit information about their counterarguments, goals, structure, and explanations. Our results showed that CBR can classify logical fallacies and can leverage past experiences to fill the gaps in LMs. CBR outperformed the LM baselines in all settings and across all thirteen logical fallacy classes. CBR was able to generalize well and transfer its knowledge to out-of-domain setting. The representation of its cases played a key role: enriching cases with counterarguments helped the most, while adding generic explanations harmed the model's performance. Furthermore, CBR models performed best when a small number of cases are provided, but showed low sensitivity to the size of the case database. Finally, our qualitative analysis demonstrated the value of CBR as an interpretable framework that benefits from past similar cases indirectly.</p>
<p>Since our experiments showed that similar cases assist CBR indirectly, future research should further qualify the relationship between the information provided by the retrieved cases and the performance of the model. Moreover, future work should focus on evaluating CBR on other natural language tasks that require abstraction, such as propaganda detection and dialogue modeling. For instance, given a taskoriented dialogue about cooking a new meal, the model may benefit from procedures for cooking similar meals. The application of CBR on such tasks might also inspire additional case enrichment strategies, e.g., that describe the causal relation between text chunks, and point to additional knowledge gaps that CBR needs to fill.</p>
<p>[ Das et al., 2020] </p>
<p>A Dataset</p>
<p>We start with the dataset from [Jin et al., 2022] that is further revised and cleaned by the authors. 3 It consists of thirteen fallacy types presented in Table 6. Due to the imbalance issue with different types of fallacies (see Table 7), we augment the dataset using common text augmentation techniques to have 281 arguments for each fallacy type. We tried back-translation, and substitution of entities in the arguments with their synonymous terms, but finally given more coherent samples created using substitution, we used substitution with synonymous terms using transformer embeddings. In this setting, we use transformer embeddings extracted from RoBERTa [Liu et al., 2019] to replace words with their most similar candidates based on the extracted embeddings.</p>
<p>B Prompts</p>
<p>We use prompting in two scenarios: to predict the fallacy type for an argument using Codex in a few-shot setting, and to extract the case representations using both ChatGPT and Codex combining zero-shot and few-shot prompting.</p>
<p>Extracting Fallacy Types. We use Codex in a few-shot setting to predict fallacy types for arguments and use these predictions as a baseline in our experiments. We do not run the same experiment using ChatGPT due to the limits imposed on its usage. The prompt we use to extract predictions from Codex contains all the thirteen possible classes of fallacies shown as a Python List followed by one example with its correct prediction per class as shown below. classes = ['fallacy of logic', 'circular reasoning', 'appeal to emotion', 'intentional', 'faulty generalization', 'fallacy of extension', 'false dilemma', 'ad populum', 'ad hominem', 'false causality', 'equivocation', 'fallacy of relevance', 'fallacy of credibility'] ---( plain text of the argument correct fallacy type ### ) ×13 plain text of the argument Extracting Case Representations. We use prompting to get case representations in two steps: first, to get limited highquality case representations, we use ChatGPT in a zero-shot manner; second, to extract case representations in high volume for all the arguments in our dataset, we prompt Codex in a few-shot setting including the sample responses extracted from ChatGPT. Table 7: Per-class analysis of the model's performance using different case representations. The last two columns are the number of test data points, and training data points before augmentation.</p>
<p>Representation</p>
<p>Prompt Sample Answer</p>
<p>Explanations</p>
<p>Analyze the argument "guys are messy and unclean, but are really hard workers"</p>
<p>It's not appropriate to make generalizations about people based on their gender. The argument "guys are messy and unclean, but are really hard workers" suggests that all men are messy and unclean, which is not true. Additionally, the argument implies that being messy and unclean is at odds with being a hard worker, which is also not necessarily true. It's important to remember that every person is an individual with their own unique characteristics, and it's not fair to stereotype people or make assumptions about them based on their gender.</p>
<p>Goals</p>
<p>Express the goal of the argument "The two courses I took at UWI were not very interesting. I don't think its a good university."</p>
<p>It's possible that the goal of the sentence "The two courses I took at UWI were not very interesting. I don't think its a good university." is to express the speaker's personal feelings and opinions about their academic experience at The University of the West Indies (UWI). The sentence could be communicating the speaker's dissatisfaction with UWI to someone else, or it may simply be a way for the speaker to process their own thoughts and feelings.</p>
<p>Counterarg.</p>
<p>Represent the counterargument to the argument "four out of five dentists agree that brushing your teeth makes your life meaningful"</p>
<p>There is no evidence to support the claim that brushing your teeth makes your life meaningful. In fact, there are many other factors that contribute to a meaningful life, and it's not fair to suggest that brushing your teeth is a necessary or important factor. results show that the attention mechanism is an essential part of our framework, and the focus on the previous cases should be adjusted so that the best results can be obtained.  Table 9: Performance of the model with and without the attention mechanism located in the Adapter for each case representation. This comparison was done using the best combination of parameters according to our previous experiments. We use one similar case from the case database and also exploit only 0.1 of the case database performing these experiments. Also, among the underlying LMs, we use ELECTRA for this experiment.</p>
<p>Figure 2 :
2Performance of the CBR framework using different ratios of the case database (left) and different numbers of cases (right). The baseline is outlined as the dotted line.</p>
<p>Table 1 :
1One example of different representations for the case There was a thunderstorm with rain therefore I did not finish my homework. LOGIC Climate dataset consists of more challenging examples for the same logical fallacy types on the climate change topic. We use LOGIC for in-domain evaluation and LOGIC Climate for out-of-domain evaluation. As these datasets are severely imbalanced, we augment them using two techniques, i.e., back-translation, and substitution of entities in the arguments with their synonymous terms. This augmentation makes the dataset have 281 arguments for each fallacy type (find a more in-depth discussion of augmentation in Appendix). Note that we do not fine-tune our model on the LOGIC Climate dataset in any of our experiments to evaluate the generalizability of our framework. We model the classification task as a multi-class classification problem and use the customary metrics of weighted precision, recall, and F1-score.Baselines. We consider three different LMs: BERT[Devlin et al., 2018], RoBERTaFallacy of Relevance, False Causality, False Dilemma, Faulty 
Generalization, and Intentional (examples and statistics pro-
vided in Appendix). </p>
<p>Table 2 :
2Comparison of the best results of the CBR framework with vanilla LMs and two external baselines on two benchmarks focusing on both in-domain (LOGIC) and out-of-domain (LOGIC Climate) settings. The best results per model are boldfaced and the overall best results are underlined.</p>
<p>Table 3 :
3Performance of the CBR framework using different case representations. The best results per model are boldfaced and the overall best results are underlined.</p>
<p>Table 5 :
5Overlap of retrieved cases' labels with true labels and predictions of the best CBR model (ELECTRA). We highlight the highest overlaps in bold.</p>
<p>Rajarshi Das, Ameya Godbole, Shehzaad Dhuliawala, Manzil Zaheer, and Andrew McCallum. A simple approach to case-based reasoning in knowledge bases. In Automated Knowledge Base Construction, 2020. [Das et al., 2022] Rajarshi Das, Ameya Godbole, Ankita Naik, Elliot Tower, Manzil Zaheer, Hannaneh Hajishirzi, Robin Jia, and Andrew McCallum. Knowledge base question answering by case-based reasoning over subgraphs. In International Conference on Machine Learning, pages 4777-4793. PMLR, 2022. [Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [Ford et al., 2020] Courtney Ford, Eoin M. Kenny, and Mark T. Keane. Play mnist for me! user studies on the effects of post-hoc, example-based explanations &amp;; error rates on debugging a deep learning, black-box classifier. arXiv preprint arXiv:2009.06349, 2020. [Gabbay et al., 2004] Dov M Gabbay, John Hayden Woods, et al. Handbook of the History of Logic, volume 2009. Elsevier North-Holland, 2004. [Gao et al., 2021] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894-6910, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. [Gentner and Smith, 2012] D. Gentner and L. Smith. Analogical reasoning. In V.S. Ramachandran, editor, Encyclopedia of Human Behavior (Second Edition), pages 130-136. Academic Press, San Diego, second edition edition, 2012. [Goffredo et al., 2022] Pierpaolo Goffredo, Shohreh Haddadan, Vorakit Vorakitphan, Elena Cabrio, and Serena Villata. Fallacious argument classification in political debates. In Thirty-First International Joint Conference on Artificial Intelligence {IJCAI-22}, pages 4143-4149. International Joint Conferences on Artificial Intelligence Organization, 2022. [Hansen, 2020] Hans Hansen. Fallacies. In Edward N. Zalta, editor, The Stanford Encyclopedia of Philosophy. Metaphysics Research Lab, Stanford University, Summer 2020 edition, 2020. [Harvey, 2009] Gordon Harvey. A brief guide to the elements of the academic essay. Harvard College Writing Program, 2009. [Hendrycks and Gimpel, 2016] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.</p>
<p>Table 8 :
8Prompts applied on ChatGPT to extract different representations for each argument (The fixed part of the prompt is underlined).
Our experiments using the framework without the attention mechanism consistently showed sub-optimal performance.
https://github.com/tmakesense/logical-fallacy/tree/main/ dataset-fixed
To get the representations from ChatGPT, we use the prompts shown inTable 8. We proofread the representations manually to verify their correct structure and soundness of the representation. Since we include these responses in the prompt we use in the second pass, we make sure they do not contain any statement about the logical fallacy that an argument contains. The described procedure is done for counterarguments, explanations, and goals. However, for the structure of the arguments, we manually write down the structure of five sample arguments to be included in the prompt for the next step. We define the structure of the argument as a statement in which the content words are replaced with symbols to serve as an abstraction from the plain text. In this representation, the same entities must have the same symbols, and also the named entities and content words that are not helping the model to recognize an argument as a logical fallacy are also replaced with symbols. For instance, the argument People either like coffee or hate it will have a structure as People either like x or hate x.Having five samples from different arguments with their counterarguments, explanations, structure, and goals, to get representations for the rest of the arguments we create a prompt filling it in with five samples developed in the previous stage and prompt Codex in the following way:( prompt: response ### ) × 5 prompt with a new argument:Note that the prompt is the same prompt we used for Chat-GPT inTable 8and the responses are the high-quality responses we extracted from ChatGPT.C Additional AnalysisEffect of Different Representations. To develop a better intuition into the difference between the case representations, we perform a per-class analysis using different representations. The results of this per-class analysis are shown inTable 7. We observe different rankings of case representations within different fallacy types. In five out of thirteen classes, namely, Ad Hominem, Circular Reasoning, Equivocation, Fallacy of Extension, and Faulty Generalization, the plain text of the arguments outperforms the other representations. In these classes, the additional information from external representations is not perceived as useful and is rather distracting to the model, although still helps CBR to outperform vanilla language models. In Ad Populum, Appeal to Emotion, Fallacy of Credibility, Fallacy of Logic, and False Causality, counterarguments turn out to be more effective, which signifies the importance of considering counterarguments when approaching these fallacies. On the other hand, structure of the arguments are helping CBR the most in Fallacy of Relevance, False Dilemma, and Intentional fallacies. This observation aligns well with the fact that these fallacies have a distinguishable look if one looks at them from a more abstract perspective. Although in none of the fallacy types, the goals Fallacy TypeDefinition ExampleAd Hominem Instead of addressing someone's argument or position, you irrelevantly attack the person or some aspect of the person who is making the argument.I don't know how Professor Resnick can be such a hard grader. He's always late for class.Ad PopulumA fallacious argument which is based on affirming that something is real or better because the majority thinks so.An increasing number of people are keeping ferrets as pets, so they must make wonderful companion animals.Appeal to EmotionManipulation of the recipient's emotions in order to win an argument.If you don't buy the warranty for your chromebook, you could find yourself without one for the rest of the schoolyear and having to pay thousands of dollars in expensive repairs.Fallacy of ExtensionAttacking an exaggerated or caricatured version of your opponent's position.Scientist: Evolution explains how animals developed, adapted and diversified over millions of years. Opponent: If we evolved from monkeys, why are there still monkeys? And why don't we have three arms? Wouldn't that give me a competitive advantage?Fallacy of RelevanceThese fallacies appeal to evidence or examples that are not relevant to the argument at hand.Grading this exam on a curve would be the most fair thing to do. After all, classes go more smoothly when the students and the professor are getting along well.IntentionalSome intentional (sometimes subconscious) action/choice to incorrectly support an argument.A woman decides to visit a certain doctor after only asking advice on the best doctors from ONE friend.False CausalityStatement that jumps to a conclusion implying a causal relationship without supporting evidence.Joan is scratched by a cat while visiting her friend. Two days later she comes down with a fever. Joan concludes that the cat's scratch must be the cause of her illness.False DilemmaPresenting only two options or sides when there are many options or sides.Eat great food at our restaurant, or eat sad, boring meals at home.Faulty GeneralizationAn informal fallacy wherein a conclusion is drawn about all or many instances of a phenomenon on the basis of one or a few instances of that phenomenon.ALL teenagers are irresponsible.Fallacy of CredibilityAttempts to disprove an argument by attacking the character of the speaker.I hold a doctorate in theology, have written 12 books, and personally met the Pope. Therefore, when I say that Jesus' favorite snack was raisins dipped in wine, you should believe me.Fallacy of LogicAn error in the logical structure of an argument. If the state can require car seats for small children and infants, they can just as easily require mothers to breast-feed instead of using formula.Circular ReasoningWhen the end of an argument comes back to the beginning without having proven itself.George Bush is a good communicator because he speaks effectively.Equivocation When a key term or phrase in an argument is used in an ambiguous way, with one meaning in one portion of the argument and then another meaning in another portion of the argument.I don't understand why you're saying I broke a promise. I said I'd never speak again to my ex-girlfriend. And I didn't. I just sent her a quick text. of the arguments nor explanations about them can outperform other representations, they are not always the last in the ranking and can outperform the text or counterarguments that are most helpful in general.Alluding to the complexity of fallacy types and the fact that classifying logical fallacies even for great philosophers has always been a challenge culminating in different taxonomies of fallacies[Aristotle, 1989;Barker, 1965;Nakpih and Santini, 2020], it is natural that it is impossible to approach all fallacies using only one point of view or representation. Instead, depending on the fallacy type and the type of logical error that we are dealing with, different perspectives are needed, which we provide using different representations of the arguments.Effect of Attention Mechanism. We investigate the sensitivity of the best-performing CBR model based on ELECTRA to the attention mechanism located in the Adapter.Table 9demonstrates the performance of this model with and without using the attention mechanism. Note that in the case where we don't use the attention mechanism, the embedding of the current case alongside previous cases E S directly gets fed to the classifier. Our results show that the model that uses the attention mechanism consistently outperforms the model that comes without the attention mechanism. This trend can be seen consistently regardless of the case representation. These
Case-based reasoning: Foundational issues, methodological variations, and system approaches. Plaza ; Agnar Aamodt, Enric Aamodt, ; Plaza, Allcott, Research &amp; Politics. 722053168019848554AI CommunicationsReferences [Aamodt and Plaza, 1994] Agnar Aamodt and Enric Plaza. Case-based reasoning: Foundational issues, methodolog- ical variations, and system approaches. AI Communica- tions, 7:39-59, 1994. 1. [Allcott et al., 2019] Hunt Allcott, Matthew Gentzkow, and Chuan Yu. Trends in the diffusion of misin- formation on social media. Research &amp; Politics, 6(2):2053168019848554, 2019.</p>
<p>Aristotle , On sophistical refutations: On Comin to be passing away -on the cosmos v. 3. Loeb Classical Library. LOEB. London, England; New YorkMcgraw-HillAristotle, 1989] Aristotle. On sophistical refutations: On Comin to be passing away -on the cosmos v. 3. Loeb Classical Library. LOEB, London, England, July 1989. [Barker, 1965] Stephen Francis Barker. The Elements of Logic. New York: Mcgraw-Hill, 1965.</p>
<p>Progress in textual case-based reasoning: predicting the outcome of legal cases from text. Barrón-Cedeno, AAAI. Brüninghaus and Ashley56Proppy: Organizing the news based on their propagandistic contentBarrón-Cedeno et al., 2019] Alberto Barrón-Cedeno, Israa Jaradat, Giovanni Da San Martino, and Preslav Nakov. Proppy: Organizing the news based on their propagan- distic content. Information Processing &amp; Management, 56(5):1849-1864, 2019. [Brüninghaus and Ashley, 2006] Stefanie Brüninghaus and Kevin D Ashley. Progress in textual case-based reason- ing: predicting the outcome of legal cases from text. In AAAI, pages 1577-1580, 2006.</p>
<p>ELECTRA: Pre-training text encoders as discriminators rather than generators. arXiv:2107.03374Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'20. the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'20Peter Clark, Oyvind Tafjord, and Kyle Richardson29arXiv preprintICLR. Copi, 1954. Introduction to logic. Philosophyet al., 2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [Clark et al., 2020] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. ELECTRA: Pre-training text encoders as discriminators rather than generators. In ICLR, 2020. [Clark et al., 2021] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI'20, 2021. [Copi, 1954] Irving M. Copi. Introduction to logic. Philoso- phy, 29(110):271-271, 1954.</p>
<p>Fine-grained analysis of propaganda in news article. Martino Da San, Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP). the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP)MIT pressMental leaps: Analogy in creative thoughtDa San Martino et al., 2019] Giovanni Da San Martino, Se- unghak Yu, Alberto Barrón-Cedeno, Rostislav Petrov, and Preslav Nakov. Fine-grained analysis of propaganda in news article. In Proceedings of the 2019 conference on em- pirical methods in natural language processing and the 9th international joint conference on natural language pro- cessing (EMNLP-IJCNLP), pages 5636-5646, 2019. [Holyoak and Thagard, 1996] Keith J Holyoak and Paul Thagard. Mental leaps: Analogy in creative thought. MIT press, 1996.</p>
<p>Logical fallacy detection. [ Jin, Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational Linguistics[Jin et al., 2022] Zhijing Jin, Abhinav Lalwani, Tejas Vaid- hya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan, Rada Mihalcea, and Bernhard Schoelkopf. Log- ical fallacy detection. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 7180- 7198, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.</p>
<p>. Argument Mining: A Survey. Computational Linguistics. Lawrence and Reed, 2020] John Lawrence and Chris Reed454[Lawrence and Reed, 2020] John Lawrence and Chris Reed. Argument Mining: A Survey. Computational Linguistics, 45(4):765-818, 01 2020.</p>
<p>Problem solving and reasoning: Case-based. ; D B Leake, Leake, International Encyclopedia of the Social &amp; Behavioral Sciences. Neil J. Smelser and Paul B. Baltes, editorsLeake, 2001] D.B. Leake. Problem solving and reasoning: Case-based. In Neil J. Smelser and Paul B. Baltes, edi- tors, International Encyclopedia of the Social &amp; Behav- ioral Sciences, pages 12117-12120. Pergamon, Oxford, 2001.</p>
<p>Gradient-based learning applied to document recognition. [ Lecun, Proceedings of the IEEE. 8611[Lecun et al., 1998] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.</p>
<p>Retrieval-augmented generation for knowledge-intensive nlp tasks. Lewis, Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20. the 34th International Conference on Neural Information Processing Systems, NIPS'20Red Hook, NY, USACurran Associates Inc[Lewis et al., 2020] Patrick Lewis, Ethan Perez, Aleksan- dra Piktus, Fabio Petroni, Vladimir Karpukhin, Na- man Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive nlp tasks. In Proceedings of the 34th International Confer- ence on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc.</p>
<p>[ Liu, arXiv:1907.11692A robustly optimized bert pretraining approach. arXiv preprint[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.</p>
<p>An Essay Concerning Human Understanding. Penguin classics. Penguin Classics. Locke ; John Locke, London, EnglandLocke, 1997] John Locke. An Essay Concerning Human Understanding. Penguin classics. Penguin Classics, Lon- don, England, June 1997.</p>
<p>Automated discovery of logical fallacies in legal argumentation. International Journal of Artificial Intelligence &amp; Applications. 112Callistus Ireneous Nakpih and Simone Santini[Nakpih and Santini, 2020] Callistus Ireneous Nakpih and Simone Santini. Automated discovery of logical fallacies in legal argumentation. International Journal of Artificial Intelligence &amp; Applications, 11(2):37-48, mar 2020.</p>
<p>Oyelade and Absalom E. Ezugwu. A case-based reasoning framework for early detection and diagnosis of novel coronavirus. Informatics in Medicine Unlocked. Openai, Chatgpt, 20100395OpenAI, 2022. Oyelade and Ezugwu, 2020[OpenAI, 2022] OpenAI. Chatgpt. https://openai.com/blog/ chatgpt, 2022. Accessed: April 30, 2023. [Oyelade and Ezugwu, 2020] Olaide N. Oyelade and Absa- lom E. Ezugwu. A case-based reasoning framework for early detection and diagnosis of novel coronavirus. Infor- matics in Medicine Unlocked, 20:100395, 2020.</p>
<p>Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. Regli ; Xiaoli Qin, William C Qin, ; Regli ; Alexander Renkl, C Roger, Schank ; Florian, Ingo Spensberger, Sabine Kollar, Pankofer, arXiv:2004.05483Journal of Social Work Education. 173Peter WestarXiv preprintAnalysis and Manufacturing[Qin and Regli, 2003] Xiaoli Qin and William C. Regli. A study in applying case-based reasoning to engineering design: Mechanical bearing design. Artificial Intelli- gence for Engineering Design, Analysis and Manufactur- ing, 17(3):235-252, 2003. [Renkl, 2014] Alexander Renkl. Toward an instructionally oriented theory of example-based learning. Cognitive Sci- ence, 38(1):1-37, 2014. [Schank, 1983] Roger C. Schank. Dynamic Memory: A The- ory of Reminding and Learning in Computers and People. Cambridge University Press, USA, 1983. [Shwartz et al., 2020] Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unsupervised commonsense question answering with self-talk. arXiv preprint arXiv:2004.05483, 2020. [Spensberger et al., 2022] Florian Spensberger, Ingo Kollar, and Sabine Pankofer. Effects of worked examples and external scripts on fallacy recognition skills: a random- ized controlled trial. Journal of Social Work Education, 58(4):622-639, 2022.</p>
<p>TWEETSPIN: Fine-grained Propaganda Detection in Social Media Using Multi-View Representations. Karen Tracy, ; Tracy, Vaswani, arXiv:2201.11903arXiv:1410.3916Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhouthe 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational Linguistics30arXiv preprintSumit Chopra, and Antoine Bordes. Memory networksTracy, 2013] Karen Tracy. Understanding face-to-face in- teraction: Issues linking goals and discourse. Routledge, 2013. [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Sys- tems, volume 30. Curran Associates, Inc., 2017. [Vijayaraghavan and Vosoughi, 2022] Prashanth Vija- yaraghavan and Soroush Vosoughi. TWEETSPIN: Fine-grained Propaganda Detection in Social Media Using Multi-View Representations. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3433-3448, Seattle, United States, July 2022. Association for Computational Linguistics. [Walia et al., 2019] Himdweep Walia, Ajay Rana, and Vi- neet Kansal. Case based interpretation model for word sense disambiguation in gurmukhi. In 2019 9th Interna- tional Conference on Cloud Computing, Data Science &amp; Engineering (Confluence), pages 359-364, 2019. [Wang et al., 2022] PeiFeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. PINTO: Faithful language reasoning using prompt-generated rationales. In Workshop on Trustworthy and Socially Responsible Ma- chine Learning, NeurIPS 2022, 2022. [Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuur- mans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large lan- guage models. arXiv preprint arXiv:2201.11903, 2022. [West et al., 2021] Peter West, Chandrasekhar Bhagavatula, Jack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language models to commonsense models. In North American Chapter of the Association for Computational Linguistics, 2021. [Weston et al., 2014] Jason Weston, Sumit Chopra, and An- toine Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.</p>
<p>Misinformation in social media: Definition, manipulation, and detection. [ Wu, SIGKDD Explor. Newsl. 212[Wu et al., 2019] Liang Wu, Fred Morstatter, Kathleen M. Carley, and Huan Liu. Misinformation in social media: Definition, manipulation, and detection. SIGKDD Explor. Newsl., 21(2):80-90, nov 2019.</p>            </div>
        </div>

    </div>
</body>
</html>
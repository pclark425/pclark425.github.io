<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1663 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1663</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1663</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-32.html">extraction-schema-32</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <p><strong>Paper ID:</strong> paper-271396631</p>
                <p><strong>Paper Title:</strong> Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot</p>
                <p><strong>Paper Abstract:</strong> : Robots in hazardous environments demand precise and advanced motion control, making extensive simulations crucial for verifying the safety of motion planning. This paper presents a simulation system that enables interactive path editing, allowing for motion planning in a simulated collaborative robot environment and its real-world application. The system includes a simulation host, a control board, and a robot. Unity 3D on a Windows platform provides the simulation environment, while a virtual Linux environment runs ROS2 for execution. Unity sends edited motion paths to ROS2 using the Unity ROS TCP Connector package. The ROS2 MoveIt framework generates trajectories, which are synchronized back to Unity for simulation and real-world validation. To control the six-axis Indy7 collaborative robot, we used the MIO5272 embedded board as an EtherCAT master. Verified trajectories are sent to the target board, synchronizing the robot with the simulation in position and speed. Data are relayed from the host to the MIO5272 using ROS2 and the Data Distribution Service (DDS) to control the robot via EtherCAT communication. The system enables direct simulation and control of various trajectories for robots in hazardous environments. It represents a major advancement by providing safe and optimized trajectories through efficient motion planning and repeated simulations, offering a clear improvement over traditional time-consuming and error-prone teach pendant methods.</p>
                <p><strong>Cost:</strong> 0.017</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1663.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1663.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Indy7 Unity-ROS2 Sim-to-Real</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Interactive Unity-based path editing and ROS2 MoveIt trajectory transfer to Neuromeka Indy7</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A Unity+ROS2 system that lets users interactively plan end-effector paths in a Unity virtual environment, generates time-parameterized trajectories with ROS2 MoveIt, and streams segment-wise joint position/velocity commands to a real Neuromeka Indy7 manipulator controlled by an MIO-5272 embedded board (EtherCAT, Preempt-RT).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>Neuromeka Indy7 (six-axis collaborative robot)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Six-degree-of-freedom collaborative manipulator; controlled via MIO-5272 embedded board running Linux with Preempt-RT and an IgH EtherCAT master; joint encoders measured by EtherCAT and used for closed-loop execution of planned trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>Unity 2022.3.17f1 with ROS-TCP-Connector / ROS-TCP-Endpoint (Unity ↔ ROS2 bridge)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>3D virtual environment built from URDF/xacro robot models imported into Unity; simulates robot kinematics and approximated joint dynamics (Articulation Body) and visual scene for interactive path editing; communicates with ROS2 MoveIt for motion planning and receives time‑parameterized trajectory segments.</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Moderate-fidelity physics: accurate kinematics (URDF) and approximated joint dynamics; not full high-fidelity contact/force or sensor-noise realism.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Robot kinematics (URDF/xacro), joint angular positions and velocities, time-parameterized trajectories, drive stiffness and damping parameters applied to Articulation Body where possible, collision geometry for planning (via MoveIt).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td>Incomplete application of some physical parameters in Unity play mode, limited contact dynamics realism, no explicit modeling of sensor noise, limited or no modeling of actuator latencies beyond the observed communication/timing delays, non-constant simulation update intervals; end-effector orientation editing was not implemented (limited IK solvability).</td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Physical lab setup with a Neuromeka Indy7 manipulator and a control embedded board MIO-5272 (Intel Core i7, Ubuntu with Preempt-RT); real-time control via IgH EtherCAT master; joint states read from encoders; trajectory segments received over DDS/ROS2 and applied at 1 ms control period.</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Execution of planned motion trajectories (path following and obstacle-avoiding maneuvers) generated interactively in simulation and executed on the physical Indy7.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Not a learned policy — classical motion planning: MoveIt framework with OMPL's PRM planner and iterative parabolic time parameterization; trajectories generated deterministically and streamed segment-wise (no reinforcement learning used in experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td>Qualitative and time-series comparison of joint velocities and temporal synchronization across three traces: MoveIt-generated trajectory (reference), Unity simulation playback (recorded joint angles), and real robot execution (encoder readings); adherence to planned total trajectory duration (1 minute) and velocity profiles were used as the success measure.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Timing/observation delays between Unity and ROS2, non-constant simulation timestep in Unity causing abrupt velocity changes (jitter) during simulation playback, incomplete application of some physical parameters in Unity play mode, inverse kinematics limitations for some robot postures (orientation not editable), and limited modeling of sensor/actuator noise and detailed contact dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Tight synchronization of trajectory segments via ROS TCP Connector / ROS TCP Endpoint and DDS, real-time control on the embedded MIO-5272 using Preempt-RT and IgH EtherCAT master, use of URDF-based models and MoveIt/Orocos KDL for kinematics/dynamics, and time-parameterized trajectories delivered segment-wise at 1 ms control periods.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td>Authors identify the need to reduce observation delays and improve compute/communication performance between Unity and ROS2 to bridge the reality gap; no quantitative fidelity thresholds were specified.</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Interactive Unity-based path editing combined with ROS2 MoveIt can generate trajectories that are successfully applied to a real Indy7 robot with good adherence to planned timing and velocities; the primary sim-to-real difficulties observed were timing/communication delays and incomplete application of some physical parameters in Unity, which caused transient jitter in the simulation (the real-time embedded controller execution remained stable). Authors recommend improving scheduling, communication, and observation-delay handling and plan to integrate reinforcement learning/domain-randomization approaches in future work to strengthen sim-to-real transfer.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1663.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1663.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Dynamics Randomization (Peng et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Sim-to-real transfer of robotic control with dynamics randomization</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited method proposing randomizing simulator dynamics parameters during training to bridge the sim-to-real gap and produce controllers robust to real-world dynamics differences.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Sim-to-real transfer of robotic control with dynamics randomization</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>simulated robotic controllers (general, as cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Controllers or policies trained in simulation with randomized dynamics parameters to improve robustness to real-world variations (procedural description as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>general robotics manipulation (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator whose dynamics parameters are randomized during training (paper cites dynamics randomization concept; implementation details are not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Approach treats simulator fidelity implicitly by randomizing dynamics to compensate for fidelity mismatch; not described quantitatively here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Dynamics parameters (e.g., masses, friction, inertias) are randomized (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Robotic control policies trained in sim to real hardware (general description in related work).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Reinforcement learning (with dynamics randomization) — described in related work context.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td>Randomizing simulator dynamics parameters during training (explicitly cited as the bridging technique).</td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Dynamics mismatch between simulator and reality (cited as the target of randomization).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Randomization of dynamics to expose policies to varied physical parameters so they generalize to real-world variations (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as a simple and effective method (in literature review) to bridge the sim-to-real reality gap by randomizing simulator dynamics during training.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1663.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1663.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Inverse Dynamics Transfer (Levine et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Transfer from simulation to real world through learning deep inverse dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited approach where vision-based manipulation policies trained in simulation are transferred to real robots by learning an inverse dynamics model with real-world data, thereby compensating for dynamics discrepancies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Transfer from simulation to real world through learning deep inverse dynamics model</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>vision-based manipulation policies (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Visuomotor policies trained end-to-end in simulation; inverse dynamics model trained with real data used to adapt simulated policies for real-world execution.</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic manipulation (vision-based)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Vision-enabled simulator for policy training (details not provided in this paper beyond citation).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Not specified here; approach relies on learning an inverse dynamics model with real data to compensate for simulator realism gaps.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Visual observations and simulated dynamics for policy training (as cited); inverse dynamics learned from real data.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Real robot used to collect data to train inverse dynamics model (cited work; details not present in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Vision-based manipulation policies transferred from sim to real via learned inverse dynamics model.</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>End-to-end visuomotor policy training in sim plus supervised learning of inverse dynamics model using real-world data (as stated in paper text).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Dynamics mismatch between sim and real that is addressed by learning an inverse dynamics model with real data (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Gathering real-world data to train inverse dynamics model that adapts simulation-trained policies to the real robot (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td>Learning inverse dynamics model using real-world data for adaptation (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example where combining sim-trained visuomotor policies with a real-data-trained inverse dynamics model enables transfer to real robots.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1663.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1663.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Digital Twin Sim-to-Real (Liu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work that uses a digital twin approach to enable sim-to-real transfer for industrial robot grasping by coupling a high-fidelity virtual model with real robot control and deep RL.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>industrial robot grasping systems (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Industrial manipulators for grasping tasks where a digital twin is used as the simulation/testbed for deep RL training and transfer (details cited but not implemented in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>industrial robot grasping</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td>digital twin (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Digital twin that duplicates real robot and environment and can be updated with sensor data to validate policies before real deployment (as described generically in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Described as digital-twin quality (intended to approximate real system closely) in related work; specifics not provided here.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td>Real-world duplication including geometry and state; intended to integrate sensor updates (cited description), but specifics not implemented in the present paper.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td>Industrial robot / grasping testbed in cited work (not detailed here).</td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Grasping policies trained via deep RL in the digital twin transferred to real robots (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning integrated with digital twin-based simulation (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td>Reality gap addressed by use of digital twin and sensor-update coupling (cited), but specific limiting factors not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td>Use of a digital twin and coupling of simulation to real sensors/data to validate and adapt trajectories/policies (as suggested in the text).</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Mentioned as an example of digital-twin-based sim-to-real transfer for RL-enabled grasping; authors of the present paper point to digital twin approaches as useful for safe validation before real deployment.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1663.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1663.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of sim-to-real transfer for robotic agents, scientific discovery agents, or laboratory automation systems, including details about simulation fidelity, transfer success, and the conditions that enable or hinder skill transfer from virtual to real environments.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Accelerated sim-to-real (Niu et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Accelerated sim-to-real deep reinforcement learning: Learning collision avoidance from human player</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Cited work on accelerating sim-to-real RL by learning collision avoidance informed by human player data (mentioned in related work as an example of sim-to-real approaches).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Accelerated sim-to-real deep reinforcement learning: Learning collision avoidance from human player</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_name</strong></td>
                            <td>collision-avoidance RL agents (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_system_description</strong></td>
                            <td>Agents trained in simulation for collision-avoidance tasks, accelerated by human-player data (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>domain</strong></td>
                            <td>robotic navigation / collision avoidance (cited)</td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>virtual_environment_description</strong></td>
                            <td>Simulator used for RL training with mechanisms to accelerate transfer using human demonstrations (details not provided in this paper).</td>
                        </tr>
                        <tr>
                            <td><strong>simulation_fidelity_level</strong></td>
                            <td>Not specified in this paper; described as sim-to-real deep RL approach in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_modeled</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_aspects_simplified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>real_environment_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>task_or_skill_transferred</strong></td>
                            <td>Collision-avoidance policies (as cited).</td>
                        </tr>
                        <tr>
                            <td><strong>training_method</strong></td>
                            <td>Deep reinforcement learning accelerated by human demonstration (cited).</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success_metric</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_sim</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance_real</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_success</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_used</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>domain_randomization_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>sim_to_real_gap_factors</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_enabling_conditions</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_requirements_identified</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_in_real_world</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fine_tuning_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_fidelity_levels</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Cited as an example of sim-to-real deep RL work that accelerates transfer using human-player data; the present paper references it in discussion of related sim-to-real techniques.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot', 'publication_date_yy_mm': '2024-07'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Sim-to-real transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
                <li>Transfer from simulation to real world through learning deep inverse dynamics model <em>(Rating: 2)</em></li>
                <li>A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping <em>(Rating: 2)</em></li>
                <li>Accelerated sim-to-real deep reinforcement learning: Learning collision avoidance from human player <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1663",
    "paper_id": "paper-271396631",
    "extraction_schema_id": "extraction-schema-32",
    "extracted_data": [
        {
            "name_short": "Indy7 Unity-ROS2 Sim-to-Real",
            "name_full": "Interactive Unity-based path editing and ROS2 MoveIt trajectory transfer to Neuromeka Indy7",
            "brief_description": "A Unity+ROS2 system that lets users interactively plan end-effector paths in a Unity virtual environment, generates time-parameterized trajectories with ROS2 MoveIt, and streams segment-wise joint position/velocity commands to a real Neuromeka Indy7 manipulator controlled by an MIO-5272 embedded board (EtherCAT, Preempt-RT).",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_system_name": "Neuromeka Indy7 (six-axis collaborative robot)",
            "agent_system_description": "Six-degree-of-freedom collaborative manipulator; controlled via MIO-5272 embedded board running Linux with Preempt-RT and an IgH EtherCAT master; joint encoders measured by EtherCAT and used for closed-loop execution of planned trajectories.",
            "domain": "general robotics manipulation",
            "virtual_environment_name": "Unity 2022.3.17f1 with ROS-TCP-Connector / ROS-TCP-Endpoint (Unity ↔ ROS2 bridge)",
            "virtual_environment_description": "3D virtual environment built from URDF/xacro robot models imported into Unity; simulates robot kinematics and approximated joint dynamics (Articulation Body) and visual scene for interactive path editing; communicates with ROS2 MoveIt for motion planning and receives time‑parameterized trajectory segments.",
            "simulation_fidelity_level": "Moderate-fidelity physics: accurate kinematics (URDF) and approximated joint dynamics; not full high-fidelity contact/force or sensor-noise realism.",
            "fidelity_aspects_modeled": "Robot kinematics (URDF/xacro), joint angular positions and velocities, time-parameterized trajectories, drive stiffness and damping parameters applied to Articulation Body where possible, collision geometry for planning (via MoveIt).",
            "fidelity_aspects_simplified": "Incomplete application of some physical parameters in Unity play mode, limited contact dynamics realism, no explicit modeling of sensor noise, limited or no modeling of actuator latencies beyond the observed communication/timing delays, non-constant simulation update intervals; end-effector orientation editing was not implemented (limited IK solvability).",
            "real_environment_description": "Physical lab setup with a Neuromeka Indy7 manipulator and a control embedded board MIO-5272 (Intel Core i7, Ubuntu with Preempt-RT); real-time control via IgH EtherCAT master; joint states read from encoders; trajectory segments received over DDS/ROS2 and applied at 1 ms control period.",
            "task_or_skill_transferred": "Execution of planned motion trajectories (path following and obstacle-avoiding maneuvers) generated interactively in simulation and executed on the physical Indy7.",
            "training_method": "Not a learned policy — classical motion planning: MoveIt framework with OMPL's PRM planner and iterative parabolic time parameterization; trajectories generated deterministically and streamed segment-wise (no reinforcement learning used in experiments).",
            "transfer_success_metric": "Qualitative and time-series comparison of joint velocities and temporal synchronization across three traces: MoveIt-generated trajectory (reference), Unity simulation playback (recorded joint angles), and real robot execution (encoder readings); adherence to planned total trajectory duration (1 minute) and velocity profiles were used as the success measure.",
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": true,
            "domain_randomization_used": false,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Timing/observation delays between Unity and ROS2, non-constant simulation timestep in Unity causing abrupt velocity changes (jitter) during simulation playback, incomplete application of some physical parameters in Unity play mode, inverse kinematics limitations for some robot postures (orientation not editable), and limited modeling of sensor/actuator noise and detailed contact dynamics.",
            "transfer_enabling_conditions": "Tight synchronization of trajectory segments via ROS TCP Connector / ROS TCP Endpoint and DDS, real-time control on the embedded MIO-5272 using Preempt-RT and IgH EtherCAT master, use of URDF-based models and MoveIt/Orocos KDL for kinematics/dynamics, and time-parameterized trajectories delivered segment-wise at 1 ms control periods.",
            "fidelity_requirements_identified": "Authors identify the need to reduce observation delays and improve compute/communication performance between Unity and ROS2 to bridge the reality gap; no quantitative fidelity thresholds were specified.",
            "fine_tuning_in_real_world": false,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": false,
            "fidelity_comparison_results": null,
            "key_findings": "Interactive Unity-based path editing combined with ROS2 MoveIt can generate trajectories that are successfully applied to a real Indy7 robot with good adherence to planned timing and velocities; the primary sim-to-real difficulties observed were timing/communication delays and incomplete application of some physical parameters in Unity, which caused transient jitter in the simulation (the real-time embedded controller execution remained stable). Authors recommend improving scheduling, communication, and observation-delay handling and plan to integrate reinforcement learning/domain-randomization approaches in future work to strengthen sim-to-real transfer.",
            "uuid": "e1663.0",
            "source_info": {
                "paper_title": "Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Dynamics Randomization (Peng et al.)",
            "name_full": "Sim-to-real transfer of robotic control with dynamics randomization",
            "brief_description": "Cited method proposing randomizing simulator dynamics parameters during training to bridge the sim-to-real gap and produce controllers robust to real-world dynamics differences.",
            "citation_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "mention_or_use": "mention",
            "agent_system_name": "simulated robotic controllers (general, as cited)",
            "agent_system_description": "Controllers or policies trained in simulation with randomized dynamics parameters to improve robustness to real-world variations (procedural description as cited).",
            "domain": "general robotics manipulation (cited)",
            "virtual_environment_name": null,
            "virtual_environment_description": "Simulator whose dynamics parameters are randomized during training (paper cites dynamics randomization concept; implementation details are not provided in this paper).",
            "simulation_fidelity_level": "Approach treats simulator fidelity implicitly by randomizing dynamics to compensate for fidelity mismatch; not described quantitatively here.",
            "fidelity_aspects_modeled": "Dynamics parameters (e.g., masses, friction, inertias) are randomized (as cited).",
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": "Robotic control policies trained in sim to real hardware (general description in related work).",
            "training_method": "Reinforcement learning (with dynamics randomization) — described in related work context.",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": true,
            "domain_randomization_details": "Randomizing simulator dynamics parameters during training (explicitly cited as the bridging technique).",
            "sim_to_real_gap_factors": "Dynamics mismatch between simulator and reality (cited as the target of randomization).",
            "transfer_enabling_conditions": "Randomization of dynamics to expose policies to varied physical parameters so they generalize to real-world variations (as cited).",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Mentioned as a simple and effective method (in literature review) to bridge the sim-to-real reality gap by randomizing simulator dynamics during training.",
            "uuid": "e1663.1",
            "source_info": {
                "paper_title": "Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Inverse Dynamics Transfer (Levine et al.)",
            "name_full": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "brief_description": "Cited approach where vision-based manipulation policies trained in simulation are transferred to real robots by learning an inverse dynamics model with real-world data, thereby compensating for dynamics discrepancies.",
            "citation_title": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "mention_or_use": "mention",
            "agent_system_name": "vision-based manipulation policies (cited)",
            "agent_system_description": "Visuomotor policies trained end-to-end in simulation; inverse dynamics model trained with real data used to adapt simulated policies for real-world execution.",
            "domain": "robotic manipulation (vision-based)",
            "virtual_environment_name": null,
            "virtual_environment_description": "Vision-enabled simulator for policy training (details not provided in this paper beyond citation).",
            "simulation_fidelity_level": "Not specified here; approach relies on learning an inverse dynamics model with real data to compensate for simulator realism gaps.",
            "fidelity_aspects_modeled": "Visual observations and simulated dynamics for policy training (as cited); inverse dynamics learned from real data.",
            "fidelity_aspects_simplified": null,
            "real_environment_description": "Real robot used to collect data to train inverse dynamics model (cited work; details not present in this paper).",
            "task_or_skill_transferred": "Vision-based manipulation policies transferred from sim to real via learned inverse dynamics model.",
            "training_method": "End-to-end visuomotor policy training in sim plus supervised learning of inverse dynamics model using real-world data (as stated in paper text).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Dynamics mismatch between sim and real that is addressed by learning an inverse dynamics model with real data (as cited).",
            "transfer_enabling_conditions": "Gathering real-world data to train inverse dynamics model that adapts simulation-trained policies to the real robot (as cited).",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": true,
            "fine_tuning_details": "Learning inverse dynamics model using real-world data for adaptation (cited).",
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Mentioned as an example where combining sim-trained visuomotor policies with a real-data-trained inverse dynamics model enables transfer to real robots.",
            "uuid": "e1663.2",
            "source_info": {
                "paper_title": "Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Digital Twin Sim-to-Real (Liu et al.)",
            "name_full": "A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping",
            "brief_description": "Cited work that uses a digital twin approach to enable sim-to-real transfer for industrial robot grasping by coupling a high-fidelity virtual model with real robot control and deep RL.",
            "citation_title": "A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping",
            "mention_or_use": "mention",
            "agent_system_name": "industrial robot grasping systems (cited)",
            "agent_system_description": "Industrial manipulators for grasping tasks where a digital twin is used as the simulation/testbed for deep RL training and transfer (details cited but not implemented in this paper).",
            "domain": "industrial robot grasping",
            "virtual_environment_name": "digital twin (cited)",
            "virtual_environment_description": "Digital twin that duplicates real robot and environment and can be updated with sensor data to validate policies before real deployment (as described generically in this paper).",
            "simulation_fidelity_level": "Described as digital-twin quality (intended to approximate real system closely) in related work; specifics not provided here.",
            "fidelity_aspects_modeled": "Real-world duplication including geometry and state; intended to integrate sensor updates (cited description), but specifics not implemented in the present paper.",
            "fidelity_aspects_simplified": null,
            "real_environment_description": "Industrial robot / grasping testbed in cited work (not detailed here).",
            "task_or_skill_transferred": "Grasping policies trained via deep RL in the digital twin transferred to real robots (as cited).",
            "training_method": "Deep reinforcement learning integrated with digital twin-based simulation (cited).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": "Reality gap addressed by use of digital twin and sensor-update coupling (cited), but specific limiting factors not detailed in this paper.",
            "transfer_enabling_conditions": "Use of a digital twin and coupling of simulation to real sensors/data to validate and adapt trajectories/policies (as suggested in the text).",
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Mentioned as an example of digital-twin-based sim-to-real transfer for RL-enabled grasping; authors of the present paper point to digital twin approaches as useful for safe validation before real deployment.",
            "uuid": "e1663.3",
            "source_info": {
                "paper_title": "Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot",
                "publication_date_yy_mm": "2024-07"
            }
        },
        {
            "name_short": "Accelerated sim-to-real (Niu et al.)",
            "name_full": "Accelerated sim-to-real deep reinforcement learning: Learning collision avoidance from human player",
            "brief_description": "Cited work on accelerating sim-to-real RL by learning collision avoidance informed by human player data (mentioned in related work as an example of sim-to-real approaches).",
            "citation_title": "Accelerated sim-to-real deep reinforcement learning: Learning collision avoidance from human player",
            "mention_or_use": "mention",
            "agent_system_name": "collision-avoidance RL agents (cited)",
            "agent_system_description": "Agents trained in simulation for collision-avoidance tasks, accelerated by human-player data (as cited).",
            "domain": "robotic navigation / collision avoidance (cited)",
            "virtual_environment_name": null,
            "virtual_environment_description": "Simulator used for RL training with mechanisms to accelerate transfer using human demonstrations (details not provided in this paper).",
            "simulation_fidelity_level": "Not specified in this paper; described as sim-to-real deep RL approach in related work.",
            "fidelity_aspects_modeled": null,
            "fidelity_aspects_simplified": null,
            "real_environment_description": null,
            "task_or_skill_transferred": "Collision-avoidance policies (as cited).",
            "training_method": "Deep reinforcement learning accelerated by human demonstration (cited).",
            "transfer_success_metric": null,
            "transfer_performance_sim": null,
            "transfer_performance_real": null,
            "transfer_success": null,
            "domain_randomization_used": null,
            "domain_randomization_details": null,
            "sim_to_real_gap_factors": null,
            "transfer_enabling_conditions": null,
            "fidelity_requirements_identified": null,
            "fine_tuning_in_real_world": null,
            "fine_tuning_details": null,
            "comparison_across_fidelity_levels": null,
            "fidelity_comparison_results": null,
            "key_findings": "Cited as an example of sim-to-real deep RL work that accelerates transfer using human-player data; the present paper references it in discussion of related sim-to-real techniques.",
            "uuid": "e1663.4",
            "source_info": {
                "paper_title": "Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot",
                "publication_date_yy_mm": "2024-07"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Sim-to-real transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        },
        {
            "paper_title": "Transfer from simulation to real world through learning deep inverse dynamics model",
            "rating": 2,
            "sanitized_title": "transfer_from_simulation_to_real_world_through_learning_deep_inverse_dynamics_model"
        },
        {
            "paper_title": "A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping",
            "rating": 2,
            "sanitized_title": "a_digital_twinbased_simtoreal_transfer_for_deep_reinforcement_learningenabled_industrial_robot_grasping"
        },
        {
            "paper_title": "Accelerated sim-to-real deep reinforcement learning: Learning collision avoidance from human player",
            "rating": 1,
            "sanitized_title": "accelerated_simtoreal_deep_reinforcement_learning_learning_collision_avoidance_from_human_player"
        }
    ],
    "cost": 0.0174345,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot
19 July 2024</p>
<p>Taeho Yoo 
Department of Electrical and Information Engineering
Seoul National University of Science and Technology
01811SeoulRepublic of Korea</p>
<p>Byoung Wook Choi bwchoi@seoultech.ac.kr 0000-0002-2404-7415
Department of Electrical and Information Engineering
Seoul National University of Science and Technology
01811SeoulRepublic of Korea</p>
<p>Interactive Path Editing and Simulation System for Motion Planning and Control of a Collaborative Robot
19 July 2024144554FBA5A9099FEECFD5BCB9018B2010.3390/electronics13142857Received: 18 June 2024 Revised: 16 July 2024 Accepted: 17 July 2024ROS2MoveItUnitymotion planningsimulationcollaborative robot
Robots in hazardous environments demand precise and advanced motion control, making extensive simulations crucial for verifying the safety of motion planning.This paper presents a simulation system that enables interactive path editing, allowing for motion planning in a simulated collaborative robot environment and its real-world application.The system includes a simulation host, a control board, and a robot.Unity 3D on a Windows platform provides the simulation environment, while a virtual Linux environment runs ROS2 for execution.Unity sends edited motion paths to ROS2 using the Unity ROS TCP Connector package.The ROS2 MoveIt framework generates trajectories, which are synchronized back to Unity for simulation and real-world validation.To control the six-axis Indy7 collaborative robot, we used the MIO5272 embedded board as an EtherCAT master.Verified trajectories are sent to the target board, synchronizing the robot with the simulation in position and speed.Data are relayed from the host to the MIO5272 using ROS2 and the Data Distribution Service (DDS) to control the robot via EtherCAT communication.The system enables direct simulation and control of various trajectories for robots in hazardous environments.It represents a major advancement by providing safe and optimized trajectories through efficient motion planning and repeated simulations, offering a clear improvement over traditional time-consuming and error-prone teach pendant methods.</p>
<p>Introduction</p>
<p>In manufacturing, industrial robots perform tasks in routine activities and unhealthy workplaces, primarily relying on the teach pendant GUI and force guidance for path planning [1][2][3].Recently, various types of robots have been applied in multiple fields, performing tasks tailored to each purpose; however, the traditional path planning methods for executing diverse and complex tasks not only take significant time but are prone to errors [4,5].Furthermore, trajectories generated for tasks in hazardous environments must ensure safety, especially as these environments continue to grow alongside the expanding robot market.</p>
<p>To address these challenges, digital twins can simulate robots operating in hazardous environments to predict potential problems.By utilizing a digital twin, the real objects associated with tasks are duplicated to create a virtual environment where virtual models behave the same as the original objects [6][7][8].The virtual environment for the robots is updated based on real-time data obtained from sensors and the simulation results of the generated trajectories are applied to real-time robot control, causing the real-world objects to change correspondingly [9,10].Levine et al. [11] extended the learning of vision-based manipulation policies [12] and trained a corresponding inverse dynamics model using real-world data to transfer policies to a real robot.In this way, digital twin ensures the safety of robots operating in hazardous environments by providing simulation results where problems can be addressed.</p>
<p>In addition to digital twins, robots can simulate complex tasks in a virtual environment and successfully solve them using deep reinforcement learning methods [13,14].Sim-to-real transfer involves pretraining in a virtual environment and then fine-tuning with real-world data [15].The policies developed through transfer learning can be deployed to perform complex tasks with robots in the real world, and can reduce the training and trajectory generation time required by deep reinforcement learning [16].Peng et al. [17] proposed a simple method to bridge the simulation-reality gap by randomizing the dynamics of the simulator during training.High-quality rendering was employed to bridge the reality gap in [18], while ref.[19] developed various locomotion skills for robots by randomizing dynamics models.However, the reality gap, which is the difference between the simulated and real environments, affects learning performance, making it crucial to consider the dynamics and observation delays when implementing the simulation environment.</p>
<p>Robot simulation systems commonly address real-world problems in virtual environments, facilitating the integration or expansion of technologies such as digital twins and the sim-to-real transfer process.Users of these systems should have the flexibility to edit the generated trajectories and consequently control the real-world robot in real-time.In this paper, we present a system that allows users to interactively edit the path of a collaborative robot.The system includes a simulation system for real-world robots.</p>
<p>Numerous studies have used visualization tools to monitor robots as they perform tasks and simulate their interactions with other objects in real time.These simulation and visualization processes demand significant computing power.Robotic simulators are rigorously assessed and chosen based on their capabilities, including 3D model implementation and user interfaces.One study [20] examined the features of Gazebo and Unity for simulating robotic assembly processes in manufacturing.In this research, the UR3 model from Universal Robots was used in the simulator to compare ROS integration, analyze performance, and conduct experimental research.The results showed that both platforms are suitable for robot simulation; however, while Gazebo offers advantages in integration with ROS, it has lower graphic quality compared to Unity.Other research [21] conducted a Systematic Literature Review (SLR) and meta-analysis of simulation platforms for applying AI techniques to Wheeled Mobile Robots (WMR).</p>
<p>In our research, we chose Unity in order to facilitate the convenient integration and expansion of various systems; it allows for the convenient implementation of a 3D virtual environment for robot simulation and enables the simulation of various planned paths and the application of algorithms.</p>
<p>In this paper, we describe a system developed within a Unity-based virtual environment that allows users to interactively edit the path of the six-axis Indy7 robot from Neuromeka [22] using a mouse.In this way, various objects and robots can be duplicated to configure the simulation environment in Unity, and multiple reinforcement learning algorithms can be implemented through ROS2.This allows for the simulation of complex tasks performed by robots in hazardous environments where direct path planning is limited and the efficient generation of multiple trajectories that can be applied to real-world robots.</p>
<p>Several studies have addressed path planning and simulation for articulated robots in virtual environments.In [23], the authors proposed a method using SolidWorks CAD for path planning and simulation of industrial robots in which trajectories for various tasks were generated through a GUI.However, due to its lack of integration with ROS2, its capabilities in path planning and controlling multiple robots are limited.In contrast, our system leverages ROS2 and Unity to generate and simulate robot trajectories, facilitating expansion to various virtual environments and multiple robots.In another previous study [24], a controller was implemented to manage multiple robots using the ROS MoveIt framework [25].In [26], a control interface was developed to manipulate a six-axis articulated robot using a keyboard, with simulations conducted in Rviz.However, the ability to control the angles and velocities of all joints is crucial for complex task planning.Our system transmits trajectory segments generated in the ROS in real time to robots in both real-world and virtual environments, ensuring synchronization with the segment data for control.</p>
<p>In [27], an AR environment was implemented with a 3D model of a six-axis articulated robot.Path planning was achieved using real-world markers and a beam search algorithm.</p>
<p>Another study [28] employed a handheld pointer recognized by a motion capture system for path planning.However, these studies require simulation to verify whether the generated trajectory is suitable for performing tasks.</p>
<p>Our system includes a host device for interactive path editing and simulation as well as an embedded board for controlling the robot.On the host, users can conveniently plan the robot's path within a 3D virtual environment via Unity using a mouse.The user-defined path is sent to ROS2 in a virtual Linux environment via the ROS TCP Connector [29].In ROS2, information about the robot modeled in the virtual environment is available and the motion planning framework MoveIt generates a trajectory consisting of segments that contain the robot's position and speed information.Each segment is transmitted back to Unity in real time through the ROS TCP Endpoint package [30], ensuring that the robot's position and velocity in the simulation are synchronized with the trajectory generated in ROS2.When the user verifies that the simulation result is suitable for task performance, the trajectory data are transferred from ROS2 to the MIO5272 embedded board which controls the Indy7 robot.These trajectory data are delivered in real-time for each segment, synchronizing control of the real-world Indy7 with the position and velocity from the robot simulation in Unity.As a result, the user can directly apply and control the simulation results from the virtual environment on the real-world robot.</p>
<p>The rest of this paper is organized as follows: Section 2 introduces the system architecture and describes the method for implementing interactive path planning in a virtual environment and applying the simulation results to a real-world robot; Section 3 details the process of generating the trajectory of the user-planned path, controlling the six-axis collaborative robot in both virtual and real environments, and comparing the robot's speed in each environment; finally, we summarize the experimental results and conclude the paper.</p>
<p>System Architecture</p>
<p>In this section, we provide a detailed description of the overall architecture and communications of the interactive path editing system.The system consists of a host device and an embedded board.The host device features an Intel Core i5 6600 CPU running at 3.3 GHz, 16 GB of SDRAM, and an NVIDIA GeForce GTX 1050 Ti GPU.The target device is an MIO-5272 embedded board, which contains an Intel Core i7 6600U CPU at 2.6 GHz and 16 GB of SDRAM.The host device uses the Windows operating system, and a VirtualBox virtual machine is employed to run Linux Ubuntu on the same device.The embedded board operates with a single operating system, which is Linux Ubuntu.Table 1 shows the hardware specifications of the interactive path editing system.Unity is a software platform used for developing games and simulations, and is expanding its applications into fields such as robotics and autonomous driving.In this paper, we have implemented Unity 2022.3.17f1, which can be accessed for download in [31].</p>
<p>We employed ROS2 (Robot Operating System 2), a middleware that includes software libraries and tools for building robot applications.On the host device, Unity operates under a Windows environment to facilitate robot path planning and simulation, while ROS2 based on Linux runs within a VirtualBox for trajectory generation.</p>
<p>The embedded board, which controls the real-world Indy7 robot, also uses Linux, consistent with previous studies [32].</p>
<p>Preempt-RT is a set of patches for the Linux kernel aimed at minimizing latency to perform real-time tasks.For real-time control of the Indy7 robot, the Linux kernel on the target device was patched with Preempt-RT.Additionally, an EtherCAT Master was implemented on the target device to implement EtherCAT, which is a network protocol based on Ethernet for real-time distributed control of multiple devices.</p>
<p>The application for trajectory generation uses ROS2 Humble, while the motion control application uses ROS2 Galactic to maintain compatibility with the version developed in previous research.Figure 1 depicts the deployment diagram of the interactive path editing and simulation system.Figure 2 shows the architecture of the robot simulation and control system.The ROS TCP Connector and ROS TCP Endpoint are packages that facilitate the transmission of ROS messages between Unity and ROS.Messages are transmitted over a TCP/IP socket and then converted to C# for Unity scripts [33,34].The user-planned robot path information is transmitted from Unity to ROS2 via the ROS TCP Connector package.</p>
<p>In the system, all transmitted ROS messages are service interfaces.The Unity-based simulation and the ROS2-based motion control application act as clients and send request messages.Conversely, the ROS2 application responsible for trajectory generation through MoveIt functions as the server and sends response messages.In this way, the service interface performs transmission according to the service requested by the client, corresponding to the number of path points or the number of segments in the generated trajectory.For trajectory generation within ROS2, libraries for kinematics, dynamics, and motion planning are utilized.</p>
<p>Orocos is a library that specializes in modeling and computing the kinematics and dynamics of robots [35].It can be integrated with ROS, and is included as a plugin in MoveIt for solving Inverse Kinematics.The Open Motion Planning Library (OMPL) is another crucial tool designed for implementing various sampling-based motion planning algorithms, and can also be integrated with ROS [36].Utilizing OMPL it as a plugin in MoveIt, specific motion planning algorithms from the library can be selected to generate the robot's trajectory.</p>
<p>After generating the trajectory in MoveIt using these libraries, it is sent back to Unity for simulation through the ROS TCP Endpoint.If the simulated trajectory proves suitable for the intended task, the trajectory data can be continuously transmitted in segments from Unity to ROS2 on the MIO-5272 embedded board, which controls the real-world Indy7 robot.The motion control application on the MIO-5272 consists of a Dynamics Node, which computes the kinematics and dynamics of the Indy7 robot using the Orocos KDL library.Additionally, a Control Node communicates with the Indy7 robot using the IgH EtherCAT protocol.</p>
<p>DDS (Data Distribution Service) is a networking middleware used for specifying publishers and subscribers to facilitate communication between nodes.In this system, it is used for node communication between the host and target device in ROS2.The Control Node receives real-time trajectory segments from the host device via DDS communication and controls the Indy7 robot using the IgH EtherCAT Master Core.</p>
<p>Simulation Environment Configuration</p>
<p>In this paper, we duplicated the Indy7 robot in Unity for the robot simulation environment.The 3D modeling files for the robots from Neuromeka are available on GitHub [37] in the xacro format, which details the robot's structure and properties.The xacro files can be converted to URDF (Unified Robot Description Format) using the xacro command.In Unity, URDF files can be imported using the URDF Importer [38].By referencing within the xacro file and applying the xacro command with the appropriate robot type and name parameters, the URDF file is generated and then uploaded along with the corresponding 3D modeling files to the Unity Assets and duplicated the Indy7 robot in the virtual environment using URDF Importer.</p>
<p>As shown in Figure 3, the structure of the Indy7 robot in Unity is organized hierarchically, where each subsequent link is a child of the preceding one.If link0 is the root body, then the robot is not capable of being anchored to the ground when in play mode.To address this, a base ground object must be added and designated as the root body in the Articulation Body component.The coordinates for the robot show the coordinates of the ground; thus, the immovable property is subsequently enabled to fix the Indy7 robot to the base.To make for a more accurate simulation, any physical parameters not represented in the URDF should be set to the Articulation Body component.Based on our experiments with the Indy7 robot [39], we assigned stiffness and damping coefficients to each link.To ensure that these physical parameters were effectively applied, the Drive type was set to 'Force.'However, not all parameters may be successfully applied in play mode due to various constraints.To overcome this limitation, an initialize button was developed to allow for direct modification of the Indy7 robot's properties through a C# script.</p>
<p>Interactive Path Editing</p>
<p>In the virtual environment, users can interactively edit the robot's path tailored to specific tasks.In this system, users can determine the three-dimensional position of the robot's end effector for path planning utilizing the mouse cursor.</p>
<p>Scripts for interactive path planning in Unity provide a ray interface and camera control for specifying path points of the end effector within a 3D virtual environment.The position information of each designated point is stored in a list through the path point list manager and later sent to ROS2 for trajectory generation.To facilitate communication with ROS2, the user control script uses ROS TCP Connector package and a path point converter to transform Unity coordinates for compatibility with ROS2 MoveIt.Additionally, it includes initialization to the first position for controlling the robot in the virtual environment and synchronizing with the velocity of the generated trajectory.User function button scripts offer various features to facilitate path planning and simulation of the robot in Unity as well as position measurement features for experimentation.Figure 4 illustrates the path editing and simulation system implemented in Unity.To specify the end effector's position in Unity, the mouse cursor position corresponding to the camera screen is captured.An invisible ray is created from this position in the direction that the user's camera faces using ScreenPointToRay().The XZ plane coordinates are then obtained by Raycast() on a terrain object.</p>
<p>However, as there is no plane for the projection of the Y coordinate, a vertical plane is created at the previously established X and Z coordinates, oriented facing the user's camera, in order to determine the projected height coordinate.The user sequentially selects the position of each coordinate to determine the position of the robot's end effector.In this system, coordinates are selected in the order (X, Z, Y).To enhance user interaction, a graphical user interface (GUI) is provided displaying lines perpendicular to the X and Z axes that follow the user's mouse cursor.Additionally, a line parallel to the Y axis is displayed which adjusts its length based on the selected height.Figure 5 displays the path planning in virtual environment for Indy7 robot.The starting point of the path is shown in green, the endpoint in red, and all other points in blue.The functions for interactive path editing and simulation of the Indy7 robot in Unity were implemented through C# scripts, and can be accessed via modeled buttons in the virtual environment.To verify the path editing process, the necessary results are displayed to the user via console messages.This system includes buttons for initializing the Indy7 robot, recording positions, and setting path points.Figure 6 shows the user function buttons and position data displayed via console messages in Unity.In the user function buttons, robot variable initialization is shown in yellow, the path point set button in blue, and the robot position recorder in red.The position data of the robot's end effector specified by the user is transmitted to an ROS2 service node via a service interface.This node employs MoveIt's inverse kinematics solver to verify whether the end effector can reach the designated position.If the solver successfully computes the angular positions of each link, these positions are relayed back to Unity through a response message and applied to the Indy7 robot.If the calculation fail, a message is displayed to the user prompting them to reselect the end effector's position in the virtual environment.This process allows the user to interactively plan the path of the robot's end effector by confirming valid positions.Figure 7 illustrates how to obtain inverse kinematics solution for an interactive edited position.</p>
<p>Trajectory Generation and Simulation for Collaborative Robot</p>
<p>To utilize the MoveIt framework for generating the robot's trajectory in ROS2, it is essential to create configuration files specific to MoveIt.The MoveIt Setup Assistant facilitates this by converting URDF or xacro files into a MoveIt configuration package.This configuration package encompasses information about the robot such as self-collisions, virtual joints, planning group, controllers, and kinematic solver.Therefore, we used the MoveIt Setup Assistant to generate the configuration package for Indy7.</p>
<p>When implementing a ROS2 robot application with the MoveIt framework, it is imperative to build the MoveIt configuration package.To confirm successful integration with the MoveIt framework, the demo file generated within the package can be launched.This allows for the visualization of the robot in Rviz2, providing a graphical interface to observe current robot state.Because Indy7 is decribed in xacro files, it is necessary to specify the robot type and name parameters in all launch files in order to ensure accurate setup and operation.</p>
<p>The settings for plugins such as OMPL and kinematics for robot trajectory generation can be configurable in the YAML files within the MoveIt configuration package.Our system utilizes the KDL kinematics plugin provided by the Orocos KDL package and employs the PRM (Probabilistic Roadmap) algorithm to generate trajectories between two points specified by the user.</p>
<p>The MoveIt main package is implemented on a virtual machine for the simulation and control system, utilizing the MoveIt framework and configuration package to validate the end effector's position and generate trajectories.The control interface of the service interface package includes information on path points or segments and is synchronized with the trajectory's velocity, transmitting these data for simulation and control of the Indy7 robot.Various functions of the MoveIt main package can be selected by the user through mode control.Figure 8 illustrates the implemented packages of MoveIt main package and service interface package with the ROS2 MoveIt framework on a virtual machine for trajectory generation.</p>
<p>The point data of the path planned by the user in Unity are transmitted to an ROS2 service node via a service interface.To constrain the robot's end effector position, our system utilizes the kinematic constraint, path constraint, and workspace constraint APIs from MoveIt.Kinematic constraints restrict the position of the robot's end effector or the angular position of each joint, and are applied to each point along the Indy7 robot's path.Path constraints limit the trajectory generation range.We implemented box-shaped constraints between two positions on the robot's end effector path to restrict the end effector's direction of movement.Finally, workspace constraints define the area within which the robot's entire body can operate.We specified a box-shaped workspace encompassing both the Indy7 robot and the two points of the end effector's path.When all constraints between the two points have been imposed, the PRM algorithm in OMPL generates a trajectory that adheres to the specified constraints.This process is repeated for each point to obtain the trajectory of the path for the Indy7 robot.Consequently, both the current point number and total count information for every point in the path planned by the user in Unity are transmitted to the service node via the service interface.</p>
<p>In MoveIt, the time parameterization algorithm can generate the velocities in the trajectory.This algorithm produces the velocities for a single connected trajectory, combining all previously obtained trajectories between each pair of points.In this system, we use the iterative parabolic time parameterization algorithm to generate the velocities in the trajectory.Robots performing complex tasks with time constraints need to synchronize their speed with the task execution time.The trajectory consists of multiple segments; to achieve synchronization, the velocity of all segments is scaled at a constant ratio.For simulation in Unity, starting from the first point of the path, the system transmits the service interface to the service node for each segment.Using the segment and point number information, the angular position and velocity data of each joint of the Indy7 robot are received iteratively and synchronized with time according to the trajectory.Figure 9 depicts the sequence diagram of the trajectory generation and simulation system.</p>
<p>Motion Control for Applying Simulation Result</p>
<p>We controlled the six-axis Indy7 collaborative robot from Neuromeka to apply the user-planned path to a real-world robot.As in previous studies, we used the MIO-5272 embedded board to control the Indy7 robot.In this system, we ported the main task and dynamics task in the motion control application to the ROS2 control node and dynamics node, respectively.Additionally, we implemented functionality in the control node to receive simulation results from the service node on the host device via DDS.Because segment data are transmitted in synchronization with the task's performance time, the robot control application on the MIO-5272 must meet time constraints.Therefore, in order to control the Indy7 robot in real time on the MIO-5272, we implemented the IgH EtherCAT Master using the EtherCAT real-time communication protocol.</p>
<p>When the robot control application starts, the dynamics node uses the Orocos KDL library to solve dynamics problems and sends the data for the Indy7 robot to the control node via the message queue.Even if the service node does not send segments or if the control data field for the robot is empty, the dynamics node continuously sends the current robot position and dynamics data to the control node.Figure 10 illustrates the architecture of the Indy7 control system in MIO-5272 that was implemented for applying the generated trajectory.The path verified through simulation in the virtual environment is transmitted as a service request to control the real-world Indy7 robot in the same manner as in Unity.DDS communication for receiving trajectory segments from the service node operates in parallel with the real-time task for EtherCAT communication, and is synchronized with the time constraints through a semaphore.When the control node receives a segment via a response message, it waits on the semaphore.When the real-time task for EtherCAT communication (running in parallel) completes the robot control, the semaphore is released; similarly, when the real-time robot control tasks for the EtherCAT communication are completed, the semaphore is released.Subsequently, the MIO-5272 embedded board transmits data to the actuators of the Indy7 robot via EtherCAT communication to control the joint angular positions.This process is repeated for each segment along the trajectory provided by the service node.Figure 11 shows the motion control flow for applying the Indy7 robot simulation.</p>
<p>As a result, users can plan the path of the Indy7 robot through the interactive path editing and simulation system, allowing complex tasks to be performed in hazardous environments using a mouse in Unity.Users can simulate the robot in real time through communication with ROS2.Additionally, the robot control application implemented in ROS2 on the embedded board receives trajectory data from the host device and controls the Indy7 robot, enabling the application of simulation results from the virtual environment.</p>
<p>Experiment</p>
<p>In this section, we report the experiments conducted to verify that the trajectory generated by the interactive path editing and simulation system implemented in this paper, showing that it can be applied to real-world robot control.In our experiments, we measured the velocity of the robot's trajectory as it avoided an obstacle, for which we used the Indy7 collaborative robot and a drawer.To replicate the experimental environment, we created models of the robot and obstacle in the same positions in Unity.The user then planned a path to avoid the obstacle using the mouse, and we simulated it to verify the generated trajectory.The MIO-5272 received the trajectory segments from the simulation and controlled the real-world Indy7 robot according to the user-planned path.</p>
<p>To measure the velocity of the Indy7 robot simulated in Unity, we recorded the rotation property of a transform component, which changes according to the angular position of each joint.In the real world, the motion control results of the Indy7 robot are measured using an encoder, with the angular positions of the joints recorded through the EtherCAT module [40].The angular positions of all joints of the Indy7 robot in both simulation and the real world were recorded at a 1 ms period, and the generated trajectories and velocities from MoveIt were compared for each segment.</p>
<p>The task of controlling the Indy7 robot in two environments is performed concurrently with the task of receiving segments from ROS2 MoveIt.In Unity, because the Indy7 robot is not simulated at a constant interval, we implemented a recorder task with a 1 ms period.On the other hand, the motion control task on the MIO-5272 operates at a 1 ms period; therefore, we incorporated a recording process to minimize the impact of multitasking.The same method was used to calculate the elapsed time in both environments.The pseudocode for measuring the angular of the Indy7 robot is provided in Algorithms 1 and 2. The source code for the experiments will be made available in our repository: https://github.com/SeoulTechEmbeddedLab/unity_indy7.git, accessed on 16 July 2024.</p>
<p>In the implemented system, the orientation of the Indy7 robot's end effector is fixed, allowing specification of coordinates in terms of X, Y, and Z, which means that MoveIt cannot solve the inverse kinematics for all positions.Specifically, for the Indy7 robot, inverse kinematics cannot be solved for certain positions of the second joint.This limitation prevented the robot's path from closely following the opposite wall of the obstacle during the experiment.Therefore, in this experiment the path of the Indy7 robot to avoid the obstacle was planned using positions where the inverse kinematics could be successfully solved.</p>
<p>The robot's workspace area can solve the inverse kinematics for all points, and is visualized in the virtual environment to handle a wider range of positions.In future research, we aim to improve this system to ensure that the Indy7 robot's end effector can reach all points within the workspace area.To this end, we will implement a system for interactively editing the orientation of the end effector in order to solve more inverse kinematics problems for the robot.Figure 12 shows the simulation and motion control of the Indy7 robot for the path chosen to avoid the obstacle.In the experiment, the Indy7 robot followed a path in the shape of an alpha symbol that had been interactively planned using a mouse in the virtual environment.In this experiment, we performed both simulation and motion control.We compared the velocity of the Indy7 robot in the simulation and real-world environments, confirming that the segments generated by MoveIt were transmitted at precise times so that all movements were synchronized.Thus, the motion control of the Indy7 robot adhered to the designated trajectory time of one minute set by MoveIt.However, because the simulation of the Indy7 robot and the real-time transmission of the trajectory occur simultaneously on the host device, the motion control time in the simulation often experiences delays, causing the speed to change abruptly in order to maintain positional synchronization.In the first experiment, abrupt changes were simultaneously observed at 11.10 s in joints 3 and 6 and at 23.10 s in joints 1, 2, 4, and 6.In the other experiment, changes were observed at 13.44 s and 15.58 s in joints 1, 4, and 6, at 17.92 s in joints 4 and 6, and at 50.48 s in joints 1, 2, and 5.</p>
<p>In future research, we will seek to smooth the trajectory velocity by considering task scheduling for simulation and segment transmission.On the MIO-5272, where only DDS communication with the host device and motion control of the Indy7 robot are performed, the speed does not experience delays or abrupt changes; as a result, the velocity of the Indy7 robot in real-world motion control and the simulation of the interactively planned path in the virtual environment reasonably followed the speed of the trajectory generated by ROS2 MoveIt.</p>
<p>Conclusions</p>
<p>In this paper, we have developed an interactive path editing and simulation system for motion planning and executed motion control based on the trajectory generated for the Indy7 collaborative robot.The interactive path editing system, implemented in Unity, allows users to plan the path of the Indy7 robot in the virtual environment by specifying X, Y, and Z coordinates using a mouse.The path information is transmitted to the ROS2 operating on a virtual machine on the same device, where the MoveIt motion planning framework generates the trajectory.The trajectory's position and velocity data are transmitted in segments to the host device and MIO-5272 embedded board in real time, where they are applied to the simulation and motion control of the Indy7 robot.</p>
<p>To evaluate the system, we used the interactive path editing system to plan the path of the Indy7 robot using a mouse and measured the velocity of each joint.For the experiment, we simulated tasks involving the end effector avoiding an obstacle and drawing the shape of an alpha symbol.We then applied these simulation results to the robot's motion control.The results showed that the Indy7 robot's velocity during both simulation and motion control closely followed the trajectory generated by ROS2 MoveIt.</p>
<p>Our system can be used to simulate hazardous and complex tasks in the welding and material processing fields and apply them to robots.Additionally, robots equipped with virtual torque sensors and a camera can be simulated in a virtual environment to detect and interact with dynamic obstacles.Based on sensor data obtained during simulations, reinforcement learning in the ROS can be employed to plan trajectories for complex tasks, requiring integration with sensor and camera interfaces.In particular, laser welding for various shapes of medical device parts demands complex trajectories.With this system, users can edit paths to quickly generate and simulate various trajectories and apply them to actual robots.</p>
<p>In future research, we plan to enhance the interactive path editing system to enable planning of the robot's path with specified orientation and joint limits.Additionally, observation delays between the real-world and virtual environments can be significantly improved based on computing performance and communication between Unity and ROS2, helping to bridge the reality gap.Our aim is to control multiple intelligent robots through interactive path editing in a virtual environment enhanced by digital twin technology.Furthermore, we intend to incorporate sim-to-real transfer using reinforcement learning in order to develop an advanced intelligent control and simulation system.</p>
<p>Figure 1 .
1
Figure 1.Deployment diagram of the interactive path editing and simulation system.</p>
<p>Figure 2 .
2
Figure 2. Architecture of the robot simulation and control system.</p>
<p>Figure 3 .
3
Figure 3. Link architecture of the Indy7 robot in Unity.</p>
<p>Figure 4 .
4
Figure 4. Path editing and simulation system in Unity.</p>
<p>Figure 5 .
5
Figure 5. Interactive path planning in the virtual environment for the Indy7 robot.</p>
<p>Figure 6 .
6
Figure 6.User function buttons and position data displayed via console messages in Unity.</p>
<p>Figure 7 .
7
Figure 7. Sequence diagram of solving an interactive edited position.</p>
<p>Figure 8 .
8
Figure 8. Packages for trajectory generation system in ROS2 MoveIt on virtual machine.</p>
<p>Figure 9 .
9
Figure 9. Sequence diagram of trajectory generation and simulation system.</p>
<p>Figure 10 .
10
Figure 10.Architecture of the Indy7 control system in MIO-5272.</p>
<p>Figure 11 .
11
Figure 11.Motion control flow for driving the Indy7 robot.</p>
<p>Figure 13 .
13
Figure 13.Velocities of Indy7 robot trajectory simulation and control result.</p>
<p>Figure 14 .
14
Figure 14.Simulation and motion control of the Indy7 robot moving along the shape of the alpha symbol.</p>
<p>Figure 15 .
15
Figure 15.Velocities of trajectory generated along the shape of alpha symbol and simulation and motion control of the Indy7 robot.</p>
<p>Author Contributions:</p>
<p>Conceptualization, T.Y. and B.W.C.; methodology, T.Y.; software, T.Y.; validation, T.Y.; investigation, T.Y.; resources, B.W.C.; data curation, T.Y.; writing-original draft preparation, T.Y.; writing-review and editing, T.Y. and B.W.C.; visualization, T.Y.; supervision, B.W.C.; project administration, B.W.C.; funding acquisition, B.W.C.All authors have read and agreed to the published version of the manuscript.Funding: This work was financially supported by SeoulTech (Seoul National University of Science and Technology).</p>
<p>Table 1 .
1
Hardware specifications of the interactive path editing system.
Hardware EnvironmentMicroprocessor UnitArchitectureOperating SystemVersionDesktop (host)Intel Core i5 6600x86-64Windows 1022H2Desktop (virtual machine)Intel Core i5 6600x86-64Linux Ubuntu 22.04.46.5.0Embedded board MIO-5272Intel Core i7 6600Ux86-64Linux Ubuntu 20.04.65.15.158-rt76
Data Availability Statement:The relevant data and source code are available upon request from the corresponding author.We compared the velocity of the trajectory generated by ROS2 MoveIt with the simulation and motion control results of the Indy7 robot.The initial joint angles of the Indy7 robot were set to zero degrees in both the virtual and real-world environments, requiring the robot to be controlled to its initial position on the planned path.Therefore, additional steps were taken to initialize the position of the Indy7 robot in both Unity and the MIO-5272.For the experiments, the total simulation and motion control time was set to one minute in order to assess the trajectory's velocity.Figure13illustrates the velocity results of the generated trajectory and the simulation and motion control of the Indy7 robot.Simulation data were obtained using Algorithm 1, while motion control data were obtained through Algorithm 2. Each segment of the trajectory is represented in red, while the simulation and motion control of the Indy7 robot are shown by blue and green lines, respectively.The path taken by the Indy7 robot to avoid the obstacle was restricted due to the inability to solve the inverse kinematics for a specific posture.Therefore, additional experiments were conducted to measure the velocity of all joints in the Indy7's motion control.In the virtual environment, the path of the Indy7 robot's end effector was planned to follow the shape of the alpha symbol.After generating the trajectory, both simulation and motion control were performed to measure the velocity of all joints.Figure14shows the simulation and motion control of the Indy7 robot moving along the shape of the alpha symbol.Figure15illustrates the velocity of the trajectory generated along the shape of the alpha symbol as well as the simulation and motion control of the Indy7 robot.
A comparison of industrial robots interface: Force guidance system and teach pendant operation. G B Rodamilans, E Villani, L G Trabasso, W R D Oliveira, R Suterio, 10.1108/IR-02-2016-0074Ind. Robot Int. J. 432016</p>
<p>Development of a Soft Teaching Pendant for a Six-Axis Manipulator. F Y Annaz, M Miyurangakaluarachchi, Int. J. Electron. Comput. Commun. Technol. 42014</p>
<p>Digital twin for fanuc robots: Industrial robot programming and simulation using virtual reality. G Garg, V Kuts, G Anbarjafari, 10.3390/su1318103362021. 1033613</p>
<p>Recent progress on programming methods for industrial robots. Z Pan, J Polden, N Larkin, S Van Duin, J Norrish, 10.1016/j.rcim.2011.08.004Robot. -Comput.-Integr. Manuf. 282012</p>
<p>Collaborative approach for swarm robot systems based on distributed DRL. N F Bar, M Karakose, 10.1016/j.jestch.2024.101701Eng. Sci. Technol. Int. J. 532024. 101701</p>
<p>M Vohra, Digital Twin Technology: Fundamentals and Applications. Hoboken, NJ, USAJohn Wiley &amp; Sons2023</p>
<p>A physical-virtual based digital twin robotic hand. O Singh, A K Ray, 10.1007/s12008-024-01773-7Int. J. Interact. Des. Manuf. (IJIDeM). 2024</p>
<p>Value-driven robotic digital twins in cyber-physical applications. E G Kaigom, J Roßmann, 10.1109/TII.2020.3011062IEEE Trans. Ind. Inform. 172020</p>
<p>Digital Twin-Based Decision Support System for the Prospective and the Retrospective Analysis of an Operating Room under Uncertainties. L Rifi, 2023Albi-Carmaux; Albi, FrancePh.D. Thesis, Ecole des Mines d</p>
<p>Digital twin of robot manipulator using ROS. S K Chinnasamy, H P Sura, A Saleem, A Kathirvel, P Rangan, Proceedings of the AIP Conference Proceedings. the AIP Conference ProceedingsMelville, NY, USAAIP Publishing20232946</p>
<p>End-to-end training of deep visuomotor policies. S Levine, C Finn, T Darrell, P Abbeel, J. Mach. Learn. Res. 172016</p>
<p>P Christiano, Z Shah, I Mordatch, J Schneider, T Blackwell, J Tobin, P Abbeel, W Zaremba, arXiv:1610.03518Transfer from simulation to real world through learning deep inverse dynamics model. 2016</p>
<p>S Zhang, Z Ding, H Dong, Deep Reinforcement Learning: Fundamentals, Research and Applications. Berlin/Heidelberg, GermanySpringer2020</p>
<p>Exploring the synergies between collaborative robotics, digital twins, augmentation, and industry 5.0 for smart manufacturing: A state-of-the-art review. M H Zafar, E F Langås, F Sanfilippo, 10.1016/j.rcim.2024.102769Robot. -Comput.-Integr. Manuf. 892024. 102769</p>
<p>Accelerated sim-to-real deep reinforcement learning: Learning collision avoidance from human player. H Niu, Z Ji, F Arvin, B Lennox, H Yin, J Carrasco, Proceedings of the 2021 IEEE/SICE International Symposium on System Integration (SII), Virtual. the 2021 IEEE/SICE International Symposium on System Integration (SII), VirtualJanuary 2021</p>
<p>A digital twin-based sim-to-real transfer for deep reinforcement learning-enabled industrial robot grasping. Y Liu, H Xu, D Liu, L Wang, 10.1016/j.rcim.2022.102365Robot. Comput.-Integr. Manuf. 782022. 102365</p>
<p>Sim-to-real transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, Proceedings of the 2018 IEEE international conference on robotics and automation (ICRA). the 2018 IEEE international conference on robotics and automation (ICRA)Brisbane, AustraliaMay 2018</p>
<p>Johns, E. 3d simulation for robot arm control with deep q-learning. S James, arXiv:1609.037592016</p>
<p>Ensemble-cio: Full-body dynamic motion planning that transfers to physical humanoids. I Mordatch, K Lowrey, E Todorov, Proceedings of the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). the 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)Hamburg, Germany28 September-3 October 2015</p>
<p>Comparative study of Gazebo and Unity 3D in performing a virtual pick and place of Universal Robot UR3 for assembly process in manufacturing. G D Wijaya, W Caesarendra, M I Petra, G Królczyk, A Glowacz, Simul. Model. Pract. Theory. 1028952024</p>
<p>Emerging Trends in Realistic Robotic Simulations: A Comprehensive Systematic Literature Review. S M Kargar, B Yordanov, C Harvey, A Asadipour, 10.1109/ACCESS.2024.3404881IEEE Access. 2024</p>
<p>. Neuromeka, 11 April 2024</p>
<p>IRoSim: Industrial Robotics Simulation Design Planning and Optimization platform based on CAD and knowledgeware technologies. K Baizid, S Ćuković, J Iqbal, A Yousnadj, R Chellali, A Meddahi, G Devedžić, I Ghionea, 10.1016/j.rcim.2016.06.003Robot. Comput.-Integr. Manuf. 422016</p>
<p>Implementation of open-architecture kinematic controller for articulated robots under ROS. Y Gao, Z Du, X Gao, Y Su, Y Mu, L N Sun, W Dong, 10.1108/IR-09-2017-0166Ind. Robot Int. J. 452018</p>
<p>Reducing the Barrier to Entry of Complex Robotic Software: A MoveIt! Case Study. D Coleman, I A Şucan, S Chitta, N Correll, J. Sofware Eng. Robot. 52014</p>
<p>Keyboard-based control and simulation of 6-DOF robotic arm using ROS. R K Megalingam, N Katta, R Geesala, P K Yadav, R C Rangaiah, Proceedings of the 2018 4th International Conference on Computing Communication and Automation (ICCCA). the 2018 4th International Conference on Computing Communication and Automation (ICCCA)Greater Noida, IndiaDecember 2018</p>
<p>Robot programming using augmented reality: An interactive method for planning collision-free paths. J W S Chong, S Ong, A Y Nee, K Youcef-Youmi, 10.1016/j.rcim.2008.05.002Robot. Comput.-Integr. Manuf. 252009</p>
<p>Augmented reality-assisted robot programming system for industrial applications. S K Ong, A Yew, N K Thanigaivel, A Y Nee, 10.1016/j.rcim.2019.101820Robot. Comput.-Integr. Manuf. 612020. 101820</p>
<p>Unity-Technologies/ROS-TCP-Connector. Unity-Technologies, 29 March 2024</p>
<p>Unity-Technologies/ROS-TCP-Endpoint. Unity-Technologies, 29 March 2024</p>
<p>Real-Time Performance Benchmarking of RISC-V Architecture: Implementation and Verification on an EtherCAT-Based Robotic Control System. T Yoo, B W Choi, 10.3390/electronics13040733202413733</p>
<p>A Path to Industry 5.0 Digital Twins for Human-Robot Collaboration by Bridging NEP+ and ROS. Robotics. E Coronado, T Ueshiba, I G Ramirez-Alpizar, 10.3390/robotics13020028202413</p>
<p>A ROS 2-compatible simulation tool for exploration and coverage algorithms. M Z Andreasen, P I Holler, M K Jensen, M Albano, Maes, 10.1007/s10015-023-00895-7Artif. Life Robot. 282023</p>
<p>Open robot control software: The OROCOS project. H Bruyninckx, Proceedings of the 2001 ICRA. IEEE International Conference on Robotics and Automation. the 2001 ICRA. IEEE International Conference on Robotics and AutomationSeoul, Republic of KoreaMay 20013Cat. No. 01CH37164</p>
<p>The open motion planning library. I A Sucan, M Moll, L E Kavraki, 10.1109/MRA.2012.2205651IEEE Robot. Autom. Mag. 192012</p>
<p>Neuromeka-Robotics, Neuromeka-Robotics/Indy-ROS2: ROS2 package for Neuromeka Indy Available online. 11 April 2024</p>
<p>Unity-Technologies/URDF-Importer. Unity-Technologies, 29 March 2024</p>
<p>A two-step method for dynamic parameter identification of indy7 collaborative robot manipulator. M Tadese, N Pico, S Seo, H Moon, 10.3390/s22249708Sensors. 2297082022</p>
<p>Feasibility study for a python-based embedded real-time control system. S Y Cho, R Delgado, B W Choi, 10.3390/electronics120614262023. 142612</p>
<p>Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods. instructions or products referred to in the content</p>            </div>
        </div>

    </div>
</body>
</html>
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1454 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1454</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1454</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-26.html">extraction-schema-26</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <p><strong>Paper ID:</strong> paper-f8c306a5e8a19f5806bac9f26de14d09f7686aac</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/f8c306a5e8a19f5806bac9f26de14d09f7686aac" target="_blank">“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO</a></p>
                <p><strong>Paper Venue:</strong> Chemical Science</p>
                <p><strong>Paper TL;DR:</strong> Using a text-based representation of molecules, chemical reactions are predicted with a neural machine translation model borrowed from language processing to describe how molecules behave in a graph-based model.</p>
                <p><strong>Paper Abstract:</strong> Using a text-based representation of molecules, chemical reactions are predicted with a neural machine translation model borrowed from language processing.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1454.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1454.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Seq2seq attention model</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-free attention-based sequence-to-sequence reaction prediction model</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An encoder-decoder LSTM sequence-to-sequence model with Luong attention that maps tokenized SMILES reactant+reagent sequences to product SMILES, trained end-to-end on patent reaction data to predict major products without hand-coded templates or explicit atom features.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Template-free attention-based seq2seq reaction predictor</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Bidirectional LSTM encoder (BLSTM) + stacked LSTM decoder with Luong attention; atom-wise tokenization for reactants, reagent tokens for common reagents, trained end-to-end with cross-entropy loss using teacher forcing; inference via beam search (beam width 10) and post-output canonicalization. Trained on large patent-derived datasets (Jin's USPTO set and Lowe's grants set).</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Organic chemistry (reaction outcome prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Automatically predicts the major product SMILES string given reactants and reagents. The system is used to improve state-of-the-art predictive performance on patent reaction datasets (i.e., it produces high-quality product predictions that can be used to accelerate synthesis planning and analysis of reaction databases).</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Full-sequence accuracy (strict token-exact match) on held-out test sets; top-k accuracy (top-1, top-2, top-3, top-5); BLEU and ROUGE scores for sequence similarity; analysis of beam-search top-1 probability as a confidence measure and thresholding (coverage vs accuracy plots); decoding time per reaction (25 ms per reaction reported). Evaluation performed on two benchmark datasets: Jin's USPTO split and Lowe's grants split (single-product reactions).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validation against held-out test sets drawn from the same data source; direct numerical comparison to a competing state-of-the-art model (Jin et al.'s WLDN) on the same test split; external/time-split validation using an independent commercial database (Pistachio) for reactions occurring after the training cutoff; deterministic canonicalization of outputs prior to scoring; treating multiple-product test cases as false when appropriate to keep comparisons fair.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is assessed primarily by empirical performance gains over prior models on benchmark test sets (reported improvements in top-1 accuracy and the first reported scores on a noisier Lowe dataset). The approach is further characterized as template-free and fully data-driven (contrasting with template-based methods), which the authors present as a methodological novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Top-1 accuracy: reported values include 80.3% (Jin's USPTO when counting multiple-product reactions as false for fair comparison) and 83.2% (on Jin's USPTO single-product test set in another reporting), and 65.4% on Lowe's noisy single-product test set. BLEU/ROUGE on Jin's USPTO: 95.9 / 96.0; on Lowe's: 90.3 / 90.9. Decoding speed: ~25 ms per reaction (beam search). Training accuracy reported (indicative of memorization) up to 99.9% on Jin's training set and 94.5% on Lowe's training set. Confidence/coverage example: at top-1 probability threshold 0.83, coverage = 70.2% with accuracy = 83.0%.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Compared numerically to Jin et al.'s WLDN: this seq2seq model attains comparable or slightly higher top-1 accuracy (80.3% vs 79.6% when evaluated under the same counting rules) but is outperformed by WLDN in top-3 and top-5 metrics because WLDN explicitly ranks candidate products while the seq2seq network directly predicts a single top-1 output.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>See impact_metrics: top-1 accuracy 80.3% (comparable benchmark), 83.2% (alternative reporting), and 65.4% on a noisier dataset; training accuracies up to 99.9% (Jin train) indicating memorization in training.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Model limitations include: (1) predictions are not guaranteed to be syntactically valid SMILES (1.3% grammatical error rate in top-1 predictions), (2) handling multiple-product reactions since ordering in target string is arbitrary, (3) sensitivity to noisy training data (tradeoff coverage vs accuracy), (4) potential overfitting/memorization (very high training accuracy plateau), (5) constrained hyperparameter search (encoder units capped at 1024) and (6) lack of explicit chemical reasoning — cannot predict outside distribution covered by training data. The paper does not discuss distinguishing incremental vs transformational scientific discoveries.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1454.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1454.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Segler & Waller KG</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge-graph approach using reaction templates (Segler & Waller)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A template-based system that builds a knowledge graph of reactions from extracted templates and searches for missing nodes to identify candidate (novel) reactions; also later combined with neural prioritization of templates.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Segler & Waller knowledge-graph / template-based system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Extract reaction templates from databases (e.g., Reaxys), construct a knowledge graph of reactions and molecules, and search for missing nodes (gaps) in the graph as indicators of potentially novel reactions; in follow-up work neural networks are used to prioritize templates.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Organic chemistry (reaction discovery / retrosynthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Reported to have 'discovered novel reactions' by searching for missing nodes in the reaction knowledge graph — i.e., automated identification of reaction transformations not present as explicit entries in the database.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not specified in detail in this paper; the method is described in related-work context as a knowledge-graph search for missing nodes, implying qualitative discovery claims rather than systematically reported numerical discovery metrics here.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Not detailed in this paper; likely validated against reaction databases and expert assessment in original Segler & Waller publications (not expanded upon here).</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty is claimed by the identification of previously unlisted reactions (missing nodes) in the constructed knowledge graph; the paper notes this as a distinct contribution of the referenced work but does not quantify novelty within this manuscript.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Template-based knowledge-graph approaches inherit limitations of rule/template coverage: they cannot predict reactions outside the space spanned by extracted templates and thus may miss chemistry not present in the template set.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1454.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1454.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>WLDN (Jin et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Weisfeiler-Lehman Networks and Weisfeiler-Lehman Difference Network (WLDN) pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A graph-based template-free approach that predicts reaction centers with a Weisfeiler-Lehman Network (WLN), enumerates candidate products by applying bond changes, and ranks candidates with a Weisfeiler-Lehman Difference Network (WLDN).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>WLN + WLDN candidate-generation and ranking pipeline</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>First WLN estimates reactivity between atom pairs to predict reaction centers; enumerates possible bond configuration changes subject to valence/connectivity filters to generate product candidates; ranks candidates using a WLDN. Trained on ~400k patent reactions without stereochemistry.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Organic chemistry (reaction outcome prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Predicts major product(s) by identifying reaction centers and ranking enumerated candidate products; reported high top-k accuracies on patent-derived test sets.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Top-1/top-3/top-5 accuracy on held-out USPTO test set (reported top-1 = 79.6%, top-3 = 87.7%, top-5 = 89.2% on Jin's USPTO test set).</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Validated on a standardized USPTO split released by Jin et al.; augmented in some experiments to ensure product coverage of 100% for fair comparison to template-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty characterized by outperforming template-based approaches by a notable margin after augmentation; novelty primarily assessed through empirical benchmark performance.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td>Reported top-1 accuracy 79.6%; top-3 87.7%; top-5 89.2% (as reported and used for comparison in this manuscript).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td>Directly compared numerically to the seq2seq model in this paper: seq2seq top-1 ~80.3% vs WLDN 79.6% under the same counting rule, while WLDN outperforms seq2seq on top-3/top-5 due to its candidate-ranking design.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Top-1 success rate reported as 79.6% on the benchmark test set.</td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Requires enumeration and ranking of candidates; complexity and coverage issues arise unless augmented; stereochemical complexity was removed in datasets to simplify the task.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1454.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1454.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Corey & Wipke system</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Corey and Wipke's rule-based synthesis/retrosynthesis system (1969)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pioneering expert-rule-based system demonstrating that synthesis and retrosynthesis could be performed by a machine, using handcrafted reaction templates encoding local atom-connectivity changes.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Rule-based template system (Corey & Wipke)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Handcrafted templates/rules written by experts encoding local connectivity changes and conditional application; used for automated synthesis and retrosynthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Organic chemistry (synthesis planning / retrosynthesis)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Demonstrated automated synthesis/retrosynthesis planning using expert-derived templates; foundational proof-of-concept that machines can assist in synthesis planning.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Historical novelty as a pioneering framework for automated retrosynthesis; cited here as the origin of template-based approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Template-rule creation is tedious, time- and labor-intensive, may not cover the entire domain, and requires expert intervention; limited when problems fall outside the encoded rules.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1454.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e1454.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Coley et al. template-ranker</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-extraction and neural ranking pipeline (Coley et al.)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A pipeline that extracts reaction templates from patent data, enumerates product candidates from those templates and ranks candidates using a neural network trained on molecular fingerprints.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Template extraction + neural ranking system</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Extracts templates from patent reactions, generates candidate products for given reactants by applying templates, and ranks the candidates using a neural network trained on molecule fingerprints; considers alternative products as negative examples during training.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Organic chemistry (product prediction / retrosynthesis assistance)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Predicts likely products for given reactants by template-based candidate generation and neural ranking; used to prioritize plausible outcomes.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Ranking accuracy on held-out patent-derived datasets; inclusion of negative (alternative) products in training to improve ranking.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Evaluation on standardized patent-derived datasets; compared to other template-based and template-free approaches in related work.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Improvements in ranking by using neural networks and inclusion of negatives; novelty limited by template-space coverage.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Dependent on quality and coverage of extracted templates; cannot predict chemistry outside the template space.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1454.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e1454.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of automated systems making scientific discoveries, including how these discoveries are characterized as incremental or transformational, and how they are evaluated and validated.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Nam & Kim seq2seq</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Neural machine translation approach to reaction prediction (Nam & Kim)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An earlier application of template-free seq2seq (neural machine translation) models to predict organic reaction outcomes, trained end-to-end on patent data but limited in scope to textbook reactions in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Linking the Neural Machine Translation and the Prediction of Organic Chemistry Reactions</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_name</strong></td>
                            <td>Seq2seq reaction predictor (Nam & Kim)</td>
                        </tr>
                        <tr>
                            <td><strong>system_description</strong></td>
                            <td>Template-free seq2seq model using TensorFlow's translate model defaults to map reactant SMILES to product SMILES, demonstrating the viability of translation-model techniques for chemistry prediction tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_domain</strong></td>
                            <td>Organic chemistry (reaction prediction)</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_description</strong></td>
                            <td>Demonstrated that seq2seq neural machine translation models can be applied to reaction outcome prediction; produced predictions mainly for textbook reactions in their reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>discovery_type_justification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_methods</strong></td>
                            <td>Not extensively detailed in this manuscript; served as a methodological precedent.</td>
                        </tr>
                        <tr>
                            <td><strong>validation_approaches</strong></td>
                            <td>Trained on patent data and self-generated examples; scope limited in reported experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>novelty_assessment</strong></td>
                            <td>Novelty as an early demonstration of casting reaction prediction as sequence translation.</td>
                        </tr>
                        <tr>
                            <td><strong>impact_metrics</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_human_discoveries</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_details</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>challenges_limitations</strong></td>
                            <td>Earlier work limited to textbook reactions and used default hyperparameters; less exploration of attention mechanisms and scaling than the present work.</td>
                        </tr>
                        <tr>
                            <td><strong>has_incremental_transformational_comparison</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': '“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO', 'publication_date_yy_mm': '2017-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Weisfeiler-Lehman Neural Machine for Predicting Reaction Outcomes <em>(Rating: 2)</em></li>
                <li>Template-based Retrosynthesis and Knowledge-Graph Search (Segler & Waller) <em>(Rating: 2)</em></li>
                <li>Predicting Organic Reaction Outcomes with Machine Learning (Coley et al.) <em>(Rating: 2)</em></li>
                <li>Linking the Neural Machine Translation and the Prediction of Organic Chemistry Reactions <em>(Rating: 2)</em></li>
                <li>Machine-aided Synthesis Planning (Corey & Wipke, historical) <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1454",
    "paper_id": "paper-f8c306a5e8a19f5806bac9f26de14d09f7686aac",
    "extraction_schema_id": "extraction-schema-26",
    "extracted_data": [
        {
            "name_short": "Seq2seq attention model",
            "name_full": "Template-free attention-based sequence-to-sequence reaction prediction model",
            "brief_description": "An encoder-decoder LSTM sequence-to-sequence model with Luong attention that maps tokenized SMILES reactant+reagent sequences to product SMILES, trained end-to-end on patent reaction data to predict major products without hand-coded templates or explicit atom features.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_name": "Template-free attention-based seq2seq reaction predictor",
            "system_description": "Bidirectional LSTM encoder (BLSTM) + stacked LSTM decoder with Luong attention; atom-wise tokenization for reactants, reagent tokens for common reagents, trained end-to-end with cross-entropy loss using teacher forcing; inference via beam search (beam width 10) and post-output canonicalization. Trained on large patent-derived datasets (Jin's USPTO set and Lowe's grants set).",
            "discovery_domain": "Organic chemistry (reaction outcome prediction)",
            "discovery_description": "Automatically predicts the major product SMILES string given reactants and reagents. The system is used to improve state-of-the-art predictive performance on patent reaction datasets (i.e., it produces high-quality product predictions that can be used to accelerate synthesis planning and analysis of reaction databases).",
            "discovery_type": null,
            "discovery_type_justification": null,
            "evaluation_methods": "Full-sequence accuracy (strict token-exact match) on held-out test sets; top-k accuracy (top-1, top-2, top-3, top-5); BLEU and ROUGE scores for sequence similarity; analysis of beam-search top-1 probability as a confidence measure and thresholding (coverage vs accuracy plots); decoding time per reaction (25 ms per reaction reported). Evaluation performed on two benchmark datasets: Jin's USPTO split and Lowe's grants split (single-product reactions).",
            "validation_approaches": "Validation against held-out test sets drawn from the same data source; direct numerical comparison to a competing state-of-the-art model (Jin et al.'s WLDN) on the same test split; external/time-split validation using an independent commercial database (Pistachio) for reactions occurring after the training cutoff; deterministic canonicalization of outputs prior to scoring; treating multiple-product test cases as false when appropriate to keep comparisons fair.",
            "novelty_assessment": "Novelty is assessed primarily by empirical performance gains over prior models on benchmark test sets (reported improvements in top-1 accuracy and the first reported scores on a noisier Lowe dataset). The approach is further characterized as template-free and fully data-driven (contrasting with template-based methods), which the authors present as a methodological novelty.",
            "impact_metrics": "Top-1 accuracy: reported values include 80.3% (Jin's USPTO when counting multiple-product reactions as false for fair comparison) and 83.2% (on Jin's USPTO single-product test set in another reporting), and 65.4% on Lowe's noisy single-product test set. BLEU/ROUGE on Jin's USPTO: 95.9 / 96.0; on Lowe's: 90.3 / 90.9. Decoding speed: ~25 ms per reaction (beam search). Training accuracy reported (indicative of memorization) up to 99.9% on Jin's training set and 94.5% on Lowe's training set. Confidence/coverage example: at top-1 probability threshold 0.83, coverage = 70.2% with accuracy = 83.0%.",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Compared numerically to Jin et al.'s WLDN: this seq2seq model attains comparable or slightly higher top-1 accuracy (80.3% vs 79.6% when evaluated under the same counting rules) but is outperformed by WLDN in top-3 and top-5 metrics because WLDN explicitly ranks candidate products while the seq2seq network directly predicts a single top-1 output.",
            "success_rate": "See impact_metrics: top-1 accuracy 80.3% (comparable benchmark), 83.2% (alternative reporting), and 65.4% on a noisier dataset; training accuracies up to 99.9% (Jin train) indicating memorization in training.",
            "challenges_limitations": "Model limitations include: (1) predictions are not guaranteed to be syntactically valid SMILES (1.3% grammatical error rate in top-1 predictions), (2) handling multiple-product reactions since ordering in target string is arbitrary, (3) sensitivity to noisy training data (tradeoff coverage vs accuracy), (4) potential overfitting/memorization (very high training accuracy plateau), (5) constrained hyperparameter search (encoder units capped at 1024) and (6) lack of explicit chemical reasoning — cannot predict outside distribution covered by training data. The paper does not discuss distinguishing incremental vs transformational scientific discoveries.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1454.0",
            "source_info": {
                "paper_title": "“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Segler & Waller KG",
            "name_full": "Knowledge-graph approach using reaction templates (Segler & Waller)",
            "brief_description": "A template-based system that builds a knowledge graph of reactions from extracted templates and searches for missing nodes to identify candidate (novel) reactions; also later combined with neural prioritization of templates.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Segler & Waller knowledge-graph / template-based system",
            "system_description": "Extract reaction templates from databases (e.g., Reaxys), construct a knowledge graph of reactions and molecules, and search for missing nodes (gaps) in the graph as indicators of potentially novel reactions; in follow-up work neural networks are used to prioritize templates.",
            "discovery_domain": "Organic chemistry (reaction discovery / retrosynthesis)",
            "discovery_description": "Reported to have 'discovered novel reactions' by searching for missing nodes in the reaction knowledge graph — i.e., automated identification of reaction transformations not present as explicit entries in the database.",
            "discovery_type": null,
            "discovery_type_justification": null,
            "evaluation_methods": "Not specified in detail in this paper; the method is described in related-work context as a knowledge-graph search for missing nodes, implying qualitative discovery claims rather than systematically reported numerical discovery metrics here.",
            "validation_approaches": "Not detailed in this paper; likely validated against reaction databases and expert assessment in original Segler & Waller publications (not expanded upon here).",
            "novelty_assessment": "Novelty is claimed by the identification of previously unlisted reactions (missing nodes) in the constructed knowledge graph; the paper notes this as a distinct contribution of the referenced work but does not quantify novelty within this manuscript.",
            "impact_metrics": null,
            "comparison_to_human_discoveries": null,
            "comparison_details": null,
            "success_rate": null,
            "challenges_limitations": "Template-based knowledge-graph approaches inherit limitations of rule/template coverage: they cannot predict reactions outside the space spanned by extracted templates and thus may miss chemistry not present in the template set.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1454.1",
            "source_info": {
                "paper_title": "“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "WLDN (Jin et al.)",
            "name_full": "Weisfeiler-Lehman Networks and Weisfeiler-Lehman Difference Network (WLDN) pipeline",
            "brief_description": "A graph-based template-free approach that predicts reaction centers with a Weisfeiler-Lehman Network (WLN), enumerates candidate products by applying bond changes, and ranks candidates with a Weisfeiler-Lehman Difference Network (WLDN).",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "WLN + WLDN candidate-generation and ranking pipeline",
            "system_description": "First WLN estimates reactivity between atom pairs to predict reaction centers; enumerates possible bond configuration changes subject to valence/connectivity filters to generate product candidates; ranks candidates using a WLDN. Trained on ~400k patent reactions without stereochemistry.",
            "discovery_domain": "Organic chemistry (reaction outcome prediction)",
            "discovery_description": "Predicts major product(s) by identifying reaction centers and ranking enumerated candidate products; reported high top-k accuracies on patent-derived test sets.",
            "discovery_type": null,
            "discovery_type_justification": null,
            "evaluation_methods": "Top-1/top-3/top-5 accuracy on held-out USPTO test set (reported top-1 = 79.6%, top-3 = 87.7%, top-5 = 89.2% on Jin's USPTO test set).",
            "validation_approaches": "Validated on a standardized USPTO split released by Jin et al.; augmented in some experiments to ensure product coverage of 100% for fair comparison to template-based approaches.",
            "novelty_assessment": "Novelty characterized by outperforming template-based approaches by a notable margin after augmentation; novelty primarily assessed through empirical benchmark performance.",
            "impact_metrics": "Reported top-1 accuracy 79.6%; top-3 87.7%; top-5 89.2% (as reported and used for comparison in this manuscript).",
            "comparison_to_human_discoveries": false,
            "comparison_details": "Directly compared numerically to the seq2seq model in this paper: seq2seq top-1 ~80.3% vs WLDN 79.6% under the same counting rule, while WLDN outperforms seq2seq on top-3/top-5 due to its candidate-ranking design.",
            "success_rate": "Top-1 success rate reported as 79.6% on the benchmark test set.",
            "challenges_limitations": "Requires enumeration and ranking of candidates; complexity and coverage issues arise unless augmented; stereochemical complexity was removed in datasets to simplify the task.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1454.2",
            "source_info": {
                "paper_title": "“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Corey & Wipke system",
            "name_full": "Corey and Wipke's rule-based synthesis/retrosynthesis system (1969)",
            "brief_description": "A pioneering expert-rule-based system demonstrating that synthesis and retrosynthesis could be performed by a machine, using handcrafted reaction templates encoding local atom-connectivity changes.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Rule-based template system (Corey & Wipke)",
            "system_description": "Handcrafted templates/rules written by experts encoding local connectivity changes and conditional application; used for automated synthesis and retrosynthesis planning.",
            "discovery_domain": "Organic chemistry (synthesis planning / retrosynthesis)",
            "discovery_description": "Demonstrated automated synthesis/retrosynthesis planning using expert-derived templates; foundational proof-of-concept that machines can assist in synthesis planning.",
            "discovery_type": null,
            "discovery_type_justification": null,
            "evaluation_methods": null,
            "validation_approaches": null,
            "novelty_assessment": "Historical novelty as a pioneering framework for automated retrosynthesis; cited here as the origin of template-based approaches.",
            "impact_metrics": null,
            "comparison_to_human_discoveries": null,
            "comparison_details": null,
            "success_rate": null,
            "challenges_limitations": "Template-rule creation is tedious, time- and labor-intensive, may not cover the entire domain, and requires expert intervention; limited when problems fall outside the encoded rules.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1454.3",
            "source_info": {
                "paper_title": "“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Coley et al. template-ranker",
            "name_full": "Template-extraction and neural ranking pipeline (Coley et al.)",
            "brief_description": "A pipeline that extracts reaction templates from patent data, enumerates product candidates from those templates and ranks candidates using a neural network trained on molecular fingerprints.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_name": "Template extraction + neural ranking system",
            "system_description": "Extracts templates from patent reactions, generates candidate products for given reactants by applying templates, and ranks the candidates using a neural network trained on molecule fingerprints; considers alternative products as negative examples during training.",
            "discovery_domain": "Organic chemistry (product prediction / retrosynthesis assistance)",
            "discovery_description": "Predicts likely products for given reactants by template-based candidate generation and neural ranking; used to prioritize plausible outcomes.",
            "discovery_type": null,
            "discovery_type_justification": null,
            "evaluation_methods": "Ranking accuracy on held-out patent-derived datasets; inclusion of negative (alternative) products in training to improve ranking.",
            "validation_approaches": "Evaluation on standardized patent-derived datasets; compared to other template-based and template-free approaches in related work.",
            "novelty_assessment": "Improvements in ranking by using neural networks and inclusion of negatives; novelty limited by template-space coverage.",
            "impact_metrics": null,
            "comparison_to_human_discoveries": false,
            "comparison_details": null,
            "success_rate": null,
            "challenges_limitations": "Dependent on quality and coverage of extracted templates; cannot predict chemistry outside the template space.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1454.4",
            "source_info": {
                "paper_title": "“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO",
                "publication_date_yy_mm": "2017-11"
            }
        },
        {
            "name_short": "Nam & Kim seq2seq",
            "name_full": "Neural machine translation approach to reaction prediction (Nam & Kim)",
            "brief_description": "An earlier application of template-free seq2seq (neural machine translation) models to predict organic reaction outcomes, trained end-to-end on patent data but limited in scope to textbook reactions in reported experiments.",
            "citation_title": "Linking the Neural Machine Translation and the Prediction of Organic Chemistry Reactions",
            "mention_or_use": "mention",
            "system_name": "Seq2seq reaction predictor (Nam & Kim)",
            "system_description": "Template-free seq2seq model using TensorFlow's translate model defaults to map reactant SMILES to product SMILES, demonstrating the viability of translation-model techniques for chemistry prediction tasks.",
            "discovery_domain": "Organic chemistry (reaction prediction)",
            "discovery_description": "Demonstrated that seq2seq neural machine translation models can be applied to reaction outcome prediction; produced predictions mainly for textbook reactions in their reported experiments.",
            "discovery_type": null,
            "discovery_type_justification": null,
            "evaluation_methods": "Not extensively detailed in this manuscript; served as a methodological precedent.",
            "validation_approaches": "Trained on patent data and self-generated examples; scope limited in reported experiments.",
            "novelty_assessment": "Novelty as an early demonstration of casting reaction prediction as sequence translation.",
            "impact_metrics": null,
            "comparison_to_human_discoveries": false,
            "comparison_details": null,
            "success_rate": null,
            "challenges_limitations": "Earlier work limited to textbook reactions and used default hyperparameters; less exploration of attention mechanisms and scaling than the present work.",
            "has_incremental_transformational_comparison": false,
            "uuid": "e1454.5",
            "source_info": {
                "paper_title": "“Found in Translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models† †Electronic supplementary information (ESI) available: Time-split test set and example predictions, together with attention weights, confidence and token probabilities. See DO",
                "publication_date_yy_mm": "2017-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Weisfeiler-Lehman Neural Machine for Predicting Reaction Outcomes",
            "rating": 2
        },
        {
            "paper_title": "Template-based Retrosynthesis and Knowledge-Graph Search (Segler & Waller)",
            "rating": 2
        },
        {
            "paper_title": "Predicting Organic Reaction Outcomes with Machine Learning (Coley et al.)",
            "rating": 2
        },
        {
            "paper_title": "Linking the Neural Machine Translation and the Prediction of Organic Chemistry Reactions",
            "rating": 2
        },
        {
            "paper_title": "Machine-aided Synthesis Planning (Corey & Wipke, historical)",
            "rating": 1
        }
    ],
    "cost": 0.01860075,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>"Found in Translation": predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models $\dagger$</h1>
<p>Philippe Schwaller, ${ }^{1}$ * Théophile Gaudin, $\ddagger$ Dávid Lányi, Costas Bekas and Teodoro Laino</p>
<p>There is an intuitive analogy of an organic chemist's understanding of a compound and a language speaker's understanding of a word. Based on this analogy, it is possible to introduce the basic concepts and analyze potential impacts of linguistic analysis to the world of organic chemistry. In this work, we cast the reaction prediction task as a translation problem by introducing a template-free sequence-to-sequence model, trained end-to-end and fully data-driven. We propose a tokenization, which is arbitrarily extensible with</p>
<p>Received 28th May 2018
Accepted 20th June 2018
DOI: 10.1039/c8sc02339e
rsc.li/chemical-science
reaction information. Using an attention-based model borrowed from human language translation, we improve the state-of-the-art solutions in reaction prediction on the top-1 accuracy by achieving $80.3 \%$ without relying on auxiliary knowledge, such as reaction templates or explicit atomic features. Also, a top-1 accuracy of $65.4 \%$ is reached on a larger and noisier dataset.</p>
<h2>1 Introduction</h2>
<p>After nearly 200 years of documented research, the synthesis of organic molecules remains one of the most important tasks in organic chemistry. The construction of a target molecule from a set of existing reactants and reagents via chemical reactions is attracting much attention because of its economical implications.</p>
<p>Multiple efforts have been made in the past 50 years to rationalize the large number of chemical compounds and reactions identified, which form the large knowledge bases for solving synthetic problems. In 1969, Corey and Wipke ${ }^{\dagger}$ demonstrated that both synthesis and retrosynthesis could be performed by a machine. Their pioneering contribution involved the use of handcrafted rules made by experts, which are commonly known as reaction templates. The templates encode the local changes to the atoms' connectivity under certain conditions accounting for various subtleties of retrosynthesis. A similar algorithm emerged in the late $1970 \mathrm{~s}^{3}$ which also requires a set of expert rules. Unfortunately, rules writing is a tedious task, both time and labor-intensive, and may not cover the entire domain for complex organic chemistry problems. In such cases, profound chemical expertise is still required, and the solutions are usually developed by trained organic chemists. However, it can be extremely challenging even for them to</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>synthesize a relatively complex molecule, which may take several reaction steps to construct. In fact, navigating the chemical space of drug-related compounds by relying only on intuition may turn a synthesis into a nearly impossible task, especially if the problem is slightly outside the expert's knowledge.</p>
<p>Other approaches extract reaction templates directly from data. ${ }^{3-6}$ In this specific context, candidate products are generated from the templates and then are ranked according to their likelihood. Satoh and Funatsu ${ }^{1,4}$ used various hard-coded criterion to perform the ranking whereas more recent approaches ${ }^{5,6}$ used a deep neural network. However, these types of approaches are fundamentally dependent on the rule-based system component and thus inherit some of its major limitations. In particular, these approaches do not produce sufficiently accurate predictions outside of the training domain.</p>
<p>Nevertheless, the class of algorithms ${ }^{1-6}$ that is based on rules manually encoded by human experts or automatically derived from a reaction database is not the only way to approach the problem of organic synthesis. A second approach for predicting chemical reactions exploits the advancements in computational chemistry to evaluate the energy barriers of a reaction, based on first-principle calculations. ${ }^{7-9}$ Although it is possible to reach very accurate levels of predictions for small systems (chemical reactions involving few hundred atoms), it is still a very computationally daunting task which limits, among other things, the sampling of the solvent degrees of freedom, possibly resulting in unrealistic entropy contributions. Therefore, while computational chemistry may intrinsically solve the problem of reaction prediction, its prohibitive cost does prevent the systematic treatment of all those degrees of freedom that may</p>
<p>drive the chemical reaction along a specific route. For such reasons, its current field of applicability in industry is mainly limited to problems that may have a purely academic interest.</p>
<p>One way to view the reaction prediction task is to cast it as a translation problem, where the objective is to map a text sequence that represents the reactants to a text sequence representing the product. Molecules can equivalently be expressed as text sequences in line notation format, such as the simplified molecular-input line-entry system (SMILES). ${ }^{10}$ Intuitively, there is an analogy between a chemist's understanding of a compound and a language speaker's understanding of a word. No matter how imaginative such an analogy is, it was only very recently that a formal verification was proved. ${ }^{11}$ Cadeddu et al. ${ }^{11}$ showed that organic molecules contain fragments whose rank distribution is essentially identical to that of sentence fragments. Moreover, it has already been shown that a text representation of molecules has been effective in chemoinformatics. ${ }^{12-16}$ This has strengthened our belief that the methods of computational linguistics can have an immense impact on the analysis of organic molecules and reactions.</p>
<p>In this work, we build on the idea of relating organic chemistry to a language and explore the application of state-of-the-art neural machine translation methods, which are sequence-to-sequence (seq2seq) models. We intend to solve the forward-reaction prediction problem, where the starting materials are known and the interest is in generating the products. This approach was first pioneered by Nam and Kim. ${ }^{17}$ Here, we propose a model with higher capacity and a different attention mechanism, which better captures the relation between reactants and products. For the tokenization, we combine an atomwise tokenization for the reactants similar to the work of Nam and $\mathrm{Kim}^{17}$ with a one-hot reagent tokenization suggested by Schneider et al. ${ }^{18}$ Given that training data for reaction condition were available, the tokenization would be arbitrarily extensible with tokens describing those conditions. In this work, we only use a set of the most common reagents. ${ }^{19}$ The overall network architecture is simple, and the model is trained end-to-end, fully data-driven and without additional external information. With this approach, we improved the top-1 accuracy by $0.7 \%$ compared to current template-free solutions, achieving a value of $80.3 \%$ using their own training and test data sets. ${ }^{20}$ The model presented set also a first score of $65.4 \%$ on a noisy single product reactions dataset extracted from US patents.</p>
<h2>2 Related work</h2>
<h3>2.1 Template-based reaction prediction</h3>
<p>Template-based reaction prediction methods have been widely researched in the past couple of years. ${ }^{5,6,21}$ Wei et al. ${ }^{21}$ used a graph-convolution neural network proposed by Duvenaud et al. ${ }^{22}$ to infer fingerprints of the reactants and reagents. They trained a network on the fingerprints to predict which reaction templates to apply to the reactants. Segler and Waller ${ }^{5}$ built a knowledge graph using reaction templates and discovered novel reactions by searching for missing nodes in the graph. Coley et al. ${ }^{6}$ generated for a given set of reactants all possible product candidates from a set of reaction templates extracted
from US patents ${ }^{23}$ and predicted the outcome of the reaction by ranking the candidates with a neural network. One major advancement by Segler and Waller ${ }^{5}$ and Coley et al. ${ }^{6}$ was to consider alternative products as negative examples. Recently, Segler and Waller ${ }^{24}$ introduced a neural-symbolic approach. They extracted reaction rules from the commercially available Reaxys database. Then, they trained a neural network on molecular fingerprints to prioritize templates. The reaction products were generated using the top-ranked templates. In any case, template-based methods have the limitation that they cannot predict anything outside the space covered by the previously extracted templates.</p>
<h3>2.2 Template-free reaction prediction</h3>
<p>While template-free approaches existed for decades, ${ }^{25-28}$ a first rule-free approach was introduced by Kayala et al. ${ }^{29}$ Using fingerprints and hand-crafted features, they predicted a series of mechanistic steps to obtain one reaction outcome. Owing to the sparsity of data on such mechanistic reaction steps, the dataset was self-generated with a template-based expert system. Recently, Jin et al. ${ }^{20}$ used a novel approach based on WeisfeilerLehman Networks (WLN). They trained two independent networks on a set of 400000 reactions extracted from US patents. The first WLN scored the reactivity between atom pairs and predicted the reaction center. All possible bond configuration changes were enumerated to generate product candidates. The candidates that were not removed by hard-coded valence and connectivity rules are then ranked by a WeisfeilerLehman Difference Network (WLDN). Their method achieved a top-1 accuracy of $79.6 \%$ on a test set of 40000 reactions. Jin et al. ${ }^{20}$ claimed to outperform template-based approaches by a margin of $10 \%$ after augmenting the model with the unknown products of the initial prediction to have a product coverage of $100 \%$ on the test set. The dataset with the exact training, validation and test split have been released. $\S$ The complexity of the reaction prediction problem was significantly reduced by removing the stereochemical information.</p>
<h3>2.3 Seq2seq models in organic reaction prediction and retrosynthesis</h3>
<p>The closest work to ours is that of Nam and Kim, ${ }^{17}$ who also used a template-free seq2seq model to predict reaction outcomes. Whereas their network was trained end-to-end on patent data and self-generated reaction examples, they limited their predictions to textbook reactions. Their model was based on the Tensorflow translate model (v0.10.0), ${ }^{30}$ from which they took the default values for most of the hyperparameters. Compared to Nam and Kim, ${ }^{17}$ our model uses Luong's attention mechanism, ${ }^{31}$ through which a mapping between input and output tokens is obtained.</p>
<p>Retrosynthesis is the opposite of reaction prediction. Given a product molecule, the goal is to find possible reactants. In contrast to major product prediction, in retrosynthesis more than one target string might be correct, e.g. a product could be the result of two different reactant pairs. Having no distinct target, the training of a seq2seq model can be more difficult.</p>
<p>The first attempt of using a seq2seq model in retrosynthesis was achieved by Liu et al. ${ }^{32}$ They used a set of 50000 reactions extracted and curated by Schneider et al. ${ }^{19}$ The reactions from that set include stereochemical information and are classified into ten different reactions classes. Overall, none of the previous works was able to demonstrate the full potential of seq2seq models.</p>
<h2>3 Dataset</h2>
<p>All the openly available chemical reaction datasets were derived in some form from the patent text-mining work of Daniel M. Lowe. ${ }^{23}$ Lowe's dataset has recently been updated and contains data extracted from US patents grants and applications dating from 1976 to September 2016. ${ }^{33}$ What makes the dataset particularly interesting is that the quality and noise may be similar to the data a chemical company might own. The portion of granted patents is made of 1808938 reactions, which are described using SMILES. ${ }^{34}$</p>
<p>Looking at the original patent data, it is surprising that a complex chemical synthesis process consisting of multiple steps, performed over hours or days, can be summarized in a simple string. Such reaction strings are composed of three groups of molecules: the reactants, the reagents, and the products, which are separated by a ' $&gt;$ ' sign. The process actions and reaction conditions, for example, have been neglected so far.</p>
<p>To date, there is no standard way of filtering duplicates, incomplete or erroneous reactions in Lowe's dataset. We kept the filtering to a minimum to show that our network is able to handle noisy data. We removed 720768 duplicates by comparing reaction strings without atom mapping and an additional 780 reactions, because the SMILES string could not be canonicalized with RDKit, ${ }^{34}$ as the explicit number of valence electrons for one of the atoms was greater than permitted. We took only single product reactions, corresponding to $92 \%$ of the dataset, to have distinct prediction targets. Although this is a current limitation in the training procedure of our model, it could be easily overcome in the future, for example by defining a specific order for the product molecules. Finally, the dataset was randomly split into training, validation and test sets (18:1:1).!| Reactions with the same reactants, but different reagents and products were kept in the same set.</p>
<p>To compare our model and results with the current state of the art, we used the USPTO set recently published by Jin et al. ${ }^{20}$ It was extracted from Lowe's grants dataset ${ }^{33}$ and contains 479035 atom-mapped reactions without stereochemical information. We restricted ourselves to single product reactions, corresponding to $97 \%$ of the reactions in Jin's USPTO set. An overview of the datasets taken as ground truths for this work is shown in Table 1.</p>
<p>In general, we observe that if reactions that do not work well with the model are removed under the assumption that they are erroneous, the model's accuracy will improve suggesting the presence of a specific tradeoff between coverage and accuracy. This calls for open datasets. The only fair way to compare models is to use datasets to which identical filtering was applied</p>
<p>Table 1 Overview of the datasets used in this work. Jin's is derived from Lowe's grants dataset</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Reactions in</th>
<th style="text-align: left;">Train</th>
<th style="text-align: left;">Valid</th>
<th style="text-align: left;">Test</th>
<th style="text-align: left;">Total</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Lowe's grants set ${ }^{33}$ no <br> duplicates single product</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
<td style="text-align: left;">1808938</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">902581</td>
<td style="text-align: left;">50131</td>
<td style="text-align: left;">50258</td>
<td style="text-align: left;">1002970</td>
</tr>
<tr>
<td style="text-align: left;">Jin's USPTO set ${ }^{20}$</td>
<td style="text-align: left;">409035</td>
<td style="text-align: left;">30000</td>
<td style="text-align: left;">40000</td>
<td style="text-align: left;">479035</td>
</tr>
<tr>
<td style="text-align: left;">single product</td>
<td style="text-align: left;">395496</td>
<td style="text-align: left;">29075</td>
<td style="text-align: left;">38647</td>
<td style="text-align: left;">463218</td>
</tr>
</tbody>
</table>
<p>or where the reactions that the model is unable to predict are counted as false predictions.</p>
<h3>3.1 Data preprocessing</h3>
<p>To prepare the reactions, we first used the atom mappings to separate reagents from reactants. Input molecules with atoms appearing in the product were classified as reactants and the others without atoms in the product as reagents. Then, we removed the hydrogen atoms and the atom mappings from the reaction string, and canonicalized the molecules. Afterwards, we tokenized reactants and products atom-wise using the following regular expression:</p>
<p>$$
\begin{aligned}
&amp; \text { token_regex }="({[\wedge} \mid+\mid[B r ?] C T ?[N] O[\$] P[F] I] b|c| n|o|s|p[\backslash{\backslash} \mid . \
&amp; \quad|=|#|-\mid+\mid \backslash \backslash \backslash \backslash \mid:|\sim| \in \mid \backslash ?|&gt;\mid * \mid \backslash \$ \mid \backslash \backslash \mid[0-9]{2}[[0-9]) " .
\end{aligned}
$$</p>
<p>As reagent atoms are never mapped into product atoms, we employed a reagent-wise tokenization using a set of the 76 most common reagents, according to the analysis in ref. 19. Reagents belonging to this set were added as distinct tokens after the first ' $&gt;$ ' sign, ordered by occurrence. Other reagents, which were not in the set, were neglected and removed completely from the reaction string. The separate tokenization would allow us to extend the reaction information and add tokens for reaction conditions without changing the model architecture. The final source sequences were made up of tokenized "reactants &gt; common reagents" and the target sequence of a tokenized "product". The tokens were separated by space characters. The preprocessing steps together with examples are summarized in Table 2. The same preprocessing steps were applied to all datasets.</p>
<h2>4 Model</h2>
<p>To map the sequence of the reactants/reagents to the sequence of the products, we adapted an existing implementation ${ }^{35}$ with minor modifications. Our model architecture, illustrated in Fig. 1, consists of two distinct recurrent neural networks (RNN) working together: (1) an encoder that processes the input sequence and emits its context vector $C$, and (2) a decoder that uses this representation to output a probability over a prediction. For these two RNNs, we rely on specific variants of long short-term memory (LSTM) ${ }^{36}$ because they are able to handle long-range relations in sequences. An LSTM consists of units that process the input data sequentially. Each unit at each time</p>
<p>Table 2 Data preparation steps to obtain source and target sequences. The tokens are separated by a space and individual molecules by a point token</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Step</th>
<th style="text-align: center;">Example (entry 23 738, Jin's USPTO test set ${ }^{20}$ ): reactants &gt; reagents &gt; products</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">(1)</td>
<td style="text-align: center;">Original string</td>
<td style="text-align: center;">$[\mathrm{Cl}: 1][\mathrm{c}: 2] 1[\mathrm{cH}: 3][\mathrm{c}: 4][\mathrm{CH} 3: 8]][\mathrm{n}: 5][\mathrm{n}: 6] 1[\mathrm{CH} 3: 7]-[\mathrm{OH}: 14][\mathrm{N}+: 15]][\mathrm{O}-: 16])=[\mathrm{O}: 17]-<a href="=[\mathrm{O}: 10]">\mathrm{S}: 9</a>(=[\mathrm{O}: 11])$ ([OH:12])[OH:13] $] \rightarrow[\mathrm{Cl}: 1][\mathrm{c}: 2] 1[\mathrm{c}: 3][<a href="=[\mathrm{O}: 14]">\mathrm{N}+: 15</a>[\mathrm{O}-: 16])[\mathrm{c}: 4][[\mathrm{CH} 3: 8]][\mathrm{n}: 5][\mathrm{n}: 6] 1[\mathrm{CH} 3: 7]$</td>
</tr>
<tr>
<td style="text-align: center;">(2)</td>
<td style="text-align: center;">Reactants and reagent separation</td>
<td style="text-align: center;">$[\mathrm{Cl}: 1][\mathrm{c}: 2] 1[\mathrm{cH}: 3][\mathrm{c}: 4][\mathrm{CH} 3: 8]][\mathrm{n}: 5][\mathrm{n}: 6] 1[\mathrm{CH} 3: 7]-[\mathrm{OH}: 14][\mathrm{N}+: 15]][\mathrm{O}-: 16])=[\mathrm{O}: 17]+<a href="=[\mathrm{O}: 10]">\mathrm{S}: 9</a>(=[\mathrm{O}: 11])$ ([OH:12])[OH:13]+[Cl:1][c:2])[c:3][<a href="=[O:14]">N+:15</a>[O-:16])[C:4][CH3:8][\mathrm{n}: 5][n:6]1[CH3:7]</td>
</tr>
<tr>
<td style="text-align: center;">(3)</td>
<td style="text-align: center;">Atom-mapping removal and canonicalization</td>
<td style="text-align: center;">$\mathrm{Cc} 1 \mathrm{cc}(\mathrm{Cl}) \mathrm{n}(\mathrm{C}) \mathrm{n} 1 . \mathrm{O}=\left[\mathrm{N}^{+}\right][[\mathrm{O}-] \mathrm{O}=\mathrm{O}=\mathrm{S}(=\mathrm{O})(\mathrm{O}) \mathrm{O}=\mathrm{Cc} 1 \mathrm{nn}(\mathrm{C}) \mathrm{c}(\mathrm{Cl}) \mathrm{c} 1<a href="=\mathrm{O}">\mathrm{~N}+</a>[\mathrm{O}-]$</td>
</tr>
<tr>
<td style="text-align: center;">(4)</td>
<td style="text-align: center;">Tokenization <br> Source <br> Target</td>
<td style="text-align: center;">$\begin{aligned} &amp; \mathrm{Cc} 1 \mathrm{cc}(\mathrm{Cl}) \mathrm{n}(\mathrm{C}) \mathrm{n} 1 . \mathrm{O}=\left[\mathrm{N}^{+}\right]\left([\mathrm{O}-]\right) \mathrm{O}&gt;\mathrm{A}<em _mathrm_t="\mathrm{t">{\mathrm{t}}&gt;\mathrm{Cc} 1 \mathrm{nn}(\mathrm{C}) \mathrm{c}(\mathrm{Cl}) \mathrm{c} 1\left<a href="=\mathrm{O}">\mathrm{~N}^{+}\right</a>[\mathrm{O}-] \ &amp; \mathrm{Cc} 1 \mathrm{cc}(\mathrm{Cl}) \mathrm{n}(\mathrm{C}) \mathrm{n} 1 . \mathrm{O}=\left[\mathrm{N}^{+}\right]\left([\mathrm{O}-]\right) \mathrm{O}&gt;\mathrm{A}</em> 1\left}} \ &amp; \mathrm{Cc} 1 \mathrm{nn}(\mathrm{C}) \mathrm{c}(\mathrm{Cl}) \mathrm{c<a href="=\mathrm{O}">\mathrm{~N}^{+}\right</a>[\mathrm{O}-] \end{aligned}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Fig. 1 Illustration of an attention-based seq2seq model.
step $t$ processes an element of the input $x_{t}$ and the network's previous hidden state $h_{t-1}$. The output and the hidden state transition is defined by</p>
<p>$$
\begin{gathered}
i_{t}=\sigma\left(W_{i} x_{t}+U_{i} h_{t-1}+b_{i}\right) \
f_{t}=\sigma\left(W_{f} x_{t}+U_{f} h_{t-1}+b_{f}\right) \
o_{t}=\sigma\left(W_{o} x_{t}+U_{o} h_{t-1}+b_{o}\right) \
c_{t}=f_{t} \otimes c_{t-1}+i_{t} \otimes \tanh \left(W_{c} x_{t}+U_{c} h_{t-1}+b_{c}\right) \
h_{t}=o_{t} \otimes \tanh \left(c_{t-1}\right)
\end{gathered}
$$</p>
<p>where $i_{t}, f_{t}$ and $o_{t}$ are the input, forget, and output gates; $c$ is the cell state vector; $W, U$ and $b$ are model parameters learned during training; $\sigma$ is the sigmoid function and $\otimes$ is the entrywise product. For the encoder, we used a bidirectional LSTM (BLSTM). ${ }^{47}$ A BLSTM processes the input sequence in both directions, so they have context not only from the past but also from the future. They comprise two LSTMs: one that processes the sequence forward and the other backward, with their forward and backward hidden states $\hat{h}<em t="t">{t}$ and $\hat{h}</em>$ for each time step. The hidden states of a BLSTM are defined as</p>
<p>$$
h_{t}=\left{\hat{h}<em t="t">{t} ; \hat{h}</em>\right}
$$</p>
<p>Thus we can formalize our encoder as</p>
<p>$$
C=f\left(W_{o} x_{t}, h_{t-1}\right)
$$</p>
<p>where $f$ is a multilayered BLSTM; $h_{t} \in \mathbb{R}^{n}$ are the hidden states at time $t ; x_{t}$ is an element of an input sequence $x=\left{x_{0}, \ldots, x_{T}\right}$, which is a one-hot encoding of our vocabulary; and $W_{o}$ are the learned embedding weights. Generally, $C$ is simply the last of the encoder's hidden states:</p>
<p>$$
C=h_{T}
$$</p>
<p>The second part of the model - the decoder - predicts the probability of observing a product $\hat{y}=\left{\hat{y}<em M="M">{1}, \ldots, \hat{y}</em>\right}$ :</p>
<p>$$
P(\hat{y})=\prod_{i=0}^{M} p\left(\hat{y}<em 1="1">{i} \mid\left{\hat{y}</em>\right}\right)
$$}, \ldots, \hat{y}_{i-1</p>
<p>and for a single token $\hat{y}_{i}$ :</p>
<p>$$
p\left(\hat{y}<em 1="1">{i} \mid\left{\hat{y}</em>}, \ldots, \hat{y<em i="i">{i-1}\right}, c</em>}\right)=g\left(\hat{y<em i="i">{i-1}, x</em>\right)
$$}, c_{i</p>
<p>where $g$ is a stack of LSTM, which outputs the probability $\hat{y}<em i="i">{i}$ for a single token; $s</em>$. We performed experiments using both models and describe Luong's method, which yielded the best overall results.}$ are the decoder's hidden states; and $c_{i}$ is a different context vector for each target token $y_{i}$. Bahdanau et al. ${ }^{48}$ and Luong et al. ${ }^{44}$ proposed attention mechanisms, i.e., different ways for computing the $c_{i}$ vector rather than taking the last hidden state of the encoder $h_{i</p>
<h3>4.1 Luong's attention mechanism</h3>
<p>To compute the context vector, we first have to compute the attention weights $\alpha$ :</p>
<p>$$
\begin{gathered}
\alpha_{o}=\frac{\exp \left(s_{i}^{\top} W_{o} h_{i}\right)}{\sum_{i^{\prime}=0}^{T} \exp \left(s_{i}^{\top} W_{o} h_{i^{\prime}}\right)} \
c_{i}=\sum_{t=0}^{T} \alpha_{o} h_{t}
\end{gathered}
$$</p>
<p>The attention vector is then defined by</p>
<p>$$
a_{i}=\tanh \left(W_{a}\left{c_{i} ; x_{i}\right}\right)
$$</p>
<p>Both $W_{a}$ and $W_{a}$ are learned weights. Then $a$ can be used to compute the probability for a particular token:</p>
<p>$$
p\left(y_{i} \mid\left{y_{1}, \ldots, y_{i-1}\right}, c_{i}\right)=\operatorname{softmax}\left(W_{\mathrm{p}} a_{i}\right)
$$</p>
<p>where $W_{\mathrm{p}}$ are also the learned projection weights.</p>
<h3>4.2 Training details</h3>
<p>During training, all parameters of the network were trained jointly using a stochastic gradient descent. The loss function was a cross-entropy function, expressed as</p>
<p>$$
H(y, \hat{y})=-\sum_{i} y_{i} \log \left(\hat{y}_{i}\right)
$$</p>
<p>for a particular training sequence. The loss was computed over an entire minibatch and then normalized. The weights were initialized using a random uniform distribution ranging from -0.1 to 0.1 . Every 3 epochs, the learning rate was multiplied by a decay factor. The minibatch size was 128 . Gradient clipping was applied when the norm of the gradient exceeded 5.0. The teacher forcing method ${ }^{39}$ was used during training.</p>
<h2>5 Architecture \&amp; hyperparameter search</h2>
<p>Finding the best-performing set of hyperparameters for a deep neural network is not trivial. As mentioned in Section 4, our model has numerous parameters that can influence both its training and its architecture. Depending on those parameters, the performance of the model can vary notably. In order to select the best parameters efficiently, we build a framework around scikit-optimize. ${ }^{40}$ After the evaluation of 10 random sets, a gradient-boosted tree model ${ }^{41}$ was used as surrogate model together with expected improvement as acquisition function ${ }^{42}$ to guide the hyperparameter search on a space defined in Table 3. The sets of hyperparameters were evaluated according to their accuracy on the validation set. In total, we trained 100 models for 30 epochs. The set of best hyperparameters found with this method is highlighted in bold. This model has been further trained to 80 epochs to improve its final accuracy.</p>
<h2>6 Experiments</h2>
<h3>6.1 Reaction prediction</h3>
<p>We evaluated our model on two data sets and compared the performance with other state-of-the-art results. After the</p>
<p>Table 3 Hyperparameters space, parameters for the best model in bold</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Parameter</th>
<th style="text-align: left;">Possible values</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Number of units</td>
<td style="text-align: left;">$128,256,512$ or $\mathbf{1 0 2 4}$</td>
</tr>
<tr>
<td style="text-align: left;">Number of layers</td>
<td style="text-align: left;">$\mathbf{2 , 4}$ or 6</td>
</tr>
<tr>
<td style="text-align: left;">Type of encoder</td>
<td style="text-align: left;">LSTM, BLSTM</td>
</tr>
<tr>
<td style="text-align: left;">Output dropout</td>
<td style="text-align: left;">$0-0.9(\mathbf{0 . 7 6 7 6 )}$</td>
</tr>
<tr>
<td style="text-align: left;">State dropout</td>
<td style="text-align: left;">$0-0.9(\mathbf{0 . 5 3 7 4})$</td>
</tr>
<tr>
<td style="text-align: left;">Learning rate</td>
<td style="text-align: left;">$0.1-5(\mathbf{0 . 3 5 5 )}$</td>
</tr>
<tr>
<td style="text-align: left;">Decay factor</td>
<td style="text-align: left;">$0.85-0.99(\mathbf{0 . 8 5 4})$</td>
</tr>
<tr>
<td style="text-align: left;">Type of attention</td>
<td style="text-align: left;">"Luong" or</td>
</tr>
<tr>
<td style="text-align: left;"></td>
<td style="text-align: left;">"Badhanau"</td>
</tr>
</tbody>
</table>
<p>hyperparameter optimization, we continued to train our best model on the 395496 reactions in Jin's USPTO train set and tested the fully trained model on Jin's USPTO test set. Additionally, we trained a second model with the same hyperparameters on 902581 randomly chosen single-product reactions from the more complex and noisy Lowe dataset and tested it on a set of 50258 reactions. As molecules are discrete data, changing a single character, such as in source code or arithmetic expressions, can lead to completely different meanings or even invalidate the entire string. Therefore we use fullsequence accuracy, the strictest criteria possible, as our evaluation metric by which a test prediction is considered correct only if all tokens are identical to the ground truth.</p>
<p>The network had to solve three major challenges. First, it had to memorize the SMILES grammar to predict synthetically correct sequences. Second, because we trained it on canonicalized molecules, the network had to learn the canonical representation. Third, the network had to map the reactants plus reagents space to the product space.</p>
<p>Although the training was performed without a beam search, we used a beam width of 10 without length penalty for the inference. Therefore the 10 most probable sequences were kept at every time step. This allowed us to know what probability the network assigned to each of the sequences. We used the top-1 probabilities to analyze the prediction confidence of the network.</p>
<p>The final step was to canonicalize the network output. This simple and deterministic reordering of the tokens improved the accuracy by $1.5 \%$. Thus, molecules that were correctly predicted, but whose tokens were not enumerated in the canonical order, were still counted as correct. The prediction accuracies of our model on different datasets are reported in Table 4. For single product reactions, we achieved an accuracy of $83.2 \%$ on Jin's USPTO test dataset and $65.4 \%$ on Lowe's test set.</p>
<p>An additional validation can be found in the ESI, ${ }^{\ddagger}$ were we used the model trained on Lowe's dataset to predict reactions from pistachio, ${ }^{43}$ a commercial database of chemical reactions extracted from the patent literature. Because the Lowe's dataset used to train the model contained reactions until September 2016, we only predicted the reactions from 2017 and hence, we had a time split.</p>
<h3>6.2 Comparison with the state of the art</h3>
<p>To the best of our knowledge, no previous work has attempted to predict reactions on the complete US patent dataset of Lowe. ${ }^{33}$ Table 5 shows a comparison with the Weisfeiler-Lehman difference networks (WLDN) from Jin et al. ${ }^{20}$ on their USPTO test set.</p>
<p>Table 4 Scores of our model on different single product datasets</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
<th style="text-align: left;">Accuracies in [\%]</th>
<th style="text-align: left;"></th>
<th style="text-align: left;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">Test set</td>
<td style="text-align: left;">Size</td>
<td style="text-align: left;">BLEU, ${ }^{43}$ ROUGE $^{44}$</td>
<td style="text-align: left;">Top-1</td>
<td style="text-align: left;">Top-2</td>
<td style="text-align: left;">Top-3</td>
</tr>
<tr>
<td style="text-align: left;">Jin's USPTO ${ }^{20}$</td>
<td style="text-align: left;">38648</td>
<td style="text-align: left;">$95.9,96.0$</td>
<td style="text-align: left;">$\mathbf{8 3 . 2}$</td>
<td style="text-align: left;">87.7</td>
<td style="text-align: left;">89.2</td>
</tr>
<tr>
<td style="text-align: left;">Lowe's ${ }^{33}$</td>
<td style="text-align: left;">50258</td>
<td style="text-align: left;">$90.3,90.9$</td>
<td style="text-align: left;">$\mathbf{6 5 . 4}$</td>
<td style="text-align: left;">71.8</td>
<td style="text-align: left;">74.1</td>
</tr>
</tbody>
</table>
<p>Table 5 Comparison with Jin et al. ${ }^{20}$ The 1352 multiple product reactions ( $3.4 \%$ of the test set) are counted as false predictions for our model</p>
<p>Jin's USPTO test set, ${ }^{20}$ accuracies in [\%]</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Top-1</th>
<th style="text-align: left;">Top-2</th>
<th style="text-align: left;">Top-3</th>
<th style="text-align: left;">Top-5</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">WLDN $^{20}$</td>
<td style="text-align: left;">79.6</td>
<td style="text-align: left;"></td>
<td style="text-align: left;">$\mathbf{8 7 . 7}$</td>
<td style="text-align: left;">$\mathbf{8 9 . 2}$</td>
</tr>
<tr>
<td style="text-align: left;">Our model</td>
<td style="text-align: left;">$\mathbf{8 0 . 3}$</td>
<td style="text-align: left;">$\mathbf{8 4 . 7}$</td>
<td style="text-align: left;">86.2</td>
<td style="text-align: left;">87.5</td>
</tr>
</tbody>
</table>
<p>To make a fair comparison, we count all the multiple product reactions in the test set as false predictions for our model because we trained only on the single product reactions. By achieving $80.3 \%$ top-1 accuracy, we perform quantitatively nearly identical. As our model does not rank candidates, but was trained on accurately predicting the top-1 outcome, it is not surprising that the WLDN beats our model in the top-3 and top-5 accuracy. The decoding of the 38648 USPTO test set reactions takes on average 25 ms per reaction, inferred with a beam search. Our model can therefore compete with the state of the art.</p>
<h3>6.3 Prediction confidence</h3>
<p>We analyzed the top-1 beam search probability to obtain information about prediction confidence and to observe how this
probability was related to accuracy. Fig. 2a illustrates the distribution of the top-1 probability for Lowe's test set in cases where the top-1 prediction is correct (left) and where it is wrong (right). A clear difference can be observed and used to define a threshold under which we determine that the network does not know what to predict. Fig. 2b shows the top-1 accuracy and coverage depending on the confidence threshold. For example, for a confidence threshold of 0.83 the model would predict the outcome of $70.2 \%$ of the reactions with an accuracy of $83.0 \%$ and for the remaining $29.8 \%$ of the reaction it would not know the outcome.</p>
<h3>6.4 Attention</h3>
<p>Attention is the key to take into account complex long-range dependencies between multiple tokens. Specific functional groups, solvents or catalysts have an impact on the outcome of a reaction, even if they are far from the reaction center in the molecular graph and therefore also in the SMILES string. Fig. 3 shows how the network learned to focus first on the $\mathrm{C}\left[\mathrm{O}^{-}\right]$ molecule, to map the $\left[\mathrm{O}^{-}\right]$in the input correctly to the O in the target, and to ignore the Br , which is replaced in the target. A few more reaction predictions together with the attention weights, confidence and token probabilities are found in the ESI. $\dagger$
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Fig. 2 Top-1 prediction confidence plots for Lowe's test set inferred with a beam search of 10.
<img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Fig. 3 Attention weights of reaction 120 from Jin's USPTO test set. The atom mapping between reactants and product is highlighted. SMILES: $\mathrm{Brc1cncc}(\mathrm{Br}) \mathrm{c} 1 . \mathrm{C}[\mathrm{O}-]&gt;\mathrm{CN}(\mathrm{C}) \mathrm{C}=\mathrm{O} .[\mathrm{Na}+]&gt;\mathrm{COc1cncc}(\mathrm{Br}) \mathrm{c} 1$. Reaction plotted with RDKit. ${ }^{34}$</p>
<h3>6.5 Limitations</h3>
<p>Our model is not without limitations. An obvious disadvantage compared to template-based methods is that the strings are not guaranteed to be a valid SMILES. Incorporating a context-free grammar layer, as was done in ref. 12, could bring minor improvements. Fortunately, only $1.3 \%$ of the top-1 predictions are grammatically erroneous for our model.</p>
<p>Another limitation of the training procedure are multiple product reactions. In contrast to words in a sentence, the exact order in which the molecules in the target string are enumerated does not matter. A viable option would be to include in the training set all possible permutations of the product molecules.</p>
<p>Our hyperparameter space during optimization was restricted to a maximum of 1024 units for the encoder. Using more units could have led to improvements. On Jin's USPTO dataset, the training plateaued because an accuracy of $99.9 \%$ was reached and the network had memorized almost the entire training set. Even on Lowe's noisier dataset, a training accuracy of $94.5 \%$ was observed. A hyperparameter optimization could be performed on Lowe's dataset to improve the prediction accuracy.</p>
<h2>7 Conclusion</h2>
<p>Predicting reaction outcomes is a routine task of many organic chemists trained to recognize structural and reactivity patterns reported in a wide number of publications. Not only did we show that a seq2seq model with correctly tuned hyperparameters can learn the language of organic chemistry, our approach also improved the current state-of-the-art in patent reaction outcome prediction by achieving $80.3 \%$ on Jin's USPTO dataset and $65.4 \%$ on single product reactions of Lowe's dataset. Similar to the work of Nam and Kim, ${ }^{17}$ our approach is fully data driven and free of chemical knowledge/rules and compared to their work, ${ }^{17}$ we take full advantage of the attention mechanism. Also worth mentioning is the overall simplicity of our model that jointly trains the encoder, decoder and attention layers end-to-end. Our hope is that, with this type of model, chemists can codify and perhaps one day fully automate the art of organic synthesis.</p>
<h2>Conflicts of interest</h2>
<p>There are no conflicts to declare.</p>
<h2>Acknowledgements</h2>
<p>We thank Nadine Schneider, Greg Landrum and Roger Sayle for the helpful discussions on RDKit and the datasets. We also would like to acknowledge Marwin Segler and Hiroko Satoh for useful feedback on our approach.</p>
<h2>Notes and references</h2>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>1 E. J. Corey and W. T. Wipke, Science, 1969, 166, 178-192.
2 T. D. Salatin and W. L. Jorgensen, J. Org. Chem., 1980, 45(11), 2043-2051.
3 H. Satoh and K. Funatsu, J. Chem. Inf. Comput. Sci., 1995, 35, $34-44$.
4 H. Satoh and K. Funatsu, J. Chem. Inf. Comput. Sci., 1996, 36, 173-184.
5 M. H. S. Segler and M. P. Waller, Chem.-Eur. J., 2017, 23, 6118-6128.
6 C. W. Coley, R. Barzilay, T. S. Jaakkola, W. H. Green and K. F. Jensen, ACS Cent. Sci., 2017, 3, 434-443.</p>
<p>7 W. R. J. Dolbier, K. Henryk, K. Houk and S. Chimin, Acc. Chem. Res., 1996, 29, 471-477.
8 D. Mondal, S. Y. Li, L. Bellucci, T. Laino, A. Tafi, S. Guccione and S. D. Lepore, J. Org. Chem., 2013, 78, 2118-2127.
9 O. Engkvist, P.-O. Norrby, N. Selmi, Y.-h. Lam, Z. Peng, E. C. Sherer, W. Amberg, T. Erhard and L. A. Smyth, Drug Discovery Today, 2018, 23(6), 1203-1218.
10 D. Weininger, J. Chem. Inf. Comput. Sci., 1988, 281413, 31-36.
11 A. Cadeddu, E. K. Wylie, J. Jurczak, M. Wampler-Doty and B. A. Grzybowski, Angew. Chem., Int. Ed., 2014, 53, 8108-8112.</p>
<p>12 R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams and A. Aspuru-Guzik, ACS Cent. Sci., 2018, 4, 268-276.</p>
<p>13 S. Jastrzębski, D. Leśniak and W. M. Czarnecki, Learning to SMILE(S), 2016, http://arxiv.org/abs/1602.06289.
14 M. J. Kusner, B. Paige and J. M. Hernández-Lobato, ICML, 2017.</p>
<p>15 E. J. Bjerrum, SMILES Enumeration as Data Augmentation for Neural Network Modeling of Molecules, 2017, http://arxiv.org/ abs/1703.07076.
16 M. H. S. Segler, T. Kogej, C. Tyrchan and M. P. Waller, ACS Cent. Sci., 2018, 4, 120-131.
17 J. Nam and J. Kim, Linking the Neural Machine Translation and the Prediction of Organic Chemistry Reactions, 2016, https://arxiv.org/pdf/1612.09529.pdf.
18 N. Schneider, D. M. Lowe, R. A. Sayle and G. A. Landrum, J. Chem. Inf. Model., 2015, 55, 39-53.</p>
<p>19 N. Schneider, N. Stiefl and G. A. Landrum, J. Chem. Inf. Model., 2016, 56, 2336-2346.
20 W. Jin, C. Coley, R. Barzilay and T. Jaakkola, NIPS, 2017, pp. 2607-2616.
21 J. N. Wei, D. Duvenaud and A. Aspuru-Guzik, ACS Cent. Sci., 2016, 2, 725-723.
22 D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel, A. Aspuru-Guzik and R. P. Adams, NIPS, 2015.</p>
<p>23 D. M. Lowe, Ph.D. thesis, University of Cambridge, 2012.
24 M. H. S. Segler and M. P. Waller, Chem.-Eur. J., 2017, 23, 5966-5971.
25 J. Bauer, E. Fontain, D. Forstmeyer and I. Ugi, Tetrahedron Comput. Methodol., 1988, 1, 129-132.
26 P. Röse and J. Gasteiger, Anal. Chim. Acta, 1990, 235, 163168.</p>
<p>27 W. L. Jorgensen, E. R. Laird, A. J. Gushurst, J. M. Fleischer, S. A. Gothe, H. E. Helson, G. D. Paderes and S. Sinclair, Pure Appl. Chem., 1990, 62, 1921-1932.</p>
<p>28 W. A. Warr, Mol. Inf., 2014, 33, 469-476.
29 M. A. Kayala and P. Baldi, J. Chem. Inf. Model., 2012, 52, 2526-2540.
30 M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner, P. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, X. Zheng and G. Brain, OSDI, 2016.</p>
<p>31 M.-T. Luong, H. Pham and C. D. Manning, EMNLP, 2015.
32 B. Liu, B. Ramsundar, P. Kawthekar, J. Shi, J. Gomes, Q. Luu Nguyen, S. Ho, J. Sloane, P. Wender and V. Pande, ACS Cent. Sci., 2017, 3, 1103-1113.
33 D. M. Lowe, Chemical reactions from US pat. (1976-Sep2016), 2017, https://figshare.com/articles/Chemical_reactions_from_ US_patents_1976-Sep2016_/5104873.
34 G. Landrum, B. Kelley, P. Tosco, S. Riniker, Gedeck, N. Schneider, R. Vianello, A. Dalke, S. Alexander, S. Turk, M. Swain, B. Cole, J. P, Strets123, JLVarjo, A. Pahl, P. Fuller, G. Doliath, M. Wójcikowski, D. Cosgrove, G. Sforna, M. Nowotka, J. H. Jensen, J. Domański, D. Hall, N. O'Boyle, W.-G. Bolick, Nhfechner and S. Roughley,</p>
<p>Rdkit/Rdkit: 2017_09_1 (Q3 2017) Release, 2017, https:// zenodo.org/record/1004356#.Wd3LDY6I2EI.
35 R. Zhao, T. Luong and E. Brevdo, Neural Machine Translation (seq2seq) Tutorial, 2017, https://github.com/tensorflow/nmt.
36 S. Hochreiter and J. Schmidhuber, Neural Comput., 1997, 9, 1735-1780.
37 A. Graves and J. Schmidhuber, Neural Network., 2005, 18, 602-610.
38 D. Bahdanau, K. Cho and Y. Bengio, ICLR, 2015.
39 R. J. Williams and D. Zipser, Neural Comput., 1989, 1, 270280.</p>
<p>40 T. Head, N. Campos, M. Cherti, A. Fabisch, T. Fan, M. Kumar, G. Louppe, K. Malone, M. Pak, I. Shcherbatyi, T. Smith and Z. Vinicius, Scikit-Optimize, 2017, http://scikitoptimize.github.io/.
41 J. H. Friedman, Ann. Stat., 2001, 29(5), 1189-1232.
42 J. Mockus, J. Global Optim., 1994, 4, 347-365.
43 K. Papineni, S. Roukos, T. Ward and W.-J. Zhu, ACL, 2001.
44 C.-Y. Lin, ACL, 2004.
45 N. Schneider, D. M. Lowe, R. A. Sayle, M. A. Tarselli and G. A. Landrum, J. Med. Chem., 2016, 59, 4385-4402.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>§ https://github.com/wengong-jin/nips17-rexgen.
$\mathbb{\square}$ https://ibm.box.com/v/ReactionSeq2SeqDataset.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
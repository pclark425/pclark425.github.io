<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3887 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3887</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3887</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-91.html">extraction-schema-91</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <p><strong>Paper ID:</strong> paper-c890b28a001c885d1f7aa05f5d24ead9bf6ae058</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/c890b28a001c885d1f7aa05f5d24ead9bf6ae058" target="_blank">MARG: Multi-Agent Review Generation for Scientific Papers</a></p>
                <p><strong>Paper Venue:</strong> arXiv.org</p>
                <p><strong>Paper TL;DR:</strong> This system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).</p>
                <p><strong>Paper Abstract:</strong> We study the ability of LLMs to generate feedback for scientific papers and develop MARG, a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from 60% to 29% and generating 3.7 good comments per paper (a 2.2x improvement).</p>
                <p><strong>Cost:</strong> 0.022</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3887.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3887.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARG-S</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Review Generation with Specialized Agents</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent LLM method that divides a scientific paper across multiple GPT-4 agent instances (leader, workers, and specialized expert agents) that communicate via natural-language messages to generate and refine actionable peer-review comments; includes a separate multi-agent refinement stage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>MARG-S</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Multi-agent architecture built from multiple chat-based LLM instances (GPT-4). A single leader agent coordinates worker agents (each given a chunk of the paper) and expert agents specialized on sub-tasks (experiments, clarity, impact). Agents communicate via a simple SEND MESSAGE protocol; the leader aggregates replies and produces review comments. A separate multi-agent refinement stage validates, refines, or prunes each comment.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Individual scientific papers (long documents split at paragraph boundaries into N chunks so each chunk fits GPT-4 8k context). Evaluation used a test set drawn from the ARIES corpus (subset of 30 papers for automated alignment; broader set for other experiments). Parsed text excludes figures/tables and has some equation parsing issues.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language task prompts given to the leader agent specifying the review-generation objective; three separate leader prompts (experiments/evaluation, clarity/reproducibility, novelty/impact). Expert agents receive specialized prompts describing their subtask (e.g., design expected experiments).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt-engineered multi-agent collaboration: distribute full-text across worker agents to overcome single-agent context limits; specialist expert agents perform focused sub-tasks; leader orchestrates queries and aggregates outputs; refinement stage runs an additional multi-agent group per comment to improve specificity and validity. Uses explicit message passing, summarization by agents, and pruning of histories to manage context window.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>A concatenated list of actionable peer-review comments (textual bullet comments), produced as three 'mini-reviews' (experiments, clarity, impact) then refined; typically yields many long, specific comments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Automated alignment-based evaluation against human reviewer comments extracted from the ARIES corpus (matching via multi-pass GPT-4 many-many and pairwise relatedness & specificity scoring), and a user study with 9 researcher participants rating specificity, accuracy, and overall helpfulness of comments (blind, randomized). Additional ablations and error analyses on agent message logs.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Outperformed baselines on automated recall (MARG-S recall 15.84% vs best baseline LiZCa 9.67% and SARG-TP 10.62%); in user study MARG-S produced on average 3.7 'good' comments per paper (vs 1.7 for the single-agent tuned baseline and 0.3 for Liang et al.'s method), and 71% of comments rated specific (vs 40% for others). MARG-S generated substantially more tokens (≈1.24M input tokens, 51k generated tokens per paper in their implementation) and took longer to run.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>High computational and token cost (order-of-magnitude more tokens/time than baselines); errors in inter-agent communication (missing context, misplaced SEND MESSAGE markers, message loops); failure modes in refinement (failure to prune invalid comments, ignored information, parsing issues from PDF parsing such as missing figures/tables and garbled equations); potential hallucinations or inaccurate comments; scalability concerns as message-history growth can approach context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Direct comparison to single-agent baselines (SARG-B, SARG-TP), a multi-agent tuned baseline (MARG-TP), and a prior GPT-4 review method (Liang et al. 2023 / LiZCa). MARG-S had highest recall and most user-rated 'good' comments, but lower precision/Jaccard (generates more comments, some invalid). Human-human agreement (human baseline) had higher precision but lower recall than some LLM methods.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e3887.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARG (general)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Review Generation</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>The general multi-agent approach introduced in this paper: multiple chat-based LLM instances (leader/worker/experts) cooperate by message passing to process long documents beyond single-agent context limits and produce synthesized outputs (here, peer-review comments).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>MARG</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Framework defining agent roles (leader, workers for chunks, experts for sub-tasks), inter-agent communication protocol (SEND MESSAGE), history pruning rules per agent type, and refinement groups; enables distributing long-text inputs across LLM instances and synthesizing information via internal discussion.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Long individual documents split into paragraph chunks; implementation used scientific paper PDFs (text parsed via Grobid) with figures/tables excluded from LLM input.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language task instructions to leader and role-specific system messages to experts/workers; leader constructs plans and delegates subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Collaborative LLM reasoning via explicit natural-language message passing and role specialization; information is preserved across agents by including chunk texts in worker histories and by leader-mediated aggregation instead of lossy summarization-only approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Aggregated textual outputs synthesized by the leader from worker and expert replies (in this study: lists of review comments), optionally refined by additional multi-agent groups.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Evaluated experimentally in this paper against baselines with automated alignment to human reviews and a user study; ablations measure effect of specialization, refinement, and number/type of agents.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Multi-agent approach enables processing full paper text (beyond single-agent context) and yields higher-quality, more specific comments when combined with specialization and refinement; raw multi-agent without specialization does not necessarily outperform single-agent tuned prompts.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Same as MARG-S: communication protocol failures, history token growth, complexity and higher compute cost, need for engineering of prompts and error-correction heuristics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Compared to single-agent baselines and a prior GPT-4 method (LiZCa); specialized multi-agent (MARG-S) clearly outperformed single-agent approaches when including experts and refinement, whereas an unspecialized multi-agent baseline (MARG-TP) sometimes underperformed tuned single-agent (SARG-TP).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e3887.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SARG-B</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Agent Review Generation with Basic prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A simple single-agent baseline that processes paper chunks independently using a minimal prompt and merges generated comment lists; emulates naive ChatGPT usage.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SARG-B</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Single GPT-4 agent run on each chunk independently with a one-line prompt 'Write feedback comments in the style of a scientific paper review...' then merge lists with another GPT call. No specialization; simple refinement applied via merging.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Same chunked scientific papers as other methods (paragraph-split chunks from parsed PDFs).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language prompt per chunk; no global coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Independent single-agent generation per chunk followed by a merge step to deduplicate/merge comments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Merged list of review comments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Same automated alignment to ARIES reviewer comments and user study.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Lowest automated scores among methods (recall 7.43%, precision 1.40%, Jaccard 1.25%), generated many generic short comments; user study fewer good comments (1.7 good comments per paper average).</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Independent chunk processing misses cross-chunk context leading to generic/invalid comments; poor specificity; truncation and lack of coordination.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Underperforms tuned single-agent (SARG-TP) and multi-agent specialized (MARG-S).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e3887.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>SARG-TP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Single-Agent Review Generation with Tuned Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A stronger single-agent baseline using a carefully tuned prompt per chunk and a refinement stage to improve specificity and validity of generated review comments.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>SARG-TP</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Single GPT-4 instance processes each chunk with a prompt engineered to produce more specific/actionable comments; includes a per-chunk refinement stage where the model sees other chunks' comments to prune incorrect ones.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Chunked scientific papers (same dataset as other methods).</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language tuned prompt; per-chunk prompts and a final merge/refine prompt.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Prompt engineering with iterative refinement: initial generation per chunk then refinement using additional context and merging.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Merged and refined list of review comments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Automated alignment against ARIES and the user study.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Better than SARG-B (recall 10.62%, precision 4.61%, Jaccard 3.46%); generated 11.6 comments on average and achieved some specificity improvement via tuned prompt and refinement.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Still constrained by single-instance context limitations; per-chunk independent processing can produce comments that require cross-chunk validation and thus rely on refinement stage to prune false positives.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Performed better than naive single-agent (SARG-B) but was outperformed by multi-agent specialized (MARG-S) in user-rated helpful comments count and specificity.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.4">
                <h3 class="extraction-instance">Extracted Data Instance 4 (e3887.4)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>MARG-TP</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Multi-Agent Review Generation with Tuned Prompt</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A multi-agent baseline similar to SARG-TP's prompt but using multiple communicating agents (leader/workers) without specialized expert agents; explores benefits of multi-agent framing alone.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>MARG-TP</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Multi-agent variant that uses the tuned single-agent prompt language but instantiates multiple communicating agents following the MARG communication protocol; lacks specialized expert agents and uses a refinement stage adapted to multi-agent flows.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Chunked papers as with other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Same tuned natural-language task prompt delivered to leader; agents coordinate via SEND MESSAGE.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Multi-agent communication with tuned prompting but without role specialization; leader mediates worker responses and merges into review comments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Synthesized list of review comments (merged across agents).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Automated alignment and user study comparisons.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mixed results: recall 8.49% (lower than SARG-TP's 10.62%); precision 5.34%; generated fewer comments on average (8.5), suggesting multi-agent framing alone does not guarantee improvement without specialization.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Multi-agent overhead without expert structure can be less effective than a well-tuned single-agent prompt; still subject to inter-agent communication errors and context-management complexity.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Underperformed tuned single-agent (SARG-TP) on recall; specialized MARG-S outperformed MARG-TP, illustrating the value of expert agents and structured subtasks.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.5">
                <h3 class="extraction-instance">Extracted Data Instance 5 (e3887.5)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>LiZCa</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Liang et al. (2023) GPT-4 review method (referred to as LiZCa)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A contemporaneous GPT-4 based method for review generation that truncates papers (rather than chunking) and produces outline-style reviews including non-actionable positive comments; used as a prior baseline in this work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>LiZCa (Liang et al., 2023 baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Single-agent GPT-4 approach that inputs a truncated portion of the paper (including figure/table captions) and prompts the model to produce an outline-style review; the authors of the current paper extract criticism-oriented items from the outlines to focus on actionable comments.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Truncated excerpts of the paper (includes captions); smaller input tokens than chunked multi-agent approaches.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language outline-review prompt applied to the truncated text; an extraction prompt is used to derive criticisms from the outline.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Direct prompting of GPT-4 on truncated paper input to generate an outline review; extraction/merging post-processing to isolate criticisms.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Outline-style review converted to a list of criticism-oriented review comments.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Included in the automated alignment evaluation and user study of this paper; comparisons against human reviews and other baselines.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Moderate automated recall (9.67%), highest precision among methods tested (9.96%), produced relatively few comments (≈4 per paper), and in the paper's user study produced few 'good' comments (0.3 good comments per paper reported in intro). LiZCa is more token-efficient and faster.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Truncation loses coverage of full paper leading to missed critiques; produced more generic/vague comments (high recall under lenient matching but drops with stricter specificity thresholds); fewer novel/specific comments; overall cheaper but less comprehensive.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Has better cost-to-recall ratio than MARG-S but lower user-rated helpful comment count and specificity; MARG-S outperformed LiZCa on number of good comments and specificity despite higher cost.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.6">
                <h3 class="extraction-instance">Extracted Data Instance 6 (e3887.6)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ARIES+GPT-4 extraction</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ARIES corpus comment extraction using GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Use of GPT-4 to extract actionable reviewer comments from the ARIES dataset of reviewer texts to create ground-truth comment sets for automated alignment evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>ARIES comment extraction with GPT-4</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>GPT-4 (gpt-4-0613) is prompted to extract actionable feedback comments from human review text in ARIES; five passes with random permutations were used in a many-many matching approach and pairwise GPT-4 scoring for alignment.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td>Review texts from ARIES dataset; the paper uses a subset of 30 papers' reviews for automated evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Natural-language instruction to GPT-4 to extract actionable reviewer criticisms (ignore minor grammar/style comments).</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>LLM-based extraction of comment-level items from unstructured reviewer text; many-many matching via multiple GPT-4 runs followed by pairwise relatedness/specificity scoring to form alignments.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Structured lists of actionable reviewer comments per paper used as ground-truth for automated matching metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td>Used as the gold/reference set for automated alignment evaluation against generated comments; authors note this approach is imperfect and likely a lower bound.</td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Provided the ground-truth comment sets enabling automated recall/precision/Jaccard metrics reported for methods. The extraction process included de-noising heuristics (requiring at least two of five many-many runs) and pairwise filtering by GPT-4.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>The extracted ground-truth is imperfect (real reviewers may miss critiques, or include unreasonable critiques); extraction depends on GPT-4 itself and may introduce biases or errors; limited annotation coverage (subset of 30 papers) and parsing inconsistency may lower reliability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.7">
                <h3 class="extraction-instance">Extracted Data Instance 7 (e3887.7)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Chunkwise summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Chunk-by-chunk summarization and concatenation (cited: Wu et al., 2021)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An input-management strategy where large documents are summarized one chunk at a time and the concatenation of summaries is used as input for downstream tasks, cited as prior context-management work.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Chunkwise summarization + concatenation</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Summarize large inputs chunk-by-chunk and then operate on concatenated summaries to stay within LLM context limits.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Not specified in detail within this paper; generally natural-language summarization prompts per chunk.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Sequential summarization of parts followed by composition (concatenation) of summaries for final processing by LLM.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Compressed textual summaries representing original chunks; used as surrogate input for downstream LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as an effective strategy when only parts of the input are needed or important details are obvious in advance; authors argue it may lose nuanced details necessary for review-generation tasks.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Potential loss of nuanced or critical details across chunk summarization steps; not ideal when specific low-level details anywhere in the document can change critique outcomes (as in paper review).</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_baselines_or_humans</strong></td>
                            <td>Mentioned as an alternative to multi-agent distribution but not directly compared experimentally in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.8">
                <h3 class="extraction-instance">Extracted Data Instance 8 (e3887.8)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Recursive summarization</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Recursive summarization / summary-of-history compression (cited: Wang et al., 2023a)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A context-management technique where models recursively summarize their input/output history to compress conversational context for longer interactions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Recursive summarization</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Iteratively compress the transcript/history of interactions (input and outputs) into shorter summaries so the effective context fits model limits while retaining salient information.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Repeated summarization of accumulated dialogue/history to produce compact representations.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Compressed conversational summaries usable as context for further LLM reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited among context-management approaches; authors note it's effective when details to keep are clear but may lose nuance important for detailed review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>May discard important low-level evidence; compression choices can remove details critical to accurate critique.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.9">
                <h3 class="extraction-instance">Extracted Data Instance 9 (e3887.9)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Retrieval-augmented approaches</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Retrieval-augmented generation and memory retrieval (cited: Xu et al., 2023; Bai et al., 2023; Wu et al., 2022)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Approaches that augment LLMs with external retrieval/memory modules to supply relevant context from large corpora at inference time, cited as context-management alternatives.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Retrieval-augmented generation / memory retrieval</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Use external retrieval over a data store (documents/corpus) or a learned memory to fetch relevant passages and provide them as additional context to the LLM, enabling reasoning over larger corpora without increasing model context size.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td>Typically a natural-language query or retrieval keys derived from the current task; not detailed in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Retrieve relevant passages and condition generation on retrieved context; sometimes combined with indexing or reranking.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Generated text (answers/summaries) conditioned on retrieved evidence.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as a viable strategy for scaling beyond context limits; authors note these work well when retrieval surfaces the necessary parts but might miss subtle cross-document details.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Relies on retrieval quality; may miss nuanced evidence not captured by retrieval queries; requires maintaining an external index/corpus.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.10">
                <h3 class="extraction-instance">Extracted Data Instance 10 (e3887.10)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Long-input transformer methods</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Long-context transformer architectures (cited: Wang et al., 2020b; Kitaev et al., 2020; Beltagy et al., 2020; Ivgi et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Architectural modifications to transformers (sparse/efficient attention, etc.) designed to increase effective input length; cited as alternatives that require model changes/retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Long-context transformer architectures</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Techniques such as sparse attention, locality-sensitive hashing, or other attention variants enabling longer sequences (e.g., Reformer, Longformer, BigBird-like approaches); these require architecture modification and retraining.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Architectural scaling of attention mechanisms to handle longer token sequences directly rather than via external strategies.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Same model outputs (text) but with larger effective receptive field over longer documents.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as promising but often not applicable when using fixed black-box APIs (e.g., GPT) that do not permit model changes.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Often requires model retraining from scratch and is not applicable for closed API LLMs; effective attention to all tokens in practice may still be limited.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.11">
                <h3 class="extraction-instance">Extracted Data Instance 11 (e3887.11)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Multi-agent LLM frameworks (recent)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>General multi-agent modeling works with LLMs (cited: Hong et al., 2023; Wu et al., 2023; Li et al., 2023; Park et al., 2023; Du et al., 2023; Wang et al., 2023b)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Recent lines of research that instantiate multiple LLM personas/agents to simulate societies or to improve reasoning via agent interaction; cited as related prior work but not focused on long-document scientific review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Multi-agent LLM frameworks</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Frameworks where multiple LLM instances (with distinct prompts/personas) interact via natural language or structured messages to achieve cooperative tasks, improve reasoning, or simulate multi-agent dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Agent specialization and interaction; message-passing protocols; sometimes role-specific prompts to induce distinct behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>Task-dependent outputs arising from agent cooperation (e.g., decisions, generated text, simulations).</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Cited as demonstrating multi-persona interactions can improve creativity and reasoning in other domains; current paper extends these ideas to scale input size and to technical scientific review generation.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Prior works do not typically address long-document inputs or scientific review-specific subtasks; communication protocols can introduce new failure modes (loops, missing context).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e3887.12">
                <h3 class="extraction-instance">Extracted Data Instance 12 (e3887.12)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of methods, systems, or studies that use large language models (LLMs) to distill theories or synthesize knowledge from large collections of scholarly papers, including details about the method, input corpus, topic/query specification, output, evaluation, results, and limitations.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Impressona</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Impressona (Benharrak et al., 2023)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An editor system that lets writers create AI personas (via GPT-4) to write comments on text; mentioned as related work focused on personalization rather than multi-agent problem-solving for scientific review.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_name</strong></td>
                            <td>Impressona</td>
                        </tr>
                        <tr>
                            <td><strong>system_or_method_description</strong></td>
                            <td>Editor tool to create AI personas to provide comments and feedback on writing using GPT-4; emphasized personalization of feedback.</td>
                        </tr>
                        <tr>
                            <td><strong>input_corpus_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>topic_or_query_specification</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>distillation_method</strong></td>
                            <td>Persona-based prompting of a single LLM instance to simulate different reviewer voices.</td>
                        </tr>
                        <tr>
                            <td><strong>output_type_and_format</strong></td>
                            <td>AI-generated comments on writer submissions.</td>
                        </tr>
                        <tr>
                            <td><strong>evaluation_or_validation_method</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>results_summary</strong></td>
                            <td>Mentioned as valuable for personalization but not tailored for scientific review generation or multi-agent LM-LM interactions; authors observed simple persona prompts perform poorly compared to their MARG-S approach for scientific peer review.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_challenges</strong></td>
                            <td>Doesn't explore LM-LM interactions or specialized expert agents; not designed for processing very long technical inputs.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MARG: Multi-Agent Review Generation for Scientific Papers', 'publication_date_yy_mm': '2024-01'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <p class="empty-note">No potentially relevant new papers extracted.</p>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3887",
    "paper_id": "paper-c890b28a001c885d1f7aa05f5d24ead9bf6ae058",
    "extraction_schema_id": "extraction-schema-91",
    "extracted_data": [
        {
            "name_short": "MARG-S",
            "name_full": "Multi-Agent Review Generation with Specialized Agents",
            "brief_description": "A multi-agent LLM method that divides a scientific paper across multiple GPT-4 agent instances (leader, workers, and specialized expert agents) that communicate via natural-language messages to generate and refine actionable peer-review comments; includes a separate multi-agent refinement stage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "MARG-S",
            "system_or_method_description": "Multi-agent architecture built from multiple chat-based LLM instances (GPT-4). A single leader agent coordinates worker agents (each given a chunk of the paper) and expert agents specialized on sub-tasks (experiments, clarity, impact). Agents communicate via a simple SEND MESSAGE protocol; the leader aggregates replies and produces review comments. A separate multi-agent refinement stage validates, refines, or prunes each comment.",
            "input_corpus_description": "Individual scientific papers (long documents split at paragraph boundaries into N chunks so each chunk fits GPT-4 8k context). Evaluation used a test set drawn from the ARIES corpus (subset of 30 papers for automated alignment; broader set for other experiments). Parsed text excludes figures/tables and has some equation parsing issues.",
            "topic_or_query_specification": "Natural-language task prompts given to the leader agent specifying the review-generation objective; three separate leader prompts (experiments/evaluation, clarity/reproducibility, novelty/impact). Expert agents receive specialized prompts describing their subtask (e.g., design expected experiments).",
            "distillation_method": "Prompt-engineered multi-agent collaboration: distribute full-text across worker agents to overcome single-agent context limits; specialist expert agents perform focused sub-tasks; leader orchestrates queries and aggregates outputs; refinement stage runs an additional multi-agent group per comment to improve specificity and validity. Uses explicit message passing, summarization by agents, and pruning of histories to manage context window.",
            "output_type_and_format": "A concatenated list of actionable peer-review comments (textual bullet comments), produced as three 'mini-reviews' (experiments, clarity, impact) then refined; typically yields many long, specific comments.",
            "evaluation_or_validation_method": "Automated alignment-based evaluation against human reviewer comments extracted from the ARIES corpus (matching via multi-pass GPT-4 many-many and pairwise relatedness & specificity scoring), and a user study with 9 researcher participants rating specificity, accuracy, and overall helpfulness of comments (blind, randomized). Additional ablations and error analyses on agent message logs.",
            "results_summary": "Outperformed baselines on automated recall (MARG-S recall 15.84% vs best baseline LiZCa 9.67% and SARG-TP 10.62%); in user study MARG-S produced on average 3.7 'good' comments per paper (vs 1.7 for the single-agent tuned baseline and 0.3 for Liang et al.'s method), and 71% of comments rated specific (vs 40% for others). MARG-S generated substantially more tokens (≈1.24M input tokens, 51k generated tokens per paper in their implementation) and took longer to run.",
            "limitations_or_challenges": "High computational and token cost (order-of-magnitude more tokens/time than baselines); errors in inter-agent communication (missing context, misplaced SEND MESSAGE markers, message loops); failure modes in refinement (failure to prune invalid comments, ignored information, parsing issues from PDF parsing such as missing figures/tables and garbled equations); potential hallucinations or inaccurate comments; scalability concerns as message-history growth can approach context limits.",
            "comparison_to_baselines_or_humans": "Direct comparison to single-agent baselines (SARG-B, SARG-TP), a multi-agent tuned baseline (MARG-TP), and a prior GPT-4 review method (Liang et al. 2023 / LiZCa). MARG-S had highest recall and most user-rated 'good' comments, but lower precision/Jaccard (generates more comments, some invalid). Human-human agreement (human baseline) had higher precision but lower recall than some LLM methods.",
            "uuid": "e3887.0",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "MARG (general)",
            "name_full": "Multi-Agent Review Generation",
            "brief_description": "The general multi-agent approach introduced in this paper: multiple chat-based LLM instances (leader/worker/experts) cooperate by message passing to process long documents beyond single-agent context limits and produce synthesized outputs (here, peer-review comments).",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "MARG",
            "system_or_method_description": "Framework defining agent roles (leader, workers for chunks, experts for sub-tasks), inter-agent communication protocol (SEND MESSAGE), history pruning rules per agent type, and refinement groups; enables distributing long-text inputs across LLM instances and synthesizing information via internal discussion.",
            "input_corpus_description": "Long individual documents split into paragraph chunks; implementation used scientific paper PDFs (text parsed via Grobid) with figures/tables excluded from LLM input.",
            "topic_or_query_specification": "Natural-language task instructions to leader and role-specific system messages to experts/workers; leader constructs plans and delegates subtasks.",
            "distillation_method": "Collaborative LLM reasoning via explicit natural-language message passing and role specialization; information is preserved across agents by including chunk texts in worker histories and by leader-mediated aggregation instead of lossy summarization-only approaches.",
            "output_type_and_format": "Aggregated textual outputs synthesized by the leader from worker and expert replies (in this study: lists of review comments), optionally refined by additional multi-agent groups.",
            "evaluation_or_validation_method": "Evaluated experimentally in this paper against baselines with automated alignment to human reviews and a user study; ablations measure effect of specialization, refinement, and number/type of agents.",
            "results_summary": "Multi-agent approach enables processing full paper text (beyond single-agent context) and yields higher-quality, more specific comments when combined with specialization and refinement; raw multi-agent without specialization does not necessarily outperform single-agent tuned prompts.",
            "limitations_or_challenges": "Same as MARG-S: communication protocol failures, history token growth, complexity and higher compute cost, need for engineering of prompts and error-correction heuristics.",
            "comparison_to_baselines_or_humans": "Compared to single-agent baselines and a prior GPT-4 method (LiZCa); specialized multi-agent (MARG-S) clearly outperformed single-agent approaches when including experts and refinement, whereas an unspecialized multi-agent baseline (MARG-TP) sometimes underperformed tuned single-agent (SARG-TP).",
            "uuid": "e3887.1",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SARG-B",
            "name_full": "Single-Agent Review Generation with Basic prompt",
            "brief_description": "A simple single-agent baseline that processes paper chunks independently using a minimal prompt and merges generated comment lists; emulates naive ChatGPT usage.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "SARG-B",
            "system_or_method_description": "Single GPT-4 agent run on each chunk independently with a one-line prompt 'Write feedback comments in the style of a scientific paper review...' then merge lists with another GPT call. No specialization; simple refinement applied via merging.",
            "input_corpus_description": "Same chunked scientific papers as other methods (paragraph-split chunks from parsed PDFs).",
            "topic_or_query_specification": "Natural-language prompt per chunk; no global coordination.",
            "distillation_method": "Independent single-agent generation per chunk followed by a merge step to deduplicate/merge comments.",
            "output_type_and_format": "Merged list of review comments.",
            "evaluation_or_validation_method": "Same automated alignment to ARIES reviewer comments and user study.",
            "results_summary": "Lowest automated scores among methods (recall 7.43%, precision 1.40%, Jaccard 1.25%), generated many generic short comments; user study fewer good comments (1.7 good comments per paper average).",
            "limitations_or_challenges": "Independent chunk processing misses cross-chunk context leading to generic/invalid comments; poor specificity; truncation and lack of coordination.",
            "comparison_to_baselines_or_humans": "Underperforms tuned single-agent (SARG-TP) and multi-agent specialized (MARG-S).",
            "uuid": "e3887.2",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "SARG-TP",
            "name_full": "Single-Agent Review Generation with Tuned Prompt",
            "brief_description": "A stronger single-agent baseline using a carefully tuned prompt per chunk and a refinement stage to improve specificity and validity of generated review comments.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "SARG-TP",
            "system_or_method_description": "Single GPT-4 instance processes each chunk with a prompt engineered to produce more specific/actionable comments; includes a per-chunk refinement stage where the model sees other chunks' comments to prune incorrect ones.",
            "input_corpus_description": "Chunked scientific papers (same dataset as other methods).",
            "topic_or_query_specification": "Natural-language tuned prompt; per-chunk prompts and a final merge/refine prompt.",
            "distillation_method": "Prompt engineering with iterative refinement: initial generation per chunk then refinement using additional context and merging.",
            "output_type_and_format": "Merged and refined list of review comments.",
            "evaluation_or_validation_method": "Automated alignment against ARIES and the user study.",
            "results_summary": "Better than SARG-B (recall 10.62%, precision 4.61%, Jaccard 3.46%); generated 11.6 comments on average and achieved some specificity improvement via tuned prompt and refinement.",
            "limitations_or_challenges": "Still constrained by single-instance context limitations; per-chunk independent processing can produce comments that require cross-chunk validation and thus rely on refinement stage to prune false positives.",
            "comparison_to_baselines_or_humans": "Performed better than naive single-agent (SARG-B) but was outperformed by multi-agent specialized (MARG-S) in user-rated helpful comments count and specificity.",
            "uuid": "e3887.3",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "MARG-TP",
            "name_full": "Multi-Agent Review Generation with Tuned Prompt",
            "brief_description": "A multi-agent baseline similar to SARG-TP's prompt but using multiple communicating agents (leader/workers) without specialized expert agents; explores benefits of multi-agent framing alone.",
            "citation_title": "here",
            "mention_or_use": "use",
            "system_or_method_name": "MARG-TP",
            "system_or_method_description": "Multi-agent variant that uses the tuned single-agent prompt language but instantiates multiple communicating agents following the MARG communication protocol; lacks specialized expert agents and uses a refinement stage adapted to multi-agent flows.",
            "input_corpus_description": "Chunked papers as with other methods.",
            "topic_or_query_specification": "Same tuned natural-language task prompt delivered to leader; agents coordinate via SEND MESSAGE.",
            "distillation_method": "Multi-agent communication with tuned prompting but without role specialization; leader mediates worker responses and merges into review comments.",
            "output_type_and_format": "Synthesized list of review comments (merged across agents).",
            "evaluation_or_validation_method": "Automated alignment and user study comparisons.",
            "results_summary": "Mixed results: recall 8.49% (lower than SARG-TP's 10.62%); precision 5.34%; generated fewer comments on average (8.5), suggesting multi-agent framing alone does not guarantee improvement without specialization.",
            "limitations_or_challenges": "Multi-agent overhead without expert structure can be less effective than a well-tuned single-agent prompt; still subject to inter-agent communication errors and context-management complexity.",
            "comparison_to_baselines_or_humans": "Underperformed tuned single-agent (SARG-TP) on recall; specialized MARG-S outperformed MARG-TP, illustrating the value of expert agents and structured subtasks.",
            "uuid": "e3887.4",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "LiZCa",
            "name_full": "Liang et al. (2023) GPT-4 review method (referred to as LiZCa)",
            "brief_description": "A contemporaneous GPT-4 based method for review generation that truncates papers (rather than chunking) and produces outline-style reviews including non-actionable positive comments; used as a prior baseline in this work.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "LiZCa (Liang et al., 2023 baseline)",
            "system_or_method_description": "Single-agent GPT-4 approach that inputs a truncated portion of the paper (including figure/table captions) and prompts the model to produce an outline-style review; the authors of the current paper extract criticism-oriented items from the outlines to focus on actionable comments.",
            "input_corpus_description": "Truncated excerpts of the paper (includes captions); smaller input tokens than chunked multi-agent approaches.",
            "topic_or_query_specification": "Natural-language outline-review prompt applied to the truncated text; an extraction prompt is used to derive criticisms from the outline.",
            "distillation_method": "Direct prompting of GPT-4 on truncated paper input to generate an outline review; extraction/merging post-processing to isolate criticisms.",
            "output_type_and_format": "Outline-style review converted to a list of criticism-oriented review comments.",
            "evaluation_or_validation_method": "Included in the automated alignment evaluation and user study of this paper; comparisons against human reviews and other baselines.",
            "results_summary": "Moderate automated recall (9.67%), highest precision among methods tested (9.96%), produced relatively few comments (≈4 per paper), and in the paper's user study produced few 'good' comments (0.3 good comments per paper reported in intro). LiZCa is more token-efficient and faster.",
            "limitations_or_challenges": "Truncation loses coverage of full paper leading to missed critiques; produced more generic/vague comments (high recall under lenient matching but drops with stricter specificity thresholds); fewer novel/specific comments; overall cheaper but less comprehensive.",
            "comparison_to_baselines_or_humans": "Has better cost-to-recall ratio than MARG-S but lower user-rated helpful comment count and specificity; MARG-S outperformed LiZCa on number of good comments and specificity despite higher cost.",
            "uuid": "e3887.5",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "ARIES+GPT-4 extraction",
            "name_full": "ARIES corpus comment extraction using GPT-4",
            "brief_description": "Use of GPT-4 to extract actionable reviewer comments from the ARIES dataset of reviewer texts to create ground-truth comment sets for automated alignment evaluation.",
            "citation_title": "",
            "mention_or_use": "use",
            "system_or_method_name": "ARIES comment extraction with GPT-4",
            "system_or_method_description": "GPT-4 (gpt-4-0613) is prompted to extract actionable feedback comments from human review text in ARIES; five passes with random permutations were used in a many-many matching approach and pairwise GPT-4 scoring for alignment.",
            "input_corpus_description": "Review texts from ARIES dataset; the paper uses a subset of 30 papers' reviews for automated evaluation.",
            "topic_or_query_specification": "Natural-language instruction to GPT-4 to extract actionable reviewer criticisms (ignore minor grammar/style comments).",
            "distillation_method": "LLM-based extraction of comment-level items from unstructured reviewer text; many-many matching via multiple GPT-4 runs followed by pairwise relatedness/specificity scoring to form alignments.",
            "output_type_and_format": "Structured lists of actionable reviewer comments per paper used as ground-truth for automated matching metrics.",
            "evaluation_or_validation_method": "Used as the gold/reference set for automated alignment evaluation against generated comments; authors note this approach is imperfect and likely a lower bound.",
            "results_summary": "Provided the ground-truth comment sets enabling automated recall/precision/Jaccard metrics reported for methods. The extraction process included de-noising heuristics (requiring at least two of five many-many runs) and pairwise filtering by GPT-4.",
            "limitations_or_challenges": "The extracted ground-truth is imperfect (real reviewers may miss critiques, or include unreasonable critiques); extraction depends on GPT-4 itself and may introduce biases or errors; limited annotation coverage (subset of 30 papers) and parsing inconsistency may lower reliability.",
            "uuid": "e3887.6",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Chunkwise summarization",
            "name_full": "Chunk-by-chunk summarization and concatenation (cited: Wu et al., 2021)",
            "brief_description": "An input-management strategy where large documents are summarized one chunk at a time and the concatenation of summaries is used as input for downstream tasks, cited as prior context-management work.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Chunkwise summarization + concatenation",
            "system_or_method_description": "Summarize large inputs chunk-by-chunk and then operate on concatenated summaries to stay within LLM context limits.",
            "input_corpus_description": null,
            "topic_or_query_specification": "Not specified in detail within this paper; generally natural-language summarization prompts per chunk.",
            "distillation_method": "Sequential summarization of parts followed by composition (concatenation) of summaries for final processing by LLM.",
            "output_type_and_format": "Compressed textual summaries representing original chunks; used as surrogate input for downstream LLM reasoning.",
            "evaluation_or_validation_method": null,
            "results_summary": "Cited as an effective strategy when only parts of the input are needed or important details are obvious in advance; authors argue it may lose nuanced details necessary for review-generation tasks.",
            "limitations_or_challenges": "Potential loss of nuanced or critical details across chunk summarization steps; not ideal when specific low-level details anywhere in the document can change critique outcomes (as in paper review).",
            "comparison_to_baselines_or_humans": "Mentioned as an alternative to multi-agent distribution but not directly compared experimentally in this paper.",
            "uuid": "e3887.7",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Recursive summarization",
            "name_full": "Recursive summarization / summary-of-history compression (cited: Wang et al., 2023a)",
            "brief_description": "A context-management technique where models recursively summarize their input/output history to compress conversational context for longer interactions.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Recursive summarization",
            "system_or_method_description": "Iteratively compress the transcript/history of interactions (input and outputs) into shorter summaries so the effective context fits model limits while retaining salient information.",
            "input_corpus_description": null,
            "topic_or_query_specification": null,
            "distillation_method": "Repeated summarization of accumulated dialogue/history to produce compact representations.",
            "output_type_and_format": "Compressed conversational summaries usable as context for further LLM reasoning.",
            "evaluation_or_validation_method": null,
            "results_summary": "Cited among context-management approaches; authors note it's effective when details to keep are clear but may lose nuance important for detailed review generation.",
            "limitations_or_challenges": "May discard important low-level evidence; compression choices can remove details critical to accurate critique.",
            "uuid": "e3887.8",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Retrieval-augmented approaches",
            "name_full": "Retrieval-augmented generation and memory retrieval (cited: Xu et al., 2023; Bai et al., 2023; Wu et al., 2022)",
            "brief_description": "Approaches that augment LLMs with external retrieval/memory modules to supply relevant context from large corpora at inference time, cited as context-management alternatives.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Retrieval-augmented generation / memory retrieval",
            "system_or_method_description": "Use external retrieval over a data store (documents/corpus) or a learned memory to fetch relevant passages and provide them as additional context to the LLM, enabling reasoning over larger corpora without increasing model context size.",
            "input_corpus_description": null,
            "topic_or_query_specification": "Typically a natural-language query or retrieval keys derived from the current task; not detailed in this paper.",
            "distillation_method": "Retrieve relevant passages and condition generation on retrieved context; sometimes combined with indexing or reranking.",
            "output_type_and_format": "Generated text (answers/summaries) conditioned on retrieved evidence.",
            "evaluation_or_validation_method": null,
            "results_summary": "Mentioned as a viable strategy for scaling beyond context limits; authors note these work well when retrieval surfaces the necessary parts but might miss subtle cross-document details.",
            "limitations_or_challenges": "Relies on retrieval quality; may miss nuanced evidence not captured by retrieval queries; requires maintaining an external index/corpus.",
            "uuid": "e3887.9",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Long-input transformer methods",
            "name_full": "Long-context transformer architectures (cited: Wang et al., 2020b; Kitaev et al., 2020; Beltagy et al., 2020; Ivgi et al., 2023)",
            "brief_description": "Architectural modifications to transformers (sparse/efficient attention, etc.) designed to increase effective input length; cited as alternatives that require model changes/retraining.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Long-context transformer architectures",
            "system_or_method_description": "Techniques such as sparse attention, locality-sensitive hashing, or other attention variants enabling longer sequences (e.g., Reformer, Longformer, BigBird-like approaches); these require architecture modification and retraining.",
            "input_corpus_description": null,
            "topic_or_query_specification": null,
            "distillation_method": "Architectural scaling of attention mechanisms to handle longer token sequences directly rather than via external strategies.",
            "output_type_and_format": "Same model outputs (text) but with larger effective receptive field over longer documents.",
            "evaluation_or_validation_method": null,
            "results_summary": "Cited as promising but often not applicable when using fixed black-box APIs (e.g., GPT) that do not permit model changes.",
            "limitations_or_challenges": "Often requires model retraining from scratch and is not applicable for closed API LLMs; effective attention to all tokens in practice may still be limited.",
            "uuid": "e3887.10",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Multi-agent LLM frameworks (recent)",
            "name_full": "General multi-agent modeling works with LLMs (cited: Hong et al., 2023; Wu et al., 2023; Li et al., 2023; Park et al., 2023; Du et al., 2023; Wang et al., 2023b)",
            "brief_description": "Recent lines of research that instantiate multiple LLM personas/agents to simulate societies or to improve reasoning via agent interaction; cited as related prior work but not focused on long-document scientific review generation.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Multi-agent LLM frameworks",
            "system_or_method_description": "Frameworks where multiple LLM instances (with distinct prompts/personas) interact via natural language or structured messages to achieve cooperative tasks, improve reasoning, or simulate multi-agent dynamics.",
            "input_corpus_description": null,
            "topic_or_query_specification": null,
            "distillation_method": "Agent specialization and interaction; message-passing protocols; sometimes role-specific prompts to induce distinct behaviors.",
            "output_type_and_format": "Task-dependent outputs arising from agent cooperation (e.g., decisions, generated text, simulations).",
            "evaluation_or_validation_method": null,
            "results_summary": "Cited as demonstrating multi-persona interactions can improve creativity and reasoning in other domains; current paper extends these ideas to scale input size and to technical scientific review generation.",
            "limitations_or_challenges": "Prior works do not typically address long-document inputs or scientific review-specific subtasks; communication protocols can introduce new failure modes (loops, missing context).",
            "uuid": "e3887.11",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        },
        {
            "name_short": "Impressona",
            "name_full": "Impressona (Benharrak et al., 2023)",
            "brief_description": "An editor system that lets writers create AI personas (via GPT-4) to write comments on text; mentioned as related work focused on personalization rather than multi-agent problem-solving for scientific review.",
            "citation_title": "",
            "mention_or_use": "mention",
            "system_or_method_name": "Impressona",
            "system_or_method_description": "Editor tool to create AI personas to provide comments and feedback on writing using GPT-4; emphasized personalization of feedback.",
            "input_corpus_description": null,
            "topic_or_query_specification": null,
            "distillation_method": "Persona-based prompting of a single LLM instance to simulate different reviewer voices.",
            "output_type_and_format": "AI-generated comments on writer submissions.",
            "evaluation_or_validation_method": null,
            "results_summary": "Mentioned as valuable for personalization but not tailored for scientific review generation or multi-agent LM-LM interactions; authors observed simple persona prompts perform poorly compared to their MARG-S approach for scientific peer review.",
            "limitations_or_challenges": "Doesn't explore LM-LM interactions or specialized expert agents; not designed for processing very long technical inputs.",
            "uuid": "e3887.12",
            "source_info": {
                "paper_title": "MARG: Multi-Agent Review Generation for Scientific Papers",
                "publication_date_yy_mm": "2024-01"
            }
        }
    ],
    "potentially_relevant_new_papers": [],
    "cost": 0.02245275,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>MARG: Multi-Agent Review Generation for Scientific Papers</h1>
<p>Mike D'Arcy ${ }^{1}$, Tom Hope ${ }^{2,3}$, Larry Birnbaum ${ }^{1}$, Doug Downey ${ }^{1,3}$<br>${ }^{1}$ Northwestern University, ${ }^{2}$ The Hebrew University of Jerusalem, ${ }^{3}$ Allen Institute for AI, m.m.darcy@u.northwestern.edu, tomh@allenai.org, l-birnbaum@northwestern.edu, dougd@allenai.org</p>
<h4>Abstract</h4>
<p>We study the ability of LLMs to generate feedback for scientific papers and develop MARG, ${ }^{1}$ a feedback generation approach using multiple LLM instances that engage in internal discussion. By distributing paper text across agents, MARG can consume the full text of papers beyond the input length limitations of the base LLM, and by specializing agents and incorporating sub-tasks tailored to different comment types (experiments, clarity, impact) it improves the helpfulness and specificity of feedback. In a user study, baseline methods using GPT-4 were rated as producing generic or very generic comments more than half the time, and only 1.7 comments per paper were rated as good overall in the best baseline. Our system substantially improves the ability of GPT-4 to generate specific and helpful feedback, reducing the rate of generic comments from $60 \%$ to $29 \%$ and generating 3.7 good comments per paper (a 2.2 x improvement).</p>
<h2>1 Introduction</h2>
<p>In recent years, the capabilities of large language models (LLMs) have advanced dramatically, resulting in modern models such as GPT-4 that can perform comparably to humans in some tasks (OpenAI, 2023). These advancements provide hope that LLMs may be able to assist human researchers with their writing (Fok and Weld, 2023; Shen et al., 2023; Mahlow, 2023; Gmeiner and Yildirim, 2023); however, most modern LLMs can only consume limited amounts of text and are primarily trained on non-technical text such as news articles and websites. The ability of models to comprehend and produce long, highly technical text-such as that of scientific papers-remains under-explored.</p>
<p>In this paper, we study the task of automatically generating actionable peer-review feedback for a scientific paper. This task comprises several reasoning challenges: a reviewer must understand the</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>intent and significance of a work, the technical details of the methodology, and the nuances of how an experiment or proof can be claimed to support a particular conclusion. They must then identify the ways in a which a paper does or does not fall short and articulate suggestions for improvement.</p>
<p>Modern large language models (LLMs) face a technical challenge in addition to the reasoning challenges involved in generating reviews: namely, they are limited in the total number of tokens they can effectively reason over at once. As scientific papers can be quite long (thousands or tens of thousands of tokens, in our case), there are many cases in which it is not even possible to provide the whole paper in the model's input. Even for models that technically support large inputs, they often cannot use the full capacity effectively in practice (Qin et al., 2023; Liu et al., 2023).</p>
<p>We propose multi-agent review generation (MARG), a method for generating peer-review feedback by prompting an LLM (GPT-4). We find that by using multiple instances of GPT (hereinafter referred to as "agents"), giving each a portion of the paper, and allowing the agents to communicate with each other, it is possible to generate feedback across the whole paper. We additionally find that by including aspect-specific "expert" GPT agents to separately assist with generating comments on experiments, clarity, and impact, the method can perform significantly better than when having a lone agent attempt to generate all types of feedback at once; we refer to this specialized variant of our method as MARG-S.</p>
<p>In a user study, MARG-S generated 3.7 "good" comments per paper (rated by users), whereas a simple baseline of having a single agent generate all comments generated only 1.7 good comments, and a recently proposed method (Liang et al., 2023) produced only 0.3 . In addition, we found that while users perceived the majority of the comments generated by the baselines as being generic, the vast ma-</p>
<p>jority (71\%) of MARG-S's comments were rated as specific. Finally, we analyze the weaknesses of MARG-S, including high cost and internal communication errors (e.g., failing to include key information in some messages), and suggest directions for future work.</p>
<p>In summary, our contributions are as follows:</p>
<ul>
<li>We propose a novel method (MARG) that can generate high-quality peer-review feedback even for papers longer than the context size of the base model.</li>
<li>We evaluate the quality of our generated feedback against two baselines, using both automatic metrics and a user study. We find that our method outperforms the strongest baseline by 6.1 recall points in the automated evaluation and generates 2.2 x as many helpful comments per review in the user study.</li>
<li>We conduct a thorough analysis of the generated feedback, finding that our proposed method preserves accuracy while generating much more specific comments.</li>
</ul>
<h2>2 Related work</h2>
<h3>2.1 Review generation</h3>
<p>There has been a variety of work that aims to score or improve papers in specific aspects, such as checking statistical tests (Nuijten and Polanin, 2020), plagiarism detection (Kalnins et al., 2015), citation recommendation (Ali et al., 2020), and review score prediction (Basuki and Tsuchiya, 2022; Bharti et al., 2023), among others (Kousha and Thelwall, 2023). While these are useful tools, they are limited in scope compared to the breadth of feedback authors receive from a real review; our work aims to produce free-form textual review comments across a variety of aspects.</p>
<p>Past work on automatic review generation primarily does so using (relatively) small models that cannot consume the full text of a paper (Yuan and Liu, 2022) or use template-filling instead of generating nuanced free-form comments (Wang et al., 2020a). More recent work has explored using GPT4 (OpenAI, 2023) to verify author checklists (Liu and Shah, 2023), but this limits the variety in generated comment types.</p>
<p>Impressona (Benharrak et al., 2023) is an editor that allows writers to create AI personas (via GPT4) to write comments on their work; this is valuable for personalization of feedback, but doesn't
focus on finding good techniques and prompts for scientific review generation, and doesn't explore LM-LM interactions; as we show, a simple prompt (akin to what a user might try initially) does poorly on our task compared to our method.</p>
<p>Contemporaneously with our work, Liang et al. (2023) conducted a large user study of review generation using GPT-4, finding that GPT-4 could generate helpful review comments. However, that work simply truncated long papers and did not attempt to address the input size limitations of GPT-4. In addition, they used a single prompt rather than attempting to construct specialized prompts and "experts" for different comment types, as we do. We compare our proposed method to that of Liang et al. (2023) and find that while their approach is more efficient, ours produces more helpful comments.</p>
<h3>2.2 Multi-agent modeling</h3>
<p>In games and robotics tasks, where there are often distinct roles being performed or multiple physical agents operating in the same environment, various problem-solving algorithms and reinforcement learning techniques have been studied to enable cooperation between agents (Zhang et al., 2021; Oroojlooy and Hajinezhad, 2022). Not all of these use communication for cooperation, and those that do typically exchange symbols or vectors rather than natural-language messages.</p>
<p>Recent work has explored multi-persona interaction with prompted LLMs to simulate artificial societies (Li et al., 2023; Park et al., 2023) and to improve reasoning abilities (Du et al., 2023; Wang et al., 2023b), but this work does not explore the use of multi-agent modeling to scale input size limits and does not investigate their potential for highly technical tasks like scientific review generation.</p>
<p>Contemporaneously with our work, Hong et al. (2023) and Wu et al. (2023) have proposed general frameworks for multi-agent modeling with large language models such as GPT. Wang et al. (2023b) has also proposed multi-persona collaboration as a way to improve LLM creativity, although they do not investigate the ability of multi-agent modeling to scale input size limits. However, none of these works explore review generation applications.</p>
<h3>2.3 LLM context management</h3>
<p>One advantage of multi-agent modeling is to circumvent the input-size limitations of LLMs, which are often prohibitive for long documents. A variety</p>
<p>of other techniques have been investigated in prior work.</p>
<p>Several works have proposed methods for modifying LLM architectures in order to increase the effective input size by using alternative attention formulations (Wang et al., 2020b; Kitaev et al., 2020; Beltagy et al., 2020; Ivgi et al., 2023) or incorporating memory retrieval (Wu et al., 2022). However, architecture changes often cannot be applied without retraining models from scratch, and powerful LLMs such as GPT are sometimes available only through a fixed API that does not allow low-level model modifications. This motivates us to explore techniques that can be applied without changing the underlying model.</p>
<p>Recently, there has been work exploring context management in LLMs by having models summarize a large input one chunk at a time and then operate on the concatenation of the summaries (Wu et al., 2021), recursively summarize their input/output history to compress it (Wang et al., 2023a), or incorporate retrieval (Xu et al., 2023; Bai et al., 2023). These strategies are effective when only part of the input is needed or when it is clear in advance what details will be important; however, in our review generation task, a paper's shortcomings may involve nuanced details that would be lost with extraction or summarization techniques, so we divide the input among multiple agents that collectively retain the full text throughout the task.</p>
<h2>3 Task definition</h2>
<p>We formulate our task as follows: given a scientific paper, generate a list of actionable feedback comments that could help authors to improve the paper. Actionable feedback is defined the same way as in D'Arcy et al. (2023); that is, we focus on suggestions and criticism (including implied suggestions-e.g., a question might imply a need for clarification in the paper) rather than positive remarks (e.g., "The paper is sound and of certain interest"). In addition, we focus on substantive comments rather than simple grammatical or stylistic errors.</p>
<p>In both our multi-agent approach and our simple baseline, a paper is split into chunks of text so that each chunk can fit into the model's input. The splits are made on paragraph boundaries to avoid breaking sentences, and when presenting the text to the model we annotate each paragraph with its position in the paper (paragraph 1, 2, 3, etc) and the name of the section it appears in.</p>
<p>We note that the input format we use does not include figures or tables (as GPT-4 is a pure language model, ${ }^{2}$ it cannot consume this information), and many equations are garbled or incomplete due to parsing limitations. Nonetheless, we expect that many comments can be identified from the text alone, as the main conclusions from tables and figures are often stated in text.</p>
<h2>4 Multi-agent review generation</h2>
<p>In this section, we describe our proposed multiagent method for generating peer-review feedback, which we call MARG-S (Multi-Agent Review Generation with Specialized Agents). At a high level, our multi-agent architecture is formulated as follows: We define an agent as one instance of a chat-based LLM (ChatGPT, in our case); each agent has its own chat history and prompt(s). We initialize a set of agents, including three distinct types: (1) a leader agent, which is in charge of coordinating the task and the communication among agents, (2) one or more worker agents, which each receive a chunk of the task data (the paper), and (3) zero or more expert agents, which are prompted to specialize in some sub-task that serves to assist the leader agent in performing the task effectively. The leader agent is given a protocol with which it can broadcast a message to all other agents and receive responses. Finally, the leader agent is given the task instructions, and must send messages to other agents in order to obtain information and delegate sub-tasks in order to produce the final output.</p>
<p>Prompts for all of our methods can be found in Appendix A. An overview of our multi-agent architecture is shown in Figure 1.</p>
<h3>4.1 Agents</h3>
<p>Chat-based LLMs, such as ChatGPT, take an input that consists of a list of messages. Each message consists of the message content and the "role" that the message is from, which in ChatGPT's case can be either the "system", the "user", or the "assistant" (i.e., generated by the LLM). Typically, an input to the model starts with a "system" message that describes general instructions that it must follow (e.g., "always give concise and helpful answers"), then the "user" writes a message ("summarize the</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h1>Multi-Agent Group</h1>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Overview of our multi-agent architecture.</p>
<p>MARG-S
<img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of MARG-S, which consists of several specialized multi-agent groups. The comments from each group are concatenated to produce the overall review, and each comment is refined (and potentially pruned) by an additional multi-agent group to produce the final review.
following passage: ..."), and the generated response is treated as an "assistant" message. The message history serves as a form of context management; with it, agents can use information from previous interactions in the conversation when formulating future responses.</p>
<p>We use the "system" message at the start of a message history to give unique instructions to each agent type. For example, the "leader" agent is told that it is the leader, that it must coordinate other agents to complete the user's requests, and that it can communicate by using a special "SEND MESSAGE" command to broadcast messages to other agents. It is also given some guidelines to improve its reasoning; for example, it is instructed to create a high-level plan from its task instructions before it begins communicating and performing sub-tasks. The "worker" agents are told that they must obey instructions from the leader agent, and "experts" are given special instructions depending on the subtask they need to perform.</p>
<p>Despite their name, expert agents do not actually have more information or expertise than any other
agent. Rather, they are given a special prompt that is designed to encourage them to specialize in a particular sub-task. For example, an expert agent that is asked to focus on experiments and evaluation is given a prompt that encourages it to think about the kinds of experiments that it would expect to see in order to support a particular claim, and then compare those hypothesized experiments to the real experiments in the paper. We found in preliminary testing that giving such instructions to the leader agent tends to work poorly and often ignores details of the instructions, as though the model is overloaded by the number of instructions it is trying to follow, while refactoring the subtask to the separate expert model produces a much higher-quality result.</p>
<p>All agents are given some information about the agent group; they are told how many agents are in the group and the IDs of the agents (while the IDs are not directly used in the communication protocol, they are useful for internal chain-of-thought; for example, the leader might note that it needs to follow up with a particular agent). In our setting</p>
<p>there is always exactly one leader agent, $N$ worker agents for a paper with $N$ chunks, and zero or more expert agents.</p>
<h3>4.2 Communication</h3>
<p>The leader agent is able to send messages to other agents by outputting a special string ("SEND MESSAGE: ") followed by the message content. The message is then broadcast to all other agents in the group. When an agent receives a message, it is appended to the history as a "system" message with the header "Message from <agent id>:" preceding the message. The LLM is then run to generate a response to the received message, and this response is always treated as a reply to the leader agent. Replies from all agents are added to the message history of the leader agent before generating the next output from the leader.</p>
<p>When the leader agent generates an output that does not send a message (and thus does not seek any additional information), the task is complete and we prompt the agent to return the final answer.</p>
<h2>Error correction</h2>
<p>We attempt to correct a few common errors that occur in practice when agents try to use the communication protocol. In some cases, agents get stuck in a loop, often when the task is complete. For example, the leader agent might send a message saying "Thank you all for your feedback and cooperation.", the workers respond with "You're welcome, Agent 0.", the leader says "Thank you all for your responses.", and this loop of unending pleasantries continues. Such loops typically devolve into exactly the same messages being sent repeatedly, so we check if a message is ever duplicated and if so, we interject with a user message indicating that the message has been duplicated and that it should not be sent again.</p>
<p>We also observe from preliminary experiments that the leader agent does not always remember to follow the protocol for sending a message and simply writes the message body without the necessary header, especially as the conversation grows longer. This is mitigated by including a short reminder every time the leader agent receives messages, reminding it that it must use the appropriate protocol if it wants to respond.</p>
<p>Finally in some cases the leader agent explicitly addresses a message to one agent (e.g., the expert), but that agent does not recognize the message as being addressed to them. To identify such cases and
speed up inference, we add a prompt instruction with a specific string an agent should output if they wish to not respond to a message. We detect the presence of any agent ID in a sent message, and if the agent in question outputs the no-response string, we inject a follow-up message reminding them that their name is in the message and encouraging them to respond.</p>
<h3>4.3 Context management</h3>
<p>Our experiments use the 8 k -token version of GPT4. Versions that can consume more tokens have been developed, but they were not available to us as of this work, and some studies have suggested that even models that can ostensibly consume a large number of tokens may not be able to attend to all of them effectively in practice (Qin et al., 2023; Liu et al., 2023). MARG can scale beyond the token limit of the base LLM by distributing the input across worker agents, and does not require any individual agent to process a large number of tokens. However, with a large number of agents or many turns of inter-agent discussion (both of which increase with paper length), the combined tokens of messages sent between agents could cause the message history to eventually exceed the input token limit for especially long papers.</p>
<p>To mitigate the impact of long discussions between agents, we prune old messages from the history on each round of communication. The pruning strategy is different depending on the agent type.</p>
<p>History length is most limited for the worker agents, which each have a paper chunk occupying most of their token limit, so the histories for workers were trimmed to the initial prompts plus the three most recent messages. For the leader agent, we observe that (1) a long history is sometimes necessary for in-depth discussions, (2) the majority of tokens in the history arise from all the messages it receives from (potentially many) other agents, and (3) as the leader relays information between other agents, it generally summarizes any important information from messages it receives. We therefore prune the past messages received from other agents, but keep the full history of outgoing messages. Finally, for expert agents we never observed issues with the token limit, so no pruning was applied.</p>
<h3>4.4 Review generation</h3>
<p>To tune prompts for review generation, we performed several hundred rounds of manual iteration on a small set of papers from ARIES (D'Arcy et al.,</p>
<p>2023). As the review generation task is somewhat subjective and there are a large number of potential shortcomings with different levels of severity, it is not always straightforward to determine whether a model has made a clear error or if it simply has a difference of opinion with respect to what the most important comments are. We found it helpful to manually alter some of the papers to create severe and obvious errors that we could expect the model to identify; for example, removing an entire section or adding an unfounded claim (e.g., "the proposed method achieves artificial general intelligence"). Surprisingly, these "obvious" errors were often not trivial for the system to recognize, making the altered papers useful for finding and mitigating blind-spots. The final prompts are shown in Appendix A, and an outline of our system structure is described in the following paragraphs and shown in Figure 2.</p>
<p>We use three independent multi-agent groups to generate different kinds of review comments. The task prompt given to the leader agent is different for each comment type, and each group has one expert. The comment types are based loosely on points in the ICLR reviewer guidelines. ${ }^{3}$ In particular, it asks "[...] is the submission clear, technically correct, experimentally rigorous, reproducible, does it present novel findings (e.g. theoretically, algorithmically, etc.)?" We group and slightly reframe these points to arrive at the following comment types:</p>
<h2>Experiments and evaluation:</h2>
<p>The leader is instructed to focus on verifying that the experiments and theoretical proofs are correct and adequately support the paper's claims. The expert in this group is told to "design high-quality experiments" given the main claims made in the paper, inspired in part by the fact that making predictions is an effective active reading strategy to improve comprehension in humans (Fielding and Others, 1990; Duke and Pearson, 2009). In preliminary experiments without the expert, the model could identify some bad experiments and give generic comments, but struggled to realize when an experiment was missing. Explicitly designing experiments provides a baseline with which to compare the experiments in the paper, allowing the model to recognize missing or incomplete experiments.</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<h2>Clarity and reproducibility:</h2>
<p>The leader is instructed to focus on ensuring that the paper clearly explains key concepts and proposed methods, and that it provides all necessary details to implement any proposed methods and reproduce experiments. The expert in this case is instructed to be "highly curious" and to ask questions of the leader agent in order to learn more about the paper. This process aids in identifying any questions that can't be answered based on the paper, which become comments.</p>
<h2>Novelty and impact:</h2>
<p>The leader is instructed to focus on the novelty and impact of the paper. However, we note that for our study the task of accurately retrieving related work is out of scope, so this comment type is limited to identifying errors in the paper's own explanations. Specifically, the model is instructed to verify that the paper clearly states and justifies its motivations, goals, and key findings, and that it thoroughly discusses how it fits into the existing literature. The expert in this case is instructed to be skeptical of the paper and ask questions to determine if it actually makes a significant contribution to its field.</p>
<h3>4.5 Refinement</h3>
<p>After generating a review, we find that it is very helpful to include a "refinement" stage, in which the model is given the review comments and asked to improve (or remove) them. Various errors can arise during the initial comment generation, and we observe that models tend to be poor at self-reflection and correction during that stage. Including refinement as a separate stage can resolve many of the errors introduced during the initial generation.</p>
<p>To refine comments, we initialize a new multiagent group with no expert agent. For each comment, we provide the comment to the leader agent with a prompt instructing it to ensure that the comment is clear, that it is specific, and that it is valid (i.e., does not suggest something that is already done in the paper). The model outputs a list; usually this list contains one element (the newly-refined comment), but may contain more (if the original comment mixed two different suggestions) or be null (if the comment was invalid). The comments are processed independently (i.e., by separate multiagent groups).</p>
<p>5 Baseline methods</p>
<p>In this section, we will describe the baseline methods that we compare against our multi-agent approach. We consider three baselines: a simple baseline that treats chunks independently and uses a one-line prompt, a baseline that treats paper chunks independently but uses a more sophisticated prompt, and a recently proposed method for generating peer-review feedback <em>Liang et al. (2023)</em>. Prompts for these methods can be found in Appendix A.</p>
<h3>5.1 Single-Agent Review Generation with Basic prompt (SARG-B)</h3>
<p>This baseline is designed to emulate a simple approach that a ChatGPT user might use to get feedback on their paper if they did not want to do any prompt tuning. We use a single agent to generate all comments for the paper. The paper is split into the same chunks as for the multi-agent baseline, but the chunks are processed independently using a very simple prompt:</p>
<blockquote>
<p>Write feedback comments in the style of a scientific paper review for the following portion of a scientific paper. You can skip minor grammar comments.</p>
</blockquote>
<p>After applying the model to each chunk, the resulting comment lists are combined by a similarly simple prompt:</p>
<blockquote>
<p>Here are some lists of review comments that were made about different portions of the paper: <comment lists>
Merge these lists into a final list of review comments. Any comments that are duplicates (saying essentially the same thing as other comments) should be merged or deleted.</p>
</blockquote>
<h3>5.2 Single-Agent Review Generation with Tuned Prompt (SARG-TP)</h3>
<p>This baseline is designed to emulate a more sophisticated approach that a ChatGPT user might use to get feedback on their paper if they were willing to do some prompt tuning. We use a single agent to generate all comments for the paper, but we use a more sophisticated prompt (subsection A.3) that is designed to encourage the model to generate more specific and actionable comments. As with the other simple baseline, we generate comments independently for each paper chunk and then merge the resulting lists with GPT.</p>
<p>Similarly to our multi-agent method, we include a refinement step in this baseline. For each paper chunk, we give the model the chunk and the final list of comments, and ask it to output a new, refined list of comments. This provides an opportunity to remove incorrect comments that arise from the independent processing of each chunk. For example, if one chunk contains the introduction but not the experiments, the model might initially write a comment that claims the experiments are missing, but in the refinement stage will be able to prune it when it sees the chunk that does contain experiments.</p>
<h3>5.3 Multi-Agent Review Generation with Tuned Prompt (MARG-TP)</h3>
<p>This baseline is designed to provide a direct comparison with the prompt-tuned single-agent baseline and explore the benefits of multi-agent modeling. Whereas our full MARG-S approach leverages several advantages of multi-agent that would be difficult to directly compare in a single-agent setting (e.g., the use of expert agents), this multi-agent baseline uses a prompt designed to be as similar as possible to the prompt-tuned single-agent baseline. Of course, we still must include some instructions that explain the communication protocol and instruct the agents to work together, but the task prompt includes all the same language as in the single-agent setting. Similarly, we use a refinement prompt that is as similar as possible to the single-agent setting, although the refinement stage still differs in that we do not manually apply it on each chunk (as this would defeat the point of using multiple agents).</p>
<h3>5.4 Multi-Agent Review Generation with Specialized Agents (MARG-S)</h3>
<p>Our full MARG-S approach is described in section 4, and uses three independent multi-agent groups to generate different kinds of review comments. MARG-S outputs the concatenation of the three mini-reviews generated by those groups. In addition to the full approach, we evaluate each of the three mini-reviews separately. We refer to these as MARG-S (experiments), MARG-S (clarity), and MARG-S (impact). In addition, we include a "no refinement" baseline that skips the refinement stage.</p>
<h3>5.5 Liang et al. (2023) baseline (LiZCa)</h3>
<p>We also compare against a recently proposed method for generating peer-review feedback <em>Liang et al. (2023)</em>, which we refer to as "LiZCa" (from the names of the lead authors of that paper; the method was not given a name in that work). Unlike our methods, this method simply truncates the</p>
<p>paper rather than applying to multiple chunks. In addition, it includes the captions of figures and tables in the input.</p>
<p>The prompt used in Liang et al. (2023) instructs the model to generate an "outline" style review, and includes non-actionable positive comments. Fortunately, when comparing their method's comments with real reviews, they developed a prompt to extract and merge the parts of an outline that focus on "criticisms" and to ignore minor grammar comments. This roughly matches the type of comments we target, so we use that prompt to produce the final list of comments that we use in this baseline.</p>
<p>We note that Liang et al. (2023) used a different PDF parsing library (pikepdf) than ours (Grobid), but for consistency with our other baselines we run it with Grobid.</p>
<h2>6 Automated evaluation</h2>
<p>To automatically evaluate the quality of generated reviews, we measure their overlap with real reviews from papers in the ARIES corpus (D'Arcy et al., 2023). That is, we attempt to match the generated comments to comments extracted from real (human-written) reviews. Because ARIES only has comment annotations for a small set of reviews, we use GPT ${ }^{4}$ to extract comments from all reviews for a subset of 30 papers and treat this as our test set. To match our intended type of feedback, GPT is instructed to focus only on actionable feedback comments and to ignore minor comments on style and grammar.</p>
<p>We note that this form of evaluation is imperfect in that real reviewers do not always identify every reasonable critique of a paper, and in some cases they may make critiques that are unreasonable. Thus, the generated review could contain good comments that happen to be different from ones the real reviewers made, or it could miss comments that are actually invalid. Thus, the measured overlap should be treated as a lower bound for the fraction of good-quality comments. In addition, the nuanced nature of the matching task makes it impossible to fully capture the similarities and differences between real and generated comments using binary alignments, and this could lead to biases. We nonetheless use automated evaluation as an inexpensive but rough approximation of the rel-</p>
<p><sup id="fnref4:0"><a class="footnote-ref" href="#fn:0">1</a></sup>ative quality of different methods, and separately conduct a user study in section 7 to obtain a more realistic evaluation.</p>
<p>The matching procedure and results are outlined in the following subsections.</p>
<h3>6.1 Measuring overlap</h3>
<p>Given a set of generated review comments $C_{\text {gen }}$ and the set of ground truth real-reviewer comments $C_{\text {real }}$ for a paper, we automatically align individual comments between the reviews that have the same meaning. That is, we ultimately obtain a binary label for every comment pair $\left(C_{\text {gen }}^{i}, C_{\text {real }}^{j}\right)$ indicating whether the two comments are making the same request. To do this, we begin with a "manymany" matching stage that efficiently compares the full set of comments in both reviews and identifies possibly-matching pairs, followed by a more accurate (but more expensive) pairwise stage that examines the candidate pairs to produce a final list.</p>
<p>In the many-many matching stage, we feed all comments from both reviews into GPT-4 and prompt it to output a list of all matching comments. As GPT has somewhat inconsistent performance, we do five such passes, randomly permuting both the order of comments within each review and the order in which reviews are presented. The final output of this stage is the list of comment pairs that were produced by at least two of the five runs-a ratio we heuristically found to work well in preliminary experiments.</p>
<p>In the pairwise stage, we give one comment pair at a time to GPT and prompt it to produce two scores: one of four levels of relatedness ("none", "weak", "medium", or "high"), and a "relative specificity" ("less", "same", "more") indicating how specific the generated comment is relative to the real review comment. To be considered a match, a comment pair must have "medium" or "high" relatedness, and the generated comment must have "same" or "more" specificity compared to the human comment. An example of an aligned pair of comments can be found in Table 1.</p>
<p>The final output is a list of alignment edges between the lists of generated and real-reviewer comments. We note that this may result in a manymany mapping; one generated comment might match multiple reviewer comments, and one reviewer comment might match multiple generated comments. This can happen when there are similar comments within one list or if, for example, a re-</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Real-reviewer comment</th>
<th style="text-align: center;">Generated comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">The experimental methodology used in the paper is not well detailed, making it difficult to reproduce the reported results.</td>
<td style="text-align: center;">More details about the experiments conducted would be beneficial. This should include information about the datasets used, the training process, and the evaluation process. To ensure the reproducibility of the results, consider providing the code used to implement the model, the specific parameters used, and any other necessary information. This will allow other researchers to replicate your work and further validate your findings. <br> [high relatedness, more specific]</td>
</tr>
<tr>
<td style="text-align: center;">The paper does not include enough baselines for Fair Federated Learning to compare against. Even if some methods do not satisfy privacy considerations, they should still be included for the reader to understand how the proposed method compares against such methods, especially given that the results are not promising. Some baselines to consider include Cui et al or Tran et al.</td>
<td style="text-align: center;">The authors should consider including a comparison of their proposed method with existing methods in the experimental results section. This would help to highlight the advantages and improvements of their proposed method. [high relatedness, less specific]</td>
</tr>
<tr>
<td style="text-align: center;">The datasets used in the study are not representative due to their simplicity and experimental nature.</td>
<td style="text-align: center;">The evaluation of the proposed method may not be comprehensive enough. The authors could include more datasets in their evaluation to demonstrate the robustness of their method. The paper could benefit from a more detailed discussion on the limitations of the proposed method. [medium relatedness, more specific]</td>
</tr>
</tbody>
</table>
<p>Table 1: Aligned pairs of comments with corresponding relatedness and relative specificity scores from the alignment model; the bold is added to emphasize key differences. Notice that in the third row with "medium" relatedness, the reviewer comment is suggesting that the datasets need to be more representative (but a larger number of datasets is not necessarily needed) whereas the generated comment only asks for more datasets (not identifying the issue with the current datasets). In the two "high" relatedness cases, one comment fully subsumes the other (high relatedness) but includes much more specific details and rationales (less/more relative specificity).</p>
<p>viewer makes a broad suggestion like "Evaluate on more datasets" and the generated review contains several comments, each with a different specific dataset recommendation.</p>
<h3>6.2 Metrics</h3>
<p>Using the alignments between $C_{\text {gen }}$ and $C_{\text {real }}$, we evaluate several metrics, described below. However, we note that the many-many nature of the mapping between the comments indicates that these are not proper sets, and traditional set-based metrics such as the union and intersection are not well-defined. For our purposes, we define directional intersection operators $\vec{\cap}$ and $\vec{\cap}$ representing the set of aligned elements in the left or right operand, respectively. For example, $C_{\text {gen }} \cap C_{\text {real }}$ is the set of elements of $C_{\text {gen }}$ that align to any element in $C_{\text {real }}$.</p>
<ul>
<li>Recall: $\frac{\left|C_{\text {gen }} \cap C_{\text {real }}\right|}{\left|C_{\text {real }}\right|}$, the fraction of realreviewer comments that are aligned to any generated comment.</li>
<li>Precision: $\frac{\left|C_{\text {gen }} \cap C_{\text {real }}\right|}{\left|C_{\text {gen }}\right|}$, the fraction of generated comments that are aligned to any realreviewer comment.</li>
<li>(Pseudo-)Jaccard: The Jaccard index is a commonly-used measure of set overlap. Let intersection $=$ $\frac{\left|C_{\text {gen }} \cap C_{\text {real }}\right|+\left|C_{\text {gen }} \cap C_{\text {real }}\right|}{2}$; then the Jaccard index is $\frac{\text { intersection }}{|C_{\text {gen }}|+\left|C_{\text {real }}\right|-\text { intersection }}$.</li>
</ul>
<p>To compute these metrics over a set of papers, we macro-average on the level of reviews. That is, given a set of papers in our test set, we generate a review for each, measure the aforementioned metrics between each generated review and each corresponding real review, and then average all of the results to obtain a single value for each metric.</p>
<h3>6.3 Results</h3>
<p>We include a selection of example generated comments in Table 3. Results of the automated evaluation are shown in Table 2. We additionally include a human-review baseline, which is the average of the metrics computed between each real review and each other real review for the same paper (i.e., $\frac{1}{n} \sum_{i=1}^{n}$ metric $\left(h u m a n_{i},\left{\right.\right.$ human $\left.\left.\left._{k} \mid k \neq i\right}\right)$. Note that while this is theoretically unbiased for recall, it may result in lower precision and Jaccard scores for human reviewers.</p>
<p>We find that our proposed MARG-S method outperforms all baselines in terms of recall, but generates more comments than other baselines and thus has lower precision and Jaccard scores. With that said, we believe that recall is the most important metric in this evaluation. While higher precision and Jaccard should be preferred at similar levels of recall, it is relatively easy for a human to recognize and ignore bad comments; thus, it is more important for the system to maximize the number of good comments than to minimize the number of bad ones.</p>
<p>The simple baseline (SARG-B) performs poorly on all metrics; despite being tied with MARG-S for the highest number of generated comments, it has the lowest recall of all methods. This is not unexpected, but highlights the importance of careful prompting with GPT-4.</p>
<p>Interestingly, we find that between SARG-TP and MARG-TP (which use essentially the same task prompt), SARG-TP generates more comments and has better recall. This suggests that simply applying a multi-agent approach does not always result in a performance improvement; instead, the use of multiple agents enables the design of richer internal problem-solving structures via expert agents. Indeed, we see that the specialized MARG-S (impact) is able to approximately match the performance of MARG-TP despite focusing on only one type of comment.</p>
<p>We notice that the human baseline actually has a lower recall than some of the LLM baselines, although it has the highest precision. This is consistent with the results of Liang et al. (2023), which found that Human-Human agreement was slightly lower than LiZCa-Human agreement. ${ }^{5}$ Humans generate fewer comments than other approaches, which offers a partial explanation for the low recall, but it is nonetheless interesting to observe that human reviewers can have very different perspectives of the same work, and highlights the challenge of the review generation task (and the potential weaknesses of alignment-based evaluation).</p>
<h2>MARG-S ablations:</h2>
<p>Among the sub-reviewers of MARG-S, the impactfocused model tends to produce the best results.</p>
<p><sup id="fnref5:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Recall</th>
<th>Precision</th>
<th>Jaccard</th>
<th># comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>SARG-B</td>
<td>7.43</td>
<td>1.40</td>
<td>1.25</td>
<td>19.7</td>
</tr>
<tr>
<td>SARG-TP</td>
<td>10.62</td>
<td>4.61</td>
<td>3.46</td>
<td>11.6</td>
</tr>
<tr>
<td>MARG-TP</td>
<td>8.49</td>
<td>5.34</td>
<td>3.52</td>
<td>8.5</td>
</tr>
<tr>
<td>LiZCa</td>
<td>9.67</td>
<td>9.96</td>
<td>5.58</td>
<td>4.0</td>
</tr>
<tr>
<td>MARG-S</td>
<td>15.84</td>
<td>4.41</td>
<td>3.53</td>
<td>19.8</td>
</tr>
<tr>
<td>no refinement</td>
<td>11.92</td>
<td>3.32</td>
<td>2.70</td>
<td>18.3</td>
</tr>
<tr>
<td>experiments-only</td>
<td>4.36</td>
<td>4.83</td>
<td>2.23</td>
<td>4.1</td>
</tr>
<tr>
<td>clarity-only</td>
<td>3.25</td>
<td>2.65</td>
<td>1.46</td>
<td>6.9</td>
</tr>
<tr>
<td>impact-only</td>
<td>8.88</td>
<td>4.75</td>
<td>3.32</td>
<td>8.8</td>
</tr>
<tr>
<td>Human</td>
<td>9.42</td>
<td>12.00</td>
<td>5.45</td>
<td>4.7</td>
</tr>
</tbody>
</table>
<p>Table 2: Automated evaluation results with recall, precision, and Jaccard values, in addition to the average number of comments generated by each method. The proposed MARG-S method outperforms all baselines in terms of recall, but generates more comments than other baselines and thus has lower precision and Jaccard scores.</p>
<p>The experiment-focused model does well considering the small number of comments it produces, but as it produces half as many comments as the impact model it also has half the recall. Finally, the clarity-focused model struggles compared to the other two. The poor performance of the clarity model may be due in part to the subjective nature of clarity judgements and the fact that language models do not necessarily perceive text in the same way that humans do (e.g., humans prefer that terms be defined before they are used, but a model that consumes a full document at once might not see a problem if terms are defined later). In addition, we note that due to the fact that the input does not capture visual information such as figures, tables, and the arrangement of symbols in equations, there are many resulting clarity issues that are not present in the full paper, and getting the model to identify the "real" issues from among the large number of parsing- and input-format-related issues is challenging.</p>
<p>We observe that without the refinement stage, MARG-S's performance is reduced on all metrics, but it still obtains reasonable results; recall remains the second-highest of all methods. Interestingly, the number of generated comments is slightly lower than with the refinement stage, indicating that the refinement stage splits one comment into multiple comments more often than it prunes comments.</p>
<h2>Effect of the matching thresholds</h2>
<p>We qualitatively observe that several of the methods we evaluate produce many generic or vague comments. Many of these are not counted towards recall due to our constraint that a generated comment must be equally or more specific compared to the real comment it matches with. In addition, some aligned pairs of comments are questionable, especially for pairs that have only "medium" relatedness; for example, a comment asking for a "more thorough comparison" to baselines is considered a match for one that indicates that the proposed method underperforms the baselines in some cases.</p>
<p>To evaluate the impact of threshold choices, we select our method and the LiZCa baseline and evaluate all combinations of thresholds for relative specificity and "medium" or "high" relatedness. The results are shown in Figure 3.</p>
<p>The difference between thresholds is striking in the figure. LiZCa generates a large number of comments that broadly match to a real comment, but are much more vague (and thus less useful) and often do not have exactly the same meaning. When either the relatedness or the specificity thresholds are increased even by one step, the recall drops sharply. In contrast, MARG-S loses relatively little recall at higher specificity thresholds, as it almost always outputs specific comments. However, MARG-S still has a drop when requiring "high" relatedness, indicating that it has many matches in which the generated comment does not express exactly the same sentiment as the real one.</p>
<h2>Cost</h2>
<p>In Table 4 we report the average number of tokens generated by each method during the alignment-</p>
<table>
<thead>
<tr>
<th style="text-align: left;">Method</th>
<th style="text-align: left;">Example comment</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">SARG-B</td>
<td style="text-align: left;">The paper could benefit from a more detailed discussion of the results, including the <br> implications of the findings and how they contribute to the existing body of knowledge.</td>
</tr>
<tr>
<td style="text-align: left;">LiZCa</td>
<td style="text-align: left;">The experimental evaluation could be more comprehensive. The authors should consider <br> including more diverse tasks and environments in their experiments to demonstrate the <br> robustness of their method. The paper could benefit from a more detailed analysis of the <br> experimental results, including a discussion on why the proposed method outperforms the <br> baselines.</td>
</tr>
<tr>
<td style="text-align: left;">MARG-S <br> (experi- <br> ments)</td>
<td style="text-align: left;">The authors have compared their method with several baselines, including DeepMDP, <br> HiP-BMDP-nobisim, Distral, PCGrad, GradNorm, and PEARL. However, it would be <br> beneficial to include comparisons with other state-of-the-art methods in multi-task and <br> Meta-RL setups to further validate the effectiveness of the proposed method. This would <br> help ensure that the results are not specific to the current set of comparisons and can <br> generalize across different settings. Additionally, providing a detailed discussion on <br> why the proposed method outperforms each baseline could offer more insights into the <br> strengths and weaknesses of the proposed method.</td>
</tr>
<tr>
<td style="text-align: left;">MARG-S <br> (clarity)</td>
<td style="text-align: left;">The paper mentions an encoder that maps observations from state space to a learned, <br> latent representation, but it does not provide specific details about the type of encoder <br> used or the process of how it learns the latent representation. These details are crucial for <br> understanding how the model works and how it achieves its performance. Therefore, I <br> recommend that the authors include this information in the paper.</td>
</tr>
<tr>
<td style="text-align: left;">MARG-S <br> (impact)</td>
<td style="text-align: left;">The authors provide a theoretical proof for the 'Transfer bound' formula, which is a <br> significant contribution. This formula is crucial for measuring the transferability of a <br> policy learned on one task to another, taking into account the error from the learned <br> representation. However, to ensure its robustness and applicability in real-world scenarios, <br> it would be beneficial if the authors could empirically test this formula in reinforcement <br> learning environments. For instance, the agent could be trained on one game and then <br> tested on a different game with similar mechanics. This would provide empirical evidence <br> supporting the theoretical proof and demonstrate the practical utility of the formula.</td>
</tr>
</tbody>
</table>
<p>Table 3: Example comments generated by each method (SARG-TP and MARG-TP omitted for brevity) for the same paper. Qualitatively, we find that MARG-S writes relatively long and specific comments, whereas other methods tend to write shorter and more generic comments.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Recall of MARG-S and LiZCa for different alignment cutoff levels of relatedness and relative specificity. The ("medium", "same") cell corresponds to our default setting. LiZCa obtains very high recall in the most lenient setting, but rapidly drops for stricter settings that prevent vague comments from being counted as matches. MARG-S obtains relatively consistent results for all levels of specificity (as most of its comments are considered "more" specific) but still experiences a decline when requiring highly-related matches.</p>
<p>based evaluation. LiZCa generates the fewest tokens and has the best cost to recall ratio overall, making it an attractive choice in budget-constrained settings. While MARG-S has the best recall, it also generates roughly an order of magnitude more tokens than other methods, suggesting that it takes on diminishing returns in efficiency to obtain the recall improvement.</p>
<p>The extra tokens used by MARG-S result in it taking roughly an hour longer than other methods to generate reviews. This may serve as an inconvenience in practice, and it would be beneficial to explore ways to reduce it. For example, it may be possible to dynamically switch to cheaper LLMs to handle simpler messages or develop methods to route communications more effectively (reducing the number of redundant messages). We also note that our implementation performs only one inference at a time for simplicity, but in theory, it is highly parallelizable (due to having three separate groups for different comment types, separate groups for the refinement stage, and several agents communicating at once in each group), and the time needed to generate a review could likely be reduced by 2-10x depending on the document size.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Input tokens</th>
<th>Generated tokens</th>
</tr>
</thead>
<tbody>
<tr>
<td>SARG-B</td>
<td>15,532</td>
<td>3,249</td>
</tr>
<tr>
<td>SARG-TP</td>
<td>54,914</td>
<td>6,853</td>
</tr>
<tr>
<td>MARG-TP</td>
<td>134,860</td>
<td>5,264</td>
</tr>
<tr>
<td>LiZCa</td>
<td>7,398</td>
<td>869</td>
</tr>
<tr>
<td>MARG-S</td>
<td>1,236,344</td>
<td>51,255</td>
</tr>
</tbody>
</table>
<p>Table 4: Average number of input and generated tokens per paper for each method. This includes tokens used for internal discussion in multi-agent methods, but not tokens used outside of the method (e.g., for measuring the alignment metric). MARG-S generates substantially more tokens than other methods, and thus is more expensive to run.</p>
<p>rate groups for different comment types, separate groups for the refinement stage, and several agents communicating at once in each group), and the time needed to generate a review could likely be reduced by 2-10x depending on the document size.</p>
<h2>7 User study</h2>
<p>We conduct a user study to obtain a more reliable (but more expensive) evaluation compared to the automated metrics. To reduce burden on participants, we only evaluate a subset of methods in the user study: MARG-S (our best method on the automated metrics), LiZCa (baseline from prior work), and SARG-B (the simplest baseline).</p>
<h3>7.1 Study design</h3>
<h4>Participants</h4>
<p>We recruit 9 volunteers^{6} from a large research organization to participate in the study. All participants are researchers in the fields of natural language processing and human-computer interaction.</p>
<h4>Survey</h4>
<p>The study was conducted using a web interface in which participants could upload a paper PDF. We then ran each review generation method to produce a set of reviews, where each review was a list of comments. When all reviews were generated, participants would receive an email notification with a link to page with reviews and a set of survey</p>
<p>^{6}While this is a small number of participants, we note that the number of rated comments is much higher (each participant rates many comments per method), and we obtain statistically significant conclusions from mixed-effect analyses in which we control for participant bias as a random effect.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: The survey interface. Participants were asked to rate the specificity, accuracy, and overall helpfulness of each comment, and to rate the overall review.
questions, depicted in Figure 4. The survey page did not describe the review generation methods or give any indication of which method generated a given review, and the generated reviews were displayed in a random order to reduce bias (the order of comments within reviews was not randomized, however).</p>
<p>For each comment, participants were asked to rate its specificity, accuracy, and to provide an overall rating. The following guidelines for these ratings were provided at the start of the survey:</p>
<ul>
<li>Specificity: Does the comment make a suggestion specific to the paper, or is it generic (could apply to many papers)? Please note that a comment may be verbose without being specific, or vice versa.</li>
<li>Accuracy: Does the comment display an accurate understanding of the paper and make a valid critique? For example, suppose a comment says the paper is missing statistical significance tests and should include them. If the paper doesn't have significance tests and could potentially benefit from including them, please rate the comment as "accurate" (even if
the importance of those tests is questionable). If the paper has tests on one or two results but not all, and the comment doesn't mention this, the comment would have a "minor" inaccuracy. If the paper already has extensive significance tests or provides substantial justification for not including them, the comment would have a "major" inaccuracy.</li>
<li>Overall rating: How helpful is the comment overall? Is the comment one that you would want to see in a review (Good), one that you might not mind seeing but don't care much about (Neutral), or one that is useless or invalid (Bad)?</li>
</ul>
<p>In addition, participants were asked questions at the end of each review. Specifically, they were asked to rate whether the review was too long or too short on a 5-point scale and to provide an overall rating for the review on a 5-point scale.</p>
<p>Finally, we asked participants about their research and reviewing experience, and about their authorship of the submitted paper.</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Bad</th>
<th>Neutral</th>
<th>Good</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>Bad</td>
<td>Neutral</td>
<td>Good</td>
<td>Total</td>
</tr>
<tr>
<td>SARG-B</td>
<td>10.2</td>
<td>4.2</td>
<td>1.7</td>
<td>16.1</td>
</tr>
<tr>
<td>LiZCa</td>
<td>2.7</td>
<td>0.9</td>
<td>0.3</td>
<td>3.9</td>
</tr>
<tr>
<td>MARG-S</td>
<td>8.1</td>
<td>5.3</td>
<td>$\mathbf{3 . 7}$</td>
<td>17.1</td>
</tr>
</tbody>
</table>
<p>Table 5: Average number of each comment rating per review for each method. MARG-S generates the most good comments. LiZCa generates substantially fewer comments than the other methods, and therefore has the fewest bad comments per review but also the fewest good comments.</p>
<h3>7.2 Total good comments</h3>
<p>As in the alignment-based evaluation, we argue that bad comments have relatively small cost compared to the value of good comments. As there is no straightforward way to adjust the total number of generated comments (unlike in a classification task, where the decision threshold could be adjusted continuously), the total number of good comments is the most appropriate metric with which to compare methods.</p>
<p>Table 5 shows the average number of each comment rating per review for each method. We find that MARG-S generates more good comments than SARG-B ( $\mathrm{p}=0.09$, related-sample t-test) and LiZCa ( $\mathrm{p}=0.003$ ). LiZCa generates substantially fewer comments than the other methods, and therefore has the fewest bad comments per review but also the fewest good comments.</p>
<h2>Review length</h2>
<p>MARG-S generates the most good comments, but does this come at the cost of generating overly-long reviews? It seems that in general, MARG-S reviews do tend to be longer than authors would like, while LiZCa reviews are too short. Specifically, MARG-S was rated as "way too long" by 6 of the 9 participants (and "just right" by the other three), while LiZCa was rated as "too short" by 4, "way too short" by 3, and "just right" by 2 of the participants. SARG-B occupied a middle ground, rated as "too short" by 2, "too long" by 3, and "just right" by 4 of the participants. Although SARG-B generates a similar number of comments as MARG-S, the comments it generates are much shorter, which is likely why its length is perceived as being more reasonable.</p>
<h3>7.3 Average comment ratings</h3>
<p>The distribution of user ratings of comment quality, accuracy, and specificity are shown in Figure 5, Figure 6, and Figure 7, respectively.</p>
<p>We find that MARG-S has the highest proportion of "good" comments, and is significantly better than SARG-B ( $\mathrm{p}=0.02$ for per-comment Barnard's exact test, $\mathrm{p}=0.12$ for per-user related sample ttest), although the difference between MARG-S and LiZCa is not significant ( $\mathrm{p}=0.09$ per-comment, $\mathrm{p}=0.16$ per-user). When asked about the overall helpfulness of the reviews, participants rated MARG-S as an average of 1.0 points higher on the 5-point scale (and 5 of the 9 participants rated MARG-S as 2-3 points higher).</p>
<p>The accuracy ratings in Figure 6 show a similar trend as the comment quality ratings. MARG-S has the highest proportion of fully accurate comments, but the differences are not significant.</p>
<p>The most striking difference between the methods is in specificity. MARG-S has "very specific" comments at more than triple the rate of the other two methods, a significant increase ( $\mathrm{p}=0.002$, peruser related-sample t-test). Overall, $71 \%$ of its comments are rated as "specific" or "very specific", compared to only $40 \%$ for LiZCa ( $\mathrm{p}=0.08$ ).</p>
<p>Finally, we observe that MARG-S has a high proportion of "good" comments rated by users despite having a relatively low precision in the automated evaluation (Table 2). The difference suggests that it may generate many comments which are helpful but also different than the kinds of suggestions a real reviewer would tend to make. This could be a promising sign indicating that MARG-S can serve as a useful source of novel inspiration for authorseven when the paper has already been reviewed by humans-and that it may be a source of inspiration for reviewers as well. ${ }^{7}$</p>
<h3>7.4 Relationships between factors</h3>
<p>Does the high specificity of MARG-S impact the quality and accuracy ratings? In particular, more specific comments may be easier to make strong judgements about, whereas comments that are generic or vague may be hard to clearly classify;</p>
<p><sup id="fnref6:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 5: Average quality ratings for each method. LiZCa and SARG-B are rated similarly, while MARG-S has over twice the fraction of "good" comments compared to the other two methods.
<img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 6: Average accuracy ratings for each method. MARG-S has the most fully accurate comments by a small margin, and SARG-B has the most major inaccuracies, but all methods have similar accuracy distributions overall.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 7: Average specificity ratings for each method. LiZCa and SARG-B have similar proportions of the specific and very-specific comments, but LiZCa has more very generic comments. MARG-S is extremely specific compared to the other two methods; $71 \%$ of MARG-S comments are rated specific or very specific, compared to only $40 \%$ for LiZCa and SARG-B.</p>
<table>
<thead>
<tr>
<th style="text-align: left;"></th>
<th style="text-align: right;">Coef</th>
<th style="text-align: right;">Std. err</th>
<th style="text-align: right;">z-value</th>
<th style="text-align: right;">$\operatorname{Pr}(&gt;\mid z \mid)$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">MARG-S</td>
<td style="text-align: right;">0.19</td>
<td style="text-align: right;">0.33</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: right;">0.56</td>
</tr>
<tr>
<td style="text-align: left;">LiZCa</td>
<td style="text-align: right;">-0.66</td>
<td style="text-align: right;">0.52</td>
<td style="text-align: right;">-1.28</td>
<td style="text-align: right;">0.20</td>
</tr>
<tr>
<td style="text-align: left;">Inaccuracy (minor)</td>
<td style="text-align: right;">-1.33</td>
<td style="text-align: right;">0.35</td>
<td style="text-align: right;">-3.83</td>
<td style="text-align: right;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: left;">Inaccuracy (major)</td>
<td style="text-align: right;">-5.54</td>
<td style="text-align: right;">0.58</td>
<td style="text-align: right;">-9.62</td>
<td style="text-align: right;">$10^{-21}$</td>
</tr>
<tr>
<td style="text-align: left;">Specificity (specific)</td>
<td style="text-align: right;">1.90</td>
<td style="text-align: right;">0.36</td>
<td style="text-align: right;">5.33</td>
<td style="text-align: right;">$10^{-7}$</td>
</tr>
<tr>
<td style="text-align: left;">Random effect std. dev</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
<p>Table 6: Cumulative link fixed effects for specificity, accuracy, and method on the overall rating of a comment. Specificity is positively associated ratings, as is accuracy (inaccuracies have a negative effect). The review generation method has a relatively small independent effect compared to the other factors, suggesting that specificity and accuracy capture a large portion of the aspects that contribute to perceived comment quality.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Factor</th>
<th style="text-align: center;">Rating=Bad</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Rating=Neutral</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Rating=Good</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coef</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">Coef</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">Coef</td>
<td style="text-align: center;">p-value</td>
</tr>
<tr>
<td style="text-align: center;">(Intercept)</td>
<td style="text-align: center;">-0.37</td>
<td style="text-align: center;">0.51</td>
<td style="text-align: center;">-0.96</td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">-1.92</td>
<td style="text-align: center;">$10^{-2}$</td>
</tr>
<tr>
<td style="text-align: center;">MARG-S</td>
<td style="text-align: center;">-0.31</td>
<td style="text-align: center;">0.45</td>
<td style="text-align: center;">0.02</td>
<td style="text-align: center;">0.95</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">0.48</td>
</tr>
<tr>
<td style="text-align: center;">LiZCa</td>
<td style="text-align: center;">0.55</td>
<td style="text-align: center;">0.36</td>
<td style="text-align: center;">-0.21</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">-0.56</td>
<td style="text-align: center;">0.49</td>
</tr>
<tr>
<td style="text-align: center;">Inaccuracy (minor)</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">0.92</td>
<td style="text-align: center;">$10^{-2}$</td>
<td style="text-align: center;">-2.34</td>
<td style="text-align: center;">$10^{-5}$</td>
</tr>
<tr>
<td style="text-align: center;">Inaccuracy (major)</td>
<td style="text-align: center;">5.42</td>
<td style="text-align: center;">$10^{-15}$</td>
<td style="text-align: center;">-2.74</td>
<td style="text-align: center;">$10^{-6}$</td>
<td style="text-align: center;">-17.99</td>
<td style="text-align: center;">-</td>
</tr>
<tr>
<td style="text-align: center;">Specificity (specific)</td>
<td style="text-align: center;">-2.03</td>
<td style="text-align: center;">$10^{-5}$</td>
<td style="text-align: center;">0.40</td>
<td style="text-align: center;">0.25</td>
<td style="text-align: center;">2.13</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">Random effect std. dev</td>
<td style="text-align: center;">$\sigma=1.12$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sigma=0.76$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sigma=0.93$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 7: Mixed-effects logistic regression coefficients and p-values for the effect of specificity, accuracy, and method on the probability of a comment receiving a given overall rating. Specificity is positively associated with neutral and good ratings, while major inaccuracies are strongly predictive of bad ratings. Note: there are no cases where a major inaccurate comment was rated as "good", leading to the extreme coefficient in that cell.</p>
<table>
<thead>
<tr>
<th style="text-align: center;">Factor</th>
<th style="text-align: center;">Inaccuracy=major</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Inaccuracy=minor</th>
<th style="text-align: center;"></th>
<th style="text-align: center;">Inaccuracy=none</th>
<th style="text-align: center;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"></td>
<td style="text-align: center;">Coef</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">Coef</td>
<td style="text-align: center;">p-value</td>
<td style="text-align: center;">Coef</td>
<td style="text-align: center;">p-value</td>
</tr>
<tr>
<td style="text-align: center;">(Intercept)</td>
<td style="text-align: center;">0.31</td>
<td style="text-align: center;">0.34</td>
<td style="text-align: center;">-1.27</td>
<td style="text-align: center;">$10^{-2}$</td>
<td style="text-align: center;">-1.71</td>
<td style="text-align: center;">$10^{-4}$</td>
</tr>
<tr>
<td style="text-align: center;">Specific</td>
<td style="text-align: center;">-0.82</td>
<td style="text-align: center;">$10^{-2}$</td>
<td style="text-align: center;">-0.12</td>
<td style="text-align: center;">0.70</td>
<td style="text-align: center;">1.06</td>
<td style="text-align: center;">$10^{-3}$</td>
</tr>
<tr>
<td style="text-align: center;">MARG-S</td>
<td style="text-align: center;">-0.35</td>
<td style="text-align: center;">0.19</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">-0.07</td>
<td style="text-align: center;">0.80</td>
</tr>
<tr>
<td style="text-align: center;">LiZCa</td>
<td style="text-align: center;">-0.34</td>
<td style="text-align: center;">0.41</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">0.39</td>
<td style="text-align: center;">-0.00</td>
<td style="text-align: center;">1.00</td>
</tr>
<tr>
<td style="text-align: center;">Random effect std. dev</td>
<td style="text-align: center;">$\sigma=0.74$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sigma=0.96$</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">$\sigma=0.91$</td>
<td style="text-align: center;"></td>
</tr>
</tbody>
</table>
<p>Table 8: Mixed-effects logistic regression coefficients and p-values for the effect of specificity on accuracy.</p>
<p>this could cause MARG-S to be over-represented at both extremes of accuracy and quality compared to the other methods. To investigate this, we fit logistic regression mixed-effects models to find the effect of specificity on the classification probabilities of the overall rating and on the accuracy while controlling for the generation method. In addition, we analyze the tendency of both specificity and accuracy to result in higher ratings using a cumulative link mixed-effects model.</p>
<p>We binarize specificity in these analyses by grouping "specific" and "very specific" judgements together as well as "generic" and "very generic" ones. The logistic regression and cumulative link models are implemented in R, using the lme4.glmer (Bates et al., 2009) and ordinal.clmm (Christensen, 2015) functions, respectively. We treat the submission ID as a group variable (random effect).</p>
<p>Results of the logistic regression analysis are shown in Table 7 (predicting overall rating) and Table 8 (predicting accuracy given specificity). Surprisingly, we find that specificity has a positive association with neutral ratings, contradicting our original speculation that the high specificity of MARG-S might push ratings to extremes. Higher specificity does not appear to produce a more extreme accuracy distribution either, and instead seems to weakly correspond with higher accuracy. It is unclear why specificity would influence accuracy in this way, but we speculate on three possibilities:</p>
<ul>
<li>Calibration: There is evidence that humans tend to give more precise answers when they are more confident (Welsh et al., 2011). The model may mimic this tendency and write more specific comments when it has greater confidence.</li>
<li>GPT-4 mode switching: GPT-4 may have an intrinsic tendency to write comments that are either good in both specificity and accuracy or bad in both. It has been rumored that GPT4 uses a mixture-of-experts architecture, ${ }^{8}$ in which case the correlated behavior may be related to expert routing.</li>
<li>Human bias: Humans may have a tendency to perceive comments as more specific when</li>
</ul>
<p><sup id="fnref7:0"><a class="footnote-ref" href="#fn:0">1</a></sup>they are more accurate, even if the specificity is not actually relevant to the accuracy. For example, "There is only one baseline for comparison. You should add more.", is very generic, and this is easy to see when it is inaccurate. However, if there really is only one baseline and adding more would be useful, it may be perceived as more specific because it appears to demonstrate an understanding of the paper.</p>
<p>The analysis in Table 6 shows that accuracy is highly predictive of overall rating, particularly for major inaccuracies. In fact, we find that $97 \%$ of all comments with a major inaccuracy are rated as bad, as opposed to $30 \%$ for minor inaccuracies and $23 \%$ for accurate comments. Specificity plays a larger role among accurate comments; within this group, only $19 \%$ of non-specific comments were rated as good, while $57 \%$ of specific comments were. Still, specificity and accuracy are not perfect predictors of comment quality; even among comments that were rated as both fully accurate and very specific, only $59 \%$ were rated as good.</p>
<h3>7.5 Compliments and ratings</h3>
<p>We observe qualitatively that some generated comments include compliments or flattering remarks; for example, a comment might say "While the authors have done a commendable job in [...], the paper could benefit from [...]". To test whether these compliments might bias the user ratings, we use GPT-4 to detect the presence of such remarks in all generated comments, using the following prompt:</p>
<div class="codehilite"><pre><span></span><code>Determine whether following review comment for a
    scientific paper includes a compliment or
flattering remark about the paper. Output a
JSON object with the key &quot;has_compliment&quot; set to
    true or false. Output only JSON with no
additional commentary.
Comment: {comment}
</code></pre></div>

<p>We find that $19 \%$ of MARG-S comments contain compliments, compared to $25 \%$ for SARG-B and $0 \%$ for LiZCa.</p>
<p>We then fit a cumulative link mixed-effect model with accuracy, specificity, method, and "has_compliment" as fixed effects and submission id as a random effect. We find that "has_compliment" has a coefficient of $0.11(p=0.76)$, which is small relative to the coefficients of other factors we observed in Table 6 and smaller than the random effect standard deviation $(\sigma=0.92)$, and we cannot reject the null hypothesis that the coefficient is 0 . Thus, it does</p>
<p>not appear that flattery causes a meaningful bias. Of course, we note that detecting compliments is somewhat subjective and can be a matter of degree, so it is still possible that there are more subtle biases in user ratings; we leave further analyses to future work.</p>
<h2>8 Failure analysis</h2>
<p>While MARG-S does well relative to other methods, there are still a large number of comments rated as "bad", and the precision and recall in the automated evaluation are still rather low in absolute terms. In this section, we qualitatively analyze the conversation message logs of the multi-agent system and identify several common classes of errors in the communication. The analysis was carried out by an author of this work with several publications in the field of machine learning and natural language processing, and the papers being analyzed were broadly related to the topic of machine learning.</p>
<h3>8.1 Scope</h3>
<p>There are two main stages of the multi-agent system: (1) the "main" stage, in which the model comes up with a list of comments, and (2) the refinement stage, in which the comments from the main stage are refined and potentially pruned if they are redundant. For 10 papers from the automated evaluation, we analyze the main stage for all three sub-reviewers (experiments, clarity, impact), for a total of 30 conversations. We additionally analyze the refinement stage for one randomly-selected comment from each of the 30 papers in the automated evaluation test set.</p>
<p>Checking each message against the paper for factual inconsistencies is expensive and error-prone, especially given the number of claims and comments that can be generated in the main stage, so for the main stage we only consider errors that are apparent from the conversations themselves. For the refinement stage, we do refer to the paper to check whether the models missed basic facts; however, it is important to note that only a limited amount of time (approximately 5-15 minutes) was spent to check comments against each paper, and due to the highly technical nature of these works it is possible that some factual errors were overlooked. Nonetheless, the fraction of invalid comments identified in this analysis is similar to the fraction of bad-rated comments found in the user study, so we believe
the findings are reasonably accurate.</p>
<h3>8.2 Main stage</h3>
<p>Below, we describe the error categories we identified for the main stage, along with the percentage of conversations that contain the error type. If the same error type appears multiple times in the same message log, we only count it once. It is worth noting that not all errors ultimately result in erroneous comments, as it is possible for agents to point out each others' errors and address them.</p>
<p>Overall, $70 \%$ of conversations contain at least one of these error types:</p>
<ul>
<li>Missing context (MC) (53\%): The leader agent fails to include key context in a message to another agent. In general, this tends to happen when it messages an expert agent and fails to include some information about the paper that the expert needs to proceed.</li>
<li>Missing context - misplaced SEND MESSAGE (MC-MSM) (47\%): A subtype of MC, this error occurs when the leader agent does include the necessary context in its generated output, but places the SEND MESSAGE marker after it instead of before.</li>
<li>Fails to Identify Error (FIE) (17\%): When the leader makes one of the aforementioned errors, worker or expert agents should point this out and ask the leader to try again, but they sometimes fail to do this.</li>
<li>Ignores Relevant Information (IRI) (10\%): An agent ignores part of a message that it should have responded to.</li>
<li>Failure to Respond (FR) (7\%): An agent does not recognize a message as being relevant and gives an empty or vapid response.</li>
<li>Skipping Steps (SS) (7\%): The leader moves to a later step too early. For example, writing the final review comments before the expert's questions are resolved, or skipping the initial step where it is supposed to get a summary of the paper.</li>
<li>
<p>Message loop (LOOP) (7\%): The agents enter a loop of similar messages, triggering the duplicate-message detector described in section 4.2.</p>
</li>
<li>
<p>Exceeds input token limit (EITL) (7\%): The conversation exceeds the input token limit for the underlying model. These cases occur when the expert asks too many questions, which can happen when the expert repeatedly asks for slightly more details each time it gets an answer to a question.</p>
</li>
</ul>
<p>Qualitatively, we noticed that there is a very common pattern for missing-context errors. Specifically, when the leader first addresses the expert, it tries to include a summary of the paper to give context for the expert, but it misplaces the SEND MESSAGE indicator.</p>
<p>This error occurs in $33 \%$ of conversations, but in $80 \%$ of those cases the expert points out the error and the leader corrects it. Interestingly, in many instances of the error, the leader tries to use a placeholder ("[insert summary here]") despite never being instructed to do so; for example (magenta text verbatim, black text is edited):</p>
<p>Agent 0 (leader): Summary: <omitted for brevity></p>
<p>Step 3: Share the summary with Agent 3 and ask for their input, specifically focusing on potential shortcomings of the paper's assumptions.</p>
<p>SEND MESSAGE: Agent 3, here is a summary of the paper: [insert summary here]. Could you please provide your input on potential shortcomings of the paper's assumptions?</p>
<p>Also interesting is the fact that in all cases when the leader fails to include the summary, there are no additional missing-context errors in the remainder of the message log. We conjecture that the early failure (and the following correction) may serve as a form of one-shot example that encourages the model to avoid such errors later in the discussion.</p>
<h3>8.3 Refinement stage</h3>
<p>Because the refinement stage works with one comment at a time, we do check the comment against the paper to determine if it is relevant. While this is somewhat subjective, we attempt to give the model the benefit of the doubt; if the comment is factually consistent and does raise a potentially valid suggestion (even if minor or difficult to address), we
consider the comment as valid in the sense that it is fine for the system not to prune it.</p>
<p>Below, we describe the error categories we identified for the refinement stage, along with the percentage of conversations that contain the error type. As with the main stage, if the same error type appears multiple times in the same conversation, we only count it once.</p>
<ul>
<li>Failure to prune a comment (47\%): The system fails to prune a comment that is invalid. This can happen for several reasons:</li>
<li>Ignored information (17\%): The comment is already addressed in the parsed paper text or contradicts information in the text, but the model did not recognize it.</li>
<li>Unavailable information (13\%): The comment is already addressed in the paper or contradicts information in the paper, but that information is not available in the parsed text (either due to parsing errors, or because it is in a figure or table).</li>
<li>Irrelevant (17\%): The comment asks for something that is trivial or does not make sense in the context of the paper (e.g., requesting an experiment to confirm a claim that the paper does not make or that is trivially true by definition).</li>
<li>Revising instead of pruning (30\%): The original comment for refinement is invalid, but the system revises it rather than pruning it. This is not always a failure, as the revision may result in a good comment, but there are also many cases where the revised comment has questionable relevance.</li>
<li>Failure to respond (17\%): An agent does not recognize a message as being relevant and gives an empty or vapid response.</li>
<li>Message loop (10\%): The agents enter a loop of similar messages, triggering the duplicatemessage detector described in section 4.2.</li>
<li>Pruned (3\%): The system marks the comment as invalid and prunes it. This only occurred for one comment in our sample, and the decision to prune was correct in that case.</li>
</ul>
<p>Our analysis finds that the system fails to prune invalid comments in $47 \%$ of cases, which is close to the $53 \%$ of comments that were rated as bad in the user study. ${ }^{9}$ Most of these cases appear to occur because the system lacks some information from the paper, either because the information isn't extracted by the PDF parser or because the model simply ignores it. The relatively high fraction of cases where models ignore information is quite interesting and suggests that GPT-4 can still fail in very basic ways, despite the strong results that have been seen in other studies of its reasoning ability. Our result is supported by some recent work, however, which suggests that large models do not always use their full input effectively, and have a particularly hard time attending to tokens near the middle of the input (Liu et al., 2023).</p>
<p>Another common trend we find is that even when presented with information that would render the original comment invalid (e.g., the paper already covers it), the GPT-4 leader agent strongly prefers to revise the comment to extend the request rather than pruning the comment. For example, consider the following comment:
"The paper does not provide specific details about the experiment setup using the MiniImagenet benchmark. The authors should provide more detail about this setup, including the specific settings and how the results were measured."</p>
<p>The paper actually does include details, which the leader agent summarizes:</p>
<p>Agent 0 (leader): Summary of findings:</p>
<ul>
<li>Agent 1 and Agent 2 did not find any information about the MiniImagenet benchmark in their parts of the paper.</li>
<li>Agent 3 found a section that discusses the setup for the MiniImagenet benchmark, including details about data efficiency and computational efficiency, and the tuning of hyper-parameters.</li>
<li>Agent 4 did not find any information about the MiniImagenet benchmark in their part of the paper.</li>
<li>Agent 5 found a section titled "D. 2 FEW-SHOT MINIIMAGENET" which provides details about the experiment setup using the MiniImagenet benchmark, including the structure of the dataset, the protocol for meta-training, and the comparison of the BMG meta-objective against the MG meta-objective.</li>
<li>Agent 6 found information about the model</li>
</ul>
<p><sup id="fnref8:0"><a class="footnote-ref" href="#fn:0">1</a></sup>used in the MiniImagenet benchmark, the training setup, and the behavior of BMG and MG with different data and time budgets. ...</p>
<p>However, while it recognizes the original comment as invalid, it refines the comment to ask for the information to be presented more clearly instead of pruning it:</p>
<p>Agent 0 (leader): ... The comment is not valid and should be removed. However, it may be helpful to suggest that the authors consolidate the information about the MiniImagenet benchmark into one section for clarity.</p>
<p>Revised comment: "The paper provides detailed information about the experiment setup using the MiniImagenet benchmark, including the specific settings and how the results were measured. However, this information is spread across different sections of the paper, which may make it difficult for readers to find and understand. The authors could improve the paper by consolidating this information into one section."</p>
<p>In this case, the revised comment is a valid comment. However, this is not always the case; the leader often doesn't check the validity of the new revised comment with the other agents, so if the refinement introduces an invalid request it typically will not catch the error. It may be possible to improve accuracy by repeating the refinement stage several times, although it would be expensive to do so for every comment.</p>
<h2>9 Conclusion</h2>
<p>In this work, we have introduced MARG, a novel method for review generation, which uses a network of LLM agents that communicate to share information across different parts of a paper and to engage in internal discussion to write better comments. We evaluated our best variant (MARG-S) against both our own simple baselines and a contemporaneously-published GPT-4 baseline and found that MARG-S produces more good comments in both an alignment-based evaluation and a user study. The user study found that MARG-S is especially strong in terms of specificity and tends to generate very detailed comments compared to other methods. However, a majority of comments across all methods (including MARG-S) are rated as bad, and $38-48 \%$ are rated as highly inaccurate, suggesting that substantial work is still needed.</p>
<p>MARG-S is substantially more expensive to use compared to other methods (in terms of both time</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{9}$ We would expect to find slightly fewer bad comments in this analysis than in the user study, as the annotator of this analysis is not as familiar with the papers as their respective authors would be.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
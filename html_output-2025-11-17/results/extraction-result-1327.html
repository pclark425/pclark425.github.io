<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1327 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1327</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1327</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-29.html">extraction-schema-29</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <p><strong>Paper ID:</strong> paper-257622981</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2303.09597v1.pdf" target="_blank">Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots</a></p>
                <p><strong>Paper Abstract:</strong> The light and soft characteristics of Buoyancy Assisted Lightweight Legged Unit (BALLU) robots have a great potential to provide intrinsically safe interactions in environments involving humans, unlike many heavy and rigid robots. However, their unique and sensitive dynamics impose challenges to obtaining robust control policies in the real world. In this work, we demonstrate robust sim-to-real transfer of control policies on the BALLU robots via system identification and our novel residual physics learning method, Environment Mimic (EnvMimic). First, we model the nonlinear dynamics of the actuators by collecting hardware data and optimizing the simulation parameters. Rather than relying on standard supervised learning formulations, we utilize deep reinforcement learning to train an external force policy to match real-world trajectories, which enables us to model residual physics with greater fidelity. We analyze the improved simulation fidelity by comparing the simulation trajectories against the real-world ones. We finally demonstrate that the improved simulator allows us to learn better walking and turning policies that can be successfully deployed on the hardware of BALLU.</p>
                <p><strong>Cost:</strong> 0.013</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1327.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1327.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>EnvMimic (augmented simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Environment Mimic: residual-dynamics augmented PyBullet simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An augmented simulation pipeline built on PyBullet that combines actuator system identification with a deep-RL learned residual-aerodynamics policy (EnvMimic) which applies 3D external forces to the balloon center-of-mass to better match real BALLU trajectories and improve sim-to-real transfer of locomotion policies.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet (augmented with EnvMimic)</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>PyBullet rigid-body physics simulator augmented by: (1) identified nonlinear actuator/cable-driven mapping (spring params, motor gains, default angles) via optimization-based system identification; and (2) a residual aerodynamics policy learned with PPO that outputs 3D forces applied to the balloon CoM to mimic unmodeled aerodynamic and other residual effects.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robot aerodynamics (fluid-structure interaction approximated)</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>medium-fidelity — rigid-body dynamics with identified actuator nonlinearities plus approximate residual aerodynamics modeled by an RL policy; not full CFD-level aerodynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Includes: identified nonlinear motor-to-joint mapping (spring parameters, motor gains, default angles), contact/friction from PyBullet, and a learned 3D external-force (x,y,z) residual applied to balloon CoM in range [-1,1] N; Uses Euler angles for orientation. Omits: full Navier–Stokes CFD modeling, distributed aerodynamic pressure/torque fields, detailed cable dynamics beyond identified polynomial fits, and complex torque residuals (only CoM forces used).</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>EnvMimic residual aerodynamics policy; locomotion policies (ForwardWalking, TurningLeft)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement-learning agents trained with Proximal Policy Optimization (stable-baselines). Residual policy: compact neural network (2 hidden layers, 64 neurons each). Locomotion policies: PPO-based agents trained in the improved simulator (state = positions/velocities/orientations; actions = actuator commands derived from identified mapping).</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Learn residual dynamics (aerodynamics and unmodeled effects) by motion imitation to enable training of locomotion controllers that transfer to real hardware; i.e., model-based reasoning about unmodeled external forces through learned policies.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world BALLU hardware (Vicon-tracked buoyancy-assisted legged robot)</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Forward-walking policy trained in augmented simulator (EnvMimic) traveled 1.12 m on hardware (vs baselines 0.27 m, 0.32 m, 0.56 m). Turning policy traveled 0.29 m left on hardware (vs baselines 0.20 m, -0.04 m, 0.11 m). The EnvMimic-augmented simulator also yielded the lowest sim-to-real errors (average CoM and final yaw) though some specific numeric error entries are not fully reported.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Compared vanilla PyBullet, supervised residual-dynamics augmentation, PyBullet + system-identification, domain randomization variants, and EnvMimic. EnvMimic (system ID + RL residuals) produced the best trajectory tracking and best sim-to-real transfer; supervised residual learning tracked worse (reduced turning behavior) and domain randomization alone was insufficient.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Authors conclude that actuator system identification (to capture nonlinear motor-to-joint mapping) plus modeling residual aerodynamics is necessary for successful transfer; domain randomization alone was insufficient. They explicitly avoided randomizing mass or buoyancy because policies are highly sensitive to those parameters, indicating certain simulator features (accurate actuator mapping and residual aerodynamics) are necessary while others (mass randomization) may be harmful for this platform.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Vanilla simulator (PyBullet) failed to capture aerodynamic-induced yaw behaviors and produced policies that turned instead of walking; supervised residual-dynamics learning (SL) trained on limited hardware data underperformed and resulted in incorrect yaw behaviors (stayed on positive Y side); policies trained with some baseline combinations rotated in place or took only one step (poor transfer).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1327.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1327.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PyBullet (vanilla)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>PyBullet physics-based simulator</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An open-source rigid-body physics engine used as the base simulator for training RL policies and for implementing the EnvMimic augmentation; in this work the vanilla PyBullet lacked sufficient fidelity for balloon aerodynamics and cable-actuated nonlinearities.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Pybullet, a python module for physics simulation for games, robotics and machine learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>PyBullet</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>Rigid-body physics engine supporting joints, contacts, friction, and basic actuators; used to simulate BALLU's rigid-body dynamics and contacts but without high-fidelity aerodynamic or detailed cable-actuator models by default.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-to-medium fidelity for buoyant balloon robot dynamics — accurate for rigid-body contacts and joints but missing detailed aerodynamic effects and complex cable-actuation nonlinearities.</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Models rigid-body dynamics, contacts, and basic friction; in default form assumes ideal actuator-to-joint mapping (unless user models otherwise); does not model full aerodynamic forces/torques or complex cable slack and nonlinear friction unless augmented.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td>Vanilla-simulation RL agents (initial policies used to generate reference actions/trajectories)</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>Reinforcement-learning agents (PPO) trained in vanilla PyBullet to produce initial action sequences; policies later used open-loop on hardware to collect reference trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td>Train locomotion policies and generate reference trajectories for system identification and residual dynamics learning.</td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td>Real-world BALLU hardware</td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td>Poor transfer: policies trained in vanilla PyBullet exhibited substantial reality gap (e.g., produced turning rather than forward walking); baseline hardware distances for policies without EnvMimic were 0.27 m, 0.32 m, 0.56 m depending on added ID/DR, whereas EnvMimic-trained policy traveled 1.12 m.</td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td>Vanilla PyBullet produced the worst tracking and transfer; adding system identification and RL residuals progressively improved fidelity and transfer, with EnvMimic best.</td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Demonstrates that vanilla rigid-body simulation lacks necessary fidelity (notably aerodynamics and realistic actuator response) for BALLU; improving actuator modeling and adding residual forces were necessary.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Vanilla PyBullet alone failed to reproduce yaw changes seen on hardware and led to poor sim-to-real transfer where policies rotated or otherwise behaved incorrectly on the real robot.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1327.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1327.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of simulators used for training models or agents on scientific reasoning tasks (especially in thermodynamics, circuits, or biology), including details about simulator fidelity levels and transfer performance to real-world or different contexts.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>CoppeliaSim (V-REP)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>CoppeliaSim (formerly V-REP) robot simulation framework</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>Mentioned as an example of a vanilla rigid-body simulator (alongside PyBullet) that, without augmentation, fails to capture aerodynamics and low-fidelity actuator effects for the BALLU platform.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Coppeliasim (formerly v-rep): a versatile and scalable robot simulation framework.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_name</strong></td>
                            <td>CoppeliaSim</td>
                        </tr>
                        <tr>
                            <td><strong>simulator_description</strong></td>
                            <td>General-purpose robot simulator for rigid-body dynamics, sensors, and scripts; cited as a common off-the-shelf rigid-body simulator.</td>
                        </tr>
                        <tr>
                            <td><strong>scientific_domain</strong></td>
                            <td>mechanics / robotics</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_level</strong></td>
                            <td>low-to-medium fidelity for balloon aerodynamics in default configuration</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_characteristics</strong></td>
                            <td>Provides contacts, joints, sensors, and scripting but typically does not include high-fidelity aerodynamic or CFD modeling by default; actuator models are often idealized unless user-implemented.</td>
                        </tr>
                        <tr>
                            <td><strong>model_or_agent_name</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>reasoning_task</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>training_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_target</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>transfer_performance</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>compares_fidelity_levels</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>fidelity_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>minimal_fidelity_discussion</strong></td>
                            <td>Cited as another example of rigid-body simulators whose low-fidelity aerodynamics and actuator models contribute to the sim-to-real gap for BALLU.</td>
                        </tr>
                        <tr>
                            <td><strong>failure_cases</strong></td>
                            <td>Not used experimentally in this paper, but mentioned that such simulators without augmentation fail to model aerodynamics and low-fidelity actuators leading to transfer issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots', 'publication_date_yy_mm': '2023-03'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Pybullet, a python module for physics simulation for games, robotics and machine learning. <em>(Rating: 2)</em></li>
                <li>Sim-toreal transfer with neural-augmented robot simulation <em>(Rating: 2)</em></li>
                <li>Augmenting physical simulators with stochastic neural networks: Case study of planar pushing and bouncing <em>(Rating: 2)</em></li>
                <li>Tossingbot: Learning to throw arbitrary objects with residual physics <em>(Rating: 2)</em></li>
                <li>Neural-fly enables rapid learning for agile flight in strong winds <em>(Rating: 1)</em></li>
                <li>Sim-toreal transfer of robotic control with dynamics randomization <em>(Rating: 2)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1327",
    "paper_id": "paper-257622981",
    "extraction_schema_id": "extraction-schema-29",
    "extracted_data": [
        {
            "name_short": "EnvMimic (augmented simulator)",
            "name_full": "Environment Mimic: residual-dynamics augmented PyBullet simulator",
            "brief_description": "An augmented simulation pipeline built on PyBullet that combines actuator system identification with a deep-RL learned residual-aerodynamics policy (EnvMimic) which applies 3D external forces to the balloon center-of-mass to better match real BALLU trajectories and improve sim-to-real transfer of locomotion policies.",
            "citation_title": "here",
            "mention_or_use": "use",
            "simulator_name": "PyBullet (augmented with EnvMimic)",
            "simulator_description": "PyBullet rigid-body physics simulator augmented by: (1) identified nonlinear actuator/cable-driven mapping (spring params, motor gains, default angles) via optimization-based system identification; and (2) a residual aerodynamics policy learned with PPO that outputs 3D forces applied to the balloon CoM to mimic unmodeled aerodynamic and other residual effects.",
            "scientific_domain": "mechanics / robot aerodynamics (fluid-structure interaction approximated)",
            "fidelity_level": "medium-fidelity — rigid-body dynamics with identified actuator nonlinearities plus approximate residual aerodynamics modeled by an RL policy; not full CFD-level aerodynamics.",
            "fidelity_characteristics": "Includes: identified nonlinear motor-to-joint mapping (spring parameters, motor gains, default angles), contact/friction from PyBullet, and a learned 3D external-force (x,y,z) residual applied to balloon CoM in range [-1,1] N; Uses Euler angles for orientation. Omits: full Navier–Stokes CFD modeling, distributed aerodynamic pressure/torque fields, detailed cable dynamics beyond identified polynomial fits, and complex torque residuals (only CoM forces used).",
            "model_or_agent_name": "EnvMimic residual aerodynamics policy; locomotion policies (ForwardWalking, TurningLeft)",
            "model_description": "Reinforcement-learning agents trained with Proximal Policy Optimization (stable-baselines). Residual policy: compact neural network (2 hidden layers, 64 neurons each). Locomotion policies: PPO-based agents trained in the improved simulator (state = positions/velocities/orientations; actions = actuator commands derived from identified mapping).",
            "reasoning_task": "Learn residual dynamics (aerodynamics and unmodeled effects) by motion imitation to enable training of locomotion controllers that transfer to real hardware; i.e., model-based reasoning about unmodeled external forces through learned policies.",
            "training_performance": null,
            "transfer_target": "Real-world BALLU hardware (Vicon-tracked buoyancy-assisted legged robot)",
            "transfer_performance": "Forward-walking policy trained in augmented simulator (EnvMimic) traveled 1.12 m on hardware (vs baselines 0.27 m, 0.32 m, 0.56 m). Turning policy traveled 0.29 m left on hardware (vs baselines 0.20 m, -0.04 m, 0.11 m). The EnvMimic-augmented simulator also yielded the lowest sim-to-real errors (average CoM and final yaw) though some specific numeric error entries are not fully reported.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Compared vanilla PyBullet, supervised residual-dynamics augmentation, PyBullet + system-identification, domain randomization variants, and EnvMimic. EnvMimic (system ID + RL residuals) produced the best trajectory tracking and best sim-to-real transfer; supervised residual learning tracked worse (reduced turning behavior) and domain randomization alone was insufficient.",
            "minimal_fidelity_discussion": "Authors conclude that actuator system identification (to capture nonlinear motor-to-joint mapping) plus modeling residual aerodynamics is necessary for successful transfer; domain randomization alone was insufficient. They explicitly avoided randomizing mass or buoyancy because policies are highly sensitive to those parameters, indicating certain simulator features (accurate actuator mapping and residual aerodynamics) are necessary while others (mass randomization) may be harmful for this platform.",
            "failure_cases": "Vanilla simulator (PyBullet) failed to capture aerodynamic-induced yaw behaviors and produced policies that turned instead of walking; supervised residual-dynamics learning (SL) trained on limited hardware data underperformed and resulted in incorrect yaw behaviors (stayed on positive Y side); policies trained with some baseline combinations rotated in place or took only one step (poor transfer).",
            "uuid": "e1327.0",
            "source_info": {
                "paper_title": "Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "PyBullet (vanilla)",
            "name_full": "PyBullet physics-based simulator",
            "brief_description": "An open-source rigid-body physics engine used as the base simulator for training RL policies and for implementing the EnvMimic augmentation; in this work the vanilla PyBullet lacked sufficient fidelity for balloon aerodynamics and cable-actuated nonlinearities.",
            "citation_title": "Pybullet, a python module for physics simulation for games, robotics and machine learning.",
            "mention_or_use": "use",
            "simulator_name": "PyBullet",
            "simulator_description": "Rigid-body physics engine supporting joints, contacts, friction, and basic actuators; used to simulate BALLU's rigid-body dynamics and contacts but without high-fidelity aerodynamic or detailed cable-actuator models by default.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "low-to-medium fidelity for buoyant balloon robot dynamics — accurate for rigid-body contacts and joints but missing detailed aerodynamic effects and complex cable-actuation nonlinearities.",
            "fidelity_characteristics": "Models rigid-body dynamics, contacts, and basic friction; in default form assumes ideal actuator-to-joint mapping (unless user models otherwise); does not model full aerodynamic forces/torques or complex cable slack and nonlinear friction unless augmented.",
            "model_or_agent_name": "Vanilla-simulation RL agents (initial policies used to generate reference actions/trajectories)",
            "model_description": "Reinforcement-learning agents (PPO) trained in vanilla PyBullet to produce initial action sequences; policies later used open-loop on hardware to collect reference trajectories.",
            "reasoning_task": "Train locomotion policies and generate reference trajectories for system identification and residual dynamics learning.",
            "training_performance": null,
            "transfer_target": "Real-world BALLU hardware",
            "transfer_performance": "Poor transfer: policies trained in vanilla PyBullet exhibited substantial reality gap (e.g., produced turning rather than forward walking); baseline hardware distances for policies without EnvMimic were 0.27 m, 0.32 m, 0.56 m depending on added ID/DR, whereas EnvMimic-trained policy traveled 1.12 m.",
            "compares_fidelity_levels": true,
            "fidelity_comparison_results": "Vanilla PyBullet produced the worst tracking and transfer; adding system identification and RL residuals progressively improved fidelity and transfer, with EnvMimic best.",
            "minimal_fidelity_discussion": "Demonstrates that vanilla rigid-body simulation lacks necessary fidelity (notably aerodynamics and realistic actuator response) for BALLU; improving actuator modeling and adding residual forces were necessary.",
            "failure_cases": "Vanilla PyBullet alone failed to reproduce yaw changes seen on hardware and led to poor sim-to-real transfer where policies rotated or otherwise behaved incorrectly on the real robot.",
            "uuid": "e1327.1",
            "source_info": {
                "paper_title": "Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots",
                "publication_date_yy_mm": "2023-03"
            }
        },
        {
            "name_short": "CoppeliaSim (V-REP)",
            "name_full": "CoppeliaSim (formerly V-REP) robot simulation framework",
            "brief_description": "Mentioned as an example of a vanilla rigid-body simulator (alongside PyBullet) that, without augmentation, fails to capture aerodynamics and low-fidelity actuator effects for the BALLU platform.",
            "citation_title": "Coppeliasim (formerly v-rep): a versatile and scalable robot simulation framework.",
            "mention_or_use": "mention",
            "simulator_name": "CoppeliaSim",
            "simulator_description": "General-purpose robot simulator for rigid-body dynamics, sensors, and scripts; cited as a common off-the-shelf rigid-body simulator.",
            "scientific_domain": "mechanics / robotics",
            "fidelity_level": "low-to-medium fidelity for balloon aerodynamics in default configuration",
            "fidelity_characteristics": "Provides contacts, joints, sensors, and scripting but typically does not include high-fidelity aerodynamic or CFD modeling by default; actuator models are often idealized unless user-implemented.",
            "model_or_agent_name": null,
            "model_description": null,
            "reasoning_task": null,
            "training_performance": null,
            "transfer_target": null,
            "transfer_performance": null,
            "compares_fidelity_levels": false,
            "fidelity_comparison_results": "",
            "minimal_fidelity_discussion": "Cited as another example of rigid-body simulators whose low-fidelity aerodynamics and actuator models contribute to the sim-to-real gap for BALLU.",
            "failure_cases": "Not used experimentally in this paper, but mentioned that such simulators without augmentation fail to model aerodynamics and low-fidelity actuators leading to transfer issues.",
            "uuid": "e1327.2",
            "source_info": {
                "paper_title": "Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots",
                "publication_date_yy_mm": "2023-03"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Pybullet, a python module for physics simulation for games, robotics and machine learning.",
            "rating": 2,
            "sanitized_title": "pybullet_a_python_module_for_physics_simulation_for_games_robotics_and_machine_learning"
        },
        {
            "paper_title": "Sim-toreal transfer with neural-augmented robot simulation",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_with_neuralaugmented_robot_simulation"
        },
        {
            "paper_title": "Augmenting physical simulators with stochastic neural networks: Case study of planar pushing and bouncing",
            "rating": 2,
            "sanitized_title": "augmenting_physical_simulators_with_stochastic_neural_networks_case_study_of_planar_pushing_and_bouncing"
        },
        {
            "paper_title": "Tossingbot: Learning to throw arbitrary objects with residual physics",
            "rating": 2,
            "sanitized_title": "tossingbot_learning_to_throw_arbitrary_objects_with_residual_physics"
        },
        {
            "paper_title": "Neural-fly enables rapid learning for agile flight in strong winds",
            "rating": 1,
            "sanitized_title": "neuralfly_enables_rapid_learning_for_agile_flight_in_strong_winds"
        },
        {
            "paper_title": "Sim-toreal transfer of robotic control with dynamics randomization",
            "rating": 2,
            "sanitized_title": "simtoreal_transfer_of_robotic_control_with_dynamics_randomization"
        }
    ],
    "cost": 0.01264625,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots</p>
<p>Nitish Sontakke nitishsontakke@gatech.edu 
School of Interactive Computing
Georgia Institute of Technol-ogy
30308AtlantaGAUSA</p>
<p>Hosik Chae hosikchae@ucla.edu 
Department of Mechanical and Aerospace Engineering
University of California
90095Los Angeles (UCLA), Los AngelesCAUSA</p>
<p>Sangjoon Lee sangjoonlee@cs.ucla.edu 
Department of Computer Science
University of California
90095Los Angeles (UCLA), Los AngelesCAUSA</p>
<p>Tianle Huang 
School of Interactive Computing
Georgia Institute of Technol-ogy
30308AtlantaGAUSA</p>
<p>Dennis W Hong dennishong@ucla.edu 
Department of Mechanical and Aerospace Engineering
University of California
90095Los Angeles (UCLA), Los AngelesCAUSA</p>
<p>Sehoon Ha sehoonha@gatech.edu 
School of Interactive Computing
Georgia Institute of Technol-ogy
30308AtlantaGAUSA</p>
<p>Residual Physics Learning and System Identification for Sim-to-real Transfer of Policies on Buoyancy Assisted Legged Robots
E784B521234C86F0D9FBA245C2B4A4F8
The light and soft characteristics of Buoyancy Assisted Lightweight Legged Unit (BALLU) robots have a great potential to provide intrinsically safe interactions in environments involving humans, unlike many heavy and rigid robots.However, their unique and sensitive dynamics impose challenges to obtaining robust control policies in the real world.In this work, we demonstrate robust sim-to-real transfer of control policies on the BALLU robots via system identification and our novel residual physics learning method, Environment Mimic (EnvMimic).First, we model the nonlinear dynamics of the actuators by collecting hardware data and optimizing the simulation parameters.Rather than relying on standard supervised learning formulations, we utilize deep reinforcement learning to train an external force policy to match real-world trajectories, which enables us to model residual physics with greater fidelity.We analyze the improved simulation fidelity by comparing the simulation trajectories against the real-world ones.We finally demonstrate that the improved simulator allows us to learn better walking and turning policies that can be successfully deployed on the hardware of BALLU.</p>
<p>I. INTRODUCTION</p>
<p>Buoyancy-assisted or balloon-based robots [1]- [3] have great potential to offer fundamental safety in human environments.Traditional mobile robots while being able to execute a variety of tasks, tend to be rigid and heavy and may cause serious damage to their surroundings or themselves in case of control or perception errors.On the other hand, buoyancy-assisted robots (BARs) [4] are typically designed to be lightweight, compact, and intrinsically safe.Therefore, they can be used for various applications that require close human-robot interaction, such as education, entertainment, and healthcare.For instance, Chae et al. [1] present Buoyancy Assisted Lightweight Legged Unit (BALLU), which is a balloon-based robot with two legs (Fig. 1), and showcase that it can be deployed to various indoor and outdoor environments without any safety concerns.</p>
<p>However, it is not straightforward to control BARs due to their unique, non-linear, and sensitive dynamics.One popular approach for robot control is model-predictive control (MPC) [5] which plans future trajectories via models and minimizes the provided cost function.The complex dynamics of BARs, however, prevent us from developing Fig. 1: An image of the successful policy for the forward walking task, which is trained in the improved simulation using our method.concise and effective models and therefore rule out MPC as a control method.In contrast, deep reinforcement learning (deep RL) offers an automated approach to training a control policy from a simple reward function without robot-specific models.On the flip side, policies trained with deep RL often experience severe performance degradation when deployed to the robot due to the difference between the simulated and real-world environments, which is commonly known as the sim-to-real gap or the reality gap [6].In our experience, this gap is further compounded in the case of BALLU when we employ a vanilla rigid body simulator, such as PyBullet [7] or CoppeliaSim [8], due to the unmodeled aerodynamics and low-fidelity actuators.</p>
<p>In this work, we mitigate the sim-to-real gap of the BALLU robot by identifying system parameters and modeling residual dynamics using a novel technique, EnvMimic.First, we iteratively tune the actuator parameters in simulation based on the data collected from hardware experiments.This system identification allows us to better illustrate the nonlinear dynamics of BALLU's cable-driven actuation mechanism.Second, we learn the residual physics of the BALLU robot from the collected real-world trajectories to capture its complex aerodynamics that are difficult to model analytically.To this end, we propose a novel technique, Environment Mimic (EnvMimic), which learns to gener-arXiv:2303.09597v1[cs.RO] 16 Mar 2023 ate external forces to match the simulation and real-world trajectories via deep RL, which is different from common supervised learning formulations [9]- [12].This is similar to using pseudo-forces like centrifugal force or Coriolis force to explain the observed behavior.Our approach can also be viewed as an inside-out flipped version of the recent motion imitation frameworks [13]- [16], which learn internal controllers that enable the robot to imitate reference motions.In our case, we treat the real-world trajectories as a reference and learn an external residual force policy to imitate that behavior in simulation.We also observe that EnvMimic exhibits a robust generalization capability, even when we have a small number of trajectories.</p>
<p>We demonstrate that the proposed techniques can successfully reduce the sim-to-real gap of the BALLU robot.Firstly, we show that modeling the actuators and capturing the aerodynamics results in a significantly improved and qualitatively richer simulation.Our augmented simulator successfully illustrates asymmetric turning behaviors, which are observed on hardware but are not captured by the vanilla version or the simulator with supervised residual dynamics learning.We also demonstrate that we can improve the simto-real transfer performance of the policies for two tasks, walking and turning, on the hardware of the BALLU robot.</p>
<p>II. RELATED WORK</p>
<p>A. Deep Reinforcement Learning</p>
<p>Deep RL [17]- [19] has allowed researchers to make great strides in various fields of robotics, including navigation [20], [21], locomotion [22], [23], and manipulation [24], [25].However, successfully deploying these controllers on hardware is still an active area of research [26], [27], which is not straightforward due to the discrepancy between the simulation and the real world [6].One of the most common approaches is domain randomization [23], [24], [28]- [31], which exposes an agent to a variety of dynamics during training.Additionally, employing extensions such as privileged learning [21], [22] and adopting structured state [21] and action space [32] representations has also enabled successful deployment of learned policies on hardware.On the other hand, researchers have developed frameworks to learn policies directly from real-world experience, which have been proven effective for both manipulators [25] and legged robots [19], [33].This paper discusses a sim-to-real transfer for the BALLU robot with highly sensitive dynamics inspired by these previous approaches.Drawing inspiration from these previous approaches, this paper discusses a sim-toreal transfer technique for the BALLU robot, which exhibits highly sensitive dynamics.Specifically, we train policies in simulation and enhance the sim-to-real transferability using real-world data.</p>
<p>B. System Identification</p>
<p>Our approach is also highly inspired by system identification that aims to identify model parameters from the collected experimental data.This is a well-studied problem that has been addressed by a variety of methods involving maximum likelihood estimation [34], [35], optimization-based strategies [36]- [38], neural networks [39]- [41] with iterative learning [42], [43], actuator dynamics identification [44], [45], adversarial learning [46], and learning residual physics [9]- [12], [25].Combining system identification with other techniques, such as dynamics randomization, latency modeling, and noise injection [47], [48], has also proven to be effective for successful sim-to-real transfer of learned policies.However, in our case, system identification even in combination with domain randomization, proves to be insufficient, necessitating the need for our residual dynamics learning framework, EnvMimic.</p>
<p>C. Balloon-based Robots</p>
<p>Balloon-based or buoyancy-assisted robots [2], [49] have been investigated because of their intrinsic stability and low costs.Therefore, many researchers have investigated them in various applications, including roof cleaning [50], planet exploration [51], disaster investigation [52]- [54], social interactions [55], security [56], and many more.However, control of these balloon robots is not straightforward due to their sensitive non-linear dynamics.One common approach is to develop model-based controllers [3], [56]- [58], often with system identification.This paper discusses the control of Balloon-based legged robots proposed by Chae et al. [1] by leveraging deep reinforcement learning and residual physics.</p>
<p>III. SIM-TO-REAL OF BALLU</p>
<p>In this section, we will describe our techniques for reducing the 'reality gap' [6] of the BALLU robot [1].We approach this challenging problem by combining traditional system identification and deep residual dynamics learning.First, we improve the simulation model of cable-driven actuation by identifying non-linear relationships between motor and joint angles.Next, we use the captured real-world trajectories to model the residual dynamics of the BALLU robot, which arise from various sources such as aerodynamics, joint slackness, and inertial parameter mismatch.Our key invention is to use deep RL for building a residual dynamics model instead of the common choice of supervised learning, which offers effective generalization over a small number of trajectories.</p>
<p>A. Background: BALLU robot BALLU (Buoyancy Assisted Lightweight Legged Unit) is a novel buoyancy-assisted bipedal robot with six helium balloons, which provide enough buoyancy to counteract the gravitational force.BALLU's base is connected to helium balloons and houses a Raspberry Pi Zero W board for computing.The robot has two passive hip joints and two active knee joints, which are actuated by two Dymond D47 servo motors at the feet via cables.The overview of the robot is illustrated in Fig. 2. For more details, please refer to the original paper by Chae et al. colleague [1].</p>
<p>Due to its unique dynamics, model-free reinforcement learning can be a promising approach for developing effective controllers for BALLU without having to rely on prior knowledge or domain expertise.However, we need to mitigate the large sim-to-real gap first, which is induced by significant drag force effects and low-fidelity hardware.</p>
<p>B. System Identification</p>
<p>One main source of the sim-to-real gap is its cabledriven actuation mechanism.In the simulation, servo motor commands and knee joint angles maintain an ideal relationship.In reality, they are affected by friction, torque saturation, and unmodeled cable dynamics, which make the actuator dynamics noisy and nonlinear.Therefore, we first perform system identification to better capture this nonlinear relationship from real-world data using optimization.</p>
<p>Our free variables p include knee spring parameters, motor gains, default motor angles, and default knee joint angles in simulation, which are sufficient to model various nonlinear relationships.As a result, we have eight free variables subject to optimization.</p>
<p>Our objective function is to minimize the discrepancy of all four joint angles (left and right, motor arm and knee) between simulation and hardware.We sample 20 actuation commands that constitute A that are uniformly distributed over the range [0, 1], which corresponds to motor arm angles in the range [0 • , 90 • ], and measure knee and motor joint angles in simulation and on hardware.Then we fit polynomial curves for all the joints and compute the directed Hausdorff distance between the corresponding curves.We use the L-BFGS-B algorithm and optimize the parameters until convergence.The entire process is summarized in Algorithm 1.</p>
<p>C. Residual Dynamics Learning via Reinforcement Learning</p>
<p>Our next step is to model the residual dynamics of BALLU.Previous methods for learning residual dynamics have employed supervised learning [9]- [12] or a combination of self-supervision with deep RL as part of the learning Optimize p using L-BFGS-B 12: end while pipeline [25].However, off-the-shelf supervised learning, in addition to requiring a large number of real-world trajectories, is plagued by limited exploration.Even a small perturbation to the states the policy has observed during training can cause it to diverge during test time.Moreover, we observe that stochasticity in the real world often leads to multiple different state trajectories arising from the same state even when we apply the same actions.The framework of deep RL lends itself naturally to addressing these issues by augmenting the data with simulated trajectories, making it a suitable choice for this problem.</p>
<p>Our key insight is to augment the original simulation framework using a learned residual aerodynamics policy.This policy allows us to capture the complex interaction between BALLU and its environment in greater detail.We will demonstrate in the next section that learning locomotion behaviors with this aerodynamics policy in the loop translate to better transfer of our simulation policies to hardware compared to traditional techniques like domain randomization.</p>
<p>Therefore, we design a framework to learn a policy that generates proper external perturbation forces that can match the simulation behavior to the ground-truth trajectory collected from the hardware.We draw inspiration from motion imitation methods [13]- [16], which have demonstrated impressive results for learning dynamics controllers to track reference motions.The fundamental difference is that we learn a policy for external perturbations, while other motion imitation works aim to learn an internal control policy for the robot's actuators.In our experience, this deep RL approach allows us to model robust residual dynamics from a limited set of real-world trajectories compared to supervised learning.Data Collection.The first step is to create a set of reference trajectories.We train several locomotion policies in the vanilla simulation and record their action trajectories.Next, we use the recorded actions as open loop control on hardware to collect multiple state trajectories.We use a motion capture system to obtain observation data due to the lack of onboard sensors on BALLU that can estimate its global position and orientation.We note that hand-designed action trajectories may work well for this step.</p>
<p>MDP Formulation.Once we have the reference dataset, we can cast learning the residual aerodynamics policy as a motion imitation problem using a Markov Decision Process (MDP).The state space consists of the balloon's position, velocity, orientation, the position and velocity of the base, and the position and velocity of the feet, at the current and last two time steps.The action space is three-dimensional and consists of x, y, and z forces that are applied to the center of mass of the balloon.The forces are in the range of [−1, 1] N. The reward function is a combination of position and orientation terms and is defined as follows: r t = w pos r pos t + w orn r orn t where the position and orientation terms respectively are computed as follows:
r pos = exp −10 ||p t − p t || 2 r orn = exp −2 ||r t − r t || 2 W ,
where pt , p t , rt , and r t are the desired position, the actual position, the desired orientation, and the actual orientation of the balloons, respectively.The position reward r pos t encourages the simulated model's balloon to track the reference balloon position as closely as possible while the orientation reward r orn t encourages it to track the reference balloon orientation.We use the Euler angle representation for orientation, which demonstrates better performance than the quaternion representation.For all experiments, we set w pos = 0.7, w orn = 0.3, and W = diag(0.2,0.4, 0.4).Training.We train the residual dynamics policy using Proximal Policy Optimization [18].We use a compact network consisting of two layers with 64 neurons each.Similar to Peng et al. [13], we also randomize the initial state for each rollout by sampling a state uniformly at random from the selected reference trajectory.This leads to the policy being exposed to a wider initial state distribution and improves robustness, especially when transferring to hardware.</p>
<p>D. Policy Training with Improved Simulation</p>
<p>Once we improve the simulation using system identification and residual dynamics learning, we can retrain a deep RL policy for better sim-to-real transfer.Once again, we formulate the problem using a Markov Decision Process framework.The state space consists of the balloon's position, velocity, orientation, the position and velocity of the base, and the position and velocity of the feet, all measured at the current time step.Our actions are two actuator commands, which will change the joint angles based on the identified nonlinear relationship in the previous section.</p>
<p>We learn two policies -one for forward walking and one for turning left.For the forward walking task, our reward function is x vel , whereas for turning left, it is y vel .</p>
<p>IV. EXPERIMENTS AND RESULTS</p>
<p>We design simulation and hardware results to answer the following two research questions.• Can we improve the fidelity of the vanilla simulator using actuator identification and residual dynamics learning?• Can we improve the performance on hardware by reducing the sim-to-real gap?</p>
<p>A. Experimental Setup</p>
<p>We conduct all the simulation experiments in PyBullet [59], an open-source physics-based simulator.We use the stable baselines [60] implementation of Proximal Policy Optimization [18] to learn the residual dynamics (Section III-C) and the policy in the improved simulation (Section III-D).We use the BALLU platform [1] for hardware experiments while capturing all the data using a Vicon motion capture system [61].</p>
<p>B. Improved Simulation Fidelity</p>
<p>This section illustrates the process for improving the simulation's fidelity.We first highlight the importance of actuator system identification in Section IV-B.1 and show the learned residual dynamics using our EnvMimic technique in Section IV-B.2.</p>
<p>1) Actuator System Identification: We collect the data and identify the system parameters, such as spring parameters, motor gains, and default joint angles, as described in Section III-B.The identified relationships between the motor commands and joint angles are illustrated in Fig. 3.As shown, the identified relationships exhibit highly nonlinear behaviors compared to the simple idealized curves in simulation, which are essential to model the dynamics of the BALLU robot.</p>
<p>We highlight the importance of system identification by comparing trajectories in simulation.We run the same action Fig. 4: Illustration of System Identification Results.We execute the same action trajectory and compare the final state of identified dynamics (blue) to that of the naive simulation (red), which is significantly different.2) EnvMimic: Residual Dynamics Learning: Next, we examine the results of residual dynamics learning using the proposed EnvMimic technique in Section III-C.We hypothesize that EnvMimic can learn compelling residual dynamics from a few trajectories, unlike data-hungry supervised learning approaches.We compare the x-y-yaw trajectories in four different environments: (1) a vanilla simulation, (2) a simulation with the residual dynamics learned with supervised learning, (3) a simulation with the residual dynamics learned with EnvMimic (ours), and (4) the ground-truth trajectory on hardware.For supervised learning, we use a neural work with two hidden layers of size [64,64].For all the trajectories, we use the same action sequences that are generated by the initial policy in a vanilla simulation.Also, please note that the testing ground-truth trajectory is unseen during training.</p>
<p>The trajectories are compared in Fig. 5. Clearly, our   EnvMimic offers much-improved tracking performance compared to the vanilla simulation that fails to capture the noticeable yaw orientation changes due to the stochasticity of the hardware experiments.In our experience, the trajectory generated with supervised residual dynamics learning tends to turn less and remains on the positive Y side.We hypothesize that the robustness of our RL-based residual dynamics might be obtained due to the mix-use of realworld and simulation trajectories.On the other hand, the supervised learning baseline is trained on the pre-collected hardware trajectories without any data augmentation.We believe the comparison of SL or RL-based approaches on a wider range of scenarios will be an interesting future research direction.Please refer to the supplemental video for qualitative comparisons, highlighting the obvious benefit of our EnvMimic-based residual dynamics learning.</p>
<p>C. Improved Sim-to-real Transfer</p>
<p>To complete the story, we investigate whether we can improve the sim-to-real transfer of policies using augmented simulation.We first train (1) a policy in the improved simulation with the system identification, learned residual dynamics, and domain randomization [28] (ours) and compare the performance with the selected baseline policies learned in the following settings: (2) a simulation only with system identification (Vanilla + sys ID), (3) a simulation only with domain randomization (Vanilla + DR), and (4) a simulation with both system identification and domain randomization (Vanilla + sys ID + DR).For domain randomization, we randomly sample parameters for friction and initial states.We decided not to randomize masses or the buoyancy coefficient due to the sensitivity of the policy to these parameters.We evaluate these simulation-learned policies on the hardware and measure their performance.</p>
<p>For the forward walking task, the learned policy with our augmented simulation is the only one which can walk forward while the other baselines turn left significantly.Both policies are also trained with domain randomization and actuator system identification.Both policies are able to achieve a similar change in yaw angle (∆α) over the entire episode, but the baseline (top) takes a single step and only turns in place, rotating over the battery cover.We also observe that our method (bottom) covers more than twice the distance in the desired y-direction.</p>
<p>Therefore, its traveled distance in the forward (x) direction is 1.12 m, which is significantly larger than 0.27 m, 0.32 m, and 0.56 m of the others.For the turning task, our approach trains the effective policy that travels the most distance in the left (y) direction, 0.29 m, which is our objective function.On the other hand, the other policies cover a shorter distance: 0.20 m, −0.04 m, and 0.11m.We note that the change in yaw angle (∆α) is slightly larger in the case of the baseline with system identification and domain randomization compared to our method.This is an artifact that arises from the policy taking a single step and rotating on the battery cover.This is evident from the y-distance traveled and can be observed clearly in the qualitative results in Figure 7 and the supplemental video.For both tasks, our augmented simulation also exhibits the least sim-to-real errors, which are defined as the average center of mass (CoM) error and final yaw angle (α) error between simulation and hardware.The performance is summarized in Table I and Table II.Please refer to the supplemental video and Fig. 6 for qualitative comparison.</p>
<p>V. CONCLUSION AND DISCUSSION</p>
<p>We present a learning-based method for the sim-to-real transfer of locomotion policies for the Buoyancy Assisted Lightweight Legged Unit (BALLU) robot, which has unique and sensitive dynamics.To mitigate a large sim-to-real gap, we first identify nonlinear relationships between motor commands and joint angles.Then we develop a novel residual dynamics learning framework, EnvMimic, which trains an external perturbation policy via deep reinforcement learning.Once we improve the simulation accuracy with the identified actuator parameters and the learned residual physics, we retrain a policy for better sim-to-real transfer.We demonstrate that using our methodology, we can train walking and turning policies that are successful on the hardware of the BALLU robot.</p>
<p>There exist several interesting future research directions we plan to investigate in the near future.In this work, we develop our residual dynamics model for each individual task, such as walking or turning, which limits generalization over other tasks.Therefore, it will be interesting if we collect a large dataset and train a general residual dynamics model using the proposed method.It will be possible to take some inspiration from the state-of-the-art motion imitation frameworks, which can track a large number of trajectories using a single policy [14].In addition, we also want to investigate various policy formulations.This paper assumes simple external forces to the center of the balloons to model aerodynamics, and it was sufficient for the locomotion tasks we tested on.However, we may need multiple forces or torques to model some sophisticated phenomena.Furthermore, the dynamics of the BALLU robot are also sensitive to time owing to the deflation of balloons.In the future, we want to introduce the concept of lifelong learning to model those gradual temporal changes.</p>
<p>Finally, we plan to evaluate the proposed residual dynamics learning approach, EnvMimic, on different tasks and robotic platforms.While showing promising results, many experiments are limited to the selected walking and turning tasks and the specific hardware of BALLU.However, we believe the algorithm itself is agnostic to the problem formulation, and it has great potential to improve the sim-to-real transferability in general scenarios, even including drones and rigid robots.We intend to explore this topic further in future research.</p>
<p>Fig. 2 :
2
Fig. 2: Illustration of our research platform, BALLU (Buoyancy Assisted Lightweight Legged Unit) with two passive hip joints and two active knee joints.</p>
<p>Algorithm 1
1
System Identification of Cable-driven Actuation 1: Input: the initial parameters p 0 2: Input: a set of pre-defined actions A 3: Measure joint angles on hardware for all actions A 4: Fit polynomial curves C 1 , C 2 , C 3 , and C 4 5: p ← p 0 6: while not converged do 7: Update the simulation with p 8: Measure joint angles for all actions A 9: Fit polynomial curves C 1 , C 2 , C 3 , and C 4 10: ← directed Hausdorff distance between C i and C i 11:</p>
<p>Fig. 3 :
3
Fig. 3: Identified non-linear, asymmetric relationships of cable-driven mechanisms.</p>
<p>Fig. 5 :
5
Fig. 5: Comparison of simulation trajectories to ground truth hardware data for forward walking.Our method, EnvMimic, shows the best tracking performance, particularly in terms of the yaw angle.Note that the ground truth trajectory shown is out-of-distribution.</p>
<p>Fig. 6 :Fig. 7 :
67
Fig. 6: Comparison of the learned forward walking policies without (top) and with (bottom, ours) the proposed residual dynamics learning.Both policies are also trained with domain randomization and actuator system identification.Please note that the baseline (top) shows a significant turning, while ours (bottom) can walk double the distance of the baseline.</p>
<p>TABLE I :
I
Sim-to-real Comparison for Forward Walking.
Experimentα sim T− α hw Ty-distance traveled (m)∆α (hardware)Vanilla + sys ID16.41 •0.2016.72 •Vanilla + DR38.18 •−0.04−4.28 •Vanilla + sys ID + DR−8.50 •0.1139.77 •EnvMimic (Ours)3.50 •0.2936.42 •</p>
<p>TABLE II :
II
Sim-to-real Comparison for Turning Left.</p>
<p>ACKNOWLEDGEMENTThis work is supported by the National Science Foundation under Award #2024768.
Ballu2: A safe and affordable buoyancy assisted biped. H Chae, M S Ahn, D Noh, H Nam, D Hong, Frontiers in Robotics and AI. 2902021</p>
<p>Control of a pneumatically actuated, fully inflatable, fabric-based, humanoid robot. C M Best, J P Wilson, M D Killpack, 2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids). 2015</p>
<p>Design, kinematics, and control of a multijoint soft inflatable arm for humansafe interaction. R Qi, A Khajepour, W W Melek, T L Lam, Y Xu, IEEE Transactions on Robotics. 3332017</p>
<p>Introduction and preliminary investigation of buoyancy assisted robots that are cheap, safe, and will never fall down. M D Williams, D Hong, International Design Engineering Technical Conferences and Computers and Information in Engineering Conference. American Society of Mechanical Engineers202185451</p>
<p>A novel model predictive control framework using dynamic model decomposition applied to dynamic legged locomotion. J Shen, D Hong, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Noise and the reality gap: The use of simulation in evolutionary robotics. N Jakobi, P Husbands, I Harvey, European Conference on Artificial Life. Springer1995</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, </p>
<p>Coppeliasim (formerly v-rep): a versatile and scalable robot simulation framework. E Rohmer, S P N Singh, M Freese, Proc. of The International Conference on Intelligent Robots and Systems (IROS). of The International Conference on Intelligent Robots and Systems (IROS)2013</p>
<p>Augmenting physical simulators with stochastic neural networks: Case study of planar pushing and bouncing. A Ajay, J Wu, N Fazeli, M Bauza, L P Kaelbling, J B Tenenbaum, A Rodriguez, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2018</p>
<p>Sim-toreal transfer with neural-augmented robot simulation. F Golemo, A A Taiga, A Courville, P.-Y Oudeyer, Conference on Robot Learning. PMLR2018</p>
<p>Neurobem: Hybrid aerodynamic quadrotor model. L Bauersfeld, E Kaufmann, P Foehn, S Sun, D Scaramuzza, arXiv:2106.080152021arXiv preprint</p>
<p>Neural-fly enables rapid learning for agile flight in strong winds. M O'connell, G Shi, X Shi, K Azizzadenesheli, A Anandkumar, Y Yue, S.-J Chung, Science Robotics. 76665972022</p>
<p>Deepmimic: Example-guided deep reinforcement learning of physics-based character skills. X B Peng, P Abbeel, S Levine, M Van De Panne, ACM Transactions On Graphics (TOG). 3742018</p>
<p>Amp: Adversarial motion priors for stylized physics-based character control. X B Peng, Z Ma, P Abbeel, S Levine, A Kanazawa, ACM Transactions on Graphics (TOG). 4042021</p>
<p>Ase: Largescale reusable adversarial skill embeddings for physically simulated characters. X B Peng, Y Guo, L Halper, S Levine, S Fidler, ACM Transactions On Graphics (TOG). 4142022</p>
<p>Adversarial motion priors make good substitutes for complex reward functions. A Escontrela, X B Peng, W Yu, T Zhang, A Iscen, K Goldberg, P Abbeel, 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2022</p>
<p>Asynchronous methods for deep reinforcement learning. V Mnih, A P Badia, M Mirza, A Graves, T Lillicrap, T Harley, D Silver, K Kavukcuoglu, Proceedings of The 33rd International Conference on Machine Learning. M F Balcan, K Q Weinberger, The 33rd International Conference on Machine LearningNew York, New York, USAPMLRJun 201648of Proceedings of Machine Learning Research</p>
<p>J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, arXiv:1707.06347Proximal policy optimization algorithms. 2017arXiv preprint</p>
<p>Learning to walk via deep reinforcement learning. T Haarnoja, S Ha, A Zhou, J Tan, G Tucker, S Levine, arXiv:1812.111032018arXiv preprint</p>
<p>Autonomous navigation of stratospheric balloons using reinforcement learning. M G Bellemare, S Candido, P S Castro, J Gong, M C Machado, S Moitra, S S Ponda, Z Wang, Nature. 58878362020</p>
<p>Learning high-speed flight in the wild. A Loquercio, E Kaufmann, R Ranftl, M Müller, V Koltun, D Scaramuzza, Science Robotics. 65958102021</p>
<p>Learning robust perceptive locomotion for quadrupedal robots in the wild. T Miki, J Lee, J Hwangbo, L Wellhausen, V Koltun, M Hutter, Science Robotics. 76228222022</p>
<p>Blind bipedal stair traversal via sim-to-real reinforcement learning. J Siekmann, K Green, J Warila, A Fern, J Hurst, arXiv:2105.083282021arXiv preprint</p>
<p>Sim-toreal transfer of robotic control with dynamics randomization. X B Peng, M Andrychowicz, W Zaremba, P Abbeel, 2018 IEEE international conference on robotics and automation (ICRA). IEEE2018</p>
<p>Tossingbot: Learning to throw arbitrary objects with residual physics. A Zeng, S Song, J Lee, A Rodriguez, T Funkhouser, IEEE Transactions on Robotics. 3642020</p>
<p>Sim-to-real transfer in deep reinforcement learning for robotics: a survey. W Zhao, J P Queralta, T Westerlund, 2020 IEEE Symposium Series on Computational Intelligence (SSCI). IEEE2020</p>
<p>Crossing the reality gap: a survey on sim-to-real transferability of robot controllers in reinforcement learning. E Salvato, G Fenu, E Medvet, F A Pellegrino, IEEE Access. 2021</p>
<p>Domain randomization for transferring deep neural networks from simulation to the real world. J Tobin, R Fong, A Ray, J Schneider, W Zaremba, P Abbeel, 2017 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE2017</p>
<p>Learning memory-based control for human-scale bipedal locomotion. J Siekmann, S Valluri, J Dao, L Bermillo, H Duan, A Fern, J Hurst, arXiv:2006.024022020arXiv preprint</p>
<p>Sim-to-real learning of all common bipedal gaits via periodic reward composition. J Siekmann, Y Godse, A Fern, J Hurst, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Reinforcement learning for robust parameterized locomotion control of bipedal robots. Z Li, X Cheng, X B Peng, P Abbeel, S Levine, G Berseth, K Sreenath, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>A benchmark comparison of learned control policies for agile quadrotor flight. E Kaufmann, L Bauersfeld, D Scaramuzza, 2022 International Conference on Robotics and Automation (ICRA). IEEE2022</p>
<p>Learning to walk in the real world with minimal human effort. S Ha, P Xu, Z Tan, S Levine, J Tan, arXiv:2002.085502020arXiv preprint</p>
<p>Parameter identification of robot dynamics. P K Khosla, T Kanade, 1985 24th IEEE conference on decision and control. IEEE1985</p>
<p>On the identification of the inertial parameters of robots. M Gautier, W Khalil, Proceedings of the 27th IEEE Conference on Decision and Control. the 27th IEEE Conference on Decision and ControlPiscataway, NJIEEE19883</p>
<p>Simulation-based design of dynamic controllers for humanoid balancing. J Tan, Z Xie, B Boots, C K Liu, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2016</p>
<p>Fast model identification via physics engines for data-efficient policy search. S Zhu, A Kimmel, K E Bekris, A Boularias, arXiv:1710.088932017arXiv preprint</p>
<p>Closing the sim-to-real loop: Adapting simulation randomization with real world experience. Y Chebotar, A Handa, V Makoviychuk, M Macklin, J Issac, N Ratliff, D Fox, 2019 International Conference on Robotics and Automation (ICRA). IEEE2019</p>
<p>Preparing for the unknown: Learning a universal policy with online system identification. W Yu, J Tan, C K Liu, G Turk, arXiv:1702.024532017arXiv preprint</p>
<p>Tunenet: Oneshot residual tuning for system identification and sim-to-real robot task transfer. A Allevato, E S Short, M Pryor, A Thomaz, Conference on Robot Learning. PMLR2020</p>
<p>A zero-shot adaptive quadcopter controller. D Zhang, A Loquercio, X Wu, A Kumar, J Malik, M W Mueller, arXiv:2209.092322022arXiv preprint</p>
<p>Iterative residual tuning for system identification and sim-to-real robot learning. A D Allevato, E Schaertl, M Short, A L Pryor, Thomaz, Autonomous Robots. 4472020</p>
<p>Autotuned sim-to-real transfer. Y Du, O Watkins, T Darrell, P Abbeel, D Pathak, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Learning agile and dynamic motor skills for legged robots. J Hwangbo, J Lee, A Dosovitskiy, D Bellicoso, V Tsounis, V Koltun, M Hutter, Science Robotics. 42658722019</p>
<p>Sim-to-real transfer for biped locomotion. W Yu, V C Kumar, G Turk, C K Liu, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE2019</p>
<p>Simgan: Hybrid simulator identification for domain adaptation via adversarial reinforcement learning. Y Jiang, T Zhang, D Ho, Y Bai, C K Liu, S Levine, J Tan, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>Sim-to-real: Learning agile locomotion for quadruped robots. J Tan, T Zhang, E Coumans, A Iscen, Y Bai, D Hafner, S Bohez, V Vanhoucke, arXiv:1804.103322018arXiv preprint</p>
<p>Deepwalk: Omnidirectional bipedal gait by deep reinforcement learning. D Rodriguez, S Behnke, 2021 IEEE International Conference on Robotics and Automation (ICRA). IEEE2021</p>
<p>A novel flying robot system driven by dielectric elastomer balloon actuators. H Zhang, Y Zhou, M Dai, Z Zhang, Journal of Intelligent Material Systems and Structures. 2018. 2018</p>
<p>Innovative service robot systems for facade cleaning of difficult-to-access areas. N Elkmann, T Felsch, M Sack, J Saenz, J Hortig, IEEE/RSJ International Conference on Intelligent Robots and Systems. 20021</p>
<p>Balloonbased concept vehicle for extreme terrain mobility. H D Nayar, M T Pauken, M L Cable, M A Hans, 2019 IEEE Aerospace Conference. IEEE2019</p>
<p>Attitude stability of a cable driven balloon robot. F Takemura, K Maeda, S Tadokoro, 2006 IEEE/RSJ International Conference on Intelligent Robots and Systems. 2006</p>
<p>Gerwalk: Lightweight mobile robot with buoyant balloon body and bamboo rimless wheel. Y Yamada, T Nakamura, 2018 IEEE International Conference on Robotics and Biomimetics (ROBIO). IEEE2018</p>
<p>Development of giacometti arm with balloon body. M Takeichi, K Suzumori, G Endo, H Nabae, IEEE Robotics and Automation Letters. 222017</p>
<p>Diri -the actuated helium balloon: A study of autonomous behaviour in interfaces. D Nowacka, N Y Hammerla, C Elsden, T Plötz, D Kirk, Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp '15. the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing, UbiComp '15New York, NY, USAAssociation for Computing Machinery2015</p>
<p>Design and system identification of a novel hybrid-lift uav. M Graham, S Bhandari, 012022</p>
<p>Full-state modeling and nonlinear control of balloon supported unmanned aerial vehicle. N Mazhar, F Mumtaz, R Azim, A Raza, R Khan, Q Khan, Assembly Automation. 112021print</p>
<p>Control system using timestate control form for balloon robot. M U Hiroki Miyamae, Katsuki Hori, IEEJ Transactions. 2021. 2021</p>
<p>Pybullet, a python module for physics simulation for games, robotics and machine learning. E Coumans, Y Bai, 2016</p>
<p>Stable-baselines3: Reliable reinforcement learning implementations. A Raffin, A Hill, A Gleave, A Kanervisto, M Ernestus, N Dormann, The Journal of Machine Learning Research. 2212021</p>
<p>Vero optimal motion capture. Vicon, </p>            </div>
        </div>

    </div>
</body>
</html>
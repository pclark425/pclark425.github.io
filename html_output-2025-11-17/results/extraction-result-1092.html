<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1092 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1092</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1092</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-24.html">extraction-schema-24</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <p><strong>Paper ID:</strong> paper-fdb8ccebef9f544f44cd9b88085d8ec5458b38ab</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/fdb8ccebef9f544f44cd9b88085d8ec5458b38ab" target="_blank">Replay-Guided Adversarial Environment Design</a></p>
                <p><strong>Paper Venue:</strong> Neural Information Processing Systems</p>
                <p><strong>Paper TL;DR:</strong> It is argued that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training, and theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), it can improve the convergence to Nash equilibria.</p>
                <p><strong>Paper Abstract:</strong> Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising self-supervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, PLR$^{\perp}$, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that PLR$^{\perp}$ improves the performance of PAIRED, from which it inherited its theoretical framework.</p>
                <p><strong>Cost:</strong> 0.019</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1092.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1092.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLR^⊥ (maze)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Level Replay (robust variant, PLR⊥) in MiniGrid mazes</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A replay-curation–based RL training procedure (PLR modified to update policy only on replayed levels) that prioritizes high-regret levels discovered by a (possibly random) generator; applied to partially-observable procedural mazes to improve zero-shot generalization and robustness.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Student policy (protagonist/primary agent) trained with PLR^⊥</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A recurrent policy (LSTM-based) trained with deep reinforcement learning (actor-critic family; PPO used in maze experiments as described in paper) that only takes gradient updates from trajectories collected on PLR replayed levels (trajectories from newly sampled generator levels are collected but stop-gradient is applied).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual, embodied in grid-world / partially-observable maze)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MiniGrid partially-observable navigation mazes (procedural)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Procedurally generated grid mazes where a teacher places up to a block budget of obstructing blocks, then places agent and goal; partial observability handled with LSTM policy. Complexity controlled via block budget (25 or 50), and teacher-generation dynamics produce evolving maze structure.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Number of obstacle blocks per level (block budget: 25 or 50), shortest path length to goal (unsolvable levels assigned 0), solved-path length, and action-sequence complexity measured by LZW of action traces.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high for 25-block budget (explicitly varied: 25 = challenging; 50 = higher complexity where DR suffices)</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generation of levels (many distinct level instances), block budget parameter (25 vs 50), replay sampling probability p (controls fraction of generator vs replay episodes), and size/contents of PLR buffer (curated diverse levels).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (many procedurally-generated instances within the UPOMDP; variation moderated by block budget and generator behavior)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Zero-shot transfer solved-rate / success rate on held-out test mazes (also episodic return used during training), aggregated across multiple test mazes</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>Mean zero-shot solve rate across the test suite: 0.6 ± 0.1 (after 250M training steps) for PLR^⊥ (Table 2 mean); per-maze values reported in paper (e.g., Labyrinth 0.5 ±0.1, LargeCorridor 0.8 ±0.1, etc.).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Explicitly discussed: (a) Increasing block budget increases baseline environment complexity (50-block budget yields high structural complexity so Domain Randomization can suffice), (b) when generator complexity is limited (25-block budget) adaptive curation (PLR^⊥) that prioritizes high-regret levels compensates for limited generator expressivity and yields better generalization. The paper reports that random search + curation can discover levels of increasing complexity and that PLR^⊥ fosters emergent complexity while improving generalization.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Dual Curriculum Design (fast curator PLR^⊥ + generator which can be random or learned); curriculum learning via prioritized replay of previously seen levels; replay probability p controls mixing of generator vs replay episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PLR^⊥ produced significantly better zero-shot generalization to held-out / human-designed mazes than PLR, PAIRED, and Domain Randomization in the 25-block-budget setting (statistically significant improvements aggregated across test mazes, p < 0.05). PLR^⊥ policies learned robust traversal strategies (e.g., approximate right-hand rule) and higher solve rates on OOD mazes.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Evaluated after 250M environment steps/training steps; PLR variants reached strong performance with fewer gradient updates compared to PAIRED which required orders of magnitude more steps in prior work (PAIRED reported at 3B steps in citation).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In partially-observable maze domains with limited generator complexity (25-block budget), curating high-regret randomly discovered levels and training only on replayed levels (PLR^⊥) improves zero-shot transfer and accelerates emergence of complex levels; training on fewer trajectories (no policy updates from random-generator episodes) can improve convergence to minimax-regret–type robust policies.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replay-Guided Adversarial Environment Design', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1092.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1092.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLR^⊥ (CarRacing)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Level Replay (robust variant, PLR⊥) in CarRacing (Bezier track) domain</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>PLR^⊥ applied to a continuous-control, pixel-observation CarRacing domain where closed-loop tracks are procedurally generated as Bézier curves; PLR^⊥ prioritizes high-regret tracks discovered by the generator and updates policy only from replayed (curated) tracks.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Continuous-control driving policy trained with PLR^⊥</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep reinforcement learning continuous-control policy (actor-critic style) trained from partial pixel observations and continuous 3D actions; training uses prioritized replay of high-regret tracks, with stop-gradient on newly generated tracks and updates only from replay samples.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual car in pixel-based continuous-control simulator)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Extended OpenAI Gym CarRacing (Bezier track parameterization) and F1 benchmark (human-designed tracks as OOD test set)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Closed-loop racetracks parameterized as Bézier curves with up to 12 control points under curvature constraints; dense reward continuous-control driving task (full-lap objective). OOD F1 tracks (human-designed) are used for zero-shot transfer evaluation and cannot be perfectly represented within the 12-control-point parameterization.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Track geometric complexity characterized by curvature and presence of challenging features (hairpin turns, high local curvature). Generator dimensionality: up to 12 control points defines space of generated tracks; track difficulty inferred from empirical return distributions and minimum returns across attempts.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>medium-to-high within generated space; F1 human tracks are out-of-distribution and qualitatively higher complexity (cannot be represented with only 12 control points).</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Number and arrangement of Bézier control points (up to 12), generator sampling distribution (random vs learned), replay probability p, and size/diversity of PLR buffer; evaluation set of 20 F1 tracks provides OOD variation.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (wide procedural variability within 12-control-point family; test set introduces stronger OOD variation)</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Zero-shot transfer episodic return (mean return per lap), and minimum returns (robustness) across attempts; statistical significance tests reported.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>PLR^⊥ achieved statistically significant higher mean zero-shot returns across the full F1 benchmark than PLR and PAIRED (p < 0.001); evaluated after 5M training steps (equivalent to ~40M environment interactions with action repeat). Specific per-track means are reported in Appendix (paper) and overall mean improvement over baselines is significant. PLR^⊥ also achieved higher minimum returns on many tracks versus other methods.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Discussed: dense rewards simplify transfer and random Bézier tracks occasionally contain F1-like challenges, so Domain Randomization performs moderately well; however, selective sampling for high regret (PLR^⊥) yields better zero-shot transfer to OOD F1 tracks, showing that curated variation (targeting difficult configurations) can compensate for generator limitations and improve generalization to more complex, out-of-distribution tracks. Also: PAIRED tended to reduce track complexity by exploiting antagonist strengths, while PLR^⊥ selected more challenging tracks resembling human-designed tracks.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Dual Curriculum Design with PLR^⊥ as curator and random (or learned) Bézier generator; curriculum by prioritized replay (replay prob. ~0.5 reported), training on replayed episodes only.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PLR^⊥ outperformed PLR and PAIRED on zero-shot transfer to a 20-track F1 benchmark (OOD), with statistically significant gains (p < 0.001). PLR^⊥ exceeded the reported performance of a state-of-the-art AttentionAgent baseline while using <0.25% of that baseline's environment steps.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Trained for 5M training steps (≈40M interactions with action repeat) for evaluation; significantly fewer steps than other reported baselines (e.g., AttentionAgent trained with ≈8.2B steps).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>In dense-reward continuous-control domains, curated replay of high-regret procedurally generated tracks (PLR^⊥) yields superior zero-shot transfer to human-designed OOD tracks compared to uniform sampling (DR) or adversarial generation (PAIRED); PLR^⊥ achieves greater robustness (higher minimum returns) and is sample-efficient despite training on fewer gradient updates (training only on replayed levels).</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replay-Guided Adversarial Environment Design', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1092.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1092.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PAIRED (protagonist)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Protagonist policy trained in PAIRED (Protagonist-Antagonist Induced Regret Environment Design)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An RL student agent (protagonist) trained adversarially in a three-player game where a teacher generates levels to maximize regret between protagonist and antagonist; designed to produce minimax-regret policies when the generator and students converge to Nash equilibria.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Emergent complexity and zero-shot transfer via unsupervised environment design</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>PAIRED protagonist (and jointly protagonist/antagonist student pair)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Two student agents (protagonist and antagonist) trained with RL to maximize their own returns; the generator (teacher) is trained to maximize regret (difference in returns) to challenge the protagonist relative to the antagonist.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agents (virtual agents in procedurally generated environments: mazes, tracks)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Procedurally generated UPOMDPs (e.g., MiniGrid mazes, CarRacing Bézier tracks in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Levels are generated by a learned adversarial teacher that adapts via gradient updates to exploit student weaknesses; complexity can evolve as teacher learns to produce harder levels.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Same metrics as used for each domain (maze block count, shortest path length, action complexity; for CarRacing: track geometry/curvature). Complexity evolves slowly because the teacher adapts via gradient updates.</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Varies; in experiments PAIRED tended to slowly increase complexity over long training; with high block budgets (50) DR and other methods reached high complexity faster.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Generator's learned distribution over environment parameters (θ); number of distinct generated levels depends on generator capacity and training dynamics.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high in principle (generator can create diverse levels), but empirically can degenerate (e.g., PAIRED sometimes overexploits antagonist leading to lower complexity tracks in CarRacing).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Zero-shot transfer solved-rate / episodic return on held-out test environments; also training returns of protagonist/antagonist and generator.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>In maze experiments PAIRED underperformed replay-based methods in the 25-block budget setting (mean across mazes ~0.2; Table 2). In CarRacing PAIRED struggled, often discovering curricula that degraded protagonist performance (protagonist returns could be worse than baselines).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>Paper reports that PAIRED's learned generator adapts slowly (gradient updates) and can lead to an emergent curriculum of increasing complexity, but that PAIRED can also exploit relative student strengths causing reductions in effective complexity (e.g., in CarRacing). PAIRED's variation is generator-driven, while PLR-type curators can more quickly switch among discovered levels.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Adversarial generator (teacher) + two student agents (protagonist/antagonist) trained jointly; curriculum emerges from generator learning to maximize regret.</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PAIRED finds minimax-regret policies in theory at NE, but empirically was slower to adapt and in the studied domains often led to worse zero-shot transfer than replay-curation methods (PLR, PLR^⊥), especially when generator adaptation is slow or when the generator overexploits antagonist strengths.</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirically less sample-efficient in the studied domains; referenced prior work reported PAIRED required ~3B steps to reach some levels of performance (compared to 250M used here for PLR variants).</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PAIRED provides a theoretical minimax-regret guarantee at NE but can be slow to adapt and can produce degenerate curricula (favoring antagonist) in some domains; combining PAIRED with replay-based curation (REPAIRED) mitigates some issues.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replay-Guided Adversarial Environment Design', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1092.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1092.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of embodied learning systems or agents operating in environments with varying levels of complexity and variation, including performance metrics, trade-offs, and relationships between environment complexity and environment variation.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>PLR (baseline)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Prioritized Level Replay (PLR)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A fast-adapting level-curation method that maintains a buffer of previously encountered levels and prioritizes replay of levels estimated to have high learning potential (original PLR uses time-averaged L1 value loss as the score).</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Prioritized level replay</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Student policy trained with PLR</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>A deep RL policy that interleaves training on newly sampled levels from a generator and prioritized replay of buffered levels scored by learning-potential heuristics (originally L1 value-loss).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_type</strong></td>
                            <td>simulated agent (virtual agents in procedural UPOMDPs)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>MiniGrid mazes and CarRacing Bézier tracks (as in experiments)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same UPOMDP parameterizations as PLR^⊥ experiments; PLR does not alter generator but adaptively selects among seen levels for replay based on score (value-loss or regret surrogates).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_measure</strong></td>
                            <td>Curated buffer yields empirically higher complexity levels (measured by block count and shortest path length in mazes); PLR scores originally based on time-averaged L1 value loss but can be changed to regret-based scores (MaxMC or positive-value-loss).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_level</strong></td>
                            <td>Produces curated levels of higher complexity than raw random sampling (paper reports PLR-curated levels show increasing path lengths over time), but less robust than PLR^⊥ when training uses all trajectories.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_measure</strong></td>
                            <td>Procedural generator variety, replay probability p, buffer size K, scoring function (L1 value loss vs regret estimates), staleness/temperature parameters for sampling.</td>
                        </tr>
                        <tr>
                            <td><strong>variation_level</strong></td>
                            <td>high (procedural generators) but curation determines effective variation presented during policy updates.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metric</strong></td>
                            <td>Zero-shot transfer solved-rate and episodic return in held-out test sets</td>
                        </tr>
                        <tr>
                            <td><strong>performance_value</strong></td>
                            <td>PLR improved sample complexity and generalization relative to naive DR in prior work; in this paper PLR mean maze solve-rate was ~0.5 (Table 2 mean) after 250M steps, lower than PLR^⊥ (0.6). In CarRacing PLR was outperformed by PLR^⊥ (statistically significant).</td>
                        </tr>
                        <tr>
                            <td><strong>complexity_variation_relationship</strong></td>
                            <td>PLR acts as curator to increase the effective complexity experienced by the learner by prioritizing high learning-potential levels discovered by the generator; however, if policy updates are taken from generator-sampled episodes (not only replay), the theoretical robustness guarantee weakens—motivating PLR^⊥ which updates only from replayed episodes.</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>high_complexity_high_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>low_complexity_low_variation_performance</strong></td>
                            <td>null</td>
                        </tr>
                        <tr>
                            <td><strong>training_strategy</strong></td>
                            <td>Curation via prioritized replay (PLR) combined with generator-sampled new levels (domain randomization or learned generator).</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_tested</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>generalization_results</strong></td>
                            <td>PLR improves sample efficiency and generalization compared to uniform sampling (DR) in prior work and in the studied domains; replacing L1 loss with regret-based scoring and training only on replayed levels further improves robustness (PLR^⊥).</td>
                        </tr>
                        <tr>
                            <td><strong>sample_efficiency</strong></td>
                            <td>Empirically more sample-efficient than PAIRED in the studied domains; PLR variants achieved strong performance within 250M steps versus PAIRED's much larger step counts referenced in prior work.</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>PLR is an effective curation mechanism for increasing effective training complexity and improving generalization; theoretical characterization as a Dual Curriculum Design component explains its robustness and motivated changes (regret scoring, training only on replayed episodes) that yield provable minimax-regret guarantees at NE.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Replay-Guided Adversarial Environment Design', 'publication_date_yy_mm': '2021-10'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Prioritized level replay <em>(Rating: 2)</em></li>
                <li>Emergent complexity and zero-shot transfer via unsupervised environment design <em>(Rating: 2)</em></li>
                <li>Paired open-ended trailblazer (POET): endlessly generating increasingly complex and diverse learning environments and their solutions <em>(Rating: 1)</em></li>
                <li>Domain randomization for transferring deep neural networks from simulation to the real world <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1092",
    "paper_id": "paper-fdb8ccebef9f544f44cd9b88085d8ec5458b38ab",
    "extraction_schema_id": "extraction-schema-24",
    "extracted_data": [
        {
            "name_short": "PLR^⊥ (maze)",
            "name_full": "Prioritized Level Replay (robust variant, PLR⊥) in MiniGrid mazes",
            "brief_description": "A replay-curation–based RL training procedure (PLR modified to update policy only on replayed levels) that prioritizes high-regret levels discovered by a (possibly random) generator; applied to partially-observable procedural mazes to improve zero-shot generalization and robustness.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Student policy (protagonist/primary agent) trained with PLR^⊥",
            "agent_description": "A recurrent policy (LSTM-based) trained with deep reinforcement learning (actor-critic family; PPO used in maze experiments as described in paper) that only takes gradient updates from trajectories collected on PLR replayed levels (trajectories from newly sampled generator levels are collected but stop-gradient is applied).",
            "agent_type": "simulated agent (virtual, embodied in grid-world / partially-observable maze)",
            "environment_name": "MiniGrid partially-observable navigation mazes (procedural)",
            "environment_description": "Procedurally generated grid mazes where a teacher places up to a block budget of obstructing blocks, then places agent and goal; partial observability handled with LSTM policy. Complexity controlled via block budget (25 or 50), and teacher-generation dynamics produce evolving maze structure.",
            "complexity_measure": "Number of obstacle blocks per level (block budget: 25 or 50), shortest path length to goal (unsolvable levels assigned 0), solved-path length, and action-sequence complexity measured by LZW of action traces.",
            "complexity_level": "medium-to-high for 25-block budget (explicitly varied: 25 = challenging; 50 = higher complexity where DR suffices)",
            "variation_measure": "Procedural generation of levels (many distinct level instances), block budget parameter (25 vs 50), replay sampling probability p (controls fraction of generator vs replay episodes), and size/contents of PLR buffer (curated diverse levels).",
            "variation_level": "high (many procedurally-generated instances within the UPOMDP; variation moderated by block budget and generator behavior)",
            "performance_metric": "Zero-shot transfer solved-rate / success rate on held-out test mazes (also episodic return used during training), aggregated across multiple test mazes",
            "performance_value": "Mean zero-shot solve rate across the test suite: 0.6 ± 0.1 (after 250M training steps) for PLR^⊥ (Table 2 mean); per-maze values reported in paper (e.g., Labyrinth 0.5 ±0.1, LargeCorridor 0.8 ±0.1, etc.).",
            "complexity_variation_relationship": "Explicitly discussed: (a) Increasing block budget increases baseline environment complexity (50-block budget yields high structural complexity so Domain Randomization can suffice), (b) when generator complexity is limited (25-block budget) adaptive curation (PLR^⊥) that prioritizes high-regret levels compensates for limited generator expressivity and yields better generalization. The paper reports that random search + curation can discover levels of increasing complexity and that PLR^⊥ fosters emergent complexity while improving generalization.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "null",
            "low_complexity_low_variation_performance": "null",
            "training_strategy": "Dual Curriculum Design (fast curator PLR^⊥ + generator which can be random or learned); curriculum learning via prioritized replay of previously seen levels; replay probability p controls mixing of generator vs replay episodes.",
            "generalization_tested": true,
            "generalization_results": "PLR^⊥ produced significantly better zero-shot generalization to held-out / human-designed mazes than PLR, PAIRED, and Domain Randomization in the 25-block-budget setting (statistically significant improvements aggregated across test mazes, p &lt; 0.05). PLR^⊥ policies learned robust traversal strategies (e.g., approximate right-hand rule) and higher solve rates on OOD mazes.",
            "sample_efficiency": "Evaluated after 250M environment steps/training steps; PLR variants reached strong performance with fewer gradient updates compared to PAIRED which required orders of magnitude more steps in prior work (PAIRED reported at 3B steps in citation).",
            "key_findings": "In partially-observable maze domains with limited generator complexity (25-block budget), curating high-regret randomly discovered levels and training only on replayed levels (PLR^⊥) improves zero-shot transfer and accelerates emergence of complex levels; training on fewer trajectories (no policy updates from random-generator episodes) can improve convergence to minimax-regret–type robust policies.",
            "uuid": "e1092.0",
            "source_info": {
                "paper_title": "Replay-Guided Adversarial Environment Design",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "PLR^⊥ (CarRacing)",
            "name_full": "Prioritized Level Replay (robust variant, PLR⊥) in CarRacing (Bezier track) domain",
            "brief_description": "PLR^⊥ applied to a continuous-control, pixel-observation CarRacing domain where closed-loop tracks are procedurally generated as Bézier curves; PLR^⊥ prioritizes high-regret tracks discovered by the generator and updates policy only from replayed (curated) tracks.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Continuous-control driving policy trained with PLR^⊥",
            "agent_description": "A deep reinforcement learning continuous-control policy (actor-critic style) trained from partial pixel observations and continuous 3D actions; training uses prioritized replay of high-regret tracks, with stop-gradient on newly generated tracks and updates only from replay samples.",
            "agent_type": "simulated agent (virtual car in pixel-based continuous-control simulator)",
            "environment_name": "Extended OpenAI Gym CarRacing (Bezier track parameterization) and F1 benchmark (human-designed tracks as OOD test set)",
            "environment_description": "Closed-loop racetracks parameterized as Bézier curves with up to 12 control points under curvature constraints; dense reward continuous-control driving task (full-lap objective). OOD F1 tracks (human-designed) are used for zero-shot transfer evaluation and cannot be perfectly represented within the 12-control-point parameterization.",
            "complexity_measure": "Track geometric complexity characterized by curvature and presence of challenging features (hairpin turns, high local curvature). Generator dimensionality: up to 12 control points defines space of generated tracks; track difficulty inferred from empirical return distributions and minimum returns across attempts.",
            "complexity_level": "medium-to-high within generated space; F1 human tracks are out-of-distribution and qualitatively higher complexity (cannot be represented with only 12 control points).",
            "variation_measure": "Number and arrangement of Bézier control points (up to 12), generator sampling distribution (random vs learned), replay probability p, and size/diversity of PLR buffer; evaluation set of 20 F1 tracks provides OOD variation.",
            "variation_level": "high (wide procedural variability within 12-control-point family; test set introduces stronger OOD variation)",
            "performance_metric": "Zero-shot transfer episodic return (mean return per lap), and minimum returns (robustness) across attempts; statistical significance tests reported.",
            "performance_value": "PLR^⊥ achieved statistically significant higher mean zero-shot returns across the full F1 benchmark than PLR and PAIRED (p &lt; 0.001); evaluated after 5M training steps (equivalent to ~40M environment interactions with action repeat). Specific per-track means are reported in Appendix (paper) and overall mean improvement over baselines is significant. PLR^⊥ also achieved higher minimum returns on many tracks versus other methods.",
            "complexity_variation_relationship": "Discussed: dense rewards simplify transfer and random Bézier tracks occasionally contain F1-like challenges, so Domain Randomization performs moderately well; however, selective sampling for high regret (PLR^⊥) yields better zero-shot transfer to OOD F1 tracks, showing that curated variation (targeting difficult configurations) can compensate for generator limitations and improve generalization to more complex, out-of-distribution tracks. Also: PAIRED tended to reduce track complexity by exploiting antagonist strengths, while PLR^⊥ selected more challenging tracks resembling human-designed tracks.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "null",
            "low_complexity_low_variation_performance": "null",
            "training_strategy": "Dual Curriculum Design with PLR^⊥ as curator and random (or learned) Bézier generator; curriculum by prioritized replay (replay prob. ~0.5 reported), training on replayed episodes only.",
            "generalization_tested": true,
            "generalization_results": "PLR^⊥ outperformed PLR and PAIRED on zero-shot transfer to a 20-track F1 benchmark (OOD), with statistically significant gains (p &lt; 0.001). PLR^⊥ exceeded the reported performance of a state-of-the-art AttentionAgent baseline while using &lt;0.25% of that baseline's environment steps.",
            "sample_efficiency": "Trained for 5M training steps (≈40M interactions with action repeat) for evaluation; significantly fewer steps than other reported baselines (e.g., AttentionAgent trained with ≈8.2B steps).",
            "key_findings": "In dense-reward continuous-control domains, curated replay of high-regret procedurally generated tracks (PLR^⊥) yields superior zero-shot transfer to human-designed OOD tracks compared to uniform sampling (DR) or adversarial generation (PAIRED); PLR^⊥ achieves greater robustness (higher minimum returns) and is sample-efficient despite training on fewer gradient updates (training only on replayed levels).",
            "uuid": "e1092.1",
            "source_info": {
                "paper_title": "Replay-Guided Adversarial Environment Design",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "PAIRED (protagonist)",
            "name_full": "Protagonist policy trained in PAIRED (Protagonist-Antagonist Induced Regret Environment Design)",
            "brief_description": "An RL student agent (protagonist) trained adversarially in a three-player game where a teacher generates levels to maximize regret between protagonist and antagonist; designed to produce minimax-regret policies when the generator and students converge to Nash equilibria.",
            "citation_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "mention_or_use": "use",
            "agent_name": "PAIRED protagonist (and jointly protagonist/antagonist student pair)",
            "agent_description": "Two student agents (protagonist and antagonist) trained with RL to maximize their own returns; the generator (teacher) is trained to maximize regret (difference in returns) to challenge the protagonist relative to the antagonist.",
            "agent_type": "simulated agents (virtual agents in procedurally generated environments: mazes, tracks)",
            "environment_name": "Procedurally generated UPOMDPs (e.g., MiniGrid mazes, CarRacing Bézier tracks in experiments)",
            "environment_description": "Levels are generated by a learned adversarial teacher that adapts via gradient updates to exploit student weaknesses; complexity can evolve as teacher learns to produce harder levels.",
            "complexity_measure": "Same metrics as used for each domain (maze block count, shortest path length, action complexity; for CarRacing: track geometry/curvature). Complexity evolves slowly because the teacher adapts via gradient updates.",
            "complexity_level": "Varies; in experiments PAIRED tended to slowly increase complexity over long training; with high block budgets (50) DR and other methods reached high complexity faster.",
            "variation_measure": "Generator's learned distribution over environment parameters (θ); number of distinct generated levels depends on generator capacity and training dynamics.",
            "variation_level": "high in principle (generator can create diverse levels), but empirically can degenerate (e.g., PAIRED sometimes overexploits antagonist leading to lower complexity tracks in CarRacing).",
            "performance_metric": "Zero-shot transfer solved-rate / episodic return on held-out test environments; also training returns of protagonist/antagonist and generator.",
            "performance_value": "In maze experiments PAIRED underperformed replay-based methods in the 25-block budget setting (mean across mazes ~0.2; Table 2). In CarRacing PAIRED struggled, often discovering curricula that degraded protagonist performance (protagonist returns could be worse than baselines).",
            "complexity_variation_relationship": "Paper reports that PAIRED's learned generator adapts slowly (gradient updates) and can lead to an emergent curriculum of increasing complexity, but that PAIRED can also exploit relative student strengths causing reductions in effective complexity (e.g., in CarRacing). PAIRED's variation is generator-driven, while PLR-type curators can more quickly switch among discovered levels.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "null",
            "low_complexity_low_variation_performance": "null",
            "training_strategy": "Adversarial generator (teacher) + two student agents (protagonist/antagonist) trained jointly; curriculum emerges from generator learning to maximize regret.",
            "generalization_tested": true,
            "generalization_results": "PAIRED finds minimax-regret policies in theory at NE, but empirically was slower to adapt and in the studied domains often led to worse zero-shot transfer than replay-curation methods (PLR, PLR^⊥), especially when generator adaptation is slow or when the generator overexploits antagonist strengths.",
            "sample_efficiency": "Empirically less sample-efficient in the studied domains; referenced prior work reported PAIRED required ~3B steps to reach some levels of performance (compared to 250M used here for PLR variants).",
            "key_findings": "PAIRED provides a theoretical minimax-regret guarantee at NE but can be slow to adapt and can produce degenerate curricula (favoring antagonist) in some domains; combining PAIRED with replay-based curation (REPAIRED) mitigates some issues.",
            "uuid": "e1092.2",
            "source_info": {
                "paper_title": "Replay-Guided Adversarial Environment Design",
                "publication_date_yy_mm": "2021-10"
            }
        },
        {
            "name_short": "PLR (baseline)",
            "name_full": "Prioritized Level Replay (PLR)",
            "brief_description": "A fast-adapting level-curation method that maintains a buffer of previously encountered levels and prioritizes replay of levels estimated to have high learning potential (original PLR uses time-averaged L1 value loss as the score).",
            "citation_title": "Prioritized level replay",
            "mention_or_use": "use",
            "agent_name": "Student policy trained with PLR",
            "agent_description": "A deep RL policy that interleaves training on newly sampled levels from a generator and prioritized replay of buffered levels scored by learning-potential heuristics (originally L1 value-loss).",
            "agent_type": "simulated agent (virtual agents in procedural UPOMDPs)",
            "environment_name": "MiniGrid mazes and CarRacing Bézier tracks (as in experiments)",
            "environment_description": "Same UPOMDP parameterizations as PLR^⊥ experiments; PLR does not alter generator but adaptively selects among seen levels for replay based on score (value-loss or regret surrogates).",
            "complexity_measure": "Curated buffer yields empirically higher complexity levels (measured by block count and shortest path length in mazes); PLR scores originally based on time-averaged L1 value loss but can be changed to regret-based scores (MaxMC or positive-value-loss).",
            "complexity_level": "Produces curated levels of higher complexity than raw random sampling (paper reports PLR-curated levels show increasing path lengths over time), but less robust than PLR^⊥ when training uses all trajectories.",
            "variation_measure": "Procedural generator variety, replay probability p, buffer size K, scoring function (L1 value loss vs regret estimates), staleness/temperature parameters for sampling.",
            "variation_level": "high (procedural generators) but curation determines effective variation presented during policy updates.",
            "performance_metric": "Zero-shot transfer solved-rate and episodic return in held-out test sets",
            "performance_value": "PLR improved sample complexity and generalization relative to naive DR in prior work; in this paper PLR mean maze solve-rate was ~0.5 (Table 2 mean) after 250M steps, lower than PLR^⊥ (0.6). In CarRacing PLR was outperformed by PLR^⊥ (statistically significant).",
            "complexity_variation_relationship": "PLR acts as curator to increase the effective complexity experienced by the learner by prioritizing high learning-potential levels discovered by the generator; however, if policy updates are taken from generator-sampled episodes (not only replay), the theoretical robustness guarantee weakens—motivating PLR^⊥ which updates only from replayed episodes.",
            "high_complexity_low_variation_performance": "null",
            "low_complexity_high_variation_performance": "null",
            "high_complexity_high_variation_performance": "null",
            "low_complexity_low_variation_performance": "null",
            "training_strategy": "Curation via prioritized replay (PLR) combined with generator-sampled new levels (domain randomization or learned generator).",
            "generalization_tested": true,
            "generalization_results": "PLR improves sample efficiency and generalization compared to uniform sampling (DR) in prior work and in the studied domains; replacing L1 loss with regret-based scoring and training only on replayed levels further improves robustness (PLR^⊥).",
            "sample_efficiency": "Empirically more sample-efficient than PAIRED in the studied domains; PLR variants achieved strong performance within 250M steps versus PAIRED's much larger step counts referenced in prior work.",
            "key_findings": "PLR is an effective curation mechanism for increasing effective training complexity and improving generalization; theoretical characterization as a Dual Curriculum Design component explains its robustness and motivated changes (regret scoring, training only on replayed episodes) that yield provable minimax-regret guarantees at NE.",
            "uuid": "e1092.3",
            "source_info": {
                "paper_title": "Replay-Guided Adversarial Environment Design",
                "publication_date_yy_mm": "2021-10"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Prioritized level replay",
            "rating": 2
        },
        {
            "paper_title": "Emergent complexity and zero-shot transfer via unsupervised environment design",
            "rating": 2
        },
        {
            "paper_title": "Paired open-ended trailblazer (POET): endlessly generating increasingly complex and diverse learning environments and their solutions",
            "rating": 1
        },
        {
            "paper_title": "Domain randomization for transferring deep neural networks from simulation to the real world",
            "rating": 1
        }
    ],
    "cost": 0.01874675,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Replay-Guided Adversarial Environment Design</h1>
<p>Minqi Jiang*<br>UCL, FAIR<br>Jakob Foerster<br>FAIR</p>
<p>Michael Dennis*<br>UC Berkeley<br>Edward Grefenstette<br>UCL, FAIR</p>
<p>Jack Parker-Holder<br>University of Oxford<br>Tim Rocktäschel<br>UCL, FAIR</p>
<h4>Abstract</h4>
<p>Deep reinforcement learning (RL) agents may successfully generalize to new settings if trained on an appropriately diverse set of environment and task configurations. Unsupervised Environment Design (UED) is a promising selfsupervised RL paradigm, wherein the free parameters of an underspecified environment are automatically adapted during training to the agent's capabilities, leading to the emergence of diverse training environments. Here, we cast Prioritized Level Replay (PLR), an empirically successful but theoretically unmotivated method that selectively samples randomly-generated training levels, as UED. We argue that by curating completely random levels, PLR, too, can generate novel and complex levels for effective training. This insight reveals a natural class of UED methods we call Dual Curriculum Design (DCD). Crucially, DCD includes both PLR and a popular UED algorithm, PAIRED, as special cases and inherits similar theoretical guarantees. This connection allows us to develop novel theory for PLR, providing a version with a robustness guarantee at Nash equilibria. Furthermore, our theory suggests a highly counterintuitive improvement to PLR: by stopping the agent from updating its policy on uncurated levels (training on less data), we can improve the convergence to Nash equilibria. Indeed, our experiments confirm that our new method, $\mathrm{PLR}^{\perp}$, obtains better results on a suite of out-of-distribution, zero-shot transfer tasks, in addition to demonstrating that $\mathrm{PLR}^{\perp}$ improves the performance of PAIRED, from which it inherited its theoretical framework.</p>
<h2>1 Introduction</h2>
<p>While deep reinforcement learning (RL) approaches have led to many successful applications in challenging domains like Atari [21], Go [35], Chess [36], Dota [4], and StarCraft [40] in recent years, deep RL agents still prove to be brittle, often failing to transfer to environments only slightly different from those encountered during training [44, 9]. To ensure learning of robust and well-generalizing policies, agents must train on sufficiently diverse and informative variations of environments (e.g. see Section 3.1 of [8]). However, it is not always feasible to specify an appropriate training distribution or a generator thereof. Agents may therefore benefit greatly from methods that automatically adapt the distribution over environment variations throughout training [10, 17]. Throughout this paper we will call a particular environment instance or configuration (e.g. an arrangement of blocks, race tracks, or generally any of the environment's constituent entities) a level.</p>
<p>Two recent works [10, 17] have sought to empirically demonstrate this need for a more targeted agentadaptive mechanism for selecting levels on which to train RL agents, so to ensure efficient learning and generalization to unseen levels-as well as to provide methods implementing such mechanisms. The first method, Protagonist Antagonist Induced Regret Environment Design (PAIRED) [10],</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup></p>
<p><img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 1: Randomly drawn samples of CarRacing tracks produced by different methods. (a) Domain Randomization (DR) produces tracks of average complexity, with few sharp turns. (b) PAIRED often overexploits the difference in the students, leading to simple tracks that incidentally favor the antagonist. (c) REPAIRED mitigates this degeneracy, recovering track complexity. (d) PLR<sup>⊥</sup> selects the most challenging randomly generated tracks, resulting in tracks that more closely resemble human-designed tracks, such as (e) the Nürburgring Grand Prix.</p>
<p>introduces a self-supervised RL paradigm called Unsupervised Environment Design (UED). Here, an environment generator (a <em>teacher</em>) is co-evolved with a <em>student</em> policy that trains on levels actively proposed by the teacher, leading to a form of adaptive curriculum learning. The aim of this coevolution is for the teacher to gradually learn to generate environments that exemplify properties of those that might be encountered at deployment time, and for the student to simultaneously learn a good policy that enables zero-shot transfer to such environments. PAIRED's specific adversarial approach to environment design ensures a useful robustness characterization of the final student policy in the form of a minimax regret guarantee [31]—assuming that its underlying teacher-student multi-agent system arrives at a Nash equilibrium [NE, 24]. In contrast, the second method, Prioritized Level Replay (PLR) [17], embodies an alternative form of dynamic curriculum learning that does not assume control of level generation, but instead, the ability to selectively replay existing levels. PLR tracks levels previously proposed by a black-box environment generator, and for each, estimates the agent's learning potential in that level, in terms of how useful it would be to gather new experience from that level again in the future. The PLR algorithm exploits these scores to adapt a schedule for revisiting or <em>replaying</em> levels to maximize learning potential. PLR has been shown to produce scalable and robust results, improving both sample complexity of agent training and the generalization of the learned policy in diverse environments. However, unlike PAIRED, PLR is motivated with heuristic arguments and lacks a useful theoretical characterization of its learning behavior.</p>
<p>In this paper, we argue that PLR is, in and of itself, an effective form of UED: Through curating even randomly generated levels, PLR can generate novel and complex levels for learning robust policies. This insight leads to a natural class of UED methods which we call <em>Dual Curriculum Design</em> (DCD). In DCD, a student policy is challenged by a team of two co-evolving teachers. One teacher actively generates new, challenging levels, while the other passively curates existing levels for replaying, by prioritizing those estimated to be most suitably challenging for the student. We show that PAIRED and PLR are distinct members of the DCD class of algorithms and prove in Section 3 that all DCD algorithms enjoy similar minimax regret guarantees to that of PAIRED.</p>
<p>We make use of this result to provide the first theoretical characterization of PLR, which immediately suggests a simple yet highly counterintuitive adjustment to PLR: By only training on trajectories in replay levels, PLR becomes provably robust at NE. We call this resulting variant PLR<sup>⊥</sup> (Section 4). From this perspective, PLR effectively performs level design in a diametrically opposite manner to PAIRED—through prioritized selection rather than active generation. A second corollary to the provable robustness of DCD algorithms shows that PLR<sup>⊥</sup> can be extended to make use of the PAIRED teacher as a level generator while preserving the robustness guarantee of PAIRED, resulting in a method we call <em>Replay-Enhanced PAIRED</em> (REPAIRED) (Section 5). We hypothesize that in this arrangement, PLR<sup>⊥</sup> plays a complementary role to PAIRED in robustifying student policies.</p>
<p>Our experiments in Section 6 investigate the learning dynamics of PLR<sup>⊥</sup>, REPAIRED, and their replay-free counterparts on a challenging maze domain and a novel continuous control UED setting based on the popular CarRacing environment [5]. In both of these highly distinct settings, our methods provide significant improvements over PLR and PAIRED, producing agents that can perform out-of-distribution (OOD) generalization to a variety of human designed mazes and Formula 1 tracks.</p>
<p>In summary, we present the following contributions: (i) We establish a common framework, Dual Curriculum Design, that encompasses PLR and PAIRED. This allows us to develop new theory, which provides the first robustness guarantees for PLR at NE as well as for REPAIRED, which</p>
<p>augments PAIRED with a PLR-based replay mechanism. (ii) Crucially, our theory suggests a highly counterintuitive improvement to PLR: the convergence to NE should be assisted by training on less data when using PLR—namely by only taking gradient updates from data that originates from the PLR buffer, using the samples from the environment distribution only for computing the prioritization of levels in the buffer. (iii) Our experiments in a maze domain and a novel car-racing domain show that our methods significantly outperform their replay-free counterparts in zero-shot generalization. We open source our methods at https://github.com/facebookresearch/dcd.</p>
<h1>2 Background</h1>
<h3>2.1 Unsupervised Environment Design</h3>
<p>Unsupervised Environment Design (UED), as introduced by [10], is the problem of automatically designing a distribution of environments that adapts to the learning agent. UED is defined in terms of an Underspecified POMDP (UPOMDP), given by $\mathcal{M}=\left\langle A, O, \Theta, S^{\mathcal{M}}, \mathcal{T}^{\mathcal{M}}, \mathcal{I}^{\mathcal{M}}, \mathcal{R}^{\mathcal{M}}, \gamma\right\rangle$, where $A$ is a set of actions, $O$ is a set of observations, $S$ is a set of states, $\mathcal{T}: S \times A \times \Theta \rightarrow \boldsymbol{\Delta}(S)$ is a transition function, $\mathcal{I}: S \rightarrow O$ is an observation (or inspection) function, $\mathcal{R}: S \rightarrow \mathbb{R}$ is a reward function, and $\gamma$ is a discount factor. This definition is identical to a POMDP with the addition of $\Theta$ to represent the free-parameters of the environment. These parameters can be distinct at every time step and incorporated into the transition function $\mathcal{T}^{\mathcal{M}}: S \times A \times \Theta \rightarrow \boldsymbol{\Delta}(S)$. For example, $\Theta$ could represent the possible positions of obstacles in a maze. We will refer to the environment resulting from a fixed $\theta \in \Theta$ as $\mathcal{M}<em _theta="\theta">{\theta}$, or with a slight abuse of notation, simply $\theta$ when clear from context. We define the value of $\pi$ in $\mathcal{M}</em>$. Aligning with terminology from [17], we refer to a fully-specified environment as a level.}$ to be $V^{\theta}(\pi)=\mathbb{E}\left[\sum_{t=0}^{T} r_{t} \gamma^{t}\right]$ where $r_{t}$ are the rewards attained by $\pi$ in $\mathcal{M}_{\theta</p>
<h3>2.2 Protagonist Antagonist Induced Regret Environment Design</h3>
<p>Protagonist Antagonist Induced Regret Environment Design [PAIRED, 10] presents a UED approach consisting of simultaneously training agents in a three player game: the protagonist $\pi_{A}$ and the antagonist $\pi_{B}$ are trained in environments generated by the teacher $\hat{\theta}$. The objective of this game is defined by $U\left(\pi_{A}, \pi_{B}, \hat{\theta}\right)=\mathbb{E}<em A="A">{\theta \sim \hat{\theta}}\left[\operatorname{REGRET}^{\theta}\left(\pi</em>\right)$. The protagonist and antagonist are both trained to maximize their discounted environment returns while the teacher is trained to maximize $U$. Note that by maximizing regret, the teacher is disincentivized from generating unsolvable levels, which will have a maximum regret of 0 . As shorthand, we will sometimes refer to the protagonist and antagonist jointly as the student agents. The counterclockwise loop beginning at the student agents in Figure 2 summarizes this approach, with the students being both the protagonist and antagonist.
As both student agents grow more adept at solving different levels, the teacher continues to adapt its level designs to exploit the weaknesses of the protagonist in relation to the antagonist. As this dynamic unfolds, PAIRED produces an emergent curriculum of progressively more complex levels along the boundary of the protagonist's capabilities. PAIRED is a creative method in the sense that the teacher may potentially generate an endless sequence of novel levels. However, as the teacher only adapts through gradient updates, it is inherently slow to adapt to changes in the student policies.}, \pi_{B}\right)\right]$, where regret is defined by $\operatorname{REGRET}^{\theta}\left(\pi_{A}, \pi_{B}\right)=V^{\theta}\left(\pi_{B}\right)-V^{\theta}\left(\pi_{A</p>
<h3>2.3 Prioritized Level Replay</h3>
<p>Prioritized Level Replay [PLR, 17] is an active-learning strategy shown to improve a policy's sample efficiency and generalization to unseen levels when training and evaluating on levels from a common UPOMDP, typically implemented as a seeded simulator. PLR maintains a level buffer $\boldsymbol{\Lambda}$ of the top $K$ visited levels with highest learning potential as estimated by the time-averaged L1 value loss of the learning agent over the last episode on each level. At the start of each training episode, with some predefined replay probability $p$, PLR uses a bandit to sample the level from $\boldsymbol{\Lambda}$ to maximize the estimated learning potential; otherwise, with probability $1-p$, PLR samples a new level from the simulator. In contrast to the generative but slow-adapting PAIRED, PLR does not create new levels, but instead, acts as a fast-adapting curation mechanism for selecting the next training level among previously encountered levels. Also unlike PAIRED, PLR does not provide a robustness guarantee. By extending the theoretical foundation of PAIRED to PLR, we will show how PLR can be modified</p>
<p>to provide a robustness guarantee at NE, as well as how PAIRED can exploit PLR’s complementary curation to quickly switch among generated levels to maximize the student’s regret.</p>
<h2>3 The Robustness of Dual Curriculum Design</h2>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 2: Overview of Dual Curriculum Design (DCD). The student learns in the presence of two co-adapting teachers that aim to maximize the student’s regret: The generator teacher designs new levels to challenge the agent, and the curator teacher prioritizes a set of levels already created, selectively sampling them for replay.</p>
<p>The previous approaches of PAIRED and PLR reveal a natural duality: Approaches that gradually learn to generate levels like PAIRED, and methods which cannot generate levels, but instead, quickly curate existing ones, like PLR. This duality suggests combining slow level generators with fast level curators. We call this novel class of UED algorithms Dual Curriculum Design (DCD). For instance, PLR can be seen as curator with a prioritized sampling mechanism with a random generator, while PAIRED, as a regret-maximizing generator without a curator. DCD can further consider Domain Randomization (DR) as a degenerate case of a random level generator without a curator.</p>
<p>To theoretically analyze this space of methods, we model DCD as a three player game among a student agent and two teachers called the <em>dual curriculum game</em>. However, to formalize this game, we must first formalize the single-teacher setting: Suppose the UPOMDP is clear from context. Then, given a utility function for a single teacher, $U_t(\pi, \theta)$, we can naturally define the <em>base game</em> between the student $s$ and teacher $t$ as $G = \langle S = S_s \times S_t, U = U_s \times U_t \rangle$, where $S_s = \Pi$ is the strategy set of the student, $S_t = \Theta$ is the strategy set of the teacher, and $U_s(\pi, \theta) = V^\theta(\pi)$ is the utility function of the student. In Sections 4 and 5, we will study settings corresponding to different choices of utility functions for the teacher agents, namely the maximum-regret objective $U_t^R(\pi, \theta)$ and the uniform objective $U_t^U(\pi, \theta)$. These two objectives are defined as follows (for any constant $C$):</p>
<p>$$
U_t^R(\pi, \theta) = \underset{\pi^<em> \in \Pi}{\text{argmax}} { V^\theta(\pi^</em>) - V^\theta(\pi) \tag{1}
$$</p>
<p>$$
U_t^U(\pi, \theta) = C \tag{2}
$$</p>
<p>In the dual curriculum game $G$, the first teacher plays the game with probability $p$, and the second, with probability $(1 - p)$—or more formally, $G = \langle S = S_s \times S_t \times S_t, U = \overline{U}_s \times \overline{U}_t^1 \times \overline{U}_t^2 \rangle$, where the utility functions for the student and two teachers respectively, $U_s, U_t^1, U_t^2$, are defined as follows:</p>
<p>$$
\overline{U}_t^1(\pi, \theta^1, \theta^2) = pU_t^1(\pi, \theta^1) \tag{3}
$$</p>
<p>$$
\overline{U}_t^2(\pi, \theta^1, \theta^2) = (1 - p)U_t^2(\pi, \theta^2) \tag{4}
$$</p>
<p>$$
\overline{U}_s(\pi, \theta^1, \theta^2) = pU_s(\pi, \theta^1) + (1 - p)U_s(\pi, \theta^2) \tag{5}
$$</p>
<p>Our main theorem is that NE in the dual curriculum game are approximate NE of both the base game for either of the original teachers and the base game with a teacher maximizing the joint-reward of $pU_t^1 + (1 - p)U_t^2$, where the quality of the approximations depends on the mixing probability $p$.</p>
<p>Theorem 1. <em>Let $B$ be the maximum difference between $U_t^1$ and $U_t^2$, and let $(\pi, \theta^1, \theta^2)$ be a NE for $G$. Then $(\pi, p \theta^1 + (1 - p) \theta^2)$ is an approximate NE for the base game with either teacher or for a teacher optimizing their joint objective. More precisely, it is a $2Bp(1 - p)$-approximate NE when $U_t = pU_t^1 + (1 - p)U_t^2$, a $2B(1 - p)$-approximate NE when $U_t = U_t^1$, and a $2Bp$-approximate NE when $U_t = U_t^2$.</em></p>
<p>The intuition behind this theorem is that, since the two teachers do not affect each other’s behavior, their best response to a fixed $\pi_s$ is to choose a strategy $\theta$ that maximizes $U_t^1$ and $U_t^2$ respectively.</p>
<p>Moreover, the two teachers' strategies can be viewed as a single combined strategy for the base game with the joint-objective, or with each teacher's own objective. In fact, the teachers provide an approximate best-response to each case of the base game simply by playing their individual best responses. Thus, when we reach a NE of the dual curriculum game, the teachers arrive at approximate best responses for both the base game with the joint objective and with their own objectives, meaning they are also in an approximate NE of the base game with either teacher. The full details of this proof are outlined in Appendix A.</p>
<h1>4 Robustifying PLR</h1>
<p>In this section, we provide theoretical justification for the empirically observed effectiveness of PLR, and in the process, motivate a counterintuitive adjustment to the algorithm.</p>
<div class="codehilite"><pre><span></span><code>Algorithm 1: Robust PLR (PLR \(\perp\) )
Randomly initialize policy \(\pi(\phi)\) and an empty level buffer, \(\boldsymbol{\Lambda}\) of size \(K\).
while not converged do
    Sample replay-decision Bernoulli, \(d \sim P_{D}(d)\)
    if \(d=0\) then
        Sample level \(\theta\) from level generator
        Collect \(\pi\) &#39;s trajectory \(\tau\) on \(\theta\), with a stop-gradient \(\phi_{\perp} \quad\) i.e. Suppress policy update
    else
        Use PLR to sample a replay level from the level store, \(\theta \sim \boldsymbol{\Lambda}\)
        Collect policy trajectory \(\tau\) on \(\theta\) and update \(\pi\) with rewards \(\boldsymbol{R}(\tau)\)
    end
    Compute PLR score, \(S=\operatorname{score}(\tau, \pi)\)
    Update \(\boldsymbol{\Lambda}\) with \(\theta\) using score \(S\)
end
</code></pre></div>

<h3>4.1 Achieving Robustness Guarantees with PLR</h3>
<p>PLR provides strong empirical gains in generalization, but lacks any theoretical guarantees of robustness. One step towards achieving such a guarantee is to replace its L1 value-loss prioritizaton with a regret prioritization, using the methods we discuss in Section 4.2: While L1 value loss may be good for quickly training the value function, it can bias the long-term training behavior toward high-variance policies. However, even with this change, PLR holds weaker theoretical guarantees because the random generating teacher can bias the student away from minimax regret policies and instead, toward policies that sacrifice robustness in order to excel in unstructured levels. We formalize this intuitive argument in the following corollary of Theorem 1.
Corollary 1. Let $\bar{G}$ be the dual curriculum game in which the first teacher maximizes regret, so $U_{t}^{1}=U_{t}^{R}$, and the second teacher plays randomly, so $U_{t}^{2}=U_{t}^{U}$. Let $V^{\theta}(\pi)$ be bounded in $\left[B^{-}, B^{+}\right]$for all $\theta, \pi$. Further, suppose that $\left(\pi, \theta^{1}, \theta^{2}\right)$ is a Nash equilibrium of $\bar{G}$. Let $R^{<em>}=\min <em A="A">{\pi</em>\left{\max } \in \Pi<em B="B">{\theta, \pi</em>\right)(1-p)$ close to having optimal worst-case regret, or formally, $\max } \in \Theta, \Pi}\left{\operatorname{REGRET}^{\theta}\left(\pi_{A}, \pi_{B}\right)\right}\right}$ be the optimal worst-case regret. Then $\pi$ is $2\left(B^{+}-B^{-<em B="B">{\theta, \pi</em>, \pi\right)\right} \geq R^{} \in \Theta, \Pi}\left{\operatorname{REGRET}^{\theta}\left(\pi_{A</em>}-2\left(B^{+}-B^{-}\right)(1-p)$. Moreover, there exists environments for all values of $p$ within a constant factor of achieving this bound.</p>
<p>The proof of Corollary 1 follows from a direct application of Theorem 1 to show that a NE of $\bar{G}$ is an approximate NE for the base game of the first teacher, and through constructing a simple example where the student's best response in $\bar{G}$ fails to attain the minimax regret in $G$. These arguments are described in full in Appendix A. This corollary provides some justification for why PLR improves robustness of the equilibrium policy, as it biases the resulting policy toward a minimax regret policy. However, it also points a way towards further improving PLR: If the probability $p$ of using a teachergenerated level directly was set to 0 , then in equilibrium, the resulting policy converges to a minimax regret policy. Consequently, we arrive at the counterintuitive idea of avoiding gradient updates from trajectories collected from randomly sampled levels, to ensure that at NE, we find a minimax regret policy. From a robustness standpoint, it is therefore optimal to train on less data. The modified PLR algorithm $\mathrm{PLR}^{\perp}$ with this counterintuitive adjustment is summarized in Algorithm 1, in which this small change relative to the original algorithm is highlighted in blue.</p>
<h1>4.2 Estimating Regret</h1>
<p>In general, levels may differ in maximum achievable returns, making it impossible to know the true regret of a level without access to an oracle. As the L1 value loss typically employed by PLR does not generally correspond to regret, we turn to alternative scoring functions that better approximate regret. Two approaches, both effective in practice, are discussed below.
Positive Value Loss Averaging over all transitions with positive value loss amounts to estimating regret as the difference between maximum achieved return and predicted return on an episodic basis. However, this estimate is highly biased, as the value targets are tied to the agent's current, potentially suboptimal policy. As it only considers positive value losses, this scoring function leads to optimistic sampling of levels with respect to the current policy. When using GAE [33] to estimate bootstrapped value targets, this loss takes the following form, where $\lambda$ and $\gamma$ are the GAE and MDP discount factors respectively, and $\delta_{t}$, the TD-error at timestep $t$ :</p>
<p>$$
\frac{1}{T} \sum_{t=0}^{T} \max \left(\sum_{k=t}^{T}(\gamma \lambda)^{k-t} \delta_{k}, 0\right)
$$</p>
<p>Maximum Monte Carlo (MaxMC) We can mitigate some of the bias of the positive value loss by replacing the value target with the highest return achieved on the given level so far during training. By using this maximal return, the regret estimates no longer depend on the agent's current policy. This estimator takes the simple form of $(1 / T) \sum_{t=0}^{T} R_{\max }-V\left(s_{t}\right)$. In our dense-reward experiments, we compute this score as the difference between the maximum achieved return and $V\left(s_{0}\right)$.</p>
<h2>5 Replay-Enhanced PAIRED (REPAIRED)</h2>
<p>We can replace the random generator teacher used by $\mathrm{PLR}^{\perp}$ with the PAIRED teacher. This extension entails a second student agent, the antagonist, also equipped with its own PLR level buffer. In each episode, with probability $p$, the students evaluate their performances (but do not train) on a newly generated level and, with probability $1-p$, train on a level sampled from each student's own regret-prioritizing PLR buffer. Training only on the highest regret levels should mitigate inefficiencies in the PAIRED teacher's optimization procedure. We refer to this extension as Replay-Enhanced PAIRED (REPAIRED). An overview of REPAIRED is provided by black arrows in Figure 2, with the students being the protagonist and antagonist, while the full pseudocode is outlined in Appendix B.</p>
<p>Since $\mathrm{PLR}^{\perp}$ and PAIRED both promote regret in equilibrium, it would be reasonable to believe that the combination of the two does the same. A straightforward corollary of Theorem 1, which we describe in Appendix 1, shows that, in a theoretically ideal setting, combining these two algorithms as is done in REPAIRED indeed finds minimax regret strategies in equilibrium.</p>
<p>Corollary 2. Let $\bar{G}$ be the dual curriculum game in which both teachers maximize regret, so $U_{t}^{1}=U_{t}^{2}=U_{t}^{B}$. Further, suppose that $\left(\pi, \theta^{1}, \theta^{2}\right)$ is a Nash equilibrium of $\bar{G}$. Then, $\pi \in$ $\operatorname{argmin}<em A="A">{\pi</em>\left{\max } \in \Pi<em B="B">{\theta, \pi</em>\right)\right}\right}$.} \in \Theta, \Pi}\left{\operatorname{REGRET}^{\theta}\left(\pi_{A}, \pi_{B</p>
<p>This result gives us some amount of assurance that, if our method arrives at NE, then the protagonist has converged to a minimax regret strategy, which has the benefits outlined in [10]: Since a minimax regret policy solves all solvable environments, whenever this is possible and sufficiently well-defined, we should expect policies resulting from the equilibrium behavior of REPAIRED to be robust and versatile across all environments in the domain.</p>
<h2>6 Experiments</h2>
<p>Our experiments firstly aim to (1) assess the empirical performance of the theoretically motivated $\mathrm{PLR}^{\perp}$, and secondly, seek to better understand the effect of replay on unsupervised environment design, specifically (2) its impact on the zero-shot generalization performance of the induced student policies, and (3) the complexity of the levels designed by the teacher. To do so, we compare PLR and REPAIRED against their replay-free counterparts, DR and PAIRED, in the two highly distinct settings of discrete control with sparse rewards and continuous control with dense rewards. We provide environment descriptions alongside model and hyperparameter choices in Appendix D.</p>
<p>6.1 Partially-Observable Navigation</p>
<p>Each navigation level is a partially-observable maze requiring student agents to take discrete actions to reach a goal and receive a sparse reward. Our agents use PPO [34] with an LSTM-based recurrent policy to handle partial observability. Before each episode, the teacher designs the level in this order: beginning with an empty maze, it places one obstructing block per time step up to a predefined block budget, and finally places the agent followed by the goal.</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 3: Zero-shot transfer performance in challenging test environments after 250M training steps. The plots show median and interquartile range of solved rates over 10 runs. An asterisk (*) next to the maze name indicates the maze is procedurally-generated, and thus each attempt corresponds to a random configuration of the maze.</p>
<p>Zero-Shot Generalization We train policies with each method for 250M steps and evaluate zero-shot generalization on several challenging OOD environments, in addition to levels from the full distribution of two procedurally-generated environments, PerfectMaze and LargeCorridor. We also compare against DR and minimax baselines. Our results in Figure 3 and 4 show that PLR<sup>⊥</sup> and REPAIRED both achieve greater sample-efficiency and zero-shot generalization than their replay-free counterparts. The improved test performance achieved by PLR<sup>⊥</sup> over both DR and PLR when trained for an equivalent number of gradient updates, aggregated over all test mazes, is statistically significant (p &lt; 0.05), as is the improved test performance of REPAIRED over PAIRED. Well before 250 million steps, both PLR and PLR<sup>⊥</sup> significantly outperform PAIRED after 3 billion training steps, as reported in [10]. Further, both PLR variants lead to policies exhibiting greater zero-shot transfer than the PAIRED variants. Notably, the PLR<sup>⊥</sup> agent learns to solve mazes via an approximate right-hand rule. Table 2 in Appendix C.1 reports performance across all test mazes. The success of designing regret-maximizing levels via random search (curation) over learning a generator with RL suggests that for some UPOMDPs, the regret landscape, as a function of the free parameters θ, has a low effective dimensionality [3]. Foregoing gradient-based learning in favor of random search may then lead to faster adaptation to the changing regret landscape, as the policy evolves throughout training.</p>
<p>Emergent Complexity As the student agents improve, the teachers must generate more challenging levels to maintain regret. We measure the resultant emergent complexity by tracking the number of blocks in each level and the shortest path length to the goal (where unsolvable levels are assigned a length of 0). These results, summarized in Figure 4, show that PAIRED slowly adapts the complexity over training while REPAIRED initially quickly grows complexity, before being overtaken by PAIRED. This more rapid onset of complexity may be due to REPAIRED's fast replay mechanism, and the long-term slowdown relative to PAIRED may be explained by its less frequent gradient updates. Our results over an extended training period in Appendix C confirm that both PAIRED and REPAIRED slowly increase complexity over time, eventually matching that attained in just a fraction of the number of gradient steps by PLR and PLR<sup>⊥</sup>. This result shows that random search is surprisingly efficient at continually discovering levels of increasing complexity, given an appropriate curation mechanism such as PLR. Figure 5 shows that, similar to methods with a regret-maximizing teacher, PLR finds levels exhibiting complex structure.</p>
<p><img alt="img-3.jpeg" src="img-3.jpeg" /></p>
<p>Figure 4: Zero-shot transfer performance during training for PAIRED and REPAIRED variants. The plots show mean and standard error across 10 runs. The dotted lines mark the mean performance of PAIRED after 3B training steps, as reported in [10], while dashed lines indicate median returns.</p>
<h1>6.2 Pixel-Based Car Racing with Continuous Control</h1>
<p>To test the versatility and scalability of our methods, we turn to an extended version of the CarRacing environment from OpenAI Gym [5]. This environment entails continuous control with dense rewards, a 3-dimensional action space, and partial, pixel observations, with the goal of driving a full lap around a track. To enable UED of any closed-loop track, we reparameterize CarRacing to generate tracks as Bézier curves [22] with arbitrary control points. The teacher generates levels by choosing a sequence of up to 12 control points, which uniquely defines a Bézier track within specific, predefined curvature constraints. After 5M steps of training, we test the zero-shot transfer performance of policies trained by each method on 20 levels replicating official human-designed Formula One (F1) tracks (see Figure 19 in the Appendix for a visualization of the tracks). Note that these tracks are significantly OOD, as they cannot be defined with just 12 control points. In Figure 6 we show the progression of zero-shot transfer performance for the original CarRacing environment, as well as three F1 tracks of varying difficulty, while also including the final performance on the full F1 benchmark. For the final performance, we also evaluated the state-of-the-art CarRacing agent from [38] on our new F1 benchmark.
<img alt="img-4.jpeg" src="img-4.jpeg" /></p>
<p>Figure 6: Zero-shot transfer performance. Plots show mean and standard error over 10 runs.</p>
<p>Unlike in the sparse, discrete navigation setting, we find DR leads to moderately successful policies for zero-shot transfer in CarRacing. Dense rewards simplify the learning problem and random Bezier tracks occasionally contain the challenges seen in F1 tracks, such as hairpin turns and observations showing parallel tracks due to high local curvature. Still, we see that policies trained by selectively sampling tracks to maximize regret significantly outperform those trained by uniformly sampling</p>
<p>from randomly generated tracks, in terms of zero-shot transfer to the OOD F1 tracks. Remarkably, with a replay rate of 0.5, $\mathrm{PLR}^{\perp}$ sees statistically significant ( $p&lt;0.001$ ) gains over PLR in zero-shot performance over the full F1 benchmark, despite directly training on only half the rollout data using half as many gradient updates. Once again, we see that random search with curation via PLR produces a rich selection of levels and an effective curriculum.</p>
<p>We also observe that PAIRED struggles to train a robust protagonist in CarRacing. Specifically, PAIRED overexploits the relative strengths of the antagonist over the protagonist, finding curricula that steer the protagonist towards policies that ultimately perform poorly even on simple tracks, leading to a gradual reduction in level complexity. We present training curves revealing this dynamic in Appendix C. As shown in Figure 6, REPAIRED mitigates this degeneracy substantially, though not completely, inducing a policy that significantly outperforms PAIRED ( $p&lt;0.001$ ) in mean performance on the full F1 benchmark, but underperforms DR. Notably, $\mathrm{PLR}^{\perp}$ exceeds the performance of the state-of-the-art AttentionAgent [38], despite not using a self-attention policy and training on less than $0.25 \%$ of the number of environment steps in comparison. These gains come purely from the induced curriculum. Figure 17 in Appendix C further reveals that $\mathrm{PLR}^{\perp}$ produces CarRacing policies that tend to achieve higher minimum returns on average compared to the baseline methods, providing further evidence of the benefits of the minimax regret property coupled with a fast replay-based mechanism for efficiently finding high-regret levels.</p>
<h1>7 Related Work</h1>
<p>In inducing parallel curricula, DCD follows a rich lineage of curriculum learning methods [2, 32, 28, 23]. Many previous curriculum learning algorithms resemble the curator in DCD, sharing similar underlying selective-sampling mechanisms as $\mathrm{PLR}^{\perp}$. Most similar is TSCL [19], which prioritizes levels based on return rather than value loss, and has been shown to overfit to training levels in some settings [17]. In our setting, replayed levels can be viewed as past strategies from a level-generating teacher. This links our replay-based methods to fictitious self-play [FSP, 13], and more closely, Prioritized FSP [40], which selectively samples opponents based on historic win ratios.
Recent approaches that make use of a generating adversary include Asymmetric Self-Play [37, 26], wherein one agent proposes tasks for another in the form of environment trajectories, and AMIGo [6], wherein the teacher is rewarded for proposing reachable goals. While our methods do not presuppose a goal-based setting, others have made progress here using generative modeling [12, 29], latent skill learning [14], and exploiting model disagreement [45]. These methods are less generally applicable than $\mathrm{PLR}^{\perp}$, and unlike our DCD methods, they do not provide well-principled robustness guarantees.
Other recent algorithms can be understood as forms of UED and like DCD, framed in the lens of decision theory. POET [41, 42], a coevolutionary approach [27], uses a population of minimax (rather than minimax regret) adversaries to construct terrain for a BipedalWalker agent. In contrast to our methods, POET requires training a large population of both agents and environments and consequently, a sizable compute overhead. APT-Gen [11] also procedurally generates tasks, but requires access to target tasks, whereas our methods seek to improve zero-shot transfer.
The DCD framework also encompasses adaptive domain randomization methods [DR, 20, 15], which have seen success in assisting sim2real transfer for robotics [39, 16, 1, 25]. DR itself is subsumed by procedural content generation [30], for which UED and DCD may be seen as providing a formal, decision-theoretic framework, enabling development of provably optimal algorithms.</p>
<h2>8 Discussion</h2>
<p>We established a novel connection between PLR and minimax regret UED approaches like PAIRED, by developing the theory of Dual Curriculum Design (DCD). In this setting, a student policy is challenged by a team of two co-adapting, regret-maximizing teachers: one, a generator that creates new levels, and the other, a curator that selectively samples previously generated levels for replay. This view unifies PLR and PAIRED, which are both instances of DCD. Our theoretical results on DCD then enabled us to prove that PLR attains a minimax regret policy at NE, thereby providing the first theoretical characterization of the robustness of PLR. Notably our theory leads to the counterintuitive result that PLR can be made provably robust by training on less data, specifically, by only using</p>
<p>the trajectories on levels sampled for replay. In addition, we developed Replay-Enhanced PAIRED (REPAIRED), which extends the selective replay-based updates of $\mathrm{PLR}^{\perp}$ to PAIRED, and proved it shares the same robustness guarantee at NE. Empirically, in two highly distinct environments, we found that $\mathrm{PLR}^{\perp}$ significantly improves zero-shot generalization over PLR, and REPAIRED, over PAIRED. As our methods solely modify the order of levels visited during training, they can, in principle, be combined with many other RL methods to yield potentially orthogonal improvements in sample-efficiency and generalization.
While these DCD-based improvements to PLR and PAIRED empirically lead to more robust policies, it is important to emphasize that our theoretical results only prove a minimax regret guarantee at NE for these methods; however, they provide no explicit guarantee of convergence to such NE. Further, it is worth highlighting that replay-based methods like $\mathrm{PLR}^{\perp}$ are completely dependent on the quality of levels proposed by the generator. Our results show that simply curating high regret levels discovered via random search is enough to outperform the RL-based PAIRED teacher in the domains studied. We expect that advancing methods for defining or adapting the generator's proposal distribution holds great potential to improve the efficacy of our methods, especially in more complex, higher-dimensional domains, where random search may prove ineffective for finding useful training levels. Importantly, our methods assume an appropriate choice of what constitutes the UPOMDP's free parameters. Our methods cannot be expected to produce robust policies for zero-shot transfer if the set of environments defined by the free parameters does not sufficiently align with the transfer domain of interest. Designing the environment parameterization for successful zero-shot transfer to a specific target domain can be highly non-trivial, posing an important problem for future research.
Looking beyond environment design, we notice that long-running UED processes in expansive UPDOMPs closely resemble continual learning in open-ended domains. The congruency of these settings suggests our contributions around DCD may extend to more general continual learning problems in which agents must learn to master a diverse sequence of tasks with predefined (or inferred) episode boundaries-if tasks are assumed to be designed by a regret-maximizing teacher. Thus, DCD-based methods like $\mathrm{PLR}^{\perp}$ may yield more general policies for continual learning. We anticipate many exciting crossovers between these areas of research in the years to come.</p>
<h1>Broader Impact</h1>
<p>Given the rapid progress in applying RL to ever more complex domains, we can confidently expect a continued rise in real-world deployments of RL systems in the coming years. Unlike in simulation, in the real world, the environment tends to exhibit much more variability, which may not be explicitly coded into the associated simulator used for training. Deployed RL agents are thus liable to make many mistakes due to unexpected environment variations. Our methods for improving UED lead to more robust RL agents across a potentially wide range of changes to the environment. Thus, our work may prove to be a useful tool in attaining safer, more reliable RL agents, helping to enable the application of RL to more real-world problems.
By increasing the applicability of RL to real-world settings, our work may exacerbate the more general risks of deploying machine learning: increased unemployment; overreliance on biased models that potentially reinforce common misconceptions and societal inequalities; and the advancement of automated weapons. Particular to our methods, as discussed in Section 8, aligning the choice of free parameters for the UPOMDP to the target domain of interest is important for successful transfer. While this choice of free parameters then acts as a potential point of failure, the UPOMDP abstraction underlying UED reveals this problem generally impacts all RL methods aiming to train robust policies; a UPOMDP simply makes explicit the otherwise implicit space of environment configurations defined by a standard POMDP. By forcing us to consider where our training environment departs from reality, UED methods encourage designing RL systems in a way that is more aware of the underlying assumptions about the environment, thereby leading to more principled, robust systems.</p>
<h2>Acknowledgments and Disclosure of Funding</h2>
<p>We would like to thank Natasha Jaques, Patrick Labatut, and Heinrich Küttler for fruitful discussions that helped inform this work. Further, we are grateful to our anonymous reviewers for their valuable feedback. MJ is supported by the FAIR PhD program. This work was funded by Facebook.</p>
<h1>References</h1>
<p>[1] O. M. Andrychowicz, B. Baker, M. Chociej, R. Józefowicz, B. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray, J. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba. Learning dexterous in-hand manipulation. The International Journal of Robotics Research, 39(1):3-20, 2020.
[2] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In Proceedings of the 26th Annual International Conference on Machine Learning, ICML '09, page 41-48, New York, NY, USA, 2009. Association for Computing Machinery.
[3] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of Machine Learning Research, 13:281-305, 2012.
[4] C. Berner, G. Brockman, B. Chan, V. Cheung, P. Dębiak, C. Dennison, D. Farhi, Q. Fischer, S. Hashme, C. Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
[5] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.
[6] A. Campero, R. Raileanu, H. Kuttler, J. B. Tenenbaum, T. Rocktäschel, and E. Grefenstette. Learning with AMIGo: Adversarially motivated intrinsic goals. In International Conference on Learning Representations, 2021.
[7] M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for openai gym. https://github.com/maximecb/gym-minigrid, 2018.
[8] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to benchmark reinforcement learning. In Proceedings of the 37th International Conference on Machine Learning, pages 2048-2056, 2020.
[9] K. Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in reinforcement learning. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, Proceedings of Machine Learning Research. PMLR, 2019.
[10] M. Dennis, N. Jaques, E. Vinitsky, A. Bayen, S. Russell, A. Critch, and S. Levine. Emergent complexity and zero-shot transfer via unsupervised environment design. In Advances in Neural Information Processing Systems, volume 33, 2020.
[11] K. Fang, Y. Zhu, S. Savarese, and F.-F. Li. Adaptive procedural task generation for hardexploration problems. In International Conference on Learning Representations, 2021.
[12] C. Florensa, D. Held, X. Geng, and P. Abbeel. Automatic goal generation for reinforcement learning agents. In J. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 1515-1528. PMLR, 102018.
[13] J. Heinrich, M. Lanctot, and D. Silver. Fictitious self-play in extensive-form games. In Proceedings of the 32nd International Conference on Machine Learning, pages 805-813, 2015.
[14] A. Jabri, K. Hsu, A. Gupta, B. Eysenbach, S. Levine, and C. Finn. Unsupervised curricula for visual meta-reinforcement learning. In Advances in Neural Information Processing Systems, volume 32, 2019.
[15] N. Jakobi. Evolutionary robotics and the radical envelope-of-noise hypothesis. Adaptive Behavior, 6(2):325-368, 1997.
[16] S. James, A. J. Davison, and E. Johns. Transferring end-to-end visuomotor control from simulation to real world for a multi-stage task. In 1st Conference on Robot Learning, 2017.
[17] M. Jiang, E. Grefenstette, and T. Rocktäschel. Prioritized level replay. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4940-4950. PMLR, 2021.
[18] X. Ma. Car racing with pytorch. https://github.com/xtma/pytorch_car_caring, 2019.
[19] T. Matiisen, A. Oliver, T. Cohen, and J. Schulman. Teacher-student curriculum learning. IEEE Transactions on Neural Networks and Learning Systems, PP, 072017.</p>
<p>[20] B. Mehta, M. Diaz, F. Golemo, C. J. Pal, and L. Paull. Active domain randomization. In L. P. Kaelbling, D. Kragic, and K. Sugiura, editors, 3rd Annual Conference on Robot Learning, CoRL 2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, volume 100 of Proceedings of Machine Learning Research, pages 1162-1176. PMLR, 2019.
[21] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
[22] M. E. Mortenson. Mathematics for Computer Graphics Applications. Industrial Press Inc., 1999.
[23] S. Narvekar, B. Peng, M. Leonetti, J. Sinapov, M. E. Taylor, and P. Stone. Curriculum learning for reinforcement learning domains: A framework and survey. Journal of Machine Learning Research, 21:181:1-181:50, 2020.
[24] J. F. Nash et al. Equilibrium points in n-person games. Proceedings of the National Academy of Sciences, 36(1):48-49, 1950.
[25] OpenAI, I. Akkaya, M. Andrychowicz, M. Chociej, M. Litwin, B. McGrew, A. Petron, A. Paino, M. Plappert, G. Powell, R. Ribas, J. Schneider, N. Tezak, J. Tworek, P. Welinder, L. Weng, Q. Yuan, W. Zaremba, and L. Zhang. Solving rubik's cube with a robot hand. CoRR, abs/1910.07113, 2019.
[26] O. OpenAI, M. Plappert, R. Sampedro, T. Xu, I. Akkaya, V. Kosaraju, P. Welinder, R. D'Sa, A. Petron, H. P. de Oliveira Pinto, A. Paino, H. Noh, L. Weng, Q. Yuan, C. Chu, and W. Zaremba. Asymmetric self-play for automatic goal discovery in robotic manipulation, 2021.
[27] E. Popovici, A. Bucci, R. P. Wiegand, and E. D. De Jong. Coevolutionary Principles, pages 987-1033. Springer Berlin Heidelberg, 2012.
[28] R. Portelas, C. Colas, L. Weng, K. Hofmann, and P.-Y. Oudeyer. Automatic curriculum learning for deep rl: A short survey. In C. Bessiere, editor, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, pages 4819-4825. International Joint Conferences on Artificial Intelligence Organization, 7 2020. Survey track.
[29] S. Racaniere, A. Lampinen, A. Santoro, D. Reichert, V. Firoiu, and T. Lillicrap. Automated curriculum generation through setter-solver interactions. In International Conference on Learning Representations, 2020.
[30] S. Risi and J. Togelius. Increasing generality in machine learning through procedural content generation. Nature Machine Intelligence, 2(8):428-436, 82020.
[31] L. J. Savage. The theory of statistical decision. Journal of the American Statistical association, 46(253):55-67, 1951.
[32] J. Schmidhuber. Powerplay: Training an increasingly general problem solver by continually searching for the simplest still unsolvable problem. Frontiers in Psychology, 4:313, 062013.
[33] J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional continuous control using generalized advantage estimation. In Y. Bengio and Y. LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016.
[34] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017.
[35] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
[36] D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel, et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):1140-1144, 2018.
[37] S. Sukhbaatar, Z. Lin, I. Kostrikov, G. Synnaeve, A. Szlam, and R. Fergus. Intrinsic motivation and automatic curricula via asymmetric self-play. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.</p>
<p>[38] Y. Tang, D. Nguyen, and D. Ha. Neuroevolution of self-interpretable agents. In Proceedings of the Genetic and Evolutionary Computation Conference, 2020.
[39] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel. Domain randomization for transferring deep neural networks from simulation to the real world. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 23-30, 2017.
[40] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al. Grandmaster level in StarCraft II using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.
[41] R. Wang, J. Lehman, J. Clune, and K. O. Stanley. Paired open-ended trailblazer (POET): endlessly generating increasingly complex and diverse learning environments and their solutions. CoRR, abs/1901.01753, 2019.
[42] R. Wang, J. Lehman, A. Rawal, J. Zhi, Y. Li, J. Clune, and K. Stanley. Enhanced POET: Open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In Proceedings of the 37th International Conference on Machine Learning, pages 9940-9951, 2020.
[43] B. L. Welch. The generalization of 'student's' problem when several different population variances are involved. Biometrika, 34(1-2):28-35, 1947.
[44] A. Zhang, N. Ballas, and J. Pineau. A dissection of overfitting and generalization in continuous reinforcement learning. arXiv preprint arXiv:1806.07937, 2018.
[45] Y. Zhang, P. Abbeel, and L. Pinto. Automatic curriculum learning through value disagreement. In Advances in Neural Information Processing Systems, volume 33, pages 7648-7659, 2020.</p>
<h1>A Theoretical Results</h1>
<p>In this section we prove the theoretical results around the dual curriculum game and use these results to show approximation bounds for our methods, given that they have reached a Nash equilibrium (NE).</p>
<p>The first theorem is the main result that allows us to analyze dual curriculum games. The high-level result says that the NE of a dual curriculum game are approximate NE of the base game from the perspective of any of the individual players, or from the perspective of the joint strategy.
Theorem 1. Let $B$ be the maximum difference between $U_{t}^{1}$ and $U_{t}^{2}$, and let $\left(\pi, \theta^{1}, \theta^{2}\right)$ be a NE for $\bar{G}$. Then $\left(\pi, p \theta^{1}+(1-p) \theta^{2}\right)$ is an approximate NE for the base game with either teacher or for a teacher optimizing their joint objective. More precisely, it is a $2 B p(1-p)$-approximate NE when $U_{t}=p U_{t}^{1}+(1-p) U_{t}^{2}$, a $2 B(1-p)$-approximate NE when $U_{t}=U_{t}^{1}$, and a $2 B p$-approximate NE when $U_{t}=U_{t}^{2}$.</p>
<p>At a high level, this is true because, for low values of $p$, the best-response strategies for the individual players can be thought of as approximate-best response strategies for the joint-player, and vis-versa. Since the Nash Equilibrium consists of each of the players playing their own best response, they must be playing an approximate best response for the joint-player. We provide a formal proof below:</p>
<p>Proof. Let $B$ be the maximum difference between $U_{t}^{1}$ and $U_{t}^{2}$, and let $\left(\pi, \theta^{1}, \theta^{2}\right)$ be a Nash Equilibrium for $\bar{G}$. Then consider $p \theta^{1}+(1-p) \theta^{2}$ as a strategy in the base game for the joint player $p U_{t}^{1}+(1-p) U_{t}^{2}$. Let $\theta^{1+2}$ be the best response for the joint player to $\pi$. Since $\pi$ is a best response by assumption, it is sufficient to show that $p \theta^{1}+(1-p) \theta^{2}$ is an approximate best response. We then have</p>
<p>$$
\begin{aligned}
&amp; U_{t}\left(\pi, p \theta^{1}+(1-p) \theta^{2}\right) \
= &amp; p^{2} U_{t}^{1}\left(\pi, \theta^{1}\right)+p(1-p) U_{t}^{2}\left(\pi, \theta^{1}\right)+p(1-p) U_{t}^{1}\left(\pi, \theta^{2}\right)+(1-p)^{2} U_{t}^{2}\left(\pi, \theta^{2}\right) \
\geq &amp; p^{2} U_{t}^{1}\left(\pi, \theta^{1}\right)+p(1-p)\left(U_{t}^{1}\left(\pi, \theta^{1}\right)-B\right)+p(1-p)\left(U_{t}^{2}\left(\pi, \theta^{2}\right)-B\right)+(1-p)^{2} U_{t}^{2}\left(\pi, \theta^{2}\right) \
= &amp; p U_{t}^{1}\left(\pi, \theta^{1}\right)+(1-p) U_{t}^{2}\left(\pi, \theta^{2}\right)-2 B p(1-p) \
\geq &amp; U_{t}\left(\pi, \theta^{1+2}\right)-2 B p(1-p)
\end{aligned}
$$</p>
<p>Thus, we have shown that $\left(\pi, p \theta^{1}+(1-p) \theta^{2}\right)$ represents an $2 B p(1-p)$-Nash equilibrium for the joint player. For the first teacher we have the opposite condition trivially, the teacher is doing a best response to the student. We must now show that the student is doing an approximate best response to the teacher.
Let $\pi^{1}$ be the best response to the first teacher (with utility $U_{t}^{1}$ ) and let $\pi^{1+2}$ be the best response policy to the joint teacher. In this argument we will start with the observation that $U_{s}\left(\pi^{1}, \theta^{1+2}\right) \leq U_{s}\left(\pi^{1+2}, \theta^{1+2}\right)$ by definition, and then argue that we can construct an upper bound on the performance of $\pi^{1}$ on $\theta^{1}, U_{s}\left(\pi^{1}, \theta^{1}\right)$, and a lower bound on the performance of $\pi^{1+2}$ on $\theta^{1}, U_{s}\left(\pi^{1+2}, \theta^{1}\right)$. We get the desired result by combining these two arguments.
First we use $U_{s}\left(\pi^{1}, \theta^{1+2}\right)$ to upper bound $U_{s}\left(\pi^{1}, \theta^{1}\right)$ :</p>
<p>$$
\begin{aligned}
U_{s}\left(\pi^{1}, \theta^{1+2}\right) &amp; =p U_{s}\left(\pi^{1}, \theta^{1}\right)+(1-p) U_{s}\left(\pi^{1}, \theta^{2}\right) \
&amp; \geq p U_{s}\left(\pi^{1}, \theta^{1}\right)+(1-p)\left(U_{s}\left(\pi^{1}, \theta^{1}\right)-B\right) \
&amp; =U_{s}\left(\pi^{1}, \theta^{1}\right)-(1-p) B
\end{aligned}
$$</p>
<p>Second we can use $U_{s}\left(\pi^{1+2}, \theta^{1+2}\right)$ to lower bound $U_{s}\left(\pi^{1+2}, \theta^{1}\right)$ :</p>
<p>$$
\begin{aligned}
U_{s}\left(\pi^{1+2}, \theta^{1+2}\right) &amp; =p U_{s}\left(\pi^{1+2}, \theta^{1}\right)+(1-p) U_{s}\left(\pi^{1+2}, \theta^{2}\right) \
&amp; \leq p U_{s}\left(\pi^{1+2}, \theta^{1}\right)+(1-p)\left(U_{s}\left(\pi^{1+2}, \theta^{1}\right)+B\right) \
&amp; =U_{s}\left(\pi^{1+2}, \theta^{1}\right)+(1-p) B
\end{aligned}
$$</p>
<p>Putting this all together, we have</p>
<p>$$
U_{s}\left(\pi^{1+2}, \theta^{1}\right)+(1-p) B \geq U_{s}\left(\pi^{1}, \theta^{1}\right)-(1-p) B
$$</p>
<p>Which, after rearranging terms, gives</p>
<p>$$
U_{s}\left(\pi^{1+2}, \theta^{1}\right) \geq U_{s}\left(\pi^{1}, \theta^{1}\right)-2(1-p) B
$$</p>
<p>as desired. Repeating the symmetric argument shows the desired property for the second teacher.
Following this main theorem, we can apply it to two of our methods. First we can apply it to naive PLR, which trains on a mixture of domain randomization (a teacher with utility $U_{t}^{P}$ ) and the PLR bandit (a teacher with utility $U_{t}^{R}$ ). This result shows that as we reduce the number of random episodes, the approximation to a minimax regret strategy improves. The intuition behind this is a direct application of Theorem 1, to show that it is an approximate Nash for the minimax regret player, and then showing that the minimax reget player has access to a strategy which ensures small regret, thus the regret that the equilibrium ensures must be approximately small.
Corollary 1. Let $\bar{G}$ be the dual curriculum game in which the first teacher maximizes regret, so $U_{t}^{1}=U_{t}^{R}$, and the second teacher plays randomly, so $U_{t}^{2}=U_{t}^{U}$. Let $V^{\theta}(\pi)$ be bounded in $\left[B^{-}, B^{+}\right]$for all $\theta, \pi$. Further, suppose that $\left(\pi, \theta^{1}, \theta^{2}\right)$ is a Nash equilibrium of $\bar{G}$. Let $R^{<em>}=\min <em A="A">{\pi</em>\left{\max } \in \Pi<em B="B">{\theta, \pi</em>\right)(1-p)$ close to having optimal worst-case regret, or formally, $\max } \in \Theta, \Pi}\left{\operatorname{REGRET}^{\theta}\left(\pi_{A}, \pi_{B}\right)\right}\right}$ be the optimal worst-case regret. Then $\pi$ is $2\left(B^{+}-B^{-<em B="B">{\theta, \pi</em>, \pi\right)\right} \geq R^{} \in \Theta, \Pi}\left{\operatorname{REGRET}^{\theta}\left(\pi_{A</em>}-2\left(B^{+}-B^{-}\right)(1-p)$. Moreover, there exists environments for all values of $p$ within a constant factor of achieving this bound.</p>
<p>Proof. Since $V^{\theta}(\pi)$ is bounded in $\left[B^{-}, B^{+}\right]$for all $\theta, \pi$, we know that $U_{t}^{1}$ and $U_{t}^{2}$ are within $\left(B^{+}-B^{-}\right)$of each other. Thus by Theorem 1 we have that $\left(\pi, \theta^{1}, \theta^{2}\right)$ is a $2\left(B^{+}-B^{-}\right)(1-p)$ Nash equilibrium of the base game when $U_{t}=U_{t}^{1}$. Thus $\pi$ is a $2\left(B^{+}-B^{-}\right)(1-p)$ approximate best-response to $\theta^{1}$. However, since $\theta^{1}$ is a best response it chooses a regret maximizing parameter distribution. Thus the $2\left(B^{+}-B^{-}\right)(1-p)$ does not just measure the sub-optimally of $\pi$ with respect to $\theta^{1}$, but measures the worst-case regret of $\pi$ across all $\theta$ as desired.
The intuition for the existence of examples in which this approximation of regret decays linearly in $p$ is that a random level and the maximal regret level can be very different, and so the two measures may diverge drastically. For an example environment where $\pi$ deviates strongly from the minimax regret strategy, consider the one-step UMDP described in Table 1.</p>
<table>
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">$\theta_{0}$</th>
<th style="text-align: center;">$\theta_{1}$</th>
<th style="text-align: center;">$\theta_{2} \ldots \theta_{n}$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">$\pi_{0}$</td>
<td style="text-align: center;">$B$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\pi_{1}$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$B$</td>
<td style="text-align: center;">0</td>
</tr>
<tr>
<td style="text-align: center;">$\pi_{2}$</td>
<td style="text-align: center;">$B p+2 \epsilon$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$\frac{B p}{2}+\epsilon$</td>
</tr>
<tr>
<td style="text-align: center;">$\pi_{3}$</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">$B p+2 \epsilon$</td>
<td style="text-align: center;">$\frac{B p}{2}+\epsilon$</td>
</tr>
</tbody>
</table>
<p>Table 1: In this environment all payoffs are between 0 and $B$ (for $p \in(0,1)$ and $\epsilon&lt;\frac{B(1-p)}{2}$ ), where $B$ is assumed to be positive. Randomizing between $\pi_{0}$ and $\pi_{1}$ minimizes regret, but choosing $\pi_{2}$ or $\pi_{3}$ is better in expectation under the uniform distribution. For large $n$ it is especially clear that $\pi_{2}$ and $\pi_{3}$ have better expected value under the uniform distribution, though we show that even for $n=2$, the optimal joint policy can mix between $\pi_{2}$ and $\pi_{3}$ incurring high regret.</p>
<p>Note that in Table 1, no policy has less than $\frac{B}{2}$ regret, since every policy will have to incur $B$ regret on either $\left{\theta_{0}, \theta_{1}\right}$ at least half the time. The minimax regret policy mixes uniformly between $\pi_{0}$ and $\pi_{1}$ to achieve regret of exactly $\frac{B}{2}$. We can ignore $\theta_{2} \ldots \theta_{n}$ for the regret calculations by assuming that $\epsilon&lt;\frac{B(1-p)}{2}$, since every policy achieves less than $\frac{B}{2}$ regret on these levels.
Our claim is that in equilibrium of $\bar{G}$ in this environment, the student policy can incur $\frac{B}{2}+\frac{B(1-p)}{2}-\epsilon$ regret, $\frac{B(1-p)}{2}-\epsilon$ more than the minimax regret policy. An example of such an equilibrium point would be when the student policy uniformly randomizes between $\pi_{2}$ and $\pi_{3}$, which we will call $\pi_{2+3}$, when the minimax teacher uniformly randomizes between $\theta_{0}$ and $\theta_{1}$ which we will call $\theta_{0+1}$, and</p>
<p>when the uniform teacher randomizes exactly which we call $\tilde{\theta}$. To check this we must show that $\left(\pi_{2+3}, \theta_{0+1}, \tilde{\theta}\right)$ is in fact a NE of $\bar{G}$. Then we must show that $\pi_{2+3}$ incurs $\frac{B}{2}+\frac{B(1-p)}{2}-\epsilon$ regret.
To show that $\left(\pi_{2+3}, \theta_{0+1}, \tilde{\theta}\right)$ is a NE of $\bar{G}$ first note that $\tilde{\theta}$ is trivially a best response for the uniform utility function. Also note that $\theta_{0+1}$ maximizes the regret of $\pi_{2+3}$ since $\theta_{0}$ and $\theta_{1}$ are the only two parameters on which $\pi_{2+3}$ incur regret, and they incur the same regret; thus, any mixture over them will be optimal for the regret-based teacher. Finally, we need to show that $\pi_{2+3}$ is optimal for the student. To do this we will calculate the expected value of each policy and notice that the expected values for $\pi_{2}$ and $\pi_{3}$ are higher than for $\pi_{0}$ and $\pi_{1}$. Thus any optimal policy will place no weight on $\pi_{0}$ and $\pi_{1}$, but any distribution over $\pi_{2}$ and $\pi_{3}$ will be equivalently optimal. By symmetry, we can show only the calculations for $\pi_{0}$ and $\pi_{2}$ :</p>
<p>$$
\begin{aligned}
&amp; \pi_{0}=p\left(\frac{1}{2} B+\frac{1}{2} 0\right)+(1-p) 0=\frac{B p}{2} \
&amp; \pi_{2}=p\left(\frac{1}{2}(B p+2 \epsilon)+\frac{1}{2} 0\right)+(1-p)\left(\frac{B p}{2}+\epsilon\right)=\frac{B p}{2}+\epsilon
\end{aligned}
$$</p>
<p>Thus $\pi_{2}$ and $\pi_{3}$ achieve $\epsilon$ higher expected value by the joint distribution. Thus, we know that $\pi_{2+3}$ is a best response and $\left(\pi_{2+3}, \theta_{0+1}, \tilde{\theta}\right)$ is in fact a NE of $\bar{G}$.
Finally, we simply need to show that $\pi_{2+3}$ incurs $\frac{B}{2}+\frac{B(1-p)}{2}-\epsilon$ regret. WLOG, we can evaluate its regret on $\theta_{0}$. On $\theta_{0}, \pi_{2+3}$ achieves $\frac{B p}{2}+\epsilon$ reward while $\pi_{0}$ achieves $B$. Thus $\pi_{2+3}$ incurs regret of $B-\left(\frac{B p}{2}+\epsilon\right)=\frac{B}{2}+\frac{B-B p}{2}-\epsilon=\frac{B}{2}+\frac{B(1-p)}{2}-\epsilon$ as desired. As discussed before, since the minimax regret policy achieves $\frac{B}{2}$, this is $\frac{B(1-p)}{2}-\epsilon$ more regret than optimal.</p>
<p>Lastly, we can also apply Theorem 1 to prove that REPAIRED achieves a minimax regret strategy in equilibrium. The intuition behind this corollary is that, since the utility functions of both teachers are the same, the approximate NE ensured by Theorem 1 is actually a true NE; therefore, the minimax theorem applies.
Corollary 2. Let $\bar{G}$ be the dual curriculum game in which both teachers maximize regret, so $U_{t}^{1}=U_{t}^{2}=U_{t}^{R}$. Further, suppose that $\left(\pi, \theta^{1}, \theta^{2}\right)$ is a Nash equilibrium of $\bar{G}$. Then, $\pi \in$ $\operatorname{argmin}<em A="A">{\pi</em>\left{\max } \in \Pi<em B="B">{\theta, \pi</em>\right)\right}\right}$.} \in \Theta, \Pi}\left{\operatorname{REGRET}^{\theta}\left(\pi_{A}, \pi_{B</p>
<p>Proof. Since $U_{t}^{1}=U_{t}^{2}=U_{t}^{R}$ the joint objective is $p U_{t}^{1}+(1-p) U_{t}^{2}=U_{t}^{R}$. Note that since $U_{t}^{1}=U_{t}^{2}, B=0$. Thus by Theorem $1\left(\pi, p \theta^{1}+(1-p) \theta^{2}\right)$ is a 0 -Nash Equilibrium of the base game with teacher objective $U_{t}^{R}$, thus by the minimax theorem, $\pi \in$ $\operatorname{argmin}<em A="A">{\pi</em>\left{\max } \in \Pi<em B="B">{\theta, \pi</em>\right)\right}\right}$ as desired.} \in \Theta, \Pi}\left{\operatorname{REGRET}^{\theta}\left(\pi_{A}, \pi_{B</p>
<h1>B Algorithms</h1>
<p>Although the PLR update rule for the level buffer of size $K$ in the case of unbounded training levels is described in [17], we provide the pseudocode for this update rule in Algorithm 2 for completeness. Given staleness coefficient $\rho$, temperature $\beta$, a prioritization function $h$ (e.g. rank), level buffer scores $S$, level buffer timestamps $C$, and the current episode count $c$ (i.e. current timestamp), the $P_{\text {replay }}$ update takes the form</p>
<p>$$
\begin{aligned}
P_{\text {replay }} &amp; =(1-\rho) \cdot P_{S}+\rho \cdot P_{C} \
P_{S} &amp; =\frac{h\left(S_{i}\right)^{1 / \beta}}{\sum_{j} h\left(S_{j}\right)^{1 / \beta}} \
P_{C} &amp; =\frac{c-C_{i}}{\sum_{C_{j} \in C} c-C_{j}}
\end{aligned}
$$</p>
<p>The pseudocode for Replay-Enhanced PAIRED (REPAIRED), the method described in Section 5, is presented in Algorithm 3.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">Algorithm</span><span class="w"> </span><span class="nt">2</span><span class="o">:</span><span class="w"> </span><span class="nt">PLR</span><span class="w"> </span><span class="nt">level-buffer</span><span class="w"> </span><span class="nt">update</span><span class="w"> </span><span class="nt">rule</span>
<span class="nt">Input</span><span class="o">:</span><span class="w"> </span><span class="nt">Level</span><span class="w"> </span><span class="nt">buffer</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\Lambda</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">of</span><span class="w"> </span><span class="nt">size</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">K</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">scores</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">timestamps</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">C</span><span class="err">\</span><span class="o">);</span><span class="w"> </span><span class="nt">level</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="o">);</span><span class="w"> </span><span class="nt">level</span><span class="w"> </span><span class="nt">score</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="w"> </span><span class="o">;</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span>
<span class="w">        </span><span class="nt">current</span><span class="w"> </span><span class="nt">episode</span><span class="w"> </span><span class="nt">count</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">c</span><span class="err">\</span><span class="o">)</span>
<span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(|</span><span class="err">\</span><span class="nt">Lambda</span><span class="o">|&lt;</span><span class="nt">K</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">    </span><span class="nt">Insert</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">into</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\Lambda</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="o">)=</span><span class="nt">S_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">C</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="o">)=</span><span class="nt">c</span><span class="err">\</span><span class="o">)</span>
<span class="nt">else</span>
<span class="w">    </span><span class="nt">Find</span><span class="w"> </span><span class="nt">level</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">minimal</span><span class="w"> </span><span class="nt">support</span><span class="o">,</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">\min</span><span class="w"> </span><span class="p">}</span><span class="o">=</span><span class="err">\</span><span class="nt">underset</span><span class="p">{</span><span class="err">\theta</span><span class="p">}{</span><span class="err">\arg</span><span class="w"> </span><span class="err">\min</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="nt">P_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{replay</span><span class="w"> </span><span class="p">}</span><span class="err">}</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="o">)</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">if</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S</span><span class="err">\</span><span class="nt">left</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">\min</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="nt">right</span><span class="o">)&lt;</span><span class="nt">S_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">then</span>
<span class="w">        </span><span class="nt">Remove</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta_</span><span class="p">{</span><span class="err">\min</span><span class="w"> </span><span class="p">}</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">from</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\Lambda</span><span class="p">}</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Insert</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">into</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="err">\</span><span class="nt">boldsymbol</span><span class="p">{</span><span class="err">\Lambda</span><span class="p">}</span><span class="err">\</span><span class="o">),</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">set</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="o">)=</span><span class="nt">S_</span><span class="p">{</span><span class="err">\theta</span><span class="p">}</span><span class="o">,</span><span class="w"> </span><span class="nt">C</span><span class="o">(</span><span class="err">\</span><span class="nt">theta</span><span class="o">)=</span><span class="nt">c</span><span class="err">\</span><span class="o">)</span>
<span class="w">        </span><span class="nt">Update</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">P_</span><span class="p">{</span><span class="err">\text</span><span class="w"> </span><span class="err">{replay</span><span class="w"> </span><span class="p">}</span><span class="err">}\</span><span class="o">)</span><span class="w"> </span><span class="nt">with</span><span class="w"> </span><span class="nt">latest</span><span class="w"> </span><span class="nt">scores</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">S</span><span class="err">\</span><span class="o">)</span><span class="w"> </span><span class="nt">and</span><span class="w"> </span><span class="nt">timestamps</span><span class="w"> </span><span class="err">\</span><span class="o">(</span><span class="nt">C</span><span class="err">\</span><span class="o">)</span>
<span class="w">    </span><span class="nt">end</span>
<span class="nt">end</span>
</code></pre></div>

<h2>Algorithm 3: REPAIRED</h2>
<p>Randomly initialize Protagonist, Antagonist, and Generator policies $\pi^{A}\left(\phi^{A}\right), \pi^{B}\left(\phi^{B}\right)$, and $\tilde{\theta}$ Initialize Protagonist and Antagonist PLR level buffers $\boldsymbol{\Lambda}^{A}$ and $\boldsymbol{\Lambda}^{B}$
while not converged do
Sample replay-decision Bernoulli, $d \sim P_{D}(d)$
if $d=0$ then
Teacher policy $\tilde{\theta}$ generates the next level, $\theta$
Set $\theta^{A}=\theta^{B}=\theta$
Collect trajectory $\tau^{A}$ on $\theta^{A}$ and $\tau^{B}$ on $\theta^{B}$ with stop-gradients $\phi_{\perp}^{A}, \phi_{\perp}^{B}$
Update $\tilde{\theta}$ with $\operatorname{REGRET}^{\theta}\left(\pi^{A}, \pi^{B}\right)$
else
PLR samples replay levels, $\theta^{A} \sim \boldsymbol{\Lambda}^{A}$ and $\theta^{B} \sim \boldsymbol{\Lambda}^{B}$
Collect trajectory $\tau^{A}$ on $\theta^{A}$ and $\tau^{B}$ on $\theta^{B}$
Update $\pi^{A}$ with rewards $\boldsymbol{R}\left(\tau^{A}\right)$, and $\pi^{B}$, with rewards $\boldsymbol{R}\left(\tau^{B}\right)$
end
Compute PLR score $S^{A}=\operatorname{score}\left(\tau^{A}, \tau^{B}, \pi^{A}\right)$
Compute PLR score $S^{B}=\operatorname{score}\left(\tau^{B}, \tau^{A}, \pi^{B}\right)$
Update $\boldsymbol{\Lambda}^{A}$ with $\theta^{A}$ using score $S^{A}$
Update $\boldsymbol{\Lambda}^{B}$ with $\theta^{B}$ using score $S^{B}$
end</p>
<p><img alt="img-5.jpeg" src="img-5.jpeg" /></p>
<p>Figure 7: Test maze environments for evaluating zero-shot transfer. An asterisk (*) next to the maze name indicates the maze is procedurally-generated, and thus each attempt corresponds to a random configuration of the maze.</p>
<h1>C Additional Experimental Results</h1>
<p>This section provides additional experimental results in MiniGrid and CarRacing environments. Note that we determine the statistical significance of our results using a Welch t-test [43].</p>
<h2>C. 1 Extended Results for MiniGrid</h2>
<p>Unlike the original maze experiments used to evaluate PAIRED in [10], we conduct our main maze experiments with a block budget of 25 blocks (reported in Section 6.1), rather than 50 blocks. Following the environment parameterization in [10], for a block budget of $B$, the teacher attempts to place $B$ blocks that act as obstacles when designing each maze level. However, the teacher can place fewer than $B$ blocks, as placing a block in a location already occupied by a block results in a no-opt. We found that PAIRED underperforms DR when both methods are given a budget of 50 blocks, a setting in which randomly sampled mazes exhibit enough structural complexity to allow DR to learn highly robust policies. Note that [10] used a DR baseline with a 25 -block budget. With a 50 -block budget, DR and all replay-based methods are able to fully solve almost all test mazes after around 500M steps of training, making UED of mazes with a 50-block budget too simple of a setting to provide an informative comparison among the methods studied.</p>
<h2>C.1.1 Mazes with a 25-block budget</h2>
<p>We report the results of evaluating policies produced by each method after 250M training steps on each of the zero-shot transfer environments in Figure 8 and Table 2. Examples of each test environment are presented in Figure 7. All replay-based UED methods lead to policies with statistically significantly ( $p&lt;0.05$ ) higher test performance than PAIRED, and PLR ${ }^{\perp}$, after 500M training steps, similarly improves over PLR when trained for an equivalent number of gradient updates (as replay rate is set to $0.5)$. Note that for PAIRED and REPAIRED, we evaluate the protagonist policy.</p>
<p>To provide a further sense of the training dynamics, we present the per-agent training returns for each method in Figure 9. Notably PAIRED results in antagonists that attain higher returns than the protagonist as expected. This dynamic takes on a mild oscillation, visible in the training return curve of the generator (adversary). As the protagonist adapts to the adversarial levels, the generator's return reduces, until the generator discovers new configurations that better exploit the relative differences between the two student policies. Notably, the adversary under REPAIRED seems to propose more difficult levels for both the protagonist and antagonist, while the resulting protagonist policy exhibits improved test performance, as seen in Figure 4.
<img alt="img-6.jpeg" src="img-6.jpeg" /></p>
<p>Figure 8: Zero-shot test performance on OOD environments when trained with a 25-block budget. The plots report the median and interquartile range of solved rates over 10 runs.</p>
<p>Table 2: Mean test returns and standard errors on zero-shot transfer mazes for each method using a 25-block budget. Results are aggregated over 100 attempts for each maze across 10 runs per method. Bolded figures overlap in standard error with the method attaining the maximum mean test return in each row.</p>
<table>
<thead>
<tr>
<th>Environment</th>
<th>DR</th>
<th>Minimax</th>
<th>PAIRED</th>
<th>REPAIRED</th>
<th>PLR</th>
<th>PLR^{⊥}</th>
<th>PLR^{⊥} (500M)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Labyrinth</td>
<td>$0.2 \pm 0.1$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.3 \pm 0.1$</td>
<td>$0.1 \pm 0.0$</td>
<td>$0.3 \pm 0.1$</td>
<td>$\mathbf{0 . 5} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 7} \pm \mathbf{0 . 1}$</td>
</tr>
<tr>
<td>Labyrinth2</td>
<td>$0.2 \pm 0.1$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.2 \pm 0.1$</td>
<td>$0.2 \pm 0.1$</td>
<td>$0.4 \pm 0.1$</td>
<td>$\mathbf{0 . 6} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 8} \pm \mathbf{0 . 1}$</td>
</tr>
<tr>
<td>LargeCorridor</td>
<td>$\mathbf{0 . 7} \pm \mathbf{0 . 1}$</td>
<td>$0.1 \pm 0.1$</td>
<td>$0.3 \pm 0.1$</td>
<td>$0.5 \pm 0.1$</td>
<td>$\mathbf{0 . 7} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 8} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 8} \pm \mathbf{0 . 1}$</td>
</tr>
<tr>
<td>Maze</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.2 \pm 0.1$</td>
<td>$0.3 \pm 0.1$</td>
<td>$\mathbf{0 . 6} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 5} \pm \mathbf{0 . 1}$</td>
</tr>
<tr>
<td>Maze2</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.1 \pm 0.1$</td>
<td>$0.1 \pm 0.1$</td>
<td>$\mathbf{0 . 4} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 4} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 5} \pm \mathbf{0 . 1}$</td>
</tr>
<tr>
<td>PerfectMaze</td>
<td>$0.3 \pm 0.1$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.4 \pm 0.1$</td>
<td>$0.4 \pm 0.1$</td>
<td>$\mathbf{0 . 6} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 5} \pm \mathbf{0 . 1}$</td>
</tr>
<tr>
<td>SixteenRooms</td>
<td>$0.9 \pm 0.0$</td>
<td>$0.1 \pm 0.1$</td>
<td>$0.7 \pm 0.1$</td>
<td>$0.9 \pm 0.1$</td>
<td>$\mathbf{1 . 0} \pm \mathbf{0 . 0}$</td>
<td>$0.8 \pm 0.1$</td>
<td>$\mathbf{1 . 0} \pm \mathbf{0 . 0}$</td>
</tr>
<tr>
<td>SixteenRooms2</td>
<td>$\mathbf{0 . 7} \pm \mathbf{0 . 1}$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>$\mathbf{0 . 6} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 5} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 7} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 7} \pm \mathbf{0 . 1}$</td>
</tr>
<tr>
<td>Mean</td>
<td>$0.4 \pm 0.0$</td>
<td>$0.0 \pm 0.0$</td>
<td>$0.2 \pm 0.0$</td>
<td>$0.4 \pm 0.0$</td>
<td>$0.5 \pm 0.1$</td>
<td>$\mathbf{0 . 6} \pm \mathbf{0 . 1}$</td>
<td>$\mathbf{0 . 7} \pm \mathbf{0 . 1}$</td>
</tr>
</tbody>
</table>
<p><img alt="img-7.jpeg" src="img-7.jpeg" /></p>
<p>Figure 9: Training returns for each participating agent in each method, when trained with a 25-block budget. Plots show the mean and standard error over 10 runs.
<img alt="img-8.jpeg" src="img-8.jpeg" /></p>
<p>Figure 10: Complexity metrics of environments generated by the teacher throughout training with a 25-block budget. Plots show the mean and standard error of 10 runs.</p>
<p>Additional complexity metrics tracked during training are shown in Figure 10. Alongside the number of blocks and shortest path length of levels seen during training, we also track solved path length and action complexity. Solved path length corresponds to the shortest path length from start position to goal in the levels successfully solved by the primary student agent (e.g. the protagonist in PAIRED). Action complexity corresponds to the Lempel-Ziv-Welch (LZW) complexity—a commonly used measure of string compressibility—of the action sequence taken during the primary student agent's trajectories. As expected, DR results in constant complexity for number of blocks and path length metrics. REPAIRED generates mazes with significantly greater complexity in terms of block count. The lower path lengths seen by REPAIRED suggest that it trains agents that more readily generalize to different path lengths, thereby pressuring the adversary to raise complexity in terms of block count. Further, given the high replay rates used, the REPAIRED adversary sees far fewer gradient updates with which to adjust its policy. As its shortest path lengths exceed that of PAIRED after adjusting proportionately by replay rate, foreseeably, over a longer period, the shortest path lengths generated by REPAIRED may meet or exceed that of PAIRED. In all cases, the action complexity reduces as the agent becomes more decisive, and we see that both PAIRED and REPAIRED lead to more decisive policies-as indicated by the simultaneously lower action complexity and greater level complexity in terms of higher block count (relative to DR) and, in the case of PAIRED, higher path length metrics. Lastly, it is interesting to note that while the random generator used by PLR produces levels of average complexity, the complexity of curated levels, as revealed in Figure 4, is significantly higher and, in the case of path length, steadily increasing.</p>
<h1>C.1.2 Mazes with a 50-block budget</h1>
<p>Similarly, Figures 12, 13, and 14 report the training dynamics and test performance of agents trained using each method with a 50-block budget for 500M steps. Figure 11 shows that DR and all replaybased methods are able to reach near perfect solve rates on most test mazes after 500M steps of training, with the exception of the Maze and PerfectMaze environments, where the test performances across methods are not markedly dissimilar, making the setting with a 50-block budget uninformative for assessing performance differences among these methods. The example mazes generated by each method, presented in Figure 15, shows that the larger block budget allows DR to sample mazes with greater structural complexity, leading to robust policies and diminishing the benefits of the UED methods studied. Therefore, in this work, we focus the main results for the maze domain on the more challenging setting with a 25 -block budget. Note that the impact of the block budget on test performance further highlights the importance of properly adapting the training distribution for producing policies exhibiting high generality-a problem that our replay-based UED methods effectively address, as demonstrated by the results for the 25 -block setting.
<img alt="img-9.jpeg" src="img-9.jpeg" /></p>
<p>Figure 11: Zero-shot test performance on OOD environments when trained with a 50-block budget. The plots show the median and interquartile range of solved rates over 10 runs.
<img alt="img-10.jpeg" src="img-10.jpeg" /></p>
<p>Figure 12: Test performance as a function of number of training steps with a 50-block budget (left), and test performance and complexity metrics as a function of number of PPO updates (right). The plots show the mean and standard error over 10 runs.
<img alt="img-11.jpeg" src="img-11.jpeg" /></p>
<p>Figure 13: Training returns for each participating agent in each method when training with a 50-block budget. Plots show the mean and standard error over 10 runs.</p>
<p><img alt="img-12.jpeg" src="img-12.jpeg" /></p>
<p>Figure 14: Complexity metrics of environments generated by the teacher throughout training with a 50-block budget. Plots show the mean and standard error of 10 runs.
<img alt="img-13.jpeg" src="img-13.jpeg" /></p>
<p>Figure 15: Example mazes generated by each method when using a 50-block budget.</p>
<h1>C. 2 Extended Results for CarRacing</h1>
<p>The training return plots for each agent, shown in Figure 16, reveal that PAIRED's generator (adversary) overexploits the relative advantages of the antagonist over the protagonist, leading to a highly suboptimal protagonist policy. In fact, as shown in the right-most plot of Figure 16, the resulting protagonist policies suffer such performance degradation from the adversarial curriculum that they can no longer even successfully drive on the original, simpler CarRacing tracks.
Additionally, we present per-track zero-shot transfer returns for the entire CarRacing-F1 benchmark after 5M training steps (equivalent to 40M environment interaction steps due to the usage of action repeat) in Table 3. Results report the mean and standard deviation over 100 attempts per track across 10 seeds. While DR acts as a strong baseline in terms of zero-shot generalization in this setting, $\mathrm{PLR}^{\perp}$ either attains the highest mean return, or matches the method achieving the highest return within standard error on all tracks. The mean performance of $\mathrm{PLR}^{\perp}$ across the full benchmark is statistically significantly higher ( $p&lt;0.001$ ) than that of all other methods. Notably, PAIRED sees poor results, likely due to the generator's ability to overexploit the differences between antagonist and protagonist to detrimental effect in this domain. We see that REPAIRED mitigates this effect to a degree, resulting in more competitive policies. Note that due to the high compute overhead of training the AttentionAgent ( 8.2 billion steps of training over a population 256 agents) [38], we resorted to evaluating its mean F1 performance using the pre-trained model weights provided by the authors with their public code release. As a result, we only have a single training run for AttentionAgent. This means we cannot reliably compute standard errors for this baseline, but we believe that showing the performance for a single training seed of AttentionAgent on the F1 benchmark alongside our methods, as done in Figure 6, nonetheless provides a useful comparison for further contextualizing the efficacy of our methods. This comparison highlights how, by only modifying the training curriculum, our methods produce policies with test returns exceeding that of AttentionAgent-which in contrast, uses a powerful attention-based policy and a much larger number of training steps.
As a further analysis of robustness, we inspect the minimum returns over 10 attempts per track, averaged over 10 runs per method. We present these results (mean and standard error) in Figure 17. $\mathrm{PLR}^{\perp}$ achieves consistently higher minimum returns on average for many of the tracks compared to the other methods, including on the challenging Russia and USA tracks. The fact that simply curating random levels, as done by $\mathrm{PLR}^{\perp}$, more reliably approaches a minimax regret policy than PAIRED and REPAIRED suggests that RL may not be an effective means for optimizing the PAIRED teacher.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>*Equal contribution. Correspondence to msj@fb.com and michael_dennis@berkeley.edu.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>
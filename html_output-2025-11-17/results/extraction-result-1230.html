<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-1230 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-1230</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-1230</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-28.html">extraction-schema-28</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <p><strong>Paper ID:</strong> paper-273812385</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2411.01396v1.pdf" target="_blank">Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning</a></p>
                <p><strong>Paper Abstract:</strong> Exploring unknown environments efficiently is a fundamental challenge in unsupervised goal-conditioned reinforcement learning. While selecting exploratory goals at the frontier of previously explored states is an effective strategy, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior. We propose"Cluster Edge Exploration"($CE^2$), a new goal-directed exploration algorithm that when choosing goals in sparsely explored areas of the state space gives priority to goal states that remain accessible to the agent. The key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior. In challenging robotics environments including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand, $CE^2$ demonstrates superior efficiency in exploration compared to baseline methods and ablations.</p>
                <p><strong>Cost:</strong> 0.014</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e1230.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e1230.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Ant-Maze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Ant Maze (Ant robot navigation in a maze)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A high-dimensional continuous locomotion/navigation environment where a 29-D ant robot must traverse a maze with narrow hallways and an added top-left room to reach distant goals; used to study long-horizon exploration and reachability-aware goal selection.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Ant-Maze</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Continuous 3D locomotion maze: an ant robot must navigate through narrow hallways and rooms (an extra room added to top-left) to reach distant goals; domain: robotics navigation / maze traversal.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Implicitly sparse / constrained by maze geometry (hallways and rooms) — not described by explicit graph statistics in paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>Maze physical size ≈ 6 x 8 meters; episode length 500 timesteps; state dimensionality 29; 32 training goals used (various positions).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CE2 (model-based GC-Dreamer with Go-Explore: π_G and π_E)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based goal-conditioned agent (GC-Dreamer backbone) with a world model (RSSM), a goal-conditioned policy π_G (Go-phase), an undirected exploration policy π_E (Explore-phase), a learned temporal-distance embedding D_t, and CE2's latent-space GMM clustering to select cluster-edge goals filtered by imagined exploration potential P_E(g).</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Goal-reaching success rate over training (sample efficiency); imagined exploration potential P_E(g) (expected V_E at end of imagined Go-phase); qualitative cluster coverage / frontier targeting.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Model-based goal-conditioned Go-Explore with reachability-aware goal selection (cluster-edge sampling + imagined evaluation).</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Paper reports that maze geometry (long corridors, rooms) creates long-horizon reachability challenges: selecting frontier goals that are not reachable by the current policy reduces exploration. CE2 addresses this by clustering latent states that are mutually reachable and sampling goals at cluster edges that are reachable, improving exploration; thus reachability (local connectivity / cluster boundaries) strongly affects exploration efficiency and policy learning.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Goal-conditioned policies tend to reliably reach frequently visited (within-cluster) states but struggle with arbitrary frontier goals; therefore, an effective policy structure combines a goal-conditioned (reachability-capable) controller for traversing to cluster-edge goals and a separate explorer policy for undirected exploration from those edges. Using imagined rollouts to estimate exploration potential aligns goal choice with the current policy's capability.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1230.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e1230.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Point-Maze</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Point Maze (2D point agent maze)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2D point-agent maze where the agent starts at bottom-left of a 10x10 maze and must reach the top-right corner within a limited number of timesteps; used as a baseline navigation task to evaluate CE2 and other Go-Explore variants.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Point-Maze</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2D point-mass navigation in a 10×10 maze (grid-like continuous plane) with start in bottom-left and goal in top-right; domain: simple maze navigation.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Maze-constrained connectivity (channels and obstacles) but not quantified in paper (no explicit graph statistics).</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>10 x 10 maze; episode length 50 timesteps; success threshold distance 0.15; training goals: 11 positions used for CE2-G experiments.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CE2 / GC-Dreamer (π_G, π_E)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based goal-conditioned Dreamer agent augmented with CE2: latent-space temporal-distance embeddings, GMM clustering to find cluster edges, selection of candidate edge goals, evaluation with imagined rollouts and explorer value V_E, then Go-Explore execution.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Goal-reaching success rate; sample efficiency over training; time-limited Go-phase (T) policy reachability.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>Paper states CE2 performs comparably to baselines on Point-Maze (no numeric percent provided).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Reachability-aware goal-conditioned policy paired with exploration policy (Go-then-Explore); i.e., a model-based planning + local exploration policy.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>CE2's cluster-edge goal selection is intended to match agent reachability with frontier states in maze-like structures; when frontier goals are reachable by π_G, exploration improves—otherwise exploration is reduced. Paper notes CE2 comparable to baselines in Point-Maze but does not report detailed topology metrics.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Short-horizon mazes emphasize that goal-conditioned controllers that can reliably reach within-cluster states suffice; CE2 still prefers cluster-edge goals that are within reach to trigger productive exploration rather than unattainable novelty.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1230.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e1230.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>Walker (long-distance locomotion)</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Walker environment (2-legged walker navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A 2D walker robot locomotion task used to test long-distance navigation/locomotion: evaluate ability to reach distant x-axis goals at various distances, emphasizing exploration and long-range reachability.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Walker</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>2D bipedal walker on a flat plane required to reach various x-axis target positions (distant goals) within episodes; domain: locomotion/navigation along one axis.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Implicit linear/continuous connectivity along x-dimension; not represented as an explicit graph in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>State & goal space 9-D; evaluation goals increased to 12 positions along x-axis (±13 to ±28) for thorough testing; training goals for CE2-G include ±13, ±16.</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CE2 / GC-Dreamer (π_G, π_E)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based goal-conditioned policy (Dreamer) with temporal-distance-based latent embeddings and CE2 GMM clustering selecting cluster-edge candidate goals, filtered by imagined explorer-value P_E(g), then Go-Explore execution.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Goal-reaching success rate across multiple target distances; sample efficiency to reach farther goals.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Model-based goal-conditioned policy with reachability-aware goal selection to progressively extend locomotion capability to farther goals.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Longer-distance (higher effective diameter along x) goals require staged expansion of reachable clusters; CE2 expands cluster coverage from cluster boundaries, enabling progressive ability to reach distant goals. Paper highlights reachability-constrained goal selection as important for learning long-range behaviors.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Policies benefit from staged/progressive expansion of reachable regions (cluster growth) rather than attempting unattainable distant goals early; model-based imagination to assess reachability (temporal distance) guides which goals a goal-conditioned policy should attempt.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e1230.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e1230.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of navigation in text-based games or text worlds, including graph-topology features of the environments (such as diameter, clustering coefficient, dead-ends, door constraints, connectivity), exploration efficiency metrics, and how these relate to agent performance and policy structure.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>3-Block Stacking</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>3-Block Stacking (robotic manipulation / configuration-space navigation)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A robotic manipulation benchmark where a two-fingered gripper must stack three blocks into a tower; exploration corresponds to navigating configuration space of object poses and gripper states, with hard (3-block) goals used for evaluation.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>3-Block Stacking</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Robotic tabletop manipulation: pick, push, and stack three blocks into a tower (hard goal); domain: manipulation / configuration-space exploration rather than spatial navigation between rooms.</td>
                        </tr>
                        <tr>
                            <td><strong>graph_diameter</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>clustering_coefficient</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>dead_ends_count</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_present</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>door_constraints_description</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>graph_connectivity</strong></td>
                            <td>Configuration-space connectivity is complex and multimodal; not quantified by graph metrics in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_size</strong></td>
                            <td>State & goal space 14-D (5 gripper dims + 9 block xyz positions); action space 4-D; success defined by L2 distance < 3 cm per block; only hard goals used (3 hard training goals for CE2-G).</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>CE2 / GC-Dreamer (π_G, π_E)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>Model-based goal-conditioned Dreamer agent with latent temporal-distance embeddings, GMM clustering to find cluster boundaries in configuration space, and CE2's cluster-edge Go-Explore selection evaluated via imagined exploration potential.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_metric</strong></td>
                            <td>Goal-reaching success rate (task completion), spatial exploration coverage metric devised by authors (projected triangle perimeter vs sum of heights) to visualize configuration-space coverage; sample efficiency.</td>
                        </tr>
                        <tr>
                            <td><strong>exploration_efficiency_value</strong></td>
                            <td>CE2-G achieves >90% success rate on the hard 3-block stacking task (explicitly reported); baselines (MEGA-G, PEG-G, GC-Dreamer) achieve <40% on this task as reported.</td>
                        </tr>
                        <tr>
                            <td><strong>success_rate</strong></td>
                            <td>>90% (CE2-G on 3-Block Stacking, reported); <40% for MEGA-G, PEG-G, GC-Dreamer (reported).</td>
                        </tr>
                        <tr>
                            <td><strong>optimal_policy_type</strong></td>
                            <td>Model-based goal-conditioned policy that prioritizes reachable cluster-edge subgoals (Go-phase) and then undirected exploration (Explore-phase); combining reachability-filtered goal selection and explorer policy yields best performance.</td>
                        </tr>
                        <tr>
                            <td><strong>topology_performance_relationship</strong></td>
                            <td>Although not described as an explicit graph, the paper shows that configuration-space topology (clusters of reachable states and their boundaries) matters: selecting exploration-start states at cluster edges adjacent to less-explored configuration regions yields more efficient and goal-relevant exploration than sampling rare/unreachable states; reachability-aware selection substantially improves performance on a difficult compositional manipulation task.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_across_topologies</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>topology_comparison_results</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>policy_structure_findings</strong></td>
                            <td>Effective policies separate goal-reaching capability (to bring agent to cluster-edge subgoals) from undirected exploration; CE2's clustering over latent reachability embeddings identifies useful subgoal structure in configuration space that the goal-conditioned policy can reliably reach before launching exploratory behavior.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning', 'publication_date_yy_mm': '2024-11'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Planning goals for exploration <em>(Rating: 2)</em></li>
                <li>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning <em>(Rating: 2)</em></li>
                <li>Go-Explore: a new approach for hard-exploration problems <em>(Rating: 2)</em></li>
                <li>L3P: World model as a graph: Learning latent landmarks for planning <em>(Rating: 2)</em></li>
                <li>Dream to Control: Learning behaviors by latent imagination <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-1230",
    "paper_id": "paper-273812385",
    "extraction_schema_id": "extraction-schema-28",
    "extracted_data": [
        {
            "name_short": "Ant-Maze",
            "name_full": "Ant Maze (Ant robot navigation in a maze)",
            "brief_description": "A high-dimensional continuous locomotion/navigation environment where a 29-D ant robot must traverse a maze with narrow hallways and an added top-left room to reach distant goals; used to study long-horizon exploration and reachability-aware goal selection.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Ant-Maze",
            "environment_description": "Continuous 3D locomotion maze: an ant robot must navigate through narrow hallways and rooms (an extra room added to top-left) to reach distant goals; domain: robotics navigation / maze traversal.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Implicitly sparse / constrained by maze geometry (hallways and rooms) — not described by explicit graph statistics in paper.",
            "environment_size": "Maze physical size ≈ 6 x 8 meters; episode length 500 timesteps; state dimensionality 29; 32 training goals used (various positions).",
            "agent_name": "CE2 (model-based GC-Dreamer with Go-Explore: π_G and π_E)",
            "agent_description": "Model-based goal-conditioned agent (GC-Dreamer backbone) with a world model (RSSM), a goal-conditioned policy π_G (Go-phase), an undirected exploration policy π_E (Explore-phase), a learned temporal-distance embedding D_t, and CE2's latent-space GMM clustering to select cluster-edge goals filtered by imagined exploration potential P_E(g).",
            "exploration_efficiency_metric": "Goal-reaching success rate over training (sample efficiency); imagined exploration potential P_E(g) (expected V_E at end of imagined Go-phase); qualitative cluster coverage / frontier targeting.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Model-based goal-conditioned Go-Explore with reachability-aware goal selection (cluster-edge sampling + imagined evaluation).",
            "topology_performance_relationship": "Paper reports that maze geometry (long corridors, rooms) creates long-horizon reachability challenges: selecting frontier goals that are not reachable by the current policy reduces exploration. CE2 addresses this by clustering latent states that are mutually reachable and sampling goals at cluster edges that are reachable, improving exploration; thus reachability (local connectivity / cluster boundaries) strongly affects exploration efficiency and policy learning.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "",
            "policy_structure_findings": "Goal-conditioned policies tend to reliably reach frequently visited (within-cluster) states but struggle with arbitrary frontier goals; therefore, an effective policy structure combines a goal-conditioned (reachability-capable) controller for traversing to cluster-edge goals and a separate explorer policy for undirected exploration from those edges. Using imagined rollouts to estimate exploration potential aligns goal choice with the current policy's capability.",
            "uuid": "e1230.0",
            "source_info": {
                "paper_title": "Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Point-Maze",
            "name_full": "Point Maze (2D point agent maze)",
            "brief_description": "A 2D point-agent maze where the agent starts at bottom-left of a 10x10 maze and must reach the top-right corner within a limited number of timesteps; used as a baseline navigation task to evaluate CE2 and other Go-Explore variants.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Point-Maze",
            "environment_description": "2D point-mass navigation in a 10×10 maze (grid-like continuous plane) with start in bottom-left and goal in top-right; domain: simple maze navigation.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Maze-constrained connectivity (channels and obstacles) but not quantified in paper (no explicit graph statistics).",
            "environment_size": "10 x 10 maze; episode length 50 timesteps; success threshold distance 0.15; training goals: 11 positions used for CE2-G experiments.",
            "agent_name": "CE2 / GC-Dreamer (π_G, π_E)",
            "agent_description": "Model-based goal-conditioned Dreamer agent augmented with CE2: latent-space temporal-distance embeddings, GMM clustering to find cluster edges, selection of candidate edge goals, evaluation with imagined rollouts and explorer value V_E, then Go-Explore execution.",
            "exploration_efficiency_metric": "Goal-reaching success rate; sample efficiency over training; time-limited Go-phase (T) policy reachability.",
            "exploration_efficiency_value": null,
            "success_rate": "Paper states CE2 performs comparably to baselines on Point-Maze (no numeric percent provided).",
            "optimal_policy_type": "Reachability-aware goal-conditioned policy paired with exploration policy (Go-then-Explore); i.e., a model-based planning + local exploration policy.",
            "topology_performance_relationship": "CE2's cluster-edge goal selection is intended to match agent reachability with frontier states in maze-like structures; when frontier goals are reachable by π_G, exploration improves—otherwise exploration is reduced. Paper notes CE2 comparable to baselines in Point-Maze but does not report detailed topology metrics.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "",
            "policy_structure_findings": "Short-horizon mazes emphasize that goal-conditioned controllers that can reliably reach within-cluster states suffice; CE2 still prefers cluster-edge goals that are within reach to trigger productive exploration rather than unattainable novelty.",
            "uuid": "e1230.1",
            "source_info": {
                "paper_title": "Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "Walker (long-distance locomotion)",
            "name_full": "Walker environment (2-legged walker navigation)",
            "brief_description": "A 2D walker robot locomotion task used to test long-distance navigation/locomotion: evaluate ability to reach distant x-axis goals at various distances, emphasizing exploration and long-range reachability.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "Walker",
            "environment_description": "2D bipedal walker on a flat plane required to reach various x-axis target positions (distant goals) within episodes; domain: locomotion/navigation along one axis.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Implicit linear/continuous connectivity along x-dimension; not represented as an explicit graph in the paper.",
            "environment_size": "State & goal space 9-D; evaluation goals increased to 12 positions along x-axis (±13 to ±28) for thorough testing; training goals for CE2-G include ±13, ±16.",
            "agent_name": "CE2 / GC-Dreamer (π_G, π_E)",
            "agent_description": "Model-based goal-conditioned policy (Dreamer) with temporal-distance-based latent embeddings and CE2 GMM clustering selecting cluster-edge candidate goals, filtered by imagined explorer-value P_E(g), then Go-Explore execution.",
            "exploration_efficiency_metric": "Goal-reaching success rate across multiple target distances; sample efficiency to reach farther goals.",
            "exploration_efficiency_value": null,
            "success_rate": null,
            "optimal_policy_type": "Model-based goal-conditioned policy with reachability-aware goal selection to progressively extend locomotion capability to farther goals.",
            "topology_performance_relationship": "Longer-distance (higher effective diameter along x) goals require staged expansion of reachable clusters; CE2 expands cluster coverage from cluster boundaries, enabling progressive ability to reach distant goals. Paper highlights reachability-constrained goal selection as important for learning long-range behaviors.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "",
            "policy_structure_findings": "Policies benefit from staged/progressive expansion of reachable regions (cluster growth) rather than attempting unattainable distant goals early; model-based imagination to assess reachability (temporal distance) guides which goals a goal-conditioned policy should attempt.",
            "uuid": "e1230.2",
            "source_info": {
                "paper_title": "Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning",
                "publication_date_yy_mm": "2024-11"
            }
        },
        {
            "name_short": "3-Block Stacking",
            "name_full": "3-Block Stacking (robotic manipulation / configuration-space navigation)",
            "brief_description": "A robotic manipulation benchmark where a two-fingered gripper must stack three blocks into a tower; exploration corresponds to navigating configuration space of object poses and gripper states, with hard (3-block) goals used for evaluation.",
            "citation_title": "here",
            "mention_or_use": "use",
            "environment_name": "3-Block Stacking",
            "environment_description": "Robotic tabletop manipulation: pick, push, and stack three blocks into a tower (hard goal); domain: manipulation / configuration-space exploration rather than spatial navigation between rooms.",
            "graph_diameter": null,
            "clustering_coefficient": null,
            "dead_ends_present": null,
            "dead_ends_count": null,
            "door_constraints_present": null,
            "door_constraints_description": "",
            "graph_connectivity": "Configuration-space connectivity is complex and multimodal; not quantified by graph metrics in the paper.",
            "environment_size": "State & goal space 14-D (5 gripper dims + 9 block xyz positions); action space 4-D; success defined by L2 distance &lt; 3 cm per block; only hard goals used (3 hard training goals for CE2-G).",
            "agent_name": "CE2 / GC-Dreamer (π_G, π_E)",
            "agent_description": "Model-based goal-conditioned Dreamer agent with latent temporal-distance embeddings, GMM clustering to find cluster boundaries in configuration space, and CE2's cluster-edge Go-Explore selection evaluated via imagined exploration potential.",
            "exploration_efficiency_metric": "Goal-reaching success rate (task completion), spatial exploration coverage metric devised by authors (projected triangle perimeter vs sum of heights) to visualize configuration-space coverage; sample efficiency.",
            "exploration_efficiency_value": "CE2-G achieves &gt;90% success rate on the hard 3-block stacking task (explicitly reported); baselines (MEGA-G, PEG-G, GC-Dreamer) achieve &lt;40% on this task as reported.",
            "success_rate": "&gt;90% (CE2-G on 3-Block Stacking, reported); &lt;40% for MEGA-G, PEG-G, GC-Dreamer (reported).",
            "optimal_policy_type": "Model-based goal-conditioned policy that prioritizes reachable cluster-edge subgoals (Go-phase) and then undirected exploration (Explore-phase); combining reachability-filtered goal selection and explorer policy yields best performance.",
            "topology_performance_relationship": "Although not described as an explicit graph, the paper shows that configuration-space topology (clusters of reachable states and their boundaries) matters: selecting exploration-start states at cluster edges adjacent to less-explored configuration regions yields more efficient and goal-relevant exploration than sampling rare/unreachable states; reachability-aware selection substantially improves performance on a difficult compositional manipulation task.",
            "comparison_across_topologies": false,
            "topology_comparison_results": "",
            "policy_structure_findings": "Effective policies separate goal-reaching capability (to bring agent to cluster-edge subgoals) from undirected exploration; CE2's clustering over latent reachability embeddings identifies useful subgoal structure in configuration space that the goal-conditioned policy can reliably reach before launching exploratory behavior.",
            "uuid": "e1230.3",
            "source_info": {
                "paper_title": "Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning",
                "publication_date_yy_mm": "2024-11"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Planning goals for exploration",
            "rating": 2,
            "sanitized_title": "planning_goals_for_exploration"
        },
        {
            "paper_title": "Maximum entropy gain exploration for long horizon multi-goal reinforcement learning",
            "rating": 2,
            "sanitized_title": "maximum_entropy_gain_exploration_for_long_horizon_multigoal_reinforcement_learning"
        },
        {
            "paper_title": "Go-Explore: a new approach for hard-exploration problems",
            "rating": 2,
            "sanitized_title": "goexplore_a_new_approach_for_hardexploration_problems"
        },
        {
            "paper_title": "L3P: World model as a graph: Learning latent landmarks for planning",
            "rating": 2,
            "sanitized_title": "l3p_world_model_as_a_graph_learning_latent_landmarks_for_planning"
        },
        {
            "paper_title": "Dream to Control: Learning behaviors by latent imagination",
            "rating": 1,
            "sanitized_title": "dream_to_control_learning_behaviors_by_latent_imagination"
        }
    ],
    "cost": 0.01422975,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning
3 Nov 2024</p>
<p>Yuanlin Duan yuanlin.duan@rutgers.edu 
Rutgers University</p>
<p>Guofeng Cui 
Rutgers University</p>
<p>He Zhu 
Rutgers University</p>
<p>Exploring the Edges of Latent State Clusters for Goal-Conditioned Reinforcement Learning
3 Nov 2024102C0EAABB670B99E8FC8B813AF0FB8BarXiv:2411.01396v1[cs.LG]
Exploring unknown environments efficiently is a fundamental challenge in unsupervised goal-conditioned reinforcement learning.While selecting exploratory goals at the frontier of previously explored states is an effective strategy, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior.We propose "Cluster Edge Exploration" (CE 2 ), a new goal-directed exploration algorithm that when choosing goals in sparsely explored areas of the state space gives priority to goal states that remain accessible to the agent.The key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior.In challenging robotics environments including navigating a maze with a multi-legged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand, CE 2 demonstrates superior efficiency in exploration compared to baseline methods and ablations.38th Conference on Neural Information Processing Systems (NeurIPS 2024).</p>
<p>Introduction</p>
<p>In recent years, Goal-Conditioned Reinforcement Learning (GCRL) (Andrychowicz et al. (2017)) has emerged as a powerful paradigm for training agents to accomplish diverse tasks in complex and dynamic environments.GCRL enables agents to learn goal-directed behaviors, allowing them to achieve specific objectives in a flexible and adaptive manner.However, a central challenge in GCRL lies in guiding agents to effectively explore their environment during training.The exploration problem in GCRL can be viewed as the task of setting goals for the agent during training to guide the agent's environment navigation to collect exploratory data that improves its learning process.In this paper, we address this critical challenge by proposing a novel strategy for selecting explorationinducing goals in GCRL.</p>
<p>Because goal-conditioned policies excel at reaching states encountered frequently during training, a simple strategy is setting goals in less-visited areas of the state space to broaden the range of reachable states.However, throughout training, goal-conditioned policies may encounter difficulties in reaching arbitrary goals.For example, when instructed to navigate to an unexplored section of a maze environment, a novice agent might instead revisit a previously traversed area that provides low exploration value.To address this shortcoming, the environment exploration procedure must set up additional mechanisms to filter out unreachable goals.A common strategy in the literature is to select goals at the frontier of previously explored states and launch an exploration phase immediately after these goals are achieved, adhering to a Go-Explore principle (Ecoffet et al. (2019)).For example, Skewfit (Pong et al. (2019)) estimates state densities and selects goals at the frontier from the replay buffer in inverse proportion to their density.Similarly, MEGA (Pitis et al. (2020)) uses kernel density estimates (KDE) of state densities and selects frontier goals with low density from the replay buffer.</p>
<p>However, precisely identifying the frontier of known states can be challenging with these heuristics.Even once the frontier is identified, the policy during training may still have limited capability of reaching rare goals on the frontier, resulting in reduced exploratory behavior.</p>
<p>To address the aforementioned challenge, we propose a new goal-directed exploration algorithm, CE 2 (short for "Cluster Edge Exploration").When choosing goals in sparsely explored areas of the state space, CE 2 gives priority to goal states that remain accessible to the agent.For this purpose, our key idea is clustering to group known states that are easily reachable from one another by the current policy under training, and traversing to states holding significant exploration potential on the boundary of these clusters before doing exploratory behavior.In this way, our method accounts for the capability of the current policy for exploratory goals.First, a state cluster likely represents part of the state space where the training policy is familiar with.Second, given the easy accessibility of states within each cluster by the training policy, the agent's capability extends to reaching states even at cluster boundaries.Moreover, less explored regions naturally reside adjacent to the periphery of state clusters.This Go-Explore strategy enables the agent to progressively broaden the coverage of each state cluster to effectively explore a novel environment.We instantiate CE 2 in the context of model-based GCRL, demonstrating how learned world models can facilitate clustering environment states that are easily reachable from one another by the training policy in a latent space.We validate the effectiveness of CE 2 in challenging robotics scenarios, including navigating a maze with a multilegged ant robot, manipulating objects with a robot arm on a cluttered tabletop, and rotating objects in the palm of an anthropomorphic robotic hand.In each scenario, CE 2 exploration results in more efficient training of adaptable GCRL policies compared to baseline methods and ablations.</p>
<p>Problem Setup and Background</p>
<p>Our work focuses on the exploration problem in unsupervised goal-conditioned reinforcement learning (GCRL) settings.In this section, we set up notation and preliminary concepts.</p>
<p>GCRL.A goal-conditioned Markov decision process (MDP) is defined by the tuple (S, A, G, T , η) where the state space S defines the set of all possible agent's observations into the environment, the action space A defines all possible actions that the agent can take in each state, G is the set of all possible goals that the agent may aim to achieve in the environment, and the transition function T describes the probability of transitioning from one state to another given an action.It is defined as T (s ′ |s, a), where s ′ ∈ S is the next state, s ∈ S is the current state, and a ∈ A is the action taken.η : S → G is a tractable mapping function that maps a state to a specific goal.A goal-conditioned π(a|s, g) represents the agent's strategy for selecting actions based on states and goal commands, indicating the probability of taking action a in state s given goal command g ∈ G.In this paper, for ease of presentation, we assume S = G and η is an identify function.Our goal is to develop agents capable of unsupervised exploration when dropped into an unknown environment.During the unsupervised exploration stage, there are no predefined tasks or goals.The agent sets its own goal command g ∈ G as it explores the environment.Following this exploration phase, a successful agent should be able to navigate to a wide range of previously unknown goal states in the environment upon goal commands.</p>
<p>Model-based GCRL.Model-based reinforcement learning (MBRL) is an approach where an agent learns a model of the environment's dynamics to predict future states, enabling more efficient policy learning.Fig. 1 shows the general MBRL framework.We use the world model structure M of Dreamer (Hafner et al. (2019a(Hafner et al. ( ,b, 2020(Hafner et al. ( , 2023))) to learn real environment dynamics as a recurrent state-space model (RSSM).We provide a detailed explanation of the network architecture and working principles of the RSSM in Appendix C.1.Particularly, we consider GC-Dreamer (goal-conditioned Dreamer) as a baseline.In GC-Dreamer, the goal-conditioned agent π G (a|s, g) samples goal commands g ∈ G from a given environment goal distribution p g to collect trajectories in the real world.These trajectories are used to train the world model M , and subsequently, π G is trained on imagined rollouts generated by M , with these two steps run in alternation.The reward function used to train π G is determined by a temporal distance network D t (see below).</p>
<p>Go-Explore.In unsupervised GCRL, the goal distribution p g is only revealed at test time."Go-Explore" (Ecoffet et al. (2019); Pislar et al. (2021); Tuyls et al. (2022); Hu et al. (2023)) is a popular mechanism tailored for long-term GCRL scenarios that require extensive exploration.The Go-Explore methodology splits each training episode into two distinct phases: the "Go-phase" and the "Explore-phase".In the "Go-phase", the agent is guided to an "interesting" goal g (Pong et al. (2019); Pitis et al. (2020)) (e.g., states rarely encountered in the replay buffer) by the GCRL policy π G , reaching a final state s T .Subsequently, the "Explore-phase" kicks in, with an undirected exploration policy π E taking over from s T for the remaining timesteps.This exploration policy is optimized to maximize an intrinsic exploration reward (Bellemare et al. ( 2016 2020)) (e.g., to explore less familiar areas of the environment that the world models have not adequately learned).</p>
<p>Recently, Go-Explore has been integrated with model-based unsupervised GCRL (Mendonca et al. (2021); Hu et al. (2023)), as depicted in Fig. 1.In addition to the goal-conditioned policy π G (a|s, g), an exploration policy π E (s) is introduced into the model-based GCRL framework.The agent's training process involves learning the following components:</p>
<p>World Model:
M (s t |s t−1 , a t−1 )
Exploration policy:
π E (s t ) Goal Reaching policy: π G (s t , g) Exploration value: V E (s t ) Goal Reaching value: V G (s t , g)
where both π G and π E are trained using the model-based actor-critic algorithm in Dreamer (Hafner et al. (2020)).They are entirely trained with the imagined rollouts of the world model M to maximize the accumulated rewards t r G t and t r E t , respectively.The explorer reward r E encourages exploration by leveraging the Plan2Explore (Sekar et al. (2020)) disagreement objective, which motivates the agent to seek states that induce discrepancies among an ensemble of world models.In contrast, the goal-reaching reward r G is driven by the self-supervised temporal distance objective D t (Mendonca et al. (2021)), which reinforces the policy to minimize the action steps required to transition from the current state s to a sampled goal state g in an imagined rollout, i.e., r G (s, g) = −D t (Ψ(s), Ψ(g)).The temporal distance network D t predicts the anticipated number of action steps needed to transition from s to g.It is trained by extracting pairs of states s t and s t+k from an imagined rollout generated by M and predicting the distance k as shown in Equation 1 where H is the total length of the imagined rollout:
D t Ψ(s t ), Ψ(s t+k ) ≈ k/H
(1) Here, Ψ is a learned function for state embeddings in the world model (we assume S = G in the paper).Further details on the training procedure of D t can be found in Appendix C.2. CE 2 aims to address the core challenge in the Go-Explore mechanism: how do we select an interesting goal command g at the frontier of known states with high exploration potential and effectively guide the agent to g?</p>
<p>State Cluster Edge Exploration</p>
<p>The major limitation in existing Go-Explore approaches, such as those described in (Pong et al. (2019); Pitis et al. (2020)) is that the policy under training can struggle to reach heuristically chosen rare goals at the frontier of known states (Hu et al. (2023)).This difficulty arises because the goal commands are selected without a systematic method to filter out unachievable goals for the agent, leading to diminished exploratory behavior.In CE 2 , when choosing goals in sparsely explored areas of the state space in the "Go-phase", our method gives priority to goal states that remain accessible.For this purpose, the key idea is clustering to group states that are easily reachable from one another by the current policy under training in a latent space, and selecting states holding significant exploration potential on the boundary of these clusters as the "interesting" goals to explore.In Sec.3.1, we discuss how to learn a latent space that can represent the reachability relationships between environment states.In Sec.3.2, we explain how this latent space can be used to cluster states in the replay buffer that are easily reachable from one another.In Sec.3.3, we demonstrate how the agent can be brought to interesting states on the boundary of latent state clusters to effectively explore its environment.</p>
<p>Latent Space Learning</p>
<p>Typically, during the learning process of a world model M as a neural network, an essential step involves encoding states from the original observation space into a latent space using an encoder, which can then be decoded back to the original observation space by a decoder.This latent space is subsequently used to learn the dynamic model of the real environment Hafner et al. (2019aHafner et al. ( , 2020)).</p>
<p>In CE 2 , we additionally require the latent space can express the temporal distance between different states.In other words, we aim for the distances between various states in the latent space to represent the number of steps required to transition from one another in the real environment (after decoding) by the training policy.Therefore, the loss function of training the latent space in CE 2 comprises two components.The first component is the reconstruction loss L rec , akin to the latent space loss function in Dreamer framework (Hafner et al. (2019a(Hafner et al. ( , 2020))).It captures the association between the latent space and the re-decoding to the observation space, along with predicting dynamic transition in the latent space.We introduce a second loss term L dt that leverages the temporal distance network D t in Equation 1 to guide the learning of the latent space structure.For any pair of states (s1, s2) sampled from the replay buffer, the L dt loss function is formulated as follows (Ψ is a learned function for state embeddings in the world model):
L dt = (∥Ψ(s 1 ) − Ψ(s 2 )∥ 2 2 − 1 2 (D t (Ψ(s 1 ), Ψ(s 2 )) + D t (Ψ(s 2 ), Ψ(s 1 )))) 2
(2)
L latent = L rec + L dt
(3) We use the loss function L latent to supervise the training of the latent space.The trained latent space provides the agent with a deeper understanding of the real environment, where states that are easily reachable from one another in the real environment are closer in proximity within the latent space.</p>
<p>Latent State Clustering</p>
<p>To identify the frontier of known states, CE 2 conducts state clustering to group states in the replay buffer.States that are easily reachable from one another are classified in the same cluster in the latent space by Gaussian Mixture Models (GMMs), based on the temporal distances between the encoded states.GMMs are probabilistic models that assume all data points are generated by a mixture of a finite number of Gaussian Distributions.We initialize the Gaussian models in the latent space with N c trainable latent centroids c = {c 1 , . . ., c Nc } and a shared variance σ, where N c represents the desired number of clusters.These N c latent centroids are initialized by applying the Farthest Point Sampling (FPS) algorithm (Eldar et al. (1997)) to select a representative subset of states from a batch of data sampled from the replay buffer.We provide a detailed description of the FPS algorithm in Appendix G.1.After initialization, we optimize the clustering model by maximizing the Evidence Lower Bound (ELBO) iteratively on sampled batches from the replay buffer with a uniform prior p(c) to scatter out the latent centroids (Zhang et al. (2021)):
log p(z = Ψ(s)) ≥ E q(c|Ψ(s)) [log p(Ψ(s)|c)] − D KL (q(c|Ψ(s))||p(c))(4)
where p and q are represented as Gaussian distributions within the GMMs.q(c|Ψ(s)) is the postior distribution over c (the clusters) given an encoded state Ψ(s).log p(Ψ(s)|c) is the distribution donating the probability of the encoded state Ψ(s) in cluster c. p(c) is the prior distribution of the weight of clusters in GMMs.For each round of optimization, we increase the probability of the sampled batches in GMMs by updating the weight of each cluster c in GMMs and the mean and variance of them.</p>
<p>Exploring the Boundaries of Latent State Clusters</p>
<p>Assuming we have already trained N c state clusters in the latent space, each representing part of the state space where the goal-conditioned policy under training is familiar with, how can we utilize these state clusters to plan an exploration strategy?CE 2 selects goal states at the edges of these latent state clusters for exploration because (1) less explored regions are naturally adjacent to these boundaries, and (2) given the easy accessibility between states within each cluster by the training policy, the agent's capability extends to reaching states even at the cluster boundaries.</p>
<p>We outline our exploration algorithm in Algorithm 1.At line 3, it samples N candidate latent states as S candidate from GMMs.A higher sampling quantity ensures sampling from more states at the edges of the clusters.We set N candidate = 1000 in CE 2 .We compute the total probability of each latent state ŝ ∈ S candidate in the Gaussian mixture model, given by the formula:
p(ŝ) = Nc i=1 β i N (ŝ|c i , σ)(5)
In this formula, β i are the mixture weights satisfying β i ≥ 0 and Nc i=1 β i = 1, N (ŝ|c i , σ) represents the i-th Gaussian distribution with mean c i and the shared standard deviation σ.At line 4, we select N edge latent states with the lowest total probability from S candidate by Equation 5 as a set S edge .Intuitively, these states reside on the edges of the latent state clusters and, therefore, induce a set of a goal commands G edge = {η(f D (ŝ))|ŝ ∈ S edge } that may be used for the "Go-phase" for Go-Explore, where f D is the state decoder and η is the goal mapping function.</p>
<p>Algorithm 1 Cluster Edge Exploration(CE 2 ) 1: D exp ← {} 2: for episode i = 1 to N τ do 3:
S candidate ← Sample N candidate points from GM M 4:
G edge ← N edge states in S candidate with the smallest total probability based on Equation 5. 5:
g E ← argmax g∈Gedge P E (g) through imagination with M 6: τ ← GO-EXPLORE(g E , π G , π E ) 7: D exp ← D exp ∪ τ
However, randomly picking a goal command from G edge overlooks whether the policy can exactly navigate the agent to the sampled goal in the real environment.</p>
<p>Although determining the exact outcome of the policy without execution is impractical, similar to PEG (Hu et al. (2023)), we can leverage the world model to provide an approximation of the exploration potential P E (g) of a goal command g:
pπ G (•|•,g)(τ ) = p(s 0 )<a href="6"> T t=1 M (s t |s t−1 , a t−1 )π G (a t−1 |s t−1 , g)</a>P E (g) = E p π G (•|•,g)(s T ) [V E (s T )] ≈ 1 K K k V E (s k T ) where s k T ∼ pπ G (•|•,g)(τ )(7)
In Equation 6, we simulate the "Go-phase" of Go-Explore over the world model M .We set each state from G edge as the goal command g for the goal-conditioned policy π G to run over M and denote s T as the final state of the resulting imagined trajectory (here pπ G (•|•,g)(τ ) essentially induces the imagined trajectory distribution over the world model).In our implementation, we set the length of "Go-phase" T to half of the maximum episode length for all environments.The time limits for both the Go and Explore phases during real environment exploration are also set to this value.We use the learned exploration value function V E of explorer π E to estimate the exploration value of s k T , the final state of k-th imagined trajectory.We average the estimated exploration potential over K such imagined trajectories.</p>
<p>At line 5 in Algorithm 1, after selecting the exploration target g E with the highest exploration potential P E from the latent cluster boundaries, we start the Go-Explore procedure in the real environment by executing the goal-conditioned policy π G to approach g E as closely as possible limited in T timesteps, followed by launching the explore policy π E for exploration limited in T E timesteps.Update π E in imagination with M to maximize r E We depict the main learning algorithm of CE 2 in Algorithm 2. Recall that the learning objective is to train an agent that can achieve diverse goals revealed to it only at test time.Accordingly, in this algorithm at line 7, the data D exp collected to train the world model M is generated solely by our Go-Explore strategy as outlined in Algorithm 1.At line 6, we periodically update the centroids of the latent clusters again using the FPS algorithm (Eldar et al. (1997)) from a batch of latest trajectories from the replay buffer.This ensures that the candidate goal states selected for exploration are indeed located at the boundaries of key state regions.At line 10, we train the clustering model using data from the replay buffer in each round.This ensures that latent state clustering and the agent's goal-reaching capability are kept synchronized.Update π E in imagination with M to maximize r E</p>
<p>The Main Algorithm</p>
<p>Algorithm 3 The main training algorithm for CE
2 -G 1: Input: π G , π E , G, World Model M , GMM, r G , r E ,
In our experiment, we also designed a variant of CE 2 in Algorithm 3, called CE 2 -G.This algorithm is given the environment goal distribution p g at training time.The main idea is to progressively expand the scope of exploration around the possible trajectories leading to the environment goals.In this algorithm, the replay buffer additionally includes D egc the trajectories sampled by π G conditioned on the environment goals in p g .We only use D egc to initialize and train latent state clusters.In this way, the agent is encouraged to prioritize exploration starting from the edges of latent state clusters along the trajectories towards the goal states in p g .CE 2 -G can be considered as learning policies and world models specific to a given goal distribution.</p>
<p>Experiments</p>
<p>Our experiments evaluate CE 2 over goal-reaching tasks that demand significant exploration to solve.We aim to address the following questions: (1) Does CE 2 lead to improved exploration and goalreaching performance?(2) How does CE 2 exploration qualitatively differ from those in previous goal-directed exploration methods?(3) Which components of CE 2 are crucial to its success?</p>
<p>Benchmarks</p>
<p>We evaluate our method on six hard exploration goal-conditioned RL tasks: Point-Maze, Ant-Maze, Walker, 3-Block Stacking, Block Rotation and Pen Rotation.Point-Maze: A blue point is placed at the bottom left of the maze and be trained to explore the structure of maze.Ant-Maze: An ant robot must master intricate four-legged locomotion behaviors and maneuver through narrow hallways.Walker: A 2-legged robot needs to learn how to control its leg joints to walk on a flat plane to move forward or backward.In 3-Block Stacking, a robot arm with a two-fingered gripper operates on a tabletop with three blocks.The goal is to stack the blocks into a tower configuration.The agent needs to learn pushing, picking, and stacking, as well as discovering intricate action paths to accomplish the task within the environment.Previous solutions have relied on methods like demonstrations, curriculum learning, or extensive simulator data, highlighting the task's difficulty.The Gymnasium Block Rotation and Pen Rotation tasks involve manipulating a block and a pen, respectively, to achieve a random target rotation along all axes.Pen Rotation is particularly challenging due to the pen's thinness, requiring precise control to prevent it from dropping.For evaluation, we use the most challenging goals, such as the farthest goal locations, in Point Maze, Ant Maze, Walker, and 3-Block Stacking.In the other two environments, we utilize random goals as defined by the environment.For more settings and information about the environments, please refer to the Appendix E.</p>
<p>Baselines</p>
<p>In the unsupervised GCRL setting, we compared CE 2 with state-of-the-art methods based on the Go-Explore strategy, which has demonstrated high efficiency in this setup: PEG (Hu et al. ( 2023)) and MEGA (Pitis et al. ( 2020))1 .MEGA commands the agent to rarely seen states at the frontier by using kernel density estimates (KDE) of state densities and chooses low-density goals from the replay buffer.PEG selects goal commands to guide an agent's goal-conditioned policy toward states with the highest exploration potential given its current level of training.This potential is defined as the expected accumulated exploration reward during the Explore-phase.</p>
<p>In scenarios where environment goal distributions are available to the agents, we compare CE 2 -G with GC-Dreamer (illustrated in Sec. 2), PEG-G, MEGA-G and L3P.Similar to CE 2 -G, PEG-G and MEGA-G augment GC-Dreamer with the PEG and MEGA Go-Explore strategies, respectively.In these methods, the replay buffer D contains not only trajectories sampled by the goal-conditioned policy π G commanded by environment goals but also exploratory trajectories sampled using the corresponding Go-Explore strategies.L3P trains a latent space using temporal distances and performs clustering in this latent space, similar to CE 2 -G.However, L3P does not employ a Go-Explore strategy.Instead, it constructs a directed graph with cluster centroids as nodes and utilizes online planning with graph search to determine subgoals for task execution.   of the state space to encourage exploration, unlike CE 2 , it lacks a systematic method to filter out unachievable goals for the agent, which can result in inefficient exploration.Theoretically, PEG can induce more exploration than MEGA because it can sample goal commands as any state within the state space to initiate exploration, including those beyond the frontier of known states in the replay buffer.However, because a learned world model is typically unfamiliar with rarely observed states, it may select goal commands that appear to have high exploration potential in the model but perform poorly in the real environment as shown in Fig. 4.</p>
<p>Results</p>
<p>CE 2 -G Results.Fig. 5 depicts the mean learning performance of all the tools in terms of the agent's goal-reaching success rate averaged over 5 random seeds when the environment goal distribution is revealed to the agent at training time.GC-Dreamer is the only tool that lacks a Go-Explore phase, which may limit its exploration potential.Even so, it can sometimes outperform MEGA-G and PEG-G (see block rotation and pen rotation).This indicates that, without reasonably accounting the agent's capability to reach selected goal commands, the Go-Explore strategy does not always guarantee improved exploration.Suboptimal goal-setting during the "Go-phase" can even hinder exploration (see 3 block stacking).Notably, for the challenging 3-block stacking task, CE 2 -G achieves a high success rate exceeding 90%.In comparison, MEGA-G, PEG-G and GC-Dreamer only achieve less than 40% success rates.Refer to Appendix H.4 for full results of CE 2 -G.</p>
<p>Exploration Process</p>
<p>Fig 6 shows the evolution of state clusters (learned in a latent space) during the training process for Ant Maze (in different colors).The red points represent the selected goal commands used to induce exploration.We observe that the self-directed exploration goals set by CE 2 improve progressively as the agent's capabilities increase, consistently targeting the cluster edges that require further exploration and are within the agent's reach.We compare the exploration targets generated by CE 2 with those produced by the MEGA and PEG approaches throughout the training process in the Ant Maze environment in Appendix H.2.</p>
<p>Ablation Study</p>
<p>In the ablation experiment, our goal is to determine the individual contributions of each component to our method's overall performance.The "Go-phase" of the Go-Explore procedure in CE 2 consists The superior performance of CE 2 -noPEG compared to both MEGA and MEGA-wPEG further reinforces this.Block Rotation is the only environment where CE 2 -noPEG outperforms CE 2 .In this environment, the CE 2 agent often pursues states where the block falls from the palm, due to their "high" exploration potential determined by the exploration policy value functions.In contrast, CE 2 -noPEG agent explores the state space more evenly, gaining more in-hand manipulation skills, which is crucial for achieving the block-rotation goals revealed at test time.MEGA achieves similar or better performance compared to MEGA-wPEG, indicating that PEG's effectiveness relies on the quality of the candidate goal set.The exploratory goals sampled from the lowest-density regions in the replay buffer might be beyond the agent's capability, leading PEG to assess the true exploration potential of the candidate goals inaccurately.</p>
<p>We also conducted experiments in the CE 2 with different numbers of latent state clusters N c and observed that CE 2 is insensitive to this hyperparameter.See Appendix H.5 for more discussion.</p>
<p>Related Work</p>
<p>Our method addresses the challenging and inefficient exploration problem inherent in goalconditioned reinforcement learning (RL) settings with sparse rewards, commonly used in robotics and control fields (Ghosh et 2020)).</p>
<p>In addition to reshaping the exploration reward function, goal-directed exploration represents a widely employed strategy that sets exploration goals distinct from the final task objective.Essentially, this approach aims to select goals that present challenges to the current policy while remaining achievable.Prior works have proposed various methods to generate goals for goal-directed exploration.(Zhang et al. ( 2019)) introduce a more efficient exploration methodology known as Go-Explore.This approach initially employs the goal-conditioned policy (Go-phase), followed by the rollout of the exploration policy from the terminal state of the goal-conditioned phase (Explore-phase).</p>
<p>Go-Explore facilitates exploration initiation from a state area accessible by the current capabilities of the goal-conditioned policy.</p>
<p>PEG (Hu et al. (2023)) proposes computing the exploration potential by simulating Go-Explore trajectories using a world model to identify goals characterized by elevated average exploration rewards in the Explore-phase.This metric incorporates anticipated exploration rewards of the Explore-phase, providing an advantage for Go-Explore.However, the goals sampled for evaluating this exploration potential metric in PEG are drawn from a distribution updated by the MPPI method (Williams et al. (2015); Nagabandi et al. (2020)) directly in the observation space.L3P (Zhang et al. (2021)) employs temporal distance to train a latent space, facilitating clustering within this space to delineate key state areas based on reachability.Our approach proposes exploration from the periphery of these key state regions, aiming to balance exploration of unknown territories while constraining exploration starting points to the edges of key state regions, thus avoiding meaningless exploration from widely sampled points from observation space.See Appendix A, B for more related work discussion.</p>
<p>Conclusion</p>
<p>We present CE 2 , a novel Go-Explore mechanism designed to tackle hard exploration problems in unsupervised goal-conditioned reinforcement learning tasks.While CE 2 outperforms prior exploration approaches in challenging robotics scenarios, the requirement to learn state clusters to identify frontier states and the reliance on world models to determine exploration potential introduce nontrivial computational costs.Exploring whether CE 2 's Go-Explore strategy can be effectively applied to model-free GCRL settings remains an interesting avenue for future work.</p>
<p>Reproducibility Statement</p>
<p>The codebase of our method is provided on https://github.com/RU-Automated-Reasoning-Group/CE2.For hyperparameter settings and baselines' pesudocode, please refer to Appendix F and G.3.</p>
<p>Appendix A Discussion</p>
<p>Why does CE 2 perform better than the original Go-Explore mechanism?</p>
<p>Our algorithm, CE 2 , tackles the core challenge in the Go-Explore mechanism: how to select an exploration-inducing goal command g and effectively guide the agent to g? Previous approaches, such as MEGA, set exploratory goals at rarely visited regions of the state space.However, in these approaches, the policies under training may have limited capability of reaching the chosen rare goals, leading to less effective exploration.Our contribution is a novel goal selection algorithm that prioritizes goal states in sparsely explored areas of the state space, provided they remain accessible to the agent.This is the key factor in why CE 2 outperforms the MEGA and PEG baselines in our benchmark suite in Fig. 3.As visualized in Fig. 11 in the appendix for the Ant Maze environment, CE 2 enhances exploration efficiency by consistently setting exploratory goals within the current policy's capabilities.In contrast, MEGA and PEG often set goals that are unlikely to be reachable by the current agent.</p>
<p>Why don't we choose the original Go-Explore as a direct baseline?</p>
<p>As discussed above, the core challenge in the Go-Explore mechanism lies in selecting goal states that effectively trigger further exploration upon being reached.However, the original Go-Explore method (Ecoffet et al. (2019)) does not prescribe a general goal selection method, instead opting for a hand-engineered novelty bonus for each task (e.g.task-specific pseudo-count tables).CE 2 is more related to recent instantiations of Go-Explore that automatically selects exploration-inducing goals in less-visited areas of the state space to broaden the range of reachable states, e.g.MEGA and PEG.Therefore, we compare our method with these tools instead of Ecoffet et al. (2019) in environments where these tools are applicable, to evaluate the strength of our goal selection method.</p>
<p>Why our clustering algorithm does not structure the latent space in the learning process?</p>
<p>While our clustering algorithm does not directly structure the latent space, it requires the latent space to be organized in a specific manner to be effective.In other words, the latent space learning algorithm is a key prerequisite for the latent state clustering algorithm.Specifically, our latent space learning algorithm structures the latent space such that states easily reachable from one another in the real environment (as determined by the learned temporal distance network as Equation 1) are also close together in the latent space.The clustering algorithm leverages this structure-property to ensure that the latent state cluster boundaries align with the frontier of previously explored states.As such, CE 2 can efficiently generate exploratory goals at the frontier at training time.</p>
<p>B Extended Related Work</p>
<p>Model-based reinforcement learning (MBRL) has seen significant advancements in recent years, driven by the development of sophisticated world models and planning algorithms.One notable approach is Stochastic Ensemble Value Expansion (STEVE) (Buckman et al. ( 2018)), which enhances sample efficiency by leveraging ensemble models to reduce overfitting and uncertainty in value estimates.Similarly, the work by Chua et al. (Chua et al. (2018)) demonstrates that probabilistic dynamics models can be effectively used to achieve high performance in a small number of trials.In the realm of combining model-based and model-free methods, Deisenroth and Rasmussen (Deisenroth and Rasmussen (2011)) introduced PILCO, a data-efficient policy search method that uses Gaussian processes for dynamics modeling.More recent advancements include the integration of large pretrained models for world model construction and task planning, as explored by Guan et al. (Guan et al. (2023)).The Dreamer framework by Hafner et al. (Hafner et al. (2019a)) utilizes latent imagination to learn behaviors directly from pixel observations, and its extensions (Hafner et al. (2020(Hafner et al. ( , 2023))) have shown impressive results in mastering diverse domains.The Recurrent World Models by Ha and Schmidhuber (Ha and Schmidhuber (2018)) also contribute to this line of work by facilitating policy evolution through latent space planning.Several approaches focus on improving exploration strategies within MBRL.For instance, the use of cross-entropy methods for Monte-Carlo Tree Search (Chaslot et al. (2008)) and the Curious Replay mechanism (Kauvar et
h t = f ϕ (h t−1 , z t−1 , a t−1 ) Representation model: z t ∼ q ϕ (z t |h t , e t ) Transition predictor: ẑt ∼ p ϕ (ẑ t |h t ) Decoder: xt ∼ f D (x t |h t , z t )(8)</p>
<p>C.2 Temporal Distance Training in LEXA</p>
<p>The goal-reaching reward r G is defined by the self-supervised temporal distance objective (Mendonca et al. (2021)) which aims to minimize the number of action steps needed to transition from the current state to a goal state within imagined rollouts.We use b t to denote the concatenate of the deterministic state h t and the posterior state z t at time step t.
b t = (h t , z t )(9)
The temporal distance D t is trained by sampling pairs of imagined states b t , b t+k from imagined rollouts and predicting the action steps number k between the embedding of them, with a predicted embedding êt from b t to approximate the true embedding e t of the observation x t .</p>
<p>Figure 9: Illustration of differences between our mothod CE 2 -G and other exploration methods.</p>
<p>Predicted embedding:
emb(b t ) = êt ≈ e t ,
where
e t = f E (x t )(10)
Temporal distance:
D t (ê t , êt+k ) ≈ k/H where êt = emb(b t ) êt+k = emb(b t+k ) (11) r G t (b t , b t+k ) = −D t (ê t , êt+k )(12)</p>
<p>D Limitations and Future Work</p>
<p>Our method clusters in the latent space, which necessitates a well-trained latent space.This latent space must not only accurately reconstruct the original state space and facilitate dynamic prediction but also reflect the reachable distances between different states.Therefore, training this latent space requires a temporal distance predictor that can accurately estimate the number of action steps needed between two states.We utilize the temporal distance predictor network from LEXA, which constructs intrinsic goal-conditioned rewards, and this network is trained using simulated trajectories.Compared to training the temporal distance predictor with real trajectories, using simulated trajectories offers greater stability.Our method requires the temporal distance predictor to reliably estimate the number of action steps needed to transition from one state to another, which is a crucial prerequisite for ensuring the effectiveness of CE 2 .Moreover, Although CE 2 has achieved remarkable success in tasks such as stacking blocks and rotating objects, there remain more challenging tasks that CE 2 needs to address.For instance, environments such as inserting a peg into a hole or fluid tasks in ManiSkill2 (Gu et al. (2023)).</p>
<p>Besides, our realization of CE 2 is based on Dreamer, a model-based reinforcement learning(MBRL) agent known for its higher sample efficiency but greater computational demands compared to modelfree alternatives.This increased resource requirement stems from the necessity to develop a world model.In CE 2 , this world model is utilized to train policies and value functions through simulated trajectories.At the same time, CE 2 use the PEG as the filter of exploration potential, which rely on world model to select goals that guide exploration.Creating a model-free version of CE 2 would simplify both its computational and conceptual aspects, a task we plan to undertake in future research.</p>
<p>E Environments E.1 3-Block Stacking</p>
<p>In this task, a robot is required to stack three blocks into a tower.In PEG, evaluations are conducted on goals of varying difficulty levels, including 3 easy goals(picking up a single block), 6 medium goals(stacking two blocks), and 6 hard goals(stacking three blocks).We evaluate our agent solely on the 6 hard goals.At the same time, we use only 3 hard goals provided by the training environment as guiding goals for CE 2 -G.Relying solely on the most challenging goals for training and evaluation presents a heightened challenge for both CE 2 and CE 2 -G.However, we observed that CE 2 and CE 2 -G are capable of spontaneously discovering additional easy and medium difficulty goals through clustering in latent space, as these serve as crucial transitional states towards the hard goals.The environment features a 14-dimensional state and goal space: the first five dimensions capture the gripper's state, while the remaining nine dimensions correspond to the xyz positions of each block.The action space is 4-dimensional, with three dimensions dedicated to the xyz movements of the gripper and the fourth dimension controlling the gripper's finger movement.The robot achieves success when the L2 distance between each block's xyz position and its target position is less than 3 cm.This environment is a modified version of the FetchStack3 environment from Pitis et al. (2020), incorporating adjustments to better test the robot's precision in stacking.</p>
<p>E.2 Walker</p>
<p>In this environment, a 2D walker robot is trained and evaluated the locomotion capabilities of on a flat surface.The environment code is sourced from Mendonca et al. (2021).In order to fully evaluate the agent's ability and accuracy to travel to longer distances, we increased the number of evaluation goals in PEG from 4(±7, ±12) to 12(±13, ±16, ±19, ±22, ±25, ±28) along the x axis from its initial position.Noting that, in CE 2 -G, we only use goals at ±13, ±16 as the training goals returned by environments, but evaluate on all 12 goals.Success is determined by checking if the agent's x position falls within a small margin of the target x position.The state and goal space are nine-dimensional, encompassing the walker's xz positions and joint angles.</p>
<p>E.3 Ant Maze</p>
<p>This environment is adapted from the Ant Maze described in Pitis et al. (2020), with a little modifications.Like PEG, we set the goal space to be same with state space which including the ant's xyz positions along with joint positions and velocities, and an additional room was added in the top left to introduce a more challenging goal.In this complex environment, a high-dimensional ant robot must navigate from the bottom left to the top left of a maze, passing through hallways.The task is challenging due to its long duration, with episodes lasting 500 timesteps, and the considerable distance to be traversed.Compared to evaluation on goals both in top left room and in the central hallway in PEG, our evaluation only focuses on the ant reaching the most difficult four goals in the top left room.Besides, we use all 32 goals of different positions in the maze to be the training goals returned by environment for CE 2 -G.The maze itself measures approximately 6 x 8 meters.Success is determined by ensuring the L2 distance between the ant's xy position and the goal is less than 1.0 meter, roughly the width of a cell in the maze.The Ant Maze environment features the highest dimensional state and goal spaces, totaling 29 dimensions.These include the ant's position, joint angles, and joint velocities.Specifically, the first three dimensions represent the xyz position, the next 12 dimensions correspond to the joint angles of the ant's four limbs, and the remaining 14 dimensions capture the velocities in the xy plane and of each joint.The action space consists of 8 dimensions, controlling the hip and ankle actuators of the ant's limbs.</p>
<p>E.4 Point Maze</p>
<p>The 2D point agent starts at the bottom left corner of a 10 x 10 maze and is tasked with reaching the top right corner within 50 timesteps.The state space and action space are both two-dimensional, corresponding to the agent's position and velocity on the plane.Success is determined if the L2 distance between the agent's position and the goal is less than 0.15.This environment is directly adapted from Pitis et al. (2020) without any modifications.In CE 2 -G, the training goals from environments is randomly chose from 11 goals in different positions of maze.</p>
<p>E.5 Block and Pen Rotation</p>
<p>The hand must manipulate both a thin pen or a block to achieve target rotations.Manipulating the thin pen presents a greater challenge than manipulating the block due to the pen's tendency to slip, requiring more precise control.We utilize variant versions of the gymnasium environments: "HandManipulatePenRotate-v1" and "HandManipulateBlockRotateXYZ-v1".These environments introduce randomized target rotations for all axes of the block and x, y axes of the pen in each episode.The state space for both tasks consists of 61 dimensions, providing details on the robot's joint and object states, as well as goal information.The goal space remains consistent at 7 dimensions, indicating the target pose information.During evaluation, the latest policy is evaluated across 50 episodes for each task, with each episode featuring a distinct random goal.In CE 2 -G, the training goals from environments is also randomly generated.Pen Rotation is particularly challenging due to the pen's thin structure, which requires precise control to prevent it from dropping.We intended to convey that this is the most difficult benchmark (with 61 observation space dimensions and 20 action space dimensions) in our test suite.</p>
<p>F Baselines</p>
<p>In this section, we present the pseudocode for all baseline methods.Note that, except for L3P, each baseline employs a different strategy for sampling data in the real environment within this framework.Therefore, we first display the general training framework for MBRL and subsequently provide the pseudocode for each baseline's data sampling method in the real environment.Update π E in imagination with M to maximize r E</p>
<p>F.1 Go-Explore</p>
<p>To enhance the exploration area of the explorer, we adopt the Go-Explore strategy (Ecoffet et al. (2019)).This approach initially employs a goal-conditioned policy, π G , to approach a specified goal g as closely as possible, a process referred to as the Go-phase.Following this, the explorer, π E , is used to further explore the environment starting from the terminal state of the Go-phase, known as the Explore-phase.</p>
<p>The effectiveness of the trajectories generated by the Go-Explore strategy heavily depends on the choice of the goal g during the Go-phase.Thus, establishing an efficient goal selection mechanism for the Go-phase is crucial.If the chosen goal g is too easy, the explorer will not sufficiently explore the environment.Conversely, if the goal g is too difficult, the goal-reaching policy π G will be unable to approach it effectively.Therefore, the objective is to develop a goal selection mechanism that identifies a goal g capable of guiding the agent to a region with high exploration potential during the Go-phase.Below, we provide the pseudocode for the Go-Explore strategy.</p>
<p>Algorithm 5 Go Explore Framework
1: function GO-EXPLORE(g, π G , π E ) 2: s 0 ← env.reset() 3: τ ← {s 0 } 4: for Step t = 1 to T Go do 5: s t ← env.step(π G (s t−1 , g)) 6: τ ← τ ∪ {s t } 7:
if agent reach g then for Step t = t e to t e + T Explore do 11:
s t ← env.step(π E (s t−1 )) 12: τ ← τ ∪ {s t } 13: return τ F.2 GC-Dreamer
GC-Dreamer follows a goal-conditioned approach where trajectories are collected by goal-conditioned policy π G , and the goals are returned from the training environment.</p>
<p>Algorithm 6 GC-Dreamer Goal Sampling 1: function COLLECT TRAJECTORIES(. ..)</p>
<p>2:</p>
<p>g ← Returned by environment 3:</p>
<p>τ ← Sample a trajectories by π G using goal g 4: return τ F.3 PEG PEG adopts a strategy where trajectories are collected by optimizing a specific equation using the MPPI method.The optimized goal is then used to guide exploration through the GO-EXPLORE algorithm.</p>
<p>Algorithm 7 PEG Sampling 1: function COLLECT TRAJECTORIES(. ..)</p>
<p>2:</p>
<p>g ← Optimize Equation 7 with MPPI 3:
τ ← GO-EXPLORE(g, π G , π E ) 4: return τ F.4 PEG-G
PEG-G combines the utilization of goals from the environment with those generated by optimizing the equation using MPPI.This approach alternates between the two strategies based on the episode index.</p>
<p>Algorithm 8 PEG-G Sampling 1: function COLLECT TRAJECTORIES(. ..)</p>
<p>2:</p>
<p>if episode i%2 = 0 then 3:</p>
<p>g ← Optimize Equation 7 with MPPI 4:
τ ← GO-EXPLORE(g, π G , π E ) 5: else 6:
g ← Returned by environment 7:</p>
<p>τ ← Sample a trajectories by π G using goal g 8: return τ F.5 MEGA For model-based MEGA, we directly utilize the implementation method described in the PEG paper.This involves transplanting MEGA's KDE model and using a goal-conditioned value function within the LEXA framework to filter goals based on reachability.The PEG paper has demonstrated that their implementation of MEGA achieves superior performance compared to the original MEGA baseline.</p>
<p>Algorithm 9 MEGA Goal Sampling 1: function COLLECT TRAJECTORIES(. ..)</p>
<p>2:</p>
<p>g ← min g∈D p(g)
3: τ ← GO-EXPLORE(g, π G , π E ) 4: return τ F.6 MEGA-G
Similar to PEG-G, MEGA-G alternates between using goals from the environment and MEGA goal picking strategy.</p>
<p>Algorithm 10 MEGA-G Goal Sampling 1: function COLLECT TRAJECTORIES(. ..)</p>
<p>2:</p>
<p>if episode i%2 = 0 then 3:</p>
<p>g ← min g∈D p(g)
4: τ ← GO-EXPLORE(g, π G , π E ) 5: else 6:
g ← Returned by environment 7:</p>
<p>τ ← Sample a trajectories by π G using goal g 8: return τ F.7 MEGA+PEG MEGA+PEG combines the strategies of MEGA and PEG. this baseline firstly employs MEGA to sample a batch of candidate goals, all of which have low density in the replay buffer.Subsequently, their exploration potential is evaluated using PEG, with the most valuable one selected as the exploration goal.</p>
<p>Algorithm 11 MEGA+PEG Goal Sampling 1: function COLLECT TRAJECTORIES(. ..)</p>
<p>2:</p>
<p>G ← Top-10 smallest p(g) for g ∈ D 3:
g ← Optimize Equation 7 for g ∈ G 4: τ ← GO-EXPLORE(g, π G , π E ) 5: return τ F.8 CE 2 -noPEG CE 2 -
noPEG utilizes a goal-picking strategy based on Gaussian Mixture Model (GMM) clustering to generate goals for exploration without employing PEG optimization.The goal picked by this method is sampled at the edge of our latent space clusters, which is the main contribution of our paper.</p>
<p>Algorithm 12 CE 2 -noPEG Goal Sampling 1: function COLLECT TRAJECTORIES(. ..)</p>
<p>2:</p>
<p>D exp ← {} 3:
for episode i = 1 to N τ do 4: G candidate ← Sample N candidate points from GM M 5:
G edge ← N edge points in G candidate with the smallest total probability of the GM M .</p>
<p>6:
g E ← Randomly select a g from G edge 7: τ ← GO-EXPLORE(g E , π G , π E ) 8: return τ F.9 L3P
Our implementation of L3P follows the original code provided in the L3P paper (Zhang et al. (2021)).For more details on the pseudocode and specific implementation, please refer to the descriptions in their paper.return sampled_points</p>
<p>G Implementation Details</p>
<p>The Farthest Point Selection (FPS) algorithm, commonly employed in various applications including point cloud simplification and image sampling, initializes by creating an empty list termed 'sam-pled_points' to retain the selected points.The process initiates by randomly selecting an initial point from the input point set, designated as 'points', and appending it to 'sampled_points'.Subsequently, 'min_distances' is initialized to track the minimum distance from each point to any of the sampled points, with initial values set to infinity.The core procedure entails iteratively selecting points until reaching the desired number of samples.At each iteration, the algorithm identifies the point in 'points' with the maximum minimum distance to the previously sampled points and includes it in 'sampled_points'.Concurrently, 'min_distances' is updated to reflect the recalculated minimum distance of each point to any of the sampled points.The algorithm incorporates two auxiliary functions: 'distance(point1, point2)', facilitating the computation of the Euclidean distance between two points, and 'argmax(array)', which returns the index of the maximum value within an array.See pseudocode Algorithm 13 for more details about FPS.We conduct each experiment on GPU Nvidia A100 and require about 5GB of GPU memory.See Table 1 for specific running time of CE 2 for different task.The running time of CE 2 -G has no big difference with CE 2 .The neural network updates of the policies and world model take most of runtime.However, CE 2 takes more time in goal selection compared to PEG and MEGA.This is because CE 2 need evaluate the exploration potential of candidate goals sampled at the edge of latent clusters every time it picks a goal for exploration.In the PEG algorithm, the MPPI parameters are only updated at fixed intervals of multiple episodes.This means that the exploration potential is assessed for a batch of data after a certain number of episodes have been completed.In contrast, our method evaluates the exploration potential for a batch of candidate goals at each episode when sampling exploration goals.However, we can also follow the PEG by evaluating the exploration potential of a batch of candidate goals after multiple episodes.Between these updates, we can select exploration goals from the previously evaluated set of candidate goals.While this may sacrifice some algorithm performance, it can significantly reduce the time CE 2 requires to select goals.We tried this setting and compared the computation time needed to optimize goal states for launching the Go-Explore procedure among our CE 2 and the baseline methods MEGA and PEG in the 3-Block Stacking environment.The average wall clock time are recorded in the Table 2.</p>
<p>G.2 Runtime</p>
<p>G.3 Hyperparameters</p>
<p>Similar to PEG, we use the default hyperparameters of the LEXA backbone MBRL agent (e.g., learning rate, optimizer, network architecture) and keep them consistent across all baselines.For the Gaussian Mixture Model(GMM), we set the learning rate to be 3e-4, optimizer to be Adam.We also test result of different cluster number(10, 30, 50) setting as we show in ablation experiment.Every time we sample points in the GMM, we set the point number N candidate = 1000 and we set the edge point number N edge = 100.After then, we evaluate the exploration potential of these 100 points and pick the point as goal state which have largest exploration potential value.In addition, to improve clustering in the latent space, we reduced the latent space dimension from 400(LEXA setting) to 50.We tested the training speed and results with both a latent space dimension of 400 and 50.We found that a latent space dimension of 50 allows for faster clustering and accelerates the training process.Meanwhile, although a latent space dimension of 400 reduces the training speed a little, it does not affect the final success rate.</p>
<p>H Additional Experiments</p>
<p>H.1 Space explored image for 3-Block Stacking</p>
<p>In 3-Block Stacking, we innovatively designed a method based on the coordinates of three blocks to demonstrate the degree of exploration in the environment, showed in Fig 10 .Firstly, we establish connections between the coordinates of three blocks in three-dimensional space, forming a spatial triangle.This spatial triangle serves to express the relative positions and distances of the three blocks within the space.Subsequently, we project this spatial triangle onto the xy-plane.The summation of the lengths of the sides of the projected triangle on the xy-plane reflects the dispersal of the three blocks within the xy-plane, while the total sum of the z-coordinates of the three blocks indicates their relative positions in height Utilizing the former as the x-axis and the latter as the y-axis, we depict a schematic illustration of the spatial exploration of 3-Block Stacking.We observe that CE 2 exhibits a more targeted and in-depth exploration around the target space (highlighted within the red box) compared to PEG.Simultaneously, we observed that PEG tends to conduct numerous explorations in the upper-left region of the exploration graph, which are often futile and irrelevant to the goal of completing block stacking.This highlights the advantage of CE 2 , which benefits from the constraints imposed by clustering and avoids blindly exploring areas.Moreover, sampling at the edges of clusters ensures the profitability of exploration, enabling more efficient exploration of the vicinity of the goal space compared to PEG.</p>
<p>H.2 More Exploration Process</p>
<p>We present a comparison of exploration targets generated by CE 2 , MEGA and PEG approaches over the training process in the Ant Maze environment.In the Fig 11, red points represent the generated exploration targets by different methods.We observe that the exploration targets generated by CE 2 are significantly superior to those generated by MEGA and PEG.Specifically, CE 2 consistently generates points located at the forefront of agent exploration and within the agent's reachable capability range.In contrast, the targets generated by MEGA exhibit greater dispersion and sparsity, which are disadvantageous for concentrated exploration of forefront regions.Moreover, PEG consistently generates targets outside the Maze channels, rendering these exploration targets not only far beyond the agent's capability range but also meaningless.We found that the performance of CE 2 is not strongly correlated with the number of clusters; as long as the number of clusters is sufficient to represent key state regions, the results tend to be stable, which demonstrates the robustness of our approach.• If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.• While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper.The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community.Reviewers will be specifically instructed to not penalize honesty concerning limitations.</p>
<p>Theory Assumptions and Proofs</p>
<p>Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?</p>
<p>Answer: [NA] Justification: We does not include theoretical results</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include theoretical results.</p>
<p>• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced.• All assumptions should be clearly stated or referenced in the statement of any theorems.</p>
<p>• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.• Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.• Theorems and Lemmas that the proof relies upon should be properly referenced.</p>
<p>Experimental Result Reproducibility</p>
<p>Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?</p>
<p>Answer: [Yes] Justification: In the Experiment section and appendix of our paper, we provide detailed descriptions of our experimental procedures and configurations.This includes elucidating the origins and modifications made to all testing environments.We also present the pseudocode and implementation methods for all baseline models.Additionally, we specify the devices and memory resources utilized, as well as enumerate the exact numerical values of the hyperparameters employed.Moreover, we have made our code openly accessible.For further details, please refer to the Reproducibility Statement section.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.• If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.• Depending on the contribution, reproducibility can be accomplished in various ways.</p>
<p>For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model.In general.releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.</p>
<p>• While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution.For example (a) If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm.(b) If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.(c) If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).(d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.</p>
<p>In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.Justification: We provide detailed descriptions of the hyperparameters used for CE 2 in Appendix, such as varying cluster numbers and latent space dimensions.We also desbribe the setting of training, including learning rates, optimizers, and network architectures.This information is crucial for understanding and reproducing our experimental results.Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.• The full details can be provided either with the code, in appendix, or as supplemental material.</p>
<p>Experiment Statistical Significance</p>
<p>Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?</p>
<p>Answer: [Yes] Justification: We repeated each experiment at least five times using different random seeds, and when plotting the results.As we showing in the Experiment section, we displayed the experimental error.The solid line represents the average success rate, while the shaded region represents the standard deviation between the repeated experimental results.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.• The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).• The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) • The assumptions made should be given (e.g., Normally distributed errors).</p>
<p>• It should be clear whether the error bar is the standard deviation or the standard error of the mean.• It is OK to report 1-sigma error bars, but one should state it.The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.• For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g.negative error rates).• If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.</p>
<p>Experiments Compute Resources</p>
<p>Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?</p>
<p>Answer: [Yes] Justification: We clearly specifies the computer resources(Nvidia A100 GPU) and the amount of GPU memory required (approximately 5GB) in Appendix.Additionally, we provides detailed information on the runtime of each experiment, including specific time metrics such as episode length and seconds per episode.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not include experiments.</p>
<p>• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.• The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.• The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).</p>
<p>• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.• Datasets that have been scraped from the Internet could pose safety risks.The authors should describe how they avoided releasing unsafe images.• We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.</p>
<ol>
<li>Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?</li>
</ol>
<p>Answer: [Yes] Justification: Our paper properly credits the creators or original owners of assets used, including code, data, and models.The licenses and terms of use are explicitly respected.Specifically, we cite the original papers for code packages or datasets used, state the version of the assets, and include URLs where possible.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not use existing assets.</p>
<p>• The authors should cite the original paper that produced the code package or dataset.</p>
<p>• The authors should state which version of the asset is used and, if possible, include a URL.• The name of the license (e.g., CC-BY 4.0) should be included for each asset.</p>
<p>• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.• If assets are released, the license, copyright information, and terms of use in the package should be provided.For popular datasets, paperswithcode.com/datasetshas curated licenses for some datasets.Their licensing guide can help determine the license of a dataset.• For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.• If this information is not available online, the authors are encouraged to reach out to the asset's creators.</p>
<p>New Assets</p>
<p>Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?</p>
<p>Answer: [Yes] Justification: We have documented our code and provided detailed instructions on its usage, licenses, and permissible scope of use.Additionally, we have included the documentation alongside the assets to ensure accessibility and clarity for users.</p>
<p>Guidelines:</p>
<p>• The answer NA means that the paper does not release new assets.</p>
<p>• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates.This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used.• At submission time, remember to anonymize your assets (if applicable).You can either create an anonymized URL or include an anonymized zip file.</p>
<p>Crowdsourcing and Research with Human Subjects</p>
<p>! ,  !,  " ) ( #$! ,  #$! ,  # ) … World Model  # = (  % ( #$! ,  #$! ) Train World Model       , )   (  |  )</p>
<p>Figure 1 :
1
Figure 1: Model-based GCRL Framework</p>
<p>); Pathak et al. (2017); Burda et al. (2018); Sekar et al. (</p>
<p>Algorithm 2
2
The main training algorithm for CE 2 1: Input: π G , π E , World Model M , GMM, r G , r E 2: Initialize replay buffer D 3: for i = 1 to N train do 4: if Should assign centroids then 5: B exp ← A batch of data from D exp 6: GMM ← Choose N c centroids from B exp by FPS 7: D exp ← Cluster Edge Exploration(...) with Algorithm 1 8: D ← D ∪ D exp 9: Update M with D (update latent space by L rec + L latent ) 10: Update GMM with D exp 11: Update π G in imagination with M to maximize r G 12:</p>
<p>Figure 2 :
2
Figure 2: We conduct experiments on 6 environments: Point Maze, Ant Maze, Walker, 3-Block Stacking, Block Rotation, Pen Rotation.</p>
<p>p g 2: Initialize replay buffer D 3: for i = 1 to N train do 4: if Should assign centroids then 5: B egc ← A batch of data from D egc 6: GM M ← Choose N c centroids from B egc by FPS 7: D exp ← Cluster Edge Exploration(...) with Algorithm 1 8: D egc ← Rollouts of π G using the env goal distribution p g 9: D ← D ∪ D exp ∪ D egc 10: Update M with D (update latent space by L rec + L latent ) 11: Update GMM with D egc 12: Update π G in imagination with M to maximize r G 13:</p>
<p>Figure 3 :
3
Figure 3: Experiment results comparing CE 2 with the baselines over 5 random seeds.</p>
<p>Figure 4 :
4
Figure 4: Comparison of exploration goals (represented as red points) generated by CE 2 , MEGA, and PEG in the Ant Maze environment.CE 2 Results.Fig. 3 depicts the mean learning performance of all the unsupervised GCRL tools in terms of the agent's goal-reaching success rate averaged over 5 random seeds.The evaluation goal distribution is revealed to the agent only at test time.In all tasks except PointMaze, CE 2 significantly outperforms PEG and MEGA in terms of both learning performance and learning speed.On PointMaze, CE 2 performs comparably with the baselines.Although MEGA can set goal commands in sparsely explored areas</p>
<p>Figure 5 :
5
Figure 5: Experiment results comparing CE 2 -G with the baselines over 5 random seeds.</p>
<p>Figure 6 :
6
Figure 6: Cluster evolution in CE 2 as the training progresses.The red points means the goals picked by CE 2 to explore and other points in different colors represent the clusters CE 2 learned.</p>
<p>Figure 7 :
7
Figure 7: Ablation study on the importance of each component of CE 2 over 5 random seeds.</p>
<p>)) proposed to do automatic curriculum generation of goals based on the epistemic uncertainty of value functions.(Florensa et al. (2018)) use generative adversarial training to automatically generate goals, leveraging goal difficulty as a guiding factor.(Pong et al. (2019);Pitis et al. (2020)) proposed to use the maximum entropy of achieved goal distribution to guide goal selection.(Ecoffetet al. (</p>
<p>Algorithm 4
4
General MBRL Training Framework 1: Input: Policy π G , π E , Environment Goal Distribution G, World Model M , reward function r G , r E 2: D ← {} Initialize buffer.3: for Episode i = 1 to N train do 4: τ ← Collect trajectories(. ..) model M with (s t , a t , s t+1 ) ∼ D 7:Update π G in imagination with M to maximize r G 8:</p>
<p>for each point p in points do 13: min_distances[p] ← min(min_distances[p], distance(p, farthest_point)) 14:</p>
<p>Figure 10 :
10
Figure 10: Space explored by CE 2 and PEG in the 3-Block Stacking environment at 1M steps.X-axis: the sum of the three sides of the triangle projected on the x-y plane by the three block-connected triangles.Y-axis: sum of heights (z-coordinates) of the three blocks.Red points: evaluation goals.Other points: observations of trajectories sampled in real environment.Color from green to yellow means to be sampled more recent.</p>
<p>Figure 11 :
11
Figure 11: Comparison of exploration goals generated by CE 2 , MEGA and PEG</p>
<p>Figure 12 :
12
Figure 12: some centroids visualization of GMMs in CE 2 .</p>
<p>13</p>
<p>Figure 13 :
13
Figure 13: Full experiment Results comparing CE 2 -G with the baselines in six environments.</p>
<p>Figure 14 :
14
Figure 14: Ablation Study with Different Cluster Number.</p>
<p>Shyam et al. (2019)))1)t incentivizing visits to states with low visitation frequencies(Bellemare et al. (2016); Burda et al. (2018)).These approaches typically involve identifying states with infrequent occurrences within the replay buffer and targeting them for exploration, thus facilitating the discovery of unknown regions in the environment.Furthermore, some research emphasizes the exploration of states with high variance between ensemble predictions of future states(McCarthy et al. (2021);Oudeyer et al. (2007);Pathak et al. (2017);Henaff (2019);Shyam et al. (2019);Sekar et al. (
al. (2019); Liu et al. (2022); Plappert et al. (2018)). In goal-conditionedRL, agents are trained to achieve various goals based on predefined commands, with rewards typicallybeing binary, indicating positive feedback from the environment only upon reaching the specified
Mendonca et al. (2021)019)tt et al. (2019)tly complicates achieving sample efficiency and effective learning processes(Ren et al. (2019);Florensa et al. (2018);Trott et al. (2019)).To mitigate this challenge, various methods have been proposed.Some reshape the sparse reward function into a denser form by incorporating metrics such as distance between achieved and desired goals(Trott et al. (2019)) or temporal distance(Hartikainen et al. (2019);Mendonca et al. (2021)).Additionally, exploration strategies often</p>
<p>information processing systems, 30.Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. (2016).Unifying count-based exploration and intrinsic motivation.Advances in neural information processing systems, 29.
Buckman, J., Hafner, D., Tucker, G., Brevdo, E., and Lee, H. (2018). Sample-efficient reinforcementlearning with stochastic ensemble value expansion. Advances in neural information processingsystems, 31.Burda, Y., Edwards, H., Storkey, A., and Klimov, O. (2018). Exploration by random networkdistillation. arXiv preprint arXiv:1810.12894.</p>
<p>(Hafner et al. (2019a)r systems.Additionally, t(Hafner et al. ( , 2023))eraged for their sample efficiency in world modeling(Micheli et al. (2022);Zhang et al. (2024)), demonstrating their potential in complex environments.The integration of demonstrations into visual model-based reinforcement learning, as seen in MoDem(Hansen et al. (2022)), showcases another avenue for improving learning efficiency.Luo et al.(Luo et al. (2018)) provide a comprehensive framework with theoretical guarantees, while Janner et al.(Janner et al. (2019)) address the critical question of when to trust the learned models.We use the world model structure M of recurrent state-space model (RSSM) of Dreamer(Hafner et al. (2019a(Hafner  et al. ( ,b, 2020(Hafner et al. ( , 2023))) to learn the dynamics.The complete model state of the RSSM is the concatenation of deterministic states and stochastic states, with the latter being generated by the former.The deterministic state h t can used to get the prior state ẑt and posterior state z t .The ẑt aims to predict the posterior without access to the current input state x t while the posterior state z
C Extended BackgroundC.1 Dreamer World Model
al. (2023)) have been proposed to enhance exploration efficiency.The work by Wagenmaker et al. (Wagenmaker et al. (2024)) further explores optimal exploration t is concluded by integrating the encoded information of current input state x t .The deterministic state h t is updated by the recurrent transition function f ϕ using the concatenation (h t , z t ) or (h t , ẑt ) as input.The world model is summarized in Fig 8, and the formulas of components are shown in Equation 8:Figure 8: RSSM Structure Encoder: e t = f E (e t |x t ) Recurrent model:</p>
<p>Table 1 :
1
Runtimes per experiment.
Total Runtime (Hours) Total Steps Episode Length Seconds per Episode3-Block Stacking601e615031.34Walker361e615018.62Ant Maze561e650096.91Point Maze361e6505.60Block Rotation581e615030.74Pen Rotation581e615030.42</p>
<p>Table 2 :
2
Computation time needed to optimize goal states
Method Seconds/EpisodeCE 20.56PEG0.53MEGA0.47</p>
<ol>
<li>Open access to data and codeQuestion: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: As we answer in the previous question.We have open source our code and provide detailed instructions to reproduce the main experimental results.We illustrate the benchmark source, baseline settings and CE 2 implementation details.Guidelines:• The answer NA means that paper does not include experiments requiring code.•Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.• While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer.Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).• The instructions should contain the exact command and environment needed to run to reproduce the results.See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy)for more details.• The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. • The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines.If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.• At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).• Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.</li>
<li>Experimental Setting/DetailsQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-parameters, how they were chosen, type of optimizer, etc.) necessary to understand theresults?Answer: [Yes]
Our model-based MEGA baseline is borrowed from(Hu et al. (2023)) 
AcknowledgementsWe thank the anonymous reviewers for their comments and suggestions.This work was supported by NSF Award #CCF-2124155.NeurIPS Paper ChecklistClaimsQuestion: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?Answer:[Yes]Justification: Our abstract and introduction clearly articulate the primary claims of the paper, specifically focusing on the development and implementation of the CE 2 algorithm for goal-directed exploration in goal-conditioned reinforcement learning (GCRL).The introduction outlines the main contributions, including the proposal of a new exploration mechanism, the clustering strategy to prioritize accessible goals, and the validation of CE 2 in various challenging robotics environments.These claims are aligned with the theoretical framework and experimental results presented in the paper, ensuring they accurately reflect the contributions and scope of the paper.Guidelines:• The answer NA means that the abstract and introduction do not include the claims made in the paper.• The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations.A No or NA answer to this question will not be perceived well by the reviewers.• The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.• It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.LimitationsQuestion: Does the paper discuss the limitations of the work performed by the authors?Answer:[Yes]Justification: Our paper clearly outlines several limitations of the proposed CE 2 algorithm in Appendix.Firstly, it emphasizes the dependency on a well-trained latent space, which must accurately reflect reachable distances between states and facilitate dynamic prediction.Training this latent space requires a robust temporal distance predictor, which we address by using the predictor network from LEXA, trained with simulated trajectories for stability.Additionally, we discuss that CE 2 may face challenges with more complex tasks such as peg insertion or fluid tasks in ManiSkill2.Guidelines:• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.• The authors are encouraged to create a separate "Limitations" section in their paper.• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally).The authors should reflect on how these assumptions might be violated in practice and what the implications would be.• The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs.In general, empirical results often depend on implicit assumptions, which should be articulated.• The authors should reflect on the factors that influence the performance of the approach.For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting.Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.• The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.Code Of EthicsQuestion: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Answer:[Yes]Justification: The research conducted in our paper aligns with the NeurIPS Code of Ethics.We have reviewed the guidelines and ensured that our research adheres to ethical standards.Additionally, we have taken measures to preserve anonymity and comply with relevant laws and regulations.Guidelines:• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.• The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).Broader ImpactsQuestion: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?Answer:[NA]Justification: Our study focuses on solving exploration issues in the GCRL environment.At this stage, it remains largely theoretical and has negligible societal implications.Guidelines:• The answer NA means that there is no societal impact of the work performed.• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.• Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.• The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments.However, if there is a direct path to any negative applications, the authors should point it out.For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation.On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.• The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.• If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).SafeguardsQuestion: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer:[NA]Justification: Our paper poses no such risks.Guidelines:• The answer NA means that the paper poses no such risks.• We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.• For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.
Hindsight experience replay. M Andrychowicz, F Wolski, A Ray, J Schneider, R Fong, P Welinder, B Mcgrew, J Tobin, O Pieter Abbeel, W Zaremba, 2017Advances in neural</li>
</ol>
<p>Cross-entropy for monte-carlo tree search. G M Chaslot, .-B Winands, M H Szita, I Van Den Herik, H J , Icga Journal. 3132008</p>
<p>Deep reinforcement learning in a handful of trials using probabilistic dynamics models. K Chua, R Calandra, R Mcallister, S Levine, Advances in neural information processing systems. 201831</p>
<p>Pilco: A model-based and data-efficient approach to policy search. M Deisenroth, C E Rasmussen, Proceedings of the 28th International Conference on machine learning (ICML-11). the 28th International Conference on machine learning (ICML-11)2011</p>
<p>A Ecoffet, J Huizinga, J Lehman, K O Stanley, J Clune, arXiv:1901.10995Go-explore: a new approach for hard-exploration problems. 2019arXiv preprint</p>
<p>The farthest point strategy for progressive image sampling. Y Eldar, M Lindenbaum, M Porat, Y Y Zeevi, IEEE transactions on image processing. 691997</p>
<p>Automatic goal generation for reinforcement learning agents. C Florensa, D Held, X Geng, P Abbeel, International conference on machine learning. PMLR2018</p>
<p>D Ghosh, A Gupta, A Reddy, J Fu, C Devin, B Eysenbach, S Levine, arXiv:1912.06088Learning to reach goals via iterated supervised learning. 2019arXiv preprint</p>
<p>J Gu, F Xiang, X Li, Z Ling, X Liu, T Mu, Y Tang, S Tao, X Wei, Y Yao, arXiv:2302.04659Maniskill2: A unified benchmark for generalizable manipulation skills. 2023arXiv preprint</p>
<p>Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. L Guan, K Valmeekam, S Sreedharan, S Kambhampati, Advances in Neural Information Processing Systems. 202336</p>
<p>Recurrent world models facilitate policy evolution. D Ha, J Schmidhuber, Advances in neural information processing systems. 201831</p>
<p>D Hafner, T Lillicrap, J Ba, M Norouzi, arXiv:1912.01603Dream to control: Learning behaviors by latent imagination. 2019aarXiv preprint</p>
<p>Learning latent dynamics for planning from pixels. D Hafner, T Lillicrap, I Fischer, R Villegas, D Ha, H Lee, J Davidson, International conference on machine learning. PMLR2019b</p>
<p>Mastering atari with discrete world models. D Hafner, T Lillicrap, M Norouzi, J Ba, arXiv:2010.021932020arXiv preprint</p>
<p>D Hafner, J Pasukonis, J Ba, T Lillicrap, arXiv:2301.04104Mastering diverse domains through world models. 2023arXiv preprint</p>
<p>N Hansen, Y Lin, H Su, X Wang, V Kumar, A Rajeswaran, arXiv:2212.05698Modem: Accelerating visual model-based reinforcement learning with demonstrations. 2022arXiv preprint</p>
<p>Dynamical distance learning for semi-supervised and unsupervised skill discovery. K Hartikainen, X Geng, T Haarnoja, S Levine, arXiv:1907.082252019arXiv preprint</p>
<p>Explicit explore-exploit algorithms in continuous state spaces. M Henaff, Advances in Neural Information Processing Systems. 201932</p>
<p>E S Hu, R Chang, O Rybkin, D Jayaraman, arXiv:2303.13002Planning goals for exploration. 2023arXiv preprint</p>
<p>When to trust your model: Model-based policy optimization. M Janner, J Fu, M Zhang, S Levine, Advances in neural information processing systems. 201932</p>
<p>I Kauvar, C Doyle, L Zhou, N Haber, arXiv:2306.15934Curious replay for model-based adaptation. 2023arXiv preprint</p>
<p>M Liu, M Zhu, W Zhang, arXiv:2201.08299Goal-conditioned reinforcement learning: Problems and solutions. 2022arXiv preprint</p>
<p>Algorithmic framework for modelbased deep reinforcement learning with theoretical guarantees. Y Luo, H Xu, Y Li, Y Tian, T Darrell, T Ma, arXiv:1807.038582018arXiv preprint</p>
<p>Imaginary hindsight experience replay: Curious model-based learning for sparse reward tasks. R Mccarthy, Q Wang, S J Redmond, arXiv:2110.024142021arXiv preprint</p>
<p>Discovering and achieving goals via world models. R Mendonca, O Rybkin, K Daniilidis, D Hafner, D Pathak, Advances in Neural Information Processing Systems. 202134</p>
<p>V Micheli, E Alonso, F Fleuret, arXiv:2209.00588Transformers are sample-efficient world models. 2022arXiv preprint</p>
<p>Deep dynamics models for learning dexterous manipulation. A Nagabandi, K Konolige, S Levine, V Kumar, Conference on Robot Learning. PMLR2020</p>
<p>Intrinsic motivation systems for autonomous mental development. P.-Y Oudeyer, F Kaplan, V V Hafner, IEEE transactions on evolutionary computation. 1122007</p>
<p>Curiosity-driven exploration by self-supervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, International conference on machine learning. PMLR2017</p>
<p>M Pislar, D Szepesvari, G Ostrovski, D Borsa, T Schaul, arXiv:2108.11811When should agents explore?. 2021arXiv preprint</p>
<p>Maximum entropy gain exploration for long horizon multi-goal reinforcement learning. S Pitis, H Chan, S Zhao, B Stadie, J Ba, International Conference on Machine Learning. PMLR2020</p>
<p>M Plappert, M Andrychowicz, A Ray, B Mcgrew, B Baker, G Powell, J Schneider, J Tobin, M Chociej, P Welinder, arXiv:1802.09464Multi-goal reinforcement learning: Challenging robotics environments and request for research. 2018arXiv preprint</p>
<p>Skew-fit: State-covering self-supervised reinforcement learning. V H Pong, M Dalal, S Lin, A Nair, S Bahl, S Levine, arXiv:1903.036982019arXiv preprint</p>
<p>Exploration via hindsight goal generation. Z Ren, K Dong, Y Zhou, Q Liu, J Peng, Advances in Neural Information Processing Systems. 201932</p>
<p>Planning to explore via self-supervised world models. R Sekar, O Rybkin, K Daniilidis, P Abbeel, D Hafner, D Pathak, International conference on machine learning. PMLR2020</p>
<p>Model-based active exploration. P Shyam, W Jaśkowski, F Gomez, International conference on machine learning. PMLR2019</p>
<p>Keeping your distance: Solving sparse reward tasks using self-balancing shaped rewards. A Trott, S Zheng, C Xiong, R Socher, Advances in Neural Information Processing Systems. 201932</p>
<p>J Tuyls, S Yao, S Kakade, K Narasimhan, arXiv:2201.01251Multi-stage episodic control for strategic exploration in text games. 2022arXiv preprint</p>
<p>Optimal exploration for model-based rl in nonlinear systems. A Wagenmaker, G Shi, K G Jamieson, Advances in Neural Information Processing Systems. 202436</p>
<p>G Williams, A Aldrich, E Theodorou, arXiv:1509.01149Model predictive path integral control using covariance variable importance sampling. 2015arXiv preprint</p>
<p>World model as a graph: Learning latent landmarks for planning. L Zhang, G Yang, B C Stadie, International conference on machine learning. PMLR2021</p>
<p>Storm: Efficient stochastic transformer based world models for reinforcement learning. W Zhang, G Wang, J Sun, Y Yuan, G Huang, Advances in Neural Information Processing Systems. 202436</p>
<p>Automatic curriculum learning through value disagreement. Y Zhang, P Abbeel, L Pinto, Advances in Neural Information Processing Systems. 202033</p>            </div>
        </div>

    </div>
</body>
</html>
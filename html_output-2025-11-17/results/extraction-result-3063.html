<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-3063 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-3063</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-3063</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-77.html">extraction-schema-77</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <p><strong>Paper ID:</strong> paper-263310484</p>
                <p><strong>Paper Title:</strong> <a href="https://export.arxiv.org/pdf/2309.17072v1.pdf" target="_blank">MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)</a></p>
                <p><strong>Paper Abstract:</strong> Large language models (LLMs) are advancing rapidly. Such models have demonstrated strong capabilities in learning from large-scale (unstructured) text data and answering user queries. Users do not need to be experts in structured query languages to interact with systems built upon such models. This provides great opportunities to reduce the barrier of information retrieval for the general public. By introducing LLMs into spatial data management, we envisage an LLM-based spatial database system to learn from both structured and unstructured spatial data. Such a system will offer seamless access to spatial knowledge for the users, thus benefiting individuals, business, and government policy makers alike.</p>
                <p><strong>Cost:</strong> 0.006</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e3063.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e3063.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of language models solving puzzle games that require spatial knowledge (such as Sudoku), including details about the models, the puzzles, the methods used, performance, and any analysis of how the models solve these tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>ChatGPT (unspecified version)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A conversational large language model (LLM) queried interactively in the paper to evaluate its ability to recall and reason about geographic/spatial facts (city coordinates and relative positions). The paper reports simple empirical checks of correctness on city location and pairwise relative-direction queries.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td></td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>model_name</strong></td>
                            <td>ChatGPT</td>
                        </tr>
                        <tr>
                            <td><strong>model_description</strong></td>
                            <td>An OpenAI conversational large language model (transformer-based) queried interactively by authors; the paper does not specify the exact ChatGPT version, architecture details, nor training data beyond describing it as a pre-trained LLM able to memorize facts.</td>
                        </tr>
                        <tr>
                            <td><strong>model_size</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_name</strong></td>
                            <td>City geo-coordinate and relative-position queries</td>
                        </tr>
                        <tr>
                            <td><strong>puzzle_description</strong></td>
                            <td>Not a formal puzzle/game like Sudoku, but a set of spatial knowledge tasks requiring geographic recall and simple spatial reasoning: (1) return geo-coordinates for top 50 Australian cities, and (2) given a pair of cities A and B, answer which side (relative direction) A is to B. These tasks require memorized geospatial facts and coarse directional reasoning.</td>
                        </tr>
                        <tr>
                            <td><strong>input_representation</strong></td>
                            <td>Textual prompts: authors provided lists/prompts with city names (and in some follow-up prompts, geo-coordinates) in plain text to ChatGPT.</td>
                        </tr>
                        <tr>
                            <td><strong>prompting_method</strong></td>
                            <td>Interactive text prompting (iterative): authors asked for coordinates of cities until 50 returned, and separately issued pairwise 'A is to which side of B' prompts. In some cases geo-coordinates were provided in the prompt to improve accuracy.</td>
                        </tr>
                        <tr>
                            <td><strong>spatial_reasoning_analysis</strong></td>
                            <td>The paper reports a simple empirical correctness count as evidence of LLM spatial knowledge: coordinates returned were compared to ground truth; pairwise relative-direction answers were compared to true directions. No deeper analysis (e.g., attention, intermediate chain-of-thought traces, ablations, or representational probing) was performed; authors note these results as 'demonstrating potential' and discuss the need for further research on faithful spatial reasoning and hallucination mitigation.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_metrics</strong></td>
                            <td>Coordinates: correct for 49 out of 50 cities (one coordinate ‚Äî Hervey Bay ‚Äî off by ~10 km). Pairwise relative-position queries: correct for 44 out of 50 pairs; 5 additional pairs became correct after including geo-coordinates in the prompt; 1 pair (Canberra vs Orange) remained incorrect (returned 'southwest' vs ground truth 'south'). No formal accuracy confidence intervals or larger-scale benchmarks provided.</td>
                        </tr>
                        <tr>
                            <td><strong>limitations_or_failure_modes</strong></td>
                            <td>Observed errors include (a) small numeric coordinate error for one city (~10 km), (b) incorrect coarse relative-direction for one city pair (Canberra/Orange), and (c) sensitivity to prompt content‚Äîproviding geo-coordinates corrected several otherwise-wrong relative-position answers. The paper emphasizes general LLM failure modes like hallucination and lack of guaranteed faithfulness for generated spatial answers, and calls for faithfulness scoring and grounding mechanisms.</td>
                        </tr>
                        <tr>
                            <td><strong>comparison_to_other_models_or_humans</strong></td>
                            <td>No formal comparison to other LLMs, non-language spatial models, or human performance is reported. The authors only present raw correctness counts for ChatGPT and discuss qualitatively that LLMs can 'memorize' spatial facts; no benchmarking against baselines or human accuracy is provided.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)', 'publication_date_yy_mm': '2023-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Faith and Fate: Limits of Transformers on Compositionality <em>(Rating: 2)</em></li>
                <li>Querying Large Language Models with SQL <em>(Rating: 1)</em></li>
                <li>Unstructured and Structured Data: Can We Have the Best of Both Worlds with Large Language Models? <em>(Rating: 1)</em></li>
                <li>Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-3063",
    "paper_id": "paper-263310484",
    "extraction_schema_id": "extraction-schema-77",
    "extracted_data": [
        {
            "name_short": "ChatGPT",
            "name_full": "ChatGPT (unspecified version)",
            "brief_description": "A conversational large language model (LLM) queried interactively in the paper to evaluate its ability to recall and reason about geographic/spatial facts (city coordinates and relative positions). The paper reports simple empirical checks of correctness on city location and pairwise relative-direction queries.",
            "citation_title": "",
            "mention_or_use": "use",
            "model_name": "ChatGPT",
            "model_description": "An OpenAI conversational large language model (transformer-based) queried interactively by authors; the paper does not specify the exact ChatGPT version, architecture details, nor training data beyond describing it as a pre-trained LLM able to memorize facts.",
            "model_size": null,
            "puzzle_name": "City geo-coordinate and relative-position queries",
            "puzzle_description": "Not a formal puzzle/game like Sudoku, but a set of spatial knowledge tasks requiring geographic recall and simple spatial reasoning: (1) return geo-coordinates for top 50 Australian cities, and (2) given a pair of cities A and B, answer which side (relative direction) A is to B. These tasks require memorized geospatial facts and coarse directional reasoning.",
            "input_representation": "Textual prompts: authors provided lists/prompts with city names (and in some follow-up prompts, geo-coordinates) in plain text to ChatGPT.",
            "prompting_method": "Interactive text prompting (iterative): authors asked for coordinates of cities until 50 returned, and separately issued pairwise 'A is to which side of B' prompts. In some cases geo-coordinates were provided in the prompt to improve accuracy.",
            "spatial_reasoning_analysis": "The paper reports a simple empirical correctness count as evidence of LLM spatial knowledge: coordinates returned were compared to ground truth; pairwise relative-direction answers were compared to true directions. No deeper analysis (e.g., attention, intermediate chain-of-thought traces, ablations, or representational probing) was performed; authors note these results as 'demonstrating potential' and discuss the need for further research on faithful spatial reasoning and hallucination mitigation.",
            "performance_metrics": "Coordinates: correct for 49 out of 50 cities (one coordinate ‚Äî Hervey Bay ‚Äî off by ~10 km). Pairwise relative-position queries: correct for 44 out of 50 pairs; 5 additional pairs became correct after including geo-coordinates in the prompt; 1 pair (Canberra vs Orange) remained incorrect (returned 'southwest' vs ground truth 'south'). No formal accuracy confidence intervals or larger-scale benchmarks provided.",
            "limitations_or_failure_modes": "Observed errors include (a) small numeric coordinate error for one city (~10 km), (b) incorrect coarse relative-direction for one city pair (Canberra/Orange), and (c) sensitivity to prompt content‚Äîproviding geo-coordinates corrected several otherwise-wrong relative-position answers. The paper emphasizes general LLM failure modes like hallucination and lack of guaranteed faithfulness for generated spatial answers, and calls for faithfulness scoring and grounding mechanisms.",
            "comparison_to_other_models_or_humans": "No formal comparison to other LLMs, non-language spatial models, or human performance is reported. The authors only present raw correctness counts for ChatGPT and discuss qualitatively that LLMs can 'memorize' spatial facts; no benchmarking against baselines or human accuracy is provided.",
            "uuid": "e3063.0",
            "source_info": {
                "paper_title": "MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)",
                "publication_date_yy_mm": "2023-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Faith and Fate: Limits of Transformers on Compositionality",
            "rating": 2,
            "sanitized_title": "faith_and_fate_limits_of_transformers_on_compositionality"
        },
        {
            "paper_title": "Querying Large Language Models with SQL",
            "rating": 1,
            "sanitized_title": "querying_large_language_models_with_sql"
        },
        {
            "paper_title": "Unstructured and Structured Data: Can We Have the Best of Both Worlds with Large Language Models?",
            "rating": 1,
            "sanitized_title": "unstructured_and_structured_data_can_we_have_the_best_of_both_worlds_with_large_language_models"
        },
        {
            "paper_title": "Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables.",
            "rating": 1,
            "sanitized_title": "towards_multimodal_dbmss_for_seamless_querying_of_texts_and_tables"
        }
    ],
    "cost": 0.006213,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><p>MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)
29 Sep 2023</p>
<p>Jianzhong Qi jianzhong.qi@unimelb.edu.au 
The University of Melbourne
Australia</p>
<p>Zuqing Li zuqingl@student.unimelb.edu.au 
The University of Melbourne
Australia</p>
<p>Egemen Tanin etanin@unimelb.edu.au 
The University of Melbourne
Australia</p>
<p>MaaSDB: Spatial Databases in the Era of Large Language Models (Vision Paper)
29 Sep 2023AC29C61CA0EAE3DD72FC487D74B87F0510.1145/3589132.3625597arXiv:2309.17072v1[cs.DB]CCS CONCEPTSInformation systems ‚Üí Spatial-temporal systems; Database query processing Spatial DatabasesLarge Language ModelsModel as a Database
Large language models (LLMs) are advancing rapidly.Such models have demonstrated strong capabilities in learning from large-scale (unstructured) text data and answering user queries.Users do not need to be experts in structured query languages to interact with systems built upon such models.This provides great opportunities to reduce the barrier of information retrieval for the general public.By introducing LLMs into spatial data management, we envisage an LLM-based spatial database system to learn from both structured and unstructured spatial data.Such a system will offer seamless access to spatial knowledge for the users, thus benefiting individuals, business, and government policy makers alike.</p>
<p>spatial data representations [2,24].These studies have focused on using ML to optimize the effectiveness or efficiency of modules of spatial database systems.The ML models developed help retrieve query results but do not change the general query processing paradigm: (1) Users submit queries in a special query language (e.g., SQL) 1 , which are parsed and translated into relational algebra expressions.(2) A query optimizer module analyzes the expressions and determines the order of execution (i.e., computes a query plan).</p>
<p>(3) The expressions are executed by a query execution engine, with the help of spatial indices to streamline the execution.(4) Relevant results are retrieved from the data tables and are returned to the users.This process may be repeated with altered queries until the intended results are found for the users.</p>
<p>The latest development of ML techniques, i.e., large language models (LLMs) such as ChatGPT, offer great opportunities to develop the next-generation spatial databases where, instead of using ML models for spatial database optimization, we envisage to use machine learning models as a spatial database (MaaSDB).</p>
<p>LLMs are neural network models with billions of parameters trained on a large text corpus.While such models are trained on simple tasks of predicting the next word in a sentence, they can capture the syntax and semantics of human language with a high accuracy.They can generate text and engage with human users in dialogues, to answer user questions and to perform text processing tasks.Most importantly, such models have been shown to be able to "memorize" the facts from the training data [7], effectively making themselves large data repositories with rich information.</p>
<p>In this paper, we present the vision of the next-generation spatial database systems exploiting the capability of LLMs to memorize facts from training data.We shift ML-based spatial database optimization from ML models for spatial databases to ML models as spatial databases.Such systems consist of ML models trained on structured and unstructured spatial data, which can generate query answers directly instead of retrieving data from tables.</p>
<p>There are important advantages that come with such systems:</p>
<p>(1) The ML models in such systems can learn from both structured and unstructured spatial data (e.g., tables and free text) and generate query results based on both types of data, unlike traditional systems where typically only either type of data are available, and it is difficult to link both types of data to answer complex queries.</p>
<p>(2) Since the ML models have full knowledge about the spatial data in the database, their inbuilt natural language-based user interface will be able to understand user intent better than existing text-to-SQL systems do, which have limited information about the underlying data.This leads to more relevant results returned from such systems and hence higher system usability.Such systems will significantly enhance the accessibility of spatial knowledge entailed in data stored in spatial databases.For example, a tourist may request such systems to generate a half-day trip in Hamburg within walking distance from the conference venue of SIGSPATIAL'23; an urban planner may request such systems to return the top-10 suburbs with the highest electric vehicle ownership-charging station ratios.Search engines like Google may retrieve partial answers to such queries through keyword matching, which however are limited by the availability of directly matched web documents, again due to the retrieval nature of the query processing procedure.</p>
<p>We make the following contributions: (1) We envisage a unified spatial database system that uses ML models as its core query execution engine to optimize user accessibility and system usability.</p>
<p>(2) We conduct a pilot experimental study to verify the feasibility of such a system.(3) We identify key research challenges and opportunities in realizing such a system.</p>
<p>RELATED WORK</p>
<p>We review studies on ML-based (spatial) database optimization.</p>
<p>Existing works focus on using ML techniques to optimize the effectiveness and efficiency of different modules of spatial database systems, e.g., using ML models to replace (e.g., RSMI [13]) or to optimize (e.g., RLR-tree [4]) the structure of traditional spatial indices [26].A study [1] trains autoencoder models to compute spatial embeddings, i.e., vectors encoding dataset characteristics such as distribution to help predict range query selectivity for spatial query optimization.Other studies compute embeddings for spatial objects (e.g., road segments [2] or trajectories [24]) to encode their spatial features for spatial query processing.In these studies, the ML models are second-class citizens -they help retrieve query results but do not change the classic retrieval-based query paradigm.</p>
<p>A few other studies use ML models to answer spatial queries directly.For example, Qi et al. [14] train a feedforward neural network (FFN) to predict the shortest-path distance given two points on a road network.Zeighami et al. [25] train FFNs to predict the answer for range count queries.While these studies show that ML models can memorize facts from spatial data, they focus on aggregate queries.Their models output scalar values and not data records.They do not have a natural language-based user interface.</p>
<p>In a broader context of database research, there are text-to-SQL studies [5] that train ML models to translate textual queries into SQL queries, thus providing a natural language-based user interface.These models may exploit meta data such as column names of the data tables.However, they do not generate query results directly and typically do not access the actual data records at training.</p>
<p>Motivated by the strong performance of LLMs, several vision papers [15,17,18] use pre-trained LLMs or transformer (the building block of LLMs)-based models trained on unstructured data to answer database queries.These papers share similar visions with ours in that they also envisage ML models to become first-class citizens in a database system.They differ from our vision in that they do not consider structured spatial data and the challenges.Tan [16] presents several challenges on query processing over structured data with LLMs without envisaging a solution.A couple of other studies apply LLMs with structured data.Urban and Binnig [19] extract tables from a document using LLMs, while Nobari and Rafiei [10] transform tables into a desired representation for better joinability.They do not use the learned models to generate query answers directly.Overall, none of these studies consider the specific challenges and opportunities brought by LLMs to spatial databases.Our paper fills this gap.Musleh et al. [9] envisage a BERT-based system for trajectory analysis, while Xue et al. [22,23] use language models for time series forecasting, exploiting the analogy between trajectories/time series and sentences.Our study complements the studies by considering spatial data and queries beyond trajectories.</p>
<p>PILOT STUDY 3.1 The Vision of the Future System</p>
<p>We envisage a next-generation spatial database system as shown in Fig. 1.This system consists of a query analyzer and query plan generator, a set of query result generators, and a result synthesizer, which are all formed by ML models and are connected together to generate answers for user queries.The system provides a natural language-based interface for users to query the spatial knowledge learned by the ML models from spatial data stored in the system.Users can interact with the system (e.g., via a computer or a smartphone) to submit queries in natural language.Upon receiving a query (e.g., generate a half-day trip in Hamburg within walking distance from the conference venue of SIGSPATIAL'23), the query analyzer and query plan generator (which may be an LLM) will analyze the query intent, generate sub-queries, and assign the subqueries to the relevant subset of the query result generators (e.g., a sub-query to find the conference venue of SIGSPATIAL'23 and a sub-query to find POIs within walking distance from the conference venue).The invoked query result generators will generate an answer for each sub-query.Different types of query result generators will be built by training on different types of data.For example, a (transformer-based) query result generator trained on unstructured conference web pages will be able to answer the sub-query on the conference venue, while a (GAN-based, detailed in Section 3.2) query result generator trained on a table of POIs in Hamburg will be able to answer the sub-query about the POIs.When the results of all sub-queries have been generated, the result synthesizer (which may be another LLM) will combine them based on the user query and generate the final query answer to be returned to the user.</p>
<p>Multiple challenges and research opportunities arise from the envisaged system, which will be discussed in Section 4.</p>
<p>Preliminary Experimental Study</p>
<p>We verify the feasibility of the envisaged spatial database system through a preliminary experimental study.Settings.We focus on structured data, since the aforementioned recent vision papers have shown the feasibility of using pre-trained LLMs or transformer-based models to answer certain types of database queries.We train an ML model on a data table and study how well the model can remember the (key characteristics of the) data.</p>
<p>We use a multi-dimensional dataset (instead of a table of just spatial coordinates, for generality) named CensusIncome. 2The dataset has 48,842 records, each with 8 categorical (e.g., occupation and marital status) and 6 numeric (e.g., age and capital gain) attributes.Since the dataset does not come with a query workload, we follow a previous study [11] and generate 20,000 range queries with randomly selected numeric attributes and ranges.</p>
<p>We use a generative adversarial network (GAN)-based model which has been shown to be able to generate tabular data [12].We train a GAN model with the dataset and test how well it can generate data records that preserve the data distribution and answer range count queries, i.e., given a query range, we return the number of records in the range.A GAN model has a generator module  and a discriminator module .The generator generates a record given a random noise vector, z, as its input, while the discriminator classifies whether the generated record (i.e.,  (z)) resembles a real record from the training dataset.The model is trained with a loss function that aims to generate records that cannot be distinguished from the real records.We adapt the loss function of the generator to add a Q-Error [8] loss term, which measures how well a generated table preserves the selectivity of given range queries, as follows:
min ùê∫ L (ùê∫ ) = E z‚àºùëùz (z) [log(1‚àíùê∑ (ùê∫ (z) ) ) ]+ 1 ùëÅ ‚àëÔ∏Å ùëñ max 1, ùë†ùëíùëô (ùëû ùëñ ) ≈ù ùëíùëô (ùëû ùëñ ) , ≈ù ùëíùëô (ùëû ùëñ ) ùë†ùëíùëô (ùëû ùëñ )
The second term here is the Q-Error loss, where  denotes the number of range queries,   denotes the th range query,  (  ) denotes the ground truth selectivity of   on the training dataset, and ≈ù  (  ) denotes the selectivity of   on the generated table.We name the adapted GAN model RC-GAN.We omit the detailed model structure and hyperparameter values due to space limit.</p>
<p>The experiments are run on a desktop computer with a 16-core CPU, 32 GB memory, and 24 GB GPU memory.</p>
<p>Results.We train RC-GAN (implemented with PyTorch 1.13.1) on the CensusIncome dataset in 10 epochs (which take about an hour) and use the trained model to generate a table of the same size of the dataset.We report the Q-Error of the generated data in Table 1, where "Training queries" refers to computing the Q-Error with the 20,000 range queries as described above, which have been used in model training, while "Testing queries" refers to computing the Q-Error with another set of 5,000 range queries that are generated separately (with the same procedure) and have not been seen at training.We can see that the median Q-Errors are very close to 1 under both settings, i.e., the generated table has almost the same query selectivity as the original dataset for half of the queries.The  Q-Errors at the 75th percentile are still within 2, while they only deteriorate to larger values at the 90th percentile.Importantly, the Q-Errors for the testing queries are close to those for the training queries.These results demonstrate the potential of ML models to "memorize" the key characteristics of structured data records.We further train two Gradient Boosting classifiers with 15% of the CensusIncome dataset and with 5% of the CensusIncome dataset plus 10% of data generated by RC-GAN, respectively.The classifiers predict if the income attribute of a record is greater than 50,000 given the other attributes.We test the classifiers on 1,000 randomly selected records of CensusIncome not seen at training.Table 2 reports the results.We see that the classifiers trained under both settings have very close performance, confirming the capability of RC-GAN to "memorize" the data distribution characteristics.</p>
<p>Our results above are obtained with an ML model where the number of parameters is at the thousand scale.When larger models with more parameters are available, even better results are expected.</p>
<p>Learning spatial knowledge with LLMs.To provide further evidence on LLMs' potential to learn spatial knowledge, we query ChatGPT with prompts: the geo-coordinates of the top 50 cities in Australia are and can you give me more cities, until 50 cities were returned.The returned geo-coordinates were correct for 49 cities, with only the geo-coordinates of Hervey Bay (a small city in Queensland) being off by 10 km.We further randomly pair up the cities to form 50 pairs.For every pair of cities  and , we query Chat-GPT with prompt:  is to which side of .The returned position results were correct for 44 pairs, with another 5 pairs obtained correct results after the geo-coordinates are further included in the prompt.Only one pair (Canberra and Orange) retained a wrong result (southwest was returned while the answer should be south).</p>
<p>These demonstrate the potential of LLMs to learn spatial knowledge and answer queries, and the research opportunities to train such models to answer more complex queries faithfully.</p>
<p>CONCLUSIONS AND CHALLENGES</p>
<p>We presented a next-generation spatial database system.This system treats ML models as first-class citizens and trains such models to "memorize" data stored in a spatial database and to generate query answers.It enables a new generation-based query paradigm that replaces the traditional retrieval-based paradigm.</p>
<p>The system will significantly enhance the accessibility of spatial database systems, as the ML models can offer an inbuilt natural language-based user interface and well understand users' query needs.It will bring huge benefits in spatial analytics and query processing, encouraging a new generation of location-based services and allowing better-informed location-based decision making.</p>
<p>To realize such a system, there are various challenges, a subset of which are summarized below.Simply fine-tuning an open-sourced LLM such as Llama 2 directly cannot realize the system.</p>
<p>(1) Faithful query result generation.Being able to generate query results directly without an extra data retrieval process offers great opportunities to answer complex spatial (analytical) queries.This, however, also brings significant challenges to ensure the faithfulness of the results generated.ML models are known to return inaccurate results.In terms of LLMs, hallucination is a known problem that impinges LLMs' wider applicability.When LLMs are applied to form the query engine of spatial databases, it is important to address the hallucination problem, e.g., to build a system that returns faithfulness scores together with the generated answers.A unique opportunity arises when building such a system for spatial databases, as traditional retrieval-based query procedures can be applied in parallel to compute query answers that serve as the ground truth for training the faithfulness scoring module.</p>
<p>(2) Large model training with structured spatial data.There are two major issues that prevent training LLMs on structured data records directly (which are probably the reason why the other vision papers [15,17,18] did not take this approach): (i) There is limited availability of structured spatial data.Comparing with the volume of free texts (e.g., web documents), the number of spatial data tables available is much smaller.The number of data tables in a spatial database is even smaller.How to train an LLM generalizable to different queries with data in such smaller scale is challenging.(ii) There is incompatibility between structured spatial data and the training procedure of LLMs.LLMs are trained via the task of predicting the next word in a sentence.Simply treating every spatial data record as a sentence and every data field as a word to train an LLM is ineffective.This is because words in a sentence have a strong correlation, and the context of a word implies the semantics of the word.In contrast, different fields of a data record may be much less relevant, and the nearby fields of a value do not necessarily imply the semantics of the value.Further, values in a data record may be numeric and continuous, and the same value may have completely different meanings in different fields, while words are discrete and each word has much fewer different meanings.Novel model design and training procedures are needed for structured spatial data.</p>
<p>(3) Versatile query processing.A problem related to the difficulty in model training given limited structured spatial data is how to answer different types of spatial queries using a model trained with limited data.While data of limited scale may be easier to be "memorized" by an ML model, it does not help train a model that is generalizable to different types of queries.Also due to the limited scale of data, the trained models may not have seen too many different prompts that imply different types of queries.The generalizability of the trained models would most likely need to come from unstructured spatial data, e.g., the Wikipedia article of a POI.Algorithms to fine-tune such models and incorporate knowledge from structured spatial data await exploration.</p>
<p>Further, the models need to have multi-step reasoning capabilities to answer complex spatial queries.For example, to "generate a half-day trip in Hamburg within walking distance from the conference venue of SIGSPATIAL'23" would require (i) producing the location of the conference venue, (ii) producing POIs within walking distance around it, and (iii) selecting and ordering the POIs to form a trip.While prompting, training, and fine-tuning strategies have been proposed for this issue, achieving such advanced reasoning capabilities remains an open challenge [3].</p>
<p>(4) Challenges in managing ML models for data management.</p>
<p>There are inherent problems in data management with ML models, such as how to update the trained ML models when the underlying data have changed (e.g., moving objects [20,21]).Such challenges have been discussed in the literature [15,18] and are not reiterated.</p>
<p>Figure 1 :
1
Figure 1: Overview of the future spatial database system</p>
<p>2 https://archive.ics.uci.edu/dataset/20/census+income</p>
<p>Table 1 :
1
Q-Error of RC-GAN
Query setMedian 75th 90thTraining queries1.231.71 3.24Testing queries1.251.86 4.08</p>
<p>Table 2 :
2
Classification Accuracy with Generated Data
Training dataPrecision Recall F15% of CensusIncome + 10% of RC-GAN0.790.97 0.8715% of CensusIncome0.820.96 0.88
There are studies on using ML models to translate queries in natural language into SQL queries (text-to-SQL)[5], but not yet for spatial queries to the best of our knowledge.
ACKNOWLEDGMENTSThis work is partially supported by Australian Research Council (ARC) Discovery Project DP230101534.
Spatial Embedding: A Generic Machine Learning Model for Spatial Query Optimization. Alberto Belussi, Sara Migliorini, Ahmed Eldawy, SIGSPATIAL. 2022</p>
<p>Spatial Structure-Aware Road Network Embedding via Graph Contrastive Learning. Yanchuan Chang, Egemen Tanin, Xin Cao, Jianzhong Qi, EDBT. 2023</p>
<p>Faith and Fate: Limits of Transformers on Compositionality. Nouha Dziri, Ximing Lu, Melanie Sclar, Lorraine Xiang, Liwei Li, Bill Yuchen Jiang, Peter Lin, Chandra West, Bhagavatula, Le Ronan, Jena D Bras, Soumya Hwang, Sean Sanyal, Xiang Welleck, Allyson Ren, Zaid Ettinger, Yejin Harchaoui, Choi, NeurIPS. 2023</p>
<p>The RLR-Tree: A Reinforcement Learning Based R-Tree for Spatial Data. Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, Sheng Wang, Proceedings of the ACM on Management of Data. 2023. 2023</p>
<p>A Survey on Deep Learning Approaches for Text-to-SQL. George Katsogiannis, -Meimarakis , Georgia Koutrika, Journal. 2023. 2023</p>
<p>AI Meets Database: AI4DB and DB4AI. Guoliang Li, Xuanhe Zhou, Lei Cao, 2021In SIGMOD</p>
<p>Human Language Understanding &amp; Reasoning. D Christopher, Manning, Daedalus. 2022. 2022</p>
<p>Preventing Bad Plans by Bounding the Impact of Cardinality Estimation Errors. Guido Moerkotte, Thomas Neumann, Gabriele Steidl, 2009. 2009PVLDB</p>
<p>Let's Speak Trajectories. Mashaal Musleh, Mohamed F Mokbel, Sofiane Abbar, 2022In SIGSPATIAL</p>
<p>DTT: An Example-Driven Tabular Transformer by Leveraging Large Language Models. Arash Dargahi, Nobari , Davood Rafiei, arXiv:2303.067482023</p>
<p>Towards an Analysis of Range Query Performance in Spatial Data Structures. Bernd-Uwe Pagel, Hans-Werner Six, Heinrich Toben, Peter Widmayer, PODS. 1993</p>
<p>. Noseong Park, Mahmoud Mohammadi, Kshitij Gorde, Sushil Jajodia, Hongkyu Park, Youngmin Kim, Data Synthesis Based on Generative Adversarial Networks. PVLDB. 2018. 2018</p>
<p>. Jianzhong Qi, Guanli Liu, Christian S Jensen, Lars Kulik, Effectively Learning Spatial Indices. PVLDB. 2020. 2020</p>
<p>A Learning Based Approach to Predict Shortest-Path Distances. Jianzhong Qi, Wei Wang, Rui Zhang, Zhuowei Zhao, EDBT. 2020</p>
<p>Querying Large Language Models with SQL. Mohammed Saeed, Nicola De Cao, Paolo Papotti, arXiv:2304.004722023</p>
<p>Unstructured and Structured Data: Can We Have the Best of Both Worlds with Large Language Models?. Wang-Chiew Tan, arXiv:2304.130102023</p>
<p>James Thorne, Majid Yazdani, Marzieh Saeidi, Fabrizio Silvestri, Sebastian Riedel, Alon Halevy, From Natural Language Processing to Neural Databases. PVLDB2021. 2021</p>
<p>Giovanni Trappolini, Andrea Santilli, Emanuele Rodol√†, Alon Halevy, Fabrizio Silvestri, arXiv:2305.01447Multimodal Neural Databases. 2023</p>
<p>Matthias Urban, Carsten Binnig, arXiv:2304.13559Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables. 2023</p>
<p>Continuous Visible K Nearest Neighbor Query on Moving Objects. Yanqiu Wang, Rui Zhang, Chuanfei Xu, Jianzhong Qi, Yu Gu, Ge Yu, Information Systems. 2014. 2014</p>
<p>Real-time Continuous Intersection Joins over Large Sets of Moving Objects Using Graphic Processing Units. G D Phillip, Zhen Ward, Rui He, Jianzhong Zhang, Qi, VLDB Journal. 2014. 2014</p>
<p>Translating Human Mobility Forecasting through Natural Language Generation. Flora D Hao Xue, Yongli Salim, Charles L A Ren, Clarke, WSDM. 2022</p>
<p>Leveraging Language Foundation Models for Human Mobility Forecasting. Bhanu Hao Xue, Flora D Prakash Voutharoja, Salim, 2022In SIGSPATIAL</p>
<p>T3S: Effective Representation Learning for Trajectory Similarity Computation. Peilun Yang, Hanchen Wang, Ying Zhang, Lu Qin, Wenjie Zhang, Xuemin Lin, ICDE. 2021</p>
<p>A Neural Database for Differentially Private Spatial Range Queries. Sepanta Zeighami, Ritesh Ahuja, Gabriel Ghinita, Cyrus Shahabi, 2022. 2022PVLDB</p>
<p>Towards a Painless Index for Spatial Objects. Rui Zhang, Jianzhong Qi, Martin Stradling, Jin Huang, ACM Transactions on Database Systems. 2014. 2014</p>            </div>
        </div>

    </div>
</body>
</html>
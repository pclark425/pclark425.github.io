<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Extraction extraction-result-894 - Theorizer</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <div class="header">
        <a href="../index.html"><i class="fas fa-flask"></i> Theorizer</a>
    </div>

    <div class="content">
        <h1>Extracted Data Details for extraction-result-894</h1>

        <div class="section">
            <h2>Extracted Data (Header)</h2>
            <div class="info-section">
                <p><strong>Extraction ID:</strong> extraction-result-894</p>
                <p><strong>Extraction Schema Used (ID):</strong> <a href="../schemas/extraction-schema-21.html">extraction-schema-21</a></p>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <p><strong>Paper ID:</strong> paper-221d453c165aca6bc1a054289eb510e558a23dca</p>
                <p><strong>Paper Title:</strong> <a href="https://www.semanticscholar.org/paper/221d453c165aca6bc1a054289eb510e558a23dca" target="_blank">Interactive Fiction Games: A Colossal Adventure</a></p>
                <p><strong>Paper Venue:</strong> AAAI Conference on Artificial Intelligence</p>
                <p><strong>Paper TL;DR:</strong> This work argues that IF games are an excellent testbed for studying language-based autonomous agents and introduces Jericho, a learning environment for man-made IF games and conducts a comprehensive study of text-agents across a rich set of games, highlighting directions in which agents can improve.</p>
                <p><strong>Paper Abstract:</strong> A hallmark of human intelligence is the ability to understand and communicate with language. Interactive Fiction games are fully text-based simulation environments where a player issues text commands to effect change in the environment and progress through the story. We argue that IF games are an excellent testbed for studying language-based autonomous agents. In particular, IF games combine challenges of combinatorial action spaces, language understanding, and commonsense reasoning. To facilitate rapid development of language-based agents, we introduce Jericho, a learning environment for man-made IF games and conduct a comprehensive study of text-agents across a rich set of games, highlighting directions in which agents can improve.</p>
                <p><strong>Cost:</strong> 0.016</p>
            </div>
        </div>

        <div class="section">
            <h2>Extracted Data (Details)</h2>
            <div class="extraction-instance-container" id="e894.0">
                <h3 class="extraction-instance">Extracted Data Instance 0 (e894.0)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Deep Reinforcement Relevance Network</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A choice-based deep RL agent that scores joint observation-action pairs Q(o,a) by encoding text observations and candidate actions with GRU encoders and selecting actions by softmax over Q-values; adapted here to parser-based IF games by using Jericho's valid-action detection to enumerate valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Deep reinforcement learning with a natural language action space.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>DRRN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>DRRN encodes the textual observation using separate GRU encoders for narrative, location description, inventory and previous command, concatenating outputs into an observation vector ν_o. Candidate actions are encoded with a separate GRU into action vectors ν_a; the observation and action vectors are concatenated and scored by a small network to produce Q(o,a). Training uses Q-learning with prioritized replay and minibatches; at inference the agent scores all valid actions and samples from a softmax over Q-values. To apply to parser-based games in this paper, DRRN relies on Jericho's search over templates pruned by world-change detection to obtain A_valid(s) (the valid-action set) and then evaluates Q for each.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho (human-made interactive fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Jericho is a suite/interface for parser-based interactive fiction (IF) games; observations are textual descriptions (narrative, location, inventory) while the true latent state includes player/item locations, inventory contents, monsters, etc., making it a partially observable, language-only POMDP. Challenges include combinatorial natural-language action spaces, commonsense reasoning, and textual-SLAM (mapping & localization from text).</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Jericho-provided facilities used as handicaps: (1) world-change-detection to test whether an action changes the world (boolean valid/invalid), (2) world-object-tree to enumerate interactive objects at current location (structured object hierarchy), (3) template-based action space and extracted game-specific vocabulary to enumerate candidate actions.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Boolean world-changed flags per action (valid/invalid), structured world-object-tree (hierarchical object/location membership), lists of templates and vocabulary (categorical text tokens).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>Short-term belief is the learned, recurrent GRU-based encoding of the latest textual observation components (narrative, location description, inventory, previous action) concatenated into ν_o; no explicit persistent map or graph belief is described for DRRN in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The observation encoding ν_o is recomputed each time step from the latest text observations (processed by the GRU encoders); Jericho's world-object-tree and world-change-detection are used to enumerate valid actions at decision time but are not described as being merged into the learned ν_o state vector.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy via value-based reinforcement learning (Q-learning over observation-action pairs); action selection is greedy/softmax over learned Q-values rather than explicit model-based search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation actions (e.g., go north) are chosen from among valid text actions by their Q-values; no explicit pathfinding algorithm or graph search (e.g., A*) is described for DRRN.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Average normalized percent completion across Jericho-supported games for DRRN reported as 1.7% (normalized game score averaged over 32 games, as reported in the paper). This DRRN evaluation used Jericho's valid-action detection handicap to enumerate valid actions.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using Jericho's world-change detection to enumerate valid actions makes DRRN applicable to parser-based IF games by converting the combinatorial language action space into a manageable candidate set; DRRN's GRU-based observation encoding provides a compact short-term belief but the agent lacks an explicit persistent mapping/SLAM-like belief, limiting long-horizon spatial planning.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e894.1">
                <h3 class="extraction-instance">Extracted Data Instance 1 (e894.1)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>TDQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Template-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A parser-based RL agent that extends LSTM-DQN to a template-based action space by separately estimating Q-values for templates and for vocabulary tokens that fill template blanks, using supervised valid-action losses to cope with massive combinatorial spaces.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>here</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>Template-DQN (TDQN)</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>TDQN uses a shared GRU-based observation encoder (same general input representation as DRRN) and three output heads: one head estimates Q(o,u) over templates u∈T, and two heads estimate Q(o,p1), Q(o,p2) over vocabulary words to fill blanks in templates. During action generation it selects a template and fills blanks with high-Q words; to guide learning it adds a supervised binary cross-entropy loss from Jericho's valid-action detector indicating which template+word combinations are valid in the current state. It also batches decoding to amortize computation when many generated combinations are invalid.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho (human-made interactive fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Same as DRRN: Jericho presents partially observable text-only IF games with textual observations and latent world states (locations, inventory, nested objects), challenging combinatorial language action spaces, and textual-SLAM-style mapping difficulties.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Jericho-provided facilities: (1) template extraction and game-specific vocabulary (templates T and vocabulary V) to define the template-based action space, (2) world-change-detection to label valid actions used in the supervised loss and to prune exploration, (3) optionally world-object-tree / noun-phrase extraction to supply candidate nouns.</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Lists of templates and vocabulary tokens, boolean valid-action labels from world-change-detection, structured world-object-tree data identifying objects at the current location.</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A GRU-based encoding of the current textual observation components concatenated into ν_o (same common input representation as DRRN); TDQN does not maintain an explicit long-term spatial map in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>Observation encoding ν_o is recomputed each timestep from the latest textual inputs; Jericho's valid-action detector supplies supervisory signals for which template+vocabulary choices are valid but those labels are used as auxiliary training targets rather than being integrated into a persistent belief representation.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Value-based learned policy (Q-learning) over templates and tokens, augmented with supervised valid-action pruning; generation is conditional in two stages (template, then fill tokens) though in the presented TDQN implementation template and token Q-values are estimated independently.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation actions are selected among template+vocabulary combinations scored by Q-values; no graph search or explicit path-finding algorithm is described.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>Average normalized percent completion across Jericho-supported games for TDQN reported as 6.1% (normalized game score averaged over 32 games). This agent evaluation used Jericho handicaps (templates, vocabulary, and valid-action detection as supervision).</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Template-based decomposition plus Jericho's valid-action detection and supervised valid-action loss significantly reduces the effective search space compared to naive vocabulary generation, but TDQN still suffers from overestimation and invalid template-token pairings because templates and tokens are modeled independently; conditional generation (token prediction conditioned on chosen template) and modern transformers are suggested as future improvements.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e894.2">
                <h3 class="extraction-instance">Extracted Data Instance 2 (e894.2)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>NAIL (general interactive fiction agent)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>A hand-engineered, general IF agent that builds maps and object models via heuristics and uses an external web-based language model to choose interactions with novel objects, designed to play unseen games without training.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>NAIL: A general interactive fiction agent.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>use</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>NAIL</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>NAIL is a general-game IF agent (no per-game training) that employs a set of manually-created heuristics to: (1) build a map of locations and objects, (2) reason about which actions succeeded or failed, and (3) use a web-based language model to propose how to interact with novel objects. NAIL operates without the Jericho handicaps and relies on heuristic mapping and reasoning to drive exploration and decision-making in a single episode.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Jericho (human-made interactive fiction games)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Jericho's IF games are partially observable text-only environments (POMDPs) where observations are text and latent state includes location connectivity and object nesting; navigation is non-Euclidean and detection of whether a move succeeded is required to construct maps.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Externally-accessed web-based language model (used to generate/suggest interactions with novel objects); internal/map-building heuristics that produce an explicit map of locations and object-location relations (the map is an internal structured representation but functionally acts as a tool for planning).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Textual action suggestions from the web-based language model (text snippets / ranked action candidates); structured map data (graph of locations, objects associated with locations), and reasoning labels about action validity (success/failure).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>An explicit, heuristic-built map of locations and objects (graph-like representation) which records discovered locations, object presence, and connectivity; heuristics also mark whether navigational actions succeeded and whether a location has been seen before.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>NAIL incrementally updates its internal map and object registry using heuristics: when actions are taken the agent inspects responses to determine if the world changed or if a new location was reached, adds nodes/edges or object memberships to the map accordingly, and uses this evolving map to reason about future actions. Outputs from the web-based language model are used to generate candidate interactions but are not described as directly altering the map unless the resulting actions change the observed world and thereby trigger map updates.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Heuristic, map-aware planning driven by hand-crafted rules and the internal map; language-model outputs are used to propose candidate interactions, with heuristics selecting or filtering them. The approach is not a learned model-based planner but a heuristic symbolic planner augmented by an external LM for action generation.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation uses the internally constructed graph/map of locations and heuristics to detect previously-seen locations and decide navigational actions; no explicit shortest-path algorithm (e.g., A*) is described in the paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td>NAIL's average normalized percent completion across Jericho-supported games is reported as 4.9% (normalized game score averaged across 32 games); NAIL notably uses no Jericho-provided handicaps in evaluation and was developed on many of the evaluated games.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td>False</td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Explicit map-building heuristics and reasoning about action validity enable a single-episode general agent (NAIL) to make nontrivial progress without per-game training; augmenting heuristic planners with an external web LM helps propose interactions for novel objects, but hand-engineered heuristics are brittle and performance lags behind per-game RL agents that are trained on each game.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
            <div class="extraction-instance-container" id="e894.3">
                <h3 class="extraction-instance">Extracted Data Instance 3 (e894.3)</h3>
                <div class="extraction-query"><strong>Extraction Query:</strong> Extract any mentions of agents that use external tools for planning in partially observable text environments, including how they maintain belief states, incorporate tool outputs, and perform navigation or path-finding tasks.</div>
                <table>
                    <thead>
                        <tr>
                            <th style="width: 30%;">Field</th>
                            <th style="width: 70%;">Value</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>name_short</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>name_full</strong></td>
                            <td>Knowledge Graph DQN (KG-DQN)</td>
                        </tr>
                        <tr>
                            <td><strong>brief_description</strong></td>
                            <td>An approach that builds a knowledge graph during exploration and uses it as a state representation for a deep Q-network; also uses QA pretraining to improve action selection in partially observable text games.</td>
                        </tr>
                        <tr>
                            <td><strong>citation_title</strong></td>
                            <td>Playing text-adventure games with graph-based deep reinforcement learning.</td>
                        </tr>
                        <tr>
                            <td><strong>mention_or_use</strong></td>
                            <td>mention</td>
                        </tr>
                        <tr>
                            <td><strong>agent_name</strong></td>
                            <td>KG-DQN</td>
                        </tr>
                        <tr>
                            <td><strong>agent_description</strong></td>
                            <td>KG-DQN constructs a knowledge graph (nodes for entities/locations/objects and edges for relations) while exploring the environment; this graph is used as the agent's state representation input to a deep Q-network. The paper also uses question-answering style pretraining (asking 'what action next?') to initialize the deep Q-network, and explores transfer of policies seeded by knowledge graphs between games.</td>
                        </tr>
                        <tr>
                            <td><strong>environment_name</strong></td>
                            <td>Text-based interactive fiction games (as used in prior work; referenced in Jericho context)</td>
                        </tr>
                        <tr>
                            <td><strong>environment_description</strong></td>
                            <td>Partially observable text environments where textual observations reveal local facts (objects present, descriptions) while the full latent state (global object locations, nested objects, solved puzzles) is not directly observable; challenges include SLAM-like mapping and long-horizon planning.</td>
                        </tr>
                        <tr>
                            <td><strong>is_partially_observable</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>external_tools_used</strong></td>
                            <td>Internally constructed knowledge graph (structured graph representation built during exploration); question-answering pretraining data/methods (supervised QA models used to pretrain components).</td>
                        </tr>
                        <tr>
                            <td><strong>tool_output_types</strong></td>
                            <td>Structured graph data (nodes/edges representing entities and relations), and trained QA model outputs (textual or action-label recommendations used to pretrain the policy).</td>
                        </tr>
                        <tr>
                            <td><strong>belief_state_mechanism</strong></td>
                            <td>A persistent knowledge graph maintained during exploration that encodes discovered entities, locations, object containment, and relations; this graph functions as the agent's belief/state representation.</td>
                        </tr>
                        <tr>
                            <td><strong>incorporates_tool_outputs_in_belief</strong></td>
                            <td>True</td>
                        </tr>
                        <tr>
                            <td><strong>belief_update_description</strong></td>
                            <td>The knowledge graph is incrementally updated as the agent explores: new nodes and edges are added when observations reveal objects/locations/relations, and interactions that change the world modify the graph (e.g., moving an object updates containment edges). The updated graph is then used as input to the Q-network for subsequent action selection.</td>
                        </tr>
                        <tr>
                            <td><strong>planning_approach</strong></td>
                            <td>Learned policy via deep Q-learning where the Q-network takes the knowledge-graph-based state representation; QA pretraining helps bias the network towards useful action choices. This is a learned policy using structured belief rather than explicit planner/search.</td>
                        </tr>
                        <tr>
                            <td><strong>uses_shortest_path_planning</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>navigation_method</strong></td>
                            <td>Navigation decisions are made via the learned Q-policy operating over the knowledge-graph state representation; no explicit graph search / shortest-path algorithm is described in the referenced work as summarized in this paper.</td>
                        </tr>
                        <tr>
                            <td><strong>performance_with_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>performance_without_tools</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>has_tool_ablation</strong></td>
                            <td><span class="empty-note">null</span></td>
                        </tr>
                        <tr>
                            <td><strong>key_findings</strong></td>
                            <td>Using an explicit knowledge-graph belief state helps address partial observability by providing a structured memory of discovered entities and relations; QA-style pretraining of the Q-network accelerates learning and aids action selection. The approach shows promise for overcoming partial observability and combinatorial action spaces, motivating use of graph-based state representations in IF.</td>
                        </tr>
                        <tr>
                            <td><strong>source_info</strong></td>
                            <td>{'paper_title': 'Interactive Fiction Games: A Colossal Adventure', 'publication_date_yy_mm': '2019-09'}</td>
                        </tr>
                    </tbody>
                </table>
            </div>
        </div>

        <div class="section">
            <h2>Potentially Relevant New Papers (mentioned by this paper)</h2>
            <ol>
                <li>Playing text-adventure games with graph-based deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>NAIL: A general interactive fiction agent. <em>(Rating: 2)</em></li>
                <li>Learn what not to learn: Action elimination with deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Language understanding for text-based games using deep reinforcement learning. <em>(Rating: 2)</em></li>
                <li>Textworld: A learning environment for text-based games. <em>(Rating: 2)</em></li>
                <li>What can you do with a rock? affordance extraction via word embeddings. <em>(Rating: 1)</em></li>
                <li>Learning to speak and act in a fantasy text adventure game. <em>(Rating: 1)</em></li>
            </ol>
        </div>

        <div class="section">
            <h2>Extracted Data (Debug)</h2>
            <pre><code>{
    "id": "extraction-result-894",
    "paper_id": "paper-221d453c165aca6bc1a054289eb510e558a23dca",
    "extraction_schema_id": "extraction-schema-21",
    "extracted_data": [
        {
            "name_short": "DRRN",
            "name_full": "Deep Reinforcement Relevance Network",
            "brief_description": "A choice-based deep RL agent that scores joint observation-action pairs Q(o,a) by encoding text observations and candidate actions with GRU encoders and selecting actions by softmax over Q-values; adapted here to parser-based IF games by using Jericho's valid-action detection to enumerate valid actions.",
            "citation_title": "Deep reinforcement learning with a natural language action space.",
            "mention_or_use": "use",
            "agent_name": "DRRN",
            "agent_description": "DRRN encodes the textual observation using separate GRU encoders for narrative, location description, inventory and previous command, concatenating outputs into an observation vector ν_o. Candidate actions are encoded with a separate GRU into action vectors ν_a; the observation and action vectors are concatenated and scored by a small network to produce Q(o,a). Training uses Q-learning with prioritized replay and minibatches; at inference the agent scores all valid actions and samples from a softmax over Q-values. To apply to parser-based games in this paper, DRRN relies on Jericho's search over templates pruned by world-change detection to obtain A_valid(s) (the valid-action set) and then evaluates Q for each.",
            "environment_name": "Jericho (human-made interactive fiction games)",
            "environment_description": "Jericho is a suite/interface for parser-based interactive fiction (IF) games; observations are textual descriptions (narrative, location, inventory) while the true latent state includes player/item locations, inventory contents, monsters, etc., making it a partially observable, language-only POMDP. Challenges include combinatorial natural-language action spaces, commonsense reasoning, and textual-SLAM (mapping & localization from text).",
            "is_partially_observable": true,
            "external_tools_used": "Jericho-provided facilities used as handicaps: (1) world-change-detection to test whether an action changes the world (boolean valid/invalid), (2) world-object-tree to enumerate interactive objects at current location (structured object hierarchy), (3) template-based action space and extracted game-specific vocabulary to enumerate candidate actions.",
            "tool_output_types": "Boolean world-changed flags per action (valid/invalid), structured world-object-tree (hierarchical object/location membership), lists of templates and vocabulary (categorical text tokens).",
            "belief_state_mechanism": "Short-term belief is the learned, recurrent GRU-based encoding of the latest textual observation components (narrative, location description, inventory, previous action) concatenated into ν_o; no explicit persistent map or graph belief is described for DRRN in this paper.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "The observation encoding ν_o is recomputed each time step from the latest text observations (processed by the GRU encoders); Jericho's world-object-tree and world-change-detection are used to enumerate valid actions at decision time but are not described as being merged into the learned ν_o state vector.",
            "planning_approach": "Learned policy via value-based reinforcement learning (Q-learning over observation-action pairs); action selection is greedy/softmax over learned Q-values rather than explicit model-based search.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation actions (e.g., go north) are chosen from among valid text actions by their Q-values; no explicit pathfinding algorithm or graph search (e.g., A*) is described for DRRN.",
            "performance_with_tools": "Average normalized percent completion across Jericho-supported games for DRRN reported as 1.7% (normalized game score averaged over 32 games, as reported in the paper). This DRRN evaluation used Jericho's valid-action detection handicap to enumerate valid actions.",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Using Jericho's world-change detection to enumerate valid actions makes DRRN applicable to parser-based IF games by converting the combinatorial language action space into a manageable candidate set; DRRN's GRU-based observation encoding provides a compact short-term belief but the agent lacks an explicit persistent mapping/SLAM-like belief, limiting long-horizon spatial planning.",
            "uuid": "e894.0",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "TDQN",
            "name_full": "Template-DQN",
            "brief_description": "A parser-based RL agent that extends LSTM-DQN to a template-based action space by separately estimating Q-values for templates and for vocabulary tokens that fill template blanks, using supervised valid-action losses to cope with massive combinatorial spaces.",
            "citation_title": "here",
            "mention_or_use": "use",
            "agent_name": "Template-DQN (TDQN)",
            "agent_description": "TDQN uses a shared GRU-based observation encoder (same general input representation as DRRN) and three output heads: one head estimates Q(o,u) over templates u∈T, and two heads estimate Q(o,p1), Q(o,p2) over vocabulary words to fill blanks in templates. During action generation it selects a template and fills blanks with high-Q words; to guide learning it adds a supervised binary cross-entropy loss from Jericho's valid-action detector indicating which template+word combinations are valid in the current state. It also batches decoding to amortize computation when many generated combinations are invalid.",
            "environment_name": "Jericho (human-made interactive fiction games)",
            "environment_description": "Same as DRRN: Jericho presents partially observable text-only IF games with textual observations and latent world states (locations, inventory, nested objects), challenging combinatorial language action spaces, and textual-SLAM-style mapping difficulties.",
            "is_partially_observable": true,
            "external_tools_used": "Jericho-provided facilities: (1) template extraction and game-specific vocabulary (templates T and vocabulary V) to define the template-based action space, (2) world-change-detection to label valid actions used in the supervised loss and to prune exploration, (3) optionally world-object-tree / noun-phrase extraction to supply candidate nouns.",
            "tool_output_types": "Lists of templates and vocabulary tokens, boolean valid-action labels from world-change-detection, structured world-object-tree data identifying objects at the current location.",
            "belief_state_mechanism": "A GRU-based encoding of the current textual observation components concatenated into ν_o (same common input representation as DRRN); TDQN does not maintain an explicit long-term spatial map in this paper.",
            "incorporates_tool_outputs_in_belief": false,
            "belief_update_description": "Observation encoding ν_o is recomputed each timestep from the latest textual inputs; Jericho's valid-action detector supplies supervisory signals for which template+vocabulary choices are valid but those labels are used as auxiliary training targets rather than being integrated into a persistent belief representation.",
            "planning_approach": "Value-based learned policy (Q-learning) over templates and tokens, augmented with supervised valid-action pruning; generation is conditional in two stages (template, then fill tokens) though in the presented TDQN implementation template and token Q-values are estimated independently.",
            "uses_shortest_path_planning": false,
            "navigation_method": "Navigation actions are selected among template+vocabulary combinations scored by Q-values; no graph search or explicit path-finding algorithm is described.",
            "performance_with_tools": "Average normalized percent completion across Jericho-supported games for TDQN reported as 6.1% (normalized game score averaged over 32 games). This agent evaluation used Jericho handicaps (templates, vocabulary, and valid-action detection as supervision).",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Template-based decomposition plus Jericho's valid-action detection and supervised valid-action loss significantly reduces the effective search space compared to naive vocabulary generation, but TDQN still suffers from overestimation and invalid template-token pairings because templates and tokens are modeled independently; conditional generation (token prediction conditioned on chosen template) and modern transformers are suggested as future improvements.",
            "uuid": "e894.1",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "NAIL",
            "name_full": "NAIL (general interactive fiction agent)",
            "brief_description": "A hand-engineered, general IF agent that builds maps and object models via heuristics and uses an external web-based language model to choose interactions with novel objects, designed to play unseen games without training.",
            "citation_title": "NAIL: A general interactive fiction agent.",
            "mention_or_use": "use",
            "agent_name": "NAIL",
            "agent_description": "NAIL is a general-game IF agent (no per-game training) that employs a set of manually-created heuristics to: (1) build a map of locations and objects, (2) reason about which actions succeeded or failed, and (3) use a web-based language model to propose how to interact with novel objects. NAIL operates without the Jericho handicaps and relies on heuristic mapping and reasoning to drive exploration and decision-making in a single episode.",
            "environment_name": "Jericho (human-made interactive fiction games)",
            "environment_description": "Jericho's IF games are partially observable text-only environments (POMDPs) where observations are text and latent state includes location connectivity and object nesting; navigation is non-Euclidean and detection of whether a move succeeded is required to construct maps.",
            "is_partially_observable": true,
            "external_tools_used": "Externally-accessed web-based language model (used to generate/suggest interactions with novel objects); internal/map-building heuristics that produce an explicit map of locations and object-location relations (the map is an internal structured representation but functionally acts as a tool for planning).",
            "tool_output_types": "Textual action suggestions from the web-based language model (text snippets / ranked action candidates); structured map data (graph of locations, objects associated with locations), and reasoning labels about action validity (success/failure).",
            "belief_state_mechanism": "An explicit, heuristic-built map of locations and objects (graph-like representation) which records discovered locations, object presence, and connectivity; heuristics also mark whether navigational actions succeeded and whether a location has been seen before.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "NAIL incrementally updates its internal map and object registry using heuristics: when actions are taken the agent inspects responses to determine if the world changed or if a new location was reached, adds nodes/edges or object memberships to the map accordingly, and uses this evolving map to reason about future actions. Outputs from the web-based language model are used to generate candidate interactions but are not described as directly altering the map unless the resulting actions change the observed world and thereby trigger map updates.",
            "planning_approach": "Heuristic, map-aware planning driven by hand-crafted rules and the internal map; language-model outputs are used to propose candidate interactions, with heuristics selecting or filtering them. The approach is not a learned model-based planner but a heuristic symbolic planner augmented by an external LM for action generation.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Navigation uses the internally constructed graph/map of locations and heuristics to detect previously-seen locations and decide navigational actions; no explicit shortest-path algorithm (e.g., A*) is described in the paper.",
            "performance_with_tools": "NAIL's average normalized percent completion across Jericho-supported games is reported as 4.9% (normalized game score averaged across 32 games); NAIL notably uses no Jericho-provided handicaps in evaluation and was developed on many of the evaluated games.",
            "performance_without_tools": null,
            "has_tool_ablation": false,
            "key_findings": "Explicit map-building heuristics and reasoning about action validity enable a single-episode general agent (NAIL) to make nontrivial progress without per-game training; augmenting heuristic planners with an external web LM helps propose interactions for novel objects, but hand-engineered heuristics are brittle and performance lags behind per-game RL agents that are trained on each game.",
            "uuid": "e894.2",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        },
        {
            "name_short": "KG-DQN",
            "name_full": "Knowledge Graph DQN (KG-DQN)",
            "brief_description": "An approach that builds a knowledge graph during exploration and uses it as a state representation for a deep Q-network; also uses QA pretraining to improve action selection in partially observable text games.",
            "citation_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "mention_or_use": "mention",
            "agent_name": "KG-DQN",
            "agent_description": "KG-DQN constructs a knowledge graph (nodes for entities/locations/objects and edges for relations) while exploring the environment; this graph is used as the agent's state representation input to a deep Q-network. The paper also uses question-answering style pretraining (asking 'what action next?') to initialize the deep Q-network, and explores transfer of policies seeded by knowledge graphs between games.",
            "environment_name": "Text-based interactive fiction games (as used in prior work; referenced in Jericho context)",
            "environment_description": "Partially observable text environments where textual observations reveal local facts (objects present, descriptions) while the full latent state (global object locations, nested objects, solved puzzles) is not directly observable; challenges include SLAM-like mapping and long-horizon planning.",
            "is_partially_observable": true,
            "external_tools_used": "Internally constructed knowledge graph (structured graph representation built during exploration); question-answering pretraining data/methods (supervised QA models used to pretrain components).",
            "tool_output_types": "Structured graph data (nodes/edges representing entities and relations), and trained QA model outputs (textual or action-label recommendations used to pretrain the policy).",
            "belief_state_mechanism": "A persistent knowledge graph maintained during exploration that encodes discovered entities, locations, object containment, and relations; this graph functions as the agent's belief/state representation.",
            "incorporates_tool_outputs_in_belief": true,
            "belief_update_description": "The knowledge graph is incrementally updated as the agent explores: new nodes and edges are added when observations reveal objects/locations/relations, and interactions that change the world modify the graph (e.g., moving an object updates containment edges). The updated graph is then used as input to the Q-network for subsequent action selection.",
            "planning_approach": "Learned policy via deep Q-learning where the Q-network takes the knowledge-graph-based state representation; QA pretraining helps bias the network towards useful action choices. This is a learned policy using structured belief rather than explicit planner/search.",
            "uses_shortest_path_planning": null,
            "navigation_method": "Navigation decisions are made via the learned Q-policy operating over the knowledge-graph state representation; no explicit graph search / shortest-path algorithm is described in the referenced work as summarized in this paper.",
            "performance_with_tools": null,
            "performance_without_tools": null,
            "has_tool_ablation": null,
            "key_findings": "Using an explicit knowledge-graph belief state helps address partial observability by providing a structured memory of discovered entities and relations; QA-style pretraining of the Q-network accelerates learning and aids action selection. The approach shows promise for overcoming partial observability and combinatorial action spaces, motivating use of graph-based state representations in IF.",
            "uuid": "e894.3",
            "source_info": {
                "paper_title": "Interactive Fiction Games: A Colossal Adventure",
                "publication_date_yy_mm": "2019-09"
            }
        }
    ],
    "potentially_relevant_new_papers": [
        {
            "paper_title": "Playing text-adventure games with graph-based deep reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "NAIL: A general interactive fiction agent.",
            "rating": 2
        },
        {
            "paper_title": "Learn what not to learn: Action elimination with deep reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Language understanding for text-based games using deep reinforcement learning.",
            "rating": 2
        },
        {
            "paper_title": "Textworld: A learning environment for text-based games.",
            "rating": 2
        },
        {
            "paper_title": "What can you do with a rock? affordance extraction via word embeddings.",
            "rating": 1
        },
        {
            "paper_title": "Learning to speak and act in a fantasy text adventure game.",
            "rating": 1
        }
    ],
    "cost": 0.015555,
    "model_str": "gpt-5-mini"
}</code></pre>
        </div>
        <div class="section">
            <h2>Paper</h2>
            <div class="paper-content"><h1>Interactive Fiction Games: A Colossal Adventure</h1>
<p>Matthew Hausknecht Prithviraj Ammanabrolu Marc-Alexandre Côté<br>Microsoft Research AI Georgia Institute of Technology Microsoft Research Montréal<br>Xingdi Yuan<br>Microsoft Research Montréal</p>
<h4>Abstract</h4>
<p>A hallmark of human intelligence is the ability to understand and communicate with language. Interactive Fiction games are fully text-based simulation environments where a player issues text commands to effect change in the environment and progress through the story. We argue that IF games are an excellent testbed for studying language-based autonomous agents. In particular, IF games combine challenges of combinatorial action spaces, language understanding, and commonsense reasoning. To facilitate rapid development of languagebased agents, we introduce Jericho, a learning environment for man-made IF games and conduct a comprehensive study of text-agents across a rich set of games, highlighting directions in which agents can improve.</p>
<h2>1 Introduction</h2>
<p>Interactive fiction (IF) games are software environments in which players observe textual descriptions of the simulated world, issue text actions, and receive score progress through the story. As illustrated by Figure 1, humans bring competencies in natural language understanding, commonsense reasoning, and deduction to bear in order to infer the context and objectives of a game. We believe IF environments will provide a good testbed for studying artificial development of these capabilities. Beyond games, real-world applications such as voice-activated personal assistants also exist at the intersection of natural language understanding, natural language generation, and sequential decision making.</p>
<p>From a machine learning perspective, Interactive Fiction games exist at the intersection of natural language processing and sequential decision making. Like many NLP tasks, they require natural language understanding, but unlike most NLP tasks, IF games are sequential decision making problems in which actions change the subsequent world states of the game and choices made early in a game may have long term effects on the eventual endings. Reinforcement Learning (Sutton and Barto 1998) studies sequential decision making problems and has shown promise in visionbased (Jaderberg et al. 2016) and control-based (OpenAI et</p>
<p><sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>Observation: This bedroom is extremely spare, with dirty laundry scattered haphazardly all over the floor. Cleaner clothing can be found in the dresser. A bathroom lies to the south, while a door to the east leads to the living room. On the end table are a telephone, a wallet and some keys.
The phone rings.
Action: Answer phone
Observation: You pick up the phone. "Hadley!" a shrill voice cries. "Hadley, haven't you even left yet?? You knew that our presentation was at nine o'clock sharp! First the thing with the printers, now this - there won't even be enough left of you for Bowman to fire once he's done with you. Now get the hell down here!!"</p>
<p>Action: Examine me
Observation: You're covered with mud and dried sweat. It was an exhausting night - no wonder you overslept! Even making it to the bed before conking out was a heroic accomplishment.</p>
<p>Figure 1: Excerpt from Adam Cadre's 9:05.
al. 2018) environments, but has less commonly been applied in the context of language-based tasks.</p>
<p>Why study environments that feature ungrounded natural language instead of multimodal environments that provide visual grounding for language? We believe that ungrounded natural language is inescapable in certain real world tasks such as voice-activated personal assistants.</p>
<p>The contributions of this paper are as follows: First, we introduce Jericho, a learning environment for human-made IF games. Second, we introduce a template-based action space and that we argue is appropriate for language generation. Third, we conduct an empirical evaluation of learning agents across a set of human-made games.</p>
<h2>2 Research Challenges</h2>
<p>From the perspective of reinforcement learning, IF games can be modeled as partially observable Markov decision processes (POMDPs) defined by $(S, T, A, O, R)$. Observations $o \in O$ correspond to the game's text responses, while latent states $s \in S$ correspond to player and item locations, inventory contents, monsters, etc. Text-based actions $a \in A$ change the game state according to an latent transition function $T\left(s^{\prime} \mid s, a\right)$, and the agent receives rewards $r$ from an unknown reward function $R(s, a)$. To succeed in these environments, agents must generate natural language actions, reason about entities and affordances, and represent their knowledge about the game world. We present these challenges in greater detail:</p>
<p>Combinatorial Action Space Reinforcement learning has studied agents that operate in discrete or continuous action space environments. However, IF games require the agent to operate in the combinatorial action space of natural language. Combinatorial spaces pose extremely difficult exploration problems for existing agents. For example, an agent generating a four-word sentence from a modest vocabulary of size 700 , is effectively exploring a space of $\left|700^{4}\right|=240$ billion possible actions. Further complicating, this challenge is the fact that natural language commands are interpreted by the game's parser - which recognizes a gamespecific subset of possible commands. For example, out of the 240 billion possible actions there may be only ten valid actions at each step - actions that are both recognized by the game's parser and generate a change in world state.</p>
<p>As discussed in Section 4.1, we propose the use of a template-based action space in which the agent first chooses from a template of the form put $O B J$ in $O B J$ and then fills in the blanks using the vocabulary. A typical game may have around 200 templates each with up to two blanks - yielding an action space $\left|200 * 700^{2}\right|=98$ million, three orders of magnitude smaller than the naive space but six orders of magnitude greater than most discrete RL environments.</p>
<p>Commonsense Reasoning Due to the lack of graphics, IF games rely on the player's commonsense knowledge as a prior for how to interact with the game world. For example, a human player encountering a locked chest intuitively understands that the chest needs to be unlocked with some type of key, and once unlocked, the chest can be opened and will probably contain useful items. They may make a mental note to return to the chest if a key is found in the future. They may even mark the location of the chest on a map to easily find their way back.</p>
<p>These inferences are possible for humans who have years of embodied experience interacting with chests, cabinets, safes, and all variety of objects. Artificial agents lack the commonsense knowledge gained through years of grounded language acquisition and have no reason to prefer opening a chest to eating it. Also known as affordance extraction (Gibson 1977; Fulda et al. 2017), the problem of choosing which actions or verbs to pair with a particular noun is central to IF game playing. However, the problem of commonsense reasoning extends much further than affordance extraction: Games require planning which items to carry in a limited inventory space, strategically deciding whether to fight or
flee from a monster, and spatial reasoning such as stacking garbage cans against the wall of a building to access a second-floor window.</p>
<p>Knowledge Representation IF games span many distinct locations, each with unique descriptions, objects, and characters. Players move between locations by issuing navigational commands like go west. Due to the large number of locations in many games, humans often create maps to navigate efficiently and avoid getting lost. This gives rise to the Textual-SLAM problem, a textual variant of Simultaneous localization and mapping (SLAM) (Thrun, Burgard, and Fox 2005) problem of constructing a map while navigating a new environment. In particular, because connectivity between locations is not necessarily Euclidean, agents need to be able to detect when a navigational action has succeeded or failed and whether the location reached was previously seen or new. Beyond location connectivity, it's also helpful to keep track of the objects present at each location, with the understanding that objects can be nested inside of other objects, such as food in a refrigerator or a sword in a chest.</p>
<h2>3 Related Work</h2>
<p>In contrast to the parser-based games studied in this paper, choice-based games provide a list of possible actions at each step, so learning agents must only choose between the candidates. The DRRN algorithm for choice-based games (He et al. 2016; Zelinka 2018) estimates Q-Values for a particular action from a particular state. This network is evaluated once for each possible action, and the action with the maximum Q-Value is selected. While this approach is effective for choice-based games which have only a handful of candidate actions at each step, it is difficult to scale to parserbased games where the action space is vastly larger.</p>
<p>In terms of parser-based games, such as the ones examined in this paper, several approaches have been investigated: LSTM-DQN (Narasimhan, Kulkarni, and Barzilay 2015), considers verb-noun actions up to two-words in length. Separate Q-Value estimates are produced for each possible verb and object, and the action consists of pairing the maximally valued verb combined with the maximally valued object. LSTM-DQN was demonstrated to work on two small-scale domains, but human-made games, such as those studied in this paper, represent a significant increase in both complexity and vocabulary.</p>
<p>Another approach to affordance extraction (Fulda et al. 2017) identified a vector in word2vec (Mikolov et al. 2013) space that encodes affordant behavior. When applied to the noun sword, this vector produces affordant verbs such as vanquish, impale, duel, and battle. The authors use this method to prioritize verbs for a Q-Learning agent to pair with in-game objects.</p>
<p>An alternative strategy has been to reduce the combinatorial action space of parser-based games into a discrete space containing the minimum set of actions required to finish the game. This approach requires a walkthrough or expert demonstration in order to define the space of minimal actions, which limits its applicability to new and unseen games. Following this approach, (Zahavy et al. 2018) employ this strategy with their action-elimination network, a</p>
<p>classifier that predicts which predefined actions will not effect any world change or be recognized by the parser. Masking these invalid actions, the learning agent subsequently evaluates the set of remaining valid actions and picks the one with the highest predicted Q-Value.</p>
<p>The TextWorld framework (Côté et al. 2018) supports procedural generation of parser-based IF games, allowing complexity and content of the generated games to be scaled to the difficulty needed for research. TextWorld domains have already proven suitable for reinforcement learning agents (Yuan et al. 2018) which were shown to be capable of learning on a set of environments and then generalizing to unseen ones at test time. Recently, Yuan et al. (2019) propose QAit, a set of question answering tasks based on games generated using TextWorld. QAit focuses on helping agents to learn procedural knowledge in an informationseeking fashion, it also introduces the practice of generating unlimited training games on the fly. With the ability to scale the difficulty of domains, TextWorld may be key to creating a curriculum of learning tasks and helping agents scale to human-made games.</p>
<p>Ammanabrolu and Riedl (2019a) present the Knowledge Graph DQN or KG-DQN, an approach where a knowledge graph built during exploration is used as a state representation for a deep reinforcement learning based agent. They also use question-answering techniques-asking the question of what action best next to take-to pretrain a deep $Q$ network. These techniques are then shown to aid in overcoming the twin challenges of a partially observable state space and a combinatorial action space. Ammanabrolu and Riedl (2019b) further expand on this work, exploring methods of transferring control policies in text-games, using knowledge graphs to seed an agent with useful commonsense knowledge and transfer knowledge between different games within a domain. They show that training on a source game and transferring to target game within the same genre-e.g. horror or slice of life-is more effective and efficient than training from scratch on the target game.</p>
<p>Finally, although not a sequential decision making problem, Light (Urbanek et al. 2019) is a crowdsourced dataset of text-adventure game dialogues. The authors demonstrate that supervised training of transformer-based models have the ability to generate contextually relevant dialog, actions, and emotes.</p>
<h2>4 Jericho Environment</h2>
<p>Jericho is an open-source ${ }^{1}$ Python-based IF environment, which provides an OpenAI-Gym-like interface (Brockman et al. 2016) for learning agents to connect with IF games.</p>
<h3>4.1 Template-Based Action Generation</h3>
<p>Template-based action generation involves first selecting a template, then choosing words to fill in the blanks in that template. The set of game-specific templates are identified by decompiling a game to extract the possible subroutines - each template corresponds to a different subroutine. Templates contain a maximum of two blanks to</p>
<p><sup id="fnref2:0"><a class="footnote-ref" href="#fn:0">1</a></sup>be filled. Additionally, game-specific vocabulary is also extracted and provides a list of all words recognized by the game's parser. Combining templates with vocabulary yields a game-specific action space, which is far more tractable than operating in the pure vocabulary space.</p>
<h3>4.2 World Object Tree</h3>
<p>The world object tree is a semi-interpretable representation of game state that IF games use internally to codify the relationship between the objects and locations that populate the game world. Each object in the tree has a parent, child, and sibling. These relationships between objects are used to encode presence: a location contains children objects that correspond to the items present at that location. Similarly, there is an object corresponding to the player, whose parent is the player's location and whose children are the objects in the player's inventory.</p>
<p>Jericho's ability to extract world-object-trees forms the basis for world-change-detection (described in the next subsection) and ground-truth object detection. Ground truth object detection searches the object tree for all non-player objects present at the current location, thus sidestepping the challenge of identifying interactive objects from a location's description.</p>
<h3>4.3 World Change Detection</h3>
<p>Jericho has the ability to best-guess detect whether an action changed the world state of the game. Using this facility, it's possible to identify the valid actions from a state by performing a search over template-based actions and excluding any actions that don't change the world state. We demonstrate the feasibility of valid action detection by training a choicebased learner, DRRN (Section 5.2). World change detection is based on identifying changes to the world-object tree ${ }^{2}$ and can fail to detect valid actions whose effects alter only global variables instead of the object tree. In practice, these failures are rare.</p>
<h3>4.4 Supported Games</h3>
<p>For supported games, Jericho is able to detect game score, move count, and world change. Jericho supports a set of fifty-six human-made IF games that cover a variety of genres: dungeon crawl, Sci-Fi, mystery, comedy, and horror. Games were selected from classic Infocom titles such as Zork and Hitchhiker's Guide to the Galaxy, as well as newer, community-created titles like Anchorhead and Afflicted. All supported games use a point-based scoring system, which serves as the agent's reward.</p>
<p>Unsupported games may be played through Jericho, without the support of score detection, move counts, or worldchange detection. There exists a large collection of over a thousand unsupported games ${ }^{3}$, which may be useful for unsupervised pretraining or intrinsic motivation.</p>
<p><sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<h3>4.5 Identifying Valid Actions</h3>
<p>Valid actions are actions recognized by the game's parser that cause changes in the game state. When playing new games, identifying valid actions is one of the primary difficulties encountered by humans and agents alike. Jericho identifies valid actions using the following procedure:</p>
<div class="codehilite"><pre><span></span><code><span class="n">Algorithm</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="n">Procedure</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">Identifying</span><span class="w"> </span><span class="n">Valid</span><span class="w"> </span><span class="n">Actions</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Jericho</span><span class="w"> </span><span class="n">environment</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">T</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Set</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">action</span><span class="w"> </span><span class="n">templates</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">o</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="p">)</span><span class="w"> </span><span class="n">Textual</span><span class="w"> </span><span class="n">observation</span>
<span class="w">    </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}</span><span class="w"> </span>\<span class="n">leftarrow</span>\<span class="n">left</span>\<span class="p">{</span><span class="n">p_</span><span class="p">{</span><span class="mi">1</span><span class="p">}</span><span class="w"> </span>\<span class="n">ldots</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span><span class="n">n</span><span class="p">}</span>\<span class="n">right</span>\<span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">Interactive</span><span class="w"> </span><span class="n">objects</span><span class="w"> </span><span class="n">identified</span><span class="w"> </span><span class="n">with</span>
<span class="w">        </span><span class="n">noun</span><span class="o">-</span><span class="n">phrase</span><span class="w"> </span><span class="n">extraction</span><span class="w"> </span><span class="ow">or</span><span class="w"> </span><span class="n">world</span><span class="w"> </span><span class="n">object</span><span class="w"> </span><span class="n">tree</span><span class="o">.</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">Y</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">varnothing</span>\<span class="p">)</span><span class="w"> </span><span class="n">List</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">valid</span><span class="w"> </span><span class="n">actions</span>
<span class="w">    </span>\<span class="p">(</span><span class="n">s</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span><span class="w"> </span><span class="o">.</span><span class="w"> </span>\<span class="n">operatorname</span><span class="p">{</span><span class="n">save</span><span class="p">}()</span>\<span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Save</span><span class="w"> </span><span class="n">current</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">state</span>
<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">template</span><span class="w"> </span>\<span class="p">(</span><span class="n">u</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">T</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">        </span><span class="k">for</span><span class="w"> </span><span class="n">all</span><span class="w"> </span><span class="n">combinations</span><span class="w"> </span>\<span class="p">(</span><span class="n">p_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span><span class="w"> </span>\<span class="ow">in</span><span class="w"> </span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">P</span><span class="p">}</span>\<span class="p">)</span><span class="w"> </span><span class="n">do</span>
<span class="w">            </span><span class="n">Action</span><span class="w"> </span>\<span class="p">(</span><span class="n">a</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">u</span><span class="w"> </span>\<span class="n">Leftarrow</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span><span class="mi">1</span><span class="p">},</span><span class="w"> </span><span class="n">p_</span><span class="p">{</span><span class="mi">2</span><span class="p">}</span>\<span class="p">)</span>
<span class="w">            </span><span class="k">if</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">world_changed</span><span class="p">(</span><span class="w"> </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">step</span><span class="w"> </span>\<span class="p">((</span><span class="n">a</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="n">then</span>
<span class="w">                </span>\<span class="p">(</span><span class="n">Y</span><span class="w"> </span>\<span class="n">leftarrow</span><span class="w"> </span><span class="n">Y</span><span class="w"> </span>\<span class="n">cup</span><span class="w"> </span><span class="n">a</span>\<span class="p">)</span>
<span class="w">            </span>\<span class="p">(</span>\<span class="n">mathcal</span><span class="p">{</span><span class="n">E</span><span class="p">}</span>\<span class="p">)</span><span class="o">.</span><span class="n">load</span><span class="w"> </span>\<span class="p">((</span><span class="n">s</span><span class="p">)</span>\<span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Restore</span><span class="w"> </span><span class="n">saved</span><span class="w"> </span><span class="n">game</span><span class="w"> </span><span class="n">state</span>
<span class="w">        </span><span class="k">return</span><span class="w"> </span>\<span class="p">(</span><span class="n">Y</span>\<span class="p">)</span>
</code></pre></div>

<h3>4.6 Handicaps</h3>
<p>To ease the difficulty of IF games, Jericho has the option of the following handicaps:</p>
<ul>
<li>Inputs: Addition of a location description, player's inventory, and game score.</li>
<li>Outputs: Game-specific templates $\mathcal{T}$ and vocabulary $\mathcal{V}$.</li>
<li>World Objects: Use of world object tree for identifying interactive objects or player's current location.</li>
<li>Valid Actions: Use of world-changed-detection to identify valid actions.</li>
</ul>
<p>For reproducibilty, we report the handicaps used by all algorithms in the next section and encourage future work to do the same.</p>
<h2>5 Algorithms</h2>
<p>IF game playing has been approached from the perspective single-game agents that are trained and evaluated on the same game and general game playing agents which are trained and evaluated on different sets of games. In this section we present three agents: a choice-based single-game agent (DRRN), a parser-based single-game agent (TDQN), and a parser-based general-game agent (NAIL).</p>
<h3>5.1 Common Input Representation</h3>
<p>The input encoder $\phi_{a}$ converts observations into vectors using the following process: Text observations are tokenized by a SentencePiece model (Kudo and Richardson 2018) using an 8000-large vocab trained on strings extracted from http://www.allthingsjacq.com/index.html sessions of humans playing a variety of different IF games. Tokenized observations are processed by separate GRU encoders for
<img alt="img-0.jpeg" src="img-0.jpeg" /></p>
<p>Figure 2: DRRN architecture estimates a joint Q-Value $Q(o, a)$ over the observation $o$ and an action $a$. The observation encoder $\phi_{o}$ uses separate GRUs to process the narrative text $o_{\text {nar }}$, the players inventory $o_{\text {inv }}$, and the location description $o_{\text {desc }}$ into a vector $\nu_{o}$. DRRN uses a separate $G R U_{a}$ for processing action text into a vector $\nu_{a}$. The action-scorer $\phi_{a}$ concatenates the input and action vectors and outputs a scalar Q-Value.
the narrative (i.e., the game's response to last action), description of current location, contents of inventory, and previous text command. The outputs of these encoders are concatenated into a vector $\nu_{o}$. DRRN and Template-DQN build on this common input representation.</p>
<h3>5.2 DRRN</h3>
<p>The Deep Reinforcement Relevance Network (DRRN) (He et al. 2016) is an algorithm for choice-based games that present a set of valid actions $A_{\text {valid }}(s)$ at every game state $s$. We re-implement DRRN using a GRU $\phi_{\text {act }}(a)$ to encode each valid action into a vector $\nu_{a}$, which is concatenated with the encoded observation vector $\nu_{o}$. Using this combined vector, DRRN then computes a Q-Value $Q(o, a)$ estimating the total discounted reward expected if action $a$ is taken and $\pi_{D R R N}$ is followed thereafter. This procedure is repeated for each valid action $a_{i} \in A_{\text {valid }}(s)$. Action selection is performed by sampling from a softmax distribution over $Q\left(o, a_{i}\right)$. The network is updated by sampling a minibatch of transitions $\left(o, a, r, o^{\prime}, A_{\text {valid }}\left(s^{\prime}\right)\right) \sim \mathcal{D}$ from the prioritized replay memory (Schaul et al. 2016) and minimizing the temporal difference error $\delta=r+\gamma *$ $\max _{a}^{\prime} Q\left(o^{\prime}, a^{\prime}\right)-Q(o, a)$. From an optimization perspective, rather than performing a separate forward pass for each valid action, we batch valid-actions and perform a single forward pass computing Q-Values for all valid actions.</p>
<p>In order to make DRRN applicable to parser-based IF games, it's necessary to identify the list of valid actions available at each step. This is accomplished through a search over template-based actions, pruned by Jericho's world</p>
<p><img alt="img-1.jpeg" src="img-1.jpeg" /></p>
<p>Figure 3: Template-DQN estimates Q-Values $Q(o, u)$ for all templates $u \in \mathcal{T}$ and $Q(o, p)$ for all vocabulary $p \in \mathcal{V}$. Similar to DRRN, separate GRUs are used to encode each component of the observation, including the text of the previous action $a_{t-1}$.
change detection (Algorithm 1). This handicap bypasses language generation, one of the challenges of IF games.</p>
<h3>5.3 Template-DQN</h3>
<p>LSTM-DQN (Narasimhan, Kulkarni, and Barzilay 2015) is an agent for parser-based games that handles the combinatorial action space by generating verb-object actions using a set possible verbs and possible objects. Specifically, LSTMDQN uses two output layers to estimate Q-Values over possible verbs and objects. Actions are selected by pairing the maximally valued verb with the maximally valued noun.</p>
<p>Template-DQN (TDQN) extends LSTM-DQN by incorporating template-based action generation (Section 4.1). This is accomplished using three output heads: one for estimating Q-Values over templates $Q(o, u) ; u \in \mathcal{T}$ and two for estimating Q-Values $Q\left(o, p_{1}\right), Q\left(o, p_{2}\right) ; p \in \mathcal{V}$ to fill in the blanks of the template using vocabulary.</p>
<p>The largest action space considered in the original LSTMDQN paper was 222 actions ( 6 verbs and 22 objects). In contrast, Zork1 has 237 templates with a 697 word vocabulary, yielding an action space of 115 million. Computationally, this space is too large to naively explore as the vast majority of actions will be un-grammatical or contextually irrelevant. To help guide the agent towards valid actions, we introduce a supervised binary-cross entropy loss based on the valid actions in the current state. Valid actions are identified using the same procedure as in DRRN. This loss is mixed with the standard temporal difference error during each update.</p>
<p>We further optimize by decoding large batches of actions with each forward pass over the network and executing them sequentially until a valid action is found. When decoding actions that are often invalid, this provides a considerable speedup compared to performing an separate forward pass for each action.</p>
<h3>5.4 NAIL</h3>
<p>NAIL (Hausknecht et al. 2019) is the state-of-the-art agent for general interactive fiction game playing (Atkinson et al. 2018). Rather than being trained and evaluated on a single game, NAIL is designed to play unseen IF games and accumulate as much score as possible in a single episode. Operating without the handicaps outlined in Section 4.6, NAIL employs a set of manually-created heuristics to build a map of objects and locations, reason about which actions were valid or invalid, and uses a web-based language model to decide how to interact with novel objects. We include NAIL's performance on the same set of games in order to highlight the difference between general and single-game playing agents, and to provide a reference scores for future work in general IF game playing.</p>
<h2>6 Experiments</h2>
<p>We evaluate the agents across a set of thirty-two Jerichosupported games with the aims of 1) showing the feasibility of reinforcement learning on a variety of different IF games, 2) creating a reproducible benchmark for future work, 3) investigating the difference between choice-based and template-based action spaces, and 4) comparing performance of general IF game playing agents (NAIL), singlegame agents (DRRN and TDQN), and a random agent (RAND) which uniformly sample commands from a set of canonical actions: {north, south, east, west, up, down, look, inventory, take all, drop, yes $}$.</p>
<p>Results in Table 1, supported by learning curves in Figure 4 show that reinforcement learning is a viable approach for optimizing IF game playing agents across many different types of games. Experimentally, TDQN and DRRN agents are trained and evaluated on each game individually, but their hyperparameters are fixed across the different games.</p>
<p>In order to quantify overall progress towards story completion, we normalize agent score by maximum possible game score averaged across all games. The resulting progress scores are as follows: RANDOM $1.8 \%$, NAIL $4.9 \%$, TDQN $6.1 \%$, and DRRN $1.7 \%$ completion.</p>
<p>Comparing the different agents, the random agent shows that more than simple navigation and take actions are needed to succeed at the vast majority of games. Comparing DRRN to TDQN highlights the utility of choice-based game playing agents who need only estimate Q-Values over pre-identified valid-actions. In contrast, TDQN needs to estimate Q-Values over the full space of templates and vocabulary words. As a result, we observed that TDQN was more prone to overestimating Q-Values due to the Q-Learning update computing a max over a much larger number of possible actions.</p>
<p>Comparing the general game playing NAIL agent to single-game agents: the NAIL agent performs surprisingly well considering it uses no handicaps, no training period, and plays the game for only a single episode. It should be noted that the NAIL agent was developed on many of the same games used in this evaluation. The fact that the reinforcement learning agents outperform NAIL serves to highlight the difficulty of engineering an IF agent as well as</p>
<p>the promise of learning policies from data rather than handcoding heuristics.</p>
<p>Broadly, all algorithms have a long way to go before they are solving games of even average difficulty. Five games prove too challenging for any agents to get a nonzero reward. Games like Anchorhead are highly complex and others pose difficult exploration problems like 9:05 which features only a single terminal reward indicating success or failure at the end of the episode.</p>
<p>Additional experiment details and hyperparameters are located in the supplementary material.</p>
<p>| Game | $|T|$ | $|V|$ | RAND | NAIL | TDQN | DRRN |
| :--: | :--: | :--: | :--: | :--: | :--: |
| 905 | 82 | 296 | 0 | 0 | 0 | 0 |
| acorncourt | 151 | 343 | 0 | 0 | .05 | .33 |
| anchor | 280 | 2257 | 0 | 0 | 0 | 0 |
| advent | 189 | 786 | .1 | .1 | .1 | .1 |
| adventureland | 156 | 398 | 0 | 0 | 0 | .21 |
| afflicted | 146 | 762 | 0 | 0 | .02 | .03 |
| awaken | 159 | 505 | 0 | 0 | 0 | 0 |
| balances | 156 | 452 | 0 | .2 | .09 | .2 |
| deephome | 173 | 760 | 0 | .04 | 0 | 0 |
| detective | 197 | 344 | .32 | .38 | .47 | .55 |
| dragon | 177 | 1049 | 0 | .02 | $-.21$ | $-.14$ |
| enchanter | 290 | 722 | 0 | 0 | .02 | .05 |
| gold | 200 | 728 | 0 | .03 | .04 | 0 |
| inhumane | 141 | 409 | 0 | 0 | 0 | 0 |
| jewel | 161 | 657 | 0 | .02 | 0 | .02 |
| karn | 178 | 615 | 0 | .01 | 0 | .01 |
| library | 173 | 510 | 0 | .03 | .21 | .57 |
| ludicorp | 187 | 503 | .09 | .06 | .04 | .09 |
| moonlit | 166 | 669 | 0 | 0 | 0 | 0 |
| omnique | 207 | 460 | 0 | .11 | .34 | .1 |
| pentari | 155 | 472 | 0 | 0 | .25 | .39 |
| reverb | 183 | 526 | 0 | 0 | .01 | .16 |
| snacktime | 201 | 468 | 0 | 0 | .19 | 0 |
| sorcerer | 288 | 1013 | .01 | .01 | .01 | .05 |
| spellbrkr | 333 | 844 | .04 | .07 | .03 | .06 |
| spirit | 169 | 1112 | .01 | 0 | 0 | 0 |
| temple | 175 | 622 | 0 | .21 | .23 | .21 |
| tryst205 | 197 | 871 | 0 | .01 | 0 | .03 |
| yomomma | 141 | 619 | 0 | 0 | 0 | .01 |
| zenon | 149 | 401 | 0 | 0 | 0 | 0 |
| zork1 | 237 | 697 | 0 | .03 | .03 | .09 |
| zork3 | 214 | 564 | .03 | .26 | 0 | .07 |
| ztuu | 186 | 607 | 0 | 0 | .05 | .22 |</p>
<p>Table 1: Normalized scores for Jericho-supported games. Results are averaged over the last hundred episodes of training and across five independent training runs (i.e., with different seeds for initializing the environment and agent sampling process) conducted for each algorithm.</p>
<h2>7 Notable Games</h2>
<p>Jericho supports a vast array of games, covering a diverse set of structures and genres. These games provide us with different challenges from the perspective of reinforcement learning based agents. In this section, we highlight some specific challenges posed by Jericho supported games and provide notable examples for each of the types of challenges in addition to examples of games that the two types of deep reinforcement learning agents do well on. Learning curves for some of these examples using DRRN and TDQN are presented (Figure 4), underscoring the difficulties current rein-
forcement learning agents have in solving these games and showcasing effective training paradigms for different games.</p>
<h3>7.1 Sanity Checks</h3>
<p>The first set of games are essentially sanity checks, i.e. they are games that can be solved relatively well by existing agents. These games thus fall on the lower end on the difficulty spectrum and serve as good initial testbeds for developing new agents.</p>
<p>Detective in particular is one of the easier games, and with existing agents such as the random agent and NAIL being able to solve the majority of the game. The relatively good performance of the random agent is likely due to the game mostly requiring only navigational actions in order to accumulate score. On this game, TDQN has comparable performance to DRRN. Acorncourt also serves as a sanity check, albeit a more difficult one-with the DRRN outperforming all other agents. This game requires a higher proportion of higher complexity actions, which make generation-such as in the case of TDQN-more difficult. Omniquest is an example of a dungeon-crawler style game where TDQN outperforms the rest of the agents. In this game, due to there being a relatively smaller number of valid templates as compared to valid actions-i.e. many valid actions come from the same template-the TDQN has an effective search space that is smaller than the DRRN.</p>
<h3>7.2 Varying Rewards</h3>
<p>Most IF games provide you with positive scores in varying increments as you achieve objectives and negative scores for failing them. This reward structure is similar to most games in general and gives the player an indication of relative progress within the game. Some games such as Deephome and Adventure, however, provide relatively unintuitive scoring functions that can prove to be a challenge for reinforcement learning agents.</p>
<p>Deephome gives you an additional point of score for each new room that you visit in addition to rewards for achieving game objectives. This encourages exploration but could also prove tricky for an agent as it may not be necessary to finish the game. In Adventure, you start with a score of 36, and as the game progresses you are first given mostly negative scores followed by positive scores until you finish the game. As seen in Figure 4, this gives a reinforcement learning agent no indication that it is progressing in the short term.</p>
<h3>7.3 Moonshots</h3>
<p>Here we highlight some of the most difficult games in Jericho, current agents are quite far from being able to solve them. Zorkl is one of the original IF games and heavily influences later games using this medium. The game can best be described as a dungeon-crawler in which a player must make their way through a large dungeon, collecting treasures and fighting monsters along the way. The collection of these treasures forms the basis of Zorkl's scoring system, although some score is rewarded at intermediate steps to aid in finding the treasures. Being a dungeon-crawler, Zorkl features branching game path in terms of reward collection as well as stochasticity. The game can be completed</p>
<p><img alt="img-2.jpeg" src="img-2.jpeg" /></p>
<p>Figure 4: Episode score as a function of training steps for DRRN (top) and TDQN (bottom). Shaded regions denote standard deviation across five independent runs for each game. Additional learning curves in supplementary material.
in many different ways, often affected by the random movement of a thief and number of hits required to kill monsters. It is also interesting to note that NAIL and TDQN perform comparably on Zork1, with DRRN far outperforming them-indicating the difficulty of language generation in such a large state space. It has also been the subject of much prior work on IF game-playing agents (Zahavy et al. 2018; Yin and May 2019).</p>
<p>Anchorhead is a Lovecraftian horror game where the player must wade through a complex puzzle-based narrative. The game features very long term dependencies in piecing together the information required to solve its puzzles and is complex enough that it has been the subject of prior work on cognition in script learning and drama management ( Giannatos et al. 2011). This complexity is further reflected in the size of the vocab and number of templates-it has the largest action space of any Jericho supported game. None of our agents are able to accumulate any score on this game.</p>
<p>Although Anchorhead's game structure is more sequential than Zork1, it also contains a more sparse reward-often giving you a positive score only after the completion of a puzzle. It is also stochastic, with the exact solution depending on the random seed supplied when the game is started.</p>
<h2>8 Future Work</h2>
<p>Unsupervised learning: DRRN and TDQN agents were trained and evaluated on individual games. While this is sufficient for a proof-of-concept, it falls short of demonstrating truly general IF game playing. To this end, it's valuable to evaluate agents on a separate set of games which they have not been trained on. In the Jericho framework, we propose to use the set of Jericho supported games as a test set and the larger set of unsupported games ${ }^{4}$ as the training set. In this</p>
<p><sup id="fnref3:0"><a class="footnote-ref" href="#fn:0">1</a></sup>paradigm, it's necessary have a strong unsupervised learning component to guide the agent's exploration and learning since unsupported games do not provide rewards, and in fact many IF games do not have scores. We hypothesize that surrogate reward functions, like novelty-based rewards (Bellemare et al. 2016; Pathak et al. 2017), will be useful for discovering locations, successful interactions and objects.</p>
<p>Better Template-based Agents: There are several directions for creating better template-based agents by improving on the limitations of TDQN: When generating actions, TDQN assumes independence between templates and vocabulary words. To understand the problem with this assumption consider the templates "go .." and "take .." and the vocabulary words "north, apple". Independently, "take" and "north" may have the highest Q-Values together yield the invalid action "take north". Conditional generation of words based on the chosen template may go far to improve the quality of TDQN's actions.</p>
<p>Second, recent work on transformer-based neural architectures has yielded impressive gains in many NLP tasks (Devlin et al. 2018), including text-adventure game dialogues (Urbanek et al. 2019). We expect these advances may be applicable to human-made IF games, but will need to be adapted from a supervised training regime into reinforcement learning.</p>
<h2>9 Conclusion</h2>
<p>Interactive Fiction games are rich narrative adventures that challenge even skilled human players. In contrast to other video game environments like ALE (Machado et al. 2017), Vizdoom (Kempka et al. 2016), and Malmo (Johnson et al. 2016), IF games stress natural language understanding and commonsense reasoning, and feature combinatorial action spaces. To aid in the study of these environment, we introduce Jericho, an experimental platform with the key of feature of extracting game-specific action templates and vocabulary. Using these features, we proposed a novel templatebased action space which serves to reduce the complexity of full scale language generation. Using this space, we introduced the Template-DQN agent, which generates actions first by selecting a template then filling in the blanks with words from the vocabulary.</p>
<p>We evaluated Template-DQN as well as a choice-based agent DRRN and a general IF agent NAIL on a set of thirtytwo human-made IF games. Overall, DRRN outperformed the other agents with Template-DQN in second place. However, in many senses these agents represent very different training paradigms, sets of assumptions, and levels of handicap. Rather than comparing agents we aim to provide benchmark results for future work in these three categories of IF game playing. All in all, we believe Jericho can help the community propel research on language understanding agents and expect these environments can serve the community as benchmarks for years to come.</p>
<h2>References</h2>
<p>Ammanabrolu, P., and Riedl, M. O. 2019a. Playing text-adventure games with graph-based deep reinforcement</p>
<p>learning. In Proceedings of 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019.
Ammanabrolu, P., and Riedl, M. O. 2019b. Transfer in deep reinforcement learning using knowledge graphs. CoRR abs/1908.06556.
Atkinson, T.; Baier, H.; Copplestone, T.; Devlin, S.; and Swan, J. 2018. The text-based adventure AI competition.
Bellemare, M. G.; Srinivasan, S.; Ostrovski, G.; Schaul, T.; Saxton, D.; and Munos, R. 2016. Unifying count-based exploration and intrinsic motivation. CoRR abs/1606.01868.
Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Schulman, J.; Tang, J.; and Zaremba, W. 2016. Openai gym.
Côté, M.-A.; Kádár, A.; Yuan, X.; Kybartas, B.; Barnes, T.; Fine, E.; Moore, J.; Hausknecht, M.; Asri, L. E.; Adada, M.; Tay, W.; and Trischler, A. 2018. Textworld: A learning environment for text-based games. CoRR abs/1806.11532.
Devlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2018. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR abs/1810.04805.
Fulda, N.; Ricks, D.; Murdoch, B.; and Wingate, D. 2017. What can you do with a rock? affordance extraction via word embeddings. In IJCAI, 1039-1045.
Giannatos, S.; Nelson, M. J.; Cheong, Y.-G.; and Yannakakis, G. N. 2011. Suggesting New Plot Elements for an Interactive Story. In In Workshop on Intellignet Narrative Technologies (INT'11).
Gibson, J. J. 1977. "The theory of affordances," in Perceiving, Acting, and Knowing. Towards an Ecological Psychology. Number eds Shaw R., Bransford J. Hoboken, NJ: John Wiley \&amp; Sons Inc.
Hausknecht, M. J.; Loynd, R.; Yang, G.; Swaminathan, A.; and Williams, J. D. 2019. NAIL: A general interactive fiction agent. CoRR abs/1902.04259.
He, J.; Chen, J.; He, X.; Gao, J.; Li, L.; Deng, L.; and Ostendorf, M. 2016. Deep reinforcement learning with a natural language action space. In $A C L$.
Jaderberg, M.; Mnih, V.; Czarnecki, W. M.; Schaul, T.; Leibo, J. Z.; Silver, D.; and Kavukcuoglu, K. 2016. Reinforcement learning with unsupervised auxiliary tasks. CoRR abs/1611.05397.
Johnson, M.; Hofmann, K.; Hutton, T.; and Bignell, D. 2016. The malmo platform for artificial intelligence experimentation. In IJCAI, IJCAI'16, 4246-4247. AAAI Press.
Kempka, M.; Wydmuch, M.; Runc, G.; Toczek, J.; and Jaśkowski, W. 2016. ViZDoom: A Doom-based AI research platform for visual reinforcement learning. In CIG, 341348. Santorini, Greece: IEEE. The best paper award.</p>
<p>Kudo, T., and Richardson, J. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. CoRR abs/1808.06226.
Machado, M. C.; Bellemare, M. G.; Talvitie, E.; Veness, J.; Hausknecht, M. J.; and Bowling, M. 2017. Revisiting the
arcade learning environment: Evaluation protocols and open problems for general agents. CoRR abs/1709.06009.
Mikolov, T.; Chen, K.; Corrado, G.; and Dean, J. 2013. Efficient estimation of word representations in vector space. CoRR abs/1301.3781.
Narasimhan, K.; Kulkarni, T. D.; and Barzilay, R. 2015. Language understanding for text-based games using deep reinforcement learning. In EMNLP, 1-11.
OpenAI; Andrychowicz, M.; Baker, B.; Chociej, M.; Józefowicz, R.; McGrew, B.; Pachocki, J. W.; Pachocki, J.; Petron, A.; Plappert, M.; Powell, G.; Ray, A.; Schneider, J.; Sidor, S.; Tobin, J.; Welinder, P.; Weng, L.; and Zaremba, W. 2018. Learning dexterous in-hand manipulation. CoRR abs/1808.00177.
Pathak, D.; Agrawal, P.; Efros, A. A.; and Darrell, T. 2017. Curiosity-driven exploration by self-supervised prediction. CoRR abs/1705.05363.
Schaul, T.; Quan, J.; Antonoglou, I.; and Silver, D. 2016. Prioritized experience replay. In International Conference on Learning Representations.
Sutton, R. S., and Barto, A. G. 1998. Introduction to Reinforcement Learning. Cambridge, MA, USA: MIT Press, 1st edition.
Thrun, S.; Burgard, W.; and Fox, D. 2005. Probabilistic Robotics (Intelligent Robotics and Autonomous Agents). The MIT Press.
Urbanek, J.; Fan, A.; Karamcheti, S.; Jain, S.; Humeau, S.; Dinan, E.; Rocktäschel, T.; Kiela, D.; Szlam, A.; and Weston, J. 2019. Learning to speak and act in a fantasy text adventure game. volume abs/1903.03094.
Yin, X., and May, J. 2019. Comprehensible context-driven text game playing. CoRR abs/1905.02265.
Yuan, X.; Côté, M.; Sordoni, A.; Laroche, R.; des Combes, R. T.; Hausknecht, M. J.; and Trischler, A. 2018. Counting to explore and generalize in text-based games. CoRR abs/1806.11525.
Yuan, X.; Côté, M.-A.; Fu, J.; Lin, Z.; Pal, C.; Bengio, Y.; and Trischler, A. 2019. Interactive language learning by question answering.
Zahavy, T.; Haroush, M.; Merlis, N.; Mankowitz, D. J.; and Mannor, S. 2018. Learn what not to learn: Action elimination with deep reinforcement learning. In Bengio, S.; Wallach, H.; Larochelle, H.; Grauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Advances in Neural Information Processing Systems 31. Curran Associates, Inc. 3562-3573. Zelinka, M. 2018. Using reinforcement learning to learn how to play text-based games. CoRR abs/1801.01999.</p>
<div class="footnote">
<hr />
<ol>
<li id="fn:0">
<p>${ }^{4}$ https://github.com/BYU-PCCL/z-machine-games&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:0" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>${ }^{2}$ Game trees are standardized representations for all Z-Machine games. To learn more see https://inform-fiction.org/zmachine/ standards/z1point1/index.html
${ }^{3}$ https://github.com/BYU-PCCL/z-machine-games&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>            </div>
        </div>

    </div>
</body>
</html>